---
Date Generated: November 19, 2024
Transcription Model: whisper medium 20231117
Length: 9658s
Video Keywords: []
Video Views: 1944
Video Rating: None
Video Description: In this episode of The Cognitive Revolution, Nathan welcomes back Zvi Mowshowitz for an in-depth discussion on the latest developments in AI over the past six months. They explore Ilya's new superintelligence-focused startup, analyze OpenAI's O1 model, and debate the impact of Claude's computer use capabilities. The conversation covers emerging partnerships in big tech, regulatory changes, and the recent OpenAI profit-sharing drama. Zvi offers unique insights on AI safety, politics, and strategic analysis that you won't find elsewhere. Join us for this thought-provoking episode that challenges our understanding of the rapidly evolving AI landscape.

Check out "Don't Worry About the Vase" Blog: https://thezvi.substack.com

Be notified early when Turpentine's drops new publication: https://www.turpentine.co/exclusiveaccess

SPONSORS:
Shopify: Shopify is the world's leading e-commerce platform, offering a market-leading checkout system and exclusive AI apps like Quikly. Nobody does selling better than Shopify. Get a $1 per month trial at https://shopify.com/cognitive

Notion: Notion offers powerful workflow and automation templates, perfect for streamlining processes and laying the groundwork for AI-driven automation. With Notion AI, you can search across thousands of documents from various platforms, generating highly relevant analysis and content tailored just for you - try it for free at https://notion.com/cognitiverevolution

Oracle Cloud Infrastructure (OCI): Oracle's next-generation cloud platform delivers blazing-fast AI and ML performance with 50% less for compute and 80% less for outbound networking compared to other cloud providers13. OCI powers industry leaders with secure infrastructure and application development capabilities. New U.S. customers can get their cloud bill cut in half by switching to 
OCI before December 31, 2024 at https://oracle.com/cognitive

SelectQuote: Finding the right life insurance shouldn't be another task you put off. SelectQuote compares top-rated policies to get you the best coverage at the right price. Even in our AI-driven world, protecting your family's future remains essential. Get your personalized quote at https://selectquote.com/cognitive

RECOMMENDED PODCAST:
Unpack Pricing - Dive into the dark arts of SaaS pricing with Metronome CEO Scott Woody and tech leaders. Learn how strategic pricing drives explosive revenue growth in today's biggest companies like Snowflake, Cockroach Labs, Dropbox and more.
Apple: https://podcasts.apple.com/us/podcast/id1765716600
Spotify: https://open.spotify.com/show/38DK3W1Fq1xxQalhDSueFg

CHAPTERS:
(00:00:00) Teaser
(00:01:03) About the Episode
(00:02:57) Catching Up
(00:04:00) Ilya's New Company
(00:06:10) GPT-4 and Scaling
(00:11:49) User Report: GPT-4 (Part 1)
(00:18:11) Sponsors: Shopify | Notion
(00:21:06) User Report: GPT-4 (Part 2)
(00:24:25) Magic: The Gathering (Part 1)
(00:32:34) Sponsors: Oracle Cloud Infrastructure (OCI) | SelectQuote
(00:34:58) Magic: The Gathering (Part 2)
(00:35:59) Humanity's Last Exam
(00:41:29) Computer Use
(00:47:42) Industry Landscape
(00:55:42) Why is Gemini Third?
(01:04:32) Voice Mode
(01:09:41) Alliances and Coupling
(01:16:31) Regulation
(01:24:58) Machines of Loving Grace
(01:33:23) Taiwan and Chips
(01:41:13) SB 1047 Veto
(02:00:07) Arc AGI Prize
(02:02:23) Deepfakes and UBI
(02:09:06) Trump and AI
(02:26:31) AI Manhattan Project
(02:32:05) Virtue Ethics
(02:38:40) Closing Thoughts
(02:40:37) Outro

SOCIAL LINKS:
Website: https://www.cognitiverevolution.ai
Twitter (Podcast): https://x.com/cogrev_podcast
Twitter (Nathan): https://x.com/labenz
LinkedIn: https://www.linkedin.com/in/nathanlabenz/
Youtube: https://www.youtube.com/@CognitiveRevolutionPodcast
Apple: https://podcasts.apple.com/de/podcast/the-cognitive-revolution-ai-builders-researchers-and/id1669813431
Spotify: https://open.spotify.com/show/6yHyok3M3BjqzR0VB5MSyk
---

# Zvi’s POV Ilya’s SSI, OpenAI’s o1, Claude Computer Use, Trump’s election, and more
**Cognitive Revolution:** [November 16, 2024](https://www.youtube.com/watch?v=aiP7z-hQ1gA)
*  Ilya said that this is no longer the age of scaling.
*  Once again, everyone is looking for the next thing.
*  Scaling the right thing matters more now than ever.
*  So basically the open AI attitude of,
*  we'll just add a zero to everything and see what happens
*  is not even what open AI is doing anymore, right?
*  They've moved on to a one
*  because their previous strategy wasn't working.
*  As a result of SB 1047 being vetoed,
*  the bill is not being used as the model
*  for other legislation in other places,
*  even though it is obviously the best model legislation
*  that we have available.
*  Instead, people are considering bills like the one in Texas,
*  which use a use case targeting EUA act style approach
*  to regulation of AI.
*  That approach has all of the costs you can imagine
*  and none of the benefits to existential risk.
*  If China wants to knock TSNC out of existence, it can.
*  If I was the US government,
*  I would either deliberately have a goal
*  of US chip independence 2027,
*  or I would be pretty damn committed to defending Taiwan.
*  Hello, and welcome back to the cognitive revolution.
*  Today is Zvi Maschewicz returns
*  for another wide ranging conversation
*  as we look back on the major developments in AI,
*  including the technical, strategic and political
*  that have transpired in the six months
*  since his last appearance.
*  We discuss Ilya's new startup,
*  which aims to go straight to super intelligence.
*  The utility or lack thereof of open AI's O1 model
*  for things like coding and magic, the gathering,
*  the likely impact of Claude's new computer use capabilities
*  and the recent discourse around the quote unquote,
*  end of scaling.
*  We also analyze the complex web of partnerships
*  emerging between big tech companies,
*  the regulatory outlook in the wake of the veto of SB 1047,
*  and of course, the latest open AI drama,
*  which this time centers around the question of who should
*  and what as they convert to a for-profit company.
*  Toward the end, we consider the potential impact
*  of Donald Trump's return to the presidency on AI futures.
*  And I picked Zvi's brain on how the AI safety community
*  should evolve its understanding of individual virtue
*  in this new political reality.
*  On all these topics and more,
*  Zvi delivers a mix of strategic analysis and insight
*  that you really can't get anywhere else.
*  Except of course, for his blog,
*  Don't Worry About the Vase,
*  which is online at thezvi.substack.com.
*  I personally find these exchanges super helpful,
*  both for maintaining general situational awareness
*  and in some cases for challenging my own mental models
*  about how to most effectively work
*  toward beneficial AI outcomes.
*  As always, if you're finding value in the show,
*  we'd appreciate a review on Apple podcasts or Spotify,
*  or you could just share it with a friend
*  who's trying to make sense
*  of the rapidly evolving AI landscape.
*  Of course, we welcome your feedback,
*  either via our website, cognitiverevolution.ai,
*  or by DMing me on your favorite social network.
*  Now, I hope you enjoy this latest discussion
*  with the great Zvi Maschowitz.
*  Zvi Maschowitz, welcome back to the Cognitive Revolution.
*  Thanks, I always enjoy it here.
*  So it's been a minute and the pace hasn't slowed down,
*  I wouldn't say, although we can maybe get
*  into the meta discussion and why people seem to be
*  even seeing that question increasingly differently
*  these days.
*  I went back and looked at the timestamp
*  for the last conversation,
*  and it was right after OpenAI and Google
*  had sort of previewed their respective voice modes.
*  And then I think literally as we were recording,
*  there were like several resignations
*  from OpenAI safety teams,
*  then we did a media part two to talk about that.
*  And certainly quite a bit has happened
*  in the intervening months.
*  So what I thought we would do is just run it down,
*  get your take on a bunch of stuff.
*  We're both in the business of keeping up with this
*  on a weekly, daily and hourly basis,
*  but not everybody can do that.
*  So this is kind of the zoomed out few months recap
*  and analysis, how's that sound?
*  I like it, yeah, let's do it.
*  Cool, looking through your blog
*  and just revisiting the news myself,
*  I thought that one story that kind of came and went,
*  but which has at least some chance of proving
*  to be maybe the most important story of all
*  over the last few months was
*  Ilya's announcement that he is starting a new company,
*  which is going direct to super intelligence,
*  safely of course.
*  And so nothing to worry about there,
*  but just basically just wanna let you know,
*  we're gonna go build super intelligence
*  and we'll let you know when we've got it,
*  no products and unclear how many updates
*  we'll get in the meantime.
*  It'll definitely be safe, we know that, right?
*  It's right there in the title.
*  So I would say their ability to do this
*  is obviously still questionable,
*  but to borrow a infamous phrase, strikingly plausible.
*  And that in and of itself is pretty crazy.
*  Certainly you couldn't have made such a plan
*  at all credible up until pretty recently.
*  What do you make of it?
*  And how often do you wake up at night
*  wishing or dreaming that you might have got
*  an unexpected update on their progress?
*  We have a little bit of an update just this week actually.
*  Ilya said that this is no longer the age of scaling.
*  That quote is the 2010s were the age of scaling,
*  they never back in the age of wonder and discovery.
*  Once again, everyone is looking for the next thing.
*  Scaling the right thing matters more now than ever.
*  So basically the open AI attitude of,
*  we'll just add a zero to everything and see what happens.
*  He's not even what open AI is doing anymore, right?
*  They moved on to a one
*  because their previous strategy wasn't working.
*  And it's obviously he's not gonna spill the secrets.
*  So we don't know how well it's going,
*  but presumably they're not finding it to be that easy.
*  But yeah, it's deliberately opaque.
*  It's deliberately hard for us to tell
*  what's going on inside.
*  I do know that I am skeptical
*  that they can achieve safe super intelligence
*  if they achieve super intelligence.
*  I'm also reasonably skeptical
*  they'll get to super intelligence by this direct method,
*  but definitely not impossible, I can't rule it out.
*  So my next topic was O1 and you're touching on that.
*  And then also this sort of meta debate
*  as to what's working, what's not working.
*  I find this quite confusing because on the one hand,
*  we have Altman and Dario,
*  and I would say also the DeepMind founders,
*  although not quite as recently,
*  all pretty clearly on record saying,
*  we know what we need to do.
*  We don't really see any fundamental barriers.
*  We think we're gonna get there
*  in the two to three year timeframe.
*  And then we also have this sort of end of scaling narrative
*  that has popped up.
*  It makes some intuitive sense to me when I'm like,
*  okay, 15 trillion tokens or whatever is already a lot.
*  And yeah, there's a lot more on the internet,
*  but a lot of it is crap.
*  Data quality matters more than quantity
*  has certainly been true in my experience,
*  albeit at much smaller scale
*  than what these guys are doing.
*  We've heard these reports also that O1 is,
*  in addition to of course,
*  introducing a new inference time scaling law
*  is being used to create training data
*  for the next generation of model,
*  the reported Orion model.
*  And they're still raising more and more billions
*  and they're still planning to build
*  bigger and bigger data centers.
*  And the number of H100s that people are training on
*  is going up.
*  Elon's got his Tennessee Valley Authority contract
*  with like record time to deploy a network 100,000 H100s.
*  Meta has announced that they are training Lama 4
*  or will be, I'm not sure if it's R,
*  or we're assuming will be training Lama 4
*  on 100,000 plus H100s.
*  So how do you sort through that?
*  In your mind, is the era of scaling over
*  or are they throwing,
*  is that like throwing people off the scent or what?
*  Right, is deep learning hitting a wall in air quotes?
*  So yeah, we have these two camps.
*  We have the people who are like,
*  well, look at all the lack of amazing products
*  that are coming out, it's all height.
*  You know, you've hit a wall,
*  doesn't get much better than this.
*  We're done, it gets very, very slowly.
*  And then everyone in the major AI companies is like,
*  yep, AGI soon end of world as we know it in 47
*  or thereabouts and then who knows what happens.
*  And then they had the strange forecast of
*  we will get AGI that should be transformational,
*  that can radically accelerate the ACVI progress
*  and do all these other things.
*  And then people like all of them are like,
*  well, then nothing much will change.
*  Like you're like, we'll continue the same way.
*  And it will basically be, it's a better economic growth.
*  So like you'll pay lower taxes
*  or inflation will go down or something.
*  But that story makes no sense, right?
*  Like we might not get AGI,
*  in which case we get the story of you have changed much
*  but things get a little bit better because AI is cool.
*  But if we get AGI, obviously,
*  like there's not gonna seem the same for very long.
*  It might get way better.
*  It might get way worse.
*  It might end.
*  We don't know.
*  But the story on it stays the same.
*  Certainly wouldn't be like a automatic default outcome.
*  It would be a, we deliberately engineered
*  that it stayed the same.
*  We chose under the hood to keep it the same
*  because we couldn't do better.
*  And there's some arguments for that being a reasonable choice
*  if you don't confidently know how to do better
*  but it's not a natural outcome at all.
*  And the rhetoric brings hollow, right?
*  It makes you wonder, do they really believe
*  that what they're describing is the AGI that's coming
*  is AGI where they try to hype you up,
*  slash down the Microsoft contract,
*  slash just make themselves feel if you're doing the thing.
*  Even if it's about to be weak,
*  you know, the meticulous definition is like not that strong
*  that kind of thing.
*  So with a little bit of a sharper head and all of this again.
*  So my reconciliation is something like the strategy
*  of just pushing forward by adding more zeros to everything.
*  The same strategy that went from GPT-1 to two to three to four.
*  There's some technique involved,
*  but you know, Jack Clark's impact right out.
*  It's like expertise is everywhere.
*  It's throughout compute and data.
*  That's coming to a close.
*  That's just marginal.
*  The marginal returns are declining,
*  even in scaling a lot of terms.
*  And you're not gonna get very far from here doing that.
*  And that's why O1, right?
*  That's why these other approaches.
*  But this is my way of understanding it.
*  By using what the models have to offer
*  to refine the training process
*  by using it for this time compute in various ways,
*  by using things like the O series of models.
*  You can get these labs people believe you can get that.
*  You can overcome this barrier.
*  We can pivot to a different scaling law,
*  to a different method.
*  And we can get where we need to go to the AGI.
*  That's where we sub-definition AGI reasonably soon.
*  Slash, they just have this statement,
*  straight line-time graphs don't stop, right?
*  That like, it's like Moore's law.
*  Something will happen.
*  We'll figure out something else.
*  Even though we're doing before it starts working,
*  don't worry about it.
*  We'll figure it out.
*  And there's a lot of talk about things
*  being stronger than we know, right?
*  We're talking about how everyone's under-upgrading on O1,
*  but we don't see the O1 he sees.
*  We see O1 preview, right?
*  And I gotta say, O1 preview, super unimpressive to me.
*  Having sat with it for a while, I basically don't use it.
*  I find it to be every time I ask O1 preview
*  for a question, it like thinks for a while,
*  talks about all of these relevant details,
*  gives me lots and lots of data points.
*  And like, none of it addresses the actual thing
*  I was trying to do.
*  It doesn't follow the conceptual logic
*  that I was trying to get it to go to,
*  the thing that's useful to me.
*  Like, why don't I use use on it?
*  I should use use on it.
*  It almost happens.
*  That's quite interesting.
*  I would say I have a different user report from O1.
*  Certainly the downsides are real.
*  The wait time of my Goldfish attention span
*  is such now that even a 45 second delay to first token
*  is enough for me to like tab over to something else
*  and forget what I was doing,
*  and then come back like eight minutes later.
*  And of course the answer's been sitting there
*  for seven minutes at that point.
*  So from a user experience standpoint, it is,
*  and I actually do this outside of chat GPT,
*  so that may be a difference as well,
*  because I've found that what I want to use O1 for,
*  where I have at least found utility for it so far,
*  is in mostly coding.
*  I've been doing a fair amount of coding lately,
*  partly because I have a little app idea I wanna build,
*  partly because I wanted to catch up with Cursors.
*  Obviously had a moment and I hadn't been coding
*  for a little bit, so I was just trying to get back into it.
*  And I found that RAG in general is a real pain in my butt.
*  I don't really like any of the RAG solutions
*  that I've ever used, almost like without exception.
*  That does extend to Claude projects and to GPTs.
*  I feel like I can stuff a lot of stuff in there,
*  but they're always a little too economical
*  in terms of what they bring into context.
*  And it's often opaque too,
*  so I'm like not sure what they're actually looking at
*  from whatever I gave them access to.
*  But that's another source of confusion.
*  Claude on the main consumer facing product
*  does allow you to just paste in long documents.
*  So I like that.
*  But chat GPT often doesn't allow me
*  to just paste in the long document.
*  It'll just straight up say that's too long of an input.
*  Sorry.
*  So then I'll end up normally not using,
*  the experience is better I would say with O1 in chat GPT
*  because it gives you these like reasoning summaries
*  to entertain you while you're going.
*  If you go over to the platform console type version,
*  you can there paste in however much content you have,
*  obviously up to its context window limit.
*  But it doesn't give you this sort of real time summary
*  of reasoning, you just have to wait.
*  Anyway, with all that said,
*  I still do find it useful to take the full code base
*  of the little app that I'm developing
*  and paste the whole thing in there.
*  And I have been doing head to heads with the latest Sonnet
*  and which is amazing of course too, and the O1
*  and asking them the same question.
*  I basically will typically say,
*  here is the full code base of an app I'm developing.
*  It's to help people create gold standard examples
*  for use in AI automation.
*  So I just give it like a one sentence summary of,
*  this is the purpose of this app.
*  And then I'll ask for whatever I want help with.
*  And if it is a local code change,
*  Sonnet is I would say hard to distinguish in many cases,
*  maybe even better, a little bit more precise with the outputs
*  it has the function calling and stuff like that.
*  So it does integrate into cursor better.
*  I don't use O1 through the cursor interface so much
*  because it doesn't like reliably respect their format
*  and in turn then the whole functionality will break.
*  But there is a certain way in which it does feel more
*  like a high level collaborator to me.
*  If I say what would be the highest priorities
*  for refactoring this application,
*  or I want to implement a new feature
*  and I'm not sure how best to do it.
*  I do see something qualitatively different there
*  where Claude will give you an answer.
*  And it might even be the best answer often enough,
*  but O1 does give me something extra
*  where it'll sometimes say,
*  here's three different ways you could do it.
*  This way is the most direct but hacky way
*  that you could do but isn't really recommended.
*  Here's another way that's like gonna be a big refactor
*  and maybe involve a couple new libraries.
*  And this might be like the enterprise way,
*  but it doesn't look like you're really coding that way.
*  And here's kind of the Goldilocks way that is
*  hopefully the best of both worlds
*  and what I would recommend to you
*  where it'll be like lighter lift,
*  but not a terrible solution.
*  And I do feel like it guides me toward better strategies
*  in a way that I have not seen other things do.
*  So I had finally gotten into actually
*  using the out of child code.
*  I found cursing to be a godsend versus not using cursor.
*  I can't imagine O1 being good enough
*  that if it doesn't integrate into cursor
*  that I would want to not use cursors integration.
*  But it just has to be so far ahead.
*  We jumped from quads on it without cursor
*  to quads on it, the same exact model with cursor
*  with a 10X.
*  So it's like, well, I'm giving up that 10X
*  because it seems so insane.
*  And the thing you do,
*  and all I've gotten is to do the thing you're describing.
*  If you actually just ask it,
*  what are the different ways you could do act,
*  it will tell you.
*  You just have to use semi reasonable profiting
*  and using side cursor will spit out possibilities.
*  I'll discuss it with it, we'll select the option.
*  I also discovered that you really want to focus on
*  using one model when you do this
*  because when there was an outage
*  and switch me over to some open AI model
*  instead of quad for a few hours,
*  that always could work.
*  Because every time I tried to act at anything,
*  it would just constantly have all these different
*  situations to try to take over the code.
*  So even if the model had been as good as quad,
*  which I didn't think it was, from what I could tell,
*  well, I don't want to have to just debug
*  all of the new codes that are playing all of you
*  existing debug code.
*  I got to use the off hold and all that.
*  Quad will switch it back when I hit back.
*  Which is just my problem, I have to assume.
*  So all of that sucks.
*  So I'm just going to wait this out,
*  is what I decided.
*  Like other things I can do for a few hours
*  and I figure it will come back.
*  But yeah, I haven't used O1 for coding
*  in a substantial amount.
*  Exactly this reason that it's not affordable
*  slash integrated enough to use it as your primary
*  in cursor and OVDUGS.
*  It's only possible that it has specific uses
*  where I get into a lot of sense.
*  This is a specific place where thinking for a long time
*  at this level is really, really helpful
*  for accepting what you want to do.
*  And it comes down to where do I think of a one preview
*  in general?
*  And it's like this thing where you have the
*  not so right student,
*  but he's spending a lot of time on every assignment
*  and she's trying really hard.
*  And she's really doing her best to turn it
*  as much work as possible and try to get the right,
*  but it's not that smart.
*  It's not any smarter under the hood than the 4-up basically.
*  So it's inherently limited by it.
*  So it just sort of my use cases,
*  it turns out that like VEX is not useful to me.
*  And in fact, when I use chat to be key,
*  which I shall do, I'm normally using it for the web search,
*  which is just incompatible for one guy
*  Hey, we'll continue our interview in a moment
*  after we're with our sponsors.
*  The cognitive revolution is brought to you by Shopify.
*  I've known Shopify as the world's leading e-commerce platform
*  for years, but it was only recently when I started a project
*  with my friends at quickly that I realized
*  just how dominant Shopify really is.
*  Quickly is an urgency marketing platform
*  that's been running innovative time-limited marketing
*  activations for major brands for years.
*  Now we're working together to build an AI layer,
*  which will use generative AI to scale their service
*  to long tail e-commerce businesses.
*  And since Shopify has the largest market share,
*  the most robust APIs,
*  and the most thriving application ecosystem,
*  we are building exclusively for the Shopify platform.
*  So if you're building an e-commerce business,
*  upgrade to Shopify and you'll enjoy not only
*  their market leading checkout system,
*  but also an increasingly robust library
*  of cutting edge AI apps like quickly,
*  many of which will be exclusive to Shopify on launch.
*  Cognitive revolution listeners can sign up
*  for a $1 per month trial period at Shopify.com slash cognitive
*  where cognitive is all lowercase.
*  Nobody does selling better than Shopify.
*  So visit Shopify.com slash cognitive
*  to upgrade your selling today.
*  That's Shopify.com slash cognitive.
*  As a cognitive revolution listener,
*  you're obviously interested in cutting edge AI technology.
*  And with that in mind,
*  I'm proud to say that this episode
*  is brought to you in part by Notion.
*  Notion has been a clear leader in high value applications
*  of generative AI since the wave began.
*  Earlier this year,
*  we had Notion AI engineer Linus Lee on the podcast.
*  The quality of his insights
*  showcase the caliber of talent that Notion employs.
*  And that inside look at how Notion builds with AI
*  is still extremely valuable.
*  Given my personal focus on AI automation recently,
*  I specifically wanted to highlight Notion's library
*  of workflow and automation templates.
*  If you're looking to streamline your processes
*  and lay the foundation for future AI driven automation,
*  these templates are an excellent starting point.
*  And even if you're not ready for full automation,
*  you'll benefit immediately from Notion AI.
*  Notion's latest all in one AI implementation
*  that searches through thousands of documents,
*  regardless of whether they live in Notion
*  or on some other platform like Slack or Google Docs,
*  to deeply understand the context of your work
*  and generate highly relevant analysis and content
*  just for you.
*  Notion is used by more than half of Fortune 500 companies,
*  helping teams reduce emails, meetings
*  and time spent searching for information.
*  Wanna try it?
*  Head to Notion.com slash Cognitive Revolution.
*  You can start for free and using our link supports the show.
*  So join me in giving Notion AI a shot today
*  at Notion.com slash Cognitive Revolution.
*  It's definitely yet to be unhoveled,
*  O1 preview in quite a few different ways.
*  Right, and it really says that O1 itself
*  is much better than O1 preview, right?
*  And you've seen various claims to that.
*  And then O2 will be obviously,
*  was always much better than O1.
*  And they claimed the scaling was applied,
*  so it'll get rapidly better.
*  And it's possible that it will,
*  but I have to express skepticism
*  that here like the first product, the MVP,
*  just didn't seem like something
*  that I almost never wanted to use.
*  And also my model of what it's doing
*  has an inherent restriction on it.
*  Right, so you're thinking really hard,
*  but you're not that smart.
*  Okay, what if we thought even harder?
*  What if we thought 10 times as much,
*  100 times as much, 1000 times as much as that,
*  but we're still not that smart?
*  And my experience with humans in this spot,
*  and my experience with what I got from O1 is,
*  it wouldn't help her, right?
*  Like you're gonna have to find a way down in this box
*  and I'm not sure you can.
*  But obviously I've been surprised before
*  and I could be very wrong.
*  We didn't have a very hollow version right now.
*  Like we can't feed it contracts in the proper ways.
*  We can't have it search the web,
*  we can't have it do a variety of things.
*  Who knows what would happen with the information now?
*  The new Cloud 35 Sonnet to give it its due
*  is also definitely amazing.
*  And it's a good point that it may be,
*  my experience may be less about,
*  what constitutes a fair test or a fair comparison,
*  a fair head to head is like not always obvious.
*  Giving them the same question
*  when they have different default behavior
*  may not be the right way to go about it.
*  Maybe to do a fairer comparison,
*  I should prompt them each differently
*  or maybe the same, but like in a way that sort of tries
*  to get them to do a similar behavior.
*  I would say you should prompt them
*  the way you would prompt each one of them
*  that if it happens to be the same,
*  which your author actually is,
*  then the same problem is fine.
*  I would also add that the time delay
*  just doesn't bother me very much.
*  That's not the issue here.
*  In fact, in cursor, not only do I regularly run it,
*  I run it on a fast command,
*  I'm not going to get insight necessarily
*  what to keep paying for or words,
*  made it from the latest Norway various quality, right?
*  But also that it takes so long to write out the queries,
*  follow the code, even with fast commands on it.
*  Like it's not that inevitable one.
*  I am not used to the idea of
*  why am I thinking for substantial amount of code
*  with my question?
*  The moment I see it's actually answering my question,
*  I'm tabbing it.
*  I'm screening out.
*  I have a separate set of a desktop screens for coding.
*  And I have a desktop screen for writing.
*  And I always just tab back to write it,
*  like entirely screen shit and just complete load check.
*  And then come back because like I hear like a lot
*  of programmers, like most programmers,
*  they want to be in state, they want to focus
*  and have all this stuff in their head.
*  And they don't want to be distracted.
*  And I turn out that I'm the opposite,
*  whereas they might have to like just constantly fight
*  with these little details for too long in a row.
*  I will want to screen and much better for me
*  to do it in my five chunks.
*  I don't understand why my brain works that way,
*  but I'm able to contact you that way.
*  Like I don't lose date.
*  Great.
*  I get you just do this.
*  Yeah, I think I'm probably more like you than maybe the,
*  certainly it's hard to imagine that if the models go out,
*  I do stop coding.
*  That's for sure.
*  Because it's just so painful to sit and type code.
*  I feel like-
*  Yeah, you can do this tomorrow, right?
*  There's no ride.
*  If time is fun to you, there are other things you can do.
*  The way they do that.
*  Have you tried anything else making magic,
*  the gathering decks or anything like that with O1
*  that would be a test of its like ability to grind through?
*  Cause I feel like they're showing some pretty,
*  you know, the claim that there's a new scaling law
*  and that this inference time compute gives you like
*  better and better results,
*  albeit with a logarithmic curve,
*  but nevertheless, like in a not obviously bounded way,
*  it would seem like some of these tasks that are like
*  logical, creative, like combinatoric,
*  would be really well suited for that.
*  So I wonder if anything along the lines of a
*  magic deck creation.
*  I find that example so bizarre.
*  Just because, and it's first of all,
*  it's something that like the elements
*  is one that can tell those kids to do.
*  Right, in any interesting way at all.
*  And second of all,
*  that it is one of these tasks that's very
*  like core skill-graded.
*  And then you take a bad player and you give them a bump
*  on actively even playing games, changing cards,
*  thinking about stuff,
*  but you don't give them the information
*  of what's in other people's decks.
*  You don't let them just copy, right?
*  They won't get anywhere.
*  They will max out and a very, very low level
*  will be unusable with that.
*  Whereas if you give it to an expert tech builder,
*  they can build something almost immediately
*  with no feedback, with no iterations,
*  that will be pretty good.
*  And a lot of my best creations
*  and the best creations of other people that I know of
*  emerged like on the first day
*  or the first time you have an idea
*  very close to the final result.
*  It's all kind of very heuristic based theory crafting.
*  And it's not inference.
*  It's not inference on compute tasks, right?
*  Like inherently for humans.
*  And I would very much not expect a one
*  to be capable of anything of that type.
*  You can't logically branch out these things.
*  They get too complex, too fast.
*  There's too much randomness.
*  There's too many possibilities.
*  There's too much other information.
*  You're very much doing the pre-training style thing,
*  not the inference training style thing, right?
*  So to the extent that they can play,
*  it would be opus style things
*  that would be good at it, I would think.
*  But also we see that they can't play.
*  Someone used a quad's computer.
*  You sent it to a computer looking up to Magic Arena.
*  You see if it can play some magic.
*  And the answer is like it can try and play,
*  but very not well.
*  And deck building is so much harder than playing.
*  Most of the time in magic playing,
*  it's reasonable to just do reasonable things.
*  And then obviously over time,
*  if you're not strategic and you're not sophisticated,
*  you will run into problems
*  with people who notice the ways in which
*  you're not being strategic and you're not being reasonable.
*  But yeah, they couldn't do standard things that well.
*  I'm sure you could vastly improve it
*  if you cared enough on that.
*  But the task I'm always looking for is go into a new set
*  with nothing but a spoiler list.
*  That's not in your training data.
*  And then let's see you draft.
*  Let's see you decide what's good,
*  assemble a strategy and play it.
*  Spoiler list is like these are the cards
*  that are in this set.
*  Yeah, here's the 200 cards or 300 cards
*  that we're gonna be drafting from.
*  Here's how rare each one is, here's what each one does.
*  Right, and a human can do that
*  and then have a reasonable sense of what things are good
*  and bad and what to go after.
*  Slash, I do this thing where I did this actually last week.
*  I came my first draft yesterday
*  where I would just end up doing what's in the set.
*  I just look at the cards and I'm looking at them as I go.
*  And I'm learning as I go based on what's there,
*  what is the cards trying to tell me?
*  What are the strategies I'm supposed to be using?
*  What could be good?
*  What's inherently just naturally powerful?
*  I just work your way through it.
*  It absolutely can't be done.
*  There might be like an interesting cognitive style
*  difference here.
*  I feel like I played a little magic in my youth.
*  It's been a long time and I was never great at it
*  or particularly serious about it.
*  But I feel like it is more for me like coding.
*  Like there is some sort of like
*  in the app development process,
*  there's some sort of intuition moment
*  or some sort of often like experiential moment
*  where I'm like, oh, I wanna do this right now in this app
*  and the function doesn't exist.
*  Okay, I need to create it.
*  And then that's like, to quote Edison,
*  that's typically less than 1% of the effort
*  is that moment of inspiration.
*  And then comes obviously a lot more perspiration
*  of actually if I'm doing it by hand,
*  which I don't do anymore,
*  figuring out what files and how to change them
*  and making an implementation plan and typing it all out.
*  And that definitely is like a grind.
*  And I don't have any shortcuts from
*  that moment of inspiration to a solution
*  that don't involve like just gradually considering
*  all the different things that this touches
*  and all the files that it exists in and so on.
*  For a magic deck, I feel like I also recall
*  having some moments of, oh, this and this could be cool,
*  but then really building out the deck,
*  aren't you kind of working your way through like,
*  what if I added this one to it?
*  Yeah, that could be good, but not so good.
*  What if I added this one to it?
*  You're not going through that sort of,
*  there's no brute force for it.
*  You're just like, your decks like spring forth
*  from the head of Zvi fully formed.
*  You have a very good sense over time
*  what the best decks have to look like.
*  And you've already done this kind of research
*  where you look at what's possible, what's in the space,
*  what types of possibilities there are.
*  And by building these other decks
*  and pursuing these other strategies
*  and knowing what's out there,
*  you get a sense of like, what does the deck have to do?
*  What do decks typically need to do?
*  So you say, okay, I'm gonna play this combination
*  of three cards because this seems like a powerful thing.
*  I know what colors that has to be.
*  I know about how many lamps this deck's gonna have.
*  I know about where the meta distribution's
*  gonna have to be.
*  I know how many creatures I can afford to play.
*  All these numbers fall into place.
*  You start doing reasonable things.
*  You try several configurations.
*  There's always so many good cards
*  that sort of do the vaguely, the thing we want.
*  Sometimes you're like, I want something that I want
*  to five drop that like, it flies and it gets in this way
*  or you know, I can chant it.
*  It's this thing or whatever.
*  And you go searching, maybe you'll search for that.
*  But it's a pretty fast search because you either remember
*  or you know what type of strike call and you have an idea.
*  Yeah, occasionally you will do the literal thing
*  of especially when you're starting out
*  to a new set, new format where you just look through
*  like all the cards and you just pan through them.
*  You just like, okay, did any of these,
*  they might work in this strategy.
*  I just tried them all out.
*  You're on and rejecting like, oh, let's hope.
*  As this seems very powerful, you know what I was doing.
*  And then like what we're describing is something
*  I would occasionally do, especially in formats
*  that requires you to dig deep
*  or where you're looking for solution to a particular problem.
*  So we used to call it a crack search
*  where you just look for the entire spoiler list
*  and just go card by card.
*  Does this solve my problem?
*  Does this solve my problem?
*  Does this solve my problem?
*  And all of the answers is no, obviously not.
*  What am I even talking about?
*  And like with approach to R1,
*  okay, I have this problem of these pictureless
*  next floor removal.
*  What am I gonna do about it?
*  I'm playing white blue.
*  I know what my solution is.
*  And then I'm just going through this where I was
*  card by card, every single card,
*  and I see pure reflection, which is like a terrible card.
*  But it happens to solve this exact problem.
*  There's nobody's ever putting it in.
*  And I'm like, oh, I got it.
*  I got the solution.
*  I have nowhere to win.
*  I think I might have it.
*  Because in this particular narrow situation,
*  this will solve this specific problem.
*  When we try it, it works.
*  My sideboard, when to be the quarter final match.
*  How it's been moving in the final.
*  But that's not normally how it goes.
*  Like you are like, you're researching for patterns.
*  You're like trying stuff out.
*  Often this involves like a short period
*  of just playing a handful of games.
*  One thing you'll notice is a very good player
*  who has a lot of experience.
*  They can play like two games.
*  And they say, I know what this match is about.
*  I know what was going on with this deck.
*  This isn't gonna work.
*  I need to retool.
*  Because they can tell me if you're
*  either looking at different lines,
*  they're looking at your possibilities.
*  They can think about counterfactuals.
*  And in fact, we get so much more out of that input.
*  And they can just think 10, 100 times as best.
*  Whereas a bad player needs 10, 20, 30 games
*  to think out of the head what's going on here.
*  Practical pattern.
*  Hey, we'll continue our interview in a moment
*  after we're with our sponsors.
*  Even if you think it's a bit overhyped,
*  AI is suddenly everywhere.
*  From self-driving cars, to molecular medicine,
*  to business efficiency.
*  If it's not in your industry yet, it's coming and fast.
*  But AI needs a lot of speed and computing power.
*  So how do you compete without costs spiraling
*  out of control?
*  Time to upgrade to the next generation of the cloud.
*  Oracle Cloud Infrastructure or OCI.
*  OCI is a blazing fast and secure platform
*  for your infrastructure, database,
*  application development, plus all your AI
*  and machine learning workloads.
*  OCI costs 50% less for compute and 80% less for networking.
*  So you're saving a pile of money.
*  Thousands of businesses have already upgraded to OCI,
*  including MGM resorts, specialized bikes,
*  and fireworks AI.
*  Right now, Oracle is offering to cut your current cloud bill
*  in half if you move to OCI for new US customers
*  with minimum financial commitment.
*  Offer ends 12-31-24.
*  So see if your company qualifies for this special offer
*  at oracle.com slash cognitive.
*  That's oracle.com slash cognitive.
*  There are so many things in life we just never get around to.
*  Taking up that hobby, cleaning out the garage,
*  little things that don't really make huge differences
*  in our lives.
*  Yet there's one thing that most of us have probably
*  been neglecting that can have a huge impact on our family's
*  future.
*  It's life insurance.
*  And with select quote, getting covered
*  with the right policy for you is easier and more affordable
*  than you might think.
*  As someone who tracks AI progress on a full-time basis
*  and obsesses about its potential impact nonstop,
*  I know how tempting it can be to ignore
*  more mundane, familiar risks.
*  There's always another paper to read, podcast to listen to,
*  or product to try.
*  And yet the smartest people that I know in the AI space
*  continue to save and invest money for the future,
*  carve out time for their relationships,
*  maintain their physical and mental health,
*  and yes, protect their family with life insurance,
*  just in case anything should happen before the singularity.
*  If nothing else, it's one less thing
*  to worry about in a time of unprecedented change.
*  So get the right life insurance for you,
*  for less, at select quote dot com slash cognitive.
*  Go to select quote dot com slash cognitive today
*  to get started.
*  That's select quote dot com slash cognitive.
*  Are you aware of any projects to try to get an AI system
*  to play Magic at a high level?
*  Obviously we've seen poker beaten,
*  effectively beaten, and diplomacy.
*  Yeah, not in any serious efforts.
*  Magic is a vastly harder game than the AI.
*  I think Magic is very close to adequate.
*  If you show me your system can do the test
*  where we are gonna draft a format of new cards
*  you have never seen before,
*  so they're not just slightly different numbers
*  than cards you have seen before, maybe even that,
*  and you're gonna be able to play at a level of strong humans
*  in the resulting games with you that you created.
*  Yeah, like you could try hacking together,
*  but if you can do that without just hard coding
*  a lot of weird stuff and do it in a really hacky,
*  like you do it in a...
*  Yeah, this guy is a little bit whatever you just did.
*  Like it's just so general what you're doing.
*  I mean, that new mechanic could just work in different ways
*  and it just figures out what they need in real time.
*  Yeah, interesting.
*  So I guess maybe final point on O1,
*  my sort of headline summary for basically since GPT-4
*  has been in terms of like where AI is today,
*  what it can do, what it can't do,
*  that the best AIs are closing in
*  on human expert performance on routine tasks.
*  Obviously all those words,
*  fairly carefully chosen, carrying weight,
*  especially routine and task.
*  O1 is the first one that shows a bar
*  that is higher than the human expert level on MMLU,
*  for example, where they put that pretty much at like 90%
*  and everything before O1 was getting close,
*  but then they'll 88, 89,
*  one or two points behind the human expert
*  and then O1 they get like 92.
*  And so on the one hand is like a couple of points better.
*  On the other hand, maybe it's crossed some threshold.
*  I mean, those tests are fairly noisy.
*  Do you think that has much meaning
*  or would you infer anything from that
*  when it comes to whether there's like a top out
*  at human expert level?
*  It sounds a lot like the thing where like
*  meets over for a task at jumping between 49%, 51%
*  and everybody suddenly, whoa, there's this level
*  that's like considered human level,
*  but it's arbitrary, right?
*  Because which human, right?
*  And natural human device in practice
*  is gonna score almost 90%.
*  The best human would be-
*  Yeah, that's down to two, I think.
*  The best human is gonna score more than 90%, I assume.
*  She's not like human, that possible human.
*  So there's not like 90% is an arbitrarily chosen,
*  but see how we're basically doing it,
*  but there's no reason it'd be a wall.
*  Particularly, like not a hard wall.
*  And if everything is after 30 for a while,
*  then somebody jumps the asymptote.
*  Then it's an interesting whether or not
*  that was a human level asymptote.
*  But as far as the MOU, yeah,
*  all the questions are they missing
*  the same questions over and over again?
*  Right, like is it, you know,
*  you just look at 90% of the questions are doable.
*  And a different part of the questions
*  are either have the wrong answer or the code base
*  in the answer key or are ambiguous
*  or are just incredibly harder and they just can't be done.
*  And therefore, we get this wall on MOU,
*  which may or may not represent a real wall.
*  Right, I don't know, I haven't looked at these things,
*  but different evals have different quirks to them.
*  But in general, once you get 90% on an eval
*  and to a single calorie eval period anymore,
*  you should move on to the next eval.
*  Humanity's Last Exam coming soon.
*  Contribute your extremely difficult questions
*  to Dan Hendrickson's squad.
*  I mean, like the name Humanity's Last Exam,
*  they understand that if the AI passes this exam,
*  then there's no longer much time for us
*  to be answering your question.
*  Maybe you could formulate a magic question
*  for Humanity's Last Exam.
*  You'd be the perfect person to do it.
*  I mean, we could, but you know, we got this handled.
*  I do find it sobering, honestly,
*  how, you know, and I've flattered myself
*  as a reasonably smart person, I'm like,
*  what questions could I contribute to Humanity's Last Exam?
*  Do I know anything that's actually hard enough
*  to merit inclusion in that test set?
*  I don't know.
*  I think you would actually have a good one with magic
*  if you wanted to do it.
*  I'm looking around my life and I'm like,
*  not sure I'm doing anything that's actually hard enough
*  to get into Humanity's Last Exam.
*  I think a lot of what I do
*  is the kind of thing that doesn't go on the program
*  in the sense that it's not that I am doing
*  this very hard specific task.
*  That's just not like where the difficulty
*  in my job comes from.
*  It's just the volume of information
*  and the connection drawing across
*  connection drawing.
*  White spaces.
*  Knowing which questions to ask to emphasize,
*  being able to iterate off of like how to explain things
*  and how to draw things together.
*  Just a lot of like a way of building a map
*  and understanding things
*  and being able to hold a lot of this together
*  and think carefully about all these different questions
*  and how they relate to each other.
*  But it's not like there's one specific hard question
*  where you have to go really deep.
*  I think I did more of that earlier in my career,
*  this when I was trading or playing magic,
*  when you would really focus very narrowly
*  on getting the exact right answer
*  to a very valuable question
*  where you're having effectively a competition
*  to find the best possible answer.
*  You're up against someone else
*  or you were trying to be the best.
*  And now I'm doing something that no one else is doing.
*  To a large extent, I'm just very unique.
*  And there it's like very much not about
*  getting that last 4% of efficiency.
*  It's about the big conceptual questions
*  and it's about being able to efficiently like iterate
*  and change things towards where you wanna go.
*  And a lot of the statements I make
*  are very much not designed to be precise
*  or actually efficient in design to like
*  take this in places that are interesting
*  and explore things that I enjoy
*  that are compatible with writing my
*  and like slowly working my way towards a thing,
*  but also being like deliberately non-optimized
*  because something is being deliberately optimized
*  towards a goal, like it's inherently untrustworthy.
*  Like not just it's perceived as untrustworthy,
*  it actually is untrustworthy.
*  Right, because if you're maximizing for any one goal,
*  then you stop actually carrying out other things
*  and they just fall away.
*  And everything else people rely upon goes away.
*  And that's an AI alignment problem too,
*  but it's also a very practical problem, right?
*  If you tell a fairly ruthless person
*  to make sure the shipment gets in by Monday afternoon,
*  I hope you didn't care about how they get it, right?
*  They're gonna find a way.
*  And if you ask them exactly how they get it,
*  you might not like it.
*  But you said, just get it here.
*  So they're gonna do it.
*  Okay, so let's pause for a moment.
*  You mentioned cloud computer use on a magic application
*  that also is a big development.
*  Your first time a foundation model developer
*  has come out with something that is basically able
*  to take like open-ended action
*  in the broad digital world at least.
*  It's kind of like, oh, one is like very hobbled.
*  I try not to lie to my AI assistants more than I have to.
*  I occasionally have to say
*  that I already have a doctor appointment scheduled
*  and this is just preparation for that,
*  or similar things like that
*  when it doesn't want to take on liability.
*  In this case, similarly, it was like,
*  I can't create an account.
*  I was like, okay, I logged into my account.
*  You can use my account.
*  I can't use your account if you're logged in.
*  Finally, I was like, okay, I logged out.
*  Now you're using the free no account version.
*  And if it had checked,
*  it would have found that I had lied to it about that.
*  And it in fact was still logged into my account,
*  but it didn't check.
*  It was gullible enough to believe in my little white lie.
*  And in my case, it was able to use Waymark,
*  our video products, and pretty effectively,
*  not super fast,
*  wouldn't have been able to play real-time video games.
*  It was probably better
*  because our product is an AI product.
*  So people are prompting the product
*  for what kind of video they want,
*  typically for their small business.
*  It was actually better at prompting clearly
*  than like our typical user,
*  a little less facile with various parts of the UI.
*  It works.
*  You can expect that.
*  But all those things seem like they'll be destined
*  to be resolved.
*  How big of a deal do you think this will be
*  for data purposes?
*  Is this sort of go out and do stuff in the world,
*  a good way to get over the data wall?
*  And what do you think of their decision to put this out
*  and their rationale, which is very open AI like,
*  we'll put it out while it's weak
*  so we can start to adjust to better that
*  than dropping it on you when it's strong.
*  I feel like we've heard that before.
*  And who's imitating who at this point
*  is not entirely clear.
*  Yeah, I had to think for a while
*  about what I thought about them to release that feature.
*  Ultimately, I think my philosophy is something like,
*  either of deployment is better than non-iterative,
*  but it's still deployment.
*  Sometimes deployment is bad.
*  But if you're already determined
*  that you're gonna deploy version 2.0,
*  then deploy version 1.0.
*  Before that, it'll probably be good.
*  The exception is if you don't want to raise
*  the alarm slash dinner bell for everybody
*  and tell them how exciting this great new product is
*  and how they should all be flying into Invest in competitors
*  and build compliments and supercharge the entire ecosystem.
*  Like when they released that GPT,
*  this led to a wave of investment in artificial intelligence
*  and we're excited in artificial intelligence.
*  They very much was acceleration as in situation.
*  And if you think that accelerating that development is bad,
*  then you should be sad about it.
*  However, if you think that releasing this early version
*  of computer use is not going to do that,
*  and the computer use was coming anyway within a year
*  from someone else, regardless,
*  yeah, and if let us work the kinks out
*  in a relatively safe environment,
*  this lets entropic both its relative position,
*  this lets us diagnose various issues,
*  this lets policymakers understand
*  it's already a very good illustration
*  for policymakers of the future to come.
*  I think it is very, very good at being an aid
*  to explain to say someone in the Detroit administration,
*  if you were to say, get a compensation,
*  oh, they can just control your computer continuously
*  as an agent, if you just take three lines
*  into this cloud product.
*  And it's not good enough right now
*  that anybody would dare use it,
*  but that's just a matter of a few months, right?
*  It's very different from a year from now,
*  people are going to be doing this,
*  but nobody's doing it yet, but now you have a demo.
*  You can see someone order their DoorDash,
*  you can see someone operate Magic Arena,
*  you can see somebody use various low-key tasks,
*  and if you do that, what else can you do?
*  So I think it's the right decision in this context,
*  given that this was obviously pretty close to happening
*  anyway, and you just got to be like,
*  Roon had the statement, I think it was a week or two ago,
*  total open-A cultural victory,
*  everybody's doing iterative deployment.
*  And my response to that was, well, yeah, of course,
*  because you're doing it, clean up choice, right?
*  It's like, well, total American cultural victory,
*  everybody has nuclear weapons.
*  Yeah, because you have them, you asshole.
*  Like, what are they going to do, not build them?
*  It's the cost of iterative deployment
*  is that everybody is aware of the situation
*  and aware of what can be done.
*  The benefit is also the same thing,
*  but the cost is the supercharging of all the events,
*  the pushing forward.
*  Once you've paid that cost,
*  not releasing iteratively doesn't buy you anything, right?
*  But I think someone else in iterative deploying
*  and you've not been iterative deploying,
*  doesn't really buy you anything either.
*  And by you focus, you know, SSI, right?
*  Like, I don't have to worry about a commercial product,
*  let me focus.
*  But if you were to release foundation models along the way,
*  that wouldn't, I think, be that likely
*  to accelerate other AI progress
*  unless it was substantially beyond the frontier.
*  Right, getting, we got GROK, so what?
*  It didn't do anything.
*  I'm getting another open source model
*  that's similar to other open source models,
*  getting another commercial foundation model
*  that's opposed to other commercial foundation models.
*  These things don't change the pace of other developments much
*  at this point.
*  That ship has already sailed.
*  Now, releasing something that's a substantial leap
*  above the competition,
*  the way that GBT-4 was a substantial leap
*  above the competition when it was released,
*  that could be different, right?
*  But like the iterative stuff, right?
*  I mean, Sonic 3.5 is like an interesting possible exception
*  just because it's helping people code faster.
*  I look back on the whole GBT-4 thing
*  and I still am somewhat confused.
*  I mean, at the time, the analysis, as I understood it,
*  and the reason for such intensive secrecy around it
*  was probably somewhat competitive,
*  but also somewhat, we don't want to alert the world
*  and have people piling in,
*  and it's better that a few, one or a few responsible actors
*  have the lead.
*  And now I feel like the live players list hasn't,
*  yeah, I would put XAI in there still,
*  just because if you can wire up 100,000 H100s
*  in record time, I think you gotta be on the list.
*  And having like functionally zero cost of capital
*  as a M.U.L.A.N. business always seems to have
*  is like definitely a notable advantage in this game.
*  But we haven't really seen any new live players added,
*  aside from them, to the watch list since GBT-4.
*  We've seen a bunch of VC money lit on fire
*  with sort of second tier foundation model developers,
*  which are now largely getting like creatively acquired
*  back into big tech, whether that's Adept or Character
*  or Pi, whatever the company's name officially was,
*  with Pi.
*  Yes, thank you.
*  And those were all like, I don't know about Adept as much,
*  but both, Pi had a great model,
*  and I think broadly speaking, a very competitive product.
*  Character has maybe the most adoption for better
*  and definitely at times for worse
*  of any product.
*  Like those seem to be on the path
*  to being viable businesses if you didn't think
*  that you just needed unbelievable amounts of capital
*  or like access to some data reservoir
*  that you just can't go get on your own.
*  So I do wonder to what degree the whole thing was baked in
*  even then, and you were always gonna have five to seven
*  mega company or alliances or whatever, live players,
*  and everybody else was just never gonna be able
*  to get into the game.
*  Do you see that differently or do you see any path
*  for anybody new to get into the game at this point
*  outside of like?
*  Five to seven is so different from one or two, right?
*  Like zero is very different from one, obviously.
*  One is very different from two.
*  Two is very different from three.
*  And then three and four are not that different,
*  but they still are substantially different.
*  And then five, six, and seven are very similar, right?
*  From the perspective of the situation.
*  It's a way to think about it.
*  And I would say that like,
*  everybody I was trying to retain their lead,
*  retain their position for as long as possible
*  by other motivation.
*  They were legitimately concerned
*  with the safety of the product, which is to their credit.
*  There was a lot of benefits to releasing
*  the relation of perspective and they didn't do that.
*  But to the extent that they had a commercial reason
*  to try and stay in the lead,
*  like they're really trying to make the number one.
*  They're really trying to get a large lead
*  where at least two or, and most two or three,
*  so that agreements can be reasonably reached
*  so that dynamics of I'm gonna do this, so you do that.
*  And then I can do this, you can respond to that.
*  Then you can do that response to this.
*  So I take hold.
*  Whereas with a group of five to seven,
*  like all the game theory says, what'd you get to seven?
*  Right, like try to get cooperation in stat-cons
*  with seven players in the long run
*  is a very, very hopeless situation.
*  There's no time.
*  It can be done, but it's very difficult.
*  Where it's getting stat-cons to work,
*  or two is remarkably easy.
*  Right, so like the number of players matters quite a lot.
*  And I do think it's basically the structure
*  of the necessary capital requirements
*  and data requirements and the way that capitalism works
*  right now, that there isn't room for that many players.
*  So we're not gonna get 20, 30, 50 players
*  that are making like meaningfully advancing frontier models.
*  People will be either trading models with cheap
*  or using someone else's model
*  or easily adapt and smoke and model
*  or otherwise like not sweating it too much.
*  Because again, like, why are you duplicating
*  all this months of work at the waste of time?
*  You're not gonna get a lead.
*  You should work with what is developed
*  for you for free or damn cheap.
*  Because the marginal cost of providing it to you is zero
*  or damn near zero, of course that makes sense.
*  But we're looking at a future where there are gonna be
*  handbook companies that are out front of the rest
*  and assuming that spending more money
*  and building these bigger data centers
*  and running these bigger banks is,
*  in fact, a way to stay at an advantage,
*  which we can't assume,
*  but is definitely the way they're betting and seems likely.
*  Then yeah, we're gonna be probably in the three to five,
*  yeah, one, maybe even one to five players
*  who are like, we've seen scenario planning,
*  like Daniel Capitaggio's like scenario planning, right?
*  When he put out what 2026 looks like at Les Ron
*  and stuff like that.
*  Where in his models, like OpenAI has this nine month leak,
*  or he has nine months becomes a longer and longer period
*  as things speed up, right?
*  Every other land on the planet becomes irrelevant.
*  He might just fold from those scenarios at some point,
*  because there's no point in trying to be nine months behind.
*  Because so what, if you catch up
*  and you're six months behind a year from now,
*  that is now three cycles.
*  And then a year from that,
*  it's gonna be infinite cycles, right?
*  In that kind of scenario.
*  So like, what's the point?
*  So if you can get above of a lead at the right time
*  when things start to go vertical in terms of pace,
*  then you have a decisive advantage over everybody else.
*  And right now, yeah, I think that what we're realizing is
*  there really isn't room for more than a handful of companies
*  to be working competing in this space.
*  And the only reason Meta is able to compete in this space
*  is because of Meta's business interests,
*  investment in Instagram,
*  creating weird incentives for them to then do other things.
*  And also giving them this giant pool of money
*  they can do set on fire whenever they feel like it.
*  My head's hurt.
*  Yeah, having tens of billions of cash on hand
*  with no better way to spend it is,
*  they need to play as it turns out.
*  But also in this situation where like,
*  even a slightly better way to serve people,
*  like addictive feeds or better advertising,
*  just makes them billions and billions of dollars, right?
*  So like, they need very little effective improvement
*  from doing the work themselves.
*  Even buying insurance against their work
*  being cut off by other people to make it worth
*  for them pursuing this very aggressively.
*  So they have a lot of odds,
*  even if other people in this situation wouldn't,
*  and they have these unique data sets
*  because of this association.
*  So they're in a deep position to be competitive.
*  And then Elon, of course, is Elon,
*  and just does whatever he wants to do.
*  So he gets to redirect all this capital,
*  he gets to use his leverage to recruit a bunch of people
*  and redirect a bunch of stuff and compete.
*  But it's not obvious to me that XAI is relevant.
*  Like they might be relevant, we'll find out.
*  But so far they haven't actually done anything,
*  changed anything.
*  But yeah, my guess is it's still probably largely coming down
*  to the three plus one.
*  Right.
*  And then we'll see if they can all stay relevant.
*  Increasingly deep mind in Google.
*  Don't seem that relevant in the port sense.
*  Like they're not that far behind, but why do we care?
*  That they're there?
*  Almost.
*  Like, it's starting to feel that way.
*  Yeah, it's funny, I just had a couple Googlers on the show
*  when they did an update where they added Google search
*  grounding as a feature to the Gemini API.
*  And I asked them this question, basically saying like,
*  why is Gemini number three?
*  Because I don't, I mean, you might feel like
*  the answer is obvious.
*  I don't feel like the answer is super obvious.
*  When I look at benchmark performance,
*  when I look at cost, when I look at just like APIs
*  and rate limits and just all the things,
*  it's pretty competitive.
*  And yet it was the third provider that I integrated
*  into the app that I'm building.
*  And it's definitely the third window that I open.
*  Typically Claude is first, then Chatuchakuti,
*  and then Gemini.
*  If I want to do a contract review,
*  I'll do all three for completeness.
*  But it is always the third.
*  Is that a branding problem?
*  Why do you think it is the third?
*  Cause I can't quite put my finger on it.
*  So I am using Gemini for my Chrome extensions,
*  the thing that I'm building with the code base.
*  Part of that is cause Claude just kept being annoying
*  and giving me a 401 and Claude was exacting the model
*  I was using to fix the 401 and it couldn't fix the problem.
*  And I was just like, well, if you can't fix your own problem,
*  well, Gemini is a lot cheaper and it's probably fine
*  not to use Gemini.
*  But, and Gemini has been working okay,
*  but it's been annoying.
*  Like it will follow instructions, it has issues.
*  But my lived experience is that it's worse
*  and it doesn't have very useful places where it's better.
*  So when I type things in the Gemini advanced window,
*  I don't get much out of it.
*  And then I just stopped using it.
*  So I'm just not interested in this anymore.
*  And there was a period where their long-handle window
*  was unique, but now it's not unique anymore.
*  There was a period where they jumped briefly ahead
*  of other people in various ways.
*  And I was like excited to use Gemini,
*  but it just hadn't happened for a while.
*  And I think part of it's a marketing problem.
*  I think Google has been insanely bad
*  in marketing Gemini, and it's putting Gemini.
*  That's also the implementation sucks.
*  I bought it, yeah, $1,700,
*  that's a full, nine,
*  partly because even a marginal improvement in efficiency,
*  is worth a lot of money.
*  So why not try out the best?
*  Partly because I was told there'd be these cool
*  Gemini AI features.
*  I didn't use those features.
*  So I don't know, I haven't even get to work.
*  You're not circling to search on a regular basis.
*  It has never come up.
*  I've never wanted to go to search anything, not once.
*  Never been happened.
*  Possibly I just didn't occur to me
*  and I had a great opportunity to circle the search,
*  but I wanted my calls to have automated login.
*  They don't.
*  I wanted to be able to talk to the AI
*  and have it usefully use the contents of my phone.
*  It didn't work.
*  Every time I tried to get the AI
*  to do something useful for me,
*  I just gave up and I'm debating getting an iPhone.
*  There's a Verizon special where you can supposedly get,
*  turn in any phone in any condition
*  to get a free iPhone on them.
*  I haven't used my contract in a while.
*  I was like, why don't I get an iPhone too?
*  I've got a drawer full of old phones here
*  I could potentially cash in.
*  Yeah, I'd only do it once,
*  but I've got at least one non-working phone
*  that I can cash in or I can buy one, I'm sure, 10 bucks.
*  It's not hard.
*  But it just, they haven't done any.
*  They haven't made it useful.
*  They keep telling me,
*  Gemini now available on this app that I'm using.
*  I use a lot of Google products and they're great.
*  I have no, I love Google.
*  But like, I never use it.
*  Like the few times I tried to use it,
*  like I remember that when they put Gemini on Google Docs
*  and I just went to the end of the document
*  and I was like, Gemini, have you,
*  please summarize this document.
*  Didn't do it, can't help me.
*  And how is that not the first time?
*  Yeah, the integrations have been disappointing for sure.
*  To the point where I did a project to go back
*  and using APIs that they have.
*  It's funny, because they have such incredible APIs
*  and then they don't seem to use them
*  in their own product development.
*  And that's quite puzzling.
*  My interesting thing, Google has this like,
*  amalgamation of like different departments
*  that basically don't talk to each other
*  and basically don't copy each other's work.
*  They have to reintegrate them, re-implement everything
*  and they don't cooperate very well.
*  And like they're doing very split
*  and fighting all the time.
*  And yeah, as a result,
*  like you can readily get your things in places,
*  but often you just don't and it's a serious problem.
*  So I don't know what's happening with that.
*  Like notebook LLM for a while I was trying to use it.
*  They just fed it all my articles
*  and tried to ask it questions.
*  It's just not good enough, right?
*  It's not, it's failing on its base too often
*  and you'll feel emotionally like I want to use this
*  or I want to try using this anymore.
*  So I stopped.
*  Like the one cool feature they implemented
*  that no one else has right now is like the podcast
*  or the AI podcast.
*  And some people like, I find that one wasn't terribly terrible.
*  I tried to listen to it for a minute or two.
*  I can't take this thing.
*  I like them actually, although I do think
*  there is a step up there relative to any other
*  automatically generated audio I've heard.
*  Oh, it's amazing technical achievement.
*  And I applaud them for doing it.
*  And it's cool and ask them for sure.
*  I am very proud of it for doing it,
*  but they plan to go mad at her.
*  God, please shoot me.
*  What I want and I'm sure they will have it before too long
*  or at least I expect they will.
*  I'll certainly be asking questions about this
*  is a much more literal version.
*  Like I was trying it with biology papers
*  and talk about a world of just infinite complexity
*  and whatever that I just started to acclimate myself to,
*  but I don't want the very like poppy explanation of,
*  oh, the way that this antibody attaches to this thing
*  is it's like having the perfect key for the perfect lock
*  that only fits that one lock.
*  And I'm like, that's not how I want to have things
*  explained to me.
*  I want to be much more literal and much more grounded.
*  And I haven't been able to prompt my way out of
*  the like very superficial analogy driven approach
*  that it kind of defaults to.
*  But I do think if they, I do like the two voices.
*  I do like how they kind of keep it engaging that way.
*  And I definitely prefer it.
*  I'm very much an audio person relative to reading.
*  I know we're not,
*  I'm really like the two voices.
*  I both like both of the voices.
*  I think they're very well done.
*  I like the way they dynamically play against each other,
*  but the information density and the amount of banality
*  just drives me insane.
*  And just in general, like there's no flexibility
*  in the podcast.
*  Like the great advantage of AI is that it tunes.
*  So like one of the things I've gotten to experiment with
*  recently that I've had the time for
*  is some forms of AI storytelling,
*  including like various NSFW versions of it.
*  I've just been toying with them.
*  And like, it is amazing how much like
*  something that's relatively bad,
*  subjectively terrible.
*  I've objectively just got an awful terrible.
*  It is so much better when you have this ability to steer.
*  We have this ability to influence what's happening
*  selectively when you want to,
*  while simultaneously mostly letting it write its own story
*  or giving it a little bit of direction.
*  And that's turning something that would be unreadable
*  if you were to direct it with completely awful,
*  unreadable, complete drag into something that's exciting.
*  Right, because you feel like you have agency
*  and you can emphasize the things that you find cool
*  and exciting and the things you find bad.
*  And a podcast is the opposite of that.
*  I want the podcast to be this thing
*  where I'm constantly like giving you direction, right?
*  As you do it, right?
*  Not like it pregenerates one podcast
*  and it just presents the information
*  in this incredibly undense format.
*  What I love about AI is in learning things
*  is give me the answer.
*  Okay, now I'll drill down to the places
*  that I'm actually curious or don't understand.
*  I didn't, your explanation wasn't what I needed
*  and I can correct you in it in stereo.
*  And this is just not that.
*  Yeah, it's interesting as somebody who has spent time
*  developing AI powered products,
*  I think that balance is a tricky one to strike,
*  especially if you're trying to serve a broad base of users
*  that are not AI obsessives
*  and don't have great intuitions at this point
*  or how to steer, how to prompt, whatever.
*  I think there is something definitely to be said
*  for the end to end moonshot.
*  Like we've definitely seen that at Waymark
*  relative to other AI video products
*  where there's many products out there
*  where you can generate an image to put into this scene
*  or write me a bit of copy to drop in.
*  But very few take the, what do you want?
*  We're gonna make the entire thing for you,
*  present it to you and then let you kind of go from there.
*  And they are currently missing the go from there piece.
*  But I do think too many products
*  and especially too many Google products
*  have taken the other side where they're like,
*  keep it super, super modular, super bite size,
*  user fully in control.
*  And that also leaves quite a bit of magic.
*  Quite a bit of magic on the table.
*  So I do like how this is a different approach.
*  I do think they'll circle back.
*  I've heard them, they were on the late and fake podcast
*  not long ago, which included a little discussion
*  of adding real time audio as one item on their roadmap.
*  And that would be a qualitatively
*  different experience obviously.
*  So.
*  You know, the world in which like,
*  you can be a third person with that podcast
*  and they'll adjust to it say,
*  I thought that product was so much better.
*  The world in which like it can just go far faster
*  and be more dense and like not feel like they have
*  to set all this background and talk to us to be so slow.
*  Like, hey, you can go 1.5X or 2X or whatever,
*  but it doesn't fundamentally change the problem.
*  What has your experience been with real time voice
*  or advanced voice mode, I guess is the proper term
*  from OpenAI, you know, we were racing to her
*  however many months ago, now her is here and.
*  I guess put that into her.
*  That's like my actual response, right?
*  Like she's nice.
*  Yeah, I appreciate her for it.
*  We're trying, but yeah, I'm just on the interior.
*  Yeah, right.
*  But she needs some more things than that.
*  She needs to like to get better.
*  Part of it is you can't prompt properly.
*  It's you can't get it to do the thing you want.
*  So now it has to be like good enough that it can into it
*  from you just saying random words,
*  responses that are good enough.
*  And my experience is just,
*  no, I meant something.
*  No, no, no, I hate that.
*  No, it is not what I'm asking you to do.
*  It's not the information I want.
*  Like it's, there are occasionally magical moments
*  where you feel like you're having a conversation, right?
*  And it's just like flows really cool.
*  But for the most part, I just wasn't my experience at all.
*  It was like very, very bad at doing
*  what I actually cared about or wanted it to do.
*  And I don't blame it because again,
*  like my prompting was terrible, right?
*  It was just me speaking words in no particular order
*  that had to be transcribed without any kind of plot.
*  Like people who want to be talking instead of typing
*  to a computer in general, I'm just so confused, right?
*  Like I can't talk as fast as I type.
*  I can't talk as precise as I type.
*  I can't talk as deliberately as I type
*  and I can't correct it.
*  What am I doing?
*  I mean, sure, if I'm walking around
*  and there's a certain modality
*  or a certain like different aesthetic to it,
*  it could be an advantage.
*  But yeah, I just stop talking to your phone.
*  Like just stop talking to your phone
*  for anything but the most very specific things, right?
*  I have an Alexa, right?
*  And it's very good for a very small number
*  of very specific things where I know exactly what to say
*  to get exactly that result.
*  But say you hooked a Jack GPT to Alexa,
*  which is like something that's obviously common
*  in some form, whether or not it's exactly chat sheets.
*  And now if you give it the specific command
*  or do the specific thing,
*  but if you give it any other command,
*  it will intelligently interpret your statement.
*  Well, my assumption is this will basically greatly expand
*  the set of things that will work.
*  I'll be able to find other things
*  and do the things I want it to do,
*  but I still won't really want to do open-ended requests
*  that are complex, that are interesting
*  until the technology gets a lot better.
*  All right, and then GPT-5 can handle that, but you need.
*  I'll take the side of that a little bit
*  in as much as I do have a lot of excitement
*  for a future where I can learn while taking walks.
*  Like I'm just plain too sedentary.
*  And a big part of that has been that I'm unable
*  to learn effectively on the go.
*  I do think the voice mode will work for that.
*  The biggest, and then that was so disappointing was
*  I couldn't paste in the contents of a paper
*  that I wanted to learn about.
*  For whatever reason, you can't attach a PDF to it
*  or even paste in a decent amount of content.
*  And I was like, why?
*  Why can't I just want to talk to you about this?
*  I know you can do that,
*  but for whatever reason that is not supported,
*  that feels like the kind of thing that,
*  you know, he's an obvious candidate for un-hobbling.
*  And I still have a lot of hope for it.
*  I don't think that you can't get there.
*  I don't think you couldn't get there with, right.
*  All of these things are just like,
*  you need a certain specific set of un-hobblings or features
*  that just aren't too necessary,
*  are too vital to the things you want to do
*  to figure out enough of that stuff
*  when you have something.
*  So like, say I'm listening to a podcast on some app,
*  and I have Gemini always listening to what's happening.
*  And then I can interrupt the podcast
*  to ask Gemini about what I've been listening to, right.
*  And it will respond that intelligent,
*  what, like, wait, what did that mean?
*  What is he referring to there?
*  You know, basic question.
*  That sounds really helpful, right.
*  And then I can interrupt, like,
*  well, what if we didn't do that?
*  But it has to flow, right.
*  It has to like just work.
*  Now, that just work, I'm not going to do it.
*  I'd much rather just wait till I get home,
*  get in front of the computer,
*  paste the transcript into, you know,
*  if I have to, into Cod, it's just my thing.
*  But like, you're going to do the simple sort of
*  stripped down, simple true her version.
*  Like, it just has to be good enough.
*  And I just don't sense that we're close.
*  Yeah, I would get a lot of you.
*  They just allowed me to paste in a paper or two,
*  but yeah, I'm not sure why that hasn't happened yet.
*  Let's zoom out again a little bit from the products
*  from these live players.
*  And I wanted to get your take on the increasingly tangled
*  web of alliances between these companies.
*  If you had told me even a year ago that OpenAI
*  would manage to add an Apple partnership
*  to their existing Microsoft partnership,
*  that would have blown my mind.
*  If you told me that, I wouldn't have been so surprised
*  that Microsoft would turn around and have multiple providers
*  in GitHub Copilot, which they now do.
*  That one of them is Google is a pretty remarkable fact,
*  I would say, unto itself.
*  You know, in general, there's this like interdependence
*  coupling, if you will, going on between big tech providers.
*  Last time when we talked a little bit about like,
*  are the models converging or are they diverging
*  and is that good or bad?
*  My general sense is that this coupling is probably good.
*  It seems that it makes the overall competition
*  a little less intense and that they're all frenemies
*  in some way is like better than if they're all siloed,
*  standalone platforms that don't talk to each other
*  from just the standpoint, probably just straight up
*  consumer surplus now, but also a like,
*  when you talk about like how many companies,
*  how many players can you have in a game theoretic sense,
*  like work together nicely, this would seem to support that
*  or at least be better than many alternatives
*  if you want to see cooperation.
*  I haven't really thought about that too much,
*  but my natural instinct is something like,
*  perfect competition is the worst case scenario
*  in terms of giving people flexibility and slack
*  to do the right thing when the right thing
*  is not as competitive and to have given choices.
*  So to the extent that what's going on
*  is that everybody is willing to work,
*  every deployer, right, like Apple is willing to work
*  with every supplier, every developer,
*  then the developers competing as each other
*  in a way that like everyone who's using software
*  can swap Gemini out and cloud in or vice versa
*  in five minutes, maybe in 30 seconds,
*  whenever they feel like it or just click a button,
*  they might move on call.
*  That plays this tremendous pressure,
*  whoever is marginally better for each given task
*  gets the business.
*  Whereas Amazon is committed to profit,
*  it's an alternate world and Apple is committed to open AI
*  and Microsoft's committed to open AI,
*  but Google is committed to Google and blah, blah, blah.
*  Then, you know, just being slightly better
*  doesn't matter very much, like Apple intelligence, right?
*  Like it's gonna be succeeding or failing based on like the UI
*  and what features they decide to implement
*  and whether or not it basically works and just works
*  in a way that Apple products try to just work.
*  It's not gonna succeed or fail based on whether or not
*  it's 10% better in its core AI function, right?
*  Like it just has to be good enough
*  and whether or not it's slightly ahead or behind
*  what Google is doing with its Google Assistant
*  in terms of the actual model, it doesn't matter very much.
*  But if they're competing,
*  if all the models are competing to be the Google Assistant,
*  all the models are competing to be the Apple intelligence,
*  now the tiny difference is everything
*  and now you just have to go full speed ahead
*  and try to maintain this narrow lead
*  or this narrow functionality
*  and shift everything as fast as possible.
*  I feel like we've flipped
*  because last time I was saying that about
*  just the model developers
*  and you'll have different friends for different things.
*  But now if I understand you correctly,
*  you're taking the other side where you're saying this,
*  the ready availability of an alternate model
*  in any given product,
*  you think effectively makes the competition
*  more intense at the model.
*  What we've seen a lot of times is basically this idea
*  of a new model will come in and people will say like,
*  the more we're swapping in new models,
*  the more these new models will come.
*  And like a natural result is that a lot of people
*  would tune their systems to use a specific model
*  and they know exactly how to do what they want it to do.
*  And they're used to what that model can do
*  and they put in that model.
*  It's not just that model has some features
*  and other models have ways in it strong.
*  The more similar they are,
*  the more you keep swapping them back and forth, right?
*  The more they're close to each other in that way,
*  the more distinct they are, the more you have lock-in
*  and the lock-in is probably good, right?
*  I do want these silos to a large extent
*  because yeah, I think that actually puts a lot less pressure
*  on the labs to push forward in an important way,
*  if they over split another business immediately.
*  There are big switching costs in that world.
*  So you're also then happy to see the apparent divergence
*  in model capabilities like OpenAI has reasoning
*  and Claude has computer use and is the best at coding
*  and Gemini has long context and Google grounding
*  and you wanna see more Cambrian divergence and less copying.
*  Different features is great.
*  What do we mean by I'm actually just learning
*  about the Google integration of Gemini, right?
*  To what extent is it integrated?
*  What does that mean?
*  How are they different from this large search
*  that can be done on Chats.GPT?
*  But yeah, the fact that when I want to search
*  for basic information right now,
*  I only use Google to search for information
*  and I expect Google to pop out.
*  I'll search Chat.GPT,
*  I expect Web.GPT search to search it out better.
*  Right, I use Propoxony for a third type of thing
*  where I want like a specific set of sources
*  type of explanation.
*  If I want like coding or other type of natural chats
*  or analysis of the papers, I'm gonna use Claude.
*  If I want like a very specific set of things,
*  I might use O1 and so on.
*  I think that's good, right?
*  Now on the margins, they're trying to carve out
*  their own space and just pushing the model's
*  core capabilities is much important
*  because I don't care which model is 10% better
*  under the hood.
*  I care about which one is more adapted
*  to the use that I'm doing.
*  When I'm in a cursor, okay,
*  I will think a substantially worse coding model
*  that's integrated in the cursor
*  over a better one that isn't integrated by a lot.
*  My coding speed more than double
*  when I switched from using the Claude chat interface
*  to using the same exact model in cursor.
*  Yeah, cursor's awesome.
*  I definitely really love it.
*  I think GitHub copilot has also caught up
*  in a substantial way with their last round of updates,
*  but as far as I understand, cursor is still-
*  I'm not saying it's the best.
*  I'm saying those are the two things
*  that I've done so far in the night and day.
*  I think it still is the best.
*  I'm not understanding is it still is the best,
*  but yeah, the water line is always rising.
*  Okay, so before we move on to some other kind of
*  big picture things, talk a little bit about regulation
*  and what the government is or isn't gonna do,
*  handicap AGI for us real quick.
*  Metaculous says 2027 for the weak AGI median forecast.
*  The modal, the top of the probability curve is late 2026.
*  I have to say I buy it.
*  It doesn't feel like there's that many more turns,
*  the computer use, if it's like I look at sweet bench
*  and it's, geez, we went from five to 50% sweet bench
*  in a year.
*  So for everyone's benefit, what is their definition
*  of weak AGI that they're using?
*  Let me look it up real quick.
*  So it's four things.
*  This first one is tricky,
*  because I dug into it and my guess is that
*  we have a definition problem here,
*  but the first one is able to reliably pass a Turing test
*  of the type that would win the Lobner Silver Prize.
*  Next one is 90% or more on a robust version
*  of the Winogrand Schema Challenge.
*  That's basically like pronoun
*  and sort of referent disambiguation.
*  I would say that we're well clear of that already.
*  Right.
*  Next is be able to score in the 75th percentile
*  on the math sections of the SAT,
*  given just the images of the exam pages as input.
*  I think we probably are safely past that as well.
*  When I tested this, it was not yet multimodal,
*  but before early was getting better than 75th percent.
*  Yeah, we'll not be limiting factor on this problem.
*  We can move on.
*  Yeah, two or three.
*  And the next one was, the final one is be able to learn
*  the classic game Montezuma's Revenge
*  and explore all 24 rooms based on the equivalent
*  of less than a hundred hours of real time play.
*  And that is intended to be a unified system.
*  I haven't tried Claude on Montezuma's Revenge,
*  but I have to imagine general computer use,
*  a haiku version that's fast enough to respond
*  would probably do that.
*  Although I don't know how much strange
*  or counterintuitive whatever you'd have to do to the 24 rooms.
*  Right, the way it's going to stick it is,
*  in anything, given this definition, I would bet the number.
*  Why I would not consider that to constitute NGI
*  in the sense that I care about other AGI shows up.
*  Yeah, the reason I think this question is ultimately
*  going to age poorly is that this Turing test silver prize.
*  Next the real question,
*  because we'll get Montezuma's Revenge in 25 probably.
*  Yeah, it seems like Turing test is formulated
*  in a way that basically has expert users.
*  And I think that the real problem is that the AIs
*  are not trying to pass the Turing test.
*  Like they are, if you wanted to create an AI
*  that would pass the Turing test,
*  you would do what OpenAI warns against
*  in their fine tuning documentation,
*  where they show an example of,
*  look what happened when we trained a model
*  on a hundred thousand Slack messages.
*  We asked the AI to do something and it said,
*  sure, I'll do that tomorrow.
*  And that has become their sort of funny anti-pattern.
*  You don't want to really train on real data in many cases
*  because you want a different behavior from the AI
*  than you actually observe in the wild.
*  Not good, but my sense is that the way to tell
*  the Turing test right now is ways in which the AI
*  is just better.
*  Like humans do something stupid
*  that you don't want the AI to do.
*  The AI doesn't do it.
*  But if you wanted to, you could argue.
*  I don't know a lot.
*  You could pass as human a lot more easily.
*  There are various things you could do to make the AI pass.
*  And if you really wanted to pass,
*  yeah, yeah, yeah, if you offered me
*  a hundred billion dollar prize for patenting this
*  by the end of 25, everything listed here,
*  I consider you to be a huge favor to succeed.
*  It's just not a very hard test.
*  So maybe, okay, grab that.
*  It's probably just flawed definition.
*  And the reality is, barring a prize,
*  and I don't think there's any cash attached to this
*  for anyone, there's not really a reason
*  to try to pass the Turing test,
*  other than like scams maybe.
*  No, like maker developer is gonna do it.
*  What we're saying is actually that like,
*  this question comes down to whether they choose to pass,
*  not when they can't.
*  Right, I can turn to the grading.
*  So it's not a very interesting question.
*  So let's look over then to the Dario definition,
*  because he, this is another thing I wanted to talk about.
*  And I am of multiple minds on the machines
*  of loving grace vision.
*  Right.
*  First 50%, I give very high marks too.
*  This is the part where he defines,
*  and he doesn't like the term AGI,
*  just like we're all EA adjacent,
*  we're all, nobody likes the term AGI anymore.
*  We're all using substitutes like in his case,
*  I think powerful AI is the phrase.
*  Yeah.
*  But I mean, he lays out a pretty high level of capability
*  that is basically everything but a robotic embodiment
*  that you would want.
*  Like smarter as smarter than a human noble prize winner
*  across basically all the disciplines,
*  has the same affordance as multimodality,
*  has the ability to use computers effectively,
*  can potentially take offline actions as well.
*  Although it doesn't necessarily have a robot body,
*  there's a lot of APIs,
*  there's a lot of ways to place orders
*  or hire people to do stuff online or whatever.
*  So unlimited computer use.
*  And this then translates to a pretty exciting vision
*  if you buy it that we're gonna have potentially
*  a century's worth of biological discoveries
*  and medical advances condensed into say,
*  a five to 10 year timeframe.
*  I do find his rationale for they're pretty compelling.
*  Basically he says that if you look at the history
*  of biomedicine, it's a relatively small number
*  of really important discoveries
*  that drive most of the progress
*  and the form of those discoveries is some programmable
*  or semi-programmable tool that kind of becomes
*  a platform technology that you can then do tons of stuff on.
*  So CRISPR is like a canonical one where,
*  hey, now we can edit DNA to a certain level of precision
*  that we couldn't before,
*  now we can do all this more stuff
*  that drives all kinds of downstream discoveries.
*  This all remind me a lot of the Star Trek situation
*  in terms of you have this holodeck, right?
*  And technically speaking, you could do literally anything
*  if you want to and just nobody does.
*  And then you ask yourself, what would be the consequence
*  if people use it in exactly these ways and no other ways?
*  And yet you have really interesting stories.
*  But of course, there's a giant plot hole
*  and there is nothing stopping them from using it
*  in other ways and they often accidentally sort of use it
*  in these other ways and then they just pull back
*  or don't realize it and keep going.
*  And Machines of Love and Grace is essentially portraying
*  a world in which you have the AGI
*  that should ASI very quickly.
*  And instead, we leave society mostly alone.
*  We don't develop an ASI.
*  We only apply it one at a time to these specific areas
*  in an isolated, directed way
*  and then we see what is possible doing that.
*  And I think he does a very good job
*  of exploring what that would look like,
*  but that scenario is fucking bizarre, right?
*  It's the question of, okay,
*  what medical advances could we get
*  if we have infinite advanced PhD students
*  just riding around it, set up many X speed
*  available in the cheat to do anything you want?
*  And yeah, of course you're gonna get a lot of advances,
*  but are you bearing the lead on what happens in this world
*  when that kind of thing is doable?
*  Like what else is going on?
*  And on so many levels, including like,
*  how do we decide not to develop better AIs
*  with this tool that you just created,
*  which obviously could do that very quickly.
*  Like if you're not getting only five years
*  for a hundred years of biology and thinking then,
*  you're getting the same thing for AI advanced,
*  only more so because AI is gonna be bottlenecked
*  by all of these physical experiments you have to run
*  and biology is.
*  So there's no real explanation there.
*  It's like very weird sci-fi,
*  but it is a good counterweight
*  to people who say that wouldn't matter,
*  that you wouldn't get cool things.
*  Like, yeah, no, you've got these cool things,
*  but it's a bizarre scenario.
*  And that's like my view again of,
*  yeah, where we were up until the point
*  when you know that you have a problem, right?
*  Which is the part where he calls for this,
*  like democratic values thing,
*  this alliance of the good guy against the bad guy
*  that we also see from nobody's eyes rhetoric.
*  Yeah, that's in the second half
*  and that's the part that I don't like so much.
*  Before we go to that, he seems to think it's like,
*  of course it's a well-caviated essay,
*  but he seems to think it's more likely than not
*  that we get to this general level of capability
*  in a 2026, 2027 timeframe.
*  I've heard other credible people say,
*  yeah, that seems reasonable.
*  Maybe just am not a delay, make it 2028.
*  Maybe it doesn't go quite as smoothly as he thinks,
*  but basically it's right.
*  This morning, in the Saturday afternoon,
*  I was listening to the podcast.
*  I haven't finished it yet, but it was very long.
*  But the podcast he did with Lex.
*  And he emphasizes that he's saying
*  that's how fast it would be
*  if we don't get one of the following roadblocks.
*  So it's like a conditional prediction from him, specifically.
*  And I understand it.
*  Then that becomes the mediated slash modal production.
*  But there are only ways that can go wrong,
*  not ways that can go more right,
*  as conditions on that.
*  So he's not making that as a...
*  His prediction is somewhat less aggressive than that,
*  in effect.
*  But he is saying a large probability weight
*  on 2027 solid numbers.
*  And yeah, that's the method
*  coming out of all the major labs.
*  That is what these people actually believe.
*  So do you basically adopt their statements as your beliefs,
*  or you are, at the top,
*  more skeptical than...
*  That's why I said that they're believed, right?
*  Not that it's what's going to happen.
*  I think that I have model uncertainty.
*  So I have their perspective,
*  and they have significant information,
*  but they also have some weird incentives
*  and group think, information cascade,
*  and so things going on.
*  And there are other groups that have very different estimates.
*  So I would say if 2026, 2027 involves
*  the relevant kind of AGI,
*  that thing is very possible, right?
*  Double the chance of that happening, for sure,
*  at this point, given what we know.
*  But if I had been abstracting utils in some way
*  and took the odds,
*  I would bet against 2027.
*  I think the odds are under 30%.
*  But that's because I don't see what they see,
*  and I don't know what they know,
*  and I'm not familiar with their environment,
*  and I can't just take their word for it
*  and discount everybody else's perspective.
*  And also, I do think they're making those predictions
*  on the assumption that a variety of things don't happen.
*  And Dario was more explicit about this than most,
*  but I think they're all making the assumption, essentially.
*  And so what happens if China based on what, 2026?
*  Yeah, right.
*  Yeah, which is a perfect segue
*  to the second half of the essay,
*  which notably did not come up
*  in the Lex Friedman discussion at all.
*  And Dario, if you're listening,
*  I'd love to hash this out in a little bit greater depth,
*  which is to say at least somewhat compared to that.
*  The short version is everybody seems to be,
*  and by everybody I mean, Altman and Dario, apparently,
*  and the national security establishment blob
*  are all on this path that we're going to,
*  notably under a you-know-who second term, as it turns out,
*  I'm going to keep building the AI, jam, accelerate,
*  build the best stuff we can,
*  keep China behind by cutting them off of chips
*  and maybe other measures,
*  but certainly the chip ban is the big thing so far.
*  Then we'll achieve maybe 2026 timeframe,
*  decisive military strategic advantage,
*  and then we'll create the new American empire,
*  same as the old American empire,
*  but with a hundred million times more AI,
*  where we get to go to everybody and say,
*  we're the good guys, join with us.
*  You don't get to have your own AI,
*  but you get to make the API calls or whatever.
*  And China's isolated and Russia too.
*  And then eventually we get around to them and it's like,
*  you guys are the last ones out, but we'll invite you in.
*  You just have to play by our rules
*  and then you can have all the great AI benefits
*  that we're already enjoying.
*  Sounds crazy to me.
*  What do you think?
*  Well, one thing to note is that
*  a lot of people are talking about building AI
*  with democratic values, right?
*  We're like treating the,
*  any people who use democracy as good guys
*  and you think it's not democracy, it's bad guys.
*  And we need to understand that democracy
*  is a semantic stop sign.
*  Democracy is a word that says stop thinking, right?
*  We are inviting this with good
*  and therefore if it's democratic, it's good.
*  And you can stop worrying about what happens after that.
*  You have to figure out what that concrete means
*  or how you would implement that.
*  In a world with super intelligent entities
*  that can be copied, do they vote?
*  Do they not vote?
*  How do they vote?
*  What does this mean, democracy?
*  Is global self voting?
*  Do they get equal votes?
*  Have you considered what they actually want?
*  If you let them vote,
*  are the voters gonna understand the implications
*  of their vote when it comes to AI policies?
*  None of this is getting said, right?
*  Nobody has talked this through.
*  They're just evoking the word democracy to mean good.
*  And similarly, we are talking about a line with democracy
*  because democracy must mean.
*  I love democracy,
*  but we have to understand what democracy is for
*  and why it's here
*  and why it is better than the alternative systems we have
*  and not just treat it as a stop sign
*  and not use it as an excuse to have to figure out
*  what we actually want the future to look like
*  and like how it would reserve human values
*  and find things to care about, right?
*  Like we can't just think about later at all, we find.
*  It's not that simple.
*  Like that's an operational law.
*  That's one problem.
*  The other problem is, yeah, if you're very loud
*  about the fact that you're doing this,
*  like what is the only reasonable excuse of reaction
*  by the other side and why should you like to work?
*  But you have to keep in mind, all the options are bad.
*  You're saying we're not going to try and cut off
*  China or anyone else's ability to develop AI
*  because that would antagonize them.
*  So we should be cooperative with them
*  but that'd be equally advanced or more advanced than us.
*  And then of course, everything will somehow just work out.
*  It doesn't work.
*  You can't just ask them to become voluntary
*  secondary partners in an alliance
*  where we have permanent superiority
*  because it seems unlikely they'll buy it.
*  You can't really offer to go up your advantage
*  and go in as equals because the US government
*  is incapable of doing that even if it was somehow wise.
*  So all the options are bad, right?
*  Like you can't try to get everybody shut down AI
*  until you have some argument that can carry the day
*  at least amongst the US government
*  and then everywhere else as well that matters
*  such that you can make that agreement.
*  Like all our options are bad.
*  And some form of get the people who can get on board
*  and then work from there could easily be
*  the least better alternative at our disposal for now.
*  But the part that I hate so much is the whole like,
*  well, you just treat China and also Russia
*  and also like anybody else who you don't like
*  as an intractable entity who you can't talk to,
*  you can't negotiate, you can't reach agreements with
*  and you can't get to treat this safely.
*  And so you end up just having to smash the accelerator
*  and pray that like physics is kind to you
*  in ways that I don't think it's kind.
*  And it like definitely are not that likely to be kind.
*  Like it's definitely far from certain.
*  That's is awful.
*  But like, yeah, you pick up the phone
*  and you have to talk to these people.
*  Like we've seen evidence time after time that, you know,
*  China hears a lot about AI safety in at least some sense.
*  Right, and they don't just wanna rush willy nilly
*  as fast as possible.
*  I hope that everything works out.
*  They are very, very committed to the idea
*  of maintaining control in every sense over China.
*  And that should be a lot of something to work with.
*  And they're humans, we're all humans.
*  We all want good things.
*  Like, and this saddens me and it scares me
*  that everyone's pushing it this way.
*  And it's like a huge, obvious failure mode.
*  And if you're too out about it before you're,
*  you're too out about it especially before you can
*  actually enforce it, right?
*  Like why is TSMC still there?
*  In the four, exactly.
*  China doesn't use any of their checks, right?
*  Like if they believed what you believe,
*  when you say these things, would it still be standing?
*  Or would they bomb the hell out of it
*  even if they don't and they tell you what?
*  Yeah, that seems, I don't wanna bet on that
*  cause that just seems like somehow icky at a minimum.
*  The answer is because nobody believes it in their gut
*  at the higher levels as confident enough to do something
*  that would rough the global economy
*  and threaten the stability of their regime.
*  And they're like, you know, we should be thankful
*  that that particular move is off the table
*  for all practical purposes,
*  but there are other moves that are not.
*  And as things get far, go farther down,
*  things are gonna look less and less unthinkable.
*  And we're gonna start to be put on the table
*  by various actors, especially if we are loud
*  about the fact that we are doing a strategy like this.
*  This was an aim of world domination,
*  permanent sign of advantage.
*  I'm not saying you shouldn't seek it out
*  and say, understand what you are doing.
*  And also consider the alternatives.
*  Again, all of the choices are bad, but yeah, it kind of sucks.
*  Yeah, it does not seem like we're headed
*  in a good direction there.
*  Obviously we have, and I can't,
*  this has all been filtered for me through the news.
*  I don't speak Chinese, so there's plenty
*  of lossy translation, but my understanding is
*  that we have Xi saying 2027 is the deadline
*  for the army to be ready to do something about Taiwan
*  with the events of the Polan invasion or whatever.
*  It seems almost impossible to defend TSMC fabs
*  from a Chinese drone swarm if they're like really determined
*  to knock fabs offline.
*  I mean, quite obviously a China was to knock TSMC
*  out of existence and can.
*  Presumably if China wanted to knock American chip production
*  out through sabotage, it could.
*  Yeah, at the risk of night, you know,
*  she's just very hard to put defense in the situations
*  if you care enough, but luckily we don't live
*  in those worlds yet.
*  I mean, obviously, you know, if I was the US government,
*  I would either deliberately have a goal
*  of US chip independence to wait 27,
*  or I would be pretty damn committed to defending that.
*  Now you're hearing too that Taiwan is saying increasingly,
*  and I was wondering about this.
*  I was like, why are they so happy to send all the chips here?
*  Like this seems like a very.
*  Why are they shutting down the nuclear power plants?
*  I mean, the Taiwanese are not necessarily acting
*  strategically or rushing.
*  They have said more recently, at least I, again,
*  I filtering all this through media,
*  but my understanding now is that there are new statements
*  coming out that are like, we prohibit the export
*  of the two nanometer process,
*  and there is gonna be some most advanced technology
*  that will still be here and will not be going
*  to the United States.
*  I don't have a super clear sense
*  of how much that really matters.
*  I mean, there's so much obsession on chips
*  and five, four, three, two nanometers, whatever.
*  It seems like, does that exactly matter,
*  or is it just having the ability to make a lot of them,
*  whether it's three or two?
*  Like, it doesn't seem like that.
*  I honestly don't know.
*  I just ask someone who knows, I just don't know.
*  It seems more likely to me that raw volume
*  will be more important than one process
*  versus the next generation of process.
*  Like I would take 10, three nanometer fabs
*  over one, two nanometer fab for sure, only because.
*  It's a technical question about how they impact
*  these cases that you want,
*  and to what extent is better efficiency
*  of individual chips scale better than adding more chips?
*  Questions I just don't know the answer to.
*  This is another question I don't think anybody
*  has the answer to, but the possibility
*  of distributed training schemes popping up
*  is another area that I am trying to watch
*  as closely as I can and feel like
*  is one of the best candidates for us.
*  Big shake of the snow globe, where, you know,
*  if you have to build and not just build,
*  but defend a trillion dollar data center,
*  you have a real problem.
*  Whereas if you can do ASI at home
*  in the way that we used to have SETI at home,
*  then, and maybe still do, I haven't checked in
*  on SETI at home in a while.
*  I'm sure it's still running.
*  Yeah, it probably is.
*  But there have been some early reports recently
*  of distributed training starting to work.
*  And those have been only like partial disclosures,
*  new research had one where they were sort of like
*  coy about it and said, we've got the big working.
*  We don't really even quite know how it's working yet
*  or why it's working.
*  But that space definitely could change a lot
*  if it could get you to the point where, you know,
*  in more of like a Bitcoin sort of way,
*  like everybody's GPUs could just be distributed,
*  networked and contributing.
*  That creates like a very different dynamic.
*  Yeah.
*  Look, I have been skeptical of the quality
*  and efficiency of distributed computing proposals
*  for training.
*  And I think that's all left so far from what I can tell,
*  but obviously that could change.
*  And if that happens, that puts it kind of a clock
*  on everything in various support instances.
*  And again, the more things get released,
*  they can't be taken back.
*  And the more these types of things happen,
*  the more all of your options for not dying become bad.
*  The more you have to choose impossible choices.
*  All of your options will always suck.
*  And if you propose anything,
*  people will just cry a bloody murder.
*  But like nothing that doesn't require crying a bloody murder
*  in self-form is possibly gonna work.
*  What do you do?
*  Right?
*  But cross that bridge when we come to it
*  and we hope the physics is kind to us.
*  But yeah, the more this looks like a thing,
*  the more like we just have a lot worse options
*  and a lot worse options.
*  Unfortunately, that's how it is.
*  Do you update your P-Doom on a regular basis?
*  Do you have a ready number for P-Doom?
*  I have been using 0.6 for a while.
*  If I was being like fully robust,
*  that number would be changing.
*  Obviously on a day-to-day, week-to-week,
*  month-to-month basis.
*  Certainly, for example,
*  we would have updated when SB 1047 was vetoed.
*  It would have updated when Trump was elected.
*  Yeah, you can argue either which direction it should go,
*  but definitely should update.
*  Otherwise, you're just being very silly.
*  Should update when we find out
*  the various things about open AI, et cetera, et cetera.
*  Fundamentally speaking, I think my attitude is just,
*  it's not useful to try and get that false precision
*  or get that delta.
*  It's like the, how old is that dinosaur's health?
*  It's 75 million and three years old, right?
*  Because three years ago, I asked it, it was 75 million.
*  But yeah, you want to keep the delta accurate
*  for consistency, but you also kind of don't, right?
*  Like don't like these out, 10, 90%.
*  If you're confident that you have a lot of uncertainty
*  and you have enough ways things can go wrong,
*  but you also have enough just general,
*  like various ways things could positively go right,
*  that the things you made don't change very much.
*  Like I know it would find it that useful.
*  And I also find that there are a lot of very weird
*  and imprecise things that I would evaluate differently
*  on different days caused me to get to different numbers
*  if I were to actually do those three calculations
*  in my head that was more precise.
*  And that, yeah, I don't think we should spend too much time
*  on the details of the question.
*  It's basically a question of like,
*  if you do it as useful as that stuff,
*  like, okay, are we talking about some 1%, 1%,
*  like 2%, 5%, 10%, more than 10,
*  like potentially more than 10, but not 98.
*  And then like really high.
*  And that's mostly what you care about.
*  Certainly you have been the range of three to 70.
*  Yeah, I'm with you on that.
*  So let's go through some of these other updates
*  because I think we're in the home stretch
*  and there's like a handful of items,
*  most of which you just mentioned
*  that I wanted to get your take on.
*  SB 1047 being vetoed, we both supported it.
*  I certainly, I went through a moment
*  where I wasn't so sure when it seemed
*  like the Frontier Model Forum was gonna be like
*  potentially overly empowered.
*  And I did not like the sound of that for a minute,
*  but then the final version seemed quite good to me.
*  I don't know if you ever wavered,
*  but I know you also did support the final version at least.
*  How big of a deal do you think it was that it was vetoed
*  and how, or actually how big of a deal
*  would it have been for it to exist?
*  I think it's, we're gonna find out a lot more
*  by February 1, like by the time that Trump has had his day one
*  and we decide, he tells us what he's going to do
*  with the executive order
*  and other immediate executive orders of his own,
*  the issues on day one.
*  But I think we're seeing that maybe this is a,
*  this is a giant disaster.
*  This was a huge cost for fun.
*  As a result of SB 1047 being vetoed,
*  the bill is not being used as the model for right now,
*  other legislation in other places,
*  even though it is obviously the best model legislation
*  that we have available.
*  Instead, people are considering bills like the one in Texas,
*  right, which Deep Bell has written up very well,
*  which use a use case targeting EU style,
*  EU-AXILE approach to regulation of AI.
*  That approach has all of the costs you can imagine
*  and none of the benefits to existential risk.
*  That approach is a EU disaster.
*  And without a better alternative
*  that is currently viable to talk about,
*  without any sense of the action is being taken,
*  that approach is going to become increasingly hard
*  to stop unless we quickly do a turnaround.
*  And it doesn't seem like people like Dean
*  or others who understand how awful this is
*  are willing to get behind an SB 1047 style
*  alternative revival and propose a compromise.
*  They seem to continue to argue, go back to,
*  or they can tell that everything
*  that was proposed is always bad.
*  And we can all line up,
*  but the Texas bill is terrible, right?
*  But I don't see how you stop more and worse states
*  from adopting the worst possible kind of AI regulation
*  unless you give them something better.
*  And SB 1047 would have been something better
*  out of California, it would have carried weight,
*  it would have been copied extensively in other jurisdictions,
*  could have been complemented by narrow task bills
*  and those things like deepfakes
*  that are covered by SB 1047.
*  We could have had a net positive,
*  reasonable regulatory regime from the states
*  pending Congress's decision
*  and a much better chance that Congress adopts
*  a more policy.
*  Instead, we live in a world in which we're looking
*  at even a deeply red Texas,
*  looking to implement a bill that you would think
*  they would recognize was like EU style,
*  complete regulatory suicidal nonsense.
*  And yet here we are, right?
*  Like Texas should be the last place we should pass.
*  Texas should know better.
*  We should be like, why is Texas messing with us?
*  Like, it's like their entire state, they don't think that.
*  And yet here we are, like, sure,
*  Connecticut passing it makes some sense.
*  I get that, but Texas?
*  And yet that's the thing, you can't,
*  no regulation was never on the table, right?
*  Not for very long.
*  This idea that we'll just keep yelling about every bill
*  as if it's a disaster, no matter how well constructed,
*  no matter how well touched, no matter how many benefits are.
*  Like we had this debate between libertarians
*  and nutcase extreme libertarians, let's be honest,
*  as to whether or not this bill
*  was a good enough bill to pass.
*  And now libertarians and the nutcase libertarians
*  are lining up to oppose the regular people
*  who regulate everything into the ground, right?
*  But like those who still want to regulate AI
*  are trying to find this narrow path
*  back to something reasonable.
*  And the people who don't want anything
*  are refusing to accept that their position is not viable.
*  Right?
*  They want our help, frankly, it's how it feels.
*  It feels like I started the blade
*  and now they're burying into the door.
*  And now you want my help, but you're not offering it.
*  You just want me to stop them,
*  we're very easy to kill all of us.
*  And I will, but like, you know, somewhat,
*  but like, this is just completely insane.
*  Right, like you have to understand it.
*  And then it comes down to what Congress
*  and the executive are gonna do, right?
*  If Trump repeals the executive order
*  and doesn't replace it such that the reporting requirements
*  and the executive order go away.
*  Or even worse, if he also tries to take down Nest
*  and AISI in particular alongside it,
*  which will destroy the voluntary commitments.
*  If these things go away and we don't have
*  SB 1047 to replace them and we don't quickly
*  have something to replace them,
*  we are in a horrendously worse position.
*  Right, like if we can retain the executive order,
*  if we retain Nest, Trump can simply revise
*  the safety frameworks so that they don't have
*  the woke stuff that he hates,
*  but they still do the things that actually
*  pertain to access of slavery.
*  We can reach a compromise,
*  I think it's an entirely viable rule.
*  I think there's nothing stopping the new administration
*  from understanding that the things that we care
*  about together, we still care about,
*  getting rid of the things that they hate,
*  they legitimately hate,
*  that like, I'm not gonna argue with you
*  that you hate them because I'm not gonna convince you.
*  And also frankly, these previous regulations
*  did in fact go everything they go way too far.
*  We have possible to implement requirements.
*  Like you can't use an implicit bias,
*  you can't use a disparate impact test on an AI algorithm.
*  It will never pass.
*  Right, even if you are fully committed to the AI philosophy,
*  it is completely impossible.
*  You cannot do it, you cannot do it.
*  So these rules were not going to ever work
*  if they were trying to make them enforceable.
*  But that is the future by default
*  if we have nothing to work with.
*  Right, so I would implore the Trump administration
*  in the same way that I implore California,
*  to get it back together and give us something
*  that fills that void, right?
*  Like if you don't issue guidelines that people can follow
*  that don't include the things you don't like,
*  they typically don't adopt the guidelines you hate,
*  including the guidelines that you already issued
*  by the Biden administration.
*  You need to replace them with something else.
*  You need to be in this game to win it.
*  And also we have the national security apparatus
*  and whether or not they will take action
*  against certain actions to stop people
*  from doing the things that they really shouldn't be doing.
*  And also oppose visibility requirements that way.
*  So there's various different ways this can go.
*  But yeah, the answer is I think SB 1047 really mattered
*  and it was killed and this is really bad.
*  And I think that if SB 1047 part do,
*  with the market we settle our bill,
*  but modifying to around the edges or whatever,
*  it gets passed next year because with Trump
*  in the White House, it's completely different
*  political environment and AI is a year older
*  and you have to work out repeals
*  and everybody understands something that has to be done.
*  Maybe, but also they can let us know some,
*  also to call for youth based regulation of AI.
*  The worst possible thing you could do
*  if California goes that way,
*  so it was the nation, so it was the world.
*  Though the law, and then SB 1047,
*  it's somewhere between substantial safety loss
*  and the death of the AI industry.
*  Like I didn't want to kill humans in real time
*  because I would have been considered
*  to be hyperbole acting crazy.
*  But I think it was a very real scenario
*  that's ending up way worse than I fear.
*  Where a series of states passes these terrible bills
*  and the AI industry is either crippled
*  or forced to concentrate amongst the few big players.
*  And like SB 1047 would have stopped that from happening.
*  And the mechanism there is basically just that
*  the burdens of demonstrating neutrality
*  and whatever on all these different dimensions
*  are just impossible for anybody
*  but a big tech company to comply with.
*  Imagine a NEPA style regime where anybody can see you
*  over any of a wide variety of parts
*  by a standards that are technically impossible
*  for an AI to complete from one of several different states
*  with several different regulations.
*  And you have to verify this every time
*  you pay a chase to your AI girl or something like that.
*  Like imagine a complete nightmare.
*  This is not that unlikely to happen.
*  It's probably double digit for this kind of scenario to happen.
*  If you're like a real doomer, do you want that?
*  I mean, that you see personally,
*  but there's some argument for sabotage
*  of the industry there, right?
*  I don't want that myself.
*  I'm gonna be virtue, I have a virtue ethicist
*  and a functionalist and theorist.
*  So even if I thought that was good for the world,
*  I would not because of this reason, I would not want it.
*  But like, no, it's just bad.
*  If we do that, the people who say we can't move to China
*  are like kind of warmongering
*  and jingoistic dangerous people, like often.
*  But if we cripple our AI industry,
*  we deny ourselves the benefits.
*  We hurt America quite a lot.
*  That's terrible.
*  It's exactly what let's try to catch up.
*  Exactly what makes them into the threat.
*  And I don't think they are right now, particularly.
*  That's how we lose.
*  That's how we get this nightmare scenario.
*  That's terrible.
*  No, this is, I don't want that.
*  I don't think this ends well.
*  Like, we still gotta win.
*  I wanna come back to virtue ethics in a second
*  and get your take on virtue ethics in the time of Trump.
*  Cause that's something I've been thinking about quite a bit
*  in the last week or so.
*  Before that though, a couple more just roundup items.
*  You mentioned opening AI drama.
*  Yeah, of course, there's always more.
*  I think we just did an episode actually with Dean and Daniel
*  where they talked about their transparency proposals.
*  And in that, we got a kind of recounting
*  of Daniel's personal story of leaving
*  and not signing the thing and then posting online about it
*  and that gradually coming to light.
*  And then I guess those policies mostly are reversed.
*  He did say that, yeah, now he can basically,
*  of course he's still bound by not disclosing trade secrets
*  but he is permitted to criticize the company
*  and his equity is restored.
*  And so that turned out reasonably well, I guess.
*  They have also taken another 6 billion or so in funding
*  with a notable clause attached
*  that they must turn the thing into a for-profit
*  or else the investors can get their money back.
*  Presumably all that money is gonna be spent at that time
*  on GPUs and lots of electricity.
*  So it doesn't seem like there's any way
*  that money gets kicked back.
*  It seems like basically the train is now rolling
*  on this conversion to a normal-ish for-profit entity.
*  6 billion is not that much money
*  for a company that's worth 150 billion.
*  It's a very small raise probably because they would have had
*  to take a much worse price to get a much bigger raise.
*  But yeah, they're throwing their hat over the wall.
*  They're saying we have to convert.
*  We don't convert, then we're in a lot of trouble
*  because who's gonna give us the money?
*  Pay back the earlier investors at any reasonable price.
*  They're at risk of being sold off for parts or acqui-hired
*  or like other various disastrous scenarios in two years.
*  They can't make this through.
*  Look, they're trying to pull off the theft
*  of at least the millennium
*  and quite possibly all of human history.
*  They're trying to steal most of OpenAI from the nonprofit,
*  which currently has both the economic value of OpenAI
*  plus the control premium.
*  And they're trying to do it
*  for a small fraction of OpenAI stock.
*  And the question is, will they be allowed to pull this off
*  at broad daylight or not?
*  And time will tell.
*  I wrote articles about this and my position has not changed.
*  It seems like you're going to get away with it.
*  What do you think?
*  I think that the president is listening a lot to Elon Musk
*  and to OpenAI.
*  And it is again, the biggest theft I've ever seen.
*  Short of like, you know, the book tax cuts, for example,
*  I could argue it would be a bigger theft
*  depending on how your perspective is,
*  but where it could just be good business.
*  I guess you're a little from comedy.
*  But like the point being that like,
*  this is a giant illegal misappropriation of puns
*  from my perspective,
*  just very flat out in broad daylight.
*  There are various actors who can do something about it.
*  Many of them have reason to dislike OpenAI.
*  We're just trying to open AI in this situation.
*  And also the board has to sign off on it.
*  And the board's members could have legitimate worries
*  about the fiduciary duties and potentially
*  the future of the corporate bail
*  if they sign off on a number like 25%.
*  So I don't think it is at all obvious
*  they'll be allowed to get away with this
*  at anything like the current number.
*  But yeah, like by default,
*  when this kind of thing happens,
*  like the way it works is power just forces you into a corner
*  where they threaten to blow everything up
*  unless you give in and people one by one,
*  you then through some combination of threats and drives
*  and leverage and mindfuckery.
*  So probably they get away with
*  not necessarily exactly the deal they're proposing,
*  but they get away with quite a lot.
*  That's what I'm expecting to happen in my gut.
*  But I put a contract on the table and I might buy some,
*  right, if you give me a cheap enough price.
*  The point about Musk being in the president's ear
*  and the president potentially having a proclivity
*  to use the justice department in, let's say,
*  idiosyncratic ways definitely does change
*  the analysis quite a bit.
*  I would say in the absence of that,
*  it would have felt overwhelmingly likely to me
*  and that there is a lightning bolt
*  from a distance possibility that could change it.
*  But you also look at it as Microsoft
*  attempting to steal quite a big tech companies
*  and basically attempting to steal quite a lot of money
*  from a nonprofit and Republicans before the Congress.
*  And they might not like that.
*  Think tech is not popular.
*  Microsoft's part of big tech,
*  very essentially part of big tech.
*  So I just think they'll talk them into the whole China.
*  I mean, right now it seems like, I guess a couple things.
*  One is, I think there's always a creative argument
*  and I think there is an argument that,
*  look, it's not worth that much as a nonprofit
*  because if nobody invests the next however many billion
*  to do the next round of scale, then we lose
*  and you're just out of the game.
*  You can't really compete in this way.
*  You're entitled to some, but it goes to zero
*  if the steel doesn't go through in a very literal sense.
*  I put dynamite under your bill then.
*  So you're gonna sell me that bill for a dollar
*  because if you don't sell it to me, I'm gonna blow it up.
*  So it's only worth a dollar to you.
*  So it's only fair that you sell it to me for a dollar.
*  And I think they could argue in somewhat good faith
*  that basically this is naturally occurring dynamite, right?
*  I think the sort of Altman narrative,
*  I think it's reasonably credible is,
*  we didn't know when we started this thing
*  that it was gonna take $50 billion to get there.
*  It turns out it does.
*  I could get all of that.
*  Right, but you still have to give peer value
*  for the assets and that includes the control premium.
*  There is no world in which there's a fair compensation.
*  You can argue, I think, for 49%.
*  I think that's like the fair, aggressive,
*  but not obviously blatantly unfair of the,
*  they have to lose control.
*  They have to lose control where this company is not viable.
*  So we have to give you a 51% to other people.
*  So the company is no longer under their control.
*  So they have 49 and it'll be further diluted by investments
*  to be going forward and you'll get progressively less on us
*  or something like that.
*  But again, I don't see any attempt to wrestle
*  with the reality of the situation.
*  I don't see any attempts to say,
*  this is what open AI is worth.
*  This is the distribution of future profits.
*  This is the control premium.
*  This is why this is a fair valuation.
*  And if you want to make the argument,
*  it's a fair valuation because if you don't give it to me,
*  your company will collapse,
*  and therefore you shouldn't get very much.
*  All right, natural case, let's have that discussion.
*  Is that legal?
*  I'm inclined to say no.
*  Yeah, the other thing that's pretty interesting about it
*  is the intangible assets, goodwill argument.
*  I don't know what the law is governing this sort of stuff.
*  I think it's probably not very well developed
*  because we're only recently moving into this world
*  where so much of corporate value
*  is the intangible goodwill type of thing.
*  This ain't the railroad anymore, obviously.
*  So I don't think we've really figured that out as a society.
*  But the fact that they were all going to shift over
*  to Microsoft or that it's still not that many people, right?
*  They could reconstitute themselves in a way where it's like,
*  unless you're going to totally change non-compete
*  or create new laws to prevent us from doing this,
*  we could always just move across the street,
*  bring our know-how with us,
*  and take a bit of a hit.
*  But they were willing to do it once, and it seemed like that-
*  You can try.
*  You can use those arguments.
*  But I think we know pretty well that if they had pulled the trigger on that,
*  it would not have involved the vast majority of the people in Microsoft.
*  We have involved a large percentage of the people in Microsoft,
*  probably, but also a huge diaspora.
*  And a huge loss to OpenAI.
*  And Microsoft, in particular, Microsoft did not want that scenario, by all reports.
*  Microsoft was doing that because it was less bad than a full diaspora.
*  Right? And it was a credible threat.
*  Right? They did it strategically because they had to.
*  But they were thrilled to just reconstitute them,
*  despite the board panning control.
*  So I don't know that the credible threat is in this context,
*  or how credible a threat it is.
*  It's a credible backup plan.
*  But I-
*  It's a lot of hands on the betters, for sure.
*  I mean, a lot of nodding against Detroit, if you do that.
*  The bottom line is that we have room to cut the nonprofit in for a quick
*  in a lot of the future profit flows, OpenAI, while converting it to a B Corp.
*  That's what they want to do.
*  And if they manage to browbeat the nonprofit,
*  where it's for into accepting, matching less than their share is worth,
*  then that is a theft.
*  As far as I can tell, it's flat out.
*  And people get away with this.
*  I'll say it.
*  Seward and Valley did this all the time.
*  They just decided if someone doesn't deserve what they're legally entitled to,
*  and they just take it.
*  Not that weird.
*  So yeah, I expect them to pull off some form of it.
*  And I would be who?
*  The various attorneys general and other authorities over them to keep a very close eye.
*  I'm not allowed to do anything that's not allowed.
*  The Arc AGI prize.
*  I don't know if you have any particular thoughts on this.
*  People are generally familiar.
*  Progress, I would say, was pretty good.
*  We went from like 30 ish percent to 60 ish percent as the state of the art.
*  85% was where you were going to win the million bucks.
*  So nobody's won the million bucks.
*  But, you know, the gap has been closed by roughly half.
*  I mean, everyone should assume that million bucks goes away into next time around, right?
*  Like, this was probably the last year.
*  Yeah, it seems like can't last that much longer.
*  Test time training emerged as an interesting theme.
*  They had some pretty tight compute requirements and a totally unseen test set.
*  So people started doing small model fine tuning on the unseen test set
*  to try to learn as much as possible in that new environment.
*  And that seemed to be the big thing.
*  I feel like that is likely to be.
*  So I kind of a trend that gets exported elsewhere.
*  I found discussing this with people who did the prize,
*  and it's not eligible to win the prize.
*  It requires things prices will allow.
*  But yeah, it showed it rentably.
*  You can do a lot better on these types of tasks with training time compute.
*  That makes sense because these tests, these test questions are all part of the same
*  general logic that is deliberately different from the general logic that they're using to train.
*  So it's a very special case where a little bit of past time
*  and it's going to be worth a lot.
*  So it makes sense to me.
*  It paid off pretty well.
*  Yeah, it's an exciting potential alternative to a one's infrastructure for looting an inference plan.
*  And we don't really know how effective it's going to be in general.
*  I think it's kind of a best case scenario place to start.
*  So try it on other stops.
*  Yeah, see how it goes.
*  I'm excited.
*  But it makes sense that if you're doing a series of related questions,
*  then you should do some form of temporary tuning of your model while you do that.
*  There are various things in this general sense that fall under the
*  things I would definitely be trying if I was like directing research to a major lab.
*  Or otherwise had a lot of money to try a bunch of programming time and
*  you keep to try and figure out parts of models like.
*  Two things that maybe didn't happen.
*  One that I would say pretty safely didn't happen.
*  Deep fake election chaos.
*  Everybody was on the lookout for that, of course.
*  It didn't really happen.
*  Did it not happen because people were just too savvy
*  or the technology wasn't quite there or nobody really tried?
*  What's your read?
*  I know little there were a few incidents.
*  There was like this fake accusation against him walls.
*  There was like a small.
*  It was like I think I did think of Kamala that was shared by a must.
*  There are a small number of them, but.
*  I think the combination of two tech savvy and not tech savvy enough in various different ways.
*  Like people were just I think the main impact when people were suspecting
*  real things of being fake, not they suspected fake things of being real.
*  And that's a common theme for now.
*  My basic conclusion is.
*  The big news is demand side, not supplies.
*  People of all political orientations demand fake news.
*  They demand echo chambers.
*  They demand stories that are true and conspiracies and tall tales and so on.
*  So in a world in which they're already making up complete and utter nonsense with very
*  basically no evidence and telling these stories, and they were this time around,
*  like every other time.
*  And you don't want to know what would have happened if this election had been like.
*  Turned out differently, right?
*  Been closer.
*  Gone in either direction, but we were very fortunate.
*  Conditional on it, whatever you think otherwise happened.
*  Conditional on a trouble with.
*  We were very fortunate that he won popular vote and enough of.
*  The electoral college by enough of a margin that everybody agrees he won.
*  And there's no discussion.
*  About whether he won.
*  We're always agreeing it's over.
*  And like.
*  But either scenarios with this election were not the true nightmare.
*  The big the biggest nightmare scenarios were scenarios in which like both sides thought they won.
*  In a real way.
*  And we didn't have that.
*  But again, like 2020.
*  We had the big lie, right?
*  And it wasn't driven by it was driven by just why that likes with no,
*  with basically no real evidence behind it.
*  Just like me.
*  Shut up.
*  So the issue is it just doesn't change anything to add deep fake picture.
*  You know, that really helped you expect anything.
*  It hurts you because now we can point to the deep fake picture and with enough analysis,
*  figure out how to demonstrate that it's wrong.
*  As it improves, there will be more and more problems,
*  but I just don't think it's that central to the source of the problem.
*  Of this information, the source of the problem of people having echo chambers and feed each other
*  call tails from all directions and all sides.
*  And.
*  AI analysis of information is what you become our ally.
*  But he determined whether or not something is accurate and it will be very helpful.
*  And so I've been optimistic about you fix for a while now.
*  I did not surprise me.
*  That there was very little impact.
*  Obviously, it was about the extreme left end of impact where there's like almost no impact.
*  But it didn't surprise me very much.
*  I expected this to go fine.
*  I do expect other impacts like 2020 is going to be very influenced in a lot of different ways.
*  Unless I am very, very surprised.
*  But yeah, I know the people who thought in the 2024 was going to be the AI election.
*  I was always pretty skeptical of that.
*  I thought too soon.
*  Something that I demand fake news on is the effectiveness of UBI.
*  And another topic you've covered recently is the recently wrapped and pretty thoroughly analyzed
*  Altman backed UBI experiment.
*  My general sense, and I haven't been super deep into the literature, was most people like this
*  doesn't seem to have worked super well.
*  But I want you to tell me the story where it works great.
*  And I should expect to wait with it.
*  We end up with UBI.
*  So please do.
*  And we all want that.
*  But I'm not as you mentioned, I've heard you have this.
*  So I'm going to give you what you want here.
*  Unfortunately, the UBI experiment in context was basically a failure.
*  Not like 100% failure, the money didn't just disappear with no benefits.
*  But it was clear that wasn't the way as implemented.
*  I do think that a future world where there is math and technological unemployment,
*  and there is near universal need for the universal basic income,
*  you just did a very different scenario.
*  And this is not studied very well.
*  So the good news, the fake news that you want, the good news is that we don't know if this
*  would be a good solution to a different world, or at least much better than not doing it.
*  But basically, no, you can't just give people a bunch of money in the first world,
*  sell them at random, a bunch of money, and expect them to like,
*  perfectly improve their lives by getting a bunch of money.
*  The way you would expect.
*  So how would you summarize my sense is like, people worked less,
*  not so much less that they had the same money.
*  But I saw some headline where it was like people worked enough less that they ended up with less
*  money.
*  That seemed very strange to me.
*  I basically discounted that on the priors.
*  I don't know the exact details, but essentially, at the end of the two year period,
*  people were generally not substantially better off
*  beyond the additional consumption and decreased work during that period.
*  The resulting savings was mostly just exhausted without resulting in then scaling up or acquiring
*  permanent capital or otherwise like making their lives better.
*  Which is just not the thing that we were trying to fund.
*  Right, like we were trying to fund them getting into a perfectly better situation,
*  right?
*  Improving their situation, improving their social capital, improving their
*  productive capital, improving their skills, their educations, et cetera, et cetera.
*  At least didn't see very much of that.
*  There is no way this is efficient on that type of basis.
*  It needs to be that context.
*  But again, like these things can be very context dependent.
*  So the fact that it happened to a small number of people could be, for example,
*  higher enrollment because if you get a bunch of money in a relatively poor community,
*  but the people around you do not, you are effectively in a situation where that money
*  is being taxed very aggressively by your communal bonds and obligations.
*  And you have a very hard time identifying why you have that money and people don't.
*  You have no opportunity to move along with other people.
*  It's very difficult.
*  So I can imagine a thesis that says we've given it to everybody in Chicago.
*  It would have been very different than giving it to a very small portion of the people in
*  Chicago and say, no, and we just don't know.
*  And that's to me a very valid cause.
*  Yeah.
*  Threshold effects can be a big deal in a lot of these things.
*  It feels was there any silver lining in what you reviewed around just like happiness,
*  well-being, like time spent with family, kids going to school?
*  Non-zero effects, but again, nothing that would justify these facts.
*  Okay.
*  We're probably not going to get in the next four years anyway, because we know who is elected.
*  Interested in your take on you said that you could argue which way we should update.
*  I have found myself going into the election.
*  I was like, this does not seem like the guy we want to be president.
*  If there's a decent chance we're going to have AGI, he seems just too unpredictable,
*  unstable, inconsistent, not rigorous thinker, not trustworthy to people that might need to trust him
*  if we're going to find ourselves in sticky situations.
*  I still think all that's true.
*  I've been at least tempted to some optimism by the Samuel Hammons of the world who are like,
*  look at all this, you know, super smart policy we might get out of somebody like an Elon Musk,
*  shadow president, whatever.
*  What was your feeling coming in?
*  What's your feeling?
*  Are we gone?
*  And then-
*  I mean, prove me wrong, kids, right?
*  Prove me wrong.
*  Like I, you want this to work.
*  We want, everybody should want this to succeed, this administration to succeed.
*  On AI especially, but also in general, and to make things good as much as possible.
*  And Trump was very high-yaring, right?
*  Like there are worlds in which Trump is a huge disaster by everyone's evaluation,
*  actually including Trump.
*  There are worlds in which, like you take interstitial terrorist, it's really bad,
*  either to the country's economy or budget or democracy or literal extensional catastrophe.
*  It's all on the table with some pro-government
*  that I consider to be higher in the alternative.
*  But I also think there's more upside than there was under potential terrorist administration.
*  In particular for AI, Harris had already shown what path she was going to go down, right?
*  She was going to go down a very Biden-style path.
*  Very advantageous to that, covered some of the basics,
*  but the chance she would react decisively to events seemed low,
*  which she tends to do as far as we could tell.
*  There'd be a large amount of emphasis on, I don't necessarily want to say what her DEI,
*  but like on left-leaning muddied concerns that I don't think are the right focus here,
*  that were in fact reasonably ubiquitous amongst a lot of the things that she was
*  in her administration, collectively her and the Biden administration pushing.
*  She was involved heavily in the AI effort, so I think we can in fact
*  read them as meaningful to her personal views on this, unlike some other policies.
*  And like Trump and Vance, they could obviously choose some of the worst possible policies
*  in this area.
*  They could appeal to executive order or essentially start replacing it.
*  They could sabotage the ISI.
*  They could actively encourage open source.
*  They could in ways that are dangerous, not in the ways that are helpful.
*  They are only clear at the time and place for open models and obviously non-AI open source
*  software.
*  But they could also potentially go as far as to like attack or even break up some of the tech
*  companies creating diasporas, creating a much more multi-polar race in ways that would be
*  very harmful.
*  They could slow down immigration, legal immigration in ways that are very harmful for
*  record competitiveness, being opposed to terrorists including on GPUs in ways that are like obviously
*  just a bit terrible.
*  There's a lot of big risks.
*  This room I think will work.
*  In addition to the obvious standard, he like just like very much disambiguated equilibrium
*  and like caused massive inflation or it wasn't much.
*  Yeah, you're a lot of way that stuff that was on his article proposal.
*  But it wouldn't strike.
*  Tell me that story.
*  How does it go really well?
*  Okay.
*  So all of the things you really fear are things like deporting passing amounts of immigrants
*  and like printing tons and tons of money and deposing gigantic tariffs that would wreck
*  the global economy and et cetera, et cetera.
*  That's all like big talk and negotiating conditions and doesn't really happen.
*  The Biden executive order gets replaced by the executive order and then essentially the
*  same executive order without all about woke.
*  All of it from what called woke shit, right?
*  Regardless of what it is, basically strip out those priorities and put the rest mostly
*  back.
*  The safety standards issued by NIST and AISI are modified in similar ways, but otherwise
*  he kind of continues the same general policies.
*  He then brings down regulatory barriers, help us build the energy infrastructure generally
*  and most importantly is capable of changing his mind.
*  Like it's clear that he understands the effect to people who understand the AI
*  is ancient in fact, is capable of recognizing super duper AI and such in the danger.
*  So like when the time comes, he and Vance, who was actually plausibly like at least somewhat
*  connected to the communities that we care about potentially could be dialed in very
*  quickly to the right people, realize what's going on, understand the problem, they pay
*  that rapidly, they will put national security apparatus, things happen.
*  Generally just things get unstuck, you know, just the end or forward.
*  We have unified government that we just nominally can potentially pass things,
*  potentially reach the AI.
*  You know, it's possible that AI bill can be passed under a public administration,
*  because Democrats will be well-disployed under a public administration.
*  Whereas before the democratic administration would have been basically like the republics
*  would have just monoposed it as anything the democratic caucus could have drafted,
*  what it meant to vote for their republics to accept when they wanted universally opposed it.
*  I can tell stories, right?
*  I can tell you more stories where this works out perfectly well.
*  Including the story of he makes the one big decision, right?
*  Like when the time comes and the AI show up, he pulls the trigger on necessary outcomes
*  in ways that harass my up-huff.
*  And that just overrides everything else the entire administration does, every issue,
*  everywhere, if within the harass, other than that.
*  I think it's perfectly plausible.
*  So my story you can tell about the first Trump administration, right,
*  is you get a bunch of stuff until 2020 that was different from what Clinton would have done.
*  And the impact of all those differences were Trump changes didn't matter very much.
*  And then 2020, you know, operation works for me.
*  The role of operation works to be the entire world economy in our lives were dramatic.
*  And this was just so much more important than everything else that happened in the Trump
*  administration until election day, right?
*  At least until election day.
*  And all that happened afterwards that you should otherwise, if the election had gone differently,
*  at least you would be ecstatic that Trump won over Clinton.
*  Even if you thought there were other policies for that.
*  You think very, very, very reasonable to that.
*  And like AI is the one issue.
*  I mean, people are capable of changing their minds.
*  It is not an obviously part of an issue.
*  And they can separate.
*  The basic problem in AI is if Trump and the Republicans associate any action to keep AI
*  safe or regulate AI with Woke and DEF, and therefore, indignantly oppose all of it,
*  slash they buy the open source, the open model arguments such that they just sort of
*  get this weird thing in their heads that nothing open can ever be dangerous or bad.
*  Like this Andreessen philosophy.
*  The combination of these two factors in some form is the nightmare AI specific scenario.
*  Well, the underlying AI specific scenario is that the prediction market is saying that
*  a Taiwan invasion is potentially more likely now under Trump than it was under Trump.
*  And obviously, invasion of Taiwan is like a giant wild card at the entire problem.
*  But the specific problems in this specific area, they might not come to pass.
*  So, you know, that's both.
*  And I don't know, from what I have seen, I try not to closely follow politics.
*  I try not to pay too much attention.
*  The reason I didn't write about any of this, this is like as much as I've talked about politics
*  in a while now, and I would talk to the election to do it.
*  But the reaction to the election, I guess the right word for it would always be normal.
*  So far, it's been like a week.
*  But it seems like a normal transition with a normal set of picks for running the administration,
*  motion, which are out of the Senate House.
*  It feels like everyone is just doing what we do.
*  And that's true, then it is not a part of the mission.
*  Right.
*  And you can be happy, you can get very, very sad about politics, having power.
*  And that has nothing to do with the AI situation.
*  Certainly, the situation is its own animal.
*  And again, they're going to make some decisions.
*  We're going to find out pretty fast.
*  There's a couple of kind of keywords there that I thought would be worth maybe digging
*  in on a little bit.
*  And it's not a partisan issue.
*  That was one phrase that I have been thinking about quite a bit.
*  The other one being, it would be bad if we got into a situation where they, Trump, Musk,
*  Vance, whoever see anything related to AI safety as sort of woke shit.
*  Those points seem...
*  I would say, Musk specifically is not going to be like that.
*  It's going to be...
*  Yeah, he's smart enough to avoid that collapse, I would think.
*  Of course, how long he'll be involved, at what point their relationship may have a breaking
*  point, obviously could be any minute now.
*  I'm one generally to think like long may he remain influential, Musk that is, but there
*  could be a twist or a turn any moment, it seems.
*  I want to get your take on what a person like me and a person like me,
*  and for the purposes of this discussion, I'm like, definitely pro technology, pro progress,
*  broadly libertarian, left leaning on like a lot of issues that are more salient to most
*  of the electorate, but very focused on AI.
*  What is virtuous for me to do?
*  How should I approach all of this strategically?
*  I used to say that AI safety and AI ethics people should make much more common cause
*  because it's all about getting the AI's under control and getting good outcomes from them.
*  Now I have to maybe reevaluate that if I'm thinking, geez, I don't want to show up at
*  the White House with Sinbad J.
*  Burew, if I'm saying her name correctly, because they're not going to like that.
*  So do I need to reconsider my AI safety, AI ethics, kumbaya attitude?
*  How much should I be?
*  I definitely am like pro more people coming to America.
*  I'm pro even like people that are here technically against the law.
*  I feel like for a long time we've sort of winked and said it was okay.
*  And it feels like for the government now to like suddenly come down very harshly and say,
*  well, you're illegal.
*  That to me feels like not a virtuous thing for the government to be doing.
*  I am really struggling to think like, where do I draw the line?
*  Like deportations happen all the time.
*  If they like tick up a bit, do I like go out into the streets?
*  Probably not.
*  But if police are coming door to door in my neighborhood looking for people,
*  then there's got to be some line.
*  And I'm a little bit lost as to how to think about how much I should focus,
*  how much I should compartmentalize.
*  Should I be making like pre-commitments now?
*  I realize I'm going to have to take some pitches, but I don't have a framework yet
*  to organize what pitches I take, which ones are sort of not my lane,
*  but I still need to go out of my lane to address.
*  And also like who I should kick out of my lane if there are people in my lane that
*  no longer have a place.
*  Right.
*  I think this is a bunch of different questions.
*  So one of the questions in classic, if the bad government come,
*  right, call it fascism, call it nasty deportation, call it whatever you want to call it.
*  I just, your colleges cannot abide.
*  They're just so bad.
*  At what point do you defy the law?
*  At what point do you work against the law?
*  At what point do you take various levels of action against the law?
*  And to be clear, I think there's a not that high a chance that like
*  you face these questions in earnest.
*  I don't really expect the actions that you're worried about
*  to be that likely to take place on the scale that would cause you to seriously consider.
*  I don't think, but yes, I think that thinking now
*  about how you would act in response to different actions that you strongly oppose is wise
*  because you'd be in the moment would potentially think portly or in either direction.
*  If I'm talking to you, I decided to be afraid.
*  They're both on the table, but I'm not going to get into like exactly
*  where to draw those lines or how to think about those policies.
*  I'm going to try to think of questions like what are the consequences
*  practical of various policies because I want to stay in my own policies and I've decided on.
*  And yeah, obviously, that's deportations would be really shocking to the economy.
*  It's really bad for America's productivity and competitiveness,
*  regardless of what else you may think about in respect to or whatever.
*  So like, I'm hoping they don't happen.
*  I don't expect that to happen.
*  Next, and specifically in how you spell the immigration gets like massive versus expected.
*  And that's very bad for our AI industry compared to this as well.
*  Stainless from that.
*  But question of like the ethics people and how to deal with that.
*  So I think that like.
*  We're now going into a lot of things, the reason for which was battles in the states.
*  Where there's going to be bad legislation at the.
*  Yeah, I had a keyboard support and the area, you know,
*  kill everyone people are here purposely because.
*  Like, oh, yeah, I talk to people who think that AI is so duly
*  met in the different ethics that I do.
*  And they're like, well, you know, you don't stroke on AI.
*  I don't care.
*  But I think it's wrong.
*  I do not view that.
*  Some people, of course, will think that way.
*  But I think the vast majority of them are.
*  But the reason to be allied with the ethics people is if we have common interests in passing.
*  You can combine bills or narrow bills that are good for both of us.
*  They drive home the points that cause things to happen that are good, that cause better outcomes.
*  If they're pushing for that negative stuff, there's not really much to talk about anymore.
*  And that's where we should continue to point out that things that we want
*  advance their costs.
*  They should be supporting what we want.
*  But we shouldn't like support what they want.
*  We don't think it's good in the hopes that they will support us because they don't look back.
*  Like, we need to understand that there is nobody we're negotiating with here.
*  Right.
*  And if that there would be, there was doubt in the back of the way.
*  We don't have any leverage on this.
*  Right.
*  Now that.
*  The SP 1047 window is closed.
*  We don't get to necessarily have that money impact on what they do.
*  They're going to be who they are and they're in a gated strike.
*  They're going to massively gain strike because the decisions will become worth a million.
*  And but also they will be, you know, strongly, strongly opposed by the current.
*  So yeah, if you're trying to lobby Congress, you should be strongly,
*  strongly supporting strong air ethics calculations.
*  That'd be a mistake.
*  And in general, the entire, they're not telling everyone movement has a problem.
*  Culturally, so much of it thinks that like blue values are obviously just standard and correct on
*  most of the standard issues and that the orange man is horrible and bad and that red values are
*  terrible and obviously wrong and is in terrible position to cooperate with the red administration.
*  Let alone a trumpet, literal Trump administration.
*  And that is creating only a few Sam Hamids of the world that can actually treat the situation.
*  And usually contribute and not the shame.
*  And it's too bad, but I'm not saying what I was ready for an election night.
*  People can guess, but we live in the world we live in and we should deal with what we have.
*  Yeah.
*  I'm playing around with these like phrases or mantras.
*  And one that I've been toying with is it's better to be a live player than to play Cassandra.
*  And that's even like within the AI space.
*  I feel like we are headed for a sort of general accelerationist vibe.
*  And of course, even that could change as we've said, Trump is high variance.
*  You could get freaked out.
*  Who knows, but it seems like we're more than likely headed for more like an AI Manhattan project
*  than a pause.
*  No, no, the Manhattan project is doge.
*  Right.
*  So the AI Manhattan project, a subsidiary of the doge.
*  But I like kind of reflexively recoil at the notion of an AI Manhattan project.
*  I'm like, didn't, you know, is there something we should have been taking away from the actual
*  original Manhattan project that would cause us to not want to rush into an AI project?
*  I mean, better security for sure.
*  But yeah, but I just got a sense that I think a lot of them regretted what they had done.
*  And maybe a little bit more thinking ahead would be a good thing.
*  And just rushing to copy this dynamic of what we got to get there before China just seems
*  really bad.
*  But if it's going to happen, how do I calibrate where I want to focus my arguments,
*  where I want to try to have an influence?
*  It seems like we're headed for AI Manhattan project.
*  I probably can't stop that train.
*  Maybe I retreat to some sort of China dovishness.
*  Or can we at least make the AI Manhattan project like something positive?
*  Can we like race for how many cancers we can cure or some sort of other metric
*  besides strategic dominance?
*  I don't know.
*  Do you have a thought?
*  Can you coach me?
*  This is a decision theory problem, right?
*  So if you don't impact it any way, if you are not correlated, if you're actually in your
*  decisions and your perspective, you're not correlated in any way with the decision to
*  do the Manhattan project, then once it already exists, you have to decide whether you
*  would prefer to live in a world with a more advanced and more successful Manhattan project
*  or not.
*  And I would conclude that conditional on the Manhattan project coming into existence,
*  I would expect you were in my contribution to that project to be net positive because
*  we could pull it in a safety oriented direction on the margin.
*  And also like once you are in fact, when you race no matter what to the finish line,
*  it would be better if you won in the importance.
*  Also, if you want to safely as possible and concretely, it seems wrong not to join the
*  project.
*  In general, it's a project, right?
*  As Leopold calls it, the project does happen and there's no stopping the project.
*  Then like just based on consequentialism from there, right?
*  I think you get on the bus.
*  However, there is the problem that once you like the decision that's being made now as
*  to how to proceed partly depends on how people would react if the net product was done.
*  And you shouldn't necessarily roll over when these things happen for that reason,
*  because that leads to everyone rolling over, which leads to that the wrong plan, the bad
*  guy, etc.
*  Carrying the day in the race has happened, right?
*  Like you don't want to let them like be like, well, we're racing now, so you better
*  help us win.
*  But they wouldn't have started the race if they didn't thought you would help.
*  That's a disaster.
*  So you have to consider all of these possibilities.
*  Assuming you think the man project is in the state.
*  Obviously, the net product is the right decision to support it.
*  And it's not the worst option, right?
*  A unify the nine project by the United States style versus the project is seems clearly
*  superior to me to three to five AI companies racing to the finish line as best as possible.
*  So the other guy doesn't.
*  And that's kind of a fault.
*  You should only consider not helping the projects in that sense.
*  If you think that either your presentation actually makes the situation worse,
*  because you would be accelerating the project without helping it, you say.
*  In a way, it would be bad.
*  Or you think that the project is something you shouldn't have been encouraging by your
*  future willingness to help out in it.
*  But that required it to be an error versus the alternatives that were at the table.
*  And if the alternative to the table were worse, it's not at stake.
*  Like the project is not that unhopeful.
*  Right.
*  I guess not the best scenario, but it's far from the works.
*  And the worst is kind of the default.
*  So thanks.
*  So you're net favorable or tentatively favorable toward a nationalized the labs have one.
*  We know we we know we're not going to have a huge number of live players
*  for economic reasons as previously discussed.
*  So given that there's a small number, corral wall into one.
*  I mean, I'm choosing my words carefully because of the way that people run the
*  discourse these days and because it's generally confusing and conditional and nothing is obvious.
*  And we don't know what our scenario is going to be or what our options are going to be.
*  But if we are clearly rapidly progressing towards AGI and ASI,
*  and the situation is that we erase condition between multiple companies that are under
*  pressure to proceed faster and is reasonable under the conditions,
*  then a unified project seems better than a non unified project.
*  And there are various ways to get a unified project.
*  I'd be emerging as this clause, for example, is another way to get there.
*  Right.
*  The government be involved is not a first best solution.
*  Almost ever in a situation.
*  But it's not the worst case scenario either.
*  So it comes down to just do you have a better option?
*  Nothing is in similar to our development.
*  Once the project is inevitable, waiting longer to start it is potentially just worse faster.
*  So you only want to oppose starting the product if you can actually stop the
*  product from happening in at least some world after that.
*  So it's all very complicated, but I noticed that we're going to know more before this happens.
*  We're not going to start the project at 24.
*  We're not going to start at 25.
*  Right.
*  By 26, we'll know a lot more, be able to make better decisions, hopefully.
*  I mean, start administration.
*  So if you either the AM or positions go out, we don't really know.
*  And it's not like we can't influence it.
*  Let's try.
*  But yeah, like we should deal with the situation as with the options that are still left on the table.
*  And one way to think about the project to me is we should make decisions keeping in
*  mind that we can start it.
*  We potentially could start a project and we don't want to like make it impossible to start a
*  project if we need to start a project.
*  But that like we would prefer to live in and steer towards worlds that don't
*  require the product right where the product is necessary.
*  And then that raises the question of, well, what if the product is going to happen in a
*  world where it wasn't necessary?
*  We would do about that.
*  But yeah, we can talk forever about these hypothetical scenarios.
*  But my guess is the majority of the time that the product actually happens,
*  it was an improvement over the second alternative on the table.
*  The people in the room were talking about.
*  Yeah.
*  Man, yeah.
*  All right.
*  Last question.
*  Where do you look for inspiration when it comes to virtue?
*  You know, the challenge, of course, with virtue ethics is it's kind of tough to know
*  what's virtuous, right?
*  It's subject to debate.
*  Maybe a few shot examples is the way to inform our behavior.
*  So interested in what sources of inspiration for virtue you personally
*  look to and if there's any changes or what you're contemplating on the margin to change
*  or evolve your own work or approach in light of new circumstances.
*  Yeah.
*  The Hague Discord asked on Twitter, just in general, who was the person you tried to
*  model after in some important sense?
*  And my brain just shot back the answer is Richard Feynman.
*  To me, there is a such person, sort of embodies more than other people, I guess,
*  who I'd want to be in these spots, but was the person I can think of.
*  But the truth is I don't really have a single role model per se here.
*  I'm just trying to work my own way through these problems in a very unique mismatch
*  of different influences and perspectives.
*  I've got my Dequatet in my Robert Adam Wilson and my Aristotle and my everything else.
*  Like you just put it all together and you figure it out.
*  And like, I don't think there's a richer approach than I'm like, that is the virtue set.
*  That I want, including myself.
*  I don't think that's always a complete.
*  But I don't think I'm rich or everything depends on precision.
*  In the 40s, I guess part of the way it works.
*  So surely you're joking.
*  Mr. Feynman should be required reading.
*  I haven't actually never read that book.
*  Maybe it's required reading it, but right this way, I cannot really imagine an information diet
*  where placing the thing you are largely avoiding consumerist during Mr. Feynman is a mistake.
*  It just seems like very hard for that.
*  Okay.
*  Well, I'll put that on my list.
*  Any strategic updates for yourself?
*  Have you found yourself thinking about any changes in the very recent past?
*  I'd say I'm definitely happy with the direction I'm taking in a broad sense.
*  I pivot into doing work coding for now, primarily towards accelerating.
*  I'm just doing self-improvement, recursive self-improvement.
*  I'm just making my writing process faster.
*  So far I'm well in the hole with terms of time spent versus time saved.
*  Me too.
*  We should compare notes on that because that would have been the same stuff.
*  I am noticeably more efficient going forward than I was before.
*  I expect to win in the long run because it also gives me experience I can draw upon in various ways.
*  It should be good.
*  But beyond that, there's a thing that I'm feeling like I could have improved recently.
*  It's something like, for what I can tell, the Trump transition is kind of a scramble in many places.
*  I should have considered the possibility with enough of a scramble clusterfuck
*  that I could actually, by preparation, have influence on decisions made.
*  My brain didn't consider that possibility until after I started getting reports from people that
*  they were in contact with the administration and sharing stuff and various.
*  But if I had to do it over again, I would have gotten some stuff ready that other people could
*  at least pass along.
*  It's in better form, like prioritization.
*  Also, I appreciated the break for mental health.
*  Everyone's focused on the election.
*  So we had to relatively quiet two weeks.
*  It's nice in its own way, aside from the election part.
*  I hate it.
*  But yeah, no matter what you want to happen and whether or not you get what you want,
*  the election process always sucks.
*  It's interesting.
*  One of the things I've been wrestling with myself is how much to think about trying to influence
*  central decision-making versus maybe just going off in a different direction
*  and trying to do something good.
*  It does seem that there's going to be fewer no's in society broadly,
*  and there's potentially going to be more vacuums if, for example,
*  the education department is removed or whatever, or mandates are dramatically curtailed.
*  There might just be a lot more freedom of action in spaces that people like me have
*  shied away from because we just don't want to deal with the red tape and having to work
*  through the friction that was there.
*  We talked about the good scenario.
*  The good scenario involves maybe Doge is great.
*  Maybe they actually do cut a bunch of red tape and a bunch of regulations and do things again.
*  The Trump that I've always dreamed of getting is the real estate developer.
*  Right?
*  Like, happy.
*  He'd be the ultimate Yimby president.
*  Like, he'd build baby bill.
*  Like, potentially have my name on it.
*  Potentially getting a cut.
*  I don't care.
*  He's building a bill.
*  And we didn't see that in his first term.
*  And maybe he will get fucked.
*  Maybe his true nature will win out somehow, even though he can't be known anyway,
*  because now that he doesn't have to work for real action, he can.
*  But his true nature win and have fun and actually make money.
*  But.
*  Yeah, like, it's different timing, right?
*  I think you should definitely keep an eye out for opportunity going forward.
*  It's more of a narrow window, especially where like really early,
*  the changes haven't been made.
*  You can't see your opportunities yet.
*  And necessarily, you don't know which ones are going to be available.
*  It's hard to care for them.
*  The way you can influence what happens in that window potentially.
*  I don't know.
*  But and I introspect like the mistake I always have made
*  is not trying for the high leverage move.
*  Because it just seemed like it would never work.
*  Like it was just like, that's impossible.
*  Come on.
*  That's ridiculous.
*  But like, when you look back at the cracker, no, it's not as ridiculous as it sounds.
*  And he's tried more often.
*  That's true of a lot of things in life.
*  You don't take enough big shots.
*  Like no one does.
*  Almost no one does.
*  Yeah.
*  If we learn nothing else from Peter Thiel, we should learn that a good long-term conspiracy
*  slash plan, depending on how you want to position it, sometimes can really pay off.
*  Yeah.
*  And I took my one big shot, right?
*  With falsa.
*  And it didn't really work as such, but it still had some positive side effects
*  and got me a lot of contacts and got me a lot of knowledge.
*  And we still have some assets that like, who knows what could happen.
*  And that Jones Act just might one day meet its end, you know?
*  Look, Trump, you have this great opportunity to just come out and kill this thing.
*  Let's kill this thing.
*  You need it.
*  You need it for your end explorer.
*  You want to, you want to like balance that budget or how you do it.
*  Anything else on your mind before we break?
*  Actually, I want to pitch to the Republican side is the Trumpian pitch for a bill with
*  Jones Act is I want to just get this out there so people can pass it along is reshore.
*  Like we want to, in this philosophy, produce things in America and sell them to America.
*  Right?
*  But to do that, you have to be able to transport the goods to produce the thing
*  and then transport them goods they were produced to the person who wants to consume them.
*  And if you don't do this, if you have a situation where it costs more to ship things within the
*  United States than shipping from outside the United States, that's like an anti-terror, right?
*  It's like a tariff on yourself.
*  You're going to get a tariff on America selling to America.
*  And obviously that is insanely terrible.
*  Right?
*  So we should remove that.
*  And yes, technically speaking, we are now going to buy ships from other people.
*  We're buying ships in the name of producing stuff in America.
*  And we're not appointed American production because America has produced the ships.
*  We're supplementing it.
*  And then we have more ships.
*  It's makes us greedy.
*  So it should be a flamboyant pitch.
*  It should be intuitive.
*  It should be obvious.
*  Remember all the unions love it.
*  We usually hate it.
*  Remember if it like all the blue states, all these people, you know what to do.
*  Just follow through and then do the Dredge Act, the Maritime Passengers Act.
*  I'll be happy.
*  All right.
*  I'm sure those influential administration members who are still with us almost three
*  hours in will be acting quickly.
*  Any other closing thoughts before we break or calls to action?
*  I mean, yeah, I think it's three hours in and I've covered a lot of things,
*  but decide what you think is the most valuable thing to do and do that.
*  Right.
*  And I'm not going to try and tell you what it is.
*  The rest is left as an exercise for the reader.
*  Svi Moshwets, thank you again for being part of the cognitive revolution.
*  All right.
*  You're very welcome.
*  It is both energizing and enlightening to hear why people listen and learn what they
*  value about the show.
*  So please don't hesitate to reach out via email at tcr at turpentine.co
*  or you can DM me on the social media platform of your choice.
