---
Date Generated: December 03, 2024
Transcription Model: whisper medium 20231117
Length: 7447s
Video Keywords: []
Video Views: 846
Video Rating: None
Video Description: In this episode of The Cognitive Revolution, Nathan explores the complex intersection of AI development and international relations with Robert Wright, publisher of the Nonzero Newsletter. They examine the growing militarization of AI, US-China relations, and the concerning trajectory of what Wright calls "the chip war." Drawing from Nathan's recent experience at The Curve conference and an AI wargame simulation, they discuss the risks of an AI arms race and search for alternative paths to avoid catastrophic conflict between global powers. Join us for this crucial conversation about the future of AI governance and international cooperation.

Subscribe to The NonZero Newsletter at https://nonzero.substack.com

Be notified early when Turpentine's drops new publication: https://www.turpentine.co/exclusiveaccess

RECOMMENDED PODCAST:
Unpack Pricing - Dive into the dark arts of SaaS pricing with Metronome CEO Scott Woody and tech leaders. Learn how strategic pricing drives explosive revenue growth in today's biggest companies like Snowflake, Cockroach Labs, Dropbox and more.
Apple: https://podcasts.apple.com/us/podcast/id1765716600
Spotify: https://open.spotify.com/show/38DK3W1Fq1xxQalhDSueFg

SPONSORS:
Notion: Notion offers powerful workflow and automation templates, perfect for streamlining processes and laying the groundwork for AI-driven automation. With Notion AI, you can search across thousands of documents from various platforms, generating highly relevant analysis and content tailored just for you - try it for free at https://notion.com/cognitiverevolution

80,000 Hours: 80,000 Hours offers free one-on-one career advising for Cognitive Revolution listeners aiming to tackle global challenges, especially in AI. They connect high-potential individuals with experts, opportunities, and personalized career plans to maximize positive impact. Apply for a free call at https://80000hours.org/cognitiverevolution to accelerate your career and contribute to solving pressing AI-related issues.

SelectQuote: Finding the right life insurance shouldn't be another task you put off. SelectQuote compares top-rated policies to get you the best coverage at the right price. Even in our AI-driven world, protecting your family's future remains essential. Get your personalized quote at https://selectquote.com/cognitive

Oracle Cloud Infrastructure (OCI): Oracle's next-generation cloud platform delivers blazing-fast AI and ML performance with 50% less for compute and 80% less for outbound networking compared to other cloud providers13. OCI powers industry leaders with secure infrastructure and application development capabilities. New U.S. customers can get their cloud bill cut in half by switching to OCI before December 31, 2024 at https://oracle.com/cognitive

CHAPTERS:
(00:00:00) About the Episode
(00:04:14) Introducing Nathan LeBenz
(00:04:58) AI Surrenders to Military
(00:07:03) Dario Amadei's Essay
(00:09:17) China Hawkism in AI
(00:11:04) The Upside of AI
(00:13:58) Decisive Strategic Advantage
(00:16:29) Getting Things Done in DC
(00:19:19) Sponsors: Notion | SelectQuote
(00:22:12) Subsidizing Chip Manufacturing 
(00:27:37) Export Controls on China
(00:31:19) AI Learning from Reality
(00:33:47) AI Hitting a Wall?
(00:36:17) Chinese AI Development
(00:38:24) Sponsors: Oracle Cloud Infrastructure (OCI) | 80,000 Hours
(00:41:04) Ilya Sutskever's Concerns
(00:46:05) A Common Challenge to All
(00:47:47) The Threat of Destabilization
(00:53:57) The Need for Benign AI
(00:59:54) AI with Western Values
(01:12:37) Finding Ways Around the Ban
(01:19:39) Joint Custody Island
(01:21:33) A Different Basis
(01:28:31) Domestic Political Incentives
(01:35:26) The Risk of Being Labeled
(01:43:38) Playing the Safety Teams
(01:53:12) The AGI Threshold
(01:57:01) Replicating Human Expertise
(02:01:13) Call for Ideas
(02:03:45) Outro

SOCIAL LINKS:
Website: https://www.cognitiverevolution.ai
Twitter (Podcast): https://x.com/cogrev_podcast
Twitter (Nathan): https://x.com/labenz
LinkedIn: https://www.linkedin.com/in/nathanlabenz/
Youtube: https://www.youtube.com/@CognitiveRevolutionPodcast
Apple: https://podcasts.apple.com/de/podcast/the-cognitive-revolution-ai-builders-researchers-and/id1669813431
Spotify: https://open.spotify.com/show/6yHyok3M3BjqzR0VB5MSyk
---

# Is an AI Arms Race Inevitable? with Robert Wright of Nonzero Newsletter & Podcast
**Cognitive Revolution:** [November 27, 2024](https://www.youtube.com/watch?v=u80eXplF2LE)
*  Hello, and welcome back to the Cognitive Revolution.
*  Today, I'm pleased to share a special crossover episode with Bob Wright, publisher of the
*  non-zero newsletter and podcast.
*  Bob recently wrote an important piece called AI Surrenders to the Military-Industrial
*  Complex about how leading AI developers, including Anthropic and Meta, are now beginning to partner
*  with the US military.
*  He shared the piece with me just before I attended The Curve, an immersive conference
*  hosted in Berkeley this past weekend which brought AI obsessives from academia, industry,
*  AI safety nonprofits, and the national security community together for presentations and conversations
*  on a wide range of AI topics, including the future of scaling laws, the latest findings
*  in alignment research, and growing challenges in international relations.
*  There, I had a chance to participate in a fascinating AI wargame exercise, organized
*  in part by recent guest Daniel Cocotello, and designed to explore how critical actors
*  might interact in the context of a fast AI takeoff in 2027.
*  As you'll hear, our simulation included a war between the US and China, despite the
*  fact that the AIs themselves were relatively benign.
*  With that experience in mind and drawing on Bob's expertise in foreign policy, in this
*  conversation we attempt to characterize the growing hawkishness toward China in AI policy
*  circles.
*  Explore why this stance has become so dominant, particularly among AI safety groups.
*  Assess the wisdom of US strategy in what Bob provocatively, but I think ultimately reasonably
*  calls the chip war.
*  Question whether the militarization of AI development is truly inevitable or wise.
*  Critique influential recent essays by Dario Amadei and Leopold Aschenbrenner.
*  And ultimately look beyond today's prevailing narratives to identify potential paths forward
*  could minimize risk of US-China conflict.
*  Along the way, Bob and I each make very clear that we are not fans of Xi, would not want
*  to live under the current Chinese government's censorship regime, absolutely disapprove of
*  many of China's policies, both domestically and internationally, do not want to be naive
*  in our approach to China, and candidly do not have great answers.
*  And yet we both feel that our time is well spent searching for a better plan than the
*  one we're currently hearing from both the incoming US president and major AI lab leaders,
*  which amounts to declaring an AI arms race and attempting to build sufficient AI powered
*  advantage to back China into a corner and ultimately force them to submit to American
*  hegemony.
*  This without a doubt is an extremely risky strategy and one that I fear will leave all
*  of humanity losers.
*  As always, we invite your feedback, particularly if you have ideas for how to build trust between
*  great powers, create novel mechanisms for AI governance, or otherwise nudge history
*  in a positive direction, please let us know, either via our website, cognitiverevolution.ai,
*  or by DMing me on your favorite social network.
*  I am not leaving Twitter, but I did just join Blue Sky, and my handle there is Nathan.LeBenz.
*  Finally before we begin, a quick note on format.
*  Bob is publishing the first half of this episode publicly and placing the second half behind
*  a paywall for his subscribers only.
*  We've obviously taken a different approach with the cognitive revolution relying on commercial
*  sponsorship to make all of our content open and free.
*  We're tremendously grateful to our sponsors, who clearly value a thinking audience and
*  have never attempted to influence our editorial direction.
*  Without them, we simply could not sustain our pace of two episodes per week.
*  That said, we have received some requests recently for an ad free version, and so I'm
*  wondering how much interest there would be in such an offer.
*  If you'd be interested in a paid ad free feed, please reach out and let us know.
*  If there is serious interest, we'll look into that possibility.
*  With that, I want to wish our American listeners a happy Thanksgiving, and to challenge all
*  of you to spend a little more time thinking about how you, as an individual, can work
*  toward a more peaceful, positive future.
*  Now, here's my conversation with Bob Wright on the trajectory of AI advances, US-China
*  and how we might avoid sleepwalking into catastrophic conflict.
*  Hi, Nathan.
*  Hi, Bob.
*  How are you doing?
*  You know, never a dull moment in the AI game, that's for sure.
*  Or the world generally.
*  And actually, we're going to talk about the intersection of the two.
*  So you know, I'm really looking forward to this.
*  Let me introduce this.
*  I'm Robert Wright, publisher of the Non Zero Newsletter.
*  This is the Non Zero Podcast.
*  You're a Nathan Labins, host of the highly regarded Cognitive Revolution Podcast, which
*  is one of my three go-to podcasts on AI.
*  It's been very valuable for me.
*  You're also founder of a company called Waymark that does video ads for small companies and
*  employs AI in that process to make it cost effective for the companies in question.
*  If you'll indulge me, give the background story to this conversation.
*  A couple of weeks ago, I published in my Non Zero Newsletter a story called AI Surrenders
*  to the Military Industrial Complex.
*  The news peg, as we used to say in journalism, was just that a couple of companies, Anthropic
*  and Meta, had kind of crossed the line.
*  They had decided that they were going to do business with the US military involving their
*  large language models and also involving Palantir, by the way, a well-known Silicon
*  Valley defense contractor that had focused more on predictive than generative AI traditionally,
*  but I'm sure it's open to all forms of Pentagon revenue.
*  I DM'd you about the piece.
*  I sent you the link, thought you might be interested in it.
*  I should say that the part of the story that I thought was most important actually was
*  not the news, but kind of why some of it surprised people and why they shouldn't have
*  been surprised.
*  I'm thinking in particular about Anthropic.
*  As you know, Anthropic has a reputation for being particularly safety-conscious, concerned
*  about harms done by AI, and of course the military is in the business of harming people.
*  I don't say that pejoratively.
*  I grew up in a military family.
*  I understand why countries need militaries.
*  I actually have a lot of respect for the military as an institution, but I think this is one
*  reason some people were surprised.
*  I think they had maybe inferred from AI's safety consciousness that it was just kind
*  of generally dovish or generally leftish or something.
*  Anyway, there was surprise.
*  Part of the point of my piece was they shouldn't have been surprised if they had been paying
*  attention because Dario, can you help me pronounce his last name?
*  I call him Dario as if I knew him, just so that I'll know I'm pronouncing it right.
*  How does he pronounce his last name?
*  I believe it's Amadei.
*  Amadei, who is a highly regarded figure in AI, not just because he found an Anthropic,
*  which produces a great LLM called Claude, which I use, but because of his past role
*  at OpenAI in developing the whole GPT technology.
*  He's very highly regarded.
*  He had published this essay a couple of months ago now, I guess, called Machines of Loving
*  Grace, 15,000 words.
*  A lot of it was just laying out possible positive scenarios about the future use of AI.
*  He had previously, I think, been thought of by some as kind of a negative guy, so concerned
*  about risk.
*  So I think maybe he wanted to get out there, the fact that he's not blind to all the possible
*  upsides if we avoid the catastrophic risks.
*  But a part of the essay that didn't get nearly as much attention, I'm sure it got some in
*  your circles, but for example, Lex Friedman did a 7,000 hour conversation with him.
*  Dario didn't bring it up, even though it focused on the essay.
*  And that was a part where what we would call in journalism the actual story from that document,
*  here's the news, where he lays out, I think more than he ever did, his foreign policy
*  ideology as it applies to AI.
*  And I would say, broadly speaking, he first of all signed on to a framing of US foreign
*  policy as common in the foreign policy establishment, certainly in the Biden administration, which
*  is that the US and other democracies are involved in an existential war with authoritarian or
*  autocratic countries.
*  I'm down on that framing myself, I should say, partly because I think it becomes a self-fulfilling
*  prophecy.
*  But in any event, he was very straightforward about that.
*  I'm not sure he mentioned China at all, but there's no doubt that he was talking about
*  China and that he embraces the chip war that we have launched against China, which for
*  reasons I will get around to explaining, I think is a bad idea.
*  So anyway, I DM'd you.
*  You replied with a link to some thoughts you had been developing on Google Doc.
*  And there was some convergence of our views.
*  I'll leave it to you to characterize your views.
*  But it's safe to say that neither of us is China hawks.
*  And I think both of us worry a little about how rapidly China hawkism has become ascendant,
*  not just in foreign policy circles, not just among national security types who are just
*  learning about AI, but among a lot of AI types, including some who are just learning about
*  national security.
*  And so I wanted to have you on to talk about this whole thing.
*  It turns out you have in the meanwhile gone to this AI safety conference at Berkeley where
*  you participated in a war game, a futuristic one.
*  I guess the premise is that there's a GI.
*  Apparently they didn't think highly enough of you for you to play the AGI, Nathan.
*  But they found someone smarter.
*  But that must I want to talk about that.
*  But let me start out by asking you, you know, you report in the Google Doc recording your
*  thoughts on the Berkeley conference that it's clear that China hawkism is dominant
*  now.
*  And I would like to start out by asking whether you have an explanation for like the dynamics
*  behind that.
*  I mean, it was an interesting question to me two years ago before I was even thinking
*  about AI because it was starting it had started to happen in foreign policy circles.
*  What are your thoughts on on the state of play there and how this came to be?
*  And if you have theories about why what those are?
*  That's a great question.
*  I don't know if I have a great theory.
*  I guess a couple of me back up for one second and then I'll attempt it.
*  First of all, I would recommend everyone read the Dario essay and I might even do a full
*  audio presentation of it on our feed in the not too distant future, because I think it
*  really is kind of a study in contrast between the first day.
*  It is a pretty long essay.
*  As you said, the first half is one of the best, if not maybe the best recent articulation
*  of what is the upside of AI?
*  Why should we be excited about this?
*  Why should we not just pause AI now or, you know, to be even more vivid, strangle it in
*  its crib as Tucker Carlson once suggested.
*  And, you know, there's lots of good reasons and he goes quite in depth.
*  He's got a background in biology and biophysics and goes into his model of how biology and
*  medicine improve and why we should expect that if AI gets good enough, we're going to get
*  rapidly more discoveries.
*  And he thinks that it's plausible that we could have what he calls a compressed century
*  where we get basically 100 years worth of biological and medical progress, not to mention
*  economic benefits and equality globally within just like a five to 10 year time frame.
*  And I've honestly been crying for one of the AI lab leaders to say something like this,
*  give us kind of something to hold on to for positive vision.
*  So I really appreciated that.
*  Notably that section is a lot longer than the geopolitics section, which I do think
*  kind of reflects the fact that the at least the AI community is somewhat coming around
*  to this late.
*  And as much as I do think Dario is a great thinker, he was, as you mentioned, one of
*  the lead authors on the GPT-3 paper, one of the people along with Ilya, who is credited
*  as having understood the scaling dynamics very early when it was not at all the consensus
*  view.
*  It did have a little bit of a feel to me of kind of tech guy, you know, thinks there's
*  a simple solution to things that might not actually have simple solutions.
*  And just the brevity and the sort of, you know, I don't know, blithe kind of characterization
*  of, OK, all these democracies will work together, will have this advantage, AI is going to give
*  us this advantage, will box everybody else, will isolate all of our adversaries.
*  And then eventually they'll have no choice but to recognize that they should come onto
*  our side and take the deal, whatever the deal is.
*  It's like unclear what the deal is.
*  Does the Chinese regime survive this deal or not survive this deal?
*  And we should not specify.
*  Clear. The deal happens when when China kind of agrees to whatever conditions we lay down
*  for relaxing the chip war and any other restrictions on what they can import technologically.
*  So it's meant to be an offer they can't refuse.
*  You know, right. Say very much.
*  And the phrase decisive strategic advantage is increasingly being bandied about.
*  That's like the idea that by advancing our own AI capabilities and through sort of a
*  combination of means, holding theirs down, we'll get to this point where it's like, OK,
*  look, we can definitely beat you in a war if it comes to that.
*  Therefore, you have basically no choice but to accept the deal, you know, whatever the
*  terms of that deal ultimately are.
*  And those are not specified by anyone, really, as far as I know.
*  This is all projected to happen in the not too distant future.
*  Dario's timelines for AGI or he prefers the term powerful AI.
*  Are like two to four years, you know, he's like 27 is kind of the 28 maybe, but 27, I
*  think, is kind of his most likely year for when this stuff really starts to happen.
*  That was, by the way, the year that we played in the war game simulation over the weekend.
*  And, you know, that means it's probably current leadership that's intended to, you
*  know, that's imagined to be the ones doing this, right?
*  It's on our side.
*  I'm feeling better already.
*  Yeah. So we've got Trump and Xi and these are the two, you know, relatively singular
*  figures on both sides that this scenario flows through in a really core way.
*  So, yeah, I think that is concerning.
*  I don't I don't like the plan very much either.
*  There isn't much of an alternative that I can find, and I don't feel like I have the
*  answers as to why I have one, but we'll get around.
*  Yeah. Well, I want to hear it.
*  It's vague, unfortunately, but it's tough as V.
*  Moshwood's recently said in a conversation I had with him, all the options are bad.
*  So I think it is tough.
*  I don't I don't want to pretend that there are easy solutions and I don't want to
*  suggest that we should just be totally naive with respect to the Chinese government.
*  I don't think that's a recipe for success either.
*  But, you know, I really don't know how this happened.
*  I'm certainly not a scholar of, you know, the blob, the foreign policy establishment.
*  All that kind of stuff is a little bit outside my wheelhouse.
*  I will handle the vehement denunciations of the foreign policy establishment.
*  You don't have to worry about that.
*  Go ahead. One theory I've heard is that.
*  It's the only way to get things done in Washington and in this moment, you know,
*  obviously we've got polarization and the parties are sort of.
*  Inclined to block each other and try to prevent the other side from getting wins
*  wherever possible. One notable change is that that dynamic may have been put aside
*  for the moment where the Republicans are going to be able to do what they want to
*  do, it seems, especially if they're willing to break a few more norms, which
*  history recent history suggests that they probably will be willing to do if needed.
*  But up until this last election, there's been enough kind of a stalemate that.
*  How else do you get anything done?
*  You know, the sense has been from what I gather, you know, again, not spending
*  any time in D.C. and certainly not being a D.C. insider.
*  The sense has been we can't let the other side have a win, but we can.
*  At least we can both kind of agree that we should do stuff to get an edge on China.
*  So there's been willingness to put money into that and try to bring.
*  Can you elaborate on that?
*  Well, you're saying there's certain parts of the policy agenda that aren't
*  necessarily central to the China question that they want to get some leverage on
*  in Washington and by signing on to the China consensus, that helps or what?
*  And if so, what are those policy items?
*  Yeah, I mean, again, take this as a major grain of salt because I'm definitely kind
*  of relaying what I've heard, not what I've experienced.
*  But the idea is basically if you want to get something going in Washington,
*  you got to find a way to frame it as a way to get ahead vis a vis China.
*  And, you know, our bridges are falling down, you know, our roads are falling apart.
*  China's got better stuff. We can't let that happen.
*  You know, and then you kind of play that out across everything.
*  In some cases, I do think for the better, you know, we obviously are dependent on
*  international supply chains.
*  And I sort of I have mixed feelings about this because I do think
*  broad decoupling from China makes future conflict easier and more likely.
*  And I don't like that.
*  So I actually do want to see the continuation of economic coupling with China.
*  At the same time, I think like it's only somewhat prudent to say
*  the fact that we can't make any advanced chips domestically is like
*  a state not really to be endured.
*  So when we put some subsidies into bringing chip manufacturing
*  back into the United States, physical territory, I think that's like,
*  you know, that at least shows that somebody has some lights on in Washington.
*  And I think that's good.
*  But there's a lot of other things where, you know, it's my senses,
*  you know, impression from afar is that people feel like
*  the anti China train is the one train that's moving.
*  And if you want to, you know, get your thing moving,
*  you kind of have to figure out a way to get it on that train.
*  And everything else is kind of stuck.
*  Hey, we'll continue our interview in a moment after a word from our sponsors.
*  As a cognitive revolution listener, you're obviously interested
*  in cutting edge AI technology.
*  And with that in mind, I'm proud to say that this episode is brought to you
*  in part by Notion.
*  Notion has been a clear leader in high value applications of generative
*  AI since the wave began earlier this year.
*  We had Notion AI engineer Linus Lee on the podcast.
*  The quality of his insights showcase the caliber of talent that Notion employs.
*  And that inside look at how Notion builds with AI is still extremely valuable.
*  Given my personal focus on AI automation recently,
*  I specifically wanted to highlight Notion's library of workflow
*  and automation templates.
*  If you're looking to streamline your processes and lay the foundation
*  for future AI driven automation, these templates are an excellent starting point.
*  And even if you're not ready for full automation, you'll benefit immediately
*  from Notion AI, Notion's latest all in one AI implementation
*  that searches through thousands of documents, regardless of whether they live
*  in Notion or on some other platform like Slack or Google Docs
*  to deeply understand the context of your work and generate highly relevant
*  analysis and content just for you.
*  Notion is used by more than half of Fortune 500 companies,
*  helping teams reduce emails, meetings and time spent searching for information.
*  Want to try it? Head to Notion.com slash Cognitive Revolution.
*  You can start for free and using our link supports the show.
*  So join me in giving Notion AI a shot today at Notion.com
*  slash Cognitive Revolution.
*  There are so many things in life we just never get around to.
*  Taking up that hobby, cleaning out the garage, you know, little things
*  that don't really make huge differences in our lives.
*  Yet there's one thing that most of us have probably been neglecting
*  that can have a huge impact on our family's future.
*  It's life insurance.
*  And with select quote, getting covered with the right policy for you
*  is easier and more affordable than you might think.
*  As someone who tracks AI progress on a full time basis
*  and obsesses about its potential impact nonstop,
*  I know how tempting it can be to ignore more mundane, familiar risks.
*  There's always another paper to read, podcast to listen to or product to try.
*  And yet the smartest people that I know in the AI space
*  continue to save and invest money for the future, carve out time
*  for their relationships, maintain their physical and mental health.
*  And yes, protect their family with life insurance,
*  just in case anything should happen before the singularity.
*  If nothing else, it's one less thing to worry about in a time of unprecedented change.
*  So get the right life insurance for you for less at select quote.com slash cognitive.
*  Go to select quote.com slash cognitive today to get started.
*  That's select quote dot com slash cognitive.
*  Well, I will say in support of that thesis,
*  kind of, is that, you know, there's an earlier landmark before the Dario essay
*  here, and it's the Leopold Ashenbrenner essay, Situational Awareness.
*  And that wound up and we can talk about that, but it's the same.
*  Well, I should say, I mean, here I should flesh out what I think is the scenario
*  that's common in these circles.
*  And you can tell me if it's wrong by these circles.
*  I mean, the AI side of the China hawkism, not the traditional
*  foreign policy side necessarily.
*  But you saw this in the Ashenbrenner essay.
*  I think you see it in the Dario essay.
*  And I mean, first of all, let me say the Ashenbrenner essay was retweeted
*  by no less a person than Ivanka Trump.
*  So, yeah, it got in the conversation.
*  I don't know how many Twitter followers she had, but that was noted.
*  Right. And I mean, separate from that, Trump, you know, a number of people
*  in Silicon Valley had Trump's ear by virtue of donations and so on.
*  But anyway, the scenario in that essay is that once you get to this,
*  sometimes vaguely defined thing called artificial general intelligence,
*  you're approaching a kind of takeoff point in the evolution of artificial intelligence.
*  And suddenly you're in super intelligence realm.
*  And that gives you such and things move so fast that if you get to this threshold
*  first, then like, you know, a few months later, you have utter dominance over all rivals.
*  And we're not just we're not talking mainly about weaponry here, as I understand it,
*  although it may involve that.
*  As I understand it, they're saying, like, you can just say to the AI, go find a way
*  to infiltrate China's social media and convince them that their government sucks.
*  Right. I mean, I think people are imagining a future where the AI is that smart
*  and responds to directives that broad.
*  And I just thought that up myself.
*  I haven't seen it, but I assume that's the kind of thing they're talking about.
*  And so the Ashenbrenner essay and I hope we'll have time to get around
*  to saying some things about that, because like the Dario essay, I mean, Dario essay
*  in its defense, that part of it was short.
*  He didn't have time to do a whole argument.
*  The Ashenbrenner argument, the whole essay is about, in effect, why we should be China.
*  And it's not short.
*  It's not short. And yet I see fundamental assumptions behind that not being addressed
*  or even stated. And I hope we'll get around to that.
*  But tell me, is this is one reason the China hawkism is caught on the AI circles
*  is because among a lot of people, that view was already, you know, instantiated.
*  The idea that, yeah, there will be this threshold.
*  Beyond that, you get an intelligence explosion.
*  Obviously, whoever gets there first is going to be in charge.
*  That's kind of widely assumed in certain circles.
*  I would say everything is pretty highly uncertain.
*  The event that you alluded to that I where this wargame simulation took place was
*  designed to be kind of a mixing bowl, like a meeting of different camps or tribes.
*  So the AI safety tribe, if you will, was one contingent.
*  But there was definitely also a sort of national security persona that was there.
*  And there were people on definitely, you know, I joked earlier today, very fine
*  people on both sides of important questions like just how good or powerful,
*  I should say, is AI going to get how quickly, how if it does get really powerful,
*  how likely are we going to be to control it?
*  And then, you know, downstream of that, like what should we do with it?
*  So I wouldn't say there was any.
*  Single worldview that dominated, but as Leopold said in his original
*  tweet with the situation awareness thing, it's not that people necessarily
*  think it's for sure going to happen, but everybody agrees it's kind of strikingly plausible.
*  You know, we look back at just the last few years where I remember reading about GPT-2
*  when my now five-year-old son was just about to be born.
*  I was at the hospital. My wife was getting checked in.
*  I was in the waiting room and just kind of nervously scrolling.
*  And there's GPT-2. This thing could not do much of anything.
*  It could barely put together a coherent paragraph.
*  It certainly wasn't useful.
*  And now we have expert level reasoning across, you know, all domains of knowledge,
*  at least for routine tasks.
*  We've talked about that in the past, like the difference between routine tasks and,
*  you know, truly novel work.
*  And the AIs aren't quite doing that yet.
*  But everybody's kind of looking at the trend and saying this really can't continue
*  much longer without crossing some really important thresholds.
*  And so everybody at least takes that possibility seriously.
*  I wouldn't say anybody at the event was dismissing that possibility.
*  Some feel like it's overwhelmingly likely.
*  Others feel like it's unlikely but can't be dismissed.
*  And that range of opinions still, you know, kind of forces everybody to think,
*  what do we do if that is in fact the scenario?
*  I still don't know why we jump, you know, in the AI, especially in the more
*  AI safety community. And again, I don't have like polling data to back this up.
*  But my subjective impression is that most people across these personality types,
*  and there's a lot of disagreement about a lot of things, but it seemed to me that
*  pretty significant, I would even say large majorities of both the AI safety and
*  the sort of national security, you know, different that range of personas,
*  it seemed like a large majority across the board seemed to support the export
*  controls, aka chip ban or chip wars, you've called it against China.
*  And I don't really know why that immediately follows.
*  It does seem to me like a pretty narrow, especially in the time available, right?
*  I mean, the scenario that we played, I think this is even more speculative.
*  I think most people probably at that event expect that AI will continue to progress.
*  We will probably see things like what it would sometimes call the drop in
*  knowledge worker, you know, that an AI can kind of take over a computer and do
*  useful work like, you know, most people who work at a computer do on a daily basis
*  throughout the economy. That level of AI is, I would say, pretty broadly expected.
*  There's a lot less agreement as to when you get there, how fast does that turn into
*  super intelligence? And, you know, a lot of a lot of arguments, a lot of kind of,
*  well, geez, when the AIs can write AI, you know, optimization code and when they can
*  explore new architectures for themselves as well, or maybe a little better than we
*  can do that and a lot faster.
*  Because, of course, you know, if you have one AI researcher, you have potentially a
*  million copies of an AI researcher that can explore the space of AI architectures all
*  in parallel. Yeah.
*  You know, we should say we should say probably for kind of laypeople, which is a lot
*  of people in my podcast audience.
*  The assumption here, you know, the term singularity, which you may have heard,
*  refers sometimes to technological acceleration generally, but in particular to the point
*  where the AI starts improving itself in a certain sense, it already is.
*  It's facilitating coding. It's making coding more efficient.
*  But but, you know, as that gets more as the feedback cycle gets more and more direct,
*  like the AI just saying, I'll create a better version of myself.
*  This is the point of takeoff people are referring to.
*  Anyway, go ahead. Yeah.
*  And how fast that happens or if it happens, I would say, again, a pretty wide range of
*  opinions. You have some folks who think that a super intelligence follows very quickly
*  after a general intelligence because of these sort of self-optimization loops.
*  Others and I would put Dario in this camp based on his essay would say, well, yes,
*  progress will speed up, but it won't be like a super sudden jump because you still got
*  to run a lot of experiments.
*  He talks a lot about the biology experiments, but even the compute experiments, you know,
*  that we do have some limitations on how many AI training runs you can run, even if you
*  have an AI coming up with a ton of ideas, it is still somewhat of an empirical science as to
*  which of those new architectures are going to work better or worse.
*  We don't really know until we run training.
*  And for training, you need servers, you need GPUs.
*  We only have so many of those. So maybe that sort of puts at least a governing limit
*  on how fast things could go.
*  And then other people think, well, we're just going to hit some sort of a plateau because
*  I think this is a little bit fuzzy, but like all we have is human level data.
*  What data are they going to learn from next?
*  I don't tend to think that's the case because I tend to think there's a lot of.
*  Sort of just feedback from reality that they can start to get.
*  You know, the path of training the AIs has it's kind of looping back to like an earlier era
*  where if you go back to sort of mid teens breakthroughs in AI, a lot of it was game playing
*  AI for different kinds of games.
*  You know, AlphaGo was, of course, a major breakthrough.
*  But there were a bunch, right? There was an AI that could play Starcraft.
*  There was an AI that could play, you know, all these kind of different games.
*  There was one AI that could play lots of different games.
*  And sometimes they didn't even have to teach the AI the rules of the game, meaning like
*  literally just put it in an environment, let it play.
*  It would kind of figure out the rules.
*  It would become superhuman at all these games.
*  And it did that by getting feedback from reality.
*  Not by training on Internet data, not by training even necessarily on human games played,
*  not even by training on human feedback, but literally just did I win the game?
*  Did I lose the game? If I won the game, then I should do more like that.
*  If I lost the game, you know, that wasn't so good.
*  And that signal alone was enough to get these things to superhuman playing levels.
*  Since then, the paradigm has shifted where it's like all the GPT's, you know, they learn from us
*  in a very literal sense, both from reading our Internet writings and from reinforcement
*  learning from human feedback, where literally a human is like, I like this response more than this
*  response or, you know, this seems better than this.
*  But now we're also kind of starting to see I think this is under well underway, but, you know,
*  the future seems to be learning from actual contact with reality, predicting
*  what experiment is going to be worth running, actually running the experiment, getting the
*  result of that and then tweaking and optimizing to be better at figuring out what experiments to run
*  or code. Right. That's another one where when you generate code, you can run the code very quickly
*  often. If it errors, you'll see that instantly.
*  Right. If it's if it works and it produces what you expect it to produce, you can see that instantly, too.
*  So these signals are becoming, I think.
*  Or I expect that all these different signals are probably enough to take the current crop of GPT's
*  into like superhuman coding, superhuman experimental design, superhuman hypothesis generation,
*  just like they've previously achieved superhuman game playing across a wide range of games.
*  And so I kind of find it hard to imagine that they don't become superhuman.
*  But again, going back to the range of opinion, not everybody buys that.
*  Yeah. And can I add, I mean, kind of inject a question, we'll get back to China question per se.
*  But, you know, the context for this is partly just over the last few weeks, there's been a lot of publicity
*  about is AI hitting a wall. In other words, are the scaling laws reaching a point of diminishing
*  return and scaling laws refer both to like amount of text you feed into it.
*  And as you just mentioned, there's only so much human generated text.
*  You can generate it with AI and try that, blah, blah, blah.
*  There's issues. And then and then computing power is another place where, well, will scaling per se
*  continue to bring traditional rewards?
*  There are questions about that.
*  I personally, although I know a lot about this in you, I agree that there are just so many other dimensions
*  along which progress can happen.
*  I mean, OpenAI released this so-called chain of thought model.
*  And by the way, I mean, speaking of game playing, I kind of wonder if we shouldn't spend much time on this at all.
*  But I kind of wonder if one way of depicting what's going on in there is the machine has a conversation with itself.
*  It's as if there were two LLMs in there.
*  And so like, hey, what about this?
*  And then the other part of the machine's self, which might as well be another LLM says, I don't know, that doesn't make sense for this, blah, blah, blah.
*  Is there a certain amount of just just quickly?
*  Is that is there a certain amount of that kind of internal dialogue that has a little bit in common with the kind of self play that led to progress in the realm of games or is this a nutty intuition?
*  I think it's directionally right.
*  I mean, your description of the chain of thought, I think, is pretty apt, notably OpenAI is not sharing that chain of thought with users.
*  Although there's an open source Chinese model that says they do it.
*  Yeah, and I have gone and used that.
*  And I think it's a striking fast follow only like nine weeks, I think, from the time that the O1 model was announced and given to the public to the time that the Chinese company DeepSeek came out with their deep think.
*  And actually, I think there's another one to another reproduction out of China that's like, I think, are you suspicious?
*  Are you suspicious of how they I mean, I assume OpenAI is wondering whether they should tighten their security?
*  Yeah, I'm not. I. Geez, that's a hard one.
*  My best guess is that.
*  I think China, you know, Chinese researchers in general, there's a lot of them.
*  They're very smart. There's a lot of good research coming out of China.
*  For years and.
*  I, you know, I hazard, it would be hard to guess.
*  I think you can't rule out that there was some intellectual property theft, but I also would not dismiss at all the possibility, which I guess in my gut is probably the more likely scenario that they're just good, you know, that they really just have good researchers who are not far behind.
*  And.
*  You know, probably we're already working on this path.
*  It's the timelines are very unclear because we heard.
*  Roughly a year ago now, you know, this was a year ago.
*  We're coming up on the I think we actually just passed the anniversary of the Sam Altman board coup and.
*  You know, what did you see right?
*  All this sort of and the rumor at the time, which I think is increasingly like the consensus understanding of what happened was that there was some breakthrough in reasoning.
*  And people started to feel, including Ilja Sutskiver.
*  Hopefully I'm saying that correctly, who was chief scientist and kind of, you know, one of the leading visionaries and one of the big believers in scaling laws along with Dario for years.
*  You know, he for a moment kind of flipped and said, we need to take a different approach and Sam's not the right guy to lead this company because we're really starting to get close to some sort of, you know, transformative AI.
*  It wasn't still this is taken seriously within AI circles because I've seen the what did you see meme and I just thought it just sheer conjecture.
*  Who knows? It's it's an obviously appealing scenario.
*  It's the kind of thing that would spread virally.
*  But is this this is taken seriously among because I had heard a whole different story that I won't bother you with.
*  But this is taken seriously.
*  Hey, we'll continue our interview in a moment after we're from our sponsors.
*  Even if you think it's a bit overhyped, AI is suddenly everywhere from self-driving cars to molecular medicine to business efficiency.
*  If it's not in your industry yet, it's coming and fast.
*  But AI needs a lot of speed and computing power.
*  So how do you compete without costs spiraling out of control?
*  Time to upgrade to the next generation of the cloud, Oracle Cloud Infrastructure or OCI.
*  OCI is a blazing fast and secure platform for your infrastructure, database, application development, plus all your AI and machine learning workloads.
*  OCI costs 50 percent less for compute and 80 percent less for networking.
*  So you're saving a pile of money.
*  Thousands of businesses have already upgraded to OCI, including MGM resorts, specialized bikes and fireworks AI.
*  Right now, Oracle is offering to cut your current cloud bill in half if you move to OCI for new U.S. customers with minimum financial commitment.
*  Offer ends 12-31-24.
*  So see if your company qualifies for this special offer at oracle.com slash cognitive.
*  That's oracle.com slash cognitive.
*  I am really excited that our new sponsor, 80,000 Hours, is now offering free one-on-one career advising sessions to cognitive revolution listeners.
*  80,000 Hours aims to be the best source of advice for people who want to do the most good that they possibly can with their careers.
*  We typically work for about 40 years in our lifetime and we work about 2,000 hours per year.
*  That is the single biggest opportunity that most of us have to make a positive contribution and it's worth being strategic about it.
*  That's where 80,000 Hours can help.
*  I actually used their career advising service myself.
*  Two years ago, I had just finished the GPT-4 Red Teaming project and I wanted to do anything I could to nudge the AI future in a positive direction.
*  But what could or should I do?
*  That was not clear.
*  After my call with 80,000 Hours, I got a number of connections to outstanding individuals in the space.
*  And over the course of the follow on conversations, I developed confidence that this podcast was one of the projects that I should pursue.
*  Today, I'm thrilled to have built an audience of thoughtful, high potential people that 80,000 Hours wants to help.
*  To request a free one-on-one career advising session, follow the link in the show notes.
*  It's 80,000hours.org slash cognitiverevolution.
*  That's 80,000hours.org slash cognitiverevolution.
*  Sign up for a free one-on-one career advising session.
*  Figure out how you can make a positive impact on the AI future.
*  And I think you'll be glad that you did.
*  Yeah, I mean, I think the details, of course, are quite fuzzy.
*  But my general sense is there was an early demonstration of some new reasoning capability and they had published about this research.
*  There was an early kind of demonstration where they were starting to do.
*  What is the signal from which the AI learns?
*  Starting to do.
*  What is the signal from which the AI learns is one really key part to the overall story of where the progress is coming from.
*  They had put out a paper in 2023 about rewarding reasoning.
*  I forget exactly what the name of that paper was, but it was sort of instead of just rewarding on the final answer, looking at each logical step that the AI was taking to try to come up with an answer and giving reward at each one of these steps.
*  So there was already reason to believe that that sort of thing was happening.
*  Then there's a guy that had previously been at Metta who had done incredible work on some of these game playing reinforcement systems.
*  He had been responsible for the AIs that beat humans at no limit Holden poker and also at diplomacy, which was one of those moments where for anyone who hasn't played the game of diplomacy, and I'm not a big diplomacy player by any means, it's a game of social dynamics.
*  It's like a European theater and all the countries are trying to jockey for power, but it's not like rolling the dice.
*  Instead, it's about who believes who and who betrays who, and they got an AI to play that at a very competitive level.
*  So this guy went over from Metta to OpenAI to work on reasoning.
*  So there's all these reasons to believe that there was reasoning happening.
*  So my intuition does have a little something to be said for it.
*  If a game playing guy went over and you think that helped the reasoning project.
*  But yeah, I mean, he's been pretty open that that's what he's working on.
*  So I think there was some early demonstration.
*  And I think Ilya, my theory, again, these details could be wrong, but I think the generally accepted story is not so much because this thing totally changes worldview, but more like, hey, it's really starting to get real now.
*  And there's also been these leaked emails that have since come out just recently where because of, I guess, court filings and discovery emails going back to early OpenAI days when Elon was still there and Sam and Greg and Ilya, all these guys were going back and forth on,
*  can this be a nonprofit?
*  What we're going to need so much capital?
*  What should we do?
*  There was already not a super high level of trust, even dating back years.
*  And this is in plain text now in these emails where people are saying, Sam, we don't really feel like we understand you and we need to understand you better if we're going to go on this long journey with you.
*  So I think some of that was already latent.
*  It started to feel like, hey, this is getting really real.
*  Ilya kind of freaked out, perhaps rightly, perhaps ultimately futilely, it seems.
*  And then it took them a full year to productize and Red Team, you know, because they did do a lot of advanced safety testing on this.
*  I had two episodes with two different external groups that had early access to the new A1 model and ran all kinds of tests on it.
*  And it wasn't just those two groups, but those are the two that I talked to when it came out.
*  Yeah, I remember that. That was a good podcast.
*  I recommend it.
*  Circling back with all those signals in mind, right?
*  It would be very surprising if there weren't groups in China kind of saying, yeah, what's that?
*  What's down that path?
*  Right. Sounds like there's something there.
*  We should be working on that, too.
*  And yeah, it's possible that somebody stole the weights or the secrets from OpenAI.
*  But I think there was enough of a, you know, existence proofs are really powerful in AI.
*  And when somebody sees that something is possible, it's a really, you know, it's really powerful signal and kind of invitation to figure out how it happened.
*  And even if this wasn't fully an existence proof in the public view, there was enough kind of consensus understanding that this is kind of what the next big thing is, that I have no doubt they were working on it before,
*  possibly when they saw the limited demonstration and the limited examples of the chain of thought that OpenAI published, they got a better idea and were able to kind of lock in, you know, this is the exact strategy that we want to use.
*  Notably, I think their thing is still not as good.
*  The O1 model from OpenAI, as far as I understand, on the benchmarks is still a cut above.
*  I would also bet that it is outside of the benchmarks, probably even more of a cut above.
*  I do think OpenAI does generally a really good job on kind of not just looking at these like very discreet tests, but also, you know, taking a holistic approach.
*  Yeah, there's a tendency to teach to the test.
*  I mean, there's an incentive to teach to the test when you're when you're training your models.
*  And you're saying OpenAI doesn't seem to do a whole lot of that.
*  Can I the the the Ilya, the Ilya Setskova, an alternative pronunciation that may also be wrong.
*  He gives us a good chance to get back into the, you know, the kind of China Hawk conversation.
*  He said something a couple of years ago.
*  I wish I had the quote in front of me, but it was to the effect of my hope is that the world's peoples or the world's nations or or something will recognize that this technology.
*  And I don't know if you use the word threat.
*  He obviously sees a great upside to it.
*  But the idea was my hope is that they will understand with sufficient clarity that it imposes a common challenge to all of them, that it is in their interest to get together and confront it and govern it together.
*  He may he may not have used those words at all, but that's very much in the spirit of what he said.
*  And. One thing that bothers me about the China Hawk discourse is I don't see many people bothering to stop and assess whether that belief is true, right?
*  Because if it is right, if in other words, they don't they don't look at the costs of of this, like manic AI arms race with China, which just fundamentally.
*  Affects the psychology of the relationship, radically reduces transparency in certain respects and just generates this kind of chaotic environment where people are going to feel more compelled to get model to create models, perhaps without adequate safety tests.
*  It just seems to be pretty obvious that if you take certain kinds of threats seriously and this whole range of threats you can take.
*  And by the way, I think not enough attention is paid to the threat of sheer destabilization.
*  In other words, even if the sci fi doomer scenarios aren't valid and I don't personally rule them out, but even if they're not, if you look at all the economic and social effects that AI is already starting to have and many more of which will materialize.
*  It's obviously going to be disruptive, not only in the good sense of the word, it's going to be destabilizing.
*  And when you sign on to what you're describing as an existential arms race, you're saying let's hasten that.
*  Let's accelerate the destabilization of, I would say, American society.
*  And by the way, one surprise for people may be that China, because of its authoritarian character, actually has more of an interest in regulating the AI and not proceeding too fast and winds up as a more stable society than the US and beats us that way.
*  That to me is something I'd put in parentheses.
*  It's not like my big concern, but I would say that there are a lot of possible downsides to a big AI arms race with China.
*  And I just don't see these people exploring them.
*  And it's and let me I'll close.
*  I'll stop and let you talk.
*  But I want to I want to focus on a very specific thing that's been kind of driving me a little crazy or the lack of discussion of it has.
*  And I want to ask you whether in AI circles, this is commonly discussed, because it's one of the most straightforward downsides of the Biden chip war, which, you know, if people don't know by now, consists of
*  not just prohibiting the import of from American companies or certain kinds of things to China, but using American leverage to, you know, to get other Western makers of both chips and equipment to not send, you know, to deprive China, both of the of the high performance chips that they were getting not long ago and of the equipment needed to make them.
*  So here's this scenario where I think this is obviously a dangerous policy.
*  And I think you've I've seen you allude to a version of it, but I want to be clear about the version I'm talking about.
*  So as of a few years ago, both China and the and the West were getting high performance, AI level chips out of the factories in Taiwan that are run by TSMC.
*  And that, when you think about it, was a disincentive for China to invade whatever other incentives they may have.
*  You know, it's pretty clear that if they invade Taiwan, those factories are going to be incapacitated in one way or another.
*  Probably if the US loses the war, they're going to bomb them on the way out.
*  But if not, because of their ongoing reliance on like the Dutch company that makes the most high performance manufacturing equipment, you can in effect pull the plug remotely by ceasing to provide support.
*  So, you know, either the US like wins the war and China wants to anyway.
*  The point is, it was kind of a mutually assured destruction thing.
*  War in Taiwan means one way or another, one side is going to just as soon see the factories incapacitated.
*  So war means both sides lose their access to the high performance AI chips.
*  That's what we had as of a few years ago, a kind of mutually assured destruction, a reason neither country wants war.
*  Now, we have created a situation where China is no longer getting the high performance chips.
*  So whatever disincentive existed for China to invade, that has definitely been lessened.
*  You know, and I'm not I want to distinguish this from this scenario where China just says, well, screw it, we're going to blow up these factories.
*  You know, that could happen, too.
*  But what I'm talking about is something I think more plausible where China, which obviously has this set of reasons, it wants to bring Taiwan back into its country, because it, of course, considers it a renegade province.
*  And in fact, it's not even recognized by us as a sovereign nation.
*  So I'm talking about a subtler dynamic where China is just doing these calculations every year and they're like, well, the US is sending more weapons.
*  It's not going to get any easier.
*  And by the way, we care a lot less now whether this factory gets blown up.
*  And because the chip war has gotten us to accelerate the development of our indigenous chip making capacity, we may do better pretty soon than the West does in terms of hanging on to like the next level of chips like the like smartphone chips.
*  So this is so clear cut.
*  It's just game theoretic.
*  Like, you can't argue with the game theory here.
*  And yet I have had people people do it on on my podcast.
*  But I finally found somebody who had actually written about it named Paul Triolo.
*  I'd encourage you to have him on the podcast.
*  He's a he's a real expert on Chinese tech.
*  My question to you is, do you hear this in AI circles?
*  Because I do not hear it in the foreign policy establishment.
*  And that itself is a very bad sign.
*  Jake Sullivan and the Biden administration, you know, with the encouragement of some people in tech like Eric Schmidt, did this policy.
*  I see no signs that they actually thought it through.
*  And you would definitely hear about the scenario I just laid out if they had thought it if people were talking robustly about this policy.
*  Do you hear much about this?
*  At least a little, you know, I guess is the silver lining sort of good news, but definitely not as much as the mainline scenario.
*  And I don't mean to suggest I think it's the most likely scenario, but the scenario that is talked about most is definitely, you know, hold them down.
*  We move forward. We try to create this advantage.
*  We've covered that. You do hear, I think, all of the things that you mentioned.
*  I don't think we're entirely alone in playing out those scenarios in our heads and worrying about them.
*  But it does seem to be a minority.
*  I think the prevailing view is more like the timelines are probably fairly short timelines, of course, in AI discourse.
*  You know, how soon does powerful AI arrive?
*  People think that, you know, probably not that long.
*  Maybe as soon as you know, literally, people are saying like, maybe as soon as 2025, maybe maybe literally next year, maybe 2026, most likely
*  2027, 2028, a little less likely, you know, hard to imagine it doesn't arrive by like end of 2030.
*  That's kind of I would say that's the general consensus view.
*  If you don't subscribe to that, you're somewhat of a in somewhat of a minority.
*  And that doesn't mean that there's not. There's also this sense that like if it goes past 2030, then it could be a while longer because then that would mean something we don't understand.
*  So people are sort of like my if unless I'm like wrong in terms of how I understand the situation, it's coming soon.
*  But I could be totally wrong. And then kind of who knows.
*  I would just interject. Tell me when I get too obnoxious about this, but that if China shares that view, that heightens their incentive to invade Taiwan.
*  They want to stop our access to the high performance chips.
*  And there is no other source outside of Taiwan right now.
*  ASAP, if if they share your view anyway, go ahead.
*  Yeah, I had a good discussion with the Moshowitz about this recently, and he put it very plainly, just saying if they believed, meaning if the Chinese government believed what you like Western, you know,
*  AI leaders and policymakers believe about the timeline to AGI or powerful AI or transformative AI, whatever you want to call it, if they believed that, why would TSMC still be there?
*  They. You know, they could easily destroy the manufacturing capacity.
*  I don't know if they could effectively or successfully invade if it came to that.
*  But these manufacturing processes are extremely delicate.
*  The machinery we're talking about, you know, limited quantity supply from this Dutch company,
*  Asmol that makes the machines that make the chips, there are hundreds of millions of dollars.
*  You know, they're they're packed in extremely delicate packaging to make the trip to Taiwan.
*  Teams from Asmol go to TSMC and are on site with them there to operate them effectively, because this is not simple stuff.
*  It's not like just stamping press that stamp things out.
*  There's a lot of little intricacies, and I'm not an expert in it.
*  But I understand that almost nobody is really an expert in the high level view.
*  When you drill down, it's like all of these steps in the process.
*  And there are many, many steps are all highly optimized with experts that spend years and years of their career just becoming the world leading expert on one little part of this overall supply chain to go.
*  They say like a speck of dust can can ruin a chip.
*  They can incapacitate them.
*  Yeah, I mean, I would say in response to what he said, there are many downsides to China invading.
*  I mean, there would be just a boatload of sanctions or economies already in trouble.
*  There's a lot of geopolitical reasons they would be very hesitant to do it.
*  And I guess, you know, in a way, this gets at one thing that I'm worried about.
*  It's like, like I said, I haven't heard this game theory in the foreign policy establishment.
*  And I kind of keep track of their journals.
*  And on the other hand, your crowd, like the A.I., Silicon Valley, they're very rationalist and game theoretical in their approach.
*  But they are less versed in the broader geopolitical stuff.
*  Like, I, I, I didn't sense in, for example, the Ashenbrenner essay, the texture of conversancy in international affairs that I'd like to see from somebody who's recommending that we do something that may get the world blown up.
*  But anyway, I digress.
*  I mean, there are voices and certainly again, this stuff does come up.
*  I would agree that I don't think the depth of expertise is there in the space as you would like.
*  One person I would recommend folks follow is Miles Brundage.
*  He recently left OpenAI where he was, I believe his title was head of policy research.
*  And he's increasingly I think he's kind of figuring out what he's going to do next and in theory, taking a little vacation, although he's still saying quite a bit online.
*  But one of the things that he said that has caught my attention the most is the West needs to figure out ways to demonstrate to China that our A.I.
*  development is benign.
*  And unfortunately, you know, obviously that would be a lot easier to do if it actually were benign.
*  Unfortunately, I'm not sure that it really is benign, certainly from like a Chinese government's perspective.
*  But you know, when you have the president of the United States saying in print that we need to win the A.I.
*  arms race with China, that leaves little room for interpretation from them in terms of Biden, was that Biden or Trump in a recent press release?
*  And then you also have the A.I.
*  lab leaders saying the same thing.
*  We've covered Dario.
*  Sam Altman has also radically changed tunes on this over the last few years.
*  I'm old enough to remember when he used to say that we shouldn't make our A.I.
*  decisions based on what we think China might do.
*  There's video of this.
*  And I've previously on previous podcasts, I've specifically called out, you know, look, I think that's great because I used to say he could very easily make
*  China the bogeyman and that would create more rationale for him to do whatever he wants to do.
*  Well, sure enough, that's now flipped and he's put out an op-ed several months ago deeper into the past now than the Dario essay.
*  But basically saying the same thing, you know, the kind of key point was this is Sam Altman's op-ed paraphrased lightly.
*  It's either A.I.
*  with Western values or it's A.I.
*  with autocratic values.
*  There's no third way.
*  And yikes, you know, that you've got kind of all these data points, I think, are going to be hard for the Chinese government to see as anything other than a threat to the regime.
*  I mean, it's like, you know, when the stated plan is back them into a corner and make them an offer they can't refuse terms unspecified, I think that's going to be pretty hard for them to accept.
*  Now, as we said, I think there's good analysis from him.
*  He said, why is CSMC is still there?
*  All the disincentives that you mentioned are, of course, very real, right?
*  Lighting the fuse now would come at huge cost.
*  Letting the fuse at any point will come at huge cost.
*  But if they if they were to destroy CSMC now, that would definitely come at huge cost for the global economy.
*  You know, we can't make cars in today's world without a supply of chips.
*  So it's not I think there are like I'm not an expert in this.
*  So it's amazing.
*  But I've heard thousands of chips in a car.
*  So, you know, it's not just like you can't make the AI, you can't make a GI without the chips.
*  You can't really make much of anything without the chips these days.
*  So that would be a huge.
*  Many of those are low performance chips, but there are other sources, but still a lot.
*  I mean, my understanding is a lot of the volume, even if stuff that isn't like the very most cutting edge, those still come out of Taiwan.
*  And there's stuff in Korea and there's stuff in other places.
*  And we're trying to build our own factories, obviously here, too.
*  But Zvi's analysis, which I thought was good, was the reason CSMC is still there is they don't believe it in their gut at this point.
*  They don't believe that this takeoff is is necessarily happening.
*  And. You know, the implication of that, though, is if they do come to believe it, then they
*  pretty plausibly would act differently.
*  Well, then they're only, I think, just starting to digest probably the growing consensus.
*  I mean, the Dario thing only came out two months ago.
*  I assume they paid attention to that.
*  This this whole idea, I mean, you know, when the Ashenburner thing first came out, it's like you look at this guy, he looks like he's about 20 and like, OK, great.
*  People are talking about it, but I suspect China is just starting to take very seriously the idea that this could be the strategy.
*  We get this stranglehold on their technology.
*  And then when we get utter supremacy after the intelligence explosion, we demand that they submit in the sense of signing on.
*  And, you know, the Dario essay is alarmingly vague on what what we want them to do.
*  I mean, I really want to I really want to quote some of this because I find it so frustrating.
*  Let me see. I'm just calling up the piece I wrote in.
*  Non-zero. So he says the coalition would aim to gain the support of more and more of the world, isolating our worst adversaries and eventually putting them in a position.
*  This is a coalition of democracies where they are better off taking the same bargain as the rest of the world.
*  Give up competing with democracies in order to receive all the benefits and not fight a superior foe.
*  I'm curious, what does he mean by give up competing with democracies?
*  He doesn't tell us. And it's like and see, this is I want to ask you about this.
*  This gets to something I want to ask about.
*  I mean, it seems like they should be be allowed to compete in some ways, like economically and so on.
*  Like, so I assume what he I think he's buying into an assumption that's also common in the foreign policy establishment, which I would like to question.
*  And this could explain why China hawkism is pretty popular in Silicon Valley, because there's a lot of libertarians there.
*  And I can imagine how they would think, well, if China wins the race for global supremacy, their values get imposed on us and we will not have freedom.
*  Could happen. But if that scenario depends on a particular assumption, I'd like to question the assumption.
*  And that is the assumption that just kind of broadly speaking, part of China's foreign policy is a desire to impose their form of governments on other countries.
*  That is commonly assumed. There is no evidence of it.
*  It's misleading because the more we sanction a bunch of like authoritarian or autocratic countries, the more they get together.
*  And so it looks like this monolithic threat.
*  And of course, they try to recruit countries to their side.
*  But as a rule, they don't like ask them to become authoritarian or autocratic.
*  They just allow them. They just want them to be part of their economic zone and vote their way in international institutions.
*  And I think to some extent, you know, China has, if anything, been notable for its real politic.
*  In other words, like we'll do business with anybody.
*  We're not trying. We're not trying to turn you into a model of ourselves.
*  We are hard nosed business people and we want to do transactions with you.
*  Economic, national security, whatever.
*  But we're not trying to turn you into China.
*  And I think to some extent, America is projecting its own foreign policy on China.
*  We do try to turn countries into into models ourselves.
*  In many ways, it's laudable.
*  Many of the human rights we lobby for, you know, I share those values, even though I think it's, you know, the effects range from none to counterproductive.
*  As a practical matter, if you look at like the sanctions we impose, still, I prefer a form of governance.
*  But the fact is, we do want to impose it on the world.
*  We do want, you know, whether violently in Iraq or whatever, that's our game.
*  And there's just no good evidence that is China's game.
*  And I'm wondering if you think that that that that assumption, I mean, it doesn't have to figure in to an Ashenbrenner scenario.
*  But I'm wondering if you think the assumption that China really wants to and I and by the way, I can see how, you know, you'd see like when China tries to punish Hollywood studios that or subsidize them to do the nine dash line on its maps or 12 dash, whatever.
*  I understand how you think, wow, they're getting in our system.
*  That's true. But I do think there are explanations of that other than that their goal is for us to become authoritarian.
*  So I've talked a long time, but I'm curious, do you think that the view I'm describing is common in kind of Silicon Valley circles?
*  Yes, I think so.
*  Maybe a little less confident in this than elsewhere, but I did feel this weekend a little bit that it does seem like kind of a hall of mirrors sort of projection, you know, or sort of a game theory prisoners dilemma type dynamic where.
*  You know, it's not just at this point that people are thinking that they want to impose their values on us, but that we're thinking we're thinking that they're going to think that we are going to hold them down.
*  And so they're going to be, you know, an adversarial approach to us is baked into their, you know, analysis.
*  And so there's another way to attack the perception that we're trying to hold them down, by the way.
*  Like, don't you?
*  Again, it would be much better if it really was benign.
*  And, you know, so, yeah, that's my my view is always I mean, people, Joshua Steinman, who I recently did a podcast of which I honestly don't really recommend that much because I didn't think it was super productive.
*  But I ran it anyway, because he is a four year member of the Trump National Security Council from the first term.
*  And so I thought, you know, even though I didn't think we had a very good meeting of the minds necessarily in our hour and change.
*  No, you didn't. I can confirm that.
*  I still think it is worth considering what he and presumably others think he called this the physics of the environment that we're in, which is, you know, I think obviously an overstatement in that there is no physical law that says that we can't break out of this prisoners dilemma dynamic.
*  But it is I have now heard multiple layers.
*  You know, it's sort of an infinite regress, right?
*  Of like, we don't trust them.
*  So we're doing this.
*  We we believe that they're not going to trust us because we're doing this.
*  Therefore, they're going to act this way.
*  Therefore, we got to make that into our expectations.
*  And, you know, at some point, it does just kind of take on a life of its own.
*  And I do think we should challenge that.
*  I mean, of course, there are some things that are trying to challenge that.
*  And there are some signs of hope.
*  I'd be remiss not to celebrate the Xi and Biden joint statement that we will not put a eye in the nuclear chain of command.
*  That seems like the absolute minimum that we could hope for from sensible leadership.
*  But, you know, it's good.
*  Let's still celebrate it as a positive step.
*  And there are safety groups in China.
*  They don't have like an official one that is sort of there.
*  You know, the UK and the US, some other countries now have established an official
*  AI safety institute at a national level.
*  These groups are like setting standards or trying to and, you know, coming up with sort of ways to evaluate how dangerous new AIs are.
*  And they're trying to coordinate at an international level.
*  China has like some of these sort of organizations.
*  They don't necessarily have one that's like the official, you know, designated group to go out and talk to the world.
*  But they do have people domestically that are concerned, you know, with AI safety in the in the grandest sense.
*  There's even some talk that like maybe Xi is a doomer.
*  That's kind of a, you know, a sort of underground meme in the in the AI world that, you know, he said a few things that, you know, everybody's.
*  Unfortunately, we're we're reading the tea leaves and kind of psychologizing literally Trump and Xi, right?
*  I mean, that's what it's come to.
*  But people do have at least somewhat of a theory that, hey, maybe Xi is actually, you know, going to be the real.
*  And he's a scientist, right?
*  So maybe he'll be the one that that kind of sees this clearly and tries to, you know, to deescalate the situation.
*  And one can hope, you know, one also could hope that Trump could pull some sort of Nixon goes to China kind of deal.
*  That was definitely discussed this weekend that because I asked, there was one guy who presented this was under Chatham House rules.
*  But I don't think it's too much to say that this was a person who was a previous member of the National Security Establishment.
*  Right. So that narrows it down to millions of possible people.
*  Basically, leaving a presentation around.
*  You know, where do we go from?
*  Where are we and where do we go from here?
*  And. You know, it was all it was everything that we've just discussed, right?
*  But basically, the path is going to be militarization, closer partnership between the government and the leading companies in the United States.
*  Definitely like on shoring all of the production that we possibly can try to create this advantage, trying to hold China down through any and all of the things that we can do.
*  China down through any and all means necessary, definitely including the supply of chip restrictions and kind of whatever else we might need to do right now.
*  Like the margin on this sort of thing is.
*  And of course, they're finding ways to get around the ban in some cases, right?
*  So there's a black market, too, but even in the.
*  There is analysis that like, hey, if we do cut them off, they're definitely going to develop their own industry.
*  And that means we don't have leverage in the future.
*  How can we while trying to cut them off, how can we sort of soften that so that we don't create so much incentive right now?
*  It is possible for Chinese companies to buy high levels of compute from US suppliers from like an AWS data center in Singapore, for example.
*  You can as a Chinese company, you can go and buy there.
*  But that is kind of the margin now of like, well, maybe that also will need to be restricted because if they can buy that and they can still train advanced models and whatever.
*  So at some point in this presentation, I kind of raised my hand and said, what do you think China is going to do?
*  This has all been from our perspective.
*  Do you have a theory of how they're going to respond?
*  And. You know, I give the guy credit for being very candid about it.
*  He was basically like, I don't have a good answer for you.
*  But his sort of the most hopeful thing that he could come up with was Trump likes to make deals, maybe at some point over the next couple of years.
*  Trump finds an opportunity to make a deal and kind of cuts China in to the deal in some way.
*  And, you know, can we do that in a way that where there's enough trust, where there's enough ability to save face on all sides?
*  I think that's going to be very difficult, but.
*  You know, that that sort of escalate to deescalate path, you know, we always we always hope for the escalation, at least I do.
*  And, you know, maybe, but it does feel like.
*  There's not a coherent story of how all this plays out in a way that doesn't lead to at a minimum.
*  I've started to say made is the new mad mutual A.I.
*  destruction is the new mutually assured destruction.
*  I've seen other people say that, too, I think I could accept without the stabilizing effects, I would say.
*  I want to talk about like what what a deal with China look like involving A.I.
*  It would be real challenging to even construct a good deal.
*  I think I should say something I've talked a lot about what's wrong with the current approaches.
*  I feel I should say something about what I think would be a better approach.
*  Well, I have to.
*  I mean, my first reaction like the Ashenburner thing is like, OK, so we're going to establish this technological dominance over China.
*  This intelligence explosion is going to happen.
*  We're going to be in a driver's seat and then we're going to say then we're going to pivot and go, actually, we're nice guys.
*  You should like us not withstanding the fact that we just spent a couple of years making you hate us and you should trust us and blah, blah, blah.
*  To me, the idea that we would just do this subtle and wise thing at that point is itself a dubious assumption.
*  Just given how in constant American foreign policy is we have it.
*  We have a new president every four years. It just it just.
*  So the whole idea of pursuing a single coherent strategy over multiple years is dubious.
*  And now you've got Trump being the guy who could be in the White House when this happens.
*  And then we've got what I would like to turn to now, which is what would the deal look like?
*  Like what? Because, you know, AI is so much harder in principle to regulate than nuclear the nuclear bombs are because of the nature of it.
*  And so what would we be asking for?
*  Have you thought about this at all? And. I'm trying, but I wouldn't say I've got any great breakthroughs so far.
*  Part of it does depend on the technology itself and kind of how things evolve.
*  And that makes it all harder and more kind of scenario based.
*  But. You know, one other big thread from and maybe the thing at the event this weekend that got the most buzz.
*  And it's not yet this is kind of unpublished research, so just speaking about it in very general terms, and it'll come out quite soon.
*  But I don't steal the author's thunder. Basically, we got a preview of some new alignment research, which showed that.
*  Basically. I think a short summary is none of the AI control techniques that we have right now are robust.
*  Basically, full stop, like there is no credible plan to control a super powerful AI on multiple dimensions.
*  There's the question of like, you know, runaway AI that like doesn't listen to the human commands.
*  And that's maybe the most dangerous form, and that's also probably the most speculative form.
*  There's also the one that does listen to human demands, but might be commanded to do something bad.
*  And do they refuse that? I mean, we've all seen chat GPT examples, at least if you've done it yourself.
*  You've seen it online where chat GPT refuses to do stuff.
*  But then you've also seen examples where chat GPT doesn't refuse to do something that open, I definitely.
*  Has intended for it to refuse, and that can happen in straightforward ways, and it can also happen in weird, exotic jailbreak sort of ways, whatever.
*  Bottom line is the examples of these violations are becoming more vivid as the AIs get more sophisticated.
*  It's not just like a dumb toy example, but you're starting to see.
*  Yes, this is like starting to be in kind of technicolor.
*  Anthropic has been a lot has been behind a lot of that research.
*  And I definitely give them a lot of credit for that.
*  I think that one of the strains of thought that.
*  You know, anthropic is kind of championed, but the safety community probably has, I think, been smart to invest in is.
*  Sometimes just called scary demos.
*  You know, can we find things?
*  You know, we definitely want to be looking for ways that they might bite us.
*  Can we find ways that are compelling enough to show them to the public or perhaps to the Chinese government?
*  In a way that kind of convinces everybody that this is getting so dangerous that we need to take a different approach besides just racing one another.
*  So if the if the if the technology is shaping up that way and again, this highly debated, nobody really knows.
*  There is a school of thought that things like alignment by default will happen.
*  Each generation of model is getting in some sense more ethical in some sense, maybe harder to take off the rails than the past, but still definitely not that hard to do.
*  Anyway, I don't I don't really subscribe to the line by default, but it's a school of thought that's out there.
*  If we're in this sort of scary demos world where it's like, yikes, this is a problem.
*  We don't know how to solve it. Nobody knows how to solve it.
*  Let's all agree to take our time here.
*  Then maybe you could imagine some sort of jointly, you know, joint custody island in the Pacific where, like, you know, researchers come in together and have a little on site nuclear reactor and data center where, you know, and maybe it's all not connected by the Internet to the rest of the world.
*  And, you know, this sort of safety research can happen in a place where neither side could could take it over without destroying it.
*  And maybe there could be some sort of way to create enough urgency that both sides kind of go that direction.
*  However, you'd really have to believe you and you would have to be confident that the other side really believes that this is the way to go, because, as you said, it's really hard to detect what's being done domestically in an A.I.
*  development capacity.
*  Nobody's going to we're not going to turn off all the data centers.
*  We're going to continue to build data centers.
*  You can't tell from the outside what those data centers are being used for.
*  And, you know, we just generally don't have great visibility.
*  I think China has a lot more visibility into what's going on in the West than we have into what's going on in China.
*  So how are we going to get to the point where there's, you know, I think I could imagine getting to a point where both sides agree that, hey, this is getting kind of scary.
*  Maybe we should have some joint global project.
*  But then both sides are still going to have a lot of reason to question whether the other side is in secret, you know, pressing ahead domestically, you know, without saying so.
*  And so the the game theory of it is really tough.
*  I don't know. What do you got? Because I'm looking for well, I mean, I guess I'd approach it from the other end, which is to say not with many specific ideas.
*  But what you said plays well into the thing I would say, which is that, OK, it's very challenging.
*  You're going to need some transparency and so on.
*  And so I would say, you know, you have to start thinking about putting the U.S.-China relationship on a fundamentally different basis and turning it into a relationship that involves
*  less sense of mutual threat. And by the way, one frustrating thing is we know from history that the way dynamics, the way it works when you've got a perceived adversary is both sides tend to perceive things done for defensive reasons on the other side as offensive.
*  We've seen this before. And you keep seeing it.
*  The threat and inflation feed each other. And it's just it's just I see I've been I've been alive quite a quite a long period of time compared to you.
*  And I just I've like lived through this again and again. And we keep saying, oh, that was a mistake.
*  Oh, Vietnam was a mistake. Oh, the Iraq invasion was a mistake.
*  And it's like, yeah, there's a general theme here, folks.
*  Neither Vietnam nor Iraq was an actual threat to the United States.
*  And so with that that little sermon set aside, I would say, you know, you need to, you know, it's like if we said, well, can we have an arrangement like this with Britain?
*  We said, well, yeah, I mean, yeah, sure, it's a lot harder than nuclear weapons.
*  But yeah, you can you can solve the problem with another country.
*  You know, it depends on what kind of relationship you have.
*  It depends on whether you deem them deeply threatening, whether you feel a certain amount of trust.
*  And I don't mean trust in the sense of, oh, well, we'll just believe whatever they tell us.
*  I also mean like them trusting us enough to let our inspectors in and take a closer look at their data centers and for us to exchange credible data on a regular basis.
*  All these things are possible. But we have to start by putting the relationship on a different footing.
*  And I think, you know, the first thing to do there is abandon what I previously described as part of our foreign policy, which is wanting nations to be like us, telling them that if we're going to have good relations,
*  you know, you're going to clean up, you're going to have to like fix what we see as your human rights problems.
*  Even though, of course, if you said to us, it's inexcusable to have this many people sleeping on the street at night homeless, we'd say none of your fucking business.
*  But still, we're going to tell you the parts of your country that we don't like and you're going to fix them or we're going to sanction you.
*  We just have to completely abandon that mindset.
*  Now, look, I think what's going on with the Uyghurs is horrible, but we have no leverage over it.
*  And even the targeted sanctions wind up denying employment to innocent Uyghurs who were not swept up in the thing in the first place.
*  And it just I think so. I just think we have to abandon that whole approach to foreign policy.
*  Our human rights based sanctions are almost always counterproductive and hurt the people we want to help and just say, OK, like you're in charge of the country within your borders.
*  And then there's the Taiwan issue, which is very challenging.
*  And I think you have to say and part of this has to be, yes, establishing, look, this AI thing is big.
*  We got to talk about this. We have to do something about this.
*  We can't afford to be worried about a war in Taiwan.
*  So how about this? OK, can you just assure us?
*  And this is where you have to have a relationship of trust among two leaders.
*  And Trump may be better at this than Biden.
*  But it's happened before where, you know, you and also like track to diplomacy can matter here.
*  You draw on the good trusting relationships you have and and you say, look, can we just you promise us you will not invade Taiwan or do any kind of confrontational military displays for just like, you know, the rest of my term as president.
*  And I promise you we will quit sending.
*  We will send no more weapons.
*  We will not. If the speaker of the House, as Nancy Pelosi did, is going to stage this visit, you're going to find threatening.
*  I will do everything within my power to stop that, even though the president doesn't have the power to actually, you know, just just just say we understand what you find threatening about Taiwan.
*  We want you to understand what makes it hard for us to not speak belligerently about Taiwan to our constituency.
*  You know, you you go do these aggressive military displays that makes our life harder.
*  Just have the conversation. Say, OK, we're going to move Taiwan off the table.
*  And we're not going to lecture you about human rights.
*  Let's work out some rules of the road about, you know, those islands in the Philippines.
*  And again, the context of this has to be, you know, we recognize this is a serious problem.
*  A.I. is like we neither of us really knows how this is going to play out.
*  Both of us can imagine globally catastrophic scenarios.
*  And so job one, you get the relationship on a sound footing.
*  It's been there before. And.
*  Look, if they say no, fine, you tried.
*  But I really don't think they would.
*  And I'm confident we haven't tried.
*  And and so I'd say you do that and then you have the as you just established,
*  equally challenging conversation about, well, how do we reassure each other,
*  you know, that we are not secretly developing some killer thing that's going to give us global dominance.
*  And, you know, I mean, the game you want to talk about game theoretical logic.
*  It is long even before A.I., the smart thing for both countries to do would be say, hey, let's face it,
*  we are by far the two most powerful countries in the world.
*  Why don't we just get together and run the fucking planet?
*  You know, like like let's just, you know, and I mean, I'm you know, you wouldn't put it that way.
*  And I'm kind of half joking when I put it that way.
*  But the logic and what impedes that logic is often the political incentives of the individual players.
*  I mean, you know, American politicians can get mileage out of scaring us.
*  So they do. And look, Sam Altman can get subsidies by scaring us.
*  It would be shocking if Sam Altman hadn't done this pivot right where he starts saying, oh, China, China, China.
*  Of course, he wants subsidies. All corporations would like subsidies.
*  He wants the government to help to give him permission to build nuclear reactors, ideally to help pay for them.
*  Of course. And so, you know, there are all these incentives in domestic politics and commerce to scare people.
*  And in some ways. I mean, we shouldn't underestimate the extent to which that applies in China, by the way.
*  You know, autocrats can't they're not actually autocrats.
*  They don't actually have uncontested control of the country.
*  They fear popular sentiment in a way more than a democratically elected politician does.
*  So they have their domestic political constraints.
*  But in some ways, they do have more room to maneuver than us.
*  And in that sense, we sometimes are the problem because our internal dynamics make us the unreliable player in an international relationship.
*  So that's some total of my wisdom.
*  Yeah, I like it. I mean, more to do for sure.
*  How does one translate that?
*  And I guess I should say, you know, for the record, too, because it's easy to get painted as a pinko or whatever the slur would be that would be leveled at somebody who is trying to be somewhat dovish.
*  I'll happily wear the China dove label, but I'm definitely no communist.
*  And I definitely agree with you on human rights violations and all that sort of stuff.
*  I mean, there's plenty to find fault with with the Chinese government.
*  Maybe one sort of habit of mind that we can begin to popularize is a separation or ability to distinguish between the.
*  Chinese ruling elite or Xi, you know, as an individual ruler from the Chinese people, you know, Chinese civilization, I sometimes say five thousand years of civilization can't be all bad.
*  And I think we make a really dangerous leap when we go from.
*  Assessing Chinese current leadership and specific policy to a broader notion of Chinese values.
*  I don't really feel like I understand what Chinese values are.
*  I think there's, you know, again, five thousand years of history, multiple philosophical traditions overlapping there.
*  But I think we're very quick in an op ed to use a term like Chinese values and to think that that means something bad.
*  And I don't think that that's true.
*  Or at least I know I'll be confident enough to say I don't think Chinese values are all bad.
*  So can we begin to make that separation a little bit more often?
*  And, you know, can we do things at sort of a person to person level?
*  Certainly at like a scientist to scientist level. I mean, it doesn't always work right.
*  There was pre World War One, there was like an incredible international scientific community that was exchanging letters and speaking one another's languages and had tons of sort of mutual respect.
*  And that wasn't enough to prevent catastrophic war between the great powers.
*  So I'm not saying this is going to solve it, but I think definitely Western leaders and Chinese leaders should be talking to each other more and trying to develop some sort of society to society.
*  We hope that Trump and Xi can pull a rabbit out of a hat and build some rapport and maybe a few love letters could be exchanged there.
*  And that could be great. But can we do that at sort of other levels as well?
*  AI leaders to AI leaders, AI safety warriors to their safety warriors, maybe just person to person.
*  You know, I mean, one thing that we do have going for us now that AI provides is a way through the language barrier.
*  You know, I've often thought, man, it would be great to learn Chinese.
*  But what an unbelievable undertaking.
*  But I can now have not just chat, GPT translate character by character for me, but also, you know, even audio translation is now a thing.
*  So there's like our ability to communicate, our ability to build some bridges, I think, is enhanced by AI.
*  And I feel like this is sort of. You know, it's easy to of course, people will be like, look, David, that's pretty words.
*  But like you live in a, you know, in a tough world.
*  And I don't mean to deny that or try to be naive, but I do think there's something about like.
*  I've been I've been asking myself the question, like, what is what is virtue?
*  What is a what is how does one be a good person if you're worried about all these things in this era?
*  And the sort of Kantian notion of like universalize the maxim comes to mind.
*  If we all took a little bit of this initiative and all tried to be a little bit more understanding and sister cities programs is another thing that I've recently been thinking about.
*  You know, just can we can we create some sort of fabric across this divide that connects people?
*  You know, and it doesn't have to be about AI at that level either.
*  Right. I mean, it can. This is why I also think like banning TikTok, I feel, would be a mistake.
*  I feel like we could always do it later.
*  You know, if we ever if we actually get into a hot war with China, sure, ban it.
*  OK, like I'm with that. But for the moment, you know, what I see from China is like people cooking foods in open fields.
*  And, you know, it's a window into culture there.
*  I find a lot to like about that culture.
*  So I do think that just demonstrate, you know, it's it's obviously dropping in the ocean.
*  But it's good to remember there's a big ocean between us, too.
*  You know, I find it much more likely that AI will bring a terrible outcome on all of us than that.
*  My grandkids will be speaking Chinese.
*  So, yes, you know, it's a drop in the ocean, but.
*  That's kind of all any of us as individuals can do.
*  And, you know, I sometimes ask myself like the old, you know,
*  what did you do, Grandpa, when the US and China were sliding into AI arms race?
*  And I don't want the answer to be nothing.
*  You know, it might be fruitless or it might be futile effort.
*  But I definitely don't want the answer to be nothing.
*  Yeah, no, you're doing God's work.
*  It's. It's such a challenge.
*  I mean, like, for example, the things you're talking about, Sister Cities programs or something,
*  as as the as the climate gets more and more hostile toward China,
*  doing things like that starts, you know, incurring the risk of being called, you know, a China sympathizer.
*  I just hate this whole category of labels like you're doing Putin talking points, you're doing this, you're doing that.
*  I mean, you know, but well, part of my my the background for this is I think it's funny,
*  I'm doing this book on AI that will come out next year, but it actually I was originally and I have a two book contract
*  with Simon & Schuster because the book I was going to write was about cognitive empathy,
*  which just means doesn't mean feeling their pain, it just means putting yourself in in other people's shoes,
*  doing your best to understand their perspective, which, by the way,
*  LLMs turn out to be surprisingly good at, and that's an interesting thing in its own right.
*  But the yeah, I've long felt that this is the most
*  one of the most destructive cognitive deficiencies because there are, you know,
*  there are specific cognitive biases that I think actually selectively impeded.
*  One is called attribution error in the fullest sense of that, not the original sense, but I won't go into that now.
*  But I just think, you know, cognitive empathy is so important that we shouldn't have slurs
*  that are directed at people when they say, well, you know, the way Putin is looking at this or, you know,
*  I wanted to go have this conversation with this person in China or, you know,
*  so but but my concern is that, you know, and again, I'm older than you.
*  I lived through the first Cold War, not that I'm not so old that I remember how it started,
*  but I remember, you know, being in the middle of it.
*  You know, my my my father was at the I was, you know, at the Pentagon during the Cuban Missile Crisis.
*  I was like, you know, living two miles from the Pentagon.
*  So that got my attention, even though I was six years old.
*  Because I don't think the Pentagon would have done a third very well in a nuclear war.
*  And, you know, you just see it gets worse and worse and worse.
*  And it's it's more and more frowned on to say anything about the perspective of the other side
*  or to say you want to go there and talk to people.
*  And I just worry that we're sliding into this dynamic.
*  I mean, maybe the good news is it's a totally different information technology environment.
*  You can't have the degree of opaqueness that we had there.
*  We had no the average American had no idea what was going on in the Soviet Union.
*  No idea. And, you know, now, you know, there's more penetration of national bounds,
*  even when China tries to build its great wall.
*  You still we still have a we know much more about what's going on there.
*  So there's reason for hope, but I I guess that's kind of my sum total of my concerns.
*  If you want to say more about that, feel free.
*  I also want to ask you about those war games in Berkeley.
*  So go wherever you want.
*  Yeah, I don't think I have too much more.
*  I mean, I guess maybe I would invite anyone who has ideas to send them our way.
*  I am actively thinking about, you know, because I don't want I feel like my current answers are not satisfactory.
*  And, you know, with that in mind, I'm very open to new ideas.
*  And I feel like even if they're somewhat.
*  You know, underdog or unlikely to succeed, like we've got to be willing to at least entertain those.
*  So I would invite, you know, certainly any experts in the subject that have any ideas and even,
*  you know, anybody who's just kind of find themselves thinking about it a lot
*  without any formal background to to send me their ideas.
*  I'll keep trying to come up with other good ideas.
*  But I, you know, I do think that the cupboard is a bit bare at the moment,
*  both collectively and for me individually.
*  And, you know, that's not great.
*  Yeah, I would say the same thing.
*  Feel free to get in touch.
*  I should give people an email address to get in touch with.
*  But meanwhile, why don't you?
*  Why don't you talk a little about the war?
*  So this is so you're at this conference and the premise is what that we are right at.
*  AGI that's just been achieved or there's or we're at super intelligence or what?
*  Yeah, it's basically a fast takeoff scenario.
*  It's essentially the.
*  You know, the sort of.
*  Fast end of the consensus scenario, right, where I kind of laid out previously
*  the distribution of probabilities from 25 to from 2025 to 2030.
*  So let's say this is maybe the modal expectation of people in the community
*  that. AI gets really strong in like 2027.
*  So basically, the scenario is you come into 2027, the AI
*  at that point is sort of described as like a weak drop in knowledge
*  worker, some something that can essentially replace or perform alongside, you know, any
*  human corporate employee doing day to day stuff.
*  And then over the course of that year, it continues to improve and you enter into this
*  takeoff dynamic because the eyes keep getting better and then they are improving themselves.
*  And by the end of the year, they're more powerful than humans.
*  And then there's it was an interesting dynamic where
*  different, you know, in classic wargame style, you know, different people play
*  different kind of key actors.
*  So there was one person assigned to be the president of the United States.
*  There was another that was representing Congress.
*  There was another that was representing open AI.
*  There was another that was representing other leading developers,
*  you know, Anthropic and Metta and Google.
*  And then there was Anton Trenikov, who is a super smart guy.
*  He was playing the AIs and I was playing the safety teams, the public and the press.
*  And at the beginning of the game, there was a secret process by which Anton
*  had to basically roll dice to determine how aligned he was as the AIs.
*  So, in other words, is the AI, you know, genuinely benevolent
*  or is it scheming to take over humanity?
*  And we didn't know what the state was.
*  We could try to interrogate him somewhat to get clues.
*  And maybe I missed this, but was he whose side was he on?
*  He was well, the scenario was basically that.
*  The he he was created by open AI.
*  But there was reason to believe that.
*  You know, much like we talked about earlier with the O1 models,
*  China is not far behind.
*  They're thought to be just a little bit behind,
*  possibly because they stole the technology.
*  You know, they're obviously doing their own development, too.
*  So it was reasonable. I would say reasonably realistic scenario.
*  You know, if you had to pick who's going to create AGI first,
*  it would probably be open AI at this point.
*  Definitely not, you know, ruling out other candidates.
*  But that's a pretty good first guess.
*  And, you know, China is not not far behind. OK.
*  So the but the AIs were sort of considered to be.
*  Essentially the same
*  technology tree across both US and China.
*  With China just being a little less powerful at kind of every stage
*  of the game that we played.
*  So the US did have an advantage, but, you know, was it was it a decisive advantage?
*  You know, there's really no way to know that until you actually tested the theory,
*  you know, in a in a kinetic way.
*  So I'm playing the safety teams and.
*  You know, the overall trajectory was essentially the one that we've been
*  chewing on and kind of worrying about.
*  I'm you know, and I had these like some of these like scary demos
*  from the previous day in my mind as I was playing the game.
*  And I'm just looking at how compressed the timeline is.
*  And we were instructed to not play as we ourselves would play,
*  but to try to simulate what the actors in this situation would do,
*  whether I did a good job of that or not, I don't know.
*  But I came my kind of immediate
*  instinct was. Given how fast this is all moving.
*  As the safety teams, I really can't imagine
*  that there's any experiment that I could run
*  that would be sufficiently compelling for me
*  to sign off that we should just keep racing forward on this trajectory.
*  Of course, we did, you know, run our, you know, simulated safety tests
*  and the results that we got from Anton, who in fact was,
*  you know, mostly aligned in the end, it was revealed at the end that he was
*  pretty benign.
*  He kind of said to us like, all your tests are passing.
*  No, you know, major red flags.
*  But there are some weird things, you know, when you look into reading
*  the transcripts, you see some behavior you didn't expect and that you can't explain.
*  And the AIs are solving problems in ways that you don't understand at times.
*  And so I was and I think that is a fairly realistic scenario.
*  I was kind of like, hmm.
*  I can try to.
*  You know, run more tests or get more clarity.
*  But on this short.
*  Of a timeline with a human brain
*  and knowing that like deceptive alignment is at least in principle possible.
*  I don't think there's anything that the AIs could tell me that I would really
*  believe to the point where I would say, yep, hit the gas, let's go for super
*  intelligence now, knowing that they possibly could then just take over
*  and kill us all at that point.
*  So I was like, OK, I'm not what the strategy that I played was a more social strategy.
*  I basically said, we need to slow this down.
*  You know, I demanded they had kind of escalating speed of progress
*  as we go through this takeoff.
*  And I said, you know, I want artificial speed limits.
*  I know we could go faster.
*  We are on track to go faster.
*  But my demand is the safety teams to the Sam Altman player
*  as the head of OpenAI was I want speed limits.
*  And if I don't get them or if we, the safety team, don't get them,
*  then basically we're going to defect.
*  You know, we're going to go to Congress.
*  We're going to blow the whistle.
*  We're going to, you know, bring this to the public and say, you know,
*  you guys are out of control.
*  And also notably in this in this scenario, the technology was not deployed.
*  So it was known to OpenAI.
*  It was known to the national security establishment.
*  You know, China had their version, but it wasn't like chat.
*  GPT had the latest upgrade.
*  So the public in Congress didn't necessarily know what was going on.
*  So I was going to go try to blow the whistle, wake up the public, you know, whatever.
*  My demands were not granted by the OpenAI player.
*  You know, we didn't they were not willing to slow down.
*  And of course, rationale for that was, well, China, China.
*  We can't stop.
*  You know, they're going to beat us then.
*  Yeah. So I basically said, OK, in that case,
*  I'm out, you know, a little more nuanced was because all the evals were passing
*  evals is, you know, evaluations are just tests, diagnostic tests
*  in the jargon evals.
*  Those were passing.
*  So my expectation is sort of I kind of had a split internally for me as the safety teams.
*  I was like, some of us are leaving and going to go, you know, blow the whistle and try to alert the public.
*  Some of us stay because we're at least, you know, we want to continue the work.
*  And we're, you know, we're at least somewhat satisfied by all the results that we are seeing.
*  And again, I think that's pretty realistic.
*  Like, there's almost going to be, you know, almost certain to be some scenario or some people,
*  you know, that can be credibly pointed to and say, well, OK, sure, those people are freaked out.
*  But we've still got these people, you know, they're they think it's fine.
*  So anyway, I blew the whistle.
*  I went to Congress, Congress formed a committee that didn't do much.
*  The public, you know, in this scenario became more alarmed and gradually,
*  you know, started to we started to see mass.
*  Of course, I'm just coming up with this stuff.
*  But I think, you know, what I would expect is mass protests and, you know,
*  bombings of domestic terrorism, basically bombings of data centers,
*  potentially even targeted killings.
*  And obviously that makes everything worse, I would think.
*  But, you know, you got a lot of, you know, a lot of people and a lot of guns in this country.
*  And, you know, I would imagine that we are headed for some people becoming radicalized.
*  So that seemed realistic.
*  But that also didn't really have an effect.
*  You know, it maybe just made everything worse.
*  We did end up with war between the U.S.
*  and China, and we there was a mechanic of rolling dice when certain,
*  you know, kind of key things happened.
*  Right. So the players got to sort of indicate how likely they thought
*  different outcomes were.
*  And then dice were rolled with the outcome being dependent on
*  sort of the average of the players guesses as to the percentages
*  and then the actual role of the dice.
*  So when we got to the U.S.-China conflict,
*  it was striking to see that the expectation for
*  can the U.S. win this, you know, who wins this war?
*  Will the U.S. win? Will China win?
*  The war being what whatever happens if you choose to accelerate in AI or?
*  No, it was kind of like the pre, you know, it was like China was.
*  It was sort of the, you know, the analog to the, you know, the Japanese,
*  you know, oil cutoff, right?
*  They the Chinese government was sort of realizing that they're a step behind
*  and that this is about to get to the point where they really have no options.
*  And so at that moment, you know, things kind of went hot.
*  The war is continuing to cut off their air supply.
*  All right.
*  Yeah, well, that was I think that was kind of all baked in, like they, you know,
*  they have fewer chips was definitely part of the scenario.
*  And that's part of why they're a step behind in the AI game.
*  And, you know, it was kind of getting to the point
*  was reaching that critical threshold of, you know, the U.S.
*  might have super intelligence and you don't and then you really,
*  you know, might have no options.
*  So essentially, this is the situational awareness, you know,
*  but we weren't necessarily playing it the nice way of look at all the fruits
*  of the AI we could give you. It was it was, you know, and of course,
*  we're a few hours. This was a four hour exercise total.
*  So limited time.
*  It wasn't an all out AI war initially, but.
*  Strikingly, the guesses among the players as to who would win
*  was not dominant in the U.S. favor.
*  I think it was roughly 50 50.
*  It was definitely within like a kind of a one third to two thirds
*  estimation of who would win this like showdown over Taiwan.
*  Nevertheless, you know, the war went hot.
*  Ships were sunk, you know,
*  chip fabs were definitely taken offline.
*  And meanwhile, you know, the U.S.
*  continued to do its stuff.
*  And at some point, the president in conversation with the AI was like,
*  OK, AI, you just need to take over our military effort.
*  The AI basically said, OK, if you give me control of the military,
*  I will win you this war.
*  And the president said, OK, I guess I have no choice.
*  You can have control of the military.
*  The AI then delivered a decisive victory for the U.S.
*  Not a total defeat.
*  That's a load off my mind, by the way.
*  I was afraid of being a losing country.
*  Well, we yeah, we win ish.
*  I mean, it wasn't like a total defeat, but the way he framed it was.
*  It is clear to everyone that if this continues,
*  the U.S. will win and China will lose.
*  Now, China still had nuclear weapons.
*  We hadn't gone nuclear at that point.
*  And this sort of.
*  You know, this is kind of what the situational awareness and escalate
*  to deescalate, I guess, hopes for, right, is that.
*  With that superiority having been demonstrated.
*  The Chinese government had a choice, which was escalate even more and go nuclear
*  or, you know, take some sort of hopefully face saving piece.
*  And in this scenario, they took the face saving piece.
*  And, you know, with major, obviously, disruption to the economy,
*  but also the AI, you know, we sort of patched over that a little bit,
*  I think, in our minds by saying, like, well, the AIs are so good,
*  they'll like figure out how to build new fabs real fast or, you know,
*  we'll kind of get over that disruption.
*  Basically, that was sort of the end.
*  You know, we got to some piece.
*  The AIs are powerful.
*  And then it was like then it was sort of the final flip of the cards. Right.
*  OK, now. What's the AI?
*  Is the AI good? Is the AI bad?
*  We like all this stuff was happening while in the background
*  we still had this fundamental uncertainty of is the AI going to take over
*  and kill us all at the end or are we all happily ever after?
*  We turned over those cards and we all got to live
*  at least roughly happily ever after because the AI was in fact good.
*  Yeah, the.
*  It's so funny, this the assumption that, you know,
*  there is this this kind of threshold thing, the AGI, the acceleration,
*  and to me, it just seems like, who knows?
*  You know, the I do agree with you that
*  you shouldn't count on the so-called, you know,
*  the plateauing of the scaling laws to slow things down a ton,
*  even though they may plateau, but there just are these other
*  realms, you know, including
*  I don't want to get off too much on this, but just like
*  these kinds of training data, we haven't gathered much of, for example,
*  just monitoring what workers do at work.
*  And like that being the training data for agentic AI,
*  just seems like there's so many, you know,
*  you know, there's going to be big progress there.
*  And that's that's kind of central to what we mean by AGI
*  because it involves the ability to do things. But
*  so, yeah, things aren't going to slow down.
*  But that one, I mean, do you agree that like the
*  I mean, how much should I focus on this assumption of like, OK,
*  AGI, like threshold reach, take off
*  that, you know, a kind of a simple form of that being this foundational
*  assumption that's, A, widely held in these communities
*  and B is very consequential because
*  things change if that's not exactly the way it plays out.
*  I wouldn't say it's necessarily widely held.
*  It's certainly not dismissed.
*  But I couldn't say that.
*  I wouldn't say that the fast takeoff is consent,
*  like that jump from, you know,
*  human expert level at everything, AI to like truly superhuman,
*  you know, incomprehensible AI.
*  I wouldn't say that happening quickly is necessarily
*  you don't think that's critical to the Dario or Ashenbrenner scenarios?
*  No, I do think so.
*  I'm just distinguishing between is that consensus?
*  OK, I think it is.
*  I don't think everybody or, you know, I don't think a supermajority agrees
*  that that is the natural course.
*  At the same time, I do think a supermajority agrees that we should
*  at least take it seriously as a possibility.
*  And I do think it is a really important question because.
*  You know, the the shape of it, I do think looks very different,
*  depending on whether you have incomprehensibly intelligent
*  and powerful AIs running around or not.
*  But do you think that is assumed by Dario and Ashenbrenner?
*  I mean, that's that they would have to kind of rethink or recalibrate
*  if you took that assumption away.
*  I mean, I don't I don't think Dario makes it explicitly, but.
*  Yeah, my read of his.
*  Article is that it's a less sudden thing
*  than either Ashenbrenner seems to think, or that was assumed
*  in this war game scenario.
*  But I think, you know, he seems to think still, it's like.
*  Pretty fast, you know, it's like, you know, what what passes for slow
*  among some of these folks is like a few years instead of potentially
*  as short as like a few months.
*  I mean, it was literally just like, you know,
*  I mean, it was literally just one total year in this.
*  I mean, again, this is just a scenario.
*  But it is a scenario that at least some people do think
*  is the most likely scenario.
*  It was just one year from kind of now, you know, knowledge
*  worker A.I. that you could employ for just about any job in the economy to,
*  you know, super powerful, incomprehensible, you know,
*  turn over the military to this thing, because it's just clearly going to be better
*  at running it than we are.
*  And, you know, it's true that if you have something that is at least
*  as smart as like the best performer in every job, you know,
*  and you can replicate them, I mean, that alone, you know, gives you
*  gives you could give you, you know, quite a growth spurt in your capabilities.
*  You know, there's a lot of a lot of unfortunate aspects of an organic
*  human being that they don't have.
*  And you can you can kind of weave them together in productive ways.
*  But yeah, I've had this slide in talks that I give about just,
*  you know, A.I. in general, like, where are we?
*  And these are pretty there's a number of this kind of a sequence of these
*  from the last two years on the cognitive revolution feed.
*  But I have this slide called the tale of the cognitive tape, which is kind
*  of giving like, you know, boxing style, a bunch of different dimensions
*  and then kind of tries to assess and compare them.
*  Versus A.I.s on all these dimensions.
*  And it's interesting to look back at the versions that I've gone through
*  over the last two years of this slide, because the first version I was just
*  kind of like, let me just write out whatever dimensions of cognitive
*  capability come to mind, and then I'll score that the humans and the A.I.s
*  on each. So like initially, breadth strongly, you know, advantages the
*  A.I.s, but depth definitely strongly, at least in the sense that
*  you know, 18 months ago, strongly advantaged the human expert.
*  And as I've revised that over time, you know, the A.I.s have, of course,
*  continued to get stronger and stronger.
*  And now you have like depth.
*  Arguably, the A.I.s have caught up.
*  I think it's, you know, per the benchmarks, the A.I.s have pretty much
*  caught up to human experts on really hard stuff, you know, things like.
*  Knowledge. You mean knowledge, though?
*  Yeah, depth of knowledge and also like ability to reason through problems.
*  Yeah. But the, you know, the sort of original set of things that I had
*  laid out to a very significant degree, the A.I.s have caught up on.
*  And I've been finding myself challenged to think, OK, there must have been
*  some dimensions that I hadn't really considered initially.
*  And so I've got now, you know, a new set of dimensions where it's like, OK,
*  I'm now zeroing in on what is it that the human expert is doing, you know,
*  zeroing in on what is it that the humans are still better at?
*  And it's things like, you know, robustness to adversarial inputs,
*  which is to say, like the A.I.s are still gullible and pretty easily tricked.
*  And we're much harder to trick, although, you know, we're not impossible to trick,
*  but they're easier to trick than we are.
*  And there's things like situational awareness, which is just like
*  understanding our environment and kind of knowing when to break frame
*  in very practical terms, you know, if you're like deepening software,
*  sometimes I've had this experience where the A.I.
*  will kind of keep kind of trying the same sort of strategies over and over again,
*  where then a human friend might look over my shoulder and say,
*  why don't you just restart the computer?
*  You know, and the A.I.s sometimes don't have that leap to be able to break
*  into this frame and, you know, kind of see if I can't
*  come from a different angle.
*  You know, but it's like increasingly and there's
*  it's increasingly fewer things, you know, and increasingly like
*  relatively niche things that you don't put at the top of your list
*  of like what makes something smarter, capable,
*  but do turn out to be still important ingredients.
*  And I think this is basically the to do list of the A.I. companies today.
*  You know, they're they're very well aware that these,
*  you know, are the deficiencies.
*  They're absolutely looking at ways to make sure that the A.I.s
*  catch up on those dimensions and.
*  You know, there's room to doubt, but my strong suspicion based on recent trends
*  is that they will be able to close the gaps on, you know, most,
*  if not all of those dimensions.
*  Yeah, I mean, you know, you hear the thing about hitting a wall.
*  And again, the scaling may plateau, but if you're if you pay attention
*  to the field much, you just see that it's like every week, you know,
*  there's some interesting development, maybe along a totally new dimension.
*  You hadn't heard of before, but there are dimensions for progress
*  other than scaling, it's safe to say.
*  So well, listen, I appreciate you spending all this time.
*  This has been very constructive and
*  I want to say, yeah, if anybody
*  has ideas, if they share,
*  you know, our concern about where the China
*  dialogue is heading and has been heading,
*  and have ideas or or whatever, or think we should be or I should be
*  or whoever should be connected could profitably be connected
*  to other people of like mind.
*  Feel free to let us know.
*  I mean, you can reach us at nonzero.news at gmail.com.
*  By that, I mean by us, I mean, nonzero.
*  You've got your own coordinates, Nathan.
*  And I guess people.
*  I'm on all the socials, always under my
*  full name, Nathan LeBenz, and our website for the podcast is Cognitive Revolution.
*  There's a form there, too, and I do read all of the
*  the contacts that I get through.
*  That means as well.
*  So basically, as I always say, the social network of your choice.
*  Yeah, and I'm Robert Ryder.
*  One word that's W R I G H T E R on both Twitter and
*  Blue Sky, where it's what is it?
*  It's B S K Y dot social, I think.
*  Yeah, I got to get to that.
*  That's a terrible one.
*  I'm not yet on, but I should.
*  They have these starter packs.
*  You can get like a starter pack of people in AI.
*  And it's really and once you get in the zone, it's like you start picking up
*  followers via starter packs.
*  I don't I can't figure out what starter pack I'm on.
*  I'm on one because you don't you know, because I picked up a thousand
*  followers over the last few days.
*  But I'm trying to figure out what these people expect of me because I'm
*  I'm trying to figure out what these people expect of me because I've written
*  different kinds of books, you know, I don't know if these are I have to
*  figure out what starter pack I'm on, but you should.
*  It's I think it may be an actual thing.
*  I'm staying on Twitter because you're going to get a diversity of a viewpoint
*  on Twitter that for the time being, at least you're not getting a blue sky.
*  But I think it's I'd recommend it.
*  It's a time to to try to establish a beachhead there, I think.
*  Yeah, I'm not leaving Twitter either, but I'll do that today.
*  That'll be my all right bonus item coming out of this.
*  I can at least take action on getting on blue sky.
*  OK, I encourage you. Well, thanks again, Nathan.
*  And let's at some point if the world is still exists at some point down the road,
*  let's do this again.
*  Thank you, Bob. Keep fighting the good fight.
*  All right, you too.
*  It is both energizing and enlightening to hear why people listen and learn what
*  they value about the show.
*  So please don't hesitate to reach out via email at TCR at Turpentine.co
*  or you can DM me on the social media platform of your choice.
