---
Date Generated: May 21, 2024
Transcription Model: whisper medium 20231117
Length: 3395s
Video Keywords: []
Video Views: 2047
Video Rating: None
---

# OpenAI's Safety Team Exodus: Ilya Departs, Leike Speaks Out, Altman Responds - Zvi Analyzes Fallout
**Cognitive Revolution:** [May 19, 2024](https://www.youtube.com/watch?v=lvjs-1SpX6U)
*  Hello and welcome to the Cognitive Revolution, where we interview visionary researchers,
*  entrepreneurs, and builders working on the frontier of artificial intelligence.
*  Each week we'll explore their revolutionary ideas and together we'll build a picture of
*  how AI technology will transform work, life, and society in the coming years.
*  I'm Nathan Labenz, joined by my co-host Eric Torenberg.
*  Jim Ashwood, welcome back to a special bonus session of the Cognitive Revolution.
*  Yep, there's always more to learn.
*  It's happening quickly today.
*  So by the time we got off the recording, Jan Lijka had posted his tweet thread statement about his
*  reasons for leaving OpenAI, in which he puts it pretty plainly that he's had pretty fundamental
*  disagreements with leadership and has had trouble getting the resources that he needed to do the
*  work, including compute resources. And certainly had some nice, fond things to say to his teammates,
*  but basically it was like, I don't think we're on the right track and seems to be resigning pretty
*  much in protest. So that doesn't seem like a good thing. It doesn't seem like a good situation.
*  What else have you learned and what do we make of it?
*  We also got coverage from Vox, Bloomberg, from TechCrunch. We got Ken Altman's response,
*  actually too likey, which was extremely graceful, essentially saying, yes, we have a lot of work to
*  do and we're going to do it and expect a longer response later. It's the best possible thing you
*  can say there, but then you're on the hook for doing it. We have Kelsey Piper confirming the nature
*  of the draconian non-disbaragement clauses, which apparently have lifetime duration and include an
*  NDA that you can't reveal about violating the NDA. And she claims that when employees come
*  on board for the first time and are given an equity heavy compensation, they are not told
*  that they will be required to sign this disparagement clause or have their existing
*  vested equity confiscated upon departure. So that seems like a really bad equilibrium and
*  way to run a company. I'm honestly confused as to why that's legal.
*  Well, it maybe shouldn't be.
*  Yeah, I think you should have to very much acknowledge the disparagement clause rules very
*  clearly initially if you're going to confiscate something of immense value for someone not signing
*  them. It doesn't seem reasonable at all. It also doesn't obviously speak well of the company in
*  its openness in the good sense, right? If you're forcing every employee to never disparage you for
*  life no matter what or else, that's just not a reasonable position to take if you want people
*  to not assume the worst. And so, yeah, we look at Leckie's statement essentially saying that
*  for years they have had the trouble of shiny new products becoming the priority of a move away from
*  safety culture, that the culture is not amenable to or compatible with safety. My paraphrasing
*  here a bit, I'm not looking at the words precisely. And that he had trouble getting it last few
*  months despite the explicit 20% of existing compute commitment from OpenAI, which should have
*  been sufficient for current purposes. And indeed TechCrunch confirms that they have not been
*  honoring their commitments. They have asked for a fraction of the 20% commitment and have repeatedly
*  not gotten what they asked for. And that is part and parcel of the whole idea of running anything
*  in AI these days is compute. You need your compute to do your thing. And he was reporting this
*  specifically become a substantial barrier to doing their work. So this is not only a philosophical
*  approach because he said they should be spending vastly more, but not just a little bit more,
*  but they should be spending vastly more of their resources on preparing for our AGI future. But
*  also if you'll notice what he actually said for just the next generation that essentially he doesn't
*  think they're ready for GBT-5. He doesn't think they are on pace to have the tools they need
*  for GBT-5 to be safe in a pedestrian mundane utility sense, not in an existential sense.
*  And then later on there's the problem of super alignment, there's the problem of AGI, which many
*  people at OpenAI have said they expect within several years. It's a very short timeline. And now
*  the super alignment team has been dissolved and its people have been dispersed throughout the company.
*  They claim they will still continue the work on that level, but having dishonored their commitment
*  and having dissolved the team and having lost the leadership, it doesn't look like the kind of effort
*  they promised us, but they said they were going to do. Does the timeline still hold, Sam? I would
*  we're now nine months into the four years. If he still expects to need super alignment
*  disclosinus in four years, then these actions do not reflect somebody who understands that and
*  understands as he's repeatedly told us he understands what is at stake with super alignment.
*  And it's clear Mikey's breaking point was Ilya departing according to the Bloomberg article.
*  That makes perfect sense. But we have a series of departures. We have obvious justifications for that.
*  And this all makes sense. It's we all there was all this speculation about what did Ilya see.
*  It was the whole thing. What did Iyan see became the thing. Yes, you know, after Iyan quit.
*  And the answer is what they saw was a company that's not committed to safety. It's unwilling
*  to put its money where its mouth is that has a culture that is hostile to safety efforts.
*  And that is pivoting towards the shiny new product, which is a product company. It's a scaling startup.
*  It's devoted to making money. Nothing wrong with making money. But in this case, they're building
*  smarter than human intelligence out of their explicit company mission and goal. And as Mikey
*  points out and has ultimately is repeatedly acknowledged, this is not a safe thing to do.
*  There's not a default thing that will go well by accident. Right. We've been debate
*  how likely it is to go badly with good real efforts to make it go well. But I think any
*  reasonable person can see that there's a very good chance that if we do not put in the effort,
*  we do not do the work that things would then go badly. And it doesn't have necessarily
*  involve like some sort of specific AI catastrophe or external risk scenario, which simply means
*  this could go very badly for people as experienced by people on the planet Earth.
*  And we have to think carefully about these questions. But what's clear is that Open AI's
*  leadership as embodied by Sam Altman increasingly is not doing this. And simply in the last few
*  months is not on your commitments. Yeah, that compute one for me is pretty troubling. I could
*  try to make a in the absence of that, it would be a lot harder to parse, I feel like. Obviously,
*  people can have all sorts of disagreements. And you can imagine the Sam Altman defense being like,
*  you're doing your thing over here in the super alignment team. We're doing our thing over here
*  in the product team. Why is that inherently a problem? But yeah, if you can't get the compute
*  to do the alignment work. It's not just a promise. And we got why it's beyond specifically saying,
*  you promised us x, we needed x over n, where n is a lot more than one to do our work. And we couldn't
*  get it. And the super alignment commitment, I get sounded a lot 20% of our currently available compute.
*  That's not a lot over four years, because two years from now, open ads gonna have to time as
*  much compute. Like for sure, unless something very strange happens. So over the period, this is
*  a reasonably small, very modest commitment, as opposed to in certain similar industries,
*  where safety is paramount. Most of research costs, most of development costs can become safety.
*  And again, that's for mundane level, just make sure the plant doesn't melt down levels of safety.
*  So I don't understand it. I don't understand why it's not simply good business
*  to give you Mikey and people like that their compute.
*  They're really relative to this outcome, it seems. If we are indeed talking about something like
*  5% of compute, then that could only allow you to move, you know, 5% faster, right, than you
*  could without that compute. It does seem like a very strange decision to
*  allow people to walk and allow this to become this big of a story over a couple percentage points of
*  compute availability. Yeah, it's almost always the straw that breaks the camel's back, right?
*  It's clearly, it was being shut out of decisions, couldn't play his ambassador role that he had
*  previously played inside the company. If you're reading between, not even between the lines,
*  which is reading the lines of various articles and reports that people were turning against the
*  very idea, the board battles embodied this struggle, the way that they played out in
*  practice, whether or not this is an all fair or deserved by anyone or any philosophy or any
*  approach or anything, turn people against them, every person that leaves turns you further against
*  them. The fire ends, you have a cascade of every person that leaves, you lose more trust. Like,
*  why did they leave? What caused them to leave? How was that handled? And yeah, like he presumably
*  was just like, I'm fed up. We're not getting what we need. You need to wake up call. I can't just
*  sit here and try to pretend that I have the resources I need because I don't. And the way
*  he talks about it, they're just not investing what a company in their position needs to invest
*  in what I call mundane safety either. And in fact, I haven't investigated this, but you see this in
*  their reports that 4.0 has much less tendency to refuse inappropriate requests like building a bomb
*  than GBT-4 turbo or its rivals that somehow this new model that was sent out is actually just very
*  not robust in the jailbreak slash mundane harm zone. It just is very willing to fulfill your
*  requirements. And I haven't tried distress testing or red team it in this sense, if it's why would I?
*  I don't think it's been really a dangerous thing for the most part to have to be jailbroken. It
*  was already jailbroken for those who cared enough, but what's very clear is that the 4.0
*  development process did not involve a robust attempt to make it safe in a conventional sense.
*  They mostly just decided that the abilities that this model possessed were not so dangerous and
*  they just weren't going to give it much care. So I'll see you chained to the open AI fence or
*  what do we do? What do we do? I think we have to treat going forward until we see could respond
*  with an amazing set of commitments. He could respond with a new set of hires of people he's
*  bringing in. He could do a number of things. So we're always hopeful and as much as AI moves fast,
*  I don't think we need to move this week or anything like that. And part of it is that it's
*  very clear from Jan's testimony that we don't have to move this week. That if Jan and Elia and others
*  were concerned about something imminently happening, they would have said so. It's very clear from this
*  statement that these are long term concerns driven by things that the ships get steered slowly,
*  no matter what you do. So they have time to fix it. But barring evidence to the contrary,
*  given everything that we've learned, I think we just have to assume that open AI is functionally
*  a fully for-profit business, fully a move fast and break things hockey stick graph startup business
*  run by Sam Altman, who is running it on that basis. And that they are not taking those safety
*  problem seriously, that their culture internally is hostile to the idea of safety, the idea of
*  worrying about things, and that therefore we should expect them to handle this future badly.
*  That we should expect them not to be prepared for what is to come to be extremely cavalier.
*  And there's the possibility that for a while cavalier works out. A cavalier GPD-5 might well
*  be the best thing to happen for the world. It's possible if they're just not that dangerous.
*  We talked about not safe for work. We talked about sex and gore and other capability. And yeah,
*  it might just be good to let that stuff happen. It might be completely net positive. Right. I
*  actually expect that. And Pliny broke all the major models. He broke all of them fully. And he
*  broke GPD-4-0 in about two minutes during the announcement speech. Right. Like the moment he
*  got access to it because his first hunch just worked because why wouldn't it? But at some point,
*  that's going to stop being an acceptable situation. At some point, these things are going to be highly
*  capable. And we're going to have to worry about societal implications and then catastrophic risk
*  and then existential risk. And all of that's probabilistic because you don't know when it's
*  going to happen. When GPD-5 comes out, you're probably not going to be an existential situation
*  at all. But how many nines are you going to put on that statement? And if your answer is more than two,
*  you're crazy. Right. And I would probably put one. Right. So like you can come crying and say,
*  you said it was only 97% and it turned out not to happen. I'm like, well, right. Chalk it up to a
*  slight, slightly worse calibration than you on this one. And let's keep betting on sporting events.
*  But I don't know what else to say. Hey, we'll continue our interview in a moment after a word
*  from our sponsors. AI might be the most important new computer technology ever. It's storming every
*  industry and literally billions of dollars are being invested. So buckle up. The problem is that
*  AI needs a lot of speed and processing power. So how do you compete without costs spiraling out
*  of control? It's time to upgrade to the next generation of the cloud, Oracle Cloud Infrastructure
*  or OCI. OCI is a single platform for your infrastructure, database, application development,
*  and AI needs. OCI has four to eight times the bandwidth of other clouds, offers one consistent
*  price instead of variable regional pricing. And of course, nobody does data better than Oracle.
*  So now you can train your AI models at twice the speed and less than half the cost of other clouds.
*  If you want to do more and spend less like Uber, 8x8 and Databricks Mosaic, take a free test drive
*  of OCI at oracle.com slash cognitive. That's oracle.com slash cognitive, oracle.com slash cognitive.
*  The Brave Search API brings affordable developer access to the Brave Search index,
*  an independent index of the web with over 20 billion web pages. So what makes the Brave
*  Search index stand out? One, it's entirely independent and built from scratch. That means
*  no big tech biases or extortionate prices. Two, it's built on real page visits from actual humans,
*  collected anonymously of course, which filters out tons of junk data. And three, the index is
*  refreshed with tens of millions of pages daily. So it always has accurate up to date information.
*  The Brave Search API can be used to assemble a data set to train your AI models and help with
*  retrieval augmentation at the time of inference, all while remaining affordable with developer
*  first pricing. Integrating the Brave Search API into your workflow translates to more ethical
*  data sourcing and more human representative data sets. Try the Brave Search API for free
*  for up to 2000 queries per month at brave.com slash API.
*  I assume you have seen the Dwarkesh interview with John Schulman out this week. So as it's reported
*  now that he's taking on the responsibility for safety writ large, I guess he was already
*  responsible. As Dwarkesh presented him in the interview, he was responsible for post-training
*  the models. As described in some of these articles today, he's responsible for making today's models
*  safe. Now he's going to have this kind of additional responsibility rolling up to him of like big
*  picture long-term safety. In that context, that interview I think is going to give you serious
*  pause. Obviously you can evaluate it for yourself, but there were multiple moments.
*  Just hearing what he was doing previously. You took the mundane safety guy who is doing a job
*  with mundane safety that it's not necessarily better or worse than the rivals that we sent. So
*  I haven't evaluated for, oh, right, in the sentence. And there's again, reports that there's
*  a potentially a problem, but that's just completely irrelevant problem to the problem. He is now being
*  asked to solve. And if he was doing that job properly, then like he wouldn't necessarily
*  have mentioned that they have these concerns about the next generation of models not being prepared
*  for, that should be part of the job as well. But yeah, the idea that you're going to use post-training
*  on your AGI or even your ASI, right? To render it like HHH or like net useful or any of those terms.
*  I think that's basically a pipe dream to rely only on that. And in fact, you have to worry during even
*  the training regimen of some of these things, but it's just the kind of strategy, certainly
*  in the right way, but the strategies that he's been using so far are the kind of things that
*  like he himself acknowledged explicitly, like both online and literally five feet away from me during
*  a talk would not work. Right. He took on the 80,000 hours podcast. He explained why ROHF
*  is not a solution to this problem. And he has other proposed solutions that he wants to try
*  where I don't think they'll work. And I tried to debate him and explain to him why they wouldn't
*  work. And I was unable to make my case, especially convincingly, and he didn't buy it. I think it was
*  reasonable for him not to buy it in the sense that like I had, I was making some very bold claims.
*  They're very different from his worldview. And I didn't back them up specifically enough. I think
*  it's hard for me to do with that potential training. But like he was thinking about the
*  problem, right? On a different level than someone who's thinking about post-training,
*  thinking about the problem. So I haven't seen the interview yet. Again, I want to be very careful
*  with it. But yeah, if you think it's going to give me pause, I'm confident you're right.
*  It's going to give me pause.
*  Yeah. A couple of just for folks who might be listening to this and might not have time to go
*  through that or inclination to, it is definitely worth it. It's very interesting in multiple ways.
*  One of the things that he says is that he's expecting quote unquote AGI on kind of a two
*  to three year timeframe. Dwarakash asks him like, could it be as soon as next year? And he's,
*  I don't think so. That would be surprising. But two to three, he was pretty much willing to
*  co-sign on. And then Kesha asked a number of questions that were like, pretty fundamental.
*  What happens when this happens? How are we going to deal with it? Or at one point, I'm not sure
*  that's a, doesn't sound like a super robust plan that you're outlining here. And I appreciate the
*  candor, but at multiple points, he was like, yeah, I don't really have a great account for how that's
*  going to go. Or hopefully we'll be able to work together with the other leading developers in that
*  situation. And yeah, we don't really have a robust plan for that at this point.
*  Right. Second best plan, right? The best plan is to actually figure out what you're going to do
*  and how we're going to handle this. And the second best plan is that, no, you don't know,
*  right? To start with a blank beginner mind. So if he comes to this, if he comes day one,
*  and he says, we don't have a plan, we don't have a solution, we don't know how to align this thing.
*  We don't know what to do with it if we did align it. We don't know how society can handle this.
*  We don't know how to make the transition from AGI to ASI. Again, even if we handle alignment,
*  even if we handle the interim, we don't know any of these things. We're lost. We need to figure
*  this out. And he starts from scratch. I'm perfectly happy with that as the answer for a person who's
*  highly capable. He founded OpenAI. He's founded it. He's done a lot of impressive things. Like
*  Ilya, I thought, Ilya cares deeply about these issues and appreciates the depths and stakes of
*  the problem. But I always thought, and I still, I mean, every, going off of Ilya's publicly stated
*  remarks, I just don't think any of the things he was thinking about are anywhere near like what you
*  need to be thinking or what would work. Right? But he stuff wouldn't work what I thought like he was
*  reasonably grounded and trying hard. Ilya's stuff just felt like it often felt, okay, that just
*  seems like a misconception. It feels like you're thinking about this problem in a kind of fuzzy,
*  like not sufficiently geared way often. And in a way that like just wouldn't survive an encounter
*  with the enemy. Right? When he looks at, when he and like he and these other people run their
*  super alignment team and they run, they build these papers, they run these experiments and they
*  actually try to make these things work, they'll understand that their plans aren't working and
*  they'll either find ways to modify them so they work or throw them out and find and try new ones.
*  Because one of the things Ilya is famous for is the kind of attitude of, you know, I'm going to
*  try this as many ways as I have to and throw out everything I think I know until I make this thing
*  work. I'm committed to making this thing work. Right? And that's what you love to see. And that's
*  why I had a lot of faith that Ilya and Jan would find a way, not every time, because I don't think
*  this problem is even necessarily theoretically solvable by humans in a reasonable timeframe with
*  any attitude, but I thought they had a reasonably good shot because with a lot of resources and a
*  lot of time and four years is somehow some of a lot of time, they would figure out at least some
*  ways not to align an AI. And then we get to try again. And I think their first way definitely
*  won't work in my model is that's fine. They were a mix of ideas that were like reasonable and I
*  thought promising and they're hopeless, but nobody knows how to solve these problems. So I can't really
*  get that mad at you for being excited by ideas that I think won't solve these problems. What do
*  I have? A better suggestion? You try something, you learn something, you try again. And I think
*  there's a decent chance that looking at these generalization questions, looking at these
*  supervision questions will inform your approach to trying to find alternate solutions that might
*  themselves be more promising. If none of those types of solutions, that entire solution class
*  is entirely hopeless, like the worst case scenario that is as bad as it looks to be on first glance
*  and there's no fixing it, then those are pretty bleak worlds in many ways because that cuts off
*  a lot of people's plans. So I'm leaving room for there being places to maneuver there.
*  So with this clarity and while we await a proper response from Altman and leadership,
*  what else do you think people should be doing? Joking, but maybe I'm not entirely joking about
*  chaining oneself to the open AI fence. For reference, there was a person who's done that
*  this week. There is like mundane consumer protest. Now that chat GBT is free, it's going to be hard
*  to cancel our accounts, but we could rally app developers to boycott and switch to Claude or
*  something. We could obviously on the record on being at least like generally positively disposed
*  to SB 1047, we can think about supporting that even more forcefully or suggesting possible
*  amendments to that, to strengthen that. Something like the non-dispairment clause being made illegal
*  could be an interesting one. When I looked at 1047, I wrote it up. I was mostly looking for
*  ways in which it was too strong, but I identified a number of ways in which this bill might go too
*  far in the sense of it has these serious downsides and unless we're getting a lot in exchange for
*  these downsides, that makes it politically hard to pass, make it harder to get buy-in, it makes it
*  harder to get cooperation and nobody actually wants to tank the economy. Nobody wants to actually
*  slow down the mundane utility and not nobody, but I don't. And so how can we strengthen this bill?
*  The exception being the derivative versus non-derivative definition clause where I thought
*  that it was just a bug. It's literally, there's a major definitional mistake in this bill. Maybe
*  it's just making it worse on every level. We need to fix it because if I can pass off all of my blame
*  to you, that makes your situation terrible, but it also makes all the same to guarantee something
*  I can skirt. I can then ignore anything and that's not good either. We can't let anybody cheat.
*  We have to stop this. So some of the others were like, how do we weaken this? I don't know if
*  weakening is the right word, but how do we clarify this bill? How do we prevent potential
*  overreach or misinterpretation of this bill in a stronger sense? And then other people pointed out
*  ways in which the bill was potentially too weak. Everyone was complaining about the criminal
*  liability, but it's only under perjury. Maybe that's not enough. One could ask potentially.
*  I think for now it is where we want to be. And I think the reaction to even the perjury showed us
*  that this is just a third rail and people just get so scared of such things that it might have
*  very bad dynamics for the good people to stop touching things or they panic and they don't want
*  that to happen. But yeah, we have whistleblower or clause in the bill. And if you don't enforce
*  non-compete agreements, right? This seems so much worse than a non-compete agreement,
*  right? An ordinary non-compete agreement, which now not only California is not enforced forever,
*  but now like they're going to be illegal or high paid, all but the most prominent employees
*  the entire country, because the FTC doesn't, the much that doesn't go through. But yeah,
*  I have a hard time believing that it is in the interests of the public of the United States
*  to allow a company to hold most of somebody's well hostage to signing a lifetime full non-despairing
*  clause on the company they are leaving in an area in which things that are wrong are in the vital
*  national interest, right? If there is something wrong with the safety at OpenAI, that's something
*  we need to know. If there's anything wrong with the culture at OpenAI in other ways, that's something
*  we need to know. If you have a whistleblower provision, this is like, how do you blow the
*  whistle if you're not legally allowed to blow the whistle or the price is millions and millions of
*  dollars in equity, right? That you can't sell so they can just confiscate it. So even they don't
*  even have to sue you necessarily, they can just confiscate it in that situation. And then who
*  knows what else they might be threatening or holding over people. But you just don't know,
*  again, because they can't talk about it. So I can't know these things. So we have to assume the worst
*  in some senses because we can't talk about it. And yeah, I think it would be very reasonable to say
*  that AI companies should not be able to sign non-despairing clauses as pertains to certain
*  aspects of the company, certainly, and potentially universally. Like I think it's just, why is it good
*  for the two of us to get to an agreement where we agree that no matter what happens, I can never say
*  anything bad about you and you can never say anything bad about me. I understand why it's better
*  for us in some sense. But people not being able to talk in that way just doesn't seem great.
*  And if we're in the business of not enforcing contracts that are against the public interest,
*  this seems like a private place to look, right? Even though I have libertarian instincts that like
*  avoiding contracts is bad. Yeah, this seems to be a reasonable place to consider that,
*  obviously. But beyond that, well, if you're not going to do the safety work yourself,
*  necessarily, someone has to make sure that you do. Someone has to be doing the check-in.
*  And this is a reason to doubt. If I'm going to trust a company, I have to be able to trust
*  their commitments, I have to be able to trust their statements, their testaments. And I need
*  that to be under some sort of punishment, like the whole idea. If you lie, the idea is that you don't
*  have to do anything. You have to tell us what you are doing. And you have to be held responsible
*  if you lied your ass off. That is what the key provisions of SB 547 are about. They're about
*  saying what you're doing and being responsible if you didn't do it, right? And saying what your
*  logic is and being responsible if your logic is a lie. It just makes no sense. Willful disregard,
*  beyond the pale. That's what it's about. And also just having a mechanism where if you discover that
*  there is actually catastrophic risk in the room, you can get the model shut down. Both that they
*  have the ability to shut it down, at least locally, and that you can order back. These are the
*  fundamental things this bill is about. So these things seem important now more than ever, right,
*  in the light of this information. But, and not only speaking, yeah, I think we have to understand
*  that until proven otherwise, OpenAI is much less of a confusing than it was a week ago,
*  or six months ago, right? There were reasonable arguments to be made when they announced super
*  alignment, when they put out their reasonably good-
*  They made some of them myself.
*  Yeah, they made it. They put out a reasonably good fairness framework, right? They've done some good
*  things. They've hired a bunch of good people. They had a bunch of people who moved in circles
*  where they're credibly spending a lot of their time talking the right ways, asking the right
*  questions, even if I don't agree with their specific beliefs. And a lot of that's just gone
*  now. Their credibility is shot from a safety perspective. And so I think it's a lot less
*  confusing now. And yeah, I think that if you have a choice in whose technology to use in some sense
*  at this point, and you choose to go with OpenAI in a way that matters, well, this is part of what
*  you are considering, part of what you're doing. And I think it also means that you are taking on
*  a risk, a concrete risk yourself, because I don't think you should necessarily trust their mundane
*  safety in this world going forward. I think it's- I'm still just, again, there's just not
*  much that can go wrong with the GBT-4.0 that I am that scared of. But if they don't have a culture
*  of safety, you need a security mindset to build AI, to make these AIs do the things you want them
*  to do and have it not go up in your face, even in an ordinary, normal, mundane way. You need to be
*  thinking about these problems and working on these problems and giving them the respect they are due.
*  And so, in the entropic business case for we deeply care about this and we have a culture of this,
*  which they clearly do amongst their employees, where they encourage this concern,
*  where they have this concern. And we're going to make sure that when you use our product,
*  you get what you're trying to get and not something else you did not expect and did not want.
*  It becomes a lot more interesting. And then where Google lies on that spectrum, you can evaluate
*  for yourself. I'm not saying there are any angels in this room. I'm not saying there's anybody that
*  I trust, but there are levels. And again, we'll see what Altman says next week.
*  Their engineers follow your process and use your tools. They work with React, Next.js,
*  or your favorite front-end frameworks. And on the backend, they're experts at Node, Python,
*  Java, and anything under the sun. Full disclosure, it's going to cost more than the random person
*  you found on Upwork that's doing two hours of work per week but billing you for 40. But you'll get
*  premium quality at a fraction of the typical cost. Our engineers are vetted top 1% talent and actually
*  working hard for you every day. Increase your velocity without amping up burn. Head to
*  choose squad.com and mention Turpentine to skip the wait list.
*  Omniki uses generative AI to enable you to launch hundreds of thousands of ad iterations that
*  actually work customized across all platforms with a click of a button. I believe in Omniki
*  so much that I invested in it and I recommend you use it too. Use Cogrev to get a 10% discount.
*  We got this AI, the UK AI Safety Institute not quite getting the access. Now we have the
*  resignation clearly in protest. You're a good game design guy. How would you think about designing
*  that game so that the right people get the right kind of access to do the right kind of testing
*  and it doesn't collapse into something stupid? As soon as I emphasized, when I analyzed both the
*  preparedness framework and the responsible skilling policy for the tropic was these are
*  potentially very good policies. If the spirit of the rules is being honored, if the people at these
*  companies care about safety, they have a culture of safety and they don't just look at this in a
*  bunch of check boxes to get through so that they can get through compliance and release their thing
*  and satisfy the scolds. But they genuinely care about this result and when the answer comes back,
*  10 of you pass, but that's funny. The response is no, wait, stop. Think about what's going on.
*  Oh well, that wasn't okay. If something's going on here, we do investigate this. We need to stop this,
*  right? And we react accordingly. And certainly you can't do it if people are potentially gaming
*  to sabotage benchmarks to be like under worse of thresholds. If you're worried about them targeting
*  the test, they know what questions are going to be asked. They know what exactly is going to be
*  the attack surface that is checked and they strengthen that particular attack surface.
*  These things are dead, right? If all you have to do is verify the AI doesn't fall or cause a problem
*  in specific ways, I think you're just toast. And I think there's no set of tests that would be at any
*  reasonable cost that could possibly satisfy that. And that's why you need third party testing unless
*  you deeply trust the people who are doing this, right? Do you trust them not to teach the test?
*  Do you trust them to look for anything at all? Not just things that are specific.
*  Do you trust them to isolate these groups, right? I can imagine a role in which I would have that
*  trust, but after what we just saw, do you have that trust with OpenAI? I mean, I know that I do,
*  right? In the sense that I don't, definitely don't. But we saw that, like, you know, I trust their
*  benchmarks, right? When OpenAI comes up with a benchmark, I trust that they're not gaming the
*  benchmarks. They're not trying to do it in that way. I trust them for GPD5 in the same way that
*  I did previously. Now, maybe not as much, right? I don't know what direction they're going to do
*  these things in. But I think Colin Fraser was the first person to say, let's not assume they invented
*  the operating system from her until we get our hands on it. Let's not jump to conclusions
*  in any way, not to safety, but also capabilities. Because if you are a hype machine that is trying
*  to hype, that in a very different world, then what Chat TV was, right? Chat TV was just,
*  here is our very sterilely named, very simply presented, very clean. I still give major props
*  to them for not having glammed it up in various ways. But there are things to love about OpenAI.
*  But they just presented this thing. I'm like, hey, here's a cool thing. Let's see what you do with it.
*  We're not going to tell you how awesome it is. We're just going to put it out there. And GPD5
*  was the same way, mostly. And now, this is the third, I think, time they've gone out there and
*  gone, here are our amazing abilities, some of which we don't have yet. Hype, hype, hype.
*  And that's very different. And then Sam Altman went on Twitter the day after Google's I.O. to
*  gloat about how his hype was so much of a better vibe than Google's hype. But he hasn't addressed,
*  to my knowledge, any of the concrete things that Google announced or any comparisons as to who's
*  building the better product, even to just congratulate them on a great set of offerings
*  or anything like that. He's just like, oh, get ahold of the nerd, basically. They're trying so
*  hard. Isn't that lame? That's not a good sign either. Yeah, it's not great. It's not great all
*  the way around. And I would say I definitely strongly noticed the shift from the earlier
*  releases to the current releases. I would say last fall, the demo day, was really the first time
*  where it was like, and even that was more buttoned up than this one. But it was like,
*  the GPTs weren't really ready. They didn't really work that well. And it was the first time where
*  it felt like, man, you guys shipped this even though it wasn't really in shape to ship.
*  There's no safety concern, but they felt like vaporware, right?
*  Didn't work. Yeah, the retrieval was not good. I think that has been improved. But it was a few
*  months later that, and I need to do a little more testing with this myself, but what I've
*  been hearing in my app development circles is that the assistance API has rounded into form now,
*  where the retrieval actually does work much better than it originally did. And it's more like what
*  they described in that first release, but that's relatively recent and it's been a number of months.
*  And this one just seems even messier where it's like, they weren't even really clear on what we
*  were supposed to be getting. People are just all confused at the moment. I was confused. I was
*  trying to get Chetchu BT to modify a picture of me and my son, and it was not doing it right or
*  whatever. And then I was like, what's going on with this thing? And then I went into go on Twitter
*  and I see, oh, okay, I'm still using the old system. And it's just not clear to me what I'm
*  even dealing with. They're not updating a lot of the angles and they're not being clear. And
*  experienced by a lot of people complaining, oh, I just realized I'm not using the new version of
*  this thing. I understood it from what they were doing. I was being a journalist and being paying
*  very close attention, but to a normal person who's paying ordinary attention, it was very unfair.
*  Well, any other thoughts on, I guess one other question I had was, are there any new safety
*  related agendas or developments that you think are worth extra attention or that we should maybe be
*  increasing our bets on at the moment? I'm always looking out for something that seems like it
*  really could work. And I'm always struck by the fact that-
*  It's the most interesting thing to happen that I think is getting no attention in the alignment.
*  So did you hear about Sofon?
*  I don't think so.
*  So I mentioned this a few, I forget which week, how many weeks ago it is because time works. But
*  so the Chinese have proposed a technique called a Sofon because, you know, let's build a Sofon from
*  the ancient, from the dystopian offering the aliens of preface using Sofons. Did you read the book?
*  Anyone see the show? But the idea is you can track a model, open source or closed,
*  but in particular open source in a local maximum with respect to certain specified topics.
*  So I'll say if you attempt to fine tune it, to get it out of the local maximum, it won't work.
*  Ordinary fine tuning techniques to try and escape from a failure won't work. So the proposal was,
*  you could teach, you could actually teach, not just not, because like if you don't teach biology
*  to Lama 3 or Lama 4, let's say, Lama 4 doesn't learn biology. Well, there's only so much biology
*  that you have to learn, feed it a bunch of textbooks, suddenly it knows biology. Even
*  if you somehow manage to not have it learned by implication. But now the Sofon proposal is,
*  you can specifically teach it to not understand the biology and be really dense about it. The way
*  that certain people were like, I can't do math and just refuse to learn no matter how much you
*  teach them and how many examples you give them, because they had a trauma. You actually give the
*  thing trauma by metaphor. And so the thing is, if you can make sure that the thing can't learn
*  biology, now you've got an open model that can't give a biology, potentially, very early. We haven't
*  run it for its paces. We haven't tried it at scale. We haven't, but it's an idea. It's the beginning
*  of the first proposal I've ever made. That's in theory, maybe we could do something that raises
*  the cost above epsilon compared to the training child's model to take somebody's general model
*  and turn it to whatever specific end we have. Maybe this will start to require enough work
*  that we're not just making it easy. And if we can do that, now we still have the problem of you have
*  to enumerate all of the specific things you want it not to know. You have to figure out all the
*  things you want to stop it from knowing and block them. Again, very similar to what we saw in the
*  book, not a spoiler, but the idea of being, you see this in sci-fi all the time, right? You see
*  the villain or the oppressors or whatever it is. And they say, oh, all that matters to us is that
*  you don't do X, Y, Z. Because if you end up doing X, Y, Z, we're fine. And someone finds out a way
*  to do W, right? Someone finds out a way to do something that they don't detect. It doesn't
*  count to them. They just don't understand what's going on. And their response to that is to ignore
*  it. It constantly happens, right? People start doing weird shit, right? And the aliens that are
*  trying to take, they've taken over the enterprise or whatever it is. They go, I don't know what
*  weird shit is going on, but eh, whatever. Whereas the correct answer, of course, is I don't know
*  what weird shit is going on. So stop what you're doing until I know that's not okay. But, you know,
*  if you only can have it kind of scripted, like I detect you do bio weapons, you can't do nuclear
*  bombs, you can't do chemical weapons, you can't do cyber attacks, blah, blah, blah. Well, that's fine.
*  For now, it's incredibly helpful for level four, but it's not that helpful for level six,
*  even if it works, because it's no longer going to be the threat that you knew was coming. You're
*  not going to be able to enumerate what a smarter thing than you comes up with, right? So it works
*  up to a point, but it's still incredibly helpful. And it potentially raises the bar quite substantially
*  to the point where we can all reach an agreement if this works great. I don't know. But, you know,
*  if you want like a moment of hope or something, there are at least some proposals.
*  Yeah, that's good. I hadn't heard of that. And it definitely sounds like something I need to go do
*  a little more homework on. Anything else? Because you're one for one in terms of new and very
*  interesting pointers there. Yeah, I haven't seen that much in the alignment sphere lately,
*  unfortunately. I haven't seen new evidence that things won't work particularly. But it's just been
*  like relatively quiet, I would say on that level. I guess something has to be quiet,
*  right? You can't have everything happening all the time.
*  If you were to pitch a movie concept that you think would be most influential right now
*  in the way that like her seems to be inspiring the current moment of technology, mine might be
*  the social network meets the Lord of the Rings, where the sort of central figure would be the Sam
*  Altman type who is on this like meteoric rise of technology. But it's also being corrupted by it
*  in the way that the ring is corrupting. Here's an example just because the ring was a metaphor
*  for the actual AGI for decades. That seems like the story that we might need to all hear.
*  You used to talk about the fellowship taking the ring to Mordor as a sort of metaphor for some of
*  the things that might happen in some scenarios. But yeah, certainly you could tell that story.
*  I think if my instincts tell me that's not the most interesting approach to that, I think
*  if I was going to do it, I think I would maybe just do a very kind of straightforward AI takeover
*  scenario with not fit to smart intelligence anywhere. Just show the humans only giving up control
*  show the humans because in everyone's individual interests, no one can stop it.
*  Just show things just spiraling out of control. One thing leads to another.
*  There aren't even any idiots and there are no villains. Things just go wrong. And there's that.
*  You could also have a law and order artificial intelligence set in 2035. That'd be fun and
*  interesting. So the idea being that while there are not these open models have given everybody
*  these extra capabilities, we have to be very proactive about hunting down people who try to
*  implement catastrophic threats. And then obviously the police have all their AI is that everyone's
*  acting on a high level. But it's one of these things where you notice that the world almost
*  blows up every other week. And that's one of the challenges that I have with this in general is
*  I feel like the leap from here to there is tough. What does the procedural look like? Can you imagine
*  that getting concrete enough to be shown on TV in a way that... Procedural, right? Is Star Trek not
*  a procedural in its own way? We explore a strange new world. We find an ethical dilemma. We find a
*  technical... So we got a technological problem and we have our debates and then we encounter a setback
*  and then we implement our solution. We solve the dilemma and we go on our way. It's not that simple,
*  but it also is. And so you find a way to do a version of that potentially. But yeah, a fun game
*  for watching any sci-fi show is note how often things almost go horribly wrong. Just watch
*  a season of any Star Trek and watch how often the ship almost blows up. Or somehow the Federation is
*  almost in dire danger. And how often this happens because someone was being a complete idiot and how
*  often it happens so naturally, but they encounter all these different problems. And then ask yourself,
*  well, if you just looked at the 45-minute mark of every episode and you had to assign probabilities
*  that this wasn't a narrative someone wrote, what's the chance that humanity would have survived from
*  here? Or what's the chance the ship would have survived from here? What's the chance the Enterprise
*  actually makes for seven seasons? The answer is zero. It's so challenged in so many different
*  ways. And the Federation probably does too. The Federation is in a lot of trouble reasonably often.
*  Like, we can get out of it, we wrote it. But this is not a utopia in the sense that if you actually
*  were there, the safeguards aren't there. There's no robustness in this world. This world is fragile.
*  The Star Trek universe is so fragile. And so we get lucky a lot, but why are we getting lucky unless
*  it's the crew protecting us or the travelers or something, right? In a way that we don't understand.
*  And so you carry forward. The other game is like, how often does somebody come within five minutes
*  of building an ASI, right? Like, or how often would AI just run completely rampant here if you
*  didn't have the rule that mysteriously doesn't? And that also is like just these scenarios. So
*  you could all just have an ordinary sci-fi show where it just runs a normal sci-fi world, except
*  that every now and then, and by every now and then, one episode in three or something, that someone
*  like accidentally follows through on logic and super intelligence emerges and everybody dies,
*  or someone takes over the world, or everything is paper, or some new regime take happens, or there's
*  a there's a recursion. Who knows? I haven't thought this through. I'm brainstorming with you. But the
*  idea being that like, imagine if you got to just, and then of course the world just like you see the
*  rewind where everything goes back in reverse and then the person just not to do it. But like once
*  every episode or two, like somebody almost enters the world and they just decide not to. And there's
*  really no explanation why they don't, probably. Yeah, the sort of garden of forking paths is a
*  pretty interesting idea. I minded of the three body problem too, has part of its story kind of goes
*  that way where the civilization is being restarted and rerun over and over again. And it just ends
*  at various times and then it gets booted up again. But it's like they last different lengths
*  of time and some of them are short and others are longer, but they all kind of end and get rerun.
*  And I do think that would be an also a pretty interesting way to present the future that like,
*  some branches of this tree are terminal. Yeah, the three body problem is such a weird,
*  I don't want to spoil anything, but it's such a weird, I'm going to do my best not to, but
*  it's such a weird mix of this kind of fully cynical, hard realism beyond what I think is even
*  accurate. Where like the universe is this cold place that wants to kill you so badly and like
*  you can afford not the slightest bit of kindness and decency if you want to survive.
*  At the same time, like, you know, don't just all die. In some sense, in the,
*  I'm like, there's a book two, the book three, but it's just not the way the book exists. And the
*  book isn't like what happens to Tricelaris after we get wiped out. The book is about people
*  in some sense. So, okay, last question. Do you find yourself shifting at all in terms of your
*  sense of whether or not we may be in some form of simulation? Simulation hypothesis has always been
*  essentially all of the value lies in the world, lies in the places where we're not in one.
*  If there's, if you take, you make nine simulated copies of me and there's me and you put us in
*  10 copies of the situation, but one is real and the other nine will just be like
*  recorded in a videotape and viewed back later or something. Well, shouldn't I just act as if I'm
*  the real one? Isn't that just currently the correct strategy? Even if there's 9999 of them, maybe I
*  still have the correct strategy. Or rather, if you are the ancestor, this is an ancestor simulation.
*  Then 20,000 years later, we try to run a bunch of sims of the ancestors. Well, the ancestors who
*  decided they were in a sim, the ancestors who when faced with the ancestral situation, figured out
*  that given the situation, they were probably in an ancestor situation, ancestor simulation, and
*  therefore didn't need to actually make sure that they, that the civilization progressed to a point
*  where they could run the future ancestor civilization simulations. Those guys don't get simulated,
*  right? Because those civilizations don't make it. There's a real sense in which your only
*  simulation hypothesis is only valid if you treat it as invalid or something like that, right? You
*  have to take the situation seriously. And also what's the point of a simulation where the person
*  finds out and acts like it's true? There are a bunch of movies like that, right? No spoilers,
*  even naming them. But in general, the point is to treat it as real. The whole goal is to treat it
*  as real. And I don't really see any, I don't know, right? Obviously there is some probability
*  this is a sim of some kind. I can't rule it out. I will sometimes jokingly refer to things
*  in that kind of way. The writers were a bit on the nose today in one of my things I'll sometimes say,
*  but you can't take it seriously in the sense of changing your mind.
*  Soterios Johnson What about moving to the Caribbean and unplugging? I just saw an interesting tweet
*  from Amanda Eskel from Anthropic the other day where she said, I don't think AI is definitely
*  going to kill us all. I'm not a doomer. If I were, I wouldn't be working on this. And she was also
*  kind of like, I do think it's a real risk, but I think it's something that we can shape and hopefully
*  I can have an impact on. But if I really was a doomer, I would just head to the Caribbean and
*  spend the rest of my days there. I do know I do have a close friend who basically has that attitude
*  that he's like, I just want to enjoy the good times that we have and not worry about it too much.
*  And then she also said the downside of this or flip side of this is if I ever do get burned out
*  and decide to take some time off in the Caribbean, people will take it as a sign of doom.
*  Soterios Johnson I saw that too. I think it was Jeffery Miller, I'm not sure exactly who it was
*  though, who said it wouldn't work if the food would turn to ash in your mouth. You wouldn't get no joy
*  because you would know this. And I think that that's largely true for me. I think no one,
*  I just walked away from this thing and was ignoring it. It just wouldn't sit well with me and I
*  wouldn't be able to just go, he didn't us. It just wouldn't work. Right. You want Mr. Reagan to plug
*  back into the matrix. Well, you need your memory kind of wiped in some sense. You need to really
*  not notice for some people where I kind of enjoy, I enjoy fighting. I enjoy struggling. I enjoy
*  striving to do better and to solve problems. That's what my thing is. I would never go to
*  the Caribbean because what am I doing in the Caribbean? Right. I'm just being bored a week
*  anyway. But unless I'm just like hosting, making sports betting, bookmaking decisions again,
*  that's the only reason I've ever been to the Caribbean and had a good time. So there you go.
*  But I think it highlights, by the way, the fact that the word doomer is just a slur and has been
*  completely misappropriated and misallocated. Right. Because who is a real doomer? The doomer
*  is the person who says that there's nothing to be done. That what we do doesn't matter. Right.
*  The doomer is the person whose P-doom is 0.9 bar or otherwise just it's all over. There's nothing
*  you can do. And when you see those people on climate change, doomers who think that humanity
*  is doomed and there is nothing you can do about it. Your decision doesn't matter. Whereas if you
*  think your decision matters, as a man to point out, that makes you not a doomer. Yeah. So what if
*  we could lose? You could win and you can help fight. It's an agent you dig idea. Right. The
*  universe hangs in the balance. Right. The scales oscillate and it could be up to you which way they
*  were good and evil and God's judgment. You can decide which way this goes. And obviously this
*  isn't a God's judgment thing. It's not a, you know, there isn't a moral tone to this. It's
*  about solving a problem. But you know, if you take it from 34.007% to 34.008% chance of victory,
*  that's a great life. Right. In some sense. Look at all the utils, including just for you.
*  Imagine what happens. But yeah, if you don't have any way to interact with the problem,
*  then you just decide to go off and do something else. Makes perfect sense to me. And you can't
*  burn out. What Amanda needs to do once a year is sit my ties on a beach in the Caribbean for two
*  weeks so that she can regain her mental health and she can go back and resume. That you should do
*  that. He's doing for an entire year after five years of working because otherwise she won't be
*  able to think clearly or make a decision. You should do that. Isn't there's nothing wrong with
*  understanding of the limits. Right. Like we can't, like there's not all the fight. Right. And I is,
*  I think an effort to like work on other things, think about other things. I mean, family,
*  try to have fun. I'm going to Madison Square Garden tonight. I'm with my old magic friend.
*  And we're going to have a walk. We're going to do a watch party where 40,000 of us Knicks fans are
*  going to watch game six on a video screen because they're in Indiana. It's gonna be fun as hell.
*  Right. And I don't claim that I'm saving the world here. I'm not. I just want to reflect that.
*  Yeah. I feel you. I think it's probably a good place to end it. Any other closing thoughts? Indeed
*  you many amazing and wonderful things and weird things come to pass and best of luck to everyone.
*  And I looked forward to, you know, all this response and where all these people land next,
*  you know, like who are, you know, this is some great talent. Someone's going to snap them up
*  or they're going to do something. So we'll see what happens. No doubt about that. Well, the saga will
*  continue. But for now, I appreciate the extra time today.
*  Svi Moshowitz, thank you for being part again of the cognitive revolution.
*  Really? It's been a pleasure.
*  It is both energizing and enlightening to hear why people listen and learn what they value about
*  the show. So please don't hesitate to reach out via email at tcr at turpentine.co or you can
*  DM me on the social media platform of your choice.
