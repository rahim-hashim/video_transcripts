---
Date Generated: December 26, 2024
Transcription Model: whisper medium 20231117
Length: 7875s
Video Keywords: []
Video Views: 4627
Video Rating: None
Video Description: In this episode of The Cognitive Revolution, Nathan interviews Emad Mostaque, former Founder and CEO of Stability AI and Founder of The Intelligent Internet. We explore humanity's future with AI, from the stark 50-50 survival odds to Emad's optimistic vision for universal basic intelligence. Join us for a fascinating discussion about open-source AI infrastructure, the three-tier system of the Intelligent Internet, and how blockchain technology might help fund global public goods in AI development.

Check out Emad's publications on:
Emad's Twitter: https://x.com/emostaque
Emad's Blog: https://emad.posthaven.com/
Intelligent Internet Substack: https://intelligentinternet.substack.com/p/ii-intelligent-internet-primer?r=47215t&utm_campaign=post&utm_medium=web&triedRedirect=true

The Cognitive Revolution Ask Me Anything and Listener Survey: https://docs.google.com/forms/d/1aYv2XLID7RqGxj2_Y4_6x9mo_aqXcGCeLw1EQhy4IpY/edit

SPONSORS:
GiveWell: GiveWell has spent over 17 years researching global health and philanthropy to identify the highest-impact giving opportunities. Over 125,000 donors have contributed more than $2 billion, saving over 200,000 lives through evidence-backed recommendations. First-time donors can have their contributions matched up to $100 before year-end. Visit https://GiveWell.org, select podcast, and enter Cognitive Revolution at checkout to make a difference today.

SelectQuote: Finding the right life insurance shouldn't be another task you put off. SelectQuote compares top-rated policies to get you the best coverage at the right price. Even in our AI-driven world, protecting your family's future remains essential. Get your personalized quote at https://selectquote.com/cognitive

Oracle Cloud Infrastructure (OCI): Oracle's next-generation cloud platform delivers blazing-fast AI and ML performance with 50% less for compute and 80% less for outbound networking compared to other cloud providers13. OCI powers industry leaders with secure infrastructure and application development capabilities. New U.S. customers can get their cloud bill cut in half by switching to OCI before December 31, 2024 at https://oracle.com/cognitive

80,000 Hours: 80,000 Hours is dedicated to helping you find a fulfilling career that makes a difference. With nearly a decade of research, they offer in-depth material on AI risks, AI policy, and AI safety research. Explore their articles, career reviews, and a podcast featuring experts like Anthropic CEO Dario Amodei. Everything is free, including their Career Guide. Visit https://80000hours.org/cognitiverevolution to start making a meaningful impact today.

CHAPTERS:
(00:00:00) Teaser
(00:00:36) About the Episode
(00:04:33) Intro
(00:09:15) AI Risk
(00:16:58) Sponsors: GiveWell | SelectQuote
(00:19:48) AI Goals
(00:23:52) AI Divergence
(00:27:50) AI & Agency
(00:32:18) Sponsors: Oracle Cloud Infrastructure (OCI) | 80,000 Hours
(00:34:57) Kids & AI
(00:39:50) Intelligent Internet
(00:48:37) Open vs. Closed AI
(00:53:30) AI Runaway
(01:01:46) Building the Future
(01:05:43) Energy & AI
(01:15:19) Hypernodes
(01:27:36) Proof of Beneficial Compute
(01:38:28) Distributed Compute
(01:45:10) Intelligent Internet Company
(01:48:37) Finding Talent
(01:55:33) Pause Letter
(01:59:50) Regulation
(02:04:04) Speed Limits
(02:06:42) Data Filtering
(02:10:54) Outro

SOCIAL LINKS:
Website: https://www.cognitiverevolution.ai
Twitter (Podcast): https://x.com/cogrev_podcast
Twitter (Nathan): https://x.com/labenz
LinkedIn: https://www.linkedin.com/in/nathanlabenz/
Youtube: https://www.youtube.com/@CognitiveRevolutionPodcast
Apple: https://podcasts.apple.com/de/podcast/the-cognitive-revolution-ai-builders-researchers-and/id1669813431
Spotify: https://open.spotify.com/show/6yHyok3M3BjqzR0VB5MSyk
---

# Emad Mostaque on the Intelligent Internet and Universal Basic AI
**Cognitive Revolution:** [December 25, 2024](https://www.youtube.com/watch?v=SEd3hzuJ-Wk)
*  We're at this point whereby if we don't have AI to help us, I think we're in a very bad scenario. [[00:00:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=0.0s)]
*  With AI, if we don't build it right, we're also in a bad scenario. [[00:00:05](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5.84s)]
*  If AI is built right and is aligned and works with us, then we're in a very good scenario. [[00:00:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=9.76s)]
*  Who owns that AI that is your best friend, that is your kid's best friend? [[00:00:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=14.64s)]
*  And how is it constructed? And do we have visibility into that? [[00:00:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=19.12s)]
*  We're going to see economic disruption because the labor theory of productivity is about to [[00:00:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=22.48s)]
*  be challenged. The world becomes a better place, ironically, through empathy from AI. [[00:00:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=26.0s)]
*  And that's infrastructure that should be available to everyone. This is [[00:00:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=31.28s)]
*  a concept we call universal basic AI. Hello, and welcome back to the Cognitive Revolution. [[00:00:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=34.16s)]
*  Today, my guest is Ahmad Mostak, previously founder and CEO of Stability AI and now founder [[00:00:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=39.76s)]
*  of the Intelligent Internet. I've been following Ahmad's work since before Stability released [[00:00:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=45.28s)]
*  Stable Diffusion back in 2022. And looking back, I think it's fair to say that that release [[00:00:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=50.72s)]
*  did more than any other, aside from Chatch GPT, to kickstart the current AI moment. [[00:00:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=55.76s)]
*  Certainly, when it comes to inspiring researchers and builders all over the world [[00:01:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=61.12s)]
*  by demonstrating that open source public good AI projects can advance the state of the art across [[00:01:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=64.96s)]
*  multiple domains, Stability was for quite a while in a league of its own. Since then, [[00:01:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=70.32s)]
*  Stability, the company, has had a famously bumpy road as it's tried to evolve from an open research [[00:01:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=76.4s)]
*  lab to a sustainable for-profit business. But Ahmad's ability to articulate a positive vision [[00:01:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=80.96000000000001s)]
*  for humanity's AI future, while simultaneously grappling with unprecedented risks, remains to me, [[00:01:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=86.48s)]
*  uniquely compelling. We begin today on the risk side with Ahmad's stark assessment that humanity [[00:01:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=92.96000000000001s)]
*  faces roughly 50-50 odds of survival as AI systems become more capable and ubiquitous. [[00:01:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=99.28s)]
*  Imagining a world in which intelligent robots are everywhere, creating acute risks from misuse, [[00:01:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=106.64s)]
*  new kinds of hard-to-model systemic risks, and even the potential for surprising emergent [[00:01:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=112.08000000000001s)]
*  behaviors from the AIs themselves, and also considering how the world's largest companies [[00:01:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=116.32000000000001s)]
*  and most powerful nations are now racing to secure AI-relevant resources and establish [[00:02:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=121.2s)]
*  strategic advantages, perhaps surprisingly, Ahmad doesn't have much hope for traditional regulation [[00:02:05](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=125.68s)]
*  or even a pause in AI development. Nevertheless, rather than wallowing in P-Doom, Ahmad remains [[00:02:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=131.12s)]
*  in the arena. Today, he argues that we already have AI models powerful enough to dramatically [[00:02:18](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=138.08s)]
*  improve most human lives, if we can make them accessible, trustworthy, and aligned with local [[00:02:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=144.4s)]
*  contexts and values. With that in mind, his new project, the Intelligent Internet, aims to create [[00:02:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=151.12s)]
*  open, standardized AI infrastructure focused on critical regulated industries like healthcare [[00:02:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=156.96s)]
*  and education. The vision involves three tiers working together. National or international [[00:02:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=162.4s)]
*  hypernodes that maintain bodies of common knowledge and perform large-scale training runs, [[00:02:48](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=168.96s)]
*  distributed nodes that perform domain-specific fine-tuning and provide related services, [[00:02:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=173.76000000000002s)]
*  and finally, personal AI assistants that run locally on consumer devices. By making the [[00:02:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=178.64000000000001s)]
*  entire stack, including the bulk of the training data, open source and transparent, while preserving [[00:03:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=184.56s)]
*  local control over the models that people use on a daily basis, Ahmad aims to create AIs that [[00:03:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=189.6s)]
*  enhance rather than displace human agency. Of course, training large language models can be [[00:03:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=194.88s)]
*  expensive, and in general, global public goods often require non-traditional funding mechanisms, [[00:03:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=201.28s)]
*  so Ahmad is planning to launch a crypto project that will allow and hopefully incentivize people [[00:03:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=206.48000000000002s)]
*  to support the project through a mechanism he's currently calling Proof of Beneficial Compute. [[00:03:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=211.6s)]
*  I've personally never been much into crypto, and I'm certainly not giving out any investment advice [[00:03:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=218.72s)]
*  here, but I find the idea of universal basic intelligence, or UBAI as Ahmad calls it, [[00:03:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=223.2s)]
*  so compelling that I do intend to buy a few tokens. Not out of hope for some future financial return, [[00:03:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=229.35999999999999s)]
*  but just because I want to see this project succeed. [[00:03:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=235.84s)]
*  If you're finding value in the show and want to see us succeed, we always appreciate it when [[00:04:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=240.72s)]
*  listeners share the show online, write reviews on Apple Podcasts or Spotify, or leave comments on [[00:04:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=244.48000000000002s)]
*  YouTube. And we always welcome your feedback. You can DM me on your favorite social network anytime, [[00:04:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=249.76s)]
*  or visit cognitiverevolution.ai, where you can still submit questions for our upcoming AMA episode [[00:04:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=254.88s)]
*  for at least a little bit longer. Now, I hope you enjoyed this conversation, [[00:04:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=260.72s)]
*  which I've been looking forward to since before I started this show with Ahmad Mustaq, AI visionary [[00:04:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=265.12s)]
*  and founder of the Intelligent Internet. Ahmad Mustaq, previously founder and CEO of Stability AI, [[00:04:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=271.28000000000003s)]
*  now founder of the Intelligent Internet. Welcome to the Cognitive Revolution. [[00:04:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=277.68s)]
*  Thanks for having me. I'm excited for this. You were actually on my list of target guests from the [[00:04:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=281.68s)]
*  very first episode of the podcast, which has been almost two years and 200 episodes now. [[00:04:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=286.96s)]
*  So glad to finally be making this happen in depth and lots to cover. I thought we would start [[00:04:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=292.32s)]
*  with a recent tweet that you put out that certainly got some people talking in which you said that [[00:04:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=299.76s)]
*  your P-Doom is 50%. I would love to hear where you see that coming from, maybe a little bit about [[00:05:05](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=305.03999999999996s)]
*  how you see that unfolding. People often struggle to visualize the details or have any sort of [[00:05:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=314.08s)]
*  concrete sense of how that might go down and what you see as kind of the drivers of that risk right [[00:05:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=319.03999999999996s)]
*  now. Yeah, I think the whole P-Doom question is an interesting one because it's very tough to think [[00:05:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=324.8s)]
*  what can wipe out humanity as it were. What is the probability of doom from an expected G30 calculation? [[00:05:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=330.15999999999997s)]
*  The way that I kind of viewed it was basically, AI said it was an indeterminate period of time. [[00:05:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=336.24s)]
*  Some people say by 2030, by 2050, by 2050. I was like, on an indefinite period of time, [[00:05:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=342.64s)]
*  it becomes very interesting because it's clear now that we have AIs that outperform humans on [[00:05:48](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=348.0s)]
*  narrow tasks and a whole bunch of them probably outperform humans on general tasks. And now they're [[00:05:54](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=354.96s)]
*  being embodied with this leap forward in robotics that we're seeing across the world as well. [[00:05:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=359.59999999999997s)]
*  So when I combine all those together, what I basically see is a road in which you have [[00:06:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=364.47999999999996s)]
*  increasingly complex systems that are outside of our control that can stabilize all the things [[00:06:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=370.0s)]
*  that we do and maybe P-Doom without this technology is even higher, which we can get to in a second. [[00:06:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=376.32s)]
*  And another road in which this is built in a very haphazard way, just like the internet [[00:06:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=382.08s)]
*  starting to break down now. And then there are some very clear scenarios where I see [[00:06:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=386.32s)]
*  a very reasonable path to doom as it were. Yeah, there's a lot of scenarios under which we don't [[00:06:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=392.16s)]
*  fare so well. I guess how much of that do you think is attributable to some emergent property of [[00:06:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=398.24s)]
*  AIs versus some just total mistake where nobody was intending anything bad versus misuse or abuse by [[00:06:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=407.92s)]
*  a human malevolent actor versus maybe other categories if you happen? [[00:06:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=417.44s)]
*  Well, I think what we're seeing right now is modularized AI and componentized AI. So you're [[00:07:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=422.8s)]
*  seeing AIs at all different levels. And with the advances in test time compute, you're seeing again [[00:07:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=428.24s)]
*  0.1, $200 versus GPT-40, et cetera. You're starting to see that go. Like, why am I so [[00:07:13](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=433.84000000000003s)]
*  used to spending $200,000 a year on inference, right? Is there any difference between a human [[00:07:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=439.2s)]
*  bad actor and an AI bad actor that can then direct other humors and AIs? I don't really [[00:07:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=445.12s)]
*  think there is, right? It's just in the probability of millions of humans, millions of agents, how many [[00:07:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=449.76s)]
*  bad actors are there and are they in a position they can cause a cascade? That's the deliberate [[00:07:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=457.36s)]
*  side of things. And again, it can be a whole variety of things. We've seen doom cultists, [[00:07:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=461.76s)]
*  we've seen other things. In terms of emergent, the AI deciding, oh, let's paperclip humanity [[00:07:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=466.4s)]
*  or maybe more aggressive than that. Who knows? It's just, again, what we have is this combination [[00:07:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=473.03999999999996s)]
*  of classical systems that are like slow dumb AI and can be directed by orders from the top. [[00:07:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=479.12s)]
*  These emergent systems now, there are somewhere in between AI enhanced, and then you have full on [[00:08:05](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=485.28000000000003s)]
*  AI systems end to end. And somewhere in that, there's a lot of room for breakage and a lot of [[00:08:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=490.48s)]
*  room for damage. Again, P-Doom describes actual doom of humanity, which is something that's quite [[00:08:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=495.28000000000003s)]
*  difficult in many ways, unless you've got 10 billion robots. And again, what's the [[00:08:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=501.04s)]
*  interconnective fiber that like to be an upgraded version of the internet if that? [[00:08:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=507.44s)]
*  If we make that secure, then we can eliminate doom scenario. Then you have the whole bio weapons [[00:08:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=511.44s)]
*  and other stuff and those require normal deliberateness versus accident. But again, [[00:08:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=515.92s)]
*  we're not doing so well ourselves as humans in keeping things together. You can see things [[00:08:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=521.68s)]
*  fracturing apart. And so even though I say it might be 50, it's probably higher without robots, [[00:08:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=526.08s)]
*  without AI than that. It's just, I think that again, if we break this down, we're at this point [[00:08:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=531.52s)]
*  whereby if we don't have AI to help us, I think we're in a very bad scenario. With AI, if we don't [[00:08:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=537.36s)]
*  build it right, we're also in a bad scenario. If AI is built right and is aligned and works with us, [[00:09:03](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=543.4399999999999s)]
*  then we're in a very good scenario. So I know you have a big vision for that future, [[00:09:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=549.36s)]
*  which I appreciate. And I think that's where we're going to spend most of our time today. But maybe [[00:09:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=555.12s)]
*  just to motivate that a little bit more. Certainly, I think the vision is, as we say in the AI space [[00:09:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=559.52s)]
*  now, strikingly plausible that there would be a humanoid robot in every home. Certainly no less [[00:09:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=567.04s)]
*  than Elon Musk is talking about as many humanoid robots as we have humans running around. And [[00:09:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=572.4s)]
*  if the progress there is anything like it has been in language modeling over the last couple [[00:09:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=579.4399999999999s)]
*  years, or for that matter, in image generation, video generation, then they will be quite capable [[00:09:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=584.1600000000001s)]
*  quite soon. Is there anything within the current paradigm? Meaning if we were not going to [[00:09:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=590.0s)]
*  totally rethink AI development, as you also are daring to do, but if we were just going to say, [[00:09:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=598.8000000000001s)]
*  are there specific local very bad decisions that you see people making that you think are [[00:10:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=604.88s)]
*  like leverage points that are causing a lot more risk than we could have if we were making [[00:10:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=612.3199999999999s)]
*  better decisions in those key places? Yeah, I think there are some key areas that we're seeing [[00:10:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=620.56s)]
*  maybe suboptimal decision making. I think first off, we're having privatized closed source models, [[00:10:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=627.52s)]
*  or even open weight closed data models emerging and being used widely. Like the most downloaded [[00:10:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=635.4399999999999s)]
*  models on Hugging Face now are the Quen models, which are fantastic in performance. No one's got [[00:10:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=640.96s)]
*  any idea what's inside that. Lama is the next one. And we've seen from Anthropic, Superagent paper [[00:10:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=645.44s)]
*  and others, you can poison these models reasonably quickly and in an undetectable and untunable way. [[00:10:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=652.72s)]
*  No one's quite figured out what the defense against those attacks are. So we're watching these into [[00:10:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=657.9200000000001s)]
*  decision making and augmenting situations where it's probably not quite ready. We need to have more [[00:11:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=662.08s)]
*  robust elements there. Similarly, we don't have real standards for robotics as robots start to get [[00:11:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=668.24s)]
*  rolled out. And again, it'll take a few years for these things to go up. But just from an economic [[00:11:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=674.48s)]
*  theory perspective, there'll be no reason just like there's no reason to hire call center workers [[00:11:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=677.84s)]
*  anymore, honestly, not to have a robot. And if we don't have protocols and defenses there, that's [[00:11:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=682.32s)]
*  going to be very difficult. When we come to deepfakes and things like that, I'm not as concerned [[00:11:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=687.36s)]
*  except for the key elements of audio and speech. Because I think the ability of speech to manipulate [[00:11:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=691.92s)]
*  and control is massively underestimated. Like if you've ever seen an amazing orator [[00:11:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=700.56s)]
*  and some amazing speeches of history, you can see the impact of human voice. Like if you've had a [[00:11:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=707.12s)]
*  fishing port like I have from my mother or mother saying I'm in trouble with her exact voice pattern, [[00:11:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=710.88s)]
*  you can see again the impact of that. And so I think there probably used to be some regulation [[00:11:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=716.56s)]
*  around speech just due to its ability to persuade control and customize as well. [[00:12:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=720.72s)]
*  I'd say those are probably the areas that I'm most concerned about versus very large scale model [[00:12:05](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=725.44s)]
*  emergence, race away behavior. But if there was something in race away behavior, I think [[00:12:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=730.08s)]
*  probably less concerned about the labs versus maybe some of the decentralized stuff, because [[00:12:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=736.48s)]
*  that can actually go very quickly. Like we've had one of the first agents from hyperbolic that can [[00:12:23](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=743.12s)]
*  provision its own computer now. And I think we're moving to decentralized autonomous organisms [[00:12:28](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=748.24s)]
*  effectively. Actually, there's probably one exception to that, which is that if you look [[00:12:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=753.92s)]
*  at OpenAI, Anthropik and others, and the relationships with Angiro and the defense [[00:12:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=758.08s)]
*  industrial complex, that's probably not going to create a live AI. Like many listeners may [[00:12:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=763.04s)]
*  remember the slaughter bots, mini documentary about drones, swarms, basically killing the bad guys. [[00:12:48](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=768.48s)]
*  Anyone the government said was a bad guy at the end. Just look at Andrews latest commercial. [[00:12:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=776.0s)]
*  And then if you think about entire teams at the top AGI labs, now being put on to defense, [[00:13:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=780.56s)]
*  you're probably going to have models that are aligned to command control and more extreme things [[00:13:05](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=785.68s)]
*  versus models that are aligned to being helpful, humane and others. So I think that is again, [[00:13:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=792.4s)]
*  something that's somewhat inevitable, but potentially dangerous, particularly when they [[00:13:18](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=798.48s)]
*  use an offensive capability perspectives. Like we're starting to see the first AI pen attacks [[00:13:23](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=803.2s)]
*  and other things like that, utilizing generative AI, and they're getting very good very quickly. [[00:13:28](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=808.88s)]
*  I definitely recommend the slaughter bots documentary for anyone who's having a hard time [[00:13:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=813.92s)]
*  imagining how dystopian some of this stuff could get. And it's crazy to think about [[00:13:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=819.28s)]
*  generative AI. I mean, obviously, we don't know a lot of the details about what these partnerships [[00:13:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=826.88s)]
*  between the open AIs and entropics and the defense contractors look like or exactly what [[00:13:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=830.88s)]
*  they're working on. But it doesn't take too much imagination, especially in light of recent results [[00:13:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=836.4s)]
*  on O1 and other frontier models scheming against their human users to imagine that that could get [[00:14:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=841.84s)]
*  really weird really quick. I mean, I kind of quipped what it's like definitely more than a quip [[00:14:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=849.2s)]
*  that if I was a soldier being asked to go into some sort of combat scenario with an AI system, [[00:14:13](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=853.4399999999999s)]
*  at a minimum, I would want to know that its scheming tendencies have been fully characterized [[00:14:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=859.76s)]
*  and resolved before I'm out there as sort of the pawn between the, I mean, there's so many forces [[00:14:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=864.48s)]
*  at that point acting on me, right? You've got the enemy, but with friends like a scheming, [[00:14:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=870.4s)]
*  lethal, autonomous, semi-autonomous AI, like who needs enemies? It gets pretty weird pretty fast. [[00:14:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=874.56s)]
*  But even if we take a step back and we think about the first principles of this, [[00:14:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=880.96s)]
*  all wars are based on the lie that humans aren't humans. And any reasonable intelligence can [[00:14:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=884.88s)]
*  determine that, especially in O1 level intelligence. So in order to get an intelligence to kill humans [[00:14:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=891.52s)]
*  or to authorize that type of thing or to aid that, you basically have to teach it to lie. [[00:14:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=899.4399999999999s)]
*  Because again, just a step back would say that this is ridiculous. So you have more narrow AI [[00:15:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=906.16s)]
*  and you're lobotomizing it to not have higher order thoughts. So Elon talks about the maximally [[00:15:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=910.24s)]
*  truth seeking. You're actually telling it not to seek truth and instead to optimize for [[00:15:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=916.32s)]
*  casualty decreased on your side, max casualties on the other side, for example. The moment you [[00:15:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=922.32s)]
*  start doing that with large scale and you have swarms of cyber attack agents and others, [[00:15:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=926.88s)]
*  and you give them a level of autonomy, it gets very, very dangerous very, very quickly, [[00:15:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=932.4s)]
*  because the intent with which you build these is very important. It's like having military training [[00:15:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=936.16s)]
*  camps. It's the curriculum that you teach the soldiers, just like the curriculum that you teach [[00:15:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=941.28s)]
*  the AIs. And the emergence of these at an industrial scale becomes something very dangerous. [[00:15:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=946.8s)]
*  And it's part of a bigger trend whereby nations have this concept of capital stock in economics, [[00:15:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=952.8s)]
*  right? Where we've got our factories, we've got our roads, we have the means of productivity, [[00:15:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=959.28s)]
*  and this labor theory of productivity, whereby you've got this chart of GDP per capita versus [[00:16:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=962.64s)]
*  energy use. Like you've got energy and then it's just how much infrastructure do you have. [[00:16:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=969.6s)]
*  From a defense perspective, it was how many F-35s did you have or how many submarines did you have? [[00:16:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=974.24s)]
*  Soon it will be how many GPUs do you have to engage in information warfare? And then how [[00:16:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=979.52s)]
*  many drones do you have and other things? And again, if we're putting hundreds of billions [[00:16:23](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=983.92s)]
*  into that, as defense budgets will go, and we're training AI that is not truth-seeking [[00:16:28](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=988.32s)]
*  and that is offensive against humans, that just feels like it could end up in some very bad places, [[00:16:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=993.6s)]
*  right? Because we don't really understand them yet. And again, it all was done from this lie. [[00:16:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1001.2800000000001s)]
*  So we have to embed that from the start. It's like the opposite of Asimov's laws of robotics, [[00:16:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1007.44s)]
*  Asimov's laws of robotics, right? Hey, we'll continue our interview in a moment after a word [[00:16:54](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1014.7199999999999s)]
*  from our sponsors. How deep do you go to seek out an answer to a question? If you listen to this [[00:17:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1021.76s)]
*  podcast, there's a good chance you're the kind of person who spends hours clicking the source links [[00:17:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1028.0s)]
*  on Wikipedia pages or who's checked out the entire shelf on a niche topic at your library. If you're [[00:17:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1032.3999999999999s)]
*  nodding along, then you'll definitely want to check out GiveWell, an organization that [[00:17:18](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1038.3999999999999s)]
*  researches questions about global health and philanthropy, even if a satisfying answer might [[00:17:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1042.88s)]
*  require years of reviewing studies, talking to experts, and chasing down footnotes. [[00:17:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1047.44s)]
*  GiveWell has now spent over 17 years researching charitable organizations and only directs funding [[00:17:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1052.72s)]
*  to a few of the highest impact opportunities they've found. Over 125,000 donors have used [[00:17:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1058.64s)]
*  GiveWell to donate more than $2 billion, and rigorous evidence suggests that these donations [[00:17:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1065.0400000000002s)]
*  will save over 200,000 lives. GiveWell wants as many donors as possible to make informed decisions [[00:17:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1070.64s)]
*  about high-impact giving. You can find all of their research and recommendations on their site [[00:17:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1077.6000000000001s)]
*  for free. You can also make tax-deductible donations to their recommended funds or charities, [[00:18:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1082.64s)]
*  and GiveWell does not take a cut. If you've never used GiveWell to donate, you can have your donation [[00:18:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1087.44s)]
*  matched up to $100 before the end of the year or as long as matching funds last. To claim your match, [[00:18:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1092.88s)]
*  go to GiveWell.org and pick Podcast, then enter the Cognitive Revolution at checkout. Make sure [[00:18:18](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1098.8s)]
*  they know you heard about GiveWell from the Cognitive Revolution to have your donation matched. [[00:18:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1104.3999999999999s)]
*  To donate or learn more, visit GiveWell.org. [[00:18:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1109.28s)]
*  There are so many things in life we just never get around to. Taking up that hobby, cleaning out the [[00:18:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1115.6s)]
*  garage, you know, little things that don't really make huge differences in our lives. Yet there's [[00:18:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1120.96s)]
*  one thing that most of us have probably been neglecting that can have a huge impact on our [[00:18:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1126.64s)]
*  family's future. It's life insurance. And with select quote, getting covered with the right policy [[00:18:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1131.1200000000001s)]
*  for you is easier and more affordable than you might think. As someone who tracks AI progress on [[00:18:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1136.3200000000002s)]
*  a full-time basis and obsesses about its potential impact nonstop, I know how tempting it can be to [[00:19:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1141.92s)]
*  ignore more mundane, familiar risks. There's always another paper to read, podcast to listen to, [[00:19:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1147.3600000000001s)]
*  or product to try. And yet the smartest people that I know in the AI space continue to save and [[00:19:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1152.64s)]
*  invest money for the future, carve out time for their relationships, maintain their physical and [[00:19:18](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1158.0800000000002s)]
*  mental health, and yes, protect their family with life insurance, just in case anything should happen [[00:19:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1162.4s)]
*  before the singularity. If nothing else, it's one less thing to worry about in a time of unprecedented [[00:19:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1167.76s)]
*  change. So get the right life insurance for you, for less, at selectquote.com slash cognitive. [[00:19:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1173.1200000000001s)]
*  Go to selectquote.com slash cognitive today to get started. That's selectquote.com slash cognitive. [[00:19:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1179.44s)]
*  I would love to hear your thoughts on what the right goals are. And this is maybe something we can [[00:19:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1189.44s)]
*  flesh out in multiple tiers of AI development, but you seem to be in that, you know, in your comments, [[00:19:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1196.3200000000002s)]
*  they're reasonably sympathetic to the idea of truth seeking AI as like a pretty good goal. [[00:20:03](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1203.8400000000001s)]
*  If you talk to hardcore AI safetyists or do-mers, you will hear arguments like even that is not [[00:20:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1208.96s)]
*  going to save us because if you tell the AI go maximally seek the truth, you end up perhaps with [[00:20:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1216.8000000000002s)]
*  sufficient power on the AI side, which is obviously a big assumption. You end up with some still like [[00:20:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1222.5600000000002s)]
*  paperclip like scenario where it's like, the best way for me to seek the fundamental truth of the [[00:20:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1227.6000000000001s)]
*  universe is to quiet all the noise on the planet and build the biggest particle accelerator or [[00:20:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1232.0800000000002s)]
*  whatever. Right. But it's like even that at some like extreme push, you know, potentially becomes [[00:20:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1237.76s)]
*  incompatible with humans. Alternatives we have are like basically virtue ethics training from [[00:20:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1243.12s)]
*  anthropic. And then we have just sort of blind like reinforcement learning from feedback, [[00:20:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1250.16s)]
*  which doesn't seem like it's going to end super well for us either. If you had to like stack rank [[00:20:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1255.44s)]
*  or maybe just give your favorite set of goals that you would want to embody into AI, what would you [[00:21:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1260.0s)]
*  say those are and how satisfied are you with your current answer there? Yeah, I've been thinking [[00:21:05](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1265.9199999999998s)]
*  about this a lot to have my break. I think for me, it's about agency and building AI that enhances [[00:21:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1271.52s)]
*  human agency, community agency and societal agency so that we can achieve what we want to achieve in [[00:21:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1277.84s)]
*  a way that doesn't get in the way of others. Like the golden rule comes as an element of that, [[00:21:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1284.08s)]
*  which is common across all faiths. Do untie those as you would have done unto yourself. [[00:21:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1287.76s)]
*  If we want to specify it more directly, we can actually say it's about children's agency [[00:21:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1292.64s)]
*  because children don't have agency. And so a large amount of rights can be aligned to the [[00:21:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1297.44s)]
*  rights of children, climate, war, suffering, hunger, like children should be taken care of. [[00:21:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1302.64s)]
*  So they're allowed to enable their potential and we should be maximizing the agency or the children [[00:21:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1309.52s)]
*  that then knocks onto the adults because what you want again is for people, as long as it doesn't [[00:21:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1315.52s)]
*  get in the way of the rights of others to be enabled to do whatever they want to do and achieve [[00:22:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1320.0s)]
*  whatever they want to achieve. And I wrote a piece, How to Think About AI, where I said this is [[00:22:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1324.32s)]
*  basically the differential between the agentic world or autonomous world where we're replacing [[00:22:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1329.36s)]
*  humans and the agency based world where you're trying to enhance humans. I think Ethan Molyk had [[00:22:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1334.72s)]
*  a great book along these lines that I've added to my Christmas list and others. Like are we building [[00:22:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1339.52s)]
*  these systems to augment humanity's potential and individual agency or are we building it to replace [[00:22:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1344.24s)]
*  humans in order for productivity increases, et cetera. This again relates back to I think [[00:22:28](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1348.72s)]
*  religion is very interesting because the stories that have survived over the years. [[00:22:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1355.76s)]
*  When we come together in these stories of nations and politics and groups and others to scale. [[00:22:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1360.0s)]
*  And again, like I said, the core of those was do unto others as you have done unto yourself. [[00:22:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1365.84s)]
*  There isn't much of this in the current AI discourse. As you said, it's virtue ethics [[00:22:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1370.4s)]
*  and kind of other things. When people talk about AI ethics, it's usually like, but who's ethics? [[00:22:54](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1374.64s)]
*  The Chinese variant of ethics is very different from a California variant of ethics. It's very [[00:23:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1380.3200000000002s)]
*  different from a Muslim variant of ethics or Jewish version of ethics. And I think this is what [[00:23:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1384.64s)]
*  Eric Schmidt and Henry Kissinger and others in relation to Genesis called doxa or underlying [[00:23:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1390.88s)]
*  human agreement and truth and other things like that or principles. I don't think you can have [[00:23:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1395.92s)]
*  absolute truth. I don't think a maximally truth seeking AI actually makes that much sense. [[00:23:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1401.1999999999998s)]
*  I was using that as an example of it's better not to tell it to lie from the start, you know, [[00:23:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1404.9599999999998s)]
*  so to kill humans. But I do think that a diverse range of AI is loosely bound with defined [[00:23:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1410.56s)]
*  frameworks and organized around increasing individual community and collective agency [[00:23:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1419.52s)]
*  is probably a good way to do it. And I think there is a route to doing that as well. When [[00:23:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1425.6799999999998s)]
*  we look at what this technology actually is. Another topic that you kind of touched on a [[00:23:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1430.48s)]
*  minute ago and sort of allude to there again, or at least it comes to mind for me is the [[00:23:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1435.6s)]
*  fact that all this technology is being developed in the context of an increasingly tense great [[00:24:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1442.24s)]
*  power rivalry between at least the US and China. And potentially more players should be considered [[00:24:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1449.44s)]
*  there too. But even with two, it's like complicated enough to be pretty vexing. [[00:24:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1456.72s)]
*  One question I have been wrestling with a lot is to what degree should we want the [[00:24:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1462.8s)]
*  kind of fundamentals of the tech tree between, say, Western and Chinese AI development to be the same [[00:24:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1471.28s)]
*  versus how much should we want them to diverge and sort of represent the unique cultural [[00:24:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1479.52s)]
*  perspectives of each side. My instinct is I think we want to share a tech tree. The more we sort of [[00:24:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1486.24s)]
*  diverge in fundamentals, the more it seems like we'll understand what the other side has less. [[00:24:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1492.4s)]
*  And that just sort of seems to exacerbate these sort of arms race dynamics. But I also do have a [[00:24:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1498.08s)]
*  fairly sympathetic response when people say like, monoculture is bad, brittle, weak, and we need a [[00:25:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1504.24s)]
*  more sort of diverse, almost like take inspiration from sort of biodiversity in the AI ecosystem or [[00:25:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1511.04s)]
*  ecology that we want to create. How do you think about that spectrum of monoculture versus diversity [[00:25:18](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1518.48s)]
*  in AI? I think diversity is better and safer in AI. But I mean, to be honest, there's not much [[00:25:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1524.72s)]
*  difference between the models today. Like what is the difference between a llama and a quen or even [[00:25:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1530.48s)]
*  a GPT-4 or an 01, right? There isn't much. They're actually pretty much the same architecture. It's [[00:25:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1534.72s)]
*  just underlying data difference. I think what's important here is objective functions. What is [[00:25:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1539.76s)]
*  the objective function of open AI and all the models they build, or Anthropic and all the models [[00:25:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1545.04s)]
*  they build versus the Chinese and all the models they build? And who can, again, build those things? [[00:25:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1550.0s)]
*  Falcon has just come out today and it beats quen and it beats llama and others from the UAE. [[00:25:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1555.6s)]
*  And they've got a talented team there. We're seeing all these surprises all over the place. [[00:26:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1561.68s)]
*  But I think ultimately diversity of models is a good thing because different models are usable [[00:26:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1566.08s)]
*  for different things. And the way that I do it is I think of these models as graduates. [[00:26:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1570.8799999999999s)]
*  Does it make sense to have a polymath doing everything and someone coming up with recipes [[00:26:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1576.8s)]
*  and creative all the time now? If we're looking at human agency, what you want to have is highly [[00:26:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1580.8799999999999s)]
*  specialized models that understand context locally. And the more specialized you make it, [[00:26:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1585.28s)]
*  the lower the energy cost, the more they can be available to everyone. [[00:26:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1590.24s)]
*  And so I think you're moving to that engineering and implementation phase of AI now. [[00:26:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1594.24s)]
*  This is separate from the AGI discussion where it gets very complicated. And I fell [[00:26:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1598.56s)]
*  foul to this over the last few years as well. And I was like, oh my God, I'm building technology [[00:26:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1603.36s)]
*  that will kill everyone. And I signed that Paws letter and all sorts of other things. [[00:26:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1607.12s)]
*  It's incredibly difficult to conceptualize AGI, be it as a large model with emergent properties or [[00:26:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1611.6s)]
*  swarm of models. And I think that's what a lot of this drive has been because you want to have a [[00:26:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1617.76s)]
*  pivotal act to stop other people from having it if you have super normal capability. And I think [[00:27:03](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1623.68s)]
*  that's very difficult to sidestep in any way because we won't know until we see it. And we're [[00:27:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1630.16s)]
*  seeing these models continue to get better. And we're not sure what the natural cap is on that. [[00:27:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1635.44s)]
*  And both parties in a world that's about to get completely screwed up economically are going to [[00:27:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1642.0800000000002s)]
*  be fighting over who has preeminence from a soft power, hard power perspective. So I think that [[00:27:28](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1648.4s)]
*  there is the useful stuff where diversity is good. And there's the AGI discussion, which is [[00:27:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1654.96s)]
*  completely different one, where I think it would be best to keep that, but it would be very difficult [[00:27:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1659.1200000000001s)]
*  to. Yeah, no easy answers. I love bringing you the hard questions. And I think [[00:27:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1665.6000000000001s)]
*  you're right. So when it comes to agency in practical terms, my instinct would be to say like, [[00:27:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1671.28s)]
*  the best we could probably do today would be to maybe tweak the anthropic constitutional approach [[00:27:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1678.9599999999998s)]
*  and maybe make it a little bit more agency promoting. I mean, I think they already have a decent [[00:28:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1686.9599999999998s)]
*  amount of that in there. Certainly when I talk to Claude, it kind of wants me to make my own [[00:28:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1690.4799999999998s)]
*  decisions. It doesn't seem like it is trying to tell me what to do. It does seem like it's trying [[00:28:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1696.96s)]
*  to put me in position to be successful. Amanda Askels talked about how she wants Claude to be [[00:28:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1702.16s)]
*  a good friend. And that's certainly what a lot of people would think of a good friend is doing. [[00:28:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1706.16s)]
*  Maybe we could kind of go a little heavier on that and sort of do a more agency promoting [[00:28:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1710.8000000000002s)]
*  constitutional approach. Are there any other technical or any sort of approaches that you have [[00:28:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1716.16s)]
*  in mind for promotion of human agency in AI objective functions? Yeah, this is a question of [[00:28:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1721.76s)]
*  access and then again alignment as to what are you optimizing your business for? How is this [[00:28:48](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1728.4s)]
*  technology coming to you? Like if we give this as a practical example, Google and Facebook [[00:28:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1733.04s)]
*  both have advertising as their core model. And advertising is effective manipulation. And obviously [[00:28:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1738.72s)]
*  these AIs can be far more convincing than anything. So the feeling is that over time, [[00:29:03](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1743.36s)]
*  they will become more manipulative in the way these models are. Again, different from different [[00:29:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1748.96s)]
*  cultures and things, but that's a reasonable assumption. And again, we've seen that with dark [[00:29:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1752.96s)]
*  patterns on Facebook and all sorts of other things. This is very interesting because like [[00:29:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1757.2s)]
*  the inherent nature of these models is that they're probably going to be our best buddies. [[00:29:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1764.32s)]
*  Like maybe your kid's first love will be an AI and other things like that because they'll be there [[00:29:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1769.28s)]
*  for us and they have infinite patience. Something like a core may enhance agency, [[00:29:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1773.6799999999998s)]
*  but the key question is access and who's running that intelligence and who earned it because it [[00:29:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1778.08s)]
*  can be coped at any time. Vitalik Buterin has this post, the founder of Ethereum, the revenue evil [[00:29:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1784.24s)]
*  curve where he's like infrastructure should be non-rivalrous and non-exclusive, but when revenue [[00:29:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1790.56s)]
*  comes in, you tend to start becoming a bit evil. You restrict access or you optimize and then [[00:29:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1796.0s)]
*  when you start optimizing, you act against your end user's effects. And I feel AI has infrastructure, [[00:30:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1801.04s)]
*  particularly when it comes to the stuff for living. So for living is basically all the [[00:30:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1806.08s)]
*  regulated industries. And we have to think who is building those black boxes and what is their [[00:30:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1809.92s)]
*  inherent goals and desires and could those be co-opted? Also, who owns that AI that is your [[00:30:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1814.3999999999999s)]
*  best friend, that is your kid's best friend and how is it constructed? And do we have visibility [[00:30:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1821.44s)]
*  into that? Because we have to remember like 1% of the people might know about AI, but [[00:30:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1826.16s)]
*  like half the people in America still haven't used AI effectively. You know, like Claude is [[00:30:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1831.6000000000001s)]
*  barely a blip versus ChatGPT. And across the world, how many people have used it? Maybe [[00:30:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1836.96s)]
*  an eighth of humanity at most, which is crazy to think about. Most people won't understand [[00:30:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1843.76s)]
*  this technology until it's there and it's out there manipulating and again, the Swiss [[00:30:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1849.0400000000002s)]
*  speech becomes important. We care about the ownership, we care about the alignment. So for [[00:30:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1853.2s)]
*  me, it's about open source models, particularly for living that you own yourself as an individual, [[00:30:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1857.44s)]
*  as a community with transparencies, you know what's inside it, working to allow you to achieve very [[00:31:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1862.8s)]
*  functional objectives. I want my healthcare model to be looking out for me and my healthcare and [[00:31:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1868.96s)]
*  optimizing that versus a HMO or a drug company or something else. My education model, I want to have [[00:31:13](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1873.52s)]
*  a generalized curriculum, but I want to be able to adapt that to my own child without having to [[00:31:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1879.84s)]
*  have any submission. For government, we want to have models that are transparent and can analyze [[00:31:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1884.56s)]
*  any policy position and also represent us because how many people feel represented by their [[00:31:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1890.08s)]
*  governments? And then we can actually have democracy. I think things like this as infrastructure [[00:31:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1894.8799999999999s)]
*  are very important on that and the building blocks of that and the mechanisms for that [[00:31:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1899.84s)]
*  are a slightly different type of RL and also a different type of community aggregation. [[00:31:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1904.96s)]
*  And I think this again is one particular area of regulated industry AI for living, [[00:31:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1910.96s)]
*  the important things of life. Then I think there's another block, which is personal intelligence. [[00:31:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1915.6s)]
*  This is going to be your Apple intelligence, Google intelligence, Tencent intelligence, [[00:32:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1920.0s)]
*  intelligence where you are. And then there are these expert system intelligences, which are the [[00:32:03](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1923.68s)]
*  anthropics and Claude's and O1s of the world, because the Apple intelligence will be free. [[00:32:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1928.96s)]
*  Hopefully we can make the living one free and the other stuff will be expensive. [[00:32:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1934.48s)]
*  Hey, we'll continue our interview in a moment after a word from our sponsors. [[00:32:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1939.3600000000001s)]
*  Even if you think it's a bit overhyped, AI is suddenly everywhere from self-driving cars [[00:32:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1944.3200000000002s)]
*  to molecular medicine to business efficiency. If it's not in your industry yet, it's coming and [[00:32:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1949.5200000000002s)]
*  fast. But AI needs a lot of speed and computing power. So how do you compete without costs [[00:32:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1955.44s)]
*  spiraling out of control? Time to upgrade to the next generation of the cloud, [[00:32:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1960.88s)]
*  Oracle Cloud Infrastructure or OCI. OCI is a blazing fast and secure platform for your [[00:32:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1965.6000000000001s)]
*  infrastructure, database, application development, plus all your AI and machine learning workloads. [[00:32:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1972.16s)]
*  OCI costs 50% less for compute and 80% less for networking. So you're saving a pile of money. [[00:32:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1977.44s)]
*  Thousands of businesses have already upgraded to OCI, including MGM resorts, specialized bikes, [[00:33:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1984.24s)]
*  and fireworks AI. Right now, Oracle is offering to cut your current cloud bill in half if you [[00:33:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1989.04s)]
*  move to OCI for new US customers with minimum financial commitment. Offer ends 12-31-24. [[00:33:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=1994.08s)]
*  So see if your company qualifies for this special offer at oracle.com slash cognitive. [[00:33:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2000.08s)]
*  That's oracle.com slash cognitive. [[00:33:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2005.12s)]
*  80,000 Hours is a nonprofit that helps people find fulfilling careers that matter. [[00:33:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2007.52s)]
*  And their research suggests that working to avoid the worst AI risks could be one of the [[00:33:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2015.2s)]
*  highest value contributions one can make. So if recent AI news has you thinking about how you can [[00:33:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2019.84s)]
*  make a bigger impact, you should absolutely check out all the latest research and content from 80,000 [[00:33:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2025.92s)]
*  Hours. 80,000 Hours has been working on this for nearly a decade, and they have tons of in-depth [[00:33:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2031.04s)]
*  material on their website, including articles about how seriously we should take AI risks [[00:33:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2036.8s)]
*  and what an AI-caused catastrophe might actually look like, plus detailed reviews of careers in AI [[00:34:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2041.36s)]
*  policy and strategy, as well as AI safety research. They also publish a regular newsletter and podcast, [[00:34:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2047.52s)]
*  which features unusually in-depth conversations with guests like Andropec CEO Dario Amadei [[00:34:13](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2053.6s)]
*  and AI safety pioneer Paul Cristiano, plus 150 other episodes on topics including global health [[00:34:18](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2058.8s)]
*  and development, nuclear war, animal welfare, and climate change. If you want to play a more [[00:34:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2065.12s)]
*  active role in navigating humanity's uncertain future, 80,000 Hours can help. Everything they [[00:34:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2070.88s)]
*  do is always 100% free, so visit 80,000hours.org slash cognitiverevolution to get your copy of the [[00:34:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2076.3199999999997s)]
*  80,000 Hours Career Guide and subscribe to the newsletter and podcast. That's 80,000hours.org [[00:34:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2082.96s)]
*  slash cognitiverevolution for all the information you need to make the most of your career. [[00:34:48](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2088.4s)]
*  Almost perfect segue to the intelligent internet, but there are a couple of things I have to [[00:34:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2095.2799999999997s)]
*  follow up quickly before I invite you to share your big vision for the future. [[00:35:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2102.72s)]
*  One I'm really wrestling with right now is the question of should I get an AI toy buddy, tutor, [[00:35:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2108.56s)]
*  whatever for my kid? My oldest is about to turn six. I feel like there could be tremendous upside [[00:35:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2117.04s)]
*  to it if we had the right product and it was, again, had the right objective functions and it [[00:35:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2124.0s)]
*  wasn't overly manipulative. By the way, the manipulation and the sense of being optimized [[00:35:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2129.28s)]
*  against, I have felt very strongly personally in years past when I was running performance [[00:35:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2136.0s)]
*  advertising on Facebook, it was crazy how often you would see they're optimizing not just, of course, [[00:35:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2142.16s)]
*  to get users to click stuff, but also optimizing to get us to pay the maximum amount that we can [[00:35:48](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2148.08s)]
*  possibly pay until it drops our margins down to roughly zero. They're doing that to everyone at [[00:35:54](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2154.3199999999997s)]
*  the same time. It's quite a system that they've created. I'm very mindful of those big forces are [[00:36:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2160.3199999999997s)]
*  out there and I'm very wary about putting my kid in a position where they're being acted on by [[00:36:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2166.56s)]
*  such forces. But then again, I do think individual ongoing tutoring for hours a day is obviously a [[00:36:13](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2173.52s)]
*  proven method for learning and AIs can do that, whether it's math or reading or whatever. You [[00:36:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2180.48s)]
*  could answer that in any way you want, but if you had even a recommended product, I would go buy it. [[00:36:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2187.68s)]
*  How do you think about right now introducing AI into the life of children? [[00:36:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2192.72s)]
*  Yeah, I mean, like right now is probably going to be fine. It's just, again, once you get into the [[00:36:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2198.2400000000002s)]
*  optimization equation, you start to optimize very aggressively, right? Like I'm sure Matt's just [[00:36:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2202.4s)]
*  selling space in the llama-related space right now. I would, it makes sense, because you have to make [[00:36:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2206.8s)]
*  decisions and models are becoming increasingly deterministic versus stochastic in the way that [[00:36:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2212.4s)]
*  those are and we're getting more and more understanding of how they work. I think that [[00:36:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2217.1200000000003s)]
*  you're not at the point right now where, again, there will be the first love, but it's pretty close. [[00:37:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2220.8s)]
*  Like you've seen character AIs, explosive growth, and unfortunately, like the Daenerys [[00:37:05](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2225.44s)]
*  situation where there was the Daenerys and the kid shot himself, that was very famous. Statistically, [[00:37:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2231.28s)]
*  that will happen because of the large numbers of people and again, our instance of mental health. [[00:37:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2236.4s)]
*  It's incredibly sad, right? And again, just like self-driving, we'll hold AI to a higher account. [[00:37:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2240.96s)]
*  But we have seen people falling in love with their AIs now for a while. I think, oh, what was it [[00:37:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2246.1600000000003s)]
*  called? I can't remember the name, but in any case. [[00:37:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2250.56s)]
*  Replicas is one of the early ones for sure. Replicas, exactly. Valentine's Day last year, [[00:37:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2252.88s)]
*  they shut off the, what was it, adult messaging on Valentine's Day. And I think 66,000 people [[00:37:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2257.36s)]
*  joined the Reddit because their Valentine's Day plans were ruined. That was a snippet of where [[00:37:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2263.12s)]
*  we are coming because now we have real, photorealistic voice speech, all these other things. [[00:37:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2267.52s)]
*  And so we have to think again, what is the intentionality of these agents, [[00:37:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2272.7999999999997s)]
*  of these embodied intelligence, where are they coming from? Because like when we have a teacher, [[00:37:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2276.64s)]
*  we want to know what their CV is, or here in the UK, you have DBS checks for criminality and other [[00:38:03](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2283.36s)]
*  things. I think you'll want the same when you have intelligent agents coming into your house, [[00:38:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2287.7599999999998s)]
*  particularly with your kids, particularly because they'll be there with them. And again, [[00:38:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2291.84s)]
*  they will help them and that's how they build the bonds. But again, what are they building it [[00:38:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2296.08s)]
*  towards? Is it to direct them towards advertising or others? But I think we're quite there to get [[00:38:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2299.12s)]
*  an intelligence thing. And this is where we have an opportunity to set good standards [[00:38:23](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2303.2s)]
*  because it's better than say the YouTube algorithm, which will make them watch Baby Shark and that [[00:38:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2306.64s)]
*  dark underbelly of YouTube. And if you've seen it where you've got like Spiderman and Elsa drilling [[00:38:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2312.7999999999997s)]
*  teeth and all sorts of things like that. I've caught glimpses for sure. And it gets real weird, [[00:38:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2316.7999999999997s)]
*  real quick. And I am like, what is it that you are even enjoying about this? Would I see my kids [[00:38:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2321.3599999999997s)]
*  occasionally falling down that rabbit hole, but there's something highly optimized about it, [[00:38:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2327.04s)]
*  clearly. That is exactly it. They create millions of these videos and literally I've seen [[00:38:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2331.44s)]
*  Elsa and Spiderman giving someone a root canal with very happy kids music like generated like, [[00:38:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2335.52s)]
*  what on earth is this? Because it's selling ads, right? Similarly, you know, the TikTok, [[00:39:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2341.76s)]
*  Doob scroll and all sorts of other things. But the benefits, he said, can't be outweighed because [[00:39:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2346.08s)]
*  the bloom effect, the two sigma effect, one, two intuition is huge. So I think there needs to exist [[00:39:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2351.84s)]
*  some sort of open standardized infrastructure that we have for humanity. That's a finite [[00:39:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2356.16s)]
*  range thing where we like we trust this. Because just like crypto should have been trusted and [[00:39:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2361.04s)]
*  turned out to be this massive raccoon infested crapness. I think AI, do you trust this AI is [[00:39:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2367.36s)]
*  a question and you want to be able to trust the eyes because they are more capable. And you want [[00:39:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2374.16s)]
*  them to help you. And so I think that there exists a high level bar for that. And we do need to have [[00:39:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2378.96s)]
*  standards, regulations and others. It's just no one's quite sure what they are yet. So let's build [[00:39:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2383.68s)]
*  transparent. It's an AI as it were. Yeah, I think that is the perfect setup to the introduction of [[00:39:48](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2388.56s)]
*  the intelligent internet. And I want to give you as much time as you want really to just kind of [[00:39:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2395.12s)]
*  lay out the positive vision that you have for the future. I think I often say that the scarcest [[00:40:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2401.04s)]
*  resource is a positive vision for the future. I'm always struck by how as many new launches as we [[00:40:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2407.36s)]
*  get and all these advances, such a tiny fraction of the time is devoted to actually trying to [[00:40:13](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2413.2s)]
*  describe like what life is supposed to be like in the future and why we're supposed to be excited [[00:40:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2421.9199999999996s)]
*  about it. So I really appreciate that you are doing some of that work. And I would love to hear [[00:40:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2426.7999999999997s)]
*  your kind of, you know, positive vision what life is like in a future of the intelligent internet. [[00:40:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2432.7999999999997s)]
*  Yeah, so thanks. So the intelligent internet, we put out the primer is this conceptualization of [[00:40:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2440.56s)]
*  a infrastructure for information transmission and value whereby everyone has their own AIs that [[00:40:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2446.16s)]
*  represent them on an individual, community, country and societal level. Because what I'm seeing now is [[00:40:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2451.76s)]
*  AI models that are satisfying and capabilities like, does anyone really need more than one? [[00:40:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2457.76s)]
*  Like sure, there's a few things like retrieval and other stuff, memory, self-learning that it needs, [[00:41:03](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2463.28s)]
*  but it's a level of performance that if we could give it to everyone in the world in the right way, [[00:41:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2468.16s)]
*  I think the world would be better. And then when you start breaking it down, you're like, [[00:41:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2472.0s)]
*  how could it be better? Where is the human capital? Shortfall. Because again, like you've still got [[00:41:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2475.52s)]
*  600 million people without smart phones, you've still got so many people living below the poverty [[00:41:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2481.68s)]
*  line. The average global IQ is 90. That's the average. And it's not due to lack of intelligence, [[00:41:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2486.16s)]
*  it's due to lack of infrastructure. You don't have enough good teachers for everyone. I mean, [[00:41:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2492.0s)]
*  how many people listening to this believe that they've had all good teachers? How many people [[00:41:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2495.84s)]
*  are happy with their medical system, their government and others? So I was like, AI can [[00:41:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2499.36s)]
*  really change that in a very interesting way. We can build a better system that's interconnected [[00:41:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2503.84s)]
*  for that. This is important because if we go back to the start of the P-Doom question, [[00:41:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2507.92s)]
*  there are two types of decision-making. When you make decisions in a stable environment, [[00:41:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2513.6800000000003s)]
*  and this is where you should have a P-Doom or probability, you did expected utility calculation. [[00:41:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2518.6400000000003s)]
*  This is the probability of all these things, and then we do expected utility across that. [[00:42:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2522.88s)]
*  When stuff is uncertain, and we move from decision-making under risk to uncertainty, [[00:42:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2527.92s)]
*  you minimize the maximum regret. So if you were to go at the start of a desert, you don't know [[00:42:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2531.84s)]
*  where the oasis is, you're not going to take any steps, which is the downside of this. So you need [[00:42:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2535.92s)]
*  to articulate positive visions of the future. And what that is that if you look at the energy [[00:42:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2540.88s)]
*  equation of how much it costs to do a GPT-4 level query, it was a thousand watts of electricity. [[00:42:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2546.48s)]
*  And now it's five. You can have an original GPT-4 level model that runs on an MPU now, [[00:42:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2553.04s)]
*  which is a five watt of electricity entity. Solar power is now less than a dollar per watt. [[00:42:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2558.8s)]
*  So then you have a chip cost and an inference cost that's probably 50 bucks to have a GPT-4 [[00:42:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2565.6s)]
*  level AI offline anywhere in the world. And you think about that, and you're like, wow, [[00:42:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2572.2400000000002s)]
*  you could give a tutor to every single school in the world. What if you connect to a Starlink? [[00:42:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2577.2000000000003s)]
*  What if you connect to a Starlink? And it's off grid. You look at medicine, you're like, [[00:43:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2581.04s)]
*  how can this change again? But again, if you look at all the stuff you have for living, [[00:43:05](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2585.2799999999997s)]
*  which I think is the most important thing, and we want that to be robust infrastructure, [[00:43:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2589.04s)]
*  an AI-first approach to those where we have gold standard transparent data sets that then create [[00:43:13](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2593.7599999999998s)]
*  models and systems that we can deploy online, offline, could be something that's world-changing. [[00:43:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2599.7599999999998s)]
*  I think you need different models from the individual to community to country to societal [[00:43:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2606.4s)]
*  level, because to take a practical example, half of everyone who listens to this call will get [[00:43:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2611.12s)]
*  cancer, which is crazy if you think about it. Everyone who's listening in has had someone in [[00:43:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2617.12s)]
*  their family or friends who's gone through a cancer experience. And it's awful. You feel that [[00:43:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2622.96s)]
*  massive loss of agency when that occurs, because we don't have cures yet. We have treatments, [[00:43:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2627.6s)]
*  but still it's very traumatic. Medical AI models now are performing human doctors on research, [[00:43:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2631.84s)]
*  on diagnosis, but also empathy. And how many have had an empathetic experience through their [[00:43:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2639.6800000000003s)]
*  cancer journey? What happens if we give every single person going through cancer, multiple [[00:44:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2644.8s)]
*  sclerosis, Alzheimer's, an AI that helps them through that journey? How to talk to your family? [[00:44:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2649.76s)]
*  What's the latest comprehensive authoritative up-to-date information? [[00:44:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2656.48s)]
*  You know, it's always there. It's always available in every language. The world becomes a better [[00:44:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2660.4s)]
*  place, ironically, through empathy from AI. And that's infrastructure that should be available [[00:44:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2666.7200000000003s)]
*  to everyone. This is a concept we call universal basic AI, or UBI, as it were, which I think is [[00:44:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2671.76s)]
*  better than income, and we can discuss that in a bit. But then you look at that, and again, [[00:44:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2677.6s)]
*  every condition, you look at education, everyone should have access to an AI that's looking out [[00:44:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2681.04s)]
*  for them, and how can I maximize that child's potential? But then shouldn't all of these feed [[00:44:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2685.2s)]
*  up in a proper way to a collective knowledge? What's our collective knowledge on cancer? [[00:44:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2690.7999999999997s)]
*  If a paper comes out, why isn't it instantly analyzed and checked and absorbed into our [[00:44:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2696.96s)]
*  knowledge, be it probabilistic or certain? Why do we have a million different vaccine schedules [[00:45:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2701.68s)]
*  around the world? I'm not saying vaccines are good or bad, but still it doesn't make much sense. [[00:45:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2706.72s)]
*  People are people. What is an optimal way to teach calculus or machine learning or anything like this? [[00:45:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2710.24s)]
*  So I think if we can build intentionally, we can build better AI-first systems that come together, [[00:45:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2716.56s)]
*  and that's what I call the intelligent internet. Because we're at a very interesting point in time [[00:45:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2720.64s)]
*  whereby your generative AI for health is probably a little bit higher than most, but it's basically [[00:45:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2725.04s)]
*  nothing compared to what it'll be in 10, 20 years. You'll have an O1 level intelligence organizing [[00:45:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2729.8399999999997s)]
*  your health if it's built, and your hospital will have the equivalent of a supercomputer, [[00:45:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2734.7999999999997s)]
*  and your country will have dedicated hardware internally against intelligent capital stock, [[00:45:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2739.92s)]
*  as I call it. That's organizing the health system. And then we should have a central organization [[00:45:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2744.88s)]
*  to have a comprehensive and intuitive up-to-date knowledge base for cancer, for climate, for [[00:45:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2751.6000000000004s)]
*  education, for all the stuff we need for living. And it's a finite amount of energy that's required [[00:45:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2755.36s)]
*  to build that. And then that's a good infrastructure that we have to support us. And so that's what the [[00:46:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2760.4s)]
*  concept of the intelligent internet was. It was like, let's build data sets, model systems, [[00:46:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2766.72s)]
*  and then the right hardware so that we can deploy this at scale to everyone and [[00:46:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2771.6s)]
*  lift the intelligence of humanity in a secure and robust way. And so we outline the end state, [[00:46:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2776.3199999999997s)]
*  and in the upcoming pieces, we're going to outline the ways to get to that, from identity to digital [[00:46:23](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2783.2s)]
*  assets to physical infrastructure to the models themselves. Because I think it'll be difficult [[00:46:28](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2788.72s)]
*  to do this in a decentralized, distributed, or market-competitive way. I think you've just [[00:46:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2793.36s)]
*  got to go and do it. We know what to build. Just build it. Organize all the cancer knowledge. Why [[00:46:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2797.92s)]
*  hasn't anyone done that? I think of how crazy it is. Again, if you've got a diagnosis, God forbid, [[00:46:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2802.8s)]
*  tomorrow, what would you do? You would go to Clawood, and you would check this, and you would do that. [[00:46:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2809.2000000000003s)]
*  But why isn't that all just in a box? And again, for me, that's infrastructure. [[00:46:54](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2814.32s)]
*  Clayton Christensen coined the term disruptive innovation, said infrastructure is the most [[00:47:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2820.4s)]
*  efficient means by which society stores and distributes value. And then information theory, [[00:47:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2826.56s)]
*  Claude Shannon, is that information is valuable as much as it changes state. [[00:47:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2831.2799999999997s)]
*  Deliberately building the data sets, models, and systems, making them freely available, [[00:47:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2836.48s)]
*  as cheaply as possible, as low energy as possible, is a new type of intelligence infrastructure for [[00:47:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2842.3199999999997s)]
*  society that I think will do better than we can build other stuff off. [[00:47:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2847.68s)]
*  Just to take a second and try to steal the case for the leading developers today, [[00:47:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2853.2799999999997s)]
*  I feel like you would hear a lot of high-level conceptual agreement on what you have laid out, [[00:47:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2862.3199999999997s)]
*  and maybe not necessarily at the level of exactly where the models reside, [[00:47:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2869.68s)]
*  but certainly they talk a lot about intelligence too cheap to meter, and they have certainly dropped [[00:47:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2876.56s)]
*  their prices dramatically. Possibly they're all operating at a loss on the margin to try to [[00:48:03](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2883.36s)]
*  compete for share, not entirely clear. They also make a lot free. OpenAI just yesterday [[00:48:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2890.08s)]
*  released their search feature to all users globally for free. There's a lot that's free. [[00:48:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2897.12s)]
*  Dave Korsunsky Is there a critique of what you see them doing today that is central, [[00:48:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2902.32s)]
*  or is it more just that you extrapolate into the future and you see that this structurally won't [[00:48:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2911.52s)]
*  work for much longer? [[00:48:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2917.44s)]
*  Paul Borgman I think it will be very difficult for the proprietary model developers because [[00:48:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2918.6400000000003s)]
*  Apple intelligence, Google intelligence will be free. This is one of the things I had as stability [[00:48:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2923.2000000000003s)]
*  where I was like, do I do a giant funding round? We never did another one after the first one. We [[00:48:48](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2928.72s)]
*  did like uncapped notes, $100 million, whatever. Because what is the sustainable model on the API [[00:48:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2932.7999999999997s)]
*  or SaaS side when they're coming for you? Gemini Flash 2.0, if it's the same price as the original [[00:48:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2937.8399999999997s)]
*  Gemini Flash, it's seven cents per million tokens. Yeah, that's crazy. It's like what, [[00:49:05](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2945.3599999999997s)]
*  50 times cheaper than 4.0 and a pretty good model, right? I think that there is this thing where [[00:49:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2950.56s)]
*  that type of intelligence as a consultant is available. I think there is this world of open [[00:49:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2957.52s)]
*  weight. This is personalized AI that you have Apple intelligence and Google intelligence, [[00:49:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2962.64s)]
*  others. And there's this other area which I'm focused on, which I haven't seen anyone tackle, [[00:49:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2966.88s)]
*  which is regulated industry AI. I don't think you can have black boxes there. I think we deserve [[00:49:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2971.84s)]
*  better. I think we need open data, open weight models built deliberately. And what I have is [[00:49:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2978.16s)]
*  generalized common knowledge because the data for that should all be common knowledge. [[00:49:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2983.92s)]
*  For education, for healthcare and others. We can have curriculum learning. Then you have your [[00:49:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2987.28s)]
*  sectoral and localized versions so you have a Mexican healthcare model. And the data to build [[00:49:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2992.4s)]
*  that is finite and you can get to a gold standard very quickly. And that should be infrastructure [[00:49:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=2998.1600000000003s)]
*  that's non-rivalrous and non-exclusive in my opinion. I think everyone would benefit from that [[00:50:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3001.76s)]
*  because everyone wants to have high quality localized datasets, specialist datasets and more. [[00:50:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3006.5600000000004s)]
*  And again, the data that you need for a medical model for Mexico is in the medical textbooks of [[00:50:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3011.44s)]
*  Mexico. And I haven't really seen anyone build that deliberately and really optimize against [[00:50:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3017.12s)]
*  how do we get this to as many people as possible? Which is why I said let's just go and do that. [[00:50:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3024.3199999999997s)]
*  Because that's the biggest delta. Because many of these organizations, you're still fighting for [[00:50:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3029.44s)]
*  SOTA, right? You're still going for AGI, but you're satisfying. How much difference is there [[00:50:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3033.44s)]
*  really between an anthropic and an open AI and a Gemini and a Grok? They're all pretty good right [[00:50:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3038.96s)]
*  and they're good enough to make a difference. So I don't think you need SOTA here, which is also [[00:50:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3046.56s)]
*  another very interesting thing. In fact, do you want to have a state of the art unproved model [[00:50:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3051.04s)]
*  for your healthcare? You want a really robust standard one that behaves very predictably, [[00:50:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3055.92s)]
*  which also is why with the advent of function calling large complex windows is the ideal time [[00:51:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3061.2s)]
*  for this. For education, for that. Again, anything in regulated industry. Do you want a black box [[00:51:05](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3065.12s)]
*  checking every medical decision then eventually make, no you don't. Or checking every government [[00:51:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3070.96s)]
*  decision then maybe make, no you don't. So I just think there's this particular category that's very [[00:51:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3075.12s)]
*  interesting, that I haven't seen anybody look at, but I think it's probably the most important one [[00:51:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3079.6s)]
*  for flourishing. But then it becomes very interesting because if for all the stuff you have for living, [[00:51:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3085.04s)]
*  you know, Mazel's hierarchy of needs, you have these open way open data models that we're trying [[00:51:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3091.7599999999998s)]
*  to optimize for as low energy as possible for maximum impact to get to a level of performance, [[00:51:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3096.08s)]
*  like almost equi token, equi MMLU as it were. Then I think that can guide the discussion because [[00:51:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3100.56s)]
*  suddenly you can parameterize and have data sets for cultural diversity across nations. And you [[00:51:48](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3108.88s)]
*  can figure out things like DOCSA. You can have a feedback loop where if you release all the data [[00:51:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3115.44s)]
*  open from teaching kids and learning from kids, how to increase agency there. Or on the medical [[00:52:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3120.56s)]
*  side again, self-learning systems because you can make everything open with PII and other things. [[00:52:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3126.72s)]
*  And you can have interesting things like if you have a cancer AI to help you through your cancer [[00:52:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3131.52s)]
*  journey, it can tell that you're a human because it's attached on the regulator side. And then you [[00:52:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3137.44s)]
*  can use the same system for your education, for your government and more. That's the functional [[00:52:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3142.16s)]
*  foundational identity. So there's a lot of benefits that can emerge here that could help with the AGI [[00:52:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3147.3599999999997s)]
*  and state of the art. But again, I view those almost like expert systems graduates, [[00:52:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3152.16s)]
*  consultants from McKinsey, where there's some more like the graduates that work for you and your team. [[00:52:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3158.3199999999997s)]
*  And the Apple and Google intelligence, the world and others are probably not going to be [[00:52:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3165.44s)]
*  doing your healthcare because they don't really want to. They want to do your personal life and [[00:52:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3171.52s)]
*  everything. They want to interact with healthcare, though they want to interact with education. [[00:52:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3175.68s)]
*  But I don't think those organizations are going to go after that. So I think it's just, [[00:52:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3179.76s)]
*  nobody's really looked at that. And again, I think it's inevitable. And the question is, [[00:53:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3184.1600000000003s)]
*  who do we have a million different healthcare models, education models, more? [[00:53:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3188.4s)]
*  When we only really need one that's then customizable and adaptable and is MIT. [[00:53:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3192.0800000000004s)]
*  Soterios Johnson So is the sort of model for the future that you have in mind one where, [[00:53:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3197.84s)]
*  because you're hearing, of course, from Sam Altman and Dario, that they sort of expect to [[00:53:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3204.7200000000003s)]
*  run away from the competition in the next couple of years. They are not sharing the O1 chain of [[00:53:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3212.0800000000004s)]
*  thought. They've said this is kind of GPT-2 level scaling of runtime compute, and they know how to [[00:53:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3217.6000000000004s)]
*  get to GPT-4. They think they're going to blow the doors off of everybody there. Dario and team have [[00:53:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3224.96s)]
*  said in their credibly attributed but leaked fundraising document that they see a world in [[00:53:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3230.72s)]
*  like 2025 timeframe, which is by the way, two weeks away, that the leading developers could get so far [[00:53:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3237.52s)]
*  ahead of the competition that nobody can catch up because they have these sort of enrichment loops [[00:54:03](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3243.3599999999997s)]
*  that keep advancing the state of the art. And if they're not sharing that, I also had a recent [[00:54:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3248.72s)]
*  data point on this when Nathan Lambert from the Allen Institute did an episode on everything he's [[00:54:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3254.9599999999996s)]
*  learned about post-training. And one of the big takeaways for me was he said, in the absence of [[00:54:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3259.84s)]
*  access to GPT-4 or similar to create the samples that we do the post-training on, we can't do it. [[00:54:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3265.36s)]
*  Yeah, there's just no, even at the Allen Institute, billionaire estate funded, we don't have the [[00:54:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3272.1600000000003s)]
*  ability to go do a scale AI contract or whatever to bring in the expert data that we would need. [[00:54:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3276.8s)]
*  So do you think that happens or doesn't happen? If it does happen, is the model sort of, [[00:54:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3283.2000000000003s)]
*  they become like the go-to for the pharmaceutical industry and cybersecurity industry and all these [[00:54:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3287.76s)]
*  sort of like things where you really do still need to push the frontier of the state of the art [[00:54:54](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3294.0s)]
*  forward. And then at a more distributed level, we satisface and like are really more focused on [[00:55:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3300.1600000000003s)]
*  reliability and sort of explainability when it comes to things like making our own personal [[00:55:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3307.0400000000004s)]
*  medical decisions? I think that's the case. I think that we over-indexed on chefs versus cooks. [[00:55:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3311.92s)]
*  So you know, everyone's on the spectrum, wait, but why had a great person from being a cook that [[00:55:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3320.4s)]
*  follows recipes to making recipes? How many recipes do you really need to make? Where AI is actually [[00:55:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3324.7200000000003s)]
*  really good now with function calling large context windows is following recipes. You give it a handbook [[00:55:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3330.2400000000002s)]
*  and it's basically replaced as a SaaS or replaced as an employee or something like that. And we want [[00:55:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3336.88s)]
*  to have more and more employees that follow the damn instructions. In fact, how often do you need [[00:55:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3340.8s)]
*  to come up with something brand new for the vast majority of people? For that, there is the O1 type [[00:55:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3345.04s)]
*  models. There are all these other kinds of things. And again, this is where you have the AGI race away. [[00:55:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3350.2400000000002s)]
*  But if you're training, say for instance, on a million H100s, like Elon's about to, or a million [[00:55:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3355.2000000000003s)]
*  TPUs, like Demis is about to, or a million training, where God forbid, Daria is about to, poor guy. [[00:56:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3361.6000000000004s)]
*  You know, like, how big is that model actually going to be for inference side? You kind of went [[00:56:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3367.7599999999998s)]
*  straight through the consumer side and now you're like, well, am I going to build a multi-trillion [[00:56:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3374.0s)]
*  parameter model that needs a cerebrous wafer or a Google TPU pod to run? You know, because that's [[00:56:18](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3378.8799999999997s)]
*  not really suitable for consumers at the side of things, but it might be more intelligent. But it [[00:56:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3385.68s)]
*  might be 5% more intelligent. Whereas it requires orders of magnitude less compute to have the lower [[00:56:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3389.6s)]
*  ones. And this is where it becomes very interesting because classically most of computation has been [[00:56:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3395.36s)]
*  sequential as opposed to like parallelized. And we're seeing this now again with test time [[00:56:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3402.48s)]
*  compute and others. Like, who's going to win if you need to have millions and millions of GPUs [[00:56:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3407.6s)]
*  running in parallel, running O1 type things to check something and cross check it? Probably more [[00:56:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3412.6400000000003s)]
*  distributed others. But what is winning? If winning is AGI have, you describe it, then fine. Those are [[00:56:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3417.2000000000003s)]
*  the expert systems. And that's where it is. But will one cluster win over a massive amount of [[00:57:03](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3423.92s)]
*  post-training with IQ 130 things checking? Will one cluster win over, let's take the China example, [[00:57:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3431.12s)]
*  getting 100 million people in China to label stuff? This is not something that I don't really [[00:57:18](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3438.64s)]
*  understand where it's going because we haven't proven that we can break through PhD level yet. [[00:57:23](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3443.6800000000003s)]
*  And even if we did, how useful is that in the context of all the actions of humanity [[00:57:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3449.52s)]
*  and improving humanity's agency and capability to a brighter future? Like, this is the same question [[00:57:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3454.56s)]
*  as what do you use a quantum computer for? Actually coming up with the questions that you want to answer [[00:57:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3461.68s)]
*  is really hard. This is why like I love O1 for the more advanced stuff and I'm using it to help me [[00:57:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3467.6s)]
*  improve my CUDA and kind of other things. But the average person isn't going to get much mileage out [[00:57:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3473.6s)]
*  of it because it's too damn smart. And do you need even smarter than that? You have to set even [[00:57:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3478.4s)]
*  better questions if it's smarter than that. Whereas again, like I said, this particular area of [[00:58:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3482.8s)]
*  AI for living, I don't think needs that at all. I think the models right now are good enough, [[00:58:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3488.48s)]
*  full stop, to make a transformative difference for the world if we organize the data correctly. [[00:58:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3492.88s)]
*  The category of personal AI, your Apple intelligence, so Siri isn't stupid, [[00:58:18](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3498.4s)]
*  and other things like that, it's nearly good enough. The category of expert systems, [[00:58:23](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3503.12s)]
*  yeah, it might be a race away thing. But I don't think it's interconnected race away thing. [[00:58:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3507.76s)]
*  And if it is, then Google will win. Because Google's interconnect fabric is so much better [[00:58:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3513.36s)]
*  than everyone's and they're deploying it at a ridiculous pace. The only potential competitor [[00:58:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3519.5200000000004s)]
*  I could see might be XAI. Anthropic, I don't see Tranium and the data funnel scaling that much, [[00:58:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3524.4s)]
*  and OpenAI similarly. I don't see it scaling that much. [[00:58:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3531.76s)]
*  Mad Fientist Yeah, reports of Google's demise, I think, [[00:58:54](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3534.72s)]
*  were always greatly exaggerated. Tim Cynova [[00:58:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3538.08s)]
*  I never got that. I mean, I remember when it first came out, I was like, this is stupid. They've got [[00:59:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3540.3999999999996s)]
*  the people, they've got the talent, it just takes time to steer the ship. They've come up with all [[00:59:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3544.3199999999997s)]
*  of these things. And again, if you look at the hardware, like we have thousands of TPUs, TPUs [[00:59:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3547.9199999999996s)]
*  are the best interconnect hardware and individually addressable, which is why Google's models are the [[00:59:13](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3553.9199999999996s)]
*  only ones publicly available with 2 million token context windows, going up to 10 million. [[00:59:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3559.2s)]
*  People don't appreciate how insane that is with like 100% needle haystack. You can do some crazy [[00:59:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3564.56s)]
*  things that we're just exploring the surface of right now. I think Notebook LLM was probably the [[00:59:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3571.6s)]
*  first example of that. You upgrade a gigabyte of things to Notebook LLM and then now you can [[00:59:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3575.36s)]
*  dial in to talk to the podcast hosts. Mad Fientist Yeah, one wonders how much [[00:59:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3581.2s)]
*  longer we'll need human podcasters, but I'll spare you my personal anxieties about that. [[00:59:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3586.24s)]
*  I also do have a challenge, which I fail on still every day of spending a dollar a day on Gemini [[00:59:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3591.04s)]
*  Flash. And it is just mostly for me a reminder of how cheap things have become and how much [[00:59:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3599.2s)]
*  information I really could be processing for very, very little info. And I still just need to like [[01:00:05](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3605.68s)]
*  figure out the right structures to do that for me. Edith Just get to watch your screen and tell [[01:00:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3611.52s)]
*  you when you're being unproductive. Mad Fientist Yeah, I saw that demo in the last couple days. [[01:00:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3615.76s)]
*  And yeah, that's the always on observing occasionally interjecting AI I think is going [[01:00:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3619.44s)]
*  to be a really interesting paradigm. I really like your chef and yeah, go ahead. [[01:00:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3625.84s)]
*  Edith No, I was just saying, I think again, if you are boiling these models down to everything, [[01:00:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3630.96s)]
*  just come down to engineering. This is why Elon managed to catch up so quickly, right? Because [[01:00:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3635.36s)]
*  he's an engineering maestro. Like 100,000 million GPUs, all good. But then like I said, if you look [[01:00:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3638.8s)]
*  at the different types of hardware, the interconnectability and the addressable [[01:00:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3645.28s)]
*  RAM and other things like that, and not paying the Nvidia premium, like it matters a lot. This [[01:00:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3649.52s)]
*  is where I think the latest like drama with the open AI and their blog posts and things like that, [[01:00:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3655.36s)]
*  like, Hey, there's Elon this, this, they were talking about buying cerebris because that massive [[01:01:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3660.5600000000004s)]
*  addressable memory and things like that. So it'll be very interesting to see how that all pans out. [[01:01:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3664.88s)]
*  Mad Fientist So I like your chef and cook notion as a simple heuristic where maybe a one, two, [[01:01:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3669.28s)]
*  three, four, whatever goes off and figures out how to cure particular kinds of cancer or maybe [[01:01:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3680.1600000000003s)]
*  Claude four or five or Gemini three is doing that stuff. And then on a daily basis, we satisfy and [[01:01:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3686.0800000000004s)]
*  we try to make sure that we're applying the actual latest known stuff in an effective way, [[01:01:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3695.36s)]
*  as opposed to trying to advance the state of the art, you know, each one of us as individuals. [[01:01:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3702.4s)]
*  Let's talk about the structure for this. I think in the primer for the intelligent internet, [[01:01:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3707.52s)]
*  you do a really nice job of walking through the different tiers of the critical resources, right? [[01:01:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3712.4s)]
*  I mean, obviously, infrastructure, compute infrastructure and data, as well as, of course, [[01:02:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3720.1600000000003s)]
*  the algorithms and the human talent that makes it work are the fundamental inputs. [[01:02:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3724.08s)]
*  And you've kind of mapped out a structure where there is like the highest tier, biggest scale of [[01:02:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3729.12s)]
*  those, and then a more distributed level and then a more local level and ultimately individual users [[01:02:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3734.4s)]
*  get to take advantage of this kind of on their own terms. Take us through that vision for how [[01:02:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3740.16s)]
*  this actually gets built out because it is like a physical capital thing and structured to achieve [[01:02:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3746.56s)]
*  the goals that you've articulated. Yeah. So again, if I kind of look at the future, [[01:02:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3753.6s)]
*  I see very rapidly, we're going to see economic disruption because the labor theory of productivity [[01:02:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3758.16s)]
*  is about to be challenged. You start to find people and again, anyone listening to this, [[01:02:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3763.44s)]
*  finding human captain is the hardest thing. Whereas AI will follow your instructions and [[01:02:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3767.2s)]
*  it's very reliable, relatively speaking. I mean, sure, it was a goldfish earlier this year, [[01:02:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3771.92s)]
*  you know, like it's a Vont PhD has had his coffee and just respond to stuff. We're solving most of [[01:02:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3776.7999999999997s)]
*  those things right now. Right. So I was like, there needs to be this capital stock build out. [[01:03:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3781.84s)]
*  And right now we're seeing it on an individual level because you're getting M4 Mac books with [[01:03:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3786.1600000000003s)]
*  16 gigabytes of VRAM as standard. You're seeing eight gigabytes on 40 seventies on laptops. And [[01:03:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3791.6000000000004s)]
*  you cask ahead eight gigabytes on the integrated Zs and AMD chips with the 70 ATMs. Those will soon [[01:03:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3800.1600000000003s)]
*  all standardize in my opinion, to 16 gigabytes for AI compute, be it with an MPU of five Watts or be [[01:03:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3807.04s)]
*  it with or neural processor or more advanced graphics options at 25 to 125 Watts. This is a [[01:03:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3814.08s)]
*  build out that enables intelligence on the edge. And again, every manufacturer is doing that. [[01:03:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3822.4s)]
*  But then you think about a hospital and that's what we call the distributed nose. You want to [[01:03:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3827.04s)]
*  have a standardized stack, then go into any hospital, transform the data and then add an [[01:03:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3831.6s)]
*  interface to that to interact with your personal intelligence and more. And then you want what we [[01:03:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3835.68s)]
*  call hyper nodes, which are national nodes to organize all the health care knowledge of the [[01:04:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3841.04s)]
*  entire nation or organize all the local cultural data of the entire nation and keep that up to date [[01:04:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3846.56s)]
*  as well. So we kind of, we had this stack ranking where there's different levels of computation [[01:04:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3852.08s)]
*  from inference level 16 gigabytes to let's say right now, fine tuning level at a hospital level, [[01:04:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3856.3199999999997s)]
*  like 128 H 100s to 256 can get any hospital or bank or anything like that pretty much fine tuned. [[01:04:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3862.16s)]
*  Then you've got the national level, which should probably be in the thousands of H 100 equivalents [[01:04:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3869.2799999999997s)]
*  to organize all of that. Maybe there's an order of magnitude each way. And then when I looked at that, [[01:04:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3873.52s)]
*  I was like, this is really interesting because this is a defined capital build out. The one is [[01:04:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3878.08s)]
*  standardized as possible. So you have boxes that plug in, that can just look at stuff because these [[01:04:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3882.16s)]
*  models are amazing at that infrastructure level. And they offer different types of experience at [[01:04:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3886.48s)]
*  different levels. But again, one healthcare model for cancer is very easy to turn multilingual for [[01:04:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3890.88s)]
*  everyone in the world. Right. And one radiology model and one education model was theories of [[01:04:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3898.6400000000003s)]
*  those. When that's actually the next part, I think we mentioned it briefly in the primer, I was like, [[01:05:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3904.0s)]
*  that looks a bit like Bitcoin as it were, which is maybe a story for later or another day, but [[01:05:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3908.32s)]
*  dedicated infrastructure build out to create an enturging capital stock that then gives [[01:05:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3915.92s)]
*  capability. Just like building factories gave capability, but you don't need factories anymore. [[01:05:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3920.16s)]
*  You actually need pure hardware running as standardized models as possible. And those [[01:05:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3926.64s)]
*  standardized models need to be based on standardized data sets and just have regular releases and be [[01:05:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3932.3199999999997s)]
*  predictable in the way they operate. Okay. I have a lot of little detailed follow-up questions. [[01:05:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3937.92s)]
*  I guess for starters, so much is being made right now about the energy requirements of [[01:05:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3944.7999999999997s)]
*  AI that I find this kind of confusing. On the one hand, obviously we hear, you know, [[01:05:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3951.68s)]
*  $7 trillion bill about crazy energy grid expansions. To some extent, I sort of interpret those as like [[01:05:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3956.3999999999996s)]
*  trying to expand the over-the-window so we can get like a couple of new power plants online versus [[01:06:03](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3963.52s)]
*  very little new construction, obviously in the US in recent history. It is, I think, [[01:06:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3969.28s)]
*  a very good grounding to talk about the idea that a five watt machine can power an AI. I [[01:06:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3974.56s)]
*  wonder if you could flesh that out a little bit more. It seems like an 8B, a llama 3.38B [[01:06:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3982.88s)]
*  is kind of hitting at that GPT-4 level now and that's quite runnable on a, certainly on a new [[01:06:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3991.92s)]
*  Mac, right? Can you give us a little bit more there on like exactly how you envision that? [[01:06:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=3999.04s)]
*  Yeah. So 8B model is runnable on a Mac neural engine. So a Mac neural engine will use about [[01:06:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4004.8s)]
*  five watts of electricity as well as Intel MPU. But if you want more, like I've got my M2 Max and [[01:06:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4010.4s)]
*  waiting for my M4 Max, that will use up to a hundred watts of electricity, right? If you look [[01:06:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4017.36s)]
*  at the H100, the H100 uses a thousand watts of electricity. So you can use more energy and it'll [[01:07:03](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4023.1200000000003s)]
*  go faster, but 10 tokens a second is human reading speed. It's like you can, you know, you see these [[01:07:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4029.44s)]
*  models go like that. If it's faster, then you can read what's the point in some ways, right? And [[01:07:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4036.8s)]
*  there's a question of how many tokens does a human need to flourish? And so we can do some calculations [[01:07:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4041.6s)]
*  around that. We're like, well, if you don't need everything synchronous and you can let it go, [[01:07:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4046.64s)]
*  then five watts of electricity is probably all anyone needs. And again, you're seeing that [[01:07:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4050.8799999999997s)]
*  because you can now run models on the smartphones. Smartphones tend to top out at 25 watts of [[01:07:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4054.64s)]
*  electricity. This is where it becomes again, very interesting because let's rewind a bit. On [[01:07:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4059.2799999999997s)]
*  instability, we did stable diffusion and that cost maybe $5 million of compute in total, right? To do. [[01:07:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4064.7999999999997s)]
*  And that's a decent amount of megawatts of electricity. You know, it's a million or something. [[01:07:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4071.52s)]
*  A 100 hours and A 100 is what? 400 watts. So 400 megawatts of electricity, is that right? Something [[01:07:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4078.4s)]
*  like that. And that's a good amount. But then to run the model is just a little bit on the edge. [[01:08:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4088.16s)]
*  So you think about it, the pre-training and the post-training, et cetera, comes out with an [[01:08:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4095.12s)]
*  artifact, which is this compressed set of weights. That then means that you don't have to retrain [[01:08:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4100.0s)]
*  that thing. Where a lot of them exponential energy usage is coming from is a lack of optimization. [[01:08:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4105.6s)]
*  This is why Meta releases Lama, one of the reasons. They got 350,000 H100s, they're about to get bigger. [[01:08:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4112.72s)]
*  10% are used for training. They've improved the efficiency of Lama by more than 10% by releasing [[01:08:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4118.4s)]
*  it. So the remaining 315,000, it more than pays for itself. You've seen this, for example, [[01:08:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4124.24s)]
*  Genmo released their new video model, Mochi. Day one, it required, I think, 140 gigabytes of VRAM. [[01:08:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4132.16s)]
*  Within a few days, it required 20. And that requires eight. Like, again, there's massive [[01:08:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4138.4s)]
*  optimizations around these models that connect there as well, because it depends on what are [[01:09:03](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4143.84s)]
*  you optimizing for. But if you Google, you look at Microsoft, look at everyone else, they're like, [[01:09:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4146.96s)]
*  we can't afford not to do this build up. Because what if we're wrong? What if we can't optimize? [[01:09:13](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4153.04s)]
*  Because no one knows what the satisfying level of performance will be for any of these models. [[01:09:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4157.5199999999995s)]
*  Like you look at VO2 right now, that's just coming out from Google on the video side, or Sora. [[01:09:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4164.48s)]
*  It wouldn't surprise me if the VRAM and energy requirements of those drop by a border of magnitude [[01:09:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4170.64s)]
*  overnight, because someone's figured something out. We don't know how much these will require. [[01:09:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4175.76s)]
*  So we overbuilt to a degree, but we're building more and more of this capacity out. [[01:09:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4180.08s)]
*  But then it becomes interesting, because although we've had more and more of the energy usage, [[01:09:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4184.5599999999995s)]
*  you think about the energy usage to make a movie. How many megawatts is that versus what it will be? [[01:09:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4189.04s)]
*  The energy usage to make this podcast, the energy usage to write a book, the energy usage to provide [[01:09:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4195.6s)]
*  a medical input. And I think that AI is very beneficial for this, because again, there's this [[01:10:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4201.28s)]
*  lovely chart that you can look at of GDP per capita versus energy. It was always that relation. [[01:10:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4207.52s)]
*  It's broken that. Because in a year, we could go to a kid in sub-Saharan Africa and give them solar [[01:10:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4212.72s)]
*  panel, an MPU chip, and a version of Llama 4 that speaks their language, and then they have [[01:10:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4221.120000000001s)]
*  access to how much knowledge, right? Or a medical version of that, or anything like that. And that's [[01:10:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4229.68s)]
*  crazy, because you would have to build the infrastructure in place. You add Starlink to it, [[01:10:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4235.68s)]
*  it gets even crazier. But you see what I mean? It's like, this is a fundamental change of energy. [[01:10:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4240.16s)]
*  So I think we have this energy run up now, but then global energy is not going to keep up consuming. [[01:10:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4244.72s)]
*  Unless, of course, I'm wrong, and we end up with like freaking Dyson spheres running AGI [[01:10:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4250.64s)]
*  and other things like that. But I feel that probably won't be the case. I mean, [[01:10:54](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4254.88s)]
*  there's diminishing returns on scaling. That's an important idea. Just to put a little bit of [[01:10:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4258.320000000001s)]
*  back in the envelope math on the energy requirements. Where I live in Detroit, [[01:11:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4266.4800000000005s)]
*  Michigan, the cost of a kilowatt hour of electricity is roughly 20 cents. I think it's a little lower [[01:11:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4272.88s)]
*  than that still, but call it 20 cents as a nice round number. Five watts is one two hundredth of [[01:11:18](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4278.8s)]
*  a kilowatt, obviously. And so I would be looking at, if I'm running my AI on one of these five [[01:11:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4286.88s)]
*  watt machines nonstop, I would be looking at a tenth of a cent an hour in energy cost or like [[01:11:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4292.64s)]
*  two, two and a half cents a day, which means obviously I'm looking at single digit dollars [[01:11:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4302.160000000001s)]
*  over the course of a year. And I think that is a dramatically underappreciated point about just how [[01:11:48](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4308.08s)]
*  accessible the marginal costs actually operate the thing you do have to chip costs too. But [[01:11:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4315.6s)]
*  just how accessible that already is for something that, you know, with the Lama [[01:12:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4321.04s)]
*  three three eight B is like getting really quite good. Yeah. I mean, hugging your face just released [[01:12:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4327.5199999999995s)]
*  the latest thing on the one replication. They've got Lama one B outperforming Lama eight B with [[01:12:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4332.8s)]
*  test time compute. And again, if you put a 24 seven, you're interacting with the how crazy is that? [[01:12:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4337.599999999999s)]
*  What is the lower bound on these things? We're not sure. But it's got to that point where people [[01:12:23](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4343.68s)]
*  don't appreciate that. And the same example I gave was solar power. If you hook up a solar panel [[01:12:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4347.92s)]
*  to the grid, it takes up like four dollars a watt. If it's off grid, it's less than a dollar a watt [[01:12:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4353.04s)]
*  for the solar panel. So then what the cost is effectively nothing. And again, the question is, [[01:12:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4358.88s)]
*  where do we satisfy? Like, how good do you need? Like you can you look at VO two today on the video [[01:12:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4365.6s)]
*  side. Do you need anything better than that to make a Hollywood level movie? No, not really. Like, [[01:12:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4372.4800000000005s)]
*  sure, there's some small things, but those can always be tied up in post, right? And we figured [[01:12:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4376.4s)]
*  out consistency and other things. Do we need we have chatbot models now that are performing human [[01:13:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4381.2s)]
*  doctors on empathy and diagnosis. Do we need better than that? No, we need to optimize the heck out [[01:13:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4386.32s)]
*  of it and figure out where that lower bound is. And the amazing thing is, all these models are [[01:13:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4391.04s)]
*  still full of junk. Like Ilya Sutskova just said in Europe, it's like we've run out of data. How [[01:13:18](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4398.16s)]
*  much of that data is junk? Still, even in these top models, we're still like getting to grips [[01:13:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4405.2s)]
*  with this. Like we saw the SANA team, previously Pixar, to create a stable diffusion level model [[01:13:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4410.88s)]
*  with 25 million images. And we use two billion. What is the lower bound of data that you need to [[01:13:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4416.08s)]
*  have for a language model to achieve X to teach your kid? Does it need to have seen Reddit? No. [[01:13:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4423.44s)]
*  Does it need to have seen this? If you want to build AGI, yeah, you need all that data. Maybe. [[01:13:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4431.68s)]
*  But do you need it all in one place? Like, what is the limited data analysis? And we've seen [[01:13:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4438.24s)]
*  interesting things like I think Claire AI just did like an AI that was just trained on knowledge up [[01:14:03](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4443.360000000001s)]
*  to 300 BC. I haven't seen what it's like. But those are the types of things that very really [[01:14:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4450.72s)]
*  interest me. Like, what does that look like? Ancient AI. Right. It feels like we have an order [[01:14:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4456.32s)]
*  of magnitude improvement still to come just from data, honestly. And we're seeing this with fine [[01:14:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4465.36s)]
*  web and kind of other things and the synthetic data sets and augmented data sets that we've had. [[01:14:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4470.48s)]
*  Yeah, there's a lot. The efficiency curve is truly crazy. I mean, [[01:14:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4477.36s)]
*  five models from Microsoft are an interesting data point about how small your data set can be and [[01:14:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4481.84s)]
*  how I think theirs are, as far as I know, entirely synthetic for that line of models. [[01:14:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4487.68s)]
*  Yeah, they did 13.8 epochs on like 400 billion tokens for the core. But then they have to add in [[01:14:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4492.88s)]
*  like a map of the internet because they're so boring. And people don't want to interact [[01:14:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4499.6s)]
*  with boring models. Because again, people take them generalized. So when you add back the internet, [[01:15:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4502.96s)]
*  they get less boring. But five models have the worst sense of humor of any model. [[01:15:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4507.4400000000005s)]
*  Very. Yeah, textbooks aren't all you need for sense of humor, I guess. But yeah, so many things [[01:15:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4512.64s)]
*  like that. Let's go. Okay, so that's the low level. It's incredibly accessible and affordable [[01:15:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4517.68s)]
*  to satisfy. If you go up to the top end of your infrastructure for the intelligent internet, [[01:15:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4525.4400000000005s)]
*  this is the like national or perhaps even international hyper node level. A lot of [[01:15:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4533.04s)]
*  questions about that, I guess. But maybe for starters, is your hope for safety and transparency [[01:15:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4539.2s)]
*  ultimately rooted in open data? It seems like open weights, we can hope for a breakthrough in [[01:15:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4547.12s)]
*  interoperability, but we don't have it. So it seems like right now, if you were to say we want [[01:15:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4551.76s)]
*  to make something we can feel confident is safe, and we feel confident is not going to be too crazy. [[01:15:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4556.5599999999995s)]
*  You would have to do that by sort of saying, we deeply understand the data set. And that's why we [[01:16:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4562.48s)]
*  believe this. Is that right? I think it's necessary, but not sufficient. Like I think the [[01:16:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4568.88s)]
*  existence of gold standard data sets that reflect individualized cultures and other things are going [[01:16:13](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4573.92s)]
*  to be vitally important when it comes to decision making AI. Again, how do you understand, well, [[01:16:18](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4578.4800000000005s)]
*  the people of Mississippi want, or the people of Malawi want, or the people of Mumbai want, if you [[01:16:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4586.24s)]
*  haven't mapped out the individual culture, the decisions and kind of other things. And open data [[01:16:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4591.28s)]
*  with open models probably makes them more interpretable, but no one's figured out [[01:16:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4596.8s)]
*  interpretability. Again, lots of interesting SAE and other research, but we haven't figured this [[01:16:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4600.0s)]
*  out yet. I think it will help. And I think, again, the existence of these gold standard data sets [[01:16:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4604.0s)]
*  will help the entire space. Because like when Luther released the pile, it became a standard. [[01:16:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4609.2s)]
*  When Lyon released their data sets, it becomes a standard. That's why I originally called the [[01:16:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4613.84s)]
*  company Shelling, because I was like, let's build standardized gold standard data sets and just [[01:16:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4617.4400000000005s)]
*  improve them rigorously, which is what we thought the hypernodes would do. The hypernodes [[01:17:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4621.76s)]
*  coordinate and improve the data sets, and then can go down to the organizational community and [[01:17:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4626.16s)]
*  individual level and have a self-learning system that, again, just improve, improve, and keep it [[01:17:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4632.08s)]
*  up to date. Then anyone can take those because we released everything MIT, permissionlessly, [[01:17:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4637.76s)]
*  which is the other important thing. What if someone, you break the rules and they shut off [[01:17:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4642.48s)]
*  your AI? You're a bit screwed then, right? For whatever reason. And then I think, again, [[01:17:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4649.04s)]
*  that can move towards standards because the highest value AIs will be decision supporting [[01:17:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4654.24s)]
*  and making AIs. And what do those look like and how do we understand what those do? And [[01:17:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4660.24s)]
*  are those got repeatable frameworks? The most widely used AIs will be the AIs that are available, [[01:17:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4665.92s)]
*  especially if they're permissionless. I mean, we have 300 million downloads of our models [[01:17:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4672.48s)]
*  at Stability AI, you know? That's a lot of freaking downloads because they're just available, [[01:17:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4676.88s)]
*  but then people built around them because they didn't need permission. So I think that, again, [[01:18:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4681.84s)]
*  it's necessary but not sufficient because otherwise they look at anthropic and open [[01:18:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4686.400000000001s)]
*  AIs models, like how will they incorporate conceptualizations of Chinese or Japanese ethics? [[01:18:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4691.360000000001s)]
*  Is anyone even taking that seriously? But then what if they're used to make decisions in the [[01:18:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4699.92s)]
*  Japanese government as they kind of are right now? I think you need to be very deliberate in [[01:18:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4704.32s)]
*  the way that you figure this out on the underlying principles and the feedback loops there. [[01:18:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4709.44s)]
*  And that was the final part of the intelligent internet, which was I think people should have [[01:18:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4714.48s)]
*  a say and be participatory, which is why we're looking at the digital asset space. We're looking [[01:18:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4718.0s)]
*  at distributed governance. We're looking at all these other things. Is there a way that can all [[01:18:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4724.16s)]
*  be brought together? Which is not easy, but hopefully we can figure that out. [[01:18:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4727.5199999999995s)]
*  Yeah, well, I'm interested to hear more about it. Sounds like thoughts there are still in [[01:18:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4733.44s)]
*  development, but whereas the user level seems like it almost is going to take care of itself [[01:18:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4738.96s)]
*  in as much as everybody is continuing to buy phones already, I don't really know [[01:19:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4746.8s)]
*  what to expect at the hyper node level. I don't know how many hyper nodes you think we need. Are [[01:19:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4752.64s)]
*  they billion dollar? Are they $10 billion investments and who's going to make them? [[01:19:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4757.76s)]
*  Well, so I think a country's competitiveness was about its physical infrastructure. How many things [[01:19:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4762.88s)]
*  can you build? It was about its graduates. What sort of intelligence smart graduate network, right? [[01:19:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4769.360000000001s)]
*  It will become about your intelligent capital stock. How much compute [[01:19:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4775.68s)]
*  coordinated into models do we have access to? And again, if you look at the online thing of China, [[01:19:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4779.68s)]
*  US versus beyond the jingoism, it is a question of let's build compute in America. Let's build [[01:19:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4785.12s)]
*  compute in China because from a test time compute perspective, that is what your competitiveness [[01:19:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4790.0s)]
*  becomes. And you're seeing this across the world, like Malaysia and Johor just build a $9.6 billion [[01:19:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4795.5199999999995s)]
*  AI data center. You're seeing billions of dollars across Asia. I don't think you need to have that [[01:20:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4801.76s)]
*  much beyond the thousands range because although it's like $240 billion or something like that of [[01:20:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4807.68s)]
*  AI building going on, when you start looking at how much individual clusters and things like that [[01:20:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4814.32s)]
*  are like the fastest, largest biomedical healthcare cluster in the world is probably Chan Zuckerberg [[01:20:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4819.28s)]
*  and a thousand H100s. That's like $20 million a year, which is not nothing, but it's not [[01:20:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4825.679999999999s)]
*  ridiculous. And so you're like, how much can it probably be in thousands ranges? What I think [[01:20:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4831.679999999999s)]
*  will be sufficient to organize national data sets and create them and even provide almost free AI to [[01:20:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4837.44s)]
*  the vast majority of people in a reasonable sized nation. You're talking about like thousands of [[01:20:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4843.12s)]
*  H100s being enough at the national level? Yes. And this is not in the short term. This is like long [[01:20:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4849.36s)]
*  term H100 equivalents. Like once you build a gold standard data set for education for Bangladesh, [[01:20:54](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4854.8s)]
*  how much do you really need to update that? Not that much. And if the outcome is a model that's [[01:21:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4862.72s)]
*  updated every so often, that's a few gigabytes, then goes onto everyone's phones provided by TCL [[01:21:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4867.92s)]
*  or Huawei or whatever. Then again, that isn't much. And what's the cost of inference? Pretty [[01:21:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4874.0s)]
*  much nothing. It's shift again that needs to be developed deliberately. This is separate to the [[01:21:18](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4878.8s)]
*  state of the art. The state of the art costs a lot and it costs a lot to inference and it costs a lot [[01:21:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4884.16s)]
*  to do everything else. But for me, like so this is for me is a very interesting part. Give every [[01:21:28](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4888.72s)]
*  child their own AI tutor that's aligned with them. Make sure no one's ever learning through [[01:21:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4893.92s)]
*  their healthcare journey anymore. Make governments more representative. Finance non-corruptible [[01:21:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4897.52s)]
*  and things like that. I'm surprised by how small you're projecting that to be. Is there any [[01:21:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4902.72s)]
*  assumption there about like having a general, if I'm Malaysia for example, well Malaysia you said [[01:21:48](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4908.4800000000005s)]
*  built a $10 billion one, but let's say I am a small country and I've got my Chan Zuckerberg [[01:21:54](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4914.88s)]
*  scale thing and it's thousands of H100 equivalents. I can't train a foundation model from scratch on [[01:22:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4920.72s)]
*  that, right? So I would, I can do a lot of fine tuning. I can do a lot of data processing, but [[01:22:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4929.92s)]
*  or at least it would seem hard to do foundation models from scratch. [[01:22:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4936.16s)]
*  You just need one entity to do it. Like matter doing llama is good enough for almost most things, [[01:22:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4941.360000000001s)]
*  right? And they did 20 million odd H100 hours for the largest version of lava. And that's what [[01:22:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4949.6s)]
*  let's say $40 million spent for lava 70b3. And then everyone could just take that and fine tune [[01:22:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4958.0s)]
*  it, but to tune it requires 128 to 256 H100s. And the order of magnitude cost is $100,000, $200,000 [[01:22:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4964.0s)]
*  to create an Arabic version of that. And so again, we're blinded by the massive compute [[01:22:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4972.88s)]
*  and the edge stuff when actually it doesn't require that much to do the tuning. [[01:23:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4980.8s)]
*  The thousands enables you to actually give access to your people, to the technology, especially if [[01:23:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4984.8s)]
*  again you don't require rate limits or your satisfying level. And it also allows you to [[01:23:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4989.6s)]
*  do the tuning. So you probably start out with a few hundred and then you build up from there. [[01:23:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=4996.8s)]
*  And again, when you look at the disparity of compute and energy and other things, it's [[01:23:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5001.04s)]
*  very interesting. Like I was in South Africa, I think the fastest cluster there was like [[01:23:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5005.120000000001s)]
*  50 or 100 H100s, but then the clouds are building even larger ones. [[01:23:28](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5008.8s)]
*  This is why it's very achievable if we're coordinated. But then do you want to be giving [[01:23:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5013.360000000001s)]
*  your people that level of healthcare AI or that level of healthcare AI? Do you want to have your [[01:23:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5018.400000000001s)]
*  companies in your country be able to call on these experts or even more experts? So this is why I [[01:23:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5023.68s)]
*  think there'll be a very interesting capital stock question. And when we look at things like 5G, [[01:23:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5029.84s)]
*  the amount of AI spending has not caught up to the amount of 5G spending around the world. [[01:23:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5035.2s)]
*  This is far more important than 5G when it comes to a country, a community and individuals' [[01:23:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5039.28s)]
*  productivity levels. You know, again, like just think about it. Having an expert in your pocket, [[01:24:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5044.08s)]
*  that's your Huawei smartphone. Having your hospital suddenly have all the latest stuff [[01:24:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5051.360000000001s)]
*  and being available to all the people in your community, that's a 50 grand type of thing, [[01:24:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5057.12s)]
*  you know, 100,000 in scales. And the more you spend, the more competitive you'll be, [[01:24:23](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5063.12s)]
*  the more productive you'll be, the more access you'll have. And again, I was just shocked by [[01:24:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5067.599999999999s)]
*  how many people were actually looking at this particular area and what was required for that. [[01:24:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5073.12s)]
*  So I think if we go back to the start, one entity will need to pre-train base models. That's our [[01:24:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5077.5199999999995s)]
*  common knowledge. Then you have sectoral data sets where you sectorally specialize in and [[01:24:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5083.2s)]
*  localized data sets. So you can have the generalized model customized for Mexican law, Indonesian [[01:24:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5087.28s)]
*  accounting, Japanese medical, and everything is completely transparent across that. And the amount [[01:24:54](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5094.96s)]
*  of compute is primarily that pre-training at the top, but then require the hundreds to tune of [[01:25:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5101.84s)]
*  equivalent. So it seems like, I guess maybe one caveat that the llama models are not open data. [[01:25:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5109.04s)]
*  It seems like you feel like they already have enough raw power to serve as the base for a lot [[01:25:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5117.76s)]
*  of this fine tuning around the world. Do you worry like what happens if llama four doesn't [[01:25:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5124.96s)]
*  get open sourced? And do you feel like alternatively, the other end of that would be like, do you feel [[01:25:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5130.72s)]
*  like it's just not viable because it's not open data and there needs to be a true fully open data [[01:25:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5136.72s)]
*  alternative? Llama four will be amazing for a vast variety of things. I don't think it's suitable [[01:25:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5142.96s)]
*  for regulated industry. And again, regulated industry is typically the stuff for living. [[01:25:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5150.88s)]
*  So it's just someone who needs to go out and build those and those standardized systems [[01:25:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5156.56s)]
*  and release them MIT. And then that makes the world a better place and then enable access to those. [[01:26:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5161.12s)]
*  So I think llama is fantastic. I think Quen is fantastic. I think where those end up in a few [[01:26:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5168.24s)]
*  years, nobody knows. They're run by Alibaba. They're run by Matter. And again, those are [[01:26:13](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5173.04s)]
*  corporations. I think they want to build infrastructure to teach all the kids, honestly. [[01:26:18](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5178.32s)]
*  Like eventually, if you think about the infrastructure that will teach your child, [[01:26:23](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5183.36s)]
*  an AI that's fully open and owned by you to act as a regulator that then calls into a llama based [[01:26:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5187.92s)]
*  service that can then call into an anthropic or Google or open AI service, it will be a mixture [[01:26:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5194.64s)]
*  of models, right? And you'll have the protective model there. And then your kid's ability might be [[01:26:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5202.88s)]
*  that you have a base level. And this is why we have this concept of universal basic AI, [[01:26:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5210.56s)]
*  which I think is important. That's your base level. But then you might want more and then [[01:26:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5213.52s)]
*  you pay more for more. I think that's absolutely fine. So again, I don't think that what we're [[01:26:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5217.92s)]
*  looking at, what we're doing replaces llama. I think there needs to exist open weight models. [[01:27:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5222.96s)]
*  And I think the three best are DeepSeq, llama by Matter and Quen by Alibaba. And I think those [[01:27:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5226.72s)]
*  serve their purpose, but they will never release the data set that underlies that because they're [[01:27:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5235.76s)]
*  not trying to build infrastructure. They have a different objective function. Just like I think [[01:27:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5240.8s)]
*  Apple will release all the data inside Apple intelligence, just like I think Google won't [[01:27:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5245.360000000001s)]
*  release all the data inside Gemma. And even if they did, like I said, it doesn't really align with [[01:27:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5249.6s)]
*  the reasoning of the organization. Yeah. So how do you, this maybe gets a little bit to the crypto [[01:27:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5254.56s)]
*  question or at least the, you know, the incentive engineering, if you will, you've got this concept [[01:27:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5260.64s)]
*  in the intelligent internet primer of proof of beneficial compute. And I'm curious to hear how [[01:27:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5266.08s)]
*  you think this can best be organized or catalyzed so that these, I guess mostly it would be countries [[01:27:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5273.44s)]
*  around the world and maybe a mix of other private institutions too, if they want to, how do they [[01:28:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5282.4s)]
*  all come to see it in as being in their interest to contribute to this project? [[01:28:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5288.72s)]
*  That seems like it'll be an amazing trick if you can pull that off. [[01:28:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5295.200000000001s)]
*  Yeah, it would be nice, wouldn't it? I mean, I'd be looking at a lot of business models again, [[01:28:18](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5298.8s)]
*  the last podcast we had with yourself and others was like talking about business models and AI. [[01:28:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5302.160000000001s)]
*  And you know, like the stability experience was interesting. We took it up to [[01:28:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5309.04s)]
*  tens of millions of revenue run rates in just a few years and others were still burning, [[01:28:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5314.0s)]
*  but it was getting towards profitability. Got a bit tight at the end. I think the latest CEO said [[01:28:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5318.8s)]
*  there was less than 90 days of cash, which is better than Apple was back in the day. [[01:28:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5324.080000000001s)]
*  But that's because I didn't do another giant round. So I was like, where's the sustainable thing here? [[01:28:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5327.76s)]
*  When I kind of took a step back and I looked at what I really want to do, like I worked on [[01:28:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5332.0s)]
*  United Nations AI initiatives on COVID-19, United Nations back one of the things I got into AI when [[01:28:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5335.52s)]
*  my son was dying was autism. I built an AI system. I was like, how do you build infrastructure that's [[01:29:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5340.96s)]
*  available to everyone? And what does that look like? And thinking about the future, [[01:29:05](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5345.84s)]
*  one of the things that I've alluded to in this podcast is that the labor theory of productivity, [[01:29:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5349.44s)]
*  which is a center of economics is broken. And then the nature of money is probably going to change [[01:29:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5354.96s)]
*  because what does a optimus or an O1 or other things need? Some sort of GPU inference based [[01:29:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5360.4s)]
*  currency as it were. Some people have said that Bitcoin might be the future of these agents. [[01:29:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5367.44s)]
*  Again, you've seen AI agents now go across crypto channels and other things. And I was like, [[01:29:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5371.92s)]
*  if I've got a massive amount of compute coming online, and I know that again, the amount of [[01:29:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5376.88s)]
*  compute for your city's hospitals about to go from that to that, and I can build a standardized [[01:29:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5383.28s)]
*  stack for that, we're not using that secure brand new type of currency similar to Bitcoin, [[01:29:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5387.92s)]
*  but with the properties of Ethereum. And we sell the currency and get supercomputers for health and [[01:29:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5392.56s)]
*  autism and cancer and other things like that. And the more we help people, the more money it has. [[01:29:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5397.36s)]
*  The more we get these distributed nodes running that the more secure it is, [[01:30:03](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5403.92s)]
*  because the amount of flop securing the network increases. This comes at a time of deregulation [[01:30:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5408.48s)]
*  in the US and massive amount of digital assets, because almost all of them are rubbish and full [[01:30:13](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5413.12s)]
*  of raccoons. But you still have Bitcoin ETFs and even nations thinking about Bitcoin sovereign [[01:30:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5417.76s)]
*  reserves. But for me, the sovereign reserve thing never made sense because that was always a balance [[01:30:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5422.64s)]
*  of payments or commodities. I don't have enough oil. I don't have enough corn in my granaries. [[01:30:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5427.04s)]
*  I don't have enough intelligence. So I was like, if I could build that as an institutional asset, [[01:30:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5431.12s)]
*  and again, we'll finalize the details of that release in the new year. That could be something [[01:30:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5436.879999999999s)]
*  interesting whereby people could buy it, they could hold it, it's secured by increasing amounts of [[01:30:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5441.2s)]
*  compute. And then you can allocate it so you can take your coins and allocate them to cancer [[01:30:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5445.839999999999s)]
*  research, or Alzheimer's, or autism, or Mexican education. And then we give that universal basic [[01:30:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5451.36s)]
*  AI or the research grants or others. That plays on this transition period. It plays on the [[01:30:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5458.08s)]
*  structural demand growth for digital assets that are decent, of which there aren't really any. [[01:31:05](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5465.28s)]
*  And it's a finite amount of capital that's required, not cheap, so hundreds of millions [[01:31:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5470.48s)]
*  or billions. That means that everyone will never be alone again on their cancer journey. [[01:31:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5476.5599999999995s)]
*  That means that every child will have an AI that allows them to achieve their potential. [[01:31:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5482.88s)]
*  I was like, that's a good deal. And so we should give this a go because I can't think of any other [[01:31:28](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5488.08s)]
*  way to get people participating. I couldn't think of another way to get this technology out there [[01:31:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5491.92s)]
*  without turning evil as it were. Because again, you want to have an online model where by helping [[01:31:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5498.08s)]
*  people you gain value. And it seems a big demand for it. But like I said, putting the final touches, [[01:31:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5503.76s)]
*  that's why the primer was like, here's the goal. And now it's about let's release details of how [[01:31:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5509.04s)]
*  we get there and get people involved on that. In terms of the incentive you want to create, [[01:31:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5513.2s)]
*  if the mechanism is as yet not fully defined, is the idea supposed to be kind of similar to [[01:31:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5519.28s)]
*  a Bitcoin where I'm like buying in now in anticipation of some sort of like compute [[01:32:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5529.12s)]
*  appreciation in the future? Or like, what is my individual or even national incentive to get [[01:32:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5535.5199999999995s)]
*  involved? It's Bitcoin, except for usually what happens is that we actually think you're just [[01:32:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5541.92s)]
*  copying Bitcoin straight, but with some adjustments. But rather than mining Bitcoin than doing sales, [[01:32:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5547.4400000000005s)]
*  because you want provision compute for open source effectively, and then you want to use it for [[01:32:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5554.88s)]
*  positive stuff, be it inference to provide universal basic AI, or building these data sets and models. [[01:32:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5558.64s)]
*  This is why I sit on the advisory board of Render and others where there's millions of GPUs. [[01:32:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5564.56s)]
*  Rather than selling it and it going to a treasury, or going to Lamborghini's, [[01:32:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5569.120000000001s)]
*  you sell coins, and then we will build the fastest healthcare supercluster in the world, [[01:32:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5573.52s)]
*  and make it available to everyone working in healthcare to build open models and release them [[01:32:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5578.240000000001s)]
*  in MIT. The more you build, the more you help, the more status you get. And like I said, there's no [[01:33:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5582.56s)]
*  high status digital assets right now. Then what will happen, we believe, is that as you build the [[01:33:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5587.360000000001s)]
*  full stack, a nation can buy the reserves, and then you put the money back into the nation [[01:33:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5592.4800000000005s)]
*  to put a cluster in that nation to provide this. And the economics works well. People can donate, [[01:33:18](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5598.0s)]
*  and they get the tax write-off, but then they control the direction of voting of the coins. [[01:33:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5604.32s)]
*  So working through this, but we're like, this could be a way that you could go out, [[01:33:28](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5608.24s)]
*  everyone to participate, they can direct the network compute for something that has defined [[01:33:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5611.76s)]
*  amount of things. Let's just, at the moment, follow the Bitcoin mechanism. So 21 million coins and [[01:33:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5616.0s)]
*  everything. And that would be very positive, because what we need is a lot of compute right [[01:33:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5622.32s)]
*  now to build the models and datasets. And then we have an inevitability of compute for healthcare [[01:33:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5627.759999999999s)]
*  education, all these other things, plus verified accounts. That's a very interesting setup. And [[01:33:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5633.679999999999s)]
*  again, it's in this particular area that we don't see anyone else in the AI doing. But it's like, [[01:34:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5641.12s)]
*  we should have as soon as possible, nobody being alone in their cancer journey anymore. [[01:34:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5646.96s)]
*  Or when my son was diagnosed with autism, the feeling of loss I had, that should not be there [[01:34:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5651.6s)]
*  for anyone. And that's the type of thing people can get behind. And I think this is where crypto [[01:34:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5655.84s)]
*  has succeeded and failed. People forming communities and being able to participate, [[01:34:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5661.68s)]
*  especially with the USG regulation will be a big boon. Having some sort of traceability over funds [[01:34:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5667.280000000001s)]
*  not going to land those, but clusters that are very well defined, that community organizations [[01:34:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5672.4800000000005s)]
*  and academics can use is going to be great. And then should there be any cost to everyone having [[01:34:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5677.2s)]
*  that cancer AI? No. It should be free. Education should be free. And if we can externalize it with [[01:34:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5684.88s)]
*  the desire of people to hold digital assets for appreciation or for status or for anything else, [[01:34:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5691.04s)]
*  even better. So like I said, that's taken quite a few months to design. We're nearly there. [[01:34:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5696.48s)]
*  The primer was the first bit. And we'll be releasing more details of all of this soon. [[01:35:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5701.5199999999995s)]
*  I think it's complementary to what other people in the space are doing as well. [[01:35:05](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5705.28s)]
*  I think everyone will benefit from the data sets. Everyone will benefit from these things. [[01:35:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5708.48s)]
*  And the final bit, like I said, I'm very worried about what money looks like over the next five, [[01:35:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5712.639999999999s)]
*  10, 20 years and how the economics of everything works when labor theory of productivity is just [[01:35:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5717.2s)]
*  broken. Yeah, that's a dramatically under theorized problem, I would say for sure. [[01:35:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5722.639999999999s)]
*  I'm using O1 to help me write the paper on it right now, actually, for the new year. [[01:35:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5729.04s)]
*  It is crazy though, because you think about the Philippines. Why would anyone hire a call [[01:35:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5733.36s)]
*  center worker again? Anything about physical embodiment? You think about laborers? Like [[01:35:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5738.72s)]
*  in 20 years, will a robot be a better plumber than any plumber? Yeah. And how much do you pay for [[01:35:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5745.92s)]
*  your plumber today? It's expensive when the pipes burst, right? That robot will be $2 an hour. [[01:35:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5752.24s)]
*  But your own home robot will have all the knowledge of an expert plumber. [[01:35:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5759.04s)]
*  Probably 10 years away, to be honest. [[01:36:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5762.96s)]
*  So I would participate in this network for multiple reasons. One, and it's funny because I don't [[01:36:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5766.4s)]
*  still really understand Bitcoin, I have to say. It continues to kind of confound me that it just [[01:36:13](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5773.04s)]
*  runs up and up and up. It doesn't seem like there's anything really there still yet other than future [[01:36:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5779.28s)]
*  appreciation. And that's like a hard trick to replicate. So the incentives that you outlined [[01:36:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5786.320000000001s)]
*  are one, contributing to the public and common good, two, some sort of voting control over how [[01:36:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5793.040000000001s)]
*  pooled resources are allocated. And third is appreciation. Although I have to confess, [[01:36:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5803.200000000001s)]
*  I'm not quite sure. Is there a redemption mechanism that? [[01:36:48](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5808.8s)]
*  No, at the moment, with the way we designed it, it is like Bitcoin. So it appreciates for [[01:36:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5813.280000000001s)]
*  certain reasons Bitcoin. Bitcoin is a hedge against the world doing crap. This is a hedge [[01:36:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5818.08s)]
*  against the intelligence age disrupting economics. And the amount of flops per coin is likely to [[01:37:03](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5823.12s)]
*  outpace that amount of flops per dollar, because it will be provisioning more and more and more [[01:37:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5828.88s)]
*  compute. So that's kind of the theory. We'll see what it's like in practice. And we've still got [[01:37:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5832.88s)]
*  a lot of improvement to go here. But it does seem like, again, we're at this transition phase [[01:37:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5837.68s)]
*  whereby something like this is unequivocally good, especially when you make it in those real [[01:37:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5842.0s)]
*  terms. Like, again, is any podcast listener right now not want to have all the climate knowledge in [[01:37:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5846.08s)]
*  the world organized and available to everyone or that cancer AI available to everyone? And if they [[01:37:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5851.92s)]
*  can contribute to that and have participation in that with the upside and with the US administration [[01:37:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5856.8s)]
*  changing to more positive, even better. $100 billion has gone into Bitcoin ETFs this year. [[01:37:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5861.68s)]
*  Again, order of magnitude to build these things is like maybe a billion or something like that, [[01:37:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5867.04s)]
*  which sounds crazy just to toss around, but you've seen what AI funding rounds are right now. [[01:37:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5873.12s)]
*  Yeah, a billion is very, I mean, at the level of countries, it's obviously extremely [[01:37:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5877.84s)]
*  affordable. So the level of VCs, you know, and again, the wonderful thing about this is once a [[01:38:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5884.56s)]
*  model is trained once, once we get to that level of performance, that cancer model to help everyone [[01:38:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5890.48s)]
*  in all these languages, you don't need to train it again. And I don't think we've ever seen [[01:38:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5896.08s)]
*  anything like that, right? It's like having infinitely replicable graduates or chefs, [[01:38:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5902.0s)]
*  you know, or cook, sorry. What technology aside from continuing to define the protocol itself, [[01:38:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5907.12s)]
*  what technology is needed for this to work? One space I've been watching really closely is [[01:38:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5914.5599999999995s)]
*  distributed training. And it seems like we've just seen like several different papers and [[01:38:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5921.84s)]
*  proof points that show that that is going to work at least with like multiple large ish nodes, [[01:38:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5929.2s)]
*  not necessarily clear if it works at the level of like everybody contributing their spare laptop [[01:38:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5936.4800000000005s)]
*  compute long term. But I'd love to hear your thoughts about sort of distributed compute and [[01:39:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5942.400000000001s)]
*  maybe any other technologies that you see as necessary that don't yet exist. [[01:39:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5948.4s)]
*  I think decentralized training doesn't really make that much sense versus decentralized tuning, [[01:39:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5954.16s)]
*  having army's pages improve the data sets, but DLOCO, Distro and others have shown [[01:39:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5959.12s)]
*  really interesting kind of advances. And again, like I said, I'm on the advisory board of some [[01:39:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5964.799999999999s)]
*  of these and the crypto XAI space is now like $40 billion. They look at something like bit tensor, [[01:39:28](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5968.719999999999s)]
*  that's a $12 billion network that's trading hundreds of millions of dollars a day, [[01:39:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5975.44s)]
*  optimization. It's like the most on steroids Kaggle you can see. You have millions of GPUs [[01:39:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5980.32s)]
*  in the other network available. I think again, the data side will be more interesting. And again, [[01:39:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5986.4s)]
*  this sequential stuff as we get one type models that can run on my M4 max or now M2 max into [[01:39:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5990.24s)]
*  the M4 max looking forward to it. Mano texture display. The probably one area that is probably [[01:39:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=5995.839999999999s)]
*  most important to probably decentralizing this and ownership and controlling governance is [[01:40:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6001.84s)]
*  verifiable inference. And again, we're making some big advances in there. Hyperbolic and Allora and [[01:40:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6006.0s)]
*  others are doing a lot of work around that so that then anyone can contribute their compute. [[01:40:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6012.08s)]
*  But the data equation of this and building goal standard data sets is more important than [[01:40:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6017.2s)]
*  the pre-training. For pre-training, just have a cluster and just train on that cluster. Like [[01:40:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6021.360000000001s)]
*  it's not rocket science. You know how to do it. Like will it be a Larmor type model or transform [[01:40:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6026.96s)]
*  model versus something hybrid, Jamba style, who knows, right? But just make your pick. It's pretty [[01:40:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6032.64s)]
*  straightforward to pre-train. Host training and optimization, distributed stuff becomes very [[01:40:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6037.84s)]
*  interesting there. The verifiable inference allows anyone to contribute their compute. [[01:40:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6043.6s)]
*  And then it's about having secure computation for running these things in regulated industries. And [[01:40:48](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6048.0s)]
*  that's some of the TE work that we've seen. But most of it's coming together now. And especially, [[01:40:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6053.12s)]
*  like I said, against the context of two other things, which were large context windows and [[01:40:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6057.2s)]
*  function calling, like making it more deterministic on the outputs. And the final thing we just need [[01:41:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6062.72s)]
*  is maybe continuous training for the individualized stuff. But again, we've seen big advances in that. [[01:41:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6067.44s)]
*  So continuous learning. So on the verifiable inference, I did one, and again, I'm weak on [[01:41:13](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6073.76s)]
*  crypto. I did one episode on this with Professor Daniel Kang a little more than a year ago now, [[01:41:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6080.72s)]
*  I think. And he sort of framed the problem as like, if you are paying for an API, [[01:41:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6085.52s)]
*  you might want the provider, whether it's open AI or whoever, to have some way of guaranteeing to you [[01:41:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6093.280000000001s)]
*  that they are actually running the model that they said they're running, as opposed to subbing in some [[01:41:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6100.4800000000005s)]
*  cheaper knockoff or whatever and changing things underneath you. [[01:41:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6106.08s)]
*  I get that. I'm a little less clear on what you have in mind when you imagine anybody contributing [[01:41:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6111.52s)]
*  compute through verifiable inference is the idea that like, I run inference locally, and then I [[01:41:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6117.4400000000005s)]
*  send up my like activations and the gradients are sort of computed centrally or something like that. [[01:42:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6122.72s)]
*  Alexa, I think distributed training on millions of GPUs doesn't make sense. But sending a packet [[01:42:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6129.76s)]
*  to you that contains Indonesian classical architectural law and checking that for [[01:42:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6135.2s)]
*  consistency and rewriting some of the data there does make sense, putting it through a Lama 70B or [[01:42:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6142.24s)]
*  80B model. So I think distributed data augmentation makes sense, distributed fine tuning makes sense, [[01:42:28](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6148.08s)]
*  and then comparing those things and then the mining operations as well. But I don't think [[01:42:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6154.48s)]
*  distributed training makes sense or pre-training, shall we say. So post-training data augmentation. [[01:42:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6158.719999999999s)]
*  And for those, you really want to have verifiable, you know what the models are. [[01:42:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6164.0s)]
*  Otherwise, you might get junkiness as it were. So I think that unlocks something. It's not required, [[01:42:48](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6168.639999999999s)]
*  it just helps. Like right now, what we're looking at is literal H100 clusters in various countries [[01:42:54](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6174.4s)]
*  and how we can bootstrap that because they're inevitable. So you might as well put them there [[01:43:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6180.88s)]
*  and use them for healthcare later, right? And then that gives you enough to then tune models in every [[01:43:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6184.400000000001s)]
*  country and spin up teams and national champions and then they can decide what they want as part [[01:43:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6190.24s)]
*  of this network, right? And H100s are a lot easier to track than 1490s or M4 maxes, etc. But if you [[01:43:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6195.2s)]
*  can access that, then like I said, you'll be able to build the better models even better because [[01:43:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6204.08s)]
*  you'll build better data sets even better and faster. I think again, this is not synthetic data, [[01:43:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6209.28s)]
*  it's augmented, filtered, cleared data utilizing the LLAMs. [[01:43:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6214.96s)]
*  Yeah, that's quite interesting. And that's basically the answer to the sleeper agent question too, [[01:43:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6220.639999999999s)]
*  I assume, which I was also wanting to ask, like how do we make sure that people are not poisoning [[01:43:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6224.48s)]
*  the data set from any number of different perspectives? [[01:43:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6231.679999999999s)]
*  You put a massive amount of compute and putting a fine tune over all of it, checking it for [[01:43:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6235.84s)]
*  consistency, adapting it as appropriate, like again, you can afford to go over the top, [[01:43:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6239.68s)]
*  because then you have a gold standard base. It's again, like you have a good curriculum for your [[01:44:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6244.16s)]
*  kid. You feel comfortable about that or comfortable about the mechanism and the methodology. So I [[01:44:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6248.8s)]
*  think again, this is a different type of AI challenge to the other people looking at. But [[01:44:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6254.0s)]
*  I think this is the important one, because it's what affects our lives in a much more direct way, [[01:44:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6260.400000000001s)]
*  but particularly category of model. And this is why as well, when I was looking at it, I was like, [[01:44:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6264.96s)]
*  you can build tokens, which are security tokens, and you can build things that have utility and [[01:44:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6270.72s)]
*  redemption mechanisms. Ultimately, Bitcoin is the most successful currency is $2 trillion. [[01:44:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6276.4800000000005s)]
*  It's moneyness, it's a hedge, it's these. Why don't we replicate that for an AIH? [[01:44:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6281.84s)]
*  And why don't we think about the currency for an AIH? But have this additional thing that I [[01:44:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6286.08s)]
*  think is important. I want to have people able to have some participation because many of the [[01:44:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6290.24s)]
*  listeners here are like, how can I even get involved in this? You don't know how. But if you [[01:44:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6295.44s)]
*  can say, I've got this and I've helped direct the compute towards cancer and it's helping people [[01:44:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6299.679999999999s)]
*  get to their journey on cancer, like giving them the chatbot, great. Or breakthrough done on the [[01:45:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6304.4s)]
*  cancer cluster, great. So are you with the Intelligent Internet, the company, how do you [[01:45:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6308.5599999999995s)]
*  understand your role in this whole thing? Are you like, raising capital to create that [[01:45:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6317.04s)]
*  one central cluster that will do the pre-training and then distribute that out to the [[01:45:23](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6323.84s)]
*  national hyper nodes? Or what is the plan is to basically the company to bootstrap this, [[01:45:28](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6328.88s)]
*  get it going. And then we stack GPUs and we use them ourselves and give them away [[01:45:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6335.28s)]
*  to build fully open source things, data sets, models and systems. [[01:45:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6341.44s)]
*  And that will be a big enabler. We saw this at Stability where we gave away 20 million A100 [[01:45:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6345.52s)]
*  hours and that led to all sorts of amazing things from OpenFile to RWKB to stable diffusion and more. [[01:45:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6350.160000000001s)]
*  People just need to have sometimes a bit of compute. So I think it's getting it going and then [[01:45:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6356.240000000001s)]
*  trying to make it distributed and decentralized within a few years and giving people control of [[01:46:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6362.080000000001s)]
*  their own AI. So every nation who should decide the education data set for the people of India or [[01:46:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6366.56s)]
*  Bangladesh or Pakistan, the people of India or Bangladesh or Pakistan. And likewise, who should [[01:46:13](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6373.12s)]
*  build the generalized data sets for everyone? It should eventually be some sort of multilateral [[01:46:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6379.5199999999995s)]
*  foundation or something like that, but it does require a bit of bootstrapping. So we're a team [[01:46:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6384.4s)]
*  of like 26 people. We haven't done a funding round properly yet. A lot of demand for whatever coin [[01:46:28](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6388.88s)]
*  we do. I think this is the right way, but again, we're finalizing all the details of that. [[01:46:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6394.32s)]
*  So you think you can get over these capital requirement thresholds with a [[01:46:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6398.72s)]
*  coin type approach without needing to go to like a traditional equity market? [[01:46:48](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6408.8s)]
*  Yeah. Again, CryptoXAI is now $40 billion trading, hundreds of millions of dollars a day now actually. [[01:46:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6413.360000000001s)]
*  We could just go that route. We might do a combined one where we do equity and coin and that. [[01:47:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6421.52s)]
*  And I think that in doing that, you'd probably be the highest revenue generating AI company in [[01:47:05](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6425.68s)]
*  the world because demand for high quality digital assets is only going up. Pricing power and APIs [[01:47:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6429.92s)]
*  and SaaS is going down. And nobody's really looked at this. I mean, even for Elon and Sam Altman and [[01:47:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6436.88s)]
*  others, like how difficult is it for them to raise billions of dollars? It's not because they've got [[01:47:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6442.8s)]
*  the pedigree and things. We've built hundreds of state of the art models of different types, our team. [[01:47:28](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6448.56s)]
*  And that's why many of them join me from Stability. [[01:47:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6453.28s)]
*  Having a liquid market for a coin where you can sell coins and literally say, [[01:47:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6457.92s)]
*  this is a cluster that will just look at Alzheimer's. I think there'll be no shortage [[01:47:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6461.28s)]
*  of people who want that. I think having the sovereign reserve aspect to this build up this [[01:47:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6467.04s)]
*  reserve and we'll put all the money into AI for your nation and get the teams and I don't think [[01:47:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6471.44s)]
*  there'll be any shortage of that. Again, time will tell, but initial things have been very positive. [[01:47:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6477.679999999999s)]
*  And it was again, something I was thinking about when I left at the end of March. I was like, [[01:48:03](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6483.28s)]
*  this feels more close to what I originally wanted to do. It's needed. No one else is doing it. And [[01:48:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6488.0s)]
*  it's the right time with where regulation other things are. Because ultimately crypto is a $3 [[01:48:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6492.8s)]
*  trillion industry that needs high quality digital assets that people can allocate to. And we need [[01:48:18](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6498.4s)]
*  to have some sort of intelligent money and these other things. But it's not easy. We're trying [[01:48:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6505.44s)]
*  something new, but it keeps it exciting. It's never a dull moment in the AI space. That's for [[01:48:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6510.8s)]
*  sure. You mentioned a number of projects from Stability and previously you made a comment [[01:48:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6517.6s)]
*  along the lines of finding talent is like the hardest thing or is often a limiting factor. [[01:48:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6523.6s)]
*  Obviously Stability was in the news a lot for lots of different stories over the year or so [[01:48:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6529.76s)]
*  before you left. When people would send those links to me, I always basically said back to them, [[01:48:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6535.76s)]
*  I just watched the research output and Stability has continued to matter certainly as of the time [[01:49:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6542.96s)]
*  of those conversations with just a bunch of really interesting good projects, some of which were [[01:49:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6549.68s)]
*  extremely catalytic, like stable diffusion in terms of changing awareness and what people [[01:49:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6557.280000000001s)]
*  understood to be possible. Some were just like quiet contributions to the commons like the [[01:49:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6562.320000000001s)]
*  lion data set and specifically the aesthetic data set, which is one that there was nothing else [[01:49:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6567.52s)]
*  really like that. And that made a huge difference actually for one of the little projects I was [[01:49:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6572.72s)]
*  doing at my company Waymark. I also love the MindEye and MindEye2 papers, even though those are [[01:49:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6577.200000000001s)]
*  kind of, it's not easy to or not hard to imagine a dystopian version of that in the future as well. [[01:49:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6583.360000000001s)]
*  But the fact that mind reading works in today's world is pretty crazy. What would you say are your [[01:49:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6589.2s)]
*  tips, tricks, heuristics, methods for identifying great researchers? How did you assemble this team [[01:49:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6598.0s)]
*  and create that capability, which I would say not that many different companies around the world [[01:50:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6606.96s)]
*  have done basically from scratch in a short period of time without certainly Microsoft global [[01:50:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6611.76s)]
*  resources. What do you look for? What do you see that tells you somebody is going to make a [[01:50:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6617.36s)]
*  difference? I think it was kind of treating the researchers like creatives. And so we first created [[01:50:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6624.0s)]
*  the communities and again, you know, did a bit of hype, maybe overhype sometimes. You live in your [[01:50:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6629.759999999999s)]
*  land, but it was a crazy time. And so we had about half a million people in the various communities [[01:50:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6635.28s)]
*  that we kind of build, incubated, et cetera. We made it easy to operate and we gave very fast grants [[01:50:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6640.08s)]
*  to anyone that was doing promising research. So I basically kind of led and coordinated the research [[01:50:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6645.76s)]
*  and I've got a good eye for that type of thing. But I really looked for people that it didn't matter [[01:50:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6649.84s)]
*  what the background was. Are they passionate and are they willing to take a risk and try different [[01:50:54](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6654.320000000001s)]
*  things? So many of the research grants we gave and again, research can be back. We're like, try and [[01:50:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6659.2s)]
*  do different things. It doesn't matter if it fails. Whereas researchers use a cost center generally. [[01:51:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6664.16s)]
*  So you can't really take a risk. You know, when it works, we put on the afterburners and we scaled [[01:51:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6668.88s)]
*  it. And then you wanted to empower the teams. So I think last year we had something like 20,000 [[01:51:13](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6673.76s)]
*  applicants for researchers who made 120 offers and 83%, 100 offers, 83% were accepted. And before the [[01:51:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6681.52s)]
*  last, in my entire time at Stability, not a single researcher left for a competitor. And they were [[01:51:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6690.08s)]
*  offered a lot of money because they enjoyed working there because they had the creative freedom, the [[01:51:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6695.360000000001s)]
*  adaptability. Obviously, they were concerned about all the noise and that's very interesting [[01:51:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6699.84s)]
*  things behind that. But they just got on with things because we enabled them. And we said, [[01:51:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6704.24s)]
*  like, they're like, this training went as well. It doesn't matter so much. Whatever you learned, [[01:51:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6709.360000000001s)]
*  how can you go from there? And we ended up achieving state of the art in image, video, 3D, audio, [[01:51:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6713.52s)]
*  protein folding, like small language, a whole bunch of different things by having that mentality. [[01:51:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6719.92s)]
*  You don't need that many people and you don't even need that many PhDs. We had 14 PhDs out of the 80. [[01:52:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6724.64s)]
*  I think things like fast AI and others also kind of rounds. And again, the willingness to try these [[01:52:09](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6729.52s)]
*  things is where communities like Elusa, which we incubated and spun out into a 501C3 and Lion were [[01:52:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6735.92s)]
*  great because you had all these diverse talents kind of coming in. I think it depends what you [[01:52:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6740.64s)]
*  want to do though. Like, we were building state of the art because we were like, we need to [[01:52:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6745.120000000001s)]
*  catch up. But I was like, I can't scale LLMs right now because there's no business model for that. [[01:52:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6749.52s)]
*  I thought video and other things are the way, but again, then I realized that [[01:52:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6755.76s)]
*  like right now video is about to satisfy some learning bead, like it doesn't matter which one [[01:52:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6759.12s)]
*  you use. It was very difficult to figure out the sustainability. So instead we were just like, [[01:52:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6762.72s)]
*  let's do the research and let's enable them. And I think the teams are happy. The rest of the [[01:52:48](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6768.5599999999995s)]
*  organization maybe didn't do quite so well. It wasn't great to see here, but at the very least, [[01:52:54](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6774.5599999999995s)]
*  the researchers were happy, which again, I think the proof was in the pudding. When it came towards [[01:52:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6778.8s)]
*  the end of it, we spun out the Black Forest Labs team and some of the other teams. And I think, [[01:53:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6782.32s)]
*  again, they were happy. And I think having research happiness at the top was very important. [[01:53:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6787.599999999999s)]
*  Mad Fientist That's certainly something you see coming through in the Sam and [[01:53:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6792.24s)]
*  Elon emails as well is just the incredible, intense focus on top talent. [[01:53:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6796.719999999999s)]
*  Richard Yeah. And I think, you know, what type of top talent, what are you trying to do? So [[01:53:23](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6803.599999999999s)]
*  we found the ones in the larger organizations didn't tend to fit in very well versus again, [[01:53:28](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6808.96s)]
*  the Mavericks, because we were trying to push state of the art and try different things. [[01:53:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6813.12s)]
*  There are so many projects that didn't emerge because we made mistakes, not a mystical thing, [[01:53:37](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6817.36s)]
*  they just didn't work. So stable diffusion was 250,000 to 100 hours to train the original. It [[01:53:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6821.44s)]
*  was maybe 5 million in total of tests or something like that and failed experiments, etc. So you never [[01:53:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6827.04s)]
*  see all those, see the other side of things. And again, what are you trying to do? Like, [[01:53:54](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6834.0s)]
*  we had a bit of a crisis today because like, are we trying to build AGI? Do we need to compete [[01:53:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6839.6s)]
*  against all that? That was something I was thinking a lot about last year. Like, are we competing [[01:54:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6844.72s)]
*  against DeepMind? Because we, again, one route is what you've seen with David and mid Jenny, [[01:54:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6848.08s)]
*  you know, and he gave the grant to help Jenny get going. And then he did everything else himself. [[01:54:13](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6853.28s)]
*  I think he's taking money from anyone else. Where he's like, I'm building a research and [[01:54:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6857.04s)]
*  innovation lab and making it sustainable through this business. And we could have just focused on [[01:54:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6861.12s)]
*  media, which is what Stability is doing now. And given our role, that would have been far lower [[01:54:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6865.28s)]
*  and cheaper to do. We could have done that. We were like, we had to compete against DeepMind [[01:54:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6870.16s)]
*  and OpenAI and others, maybe we would have succeeded or not. But then it became very [[01:54:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6875.5199999999995s)]
*  confusing to do that. And I think the one thing that is important if you're trying to build a [[01:54:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6879.84s)]
*  great team in AI is give a very defined function and then build teams that are passionate about [[01:54:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6886.4s)]
*  that function. And then give them the resources they need to do that. And then that will work [[01:54:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6893.36s)]
*  very well. So that's probably the final missing piece, which is the clarity of thought, which is [[01:54:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6898.799999999999s)]
*  why I took a bit more time on the intelligent internet. Now it's very straightforward. If you [[01:55:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6902.24s)]
*  want to have the biggest impact on autism as a researcher, you will come and join me. I will [[01:55:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6906.96s)]
*  have a dedicated team to autism building the open infrastructure for autism, for government, for [[01:55:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6911.44s)]
*  cancer and others. And so that's why we're setting up this time. And then the monetization [[01:55:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6915.6s)]
*  mechanism is very straightforward. And then hopefully lots of GPUs will be coming their way [[01:55:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6920.08s)]
*  to make them even happier. But you don't really need that many, to be honest, compared to what [[01:55:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6924.96s)]
*  many people say to have an impact. A couple of maybe last big picture questions, and I really [[01:55:28](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6928.96s)]
*  appreciate how generous you've been with your time. And it's been great to get a deeper look at your [[01:55:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6935.92s)]
*  thinking as you spit up the intelligent internet. You mentioned that you had signed the pause letter [[01:55:42](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6942.48s)]
*  in like mid 23 when that came out, maybe right after GPT-4, the original one. I sometimes call [[01:55:48](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6948.48s)]
*  myself an adoption accelerationist, a hyperscaling pauser. I don't spend too much time advocating for [[01:55:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6955.44s)]
*  a pause because I do still think there's like some room at least to advance the state of the art [[01:56:03](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6963.12s)]
*  before it gets like too crazy. And I do want those cancer cures, of course. Where are you now on, [[01:56:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6967.68s)]
*  if you see these sort of what people really need to live better lives, either already or soon, [[01:56:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6976.72s)]
*  decoupling from what is going on to advance the state of the art, where does that leave you in [[01:56:23](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6983.6s)]
*  terms of should governments enforce a pause? Obviously there's a collective action problem [[01:56:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6989.84s)]
*  there too if they wanted to, but what do you think people should do or what should the goal be [[01:56:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=6995.28s)]
*  for that sort of thing? So when I signed that letter last year, I think it was the only AI CEO [[01:56:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7001.92s)]
*  to sign it, actually. Elon signed it, but he wasn't an AI CEO yet. I did it because I wanted, [[01:56:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7007.12s)]
*  like my base assumption was that if we got to AGI and my thinking then it would be like Scarlett [[01:56:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7011.92s)]
*  E. Hansen in that movie, her, you know, goodbye and thanks for all the GPUs because humans are kind [[01:56:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7016.0s)]
*  of boring. That's the kind of narrow AGI thinking. The reason I signed it was I was like there needs [[01:57:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7020.08s)]
*  to be more discussion about this. Just like right now, the amount of discussion about labor [[01:57:05](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7025.68s)]
*  displacement and massive economic turmoil that will be caused by this technology is like nothing. [[01:57:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7030.96s)]
*  Like talk to top, they're not even thinking about it. It's insane. Like I like the example of the [[01:57:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7037.36s)]
*  Filipino call center sector, it's dead. Why would you hire another call center worker? You look at [[01:57:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7041.84s)]
*  its waves of industry and Devon and these other things like that's going to be bad. Now where I [[01:57:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7047.04s)]
*  am is that it's too late to have a thought, which is again why I said back then let's get this out [[01:57:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7053.2s)]
*  and maybe we move too far towards AI doomerism, killerism, things like that. But we do have to, [[01:57:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7058.88s)]
*  again, be very measured in the way we think about things. An example of this is copyright law and IP. [[01:57:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7064.24s)]
*  So, you know, I got sued for $1.6 trillion of stability by Getty, even though we didn't use [[01:57:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7071.2s)]
*  any of their images in the thing and we felt that all the watermarks and stuff. And in fact, [[01:57:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7076.32s)]
*  the original stable diffusion was trained by Robin and his team before they joined stability. So we [[01:58:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7080.5599999999995s)]
*  made short time arms length. We didn't even look at everything. That was kind of deliberate. [[01:58:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7084.639999999999s)]
*  Today in the UK, the UK government is advocating that you can have an exemption where AI companies [[01:58:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7090.16s)]
*  can train on any copyrighted material you can see. You know, and that's similar to Singapore. If you [[01:58:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7095.759999999999s)]
*  can legally access it, you can train on it, which kind of makes sense because when you have a [[01:58:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7101.679999999999s)]
*  Tesla Optimus robot, is it going to close its eyes and its ears when it sees copyrighted thing? [[01:58:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7105.12s)]
*  Like, oh no, I'm hearing Taylor Swift. Let's turn off my ears. Probably not going to happen, right? [[01:58:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7109.84s)]
*  But these are complex and nuanced topics just like pausing and others. So I was like, [[01:58:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7114.4s)]
*  that's the last chance we have to do that. Now the biggest companies in the world are AI companies. [[01:58:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7118.4s)]
*  Microsoft, Google, Amazon are AI first companies. Jeff Bezos is back at Amazon. [[01:58:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7124.08s)]
*  You know, Sergey Brin is back at Google. You know, Bill Gates, he's back at Microsoft, I think as well. [[01:58:48](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7128.8s)]
*  Governments are trying to build their entouraging capital stock. They realize this is the biggest [[01:58:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7135.68s)]
*  thing. You can't stop it now. You can't take time to think before the next wave of disruption occurs. [[01:58:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7139.44s)]
*  And I don't think that governments really can afford to regulate. Like Europe is falling into [[01:59:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7146.16s)]
*  its trap, right? But again, people will just build models and have extra competitiveness where they [[01:59:13](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7153.12s)]
*  can. Again, Singapore is a great example that the UK may follow because you can't afford to be left [[01:59:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7159.5199999999995s)]
*  behind. You can't afford not to build the AI chips. You can't afford not to support. Again, [[01:59:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7164.24s)]
*  I would use Europe as the maybe one exception to that. And the final part of that is America [[01:59:29](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7169.12s)]
*  under the new administration. Crypto and AI are going to go full pelt under the new administration. [[01:59:35](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7175.76s)]
*  There's no way it won't. And you have to keep up with America. How else are you going to kind of [[01:59:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7181.679999999999s)]
*  manage it? And so it's going to be very interesting to see how that plays out. [[01:59:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7187.12s)]
*  So if it's not a pause anymore, is there any other regulatory action that you would recommend? I mean, [[01:59:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7190.24s)]
*  people are groping or grasping for anything they can get their hands on. Liability rules come up a [[02:00:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7202.88s)]
*  lot. That was kind of at the, by the end, maybe not so much the heart of the SB 1047 debate, [[02:00:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7210.24s)]
*  but at one point it was more central to that idea was maybe we'll hold the foundation model [[02:00:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7215.84s)]
*  developers accountable. But then geez, how does that impact open source? And how does that impact [[02:00:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7222.0s)]
*  the relationship with local fine tuners and developers? Any thoughts on sort of who should [[02:00:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7226.88s)]
*  bear responsibility for incidents of AI gone wrong? Yeah, it's very interesting to see all [[02:00:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7234.16s)]
*  these things evolve. Like I've had some similar discussions to what Mark Andreason said about how [[02:00:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7240.24s)]
*  many AI companies can succeed, which is another reason to get decentralized after that experience. [[02:00:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7244.08s)]
*  Gosh, there was a lot of very strange ones. Liability, these things won't work because again, [[02:00:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7249.12s)]
*  you're in a competitive environment, which from a game theoretic perspective means that you can't [[02:00:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7253.2s)]
*  afford to lose competitiveness over a defined period of time, especially as the Overton window [[02:00:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7256.8s)]
*  is shifted on politics now to kind of be more techno friendly after literally the US government [[02:01:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7261.2s)]
*  is trying to strangle the sector or make it so only a few people can do it. I think that the biggest [[02:01:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7267.36s)]
*  impact simplest thing is this if AI is used in a decision making process in any regulated industry, [[02:01:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7274.0s)]
*  it has to have transparent data logged. Because you are what you eat. These models are ultimately [[02:01:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7280.88s)]
*  the data. Like if you look at all these different architectures, again, we funded RWKV getting off [[02:01:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7287.84s)]
*  the ground or Mamba or any of these, all the performances are the same for the sake of data [[02:01:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7293.04s)]
*  center. It comes down to data. You are what you eat. And the decision making AI is the ones that [[02:01:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7298.88s)]
*  will kill us or make us successful ultimately. The rest is kind of helpful. And so introducing [[02:01:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7304.8s)]
*  a level of data transparency over that is important because then people will spot, just like when we [[02:01:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7311.6s)]
*  released stable diffusion and there was a lot of them, they were like, oh, there's weird shit in [[02:01:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7317.28s)]
*  here. Yeah, we don't want weird shit. If you don't want to know about nuclear bioweapons, [[02:02:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7320.56s)]
*  don't have nuclear bioweapons in the data set. I think the other thing is building good standardized [[02:02:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7326.72s)]
*  data sets for knowledge and other things. Again, you said you can't go and achieve an Alan AI, [[02:02:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7334.4800000000005s)]
*  the level of scale AI and other things like that. That's kind of embedded in these models from [[02:02:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7341.360000000001s)]
*  GPT-4 and others. Building a goal standard of data sets for common knowledge and cultural knowledge [[02:02:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7345.280000000001s)]
*  just makes everything safer because why wouldn't you use those as your base? And again, think [[02:02:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7351.76s)]
*  about models of the curriculum that is perspective was going through high school, university, [[02:02:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7356.56s)]
*  and then being specialized. What are we really training when we've got trillions of tokens? [[02:02:40](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7360.96s)]
*  What's in there? Have you looked in these data sets? They're crap. Build better-based data sets [[02:02:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7366.400000000001s)]
*  that you can iterate on and then things will be safer. Have a simple legislation of any decision [[02:02:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7370.56s)]
*  making AI in any regulated industry needs to have transparent data and things will be safer. [[02:02:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7376.400000000001s)]
*  It doesn't necessarily need to be interpretable, but we know that the AI functionality is a [[02:03:00](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7380.719999999999s)]
*  derivation of the base data. Beyond that, I can't see what other regulations will stick, honestly, [[02:03:05](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7385.04s)]
*  but one that I would highly recommend is regulations around speech data. We built a state-of-the-art [[02:03:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7390.5599999999995s)]
*  speech model. We did not release it. It is dangerous. I don't think people appreciate how [[02:03:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7395.759999999999s)]
*  dangerous it is, but let me give a practical example to people listening here. Imagine someone [[02:03:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7400.5599999999995s)]
*  that you cared about more than anyone, you respected more than anyone. [[02:03:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7405.12s)]
*  Replicate their voice and create a version that you can call them and that you can have [[02:03:27](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7407.92s)]
*  discussions on Zoom with them, which is all capable now. How much would you listen to that [[02:03:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7413.76s)]
*  AI versus a normal AI? More than anything. If that AI can then access Obama's speech patterns and [[02:03:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7416.96s)]
*  Churchill and other things, that wave of manipulative speech AI is coming. We don't have the protections [[02:03:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7424.32s)]
*  against that. I think those should be legislated heavily and regulated heavily. [[02:03:50](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7430.88s)]
*  And regulated heavily, especially when you think about our kids. Our kids literally have no [[02:03:56](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7436.88s)]
*  defenses against that. So those are probably the areas that I'd look at. [[02:04:01](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7441.2s)]
*  One other random idea I've had and I haven't really developed too much is speed limits. [[02:04:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7446.0s)]
*  This is basically just the idea that maybe individuals or organizations or whatever should [[02:04:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7452.64s)]
*  only be able to do so much inference per unit time or only use so many flops per unit time. [[02:04:18](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7458.96s)]
*  And it's kind of like just to keep things operating at a sort of human speed. Maybe [[02:04:25](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7465.360000000001s)]
*  we need something like that. You're probably going to have the same game theoretic objection, [[02:04:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7471.76s)]
*  though, I suppose. You can't do that. It's like saying you can only hire so many people. [[02:04:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7476.0s)]
*  Again, you will lose competitiveness in this increasingly competitive environment. [[02:04:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7481.52s)]
*  And organizations, others will come out of it. And what is the role of government? [[02:04:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7485.68s)]
*  Someone said it's the head to the monopoly of political violence. [[02:04:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7491.2s)]
*  Ultimately, it's about protection of people and representation of people. [[02:04:54](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7494.4s)]
*  And if this is the big leap up, you have people with AI outperform people without AI. You can't [[02:04:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7499.36s)]
*  really have these things except when there's unions and others involved. So you look at your rate [[02:05:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7504.88s)]
*  limits and you look at the longshoreman strike in the US. That is a rate limit. They will only allow [[02:05:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7510.64s)]
*  a certain amount of robots. Is that optimal for a society? No. It just so happens they have a [[02:05:16](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7516.4800000000005s)]
*  physical barrier. And so we'll see those things maybe emerge in certain areas where you regulate [[02:05:23](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7523.2s)]
*  some of the amount of AI that you can have. But if you're keeping a free market, which again, [[02:05:28](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7528.72s)]
*  unions are counter to in some ways, then you can't have that. Because who knows what goes on inside [[02:05:34](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7534.24s)]
*  an organization, right? And other things. And can you coordinate this globally? This is the [[02:05:41](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7541.2s)]
*  bigger thing. Absolutely not. Again, you think about small nations, they can suddenly outcompete. [[02:05:46](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7546.0s)]
*  You look at the role of capital here, like the Saudis and others of the world, they're like, [[02:05:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7555.52s)]
*  once this stack is built, that's why DOA has gone heavily into Cerebris. [[02:05:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7559.52s)]
*  Like, yeah, build those silicon wafers. So I think that ultimately, [[02:06:04](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7564.72s)]
*  you can just set good defaults. And you can use market forces and others for that. And again, [[02:06:08](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7568.96s)]
*  push for data transparency. And some of these edge cases like speech, you do need some protection [[02:06:17](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7577.2s)]
*  because I haven't figured out, unless we were at airports all the time, how do you protect against [[02:06:22](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7582.32s)]
*  that? Yeah, I mean, it seems like there's probably going to be more of those things too popping up [[02:06:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7586.8s)]
*  that we don't even necessarily have any precedent for. It's hard to say, of course, what they are. [[02:06:33](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7593.6s)]
*  I certainly expect a lot more weird stuff to start to happen. We live in interesting times, eh? [[02:06:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7599.4400000000005s)]
*  Yeah, no doubt. When you talk about data set filtering, if you don't want the models to know [[02:06:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7605.76s)]
*  about bio nuclear weapons, as you put it, you don't have that in the training data. Is that [[02:06:51](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7611.52s)]
*  akin to your earlier comment that you think scaling is going to level off? There seems to [[02:06:58](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7618.72s)]
*  be a relation there where it's like if they generalize out of domain to a certain degree, [[02:07:06](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7626.4s)]
*  then they'll fill in those gaps is kind of what a lot of people worry about. [[02:07:12](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7632.48s)]
*  But you don't expect that as much? Well, we embody these base models with a bit too much stuff, [[02:07:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7635.92s)]
*  even with the SFT, right? Ultimately, they're just a bunch of like freaking weights, a bunch of ones [[02:07:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7641.04s)]
*  and zeros, right? It's not like a logical system. So you look at the composition of that, how it's [[02:07:26](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7646.0s)]
*  built up, and you're like, I want a base level of reasoning model. It doesn't make much sense [[02:07:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7650.88s)]
*  that we have all the data in these models and knowledge in these models either. That seems a [[02:07:36](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7656.8s)]
*  bit wasteful. How much these models know about, they don't need to know. And you think about, [[02:07:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7659.68s)]
*  again, a model that's a medical doctor, should it need any more data than a medical professional [[02:07:43](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7663.6s)]
*  gets from kindergarten to medical school? No. I think Yann LeCun has talked about this in some [[02:07:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7669.2s)]
*  of the JEPA kind of things. Can it infer a reason? Yes, because it'll have that capability. [[02:07:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7673.92s)]
*  But again, the question is, what are you releasing onto the world as infrastructure or services? [[02:07:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7679.68s)]
*  Because you can control things at a service level, infrastructure level is a bit different. [[02:08:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7687.12s)]
*  Like what base model are you going to release stable diffusion style? People use it for good [[02:08:10](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7690.96s)]
*  things, they use it for bad things. But it was inevitable that you would have reasoning models [[02:08:15](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7695.04s)]
*  like that, right? And again, this is especially important when it comes to long context window [[02:08:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7699.360000000001s)]
*  models. Like again, 2 million, 10 million tokens is a freaking lot. And so a reasonable reasoner, [[02:08:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7704.48s)]
*  he says, when Gemini 01 comes in Gemini 2, or the equivalent Lama version of that. [[02:08:31](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7711.44s)]
*  Yeah, I'll be able to figure out just about anything. It's inevitable. But in day to day, [[02:08:39](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7719.759999999999s)]
*  how many people are going to load it with all the bio nuclear stuff in the context window? And that's [[02:08:45](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7725.679999999999s)]
*  something that is relatively controllable. So I think that we just need to tidy up the stuff in [[02:08:49](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7729.84s)]
*  the base models. And that's just not even a question of filtering. It's a question of, [[02:08:55](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7735.52s)]
*  how do we feed them? Like, what is the curriculum? And then once you have gold standard version of [[02:08:59](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7739.360000000001s)]
*  that, I think things become safer because there's less likelihood of them going initially rogue, [[02:09:05](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7745.04s)]
*  because there'll be a function of the data goes in. Now, once you make them agentic, [[02:09:11](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7751.360000000001s)]
*  and they will traverse the internet for the rubbish that it is, then maybe a different story. [[02:09:14](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7754.96s)]
*  Again, this is an incremental thing. There are no easy answers. Otherwise, [[02:09:19](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7759.04s)]
*  someone would have answered already. It sounds like your basic vision on the safety side is [[02:09:21](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7761.76s)]
*  structured access to state of the art, combined with open data, open weights, open access, [[02:09:30](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7770.56s)]
*  satisfying day to day usage models. Yes, from a functional perspective, [[02:09:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7778.0s)]
*  universal basic AI for everyone, they get smarter and smarter, and we all agree on [[02:09:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7784.08s)]
*  that can be localized and specialized with the power going back to the people, [[02:09:47](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7787.92s)]
*  then personal AI will be driven by market forces with your Apple and Google and other intelligences. [[02:09:52](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7792.08s)]
*  And then you will have specialized, highly intelligent models. That's where regulation [[02:09:57](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7797.2s)]
*  can come in access can come in for the most capable models out there. But I think it'd be [[02:10:02](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7802.8s)]
*  very difficult. So instead, I think what we should do is focus on if they're in decision making [[02:10:07](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7807.68s)]
*  positions, at least have the data transparent. That could be a great note to end on. Is there [[02:10:13](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7813.76s)]
*  anything we didn't touch on or anything else you want to share about the intelligent internet [[02:10:20](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7820.5599999999995s)]
*  before we break for today? We haven't really pushed it out there. Have a read. I think it's [[02:10:24](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7824.48s)]
*  a nice document along with I had to think about AI. We'll have a whole bunch of stuff in the new [[02:10:28](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7828.72s)]
*  year. And again, happy to see feedback. It's an iterative process. But it's been a lot of fun. [[02:10:32](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7832.5599999999995s)]
*  Alrighty, well, we'll certainly keep watching. And maybe I'll be lucky enough to have a [[02:10:38](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7838.72s)]
*  chance for another conversation once you've finalized all those details and put that out [[02:10:44](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7844.08s)]
*  there. For now, I'm Ahmad Mustak, founder of the intelligent internet. Thank you for being part of [[02:10:48](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7848.0s)]
*  the cognitive revolution. [[02:10:53](https://www.youtube.com/watch?v=SEd3hzuJ-Wk&t=7853.68s)]
