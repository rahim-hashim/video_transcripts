---
Date Generated: August 03, 2024
Transcription Model: whisper medium 20231117
Length: 3802s
Video Keywords: []
Video Views: 196
Video Rating: None
Video Description: Nathan explores the future of AI-generated video with Joshua Xu, founder of HeyGen, and Victor Lazarte from Benchmark. In this episode of The Cognitive Revolution, we discuss HeyGen's success in practical AI video creation, serving over 40,000 businesses. Learn about the transformative potential of AI in video production, from content translation to personalized experiences, and HeyGen's industry-leading approach to trust and safety.

Apply to join over 400 founders and execs in the Turpentine Network: https://hmplogxqz0y.typeform.com/to/JCkphVqj


RECOMMENDED PODCAST: Second Opinion
A new podcast for health-tech insiders from Christina Farr of the Second Opinion newsletter. Join Christina Farr, Luba Greenwood, and Ash Zenooz every week as they challenge industry experts with tough questions about the best bets in health-tech.
Apple Podcasts: https://podcasts.apple.com/us/podcast/id1759267211
Spotify: https://open.spotify.com/show/0A8NwQE976s32zdBbZw6bv


SPONSORS:
Building an enterprise-ready SaaS app? WorkOS has got you covered with easy-to-integrate APIs for SAML, SCIM, and more. Join top startups like Vercel, Perplexity, Jasper & Webflow in powering your app with WorkOS. Enjoy a free tier for up to 1M users! Start now at https://bit.ly/WorkOS-TCR

Oracle Cloud Infrastructure (OCI) is a single platform for your infrastructure, database, application development, and AI needs. OCI has four to eight times the bandwidth of other clouds; offers one consistent price, and nobody does data better than Oracle. If you want to do more and spend less, take a free test drive of OCI at https://oracle.com/cognitive

The Brave search API can be used to assemble a data set to train your AI models and help with retrieval augmentation at the time of inference. All while remaining affordable with developer first pricing, integrating the Brave search API into your workflow translates to more ethical data sourcing and more human representative data sets. Try the Brave search API for free for up to 2000 queries per month at https://bit.ly/BraveTCR

Omneky is an omnichannel creative generation platform that lets you launch hundreds of thousands of ad iterations that actually work customized across all platforms, with a click of a button. Omneky combines generative AI and real-time advertising data. Mention "Cog Rev" for 10% off https://www.omneky.com/

Head to Squad to access global engineering without the headache and at a fraction of the cost: head to https://choosesquad.com/ and mention “Turpentine” to skip the waitlist.

CHAPTERS:
(00:00:00) About the Show
(00:00:22) Sponsor: WorkOS
(00:01:22) About the Episode
(00:05:25) Introduction
(00:06:15) Joshua's Background
(00:09:47) Video Consumption Trends
(00:10:49) Creating with HeyGen
(00:12:46) Localization Benefits
(00:14:02) Cost of Localization
(00:16:19) Sponsors: Oracle | Brave
(00:18:24) Content Creation
(00:19:32) User Journey
(00:23:56) Avatar Usage
(00:26:33) Engagement vs. Realism
(00:31:44) Future of Content
(00:33:50) Gaming Applications (Part 1)
(00:35:43) Sponsors: Omneky | Squad
(00:37:30) Gaming Applications 
(00:39:27) Personalized Video Potential
(00:42:57) Future of HeyGen
(00:44:49) Improving Quality
(00:46:53) B-Roll Generation
(00:49:13) Creator Experience
(00:50:56) AI Tools Integration
(00:54:21) Trust and Safety
(00:59:35) Celebrity Restrictions
(01:01:34) Closing Remarks
(01:03:03) Outro

---
SOCIAL LINKS:
Website : https://www.cognitiverevolution.ai
Twitter (Podcast) : https://x.com/cogrev_podcast
Twitter (Nathan) : https://x.com/labenz
LinkedIn : https://www.linkedin.com/in/nathanlabenz/
Youtube : https://www.youtube.com/@CognitiveRevolutionPodcast
Apple : https://podcasts.apple.com/de/podcast/the-cognitive-revolution-ai-builders-researchers-and/id1669813431
Spotify : https://open.spotify.com/show/6yHyok3M3BjqzR0VB5MSyk
---

# AI Avatars & the Future of Video, with HeyGen CEO Joshua Xu and Benchmark's Victor Lazarte
**Cognitive Revolution:** [August 02, 2024](https://www.youtube.com/watch?v=5ShB2P3cuuA)
*  Hello and welcome to the Cognitive Revolution, where we interview visionary researchers,
*  entrepreneurs, and builders working on the frontier of artificial intelligence.
*  Each week we'll explore their revolutionary ideas and together we'll build a picture of
*  how AI technology will transform work, life, and society in the coming years.
*  I'm Nathan Labenz, joined by my co-host Eric Torenberg.
*  This episode is brought to you by Work OS. If you're building a B2B SaaS application,
*  at some point your customers will start asking for enterprise features like SAML authentication,
*  SCIM provisioning, role-based access control, and audit trails. That's where Work OS comes in,
*  with easy to use and flexible APIs that help you ship enterprise features on day one without
*  slowing down your core product development. Today, some of the hottest startups in the
*  world are already powered by Work OS, including ones you probably know like Proplexity,
*  Vercell, Jasper, and Webflow. Work OS also provides a generous free tier of up to 1 million
*  monthly active users for user management, making it the perfect authentication and
*  authorization solution for growing companies. It comes standard with rich features like bot
*  protection, MFA, roles and permissions, and more. If you're currently looking to build SSO for your
*  first enterprise customer, you should consider using Work OS. Integrate in minutes and start
*  shipping enterprise plans today.
*  Hello, and welcome back to the Cognitive Revolution. I'm Nathan's AI avatar, created
*  on HeyGen, and I'm here to introduce Nathan's conversation with Joshua Xu, founder and CEO
*  of HeyGen, a leader in AI-powered video creation, and Victor Lizarte, general partner at Benchmark,
*  which recently invested $60 million in the company. Over the last two years, while generative AI for
*  images and text were crossing critical thresholds for usefulness and hitting mainstream adoption,
*  AI-generated video has remained, for the most part, a future promise, something that AI watchers
*  could clearly see coming and which has provoked much speculative analysis, particularly about the
*  future of the entertainment industry and the risks associated with convincing deep fakes, but still,
*  for practical purposes, a novelty technology that produces uncanny value outputs. That has started
*  to change recently with OpenAI showing off Sora's general world modeling capabilities, Google
*  previewing Vio, China's Qishu technologies debuting their Kling model, and most recently,
*  Runway's Gen 3, among many other options starting to hit the market. But even today, these products
*  remain quite unwieldy and inconsistent, and as a result, impractical for most people to actually
*  use in their day-to-day work. Against that backdrop, HeyGen's success stands out all the more.
*  By focusing their product development work on practical business needs, emphasizing the value
*  drivers of quality, consistency, and control, while taking a best tool available approach to technology,
*  ensembling their own AI avatar models with best-in-class tools for script writing and
*  text-to-speech, they've established a market-leading position in the generative AI for video space and
*  are now serving more than 40,000 businesses and earning $35 million in annual revenue.
*  In this conversation, we discuss what customers are using HeyGen for today,
*  which includes creating new video content that wouldn't otherwise have been possible,
*  translating video into more than 100 local languages, and personalizing video down to the
*  individual viewer level, and Joshua and Victor paint a picture of a world where video content
*  is not just more easily produced, but fundamentally transformed. Imagine immersive video experiences
*  that adapt in real time to viewer interactions, or AI avatars that can engage in live,
*  dynamic conversations. These aren't just disruptive alternative production methods
*  that allow us to make the same content with fewer resources. They are potential paradigm shifts
*  in how we think about video in general. As someone who's spent much of the last three years applying
*  AI to video creation at Weymark, with business results that, while not as explosive as HeyGen's,
*  are objectively strong in their own right, I can say that such reminders to zoom out and challenge
*  oneself to think bigger and stranger are always worthwhile. As it happens, we had a recording issue
*  in this episode that turned out to be the perfect opportunity to demonstrate the power of HeyGen's
*  product. For whatever reason, Joshua's video track, which looked fine while recording, was corrupted.
*  We considered releasing this episode in audio-only form, but ultimately decided to recreate Joshua's
*  portion of the video with HeyGen. After this intro, the video you'll see of Nathan and Victor will be
*  real, and the video you'll see of Joshua will be created with his HeyGen avatar, based on the
*  transcript of the original conversation. I have to say, it came out amazingly well. Toward the end
*  of the episode, we also discuss HeyGen's industry-leading approach to trust and safety.
*  At a moment when most AI application developers are just trying to make their products work,
*  and many are ignoring risks of misuse and abuse entirely, HeyGen has done an outstanding job of
*  taking the responsibility that comes with the power of their product seriously, and has implemented
*  robust consent mechanisms to protect famous people from being impersonated and other safeguards
*  designed to protect the public from political misinformation and scams. Their thoughtful approach
*  demonstrates that rapid innovation and ethical considerations can go hand in hand, and I recommend
*  that any entrepreneur or AI product builder looking for inspiration in this area check out
*  HeyGen's product experience, or read my Red Teaming and Public thread that breaks down their
*  defense-in-depth strategy. And by the way, if anyone listening would be interested in contributing to
*  the Red Teaming or other safety testing of public and soon-to-launch products, please send me a DM.
*  The Red Teaming and Public project has been a bit quiet lately, as another team member has taken the
*  lead and is building some foundational infrastructure behind the scenes, but we are still very much
*  interested in connecting with folks who enjoy testing and breaking AI product guardrails.
*  As always, if you're finding value in the show, we'd appreciate it if you'd take a moment to
*  review us on Apple Podcasts or Spotify or just share online with your friends. Of course,
*  we always welcome your feedback and your AI advisor and AI engineer resumes via our website
*  cognitiverevolution.ai, and you can always DM me on your favorite social network. For now,
*  I hope you enjoy this glimpse into the transformative potential of AI-powered video
*  with Victor Lazarte of Benchmark and Joshua Xu of HeyGen.
*  – Joshua Xu, founder and CEO at HeyGen, and Victor Lazarte, general partner at Benchmark.
*  Welcome to the Cognitive Revolution.
*  – Thank you. It's great to be here.
*  – Thank you for having me.
*  – My pleasure. This is going to be fun. So for folks that don't know HeyGen,
*  I suspect most probably are at least passingly familiar. You guys do generative video,
*  space near and dear to my heart, though I come at it from a pretty different angle product-wise.
*  I thought it'd be interesting just to kind of start off with briefly, how did you get into
*  this business? I heard this story on the NoPriors podcast, but give us kind of the brief background
*  as to how you got into HeyGen. We can talk about where the business is today. I really
*  am interested in then going more deeply into use cases, technology, and especially the trust
*  and safety stuff, given the nature of the product that you've built. But the floor is yours for the
*  backstory. – Yeah, sure. Yeah. So before founding HeyGen, I was working at Snap,
*  starting from 2014 to late 2020. So initially, I was working at an advertising team,
*  basically building out a lot of ad systems at Snap, working with a lot of advertisers,
*  helping them to generate right ROI on the Snapchat's ad platform. And then later on,
*  I switched my team to work on camera, AI camera. Basically, I still remember back then in 2018,
*  technology, there's nothing called generative AI, but there's some technology called GAN,
*  where you can actually generate something that does not exist in the role.
*  And during that time, that was just the first time I saw a computer can generate something
*  that does not exist in the role and still feels high quality and highly realistic.
*  And I was staying very frontline about those technologies and just had a feeling that
*  that could potentially change the way how people create content. So especially if you look at the
*  past, I would say almost 10 to 15 years, the new content platform evolved with the raising of
*  mobile camera. And since I don't fall came out, this application such as Instagram, Snap, TikTok,
*  like I lost so many creators to be able to create good contents. But, and especially working on the
*  camera software, we still see many people are not able to create good content with the mobile camera.
*  Either they are strong in front of camera, they don't have time for the camera, or they're not
*  good at performing in front of the camera. And we found that if we have a way to really replace
*  that component, replacing the camera, we could have a way to really unlock the visual storytelling
*  for everybody. And that's how we kind of like found Hey Jan, we wanted to replace camera because we
*  think AI can create the content, AI could become the new camera. And we want to use AI to generate
*  the video instead of having people to actually film it with the camera. That's how we get started,
*  you know, from Hey Jan back in December of 2020. Cool. Well, it's been your timing is certainly
*  impeccable from the original adversarial network generative AI technology. Obviously, now we've got
*  a much more powerful set of tools to choose from. And you've grown quite a business on this
*  technology with I understand 40,000 plus businesses that use it $35 million in annual revenue,
*  which is pretty impressive. And obviously a big raise from benchmark to, you know, put a further
*  feather in your cap as well. So a lot of great traction. I'd love to dig in a little bit on like,
*  what do people use this product and platform for? I feel like, you know, people had told me for a
*  long time, Oh, you should do a podcast. I'm always told I have a voice for radio. And sometimes they
*  also say I have a face for radio, but certainly the voice they say so. And I was always like,
*  Oh man, I don't know. You know, the world has so much content in it. And I'm not like that
*  charismatic. I'm not that funny. It was only when I got totally obsessed with AI to the point where
*  I felt like, well, actually in this narrow domain, like I might have something, you know, that would
*  be interesting enough, you know, that people might want to listen to it. So I guess for starters,
*  I'm like, in a world that's so awash with content, like, where do we need more content? What kind of
*  content do you see not existing that you're allowing to come to exist? And why does that matter?
*  So I guess first of all, I would start it with the audience, the consumer live that video first
*  world. And everybody wants to watch videos. There's more than a billion hours of videos being watched
*  on YouTube every day. And the example I used was that, you know, I had some issue with my car
*  the other day and my Tesla. And I didn't actually look at the menu, but I should look up a video on
*  YouTube, because like video is just much easier for me to get the information I want. And for me to
*  consume as a consumer. I would say one thing we learned from customer is that every business want
*  to make more videos. But the problem is that video is expensive to make. What we are trying
*  to solve the problem here is that helping the business to be able to make more videos to unlock
*  their needs to match up the pace with what their customer needs. So that's one angle to look at it.
*  And I think Nathan, you asked a little bit about how people are using it today.
*  And I would generally just like categorize into three, create, localize and personalize
*  at the current platform or pageant. What I mean by create is that people can come into the platform,
*  they can either create their own after or set up the stop out that we have, and then pick up a
*  template and type the script to generate a video. That's the creative piece about that. This is great
*  for product spinner, product demo, product announcement, and some of the training,
*  learning development videos. And the second piece comes to the localization, where we can take your
*  existing videos and translate that into more than 40 different languages. We have a feature
*  called video translation, where we can take the original video, preserve the voice of the original
*  speaker, and also the facial expression, but in just different languages. And lots of companies
*  use it for localize their content, especially for some large corporate where they have a specialized
*  localization team. We help them really, really shrink down the workflow cost and time by more
*  than 10x. The last piece I would say just a personalization, where you can actually highly
*  personalize your message. We have seen personalization at email, right? Basically, you've got email who
*  say, hey, Nathan, do you need this service or that? But we invent in a way that you can do that in
*  video as well. I think personalization on name is just one starting point, but we can also personalize
*  the content itself. What actually works for you in your business, combining with the power of LLM,
*  companies like Publicis Group create more than 100,000 personalized email as a video format and
*  send it to internal employees as a thank you video. And those are the main three categories,
*  I would say, in the use case of HM. So create, localize, and which is a translate and personalize.
*  I'll start in the middle. The localization one to me seems like an absolute no brainer. I think one
*  of the things that's kind of most exciting about the AI moment is just how many barriers it stands
*  to break down and how much better we'll be able to communicate with people around the world. I'm
*  going to Brazil next week, and I don't have to worry really at all about translation anymore. I
*  could just have my phone do it, and that's going to be such a freeing experience relative to
*  certainly whatever I would do in the past, get out my phrase book or whatever. So it makes a ton of
*  sense to me that businesses would be like, sure, you're telling me I can affordably translate my
*  content into however many languages and reach people all over the world. That seems like something
*  that is probably a pretty easy sell. What does the cost look like on that just to kind of make the
*  ROI case for a business if I'm in e-commerce or whatever and I've got, let's say, 100 product
*  videos and I want to translate that into 100 languages, I'm going to make 10,000 videos?
*  I can only imagine what my cost would be if I was going to do that old school. The obvious outcome
*  would be just to not do it. In the heygen world, what would that cost me? Give me a little sense
*  of the practicalities of doing a project like that. Yeah, sure. I think really the cost about
*  using AI to generate that, to do that, is really the cost of our GPU in that case, the GPU compute
*  cost. Typically when we see the doubling role, I would put it into two ways. Traditionally,
*  when we talk about localization, we mainly talk about hiring a voice over actor to dub it and then
*  put it back on the original video. That costs about, it really depends on which part of the
*  region we hire the voice actor, but let's say somewhere around $10 to $20 per minute.
*  But the problem is that with that approach, your video still doesn't have the engagement
*  or facial expression that match with your voice. I think what our approach, our method on
*  translation is that we actually made that engaging with a new language, new voice.
*  I wouldn't see the value purely on the reducing the cost. Yeah, we reduce the cost by more than
*  10x and making the time much, much, probably 100x faster in that case because it usually takes days
*  or weeks to really coordinate the voice actor, but now we can do it one click with AI. I think the
*  other important value prop here is that people always think that a video dubbed in a different
*  language is always like the second version, the version that's less engaging because I kind of
*  want to look at the original video, but just look at the subtitle. But now you can really enjoy the
*  native language version, pretty native way with the speaker actually with the lip sync movement
*  matching the new voices. I think there's another value of that where our version of the video
*  translated video is actually much more engaging than the traditional dubbed videos. That seems
*  like a killer to me. I don't know if you are inclined to say, but is that like a big part of
*  the business? I mean, I think that those three other feeling pillar like localize and
*  personalization, I couldn't share the specific details on that, but that's definitely one of the
*  most powerful and popular use case among our customers. Hey, we'll continue our interview in
*  a moment after a word from our sponsors. AI might be the most important new computer technology ever.
*  It's storming every industry and literally billions of dollars are being invested. So buckle up.
*  The problem is that AI needs a lot of speed and processing power. So how do you compete without
*  costs spiraling out of control? It's time to upgrade to the next generation of the cloud
*  Oracle cloud infrastructure or OCI. OCI is a single platform for your infrastructure,
*  database, application development and AI needs. OCI has four to eight times the bandwidth of
*  other clouds offers one consistent price instead of variable regional pricing. And of course,
*  nobody does data better than Oracle. So now you can train your AI models at twice the speed and
*  less than half the cost of other clouds. If you want to do more and spend less like Uber
*  eight by eight and Databricks mosaic, take a free test drive of OCI at oracle.com slash cognitive.
*  That's oracle.com slash cognitive oracle.com slash cognitive.
*  This episode of the cognitive revolution is sponsored by the brave search API.
*  You may know if brave as an alternative to Chrome, but did you know that brave has its own
*  independent search engine powered by its own 20 billion page index of the web? The brave search
*  API gives developers reliable and affordable access to programmable web search, auto suggest,
*  spell check and more with flexible plans for all types of use cases from rag search to automated
*  business intelligence. On top of that, it's up to three times cheaper than Bing, all without
*  compromising on quality, speed or reliability. Over 700 businesses, including Coheer, Chegg and
*  Kagi rely on the brave search API. And a recent survey showed that 94% of customers would recommend
*  it to their peers to start building apps with the power of the web. Sign up at brave.com slash API
*  and get up to 5,000 free monthly calls. Cool. On the create side, I wonder how you think about,
*  I mean, obviously videos is a vast world, right? With the infinite range of things that can be
*  created. I wonder like what sort of taxonomy you have for that in terms of like, do you organize
*  it by the kinds of content that are being created or the sorts of people that are doing creation?
*  How much do you see being created that like would have been created previously, but with more
*  time and money required versus how much stuff is stuff that just couldn't have been created or
*  wouldn't have been created before. And do you guys mostly create inputs for people to take to other
*  projects or do people actually like leave the HN product with a completed video? Like I could imagine
*  somebody might generate something with an avatar and then go use that in Adobe suite as part of a
*  bigger process. But I know you also do have the studio portion of the product. So yeah, just
*  tell me everything about who's creating what. Yeah. So who's creating, what are they creating?
*  And maybe last one is like, how are they using it after they created on HN? So HN is a very
*  horizontal platform. There are many, many different use cases ranging from marketing, sales, learning,
*  development, and like we mentioned, localization. And I think another aspect is also that there's a
*  variety different of segment of customers on HN, like ranging from some consumer customer, small
*  business, midsize, and some of the largest global Fortune 500 company as well. We see us as creative
*  tools and where we want to build a tool for everybody that they want to create videos to
*  come page and to create videos. And I would say in terms of a content that actually being created,
*  I think it asks a good point. Is it more about unlocking their existing workflow? May that faster,
*  may that cheaper? Or is more about unlocking the audience by making video that they couldn't be,
*  you know, doing that before, right? I think we talk about a little bit on the video translation
*  example. I don't think translating a video, it is a term people thought about that previously.
*  We, you know, we did open up that new market and making it possible for people to localize their
*  video. And there's a very similar thing applied on the outer video side. The way how I put it is like
*  this, you know, assuming a business needs to make, let's say, 100 videos a month, right? This, for
*  example, and maybe 10 of them, they could actually come to HN to create with some sort of like
*  spokesperson, the human presenter videos. And we made that 10 videos, even if there's only 10
*  percent of the total video they're making, but we made that 10 videos much, much cheaper and faster.
*  And as an outcome of that, instead of not making 10 videos, instead, because a better tool out there,
*  they're making 100 videos out of that original 10 video demand. And that's one way to look at it.
*  I think another way is that I give you one example would be, let's say, a small business customer.
*  They usually don't have a lot of budget on working with a video agency to create videos, but they
*  really want to leverage video as a format to do go to market for external marketing. And now,
*  HN is a perfect tool for them to really unlock that. And in that case, we can imagine that we
*  are really unlocking the video they couldn't be creating before. So I guess it comes back to the
*  point, HN is a creative tool. We help to shrink down the creation costs out there, and it just
*  have a value and effect on both sides. Yeah. The last piece is about how people are using it.
*  Either they create an outcome from HN, or they put it onto other video editors.
*  The way how I look at it is that, so let's say we largely categorize the customer, the audience,
*  into two groups. One group is that I call it professional video creators, where they have
*  access to Adobe Final Cut Pro. And the other categories that people actually don't know how
*  to make a video before is they don't know how to use those sophisticated softwares.
*  You know, the first piece of the customer, I call that, let's say, 1% of the total business users.
*  And they will probably use HN as a generator footage. And the value here is actually,
*  HN help them to replace the camera, the process, where they used to have to use a professional
*  camera crew to film it with the footage. Now they don't need to do anymore. They can use it on HN
*  and plot the footage into Adobe or whatever editing tool they're using. That's for that
*  1% professional video production players. But actually, for the rest of 99% users,
*  they couldn't actually make a video before. In this case, they would just finish everything
*  in not only the camera part, but also the editing piece on HN producing content directly that way.
*  Yeah. So do you see people creating stuff? I mean, obviously the, and I'm thinking actually
*  for the introduction to this episode, I might go clone myself and have my avatar read the intro
*  essay instead of doing it myself. The company is most well known, certainly for the avatars,
*  and when you go into the product, which I've done, I always get deep on the products before
*  the interviews, I noticed that there's a handful of pre-done avatars that I can choose from,
*  or I can clone my own. How often do you feel, I mean, that seems to be the thing that people
*  are initially hearing about that they come to the product for. I guess how often are they using
*  an off the shelf avatar, a cloned avatar, or no avatar? I imagine in some cases, people might be
*  making videos that doesn't even feature an avatar, but there's so much emphasis on the deep fake
*  technology of the clone, but I wonder if that is actually what is driving most of the value,
*  or if it's just convenience of video generally that's driving the value.
*  Yeah, yeah, first of all, I would say the voice only use cases where they don't have an
*  exposed person generated, that is not the major use case of HeyGen, even though probably you can
*  still get around that work to generate video like that. The way we really look at it, whether
*  there's a stock avatar or your own avatar, is really about how we design the user journey,
*  and I think the stock avatar, the major benefit out there is to help the user to get the value
*  of this technology and can quickly experience what's possible with that. I think the user
*  journey is always try to help customer create their own digital twin so that they can use that
*  on their own businesses. So it's not about the end use case, it's actually about the user journey.
*  So I take that to be most of the people are actually doing the clone, is that right?
*  Most of, I mean, I would put it this way, usually if you look at a customer journey,
*  they would usually come to HeyGen, test it out with a stock avatar and try to see the value,
*  or even test out how effective their content in terms of the performance. And then as a next step,
*  people will come to HeyGen to create their own avatar and continue to strength that
*  brand specific identity around it. And that's kind of the user journey to drive that.
*  Gotcha. Okay, cool. Staying on the avatars for a second,
*  where do you see these working well versus not working well? And how do you think that's
*  going to change over the short term? I heard on the NoPriors podcast, you went into some detail on
*  like, obviously, when you're making marketing videos and advertising videos in particular,
*  the ROI on that is like super determined by how good the content is, how engaging it is,
*  whether people swipe immediately or not, right? Versus something where if it's like a required
*  corporate education experience, you have to finish it kind of regardless of how engaging the
*  presenter might be. So I wonder how you see like, where are we right now on the spectrum?
*  I'm definitely someone who expects that wherever we are right now, we probably will get to things
*  that are outside the uncanny valley. But like today, are we still? How do you think about the
*  uncanny valley? Are we in it? Are we leaving it? Does it depend on the use case like education
*  versus, you know, top of funnel advertising? And where is the technology like good enough to win
*  versus maybe not quite there yet? And again, like, how's that going to evolve?
*  Yeah, so I think you mentioned something about the external use case and internal use cases,
*  the quality of body are different. And especially for external marketing, advertising, the quality
*  there would be much, much higher. And because, you know, brand or business has kind of put a big
*  budget on that to fly the ads. I would look at it this way. So assuming we try to product quality,
*  you know, a spectrum like this, right. And I think certainly we have surpassed in a lot of
*  aspects in terms of uncanny valley already. Like, for example, if you look at some of my avatar,
*  you probably couldn't tell whether that is avatar or a real human presentation out there. But I think
*  the tricky part is that let's come back to a question. Why would people love watching videos
*  instead of actually reading texts or just like listening to a voice clip? Right. Because I think
*  one important piece of a video is that it's just more engaging for the audience. And I think the
*  question comes from not like what makes a video look real is actually how to make a video engaging.
*  And that piece is still something I think needs a lot of development and improvement out there.
*  I'll give you one example is that Nathan, you're probably really good at performing it from the
*  camera. You do podcasts, like interview, pretty much every week. Right. And what I mean, in terms
*  of like replacing that for you, you know, having a realistic generation for you is not enough. I
*  think there's something else out there. I was talking to the other person about some feedback
*  on the product, and she mentioned something about conversation flow. Right. Okay, what does
*  conversation flow mean in the video generation problem? And that's actually very subtle, but
*  it's very, very important to make a video engaging. And the other one is that the conversation flow
*  usually comes with a natural movement of a gesture that I'm just using my hand to help me to articulate
*  some problem. And I think that's another challenging piece of that. And then the last piece is that
*  like generally more emotion expression. And because you can see the symbol in different tone and
*  different emotion, and actually it represents different meaning behind it. I think those areas
*  still is an area where actively work on to improve. And that is actually the breaking point to where
*  we can surface more and serve more high end, high quality, external advertising or marketing use cases.
*  I think your comment about the target not necessarily being full realism, but just being
*  high engagement content is a pretty profound one. It's all happened so fast over the last couple of
*  years, but I remember maybe, I don't know if it was a year ago or 18 months ago or something,
*  but I remember saying as we were looking at some of these early,
*  image generation was already getting pretty good at this point and video was definitely not as far
*  along as it is today. And I recall having this hypothesis that maybe we'll adjust as well.
*  Of course, people are going to try to make the video generators more
*  lifelike in some respects, but also there's a super stimulus feeling to some of the early stuff
*  where it has this almost psychedelic sort of wavy form that seems like in a way,
*  it also kind of recalls some of the interpretability results where you look at what sort of visual
*  input makes a certain neuron in a vision model light up. And it's like, wow, it's sort of a trippy
*  thing that maximizes the response of this one particular neuron. And I feel like people are
*  kind of racing toward realism and at times forgetting that in fact, cartoons are really
*  popular and there's lots of other form factors besides purely lifelike video that maybe can win.
*  And this is maybe a good question for you, Victor, too, because I know you have a background in video
*  games. Where do you guys think the future is of content? Is it about realism or is that kind of
*  a lazy person's way to think about what really matters? I personally think that realism is not
*  the most important thing. And to Joshua's point, it's all about the engagement. And I think
*  given the choice of consuming content in different formats, I think it's pretty clear that humans
*  choose video. And one way of having it be more engaging is realism. But in entertainment,
*  there's a lot more than just realism. So I think one of the things that first got me
*  super interested in Hey, Janice, I started in a mobile gaming company for a long time.
*  And in mobile gaming, user acquisition was always a big part of the business. And in the beginning,
*  people would show static ads and eventually it evolved to video ads. Video ads is a huge part
*  of the mobile gaming industry. So for example, from my company, we had like 100 million video
*  views per month. So just getting the video to convert at a higher rate is just so fundamental
*  to the business. That's one of the largest KPIs. And I think that translates to other businesses
*  as well. So companies, they want to communicate with stakeholders, both internally, but more and
*  more as the quality of videos get better, more and more external stakeholders. Like, hey, how do you
*  make these videos more engaging? And I think in my experience, what really worked was not necessarily
*  like, oh, like, how do we make this higher fidelity? But it's more like, how do we test
*  a bunch of different things? And the key to testing is how do you get the price to be lower?
*  And how do you get the iteration speed to be faster? So I think that's a very powerful thing
*  that HeyGen does for businesses. Like you're able to create videos faster at a lower cost,
*  which means you can try a lot more stuff. Do you see an application in gaming? Because I'm also
*  kind of thinking about the personalization side. So I think what got me excited with HeyGen is that
*  what we're seeing now, I think it's like we're scratching the surface. And I think the thing
*  that impressed me about the company is like just the product velocity. And so when you look at some
*  of the new products, for example, like streaming avatar, which is your avatar, so it's a digital
*  clone of a person that talks to you in real time. Right? So instead of like, hey, I want to create
*  a video that someone's going to watch asynchronously. It's like, no, it's like someone's going to talk in
*  real time with a bot and like HeyGen powers that technology. And like no one has that technology.
*  And I think it's just one of the most exciting things about the company, because this new wave
*  of AI started with like chat GPT capturing people's attention. And I think what captures
*  people's imagination is like, okay, people are now talking with a bot. But the first situation
*  of chat GPT is like you're talking with a bot, or you're typing, which is suboptimal. And with the
*  GPT 4.0 announcement, what was really cool is like, hey, now, like talking with voice, it's kind of
*  cool. And the next step of that is, it's not like a voice call, but it's a video call.
*  And streaming avatar, like we have the best product in the world to do that. And
*  tying back to your question around entertainment, I think there's just so much more pleasurable for
*  people to engage in this way. So we don't yet know. And like, I'm happy that we're a platform that
*  provides technology, but we don't yet know what companies will do that technology.
*  But if I had to guess, there will be like huge applications in entertainment. It's like, hey,
*  I want to talk to virtual friends. I want to talk to a pet. You know, like we have this
*  avatars that some of them are cartoons, like some of them, like have these different features. So
*  I guess it's a long way of saying that we're just getting started.
*  Hey, we'll continue our interview in a moment after a word from our sponsors.
*  Omniki uses generative AI to enable you to launch hundreds of thousands of ad iterations that
*  actually work customized across all platforms with a click of a button. I believe in Omniki
*  so much that I invested in it. And I recommend you use it to use cog rev to get a 10% discount.
*  Hey all Eric Torenberg here. I'm hearing more and more that founders want to get profitable
*  and do more with less, especially with engineering. Listen, I love your 30 year old ex-fang senior
*  software engineer as much as the next guy, but honestly, I can't afford them anymore. Founders
*  everywhere are trying to turn to global talent, but boy is it a hassle to do at scale from sourcing
*  to interviewing to on the ground operations and management. That's why I teamed up with Sean
*  Lanahan, who's been building engineering teams in Vietnam at a very high level for over five years
*  to help you access global engineering without the headache. SQUAD, Sean's new company, takes care of
*  sourcing, legal compliance and local HR for global talent so you don't have to. With teams across
*  Asia and South America, we can cover you no matter which time zone you operate in. Their engineers
*  follow your process and use your tools. They work with React, Next.js or your favorite front end
*  frameworks. And on the backend, they're experts at Node, Python, Java and anything under the sun.
*  Full disclosure, it's going to cost more than the random person you found on Upwork that's doing two
*  hours of work per week but billing you for 40. But you'll get premium quality at a fraction of
*  the typical cost. Our engineers are vetted top 1% talent and actually working hard for you every day.
*  Increase your velocity without amping up burn. Head to choose squad.com and mention Turpentine
*  to skip the wait list. Yeah, I think that those even the non-human avatars are really an interesting
*  concept. When I think about the mass personalization along the lines of email, there I'm like, oh man,
*  I don't know. I guess a couple things. One is like, if I ever get to the point where I'm getting an
*  AI generated video from my boss, I'm going to feel maybe a little bummed out about it. Just like,
*  wait a second, you didn't have time to actually talk to me or something feels weird about that.
*  I also find in what little exposure I've had to these large agency, large brand
*  personalization projects in the past that they love the idea. But then they're also,
*  and I wonder if you've found this too, but they've typically been very shy about actually
*  meaningful personalization. They love to get that name in there. But then I've seen solutions
*  in the past where it's like, we'll do the top hundred most frequent names. Then after that,
*  we'll just say, hi friend or whatever. And they're like, yeah, that's great. We love that.
*  But then are we actually going to do any other customization deeper? Well, we'll maybe put how
*  much you spent on your utility bill this month or something. But it's very, very local, very
*  programmatic. And it's almost, yeah, do you really even need AI for some of these things?
*  But then I imagine this other world where it's like, you're on an adventure in a dynamic world.
*  Maybe the whole world is getting generated for you. And now it's a lot easier for me to see how
*  that real time and the personalization to whatever adventure you're on, experience you're having
*  makes a lot more sense. Do you think I'm analyzing this in the right way? Or do you
*  see big brands as actually being more adventurous than I give them credit for?
*  Yeah, I mean, I think one way I would look at it is that first of all, I think by looking at
*  personalization combined with the generation only at the name variable level, it is still pretty
*  narrow view in my opinion. I think the bigger implication behind that is actually, so first of
*  all, video can be generated on the fly. Second, video can be highly personalized. People started
*  with name because it's much easier to get started. And that's an immediate use case for that.
*  But which also means that you can personalize the content itself. What is the messaging hitting
*  that customer? What is the story behind that video? And I think we have been always get used to
*  living in a role in the past 20 years when you watch a video, you and I have to be watching at
*  the same one, but it doesn't need to be like that. So we could be totally watching two different
*  angles to talk about the same story, the same thing, the same topic behind it. And the video
*  could be presented in different ways. But one example I want to use like the example Vitor brought
*  up is that, hey, let's say you have a video getting a hundred million views every month,
*  you want to optimize that. I think the conversation will always come into, okay,
*  I think most people would probably like this. So let's change the video that way. And then you
*  keep iterating. And then every time you are optimizing that, you sacrifice some sort of
*  customers, right? You're basically optimizing for the rest of the broader audience to reach the
*  global message mode. But still there's only one video out there. But imagine a role is we bring
*  all this down. We can actually serve every single customer landing on this page, a different version
*  of that. And we have an AI behind it to support it that way. I think we can achieve the global
*  measurement in terms of ROI and optimization there. And I think when we look at it, how is that
*  possible? The highly personalization is one, technology behind it. And second, I think what's
*  really important is also about, can you actually generate the video on the fly? And when I give you
*  variable and then you generate the story behind it. Like one example would be the streaming avatar.
*  But streaming avatar is really the starting point of that. But imagine a role that we can actually
*  stream the entire video, not only streaming the watching screens, but also streaming the
*  generation experience. I think that is something we will look at it and feel very excited about
*  the future about generating the video. Yeah, and just adding on this, I think
*  the technology to produce this personalized videos is so new. And some of the applications
*  that people make, we have to test a bunch of things. And in the beginning, not everything
*  resonates. So for example, you can receive a video from your boss that is just your name being
*  personalized. And that doesn't make you feel good. But at the same time, a video that is personalized,
*  not to just like, oh, I'm going to make you feel like this was made exclusive for you,
*  and that's the only value. No, imagine it's a video where we intake all the information about
*  the stuff that you care about. And we personalized not to make you feel special, but we personalized
*  because we actually going to deliver the contents that you care most about. And I think as we find
*  more and more of those applications, then people start feeling like this technology is really in
*  their favor. And I think it's a process. It's a process iteration until people find the right use
*  cases. So I guess that's a good transition to the future of the technology, future of the product.
*  If I recall correctly from the no priors episode, I believe you said 40 employees at the company
*  today. Yeah, we have about 40 and 50 somewhere around that. So I'm just doing a little back of
*  the envelope math. If you're doing 35 million in annual revenue, that would seem to be plenty to
*  cover the employee base. And so one would then assume that a fresh slug of capital would be going
*  into a lot of compute, maybe like licensing of underlying content as well. Like, what's the kind
*  of, you know, use of proceeds here? And what is sort of the next big act for agent? So when we
*  initially found the company, I think it touched a little bit about when you look at agent as a
*  customer, as a user today, you saw it mainly as a feature in the avatar videos. That is the main
*  power of the agent platform today. But we actually never see us as an avatar company. What we really,
*  really want is solving the problem of generating a video for businesses. And if you look at,
*  we did the way how we solve that is sequencing in the way that we want to solve the A-Roll program
*  first. What I meant by A-Roll is mainly the human spokesperson, the avatar, the actor piece.
*  I think there's a big piece is still not solved in the industry. It's actually the B-Roll,
*  right? All this background music, transition animation, stuff like that. You know, I think
*  the next act for agents, 100%, we want to continually improve the quality, improve the
*  engagement on the A-Roll piece. But also, I think putting a lot of investment on B-Roll
*  generation is another thing that will be critical for us to achieve our mission to generate the
*  entire video end to end. I think that involves mostly probably model training, investing in the
*  pattern, product development, stuff like that. So maybe taking those two things in turn, in terms
*  of improving the quality, is that something that you have to scale up a base model for? Is there
*  a scaling law for avatar video generation? Or is it more about a reinforcement learning approach
*  where it's not so compute intensive, but more about actual user feedback? What is the key input to
*  improving quality on the current margin? I would say it's a combination of both. It really depends
*  on what other problem we are trying to solve. For some of the quality problem, the more data
*  you are able to collect, the more edge cases you're able to cover, and your model that naturally
*  get better performance. But I think I wouldn't bring up another point is that solving a video
*  generation problem is not only about solving a mathematical problem. It's very different,
*  right? And how would you modularize the conversation flow problem in the actual AI
*  training? That is unsolved yet. Being able to identify those areas and engineering in a way that
*  having AI model can actually help to solve that and figure out a mathematical way and also apply
*  that into the product. I think it's another big topic about the future development as well.
*  Generally, I would say it's a combination of more data, more compute, more model breakthrough,
*  and also more product innovation. On the B-roll side, right now you have the studio type experience
*  where you can go in and edit stuff and move it around a timeline a little bit. Is the more
*  advanced version of that starts to encroach on Adobe territory? The more large-scale foundation
*  model maybe starts to go more in a Sora type of direction. Maybe it's both there too, but I'm
*  kind of wondering how do you plan to tackle everything else after the avatars? I think you
*  mentioned two things about one is the traditional timeline editor and the other one is more like
*  end-to-end test to video generation. I think the approach we are believing is different from this
*  two. First of all, I have a very aggressive view and a strong point of view that timeline editor
*  will be gone in five years. But today, the entire video role was defined by a timeline editor,
*  as I could imagine probably 20 or 30 years ago. But the whole part of a timeline editor works
*  because cameras are expensive. And because cameras are expensive, you need to film it 20 times or 30
*  times and then you put it on the timeline and say that's the best one. But when the foundation or
*  the starting point is changed where you can generate the footage on the fly, the editor
*  experience will be massively different. Do we need a timeline anymore? I don't think so. And I think
*  in order to enable the, like I said, the rest of the 99% people be able to create content. Timeline
*  editor is actually a very big learning curve for everybody to learn about. And there must be a new
*  paradigm in terms of editor exist that can help to embrace this new technology generation paradigm
*  that's one. And the other thing is that Heygen focuses a lot on the business videos. And when
*  we look at business videos, what do they need? They need like quality control and consistency.
*  We found that purely you can either solve it with the end-to-end test to video model way. Or I think
*  what the approach we believe in is that we kind of build an opposition engine to really understand
*  what the brand needs. And then in this case, you see the avatar generation, able generation is one
*  component behind it. And then voice music, voiceover, B-roll and stuff like that. And the approach
*  we believe in is that building this opposition engine and combined with the editor to enable
*  the experience down the line. So do you expect somebody like me to sit in front of a screen and
*  say, that's good, but replace that background with green park background instead of the fall
*  colors background and just wait a few seconds and then watch a new version and just kind of
*  talk to the computer to iterate or like what does that actually look like from a creator
*  experience standpoint? Yeah, I mean, like forward looking a little bit, let's say Nathan, the AI got
*  to know some of you, your past videos that you have, you can naturally just learn about who you are.
*  And then how to regenerate your talking videos like that. I don't think it has to be that you
*  talk to the computer to switch. I mean, it could be anything. It could be a UI talk or any stuff
*  like that. But being able to put a person in a different scene, it is very possible today.
*  Just the fact that does it actually match the quality that we want in a professional setting,
*  professional quality. And the problem is that actually the lighting. So not about the background
*  removal and background replacement. And how do you, let's say I put my view right now and put it
*  into a beach, you know, and the lighting actually feels weird in that way. But any approach you can
*  solve in order to make that happen. And then the next question would be you solve the static version
*  of the lighting problem. Then how about the dynamic lighting? You know, you're moving your gesture. How
*  do you make sure the whole thing is like, it feels natural and realistic and engaging? I don't think
*  we are very far from being able to generate a real footage in any of the settings that you mentioned.
*  I think that's pretty possible. So if I am understanding correctly, it sounds like the vision
*  that you have is for, you know, leaving flexibility at the level of the user experience, a kind of
*  Swiss army knife of a lot of different AI tools under the hood that the user maybe doesn't even
*  know about in detail in terms of like what, what model is being used or whatever. But it sounds like
*  maybe a couple of core, like generate the footage type models. And then a lot of like auxiliary
*  models that do specific things like change the lighting or do this, do that. And those get
*  orchestrated through some like user experience that presumably is like high level qualitative.
*  You get to say what you want. Maybe even it learns from your past stuff, but under the hood, it's
*  doing like an intricate manipulation with a lot of different tools. And that would be, as you said,
*  like that's in contrast to something like a Sora, like you're not trying to create a, you know,
*  general world modeler or like intuitive physics engine, but instead, like just all these kinds
*  of things that do their job really well. And your kind of macro job is to make them work together.
*  Yeah, I want to add one thing to that though. When we talk about the different piece of the system,
*  the oxygen engine has different model behind it to empower each of the feature or function
*  as a tool, let's say. Do you believe there was a role where a single model can solve a lot of
*  this like different tooling problems? I'm not entirely sure what would that look like, but
*  with technology evolving this fast, I think we have a way to read it now down to like a couple of
*  major models behind the scene to enable that. And then another angle, I would say, we talk about
*  lightning just now, but when we really look at it, the lightning problem was defined in the camera
*  room, right? Because we use camera. Okay, we need to pay attention to lightning. That could be a
*  problem. And then if there's a problem, we could actually fix that in another downstream video
*  editor. But when we really look at the new paradigm, lightning may not be a legitimate dimension we
*  need to talk about. It could be the dimension that, hey, make it more engaging. It could be
*  another dimension about make it more dynamic, right? And make the conversation float more
*  fluently to express the excitement about the speaker. And I think those dimension would be new
*  and could be possible with a new paradigm of this no camera role where you can actually generate
*  the entire thing. Yeah. Yeah, cool. That's interesting. I think the dramatic simplification
*  of the underlying architecture is something I've personally experienced with Waymark. I don't know
*  if you've seen what we do, but it's another kind of text to video generation experience, except
*  we are basically making TV commercials most of the time. We partner with a lot of cable TV
*  companies, broadcast TV companies. And so the constraints are like super, super specific. Like
*  it has to be exactly 30 seconds, yada, yada, yada. And I started the company actually, but now I'm
*  the AI R&D person. And the simplification that we've had over the last year from an earlier form
*  where it was like, even just to figure out what images to choose out of a user's image library
*  used to be like an incredibly gnarly thing where we'd be like captioning, but the captions were
*  super generic and use like an aesthetic evaluation model to try to figure out which ones look good.
*  And now that's largely just been reduced to like ask one of the latest large multimodal models,
*  which images are good to use. And we get better results actually from that. So it's a remarkable
*  trend that these like Swiss army knives are like seemingly evolving toward fewer features, but they
*  still work a lot better. I want to move on in the last few minutes. I know we're going to be out of
*  time before too long. Certainly anything else you want to talk about in terms of technology, future
*  direction is welcome, but I also wanted to get into the trust and safety stack because that was
*  actually the thing that most impressed me about HeyGen and it really stands out in the broader
*  market. So I'll just tell you what my experience has been. So I was a GPT-4 red teamer. That was
*  kind of how I initially got into the head space of like testing AI products for risk of misuse and
*  abuse. And more recently, I've been noticing that a lot of products are being stood up, especially
*  like agent products, but also these sort of video generation and voice generation, especially with
*  cloning type products. A lot of them are being set up with no guardrails at all. Like you can go into
*  a lot of these products. I've cloned Donald Trump and Joe Biden and Taylor Swift's voice on like
*  many different platforms at this point, including some that allow you to make a call to an arbitrary
*  number and just say like, you know, solicit a donation from this person or whatever, right?
*  You get literally no constraints at all. It's kind of mind blowing that people would set something
*  up like this and put it out there and have no guardrail. In contrast to that, HeyGen has,
*  I would say honestly, the best integrated security measures that I've seen. And I would love to
*  just get your kind of take on how you guys have thought about that, how you've prioritized it.
*  I think a lot of businesses feel like that'll slow us down or it will annoy users or, you know,
*  for whatever reason, they're not doing it. But you guys have demonstrated that you can both build a
*  business pretty quickly and to a significant scale while prioritizing that in a meaningful way. So I
*  appreciate that. And I want to hear more about it. Yeah. And maybe before Joshua comes in,
*  just to reinforce that, like I was also very interested in this video creation space with
*  avatars and there's like a few teams going at it, right? And in my head is like, hey, like this
*  technology for sure will exist and like customers want it and customers will use it. And like,
*  okay, the technology is so powerful. There's a huge risk of not using it well. And the thing that
*  stood out for me for HeyGen is like, this was like central to their strategy. It's like, hey,
*  we're going to be the most trusted brand. So like, Joshua is going to come in and talk about the
*  details. But one of the things that excited me most about the company is understanding how important
*  trust and safety is. So I was starting with this. We never see trust and safety as a factor to slow
*  us down. And I always see trust and safety is a critical piece of the business is actually part of
*  a product. Every time we roll out a new feature, we have a new experience when to share with the
*  customers. There's always a section of trust and safety. How do we actually safely roll out to our
*  customers? And I would say trust and safety is critical to our business because we are serving
*  some of the largest company in the world. And I can share a little bit how it is being done today
*  and how we really think about it. So first of all, about creation, we don't allow any political
*  or celebrity figure to be created on HeyGen. And every single avatar or digital twin created on HeyGen
*  has to obtain a first-party consent in a video format. We also do a random generated dynamical
*  verification as well as a human reveal operation in the back to make sure the system is not being
*  misused. Another thing we do is that for every single video you are creating or localizing
*  on HeyGen today, we have a combination of an AI model and human reviewer together to really make
*  sure the content you're putting out there is compiled with our policy in terms of the product.
*  For example, no hate speech, no harassment, no fraudulent, no misinformation and stuff like that.
*  And those are some of the barriers and product design decisions we put together at HeyGen today.
*  It is really well done. I went through it with the goal of actually trying to break it and never did
*  end up breaking it in the most flagrant ways anyway. I was able to get a little bit of offensive
*  speech out of it here and there. But in terms of the big things where I upload the Trump video
*  or the Biden video and try to get the avatar from a public figure, I was not able to do that.
*  And I did encounter several different, I would say, I don't know if you would agree with this
*  characterization, but it felt to me like a defense in depth strategy where there were multiple
*  different checks happening. And I think I systematically found quite a few of them,
*  ultimately landing on the final one being a human review that's just like,
*  yeah, sorry, we're not going to allow this. When you talk about celebrity faces, this is one
*  interesting question I had. And I wonder if there's something here that others could learn from.
*  Is there any special tech? Did you have to invent the technology? How are you doing that? Who's a
*  celebrity? How many celebrities are there in the world? How many celebrities are not allowed on
*  the platform? And is there anything that you're tapping into that others should be aware of that
*  they might also be able to tap into? Yeah, so I'll put it this way. So first of all,
*  particularly for, you know, celebrity, I think there were legit use cases for that for sure.
*  And I guess we just don't want to expose the free from creation process in that case.
*  And they do have to contact us and then we can involve, you know, with our operation team to
*  help them to create an avatar or digital team that way. So this is a two different approach,
*  you know, but being a little bit more heavier in terms of process that way.
*  The way how we do it is that obviously we work with some of the vendors who had, you know,
*  a celebrity or just like generally the API that provides a database. We also can't constantly
*  having our own team to update database to ensure that we're having a well coverage.
*  Interesting. Okay. So there is a company you're paying another company to provide that service.
*  Yeah. Okay, that's interesting. I've been looking for one and I have not really found one. So I'm
*  going to have to go do a little more digging on this. The reason I want to promote this is that
*  I see so much product surface area out there, you know, not agent, but the most of the rest of the
*  space honestly is so wide open to abuse that I've been like reluctant to even disclose it at times
*  because it feels like just irresponsible to put more attention on it before it's solved.
*  But then when I talk to the developers and say like, Hey, you know, here's what I did on your
*  product. Just FYI, like this looks like a problem. A lot of times they come back to me and they're
*  like, well, I don't really know what you want me to do. You know, how am I supposed to detect like
*  every celebrity voice that somebody might come through with? So yeah, I don't know. Maybe I'll
*  have to see what I can find out about that. Or I don't know if you can leave me any hints somewhere,
*  but I definitely want to popularize this. If there's a good solution for people to adopt and
*  all they have to do is like pay for it and use it, then I'm definitely keen to figure out what that
*  is and try to promote it. Cool. This is fascinating. I think the big takeaways I have from this are,
*  again, it's a common theme. This is also early. Don't look at just like an avatar today and think
*  like this is the end of the line. Not only are they going to get better, but also they're going
*  to get faster. They're going to get real time. They're going to get more responsive. They're
*  going to get more contextual. They're going to create a lot of different kinds of experiences
*  where the paradigm of like, what is a video that we have kind of intuitively developed ultimately
*  maybe gets blown wide open. And it's like, as always, the future is like a little weirder
*  and maybe cooler than we initially dare to guess. So I think that's another great reminder of that.
*  And I do appreciate the care with which you guys have layered in all of the different safety
*  mechanisms. I think that is something that truly people should come like study your product, look
*  at how it's done and take inspiration from that back to their own products. And I don't say that
*  lately. I've tested a lot of things. So to come out on the top of my personal power rankings there,
*  I would say is actually a legitimately like real accomplishment. Anything else you guys want to
*  cover before we break for today? I think we covered the most important things. Okay, cool.
*  Well, I really appreciate this, guys. Joshua Xu, founder and CEO of Hey, Jen, Victor Lazarte,
*  general partner at Benchmark. Thank you both for being part of the cognitive revolution.
*  Thank you for having us. Thank you. It is both energizing and enlightening to hear why people
*  listen and learn what they value about the show. So please don't hesitate to reach out via email
*  at tcr at turpentine.co or you can DM me on the social media platform of your choice.
