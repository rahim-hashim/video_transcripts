---
Date Generated: April 03, 2024
Transcription Model: whisper medium 20231117
Length: 5993s
Video Keywords: []
Video Views: 950
Video Rating: None
---

# Replit's VP of AI, Michele Catasta, on Artificial Developer Intelligence
**Cognitive Revolution "How AI Changes Everything":** [August 08, 2023](https://www.youtube.com/watch?v=u4l6GgFaJmQ)
*  because I would have spent more time learning interesting things about coding rather than
*  setting up my coding environment and debugging very stupid, you know, syntactic mistakes.
*  These are things that are honestly not intellectually stimulating about coding.
*  And, you know, we're trying to get rid of them as soon as possible so that you can really focus on
*  the creative process of writing software. Building large software still requires a lot of human
*  effort. But allowing a lot of people to be creative will also expand dramatically the
*  pool of good ideas that we explore as, you know, as mankind. Hello and welcome to the Cognitive
*  Revolution, where we interview visionary researchers, entrepreneurs, and builders working
*  on the frontier of artificial intelligence. Each week we'll explore their revolutionary ideas,
*  and together we'll build a picture of how AI technology will transform work, life, and society
*  in the coming years. I'm Nathan LeBenz, joined by my co-host, Eric Torenberg.
*  Hello and welcome back to Replet Week on the Cognitive Revolution.
*  For context on this conversation, if you haven't already, I definitely recommend checking out our
*  most recent episode with Replet product designer, Tyler Angert. I spent 10 full minutes at the top
*  of that episode explaining why I believe that Replet could become one of the most important
*  companies in the world, why I already consider it to be one of only 15 to 20 live players in
*  the AI game globally today, and why, if I had to pick a single platform where the human AI
*  collaboration economy is most likely to take shape, I'd pick Replet. I also posted a version
*  of that essay on Twitter, and we'll put a link in the show notes. Today we're going even deeper on
*  the future of AI at Replet with special guest Michele Katosta, Replet's new VP of AI.
*  Michele recently joined the company from Google and made an immediate splash by publishing a new
*  AI manifesto that outlines Replet's plan to support the next billion software developers.
*  They plan to do this by enabling a seamless human computer symbiosis, creating an artificial
*  developer intelligence that will create AI coding agents capable of developing complex software,
*  plus a complementary economy that rewards human tool creators and hires humans for advanced
*  problem solving when needed. Michele even proposes a new meaning for Replet. In place of the
*  original read, evaluate, and print loop, which describes a traditional interactive computing
*  environment, Michele now unpacks the acronym to mean reflect, evaluate, percolate, and learn.
*  These much higher order concepts are now the core feedback loop at the heart of the company's
*  artificial development intelligence efforts. This is not just one of the biggest AI visions
*  you'll hear from any company in the world today, but one of the most well specified,
*  coherent, and given Replet's track record for execution, one of the most credible as well.
*  While it might sound like a GI that is artificial general intelligence, Michele believes that
*  Replet's focus on code, which unlike natural human language, has been designed over decades
*  to avoid ambiguity, and is also now supported by a wide range of quality assurance tools,
*  will improve reliability enough to allow Replet to unlock huge value without needing to confront
*  the most perplexing challenges related to potentially smarter than human intelligence.
*  Now, to be honest, I'm not entirely convinced that this AGI-ADI distinction will prove as meaningful
*  as Michele and team believe, and I will definitely be watching closely for new features designed to
*  protect the next billion developers. After all, I've learned from experience training executive
*  assistants in the art of AI task automation at Athena that the next billion developers will
*  inevitably be very AI dependent. Still, I am convinced that Replet is a phenomenal platform
*  for developers both seasoned and new, and definitely a company to watch, as the power of
*  their product and the influence of their platform seem almost certain to continue to grow. As always,
*  if you're finding value in the show, we would appreciate it if you'd share it with friends and
*  post a review to Apple podcasts or Spotify. And I also really appreciate your feedback,
*  including guest and topic suggestions, which you can send either by email at tcr at turpentine.co
*  or via Twitter DM, where I am at LeBenz and my DMs are open. I get a handful of very nice notes
*  every week, and they really do influence the direction of the show. So please reach out.
*  I would love to hear from you. Now, I hope you enjoy this fascinating conversation with
*  Replet's new VP of AI, Michele Katasta. Michele Katasa, welcome to the cognitive revolution.
*  Thank you, Nate. And thanks for inviting me. I'm excited to talk about Replet and AI today.
*  Yeah, me too. Regular listeners to the show will know that I'm a big fan of the Replet platform,
*  which honestly even predates the current AI push, but that has also been a pretty exciting thing
*  to see. I want to start with just a couple real big questions, and then we'll kind of come back
*  and ask some smaller, more detailed ones as well. But for starters, you guys have put forward this
*  ambition, I think one of the most ambitious that you call ADI, artificial developer intelligence.
*  So what is that? So that's how I would say midterm roadmap hopefully is not going to take us
*  decades to accomplish that. And the idea is we try to identify an obtainable goal in terms of how
*  we need to evolve our current AI Replet to make developers much more effective. So the reason why
*  we made a play on the name and made it different compared to AGI is I don't want to go on record
*  ever trying to predict the date where AGI is going to happen. I think everyone agrees largely that
*  you will happen, but we're not interested in figuring out in the next 20 years. What we care
*  about is what can we deliver in the next few years to developers. And I think LLMs have built an
*  amazing foundation to help developers be more productive. GitHub Code Pilot was a pioneer of
*  that. And we have Ghostwriter and Replet, which is helping you go to complete code. And we also have
*  a chat feature that helps you as a peer programmer while you're developing. I think this is all great
*  and we're getting a lot of amazing feedback from our users. But what we describe in the manifest
*  is how can we bring that a notch up to the next level? And the idea is we would love to really
*  empower the next billion software developers in the world. People that don't have any computer
*  science background, they're just amateurs, they land on Replet and they want to go from an idea
*  to a prototype in the shortest amount of time. We need an AI to make that happen and we need a
*  much more powerful AI compared to what we have today. LLMs generate an amazing amount of semi-coherent
*  code. Usually it's aligned with what the user wants. At times it has some mistakes, it has some
*  issues. I think that's not a blocker for people that already experience developers such as us.
*  I always know how to find my way out from a bug that no GPT or Palmer or others are giving me.
*  This is not the same for a person that's never written a LLM code. For ABI, our vision is how
*  can we make sure that through several different iterations, and I'm going to go a bit more in
*  detail what that means, eventually you reconverse towards the right solution that the user was
*  asking for. The way in which we envision that is there will always be a fundamental generative
*  component to that which could be a much more powerful LLM compared to what we have today.
*  Likely the field has been evolving at such a fast pace that I can only imagine what we're going to
*  have like say two years from now. Definitely they're going to be more powerful, more independent,
*  but Replet has a key advantage of offering us an execution environment. We've been spending years
*  building all that kind of pattern. So now you can think of a feedback loop where an AI generates
*  code, then we execute it. We also include back the feedback from the user. We include also all the
*  errors that came from the execution, and then we learn from these initial steps. We feed it back to
*  the AI model, generate the code again until we converge to something that actually runs and it
*  is aligned to what the user wants. So this is our ambitious goal as we defined it before. Hopefully
*  it's not too many years away. I have a feeling that maybe in a couple of years we can crack
*  something pretty powerful and we have been working heads down towards this goal since Agile Replet.
*  It's amazing how far you already are. I've been a user of the platform myself and I bring a certain
*  amount of coding knowledge to it, but there's much more that I don't know than that I do,
*  of course, and that's really true for everyone. So even for somebody who knows their way around,
*  the built-in autocomplete and chat tools are really useful. But I've been kind of pursuing this
*  myself. Again, listeners will know I'm advising a company called Athena, which is in the executive
*  assistant space. We're seeing that in line with your vision about the next billion developers,
*  we're kind of seeing that software is becoming much more permeable to non-software developers,
*  much more accessible. We're starting to use Replet and starting to teach what is ultimately
*  coding, or at least it's like software development. But honestly, to people that have never even coded
*  and don't know even the basics of coding, and we skip the traditional stuff these days,
*  we skip the for loops and the syntax and all that kind of stuff, and we just jump right into,
*  here's this platform. You can start to talk to it and try to get it to do what you want to do.
*  We actually do often have them go to GPT-4, which is something that's pretty interesting because
*  for the top level request, obviously GPT-4 is still the boss model and they need all the help
*  they can get in many cases. So to get that kind of first skeleton, we do often bounce out of the
*  platform and go to GPT-4. But then once you're in it, you start to iterate especially more locally,
*  a lot of the built-in tools really work well. And it's been amazing to see how quickly people
*  can go from, as you said, wandering in, not even expecting that they were going to be asked to code
*  in this job, to being able to actually manipulate applications and get somewhere. It's crazy. It's
*  only 2023 and this is starting to happen. I agree. I mean, it has been an exciting ride.
*  I would have never expected things to move this fast since we saw the first generating models
*  based on Transformers. I could see the light at the end of the thumbnail. I felt it's about to
*  happen. It's bound to happen, but who knew that the timeline would have been so compressed?
*  And now as a matter of fact, as you're saying, we see a lot of people that have never approached
*  software in their life being very productive. And for instance, at Rapid, we host monthly hackathons
*  and the vast majority of them are revolving around the AI topics. And a lot of people that come there
*  are not software engineers by trade. We got amazing projects built by PMs who spend most
*  of their time communicating with software engineers. And as a matter of fact, they're
*  extremely good at prompting models because what do they do for a living? They put in English
*  requirements and descriptions of how a software should work. So if you put them in front of GBT4,
*  they do an amazing job at describing exactly what they want. They get the output and Rapid helps them
*  to stitch pieces together, helps them to get a quick feedback loop in case there is a mistake in
*  the code. And in turn, you can also have a model helping you to debug the code. And we have a
*  debugger feature in Go's Writer that does exactly that. When you execute your repo and you hit a
*  problem, you generate an exception, then immediately we try to tell you that's how you should fix it.
*  And we have seen PMs winning hackathons just at their first experience writing code. And I think
*  this is only going to be even more pervasive in the near future. And yeah, I feel Rapid is really
*  enabling this wave more than perhaps any other company out there.
*  Rapid has really done a lot of work in terms of teaching people to code in the traditional way.
*  How close do you think you guys are now to a mode where you would say,
*  forget that and let's go this PM route and then you can kind of fill in those lower level details
*  later? My dream is that we're going to go in that direction more and more. Of course, there are
*  struggles to that. Any AI is expensive, not only to train, but especially to serve when you have a
*  lot of users. And I think right now we just went past the 24 million plus users at this point. So
*  it's not an easy platform to deal with. And whenever you're giving access to powerful models,
*  you need to think also of your economics. That being said, the trend in AI and especially in
*  the Netherlands is that of a better economies of scale, pretty much showing up every quarter.
*  So I do see a future where at least some basic AI features will be accessible to any user.
*  And that will radically change the way in which people write software today.
*  And that's a welcome change, to be fair. If I teleport myself back in time when I started to
*  write code, I was very young, it was an extremely frustrating experience. I don't regret going to
*  that. It was a lot of fun in my side, but if I had Rapid when I started, I would have logged in much
*  more. And I think that would be much more advanced today because I would have spent more time learning
*  interesting things about coding rather than setting up my coding environment and debugging
*  very stupid mistakes that I was doing because I didn't notice that it didn't cause a parents or
*  I forgot a quote here and there. These are things that are honestly not intellectually
*  stimulating about coding. And we're trying to get rid of them as soon as possible so that
*  you can really focus on the creative process of writing software. That is here to stay for a while
*  longer. Again, I'm not going to make timeline estimates of when people are going to stop to
*  write software completely. I do think that the creative process still has a place for a while
*  longer, but no one loves to deal with low level, tiny mistakes in code. And yeah, I'm happy to see
*  that the new generation is learning to write code in a completely different way compared to how it is.
*  Excellent performance, absolutely free at netsuite.com slash cognitive. That's netsuite.com slash cognitive to get your own KPI checklist.
*  netsuite.com slash cognitive. Omniki uses generative AI to enable you to launch hundreds of thousands of ad iterations that actually work,
*  customized across all platforms with a click of a button. I believe in Omniki so much that I invested in it,
*  and I recommend you use it too. Use Cogrev to get a 10% discount.
*  It really is something mind blowing to see. And by the way, it's the same even for experts,
*  because I've seen like, you know, there are some notable tweets, I think Andrey Karpati is saying,
*  I can't imagine myself writing code without GitHub Copilot. I would easily waste like 50% of my
*  productivity. And that's coming from a person that's been writing software for like as many years as
*  I've been doing. It's a very prolific, you know, researcher, they will not, you know, get rid of the new AI features in writing code.
*  And I think soon this would be agreement across every person who writes software.
*  Yeah, totally. So you came just for a little personal context on you, recently from Google to
*  Replet. And Google and Replet also have a significant partnership, which I think is multifaceted in
*  terms of like investment and cloud and now even some product connectivity. Were you involved in that,
*  in the creation of that partnership? Or were you just kind of, you know, separately attracted to Replet?
*  I mean, I was involved to an extent. I was there more like as a researcher, as a middle management.
*  So I'm definitely not the person who caused the shots about making such a, you know, big partnership happen.
*  I was an internal champion for that. I was a big fan of Replet, you know, even before I started to work there.
*  And I was giving demos of Palm to Amjad and his team back in the days, because I was one of the
*  contributors to the code skills of Palm, which we called Palm Coder back in the days. So I think
*  that that was my first interaction with the Replet team more officially. And then I, you know, I stayed
*  in touch with them. I unofficially advised them for a while. And then I became an advisor and then
*  hold the way to join them full time. So he was a slow cook. I have a lot to believe that I played a
*  role to make that happen. And, you know, after I left, I'm glad that the partnership actually
*  happened. And it's a great partner to have by our side. Hyrap is a very resource intensive company
*  to run because we give compute to millions of users. So it's something that we couldn't do on our own,
*  for sure. Yeah, that's interesting. That might be worth getting a little bit more into the weeds of
*  before getting there. You know, it's in some sense, you've just come from maybe the most
*  formidable AI company in the world, certainly with, you know, probably more top notch researchers than
*  anyone else, probably more publications than anyone else, certainly, you know, more resources,
*  more compute. And you've gone to, I think, probably the smallest company that I have on my live players
*  list at basically a hundred people at Replet, right? So we're getting close to that. Yes. So
*  it's that small. Close to, but still under a hundred. Yeah. So how do you think about that trade-off
*  and kind of what should we be watching for? I mean, typically these new technology waves are,
*  you know, startup favoring, but I've been pretty commonly of the opinion, you know, as I look at
*  different markets that I think the incumbents are going to do really well in this case. And you're
*  going from, you know, one form of strength to another. So how do you think about the trade-offs
*  in scaling resources? Yeah. So of course there is a trade-off. Some things that I'm currently doing
*  at Replet will be maybe much easier if I attempted to do them at Google, just by the sheer amount of
*  resources available in that company. At the same time, there is such a strong benefit in being
*  lean and having less process and being able to move fast and iterate faster and don't fixate
*  perhaps on mistakes because they're considered a sunk cost. At Replet, we can decide this direction
*  didn't pay off. Let's do exactly the opposite. I love it. It's a breath of fresh air. I don't think
*  I belong long-term into big tech. I'm very grateful for the experience that I had. I met a ton of
*  amazing people. As you said, the density of AI talent at Google is incredible. But then at the same
*  time, I feel at home being at Replet because we have a very narrow focus and we're very much obsessed
*  to give the best possible AI to our developers. And it doesn't mean that we necessarily have to
*  build a team exactly as the one at Google Brain has. We can be complementary. And the partnership,
*  in a sense, helps us to make that happen because we get access to models and we get access. Maybe
*  we can have discussions and we can help each other in the process. And I think there is space
*  for everyone. There is space for incumbents. And I'm glad that they're there because it would be
*  short-sighted to claim that OpenAI and Google have been changing the AI landscape thanks to the
*  APIs that they offer. But then at the same time, there is also space for startups that are capable
*  of building new generations of products that an incumbent wouldn't be making a bet on.
*  I like to use this example with office productivity. I wasn't surprised to see that
*  Microsoft and Google immediately integrated Gen.AI in their office productivity. And I wouldn't be
*  going against that kind of product as a startup if the product looked exactly the same. As in,
*  I wouldn't try to crawl in Google Docs with Gen.AI because an incumbent would do that much better.
*  Now, if you come up with an idea of a completely different way to edit slides or documents,
*  which has Gen.AI as a first-class citizen, that I would tell you that's a very good company.
*  That's something that will have its own space in the market. And we rapidly kind of play the same
*  game. We're building something that doesn't exist. We know that a lot of users want that.
*  And we're going to keep building that with the help of the incumbents as well.
*  Yeah, there's even a frame there where I would say in some ways, while it's small,
*  in terms of headcount, in some ways, Replet is an incumbent. And by that, I mean,
*  again, going back to teaching these EAs how to code without really teaching them how to code.
*  When I was introducing Replet to them, I was kind of like, it's going to be hard for you to understand
*  how much better this is than the old flow of having a local development environment and then
*  having to keep your settings in sync and your Python environments all whatever. It's just a
*  total cluster. Obviously, everybody who's experienced it kind of knows that. But if they
*  haven't experienced it, it was like to you, it's just going to feel like this is how it always
*  naturally would have been. And you'll never know the difference. So I think that in many ways,
*  the strength of the platform itself and just all the provisioning and all of the infrastructure
*  and all the seamlessness that has obviously been created over time puts Replet in kind of this
*  unique position of being both small and in a meaningful way, having an existing technology
*  mode that's super complimentary to now an AI layer put on top of it. So I guess I don't mean
*  to pitch the business too hard, but as you can tell, I'm definitely a fan of the product.
*  I appreciate that. I'm the same, otherwise I wouldn't be where I am today. But I agree with
*  you. That's one reason that drove my choice, because I think that we have such a strong
*  advantage in what has been before I even joined the company. I think it's an amazing playground
*  where to apply AI and where to evolve AI. That being said, I think as a company, we always love
*  to keep our low profile in the sense and not become complacent. So as much as I would love
*  to believe, oh, we built something that is unique and is going to help us grow over time. I love the
*  fact that we still all feel the urgency of becoming better and growing and making our users happy.
*  And it still feels as a small startup to an extent. I've been a co-founder in teams,
*  much smaller teams, and I feel the same urgency right now in a company that by some silicone
*  standard you could consider it as a company successful, but no one inside feels, oh, we made
*  it. We all talked about, oh, we need another three years to do this and five years to do that.
*  And I love this long-term goals that we have in mind to grow and become better.
*  So that maybe speaks to the company's identity. And I wanted to ask a little bit about this,
*  because people refer to Repl.in in ways, like in my case, on my list of live players,
*  it's also appeared frequently on lists of most EAC companies. So do you guys identify as an EAC
*  company? What does that mean to you? Do you want to wear that label?
*  I don't know much of these things, to be fair. I'm more of a spectator on Twitter rather than
*  an IT nurse. Of course, we're a tech company. We are focused on AI. We love progress. And
*  we are providing a novel way of writing code to users. So by definition, we're a company driven
*  by a wheel of contributing to the technological progress. And we strongly believe that giving
*  development capabilities to the next billion of people is going to make the world better.
*  Now, I don't think any of us likes labels, both as individuals and as a company. As a matter of
*  fact, we have a webpage for the new hires where we say we are apolitical. We don't take a stance.
*  And I think we love exceptional talent. We love to work with people that are respectful. And at
*  the same time, we respect any point of view. So I'm sure that Amjad is interacting with some EAC
*  people on Twitter. But for those of you who follow him, I think he's a... Myself, I've been
*  knowing him for more than a year. And now I wouldn't be able to give him a label. It's such
*  a unique person in his own beliefs and in his own rational way of seeing the world that I just found
*  him to be an inspirational leader in the company rather than a person that belongs to a certain
*  clique of people on Twitter. I wonder how that kind of apolitical... I mean,
*  there's a lot of different takes on this, right? We've gone through waves over the last few years
*  of highly political, even activist discourse at a lot of companies, including big tech companies.
*  I'm sure you were there for some of that. And then there's been this kind of movement
*  away from that and saying, okay, we're a startup. We want to focus on one thing and that's our
*  mission. And we don't want to be political. And it sounds like for most things, that's probably
*  kind of a pretty simple, relatively straightforward policy for a company like Replet. But then when
*  it comes to AI, it's kind of tricky again, because it is both core to the mission and it's
*  like inevitably kind of becoming political. But also as people that are advancing the technology,
*  you obviously have real responsibility in the ways that you do that. And it would be easy for that to
*  bleed into the increasingly political discourse around AI. So how do you guys navigate that?
*  What is the internal AI discourse at Replet like? Yeah, I think that political and responsible are
*  two not necessarily parallel tracks. And in a sense, I'm not a big fan of the fact that politics
*  bleed into AI. Conversely, I would love us as a community to be very focused on building
*  responsible AI systems. I think that the fact that we partner, for example, with Google and
*  we work with OpenAI, we are surrounded by companies that I do believe are posting a certain amount of
*  effort in that. No company out there is perfect. I think there is no playbook on how to do responsible
*  AI today. And it doesn't mean that the researchers working on that are not delivering. It's more about,
*  it's a new field. It's going to take us years to figure out how to do it correctly. If you think
*  about social media, it has been around for 15 plus years at this point. I will argue that we
*  haven't figured out still how to regulate that correctly and how to do it responsibly. So AI is
*  much newer. AI is arguably way more powerful, especially in potential. So it's going to take
*  us as mankind a while to figure out how to do it correctly. Now, as Replet, I think we are in a
*  privileged position where, for instance, we don't generate text, we rather generate code. So some of
*  the shortcomings about having a model online that generates fake news, maybe they don't apply directly
*  to us. We are not in the real, more like image generation and all the issues regarding using stock
*  images that are copyrighted. We don't do face detection. So I would say we dodge some of the
*  hottest short-term issues about AI. I don't think we're not part of the discourse about the
*  GI because I think it's not in our radar at the moment. We have way more grounded to planet Earth
*  technical challenges to work on before we think of that. And again, I'm glad that there are other
*  institutes and other companies thinking about it. So the internal discourse that we have now about
*  the AI is mostly how do we make sure that our users are aware of what we do? And that's the
*  reason why, for example, we even release our LLM is open source and we use open source data sets.
*  So we try to be as transparent as possible when it comes to our AI features. By all means, I'm not
*  claiming that everything we're doing is perfect, but by means of being as transparent as we can,
*  we get a lot of feedback and we can correct course easily. So it's better than working in a
*  vacuum. At least, we know and we get to hear what people believe we do wrong and we try to fix that.
*  So I think responsibility is important to us. The political discourse, I don't think we even
*  have the cycles to spend time on that right now. And we have to put so much work on drawing that
*  and that would be a waste of time in the short term. You guys recently put out this state of AI
*  development report. And one of the things that had jumped out at me was the rise of Lang chain,
*  which we've talked about many times, in many different episodes. I've used it moderately,
*  but not super intensively because the biggest projects I've done actually got underway before
*  Lang chain. At the same time, it's taken a lot of heat recently online for being, I'm not sure,
*  people suddenly, for a second, it seemed like it had flipped and it's not cool anymore. And I'm not
*  entirely sure why. So I'd love to hear your comments on Lang chain in particular and that
*  tool ecosystem in general and how you would guide developers towards or perhaps away from certain
*  tools. So first of all, I wouldn't guide anyone away from Lang chain. I think it's such a
*  fundamental tool in the AI space right now that at the very least is interesting to follow. And
*  I'm grateful to Arizona and the whole team building it. I found it amazing that it's basically
*  collecting most of the research papers that come out. They also have an implementation on
*  Lang chain within 24 hours. So it's a treasure trove, regardless of how much impact it has in
*  production. Now that being said, I think Lang chain is going through the same life cycle of
*  frameworks in general. I remember back in the days, when we had the first powerful web
*  frameworks such as Django and Ruby on Rails, they were getting exactly the same heat in the sense
*  that when you force or when you recommend a developer to work in a specific way, some people
*  will love that kind of mindset and they will be compatible with that. And so people will find it
*  completely counterintuitive or they will hate the fact that debugging Lang chain is of course
*  more challenging compared to debugging your own code. So it always comes with pros and cons. And
*  that's the reason why even as of today, if you want to write a basic web application, you probably
*  have a choice of at least the top 10 frameworks out there. And each one of them is good and each
*  one of them is powerful. So Lang chain is getting beat because it's probably the first one covering
*  the space. I wouldn't be surprised to see a few more popping up in the next months. And I think
*  that the code base, no matter how intricate it could be today, first of all, it can always improve.
*  And again, we've seen this in other frameworks that maybe TensorFlow went through something
*  similar or TensorFlow 2 wasn't exactly a success story. But even in deep learning,
*  we have seen libraries coming out in version one, collating all the feedback from users. And then
*  the next big release is much better because the developers learn the best practices and the
*  other patterns. So let's give some time to Arizona and the team to maybe make a next release
*  on Lang chain. But I think this is the nature of AI development today. It's early, it's scrappy,
*  and it's going to be headache-inducing. But that's also the reason why it's fun.
*  I think it's a really great answer, great perspective. And I definitely would encourage
*  everyone to follow the project at least, as you said, they are incredible. You might even call it
*  Replet Speed. They might coin their own Lang chain speed. I've seen not once, not a few times,
*  but a lot of times, as you noted too, where they very, very quickly implement some new
*  scheme that somebody published in a research paper. And that's incredibly useful from a
*  developer standpoint. Yeah, that's making AI better as a field if you think about it, because
*  whenever a paper comes out, there is always some doubts about how easy it is to reproduce,
*  are those results real or not? And the fact that the implementation, either sometimes the authors
*  themselves build it in a Lang chain so that it's easy for people to reproduce, or someone in the
*  community is going to take care of that to make a video in a day, that allows us to find out is that
*  paper actually real or are those claims completely false? And that helps us to make progress as a
*  field. So even if it doesn't have any impact in production, it's so valuable on the research side
*  that, again, I'm glad that we have so many people working in AI today compared to even six months
*  ago. It's powerful. That also just makes sense to me as to why I wasn't bothered by any of it,
*  because I generally am fine to follow the framework. So I think I'm in the framework,
*  target market, I guess. You had said, with all these tools, and especially all the AI tools,
*  obviously, the goal here is higher productivity, greater accessibility, more developers. As I
*  understand your earlier comments on the ADI notion, it's like, we're not looking to replace
*  developers, we're just looking to give them a next generation set of tools that will make them
*  much more productive. Do you have a sense for how much more productive we're talking about here?
*  Are we talking 2x more productive? Are we talking 10x more productive?
*  I would say the 2x has been already accomplished by code completion. I think in the process of at
*  least writing code, Copilot has published some metrics where they make developers up to, I think,
*  55% faster. We have something comparable. We didn't publish the metrics yet, but we see high
*  acceptance rates from our model. So I would say the 2x threshold has been reached. Also,
*  informally, I've seen several people tweeting about the fact that they would be wasting half of
*  the time if they were not using modern code completion. I think 10x is attainable, achievable,
*  like in the next year or so. And that's by means of adding better ways of debugging code,
*  more agentic behavior. That's another direction that we can go more in depth later. But this idea
*  that you don't want to use an AI exclusively to generate blocks of code. You want an AI that comes
*  up, first of all, maybe with a plan of what you need, like in terms of architectural design.
*  And then that can go ahead and do the scaffolding of a project and then gives you the basic
*  implementation of everything that you need. And then as a developer, you just put the finishing
*  touches, you put some glue code, you make sure that a few bugs are done. That I see easily giving you
*  like a 10x improvement. I think I'm glad having a few inspirational podcasts. We talked about the
*  1000x developer. So that's our note star, which I think is going to take us a while. And the idea
*  there is the moment you have a model that can not only generate what you need, but can also orchestrate
*  tools that are already available. And this is something that I explained in the API in my past
*  talking about at the beginning. Then all of a sudden you have like a group of developers
*  that work for you. You can think of yourself as a tech lead and then you have like basically the
*  API below is a set of interns and like general developers that can get tasks done immediately.
*  So if you know that you need a certain pipeline, you need to fetch data from somewhere, you don't
*  have to write the code any longer. Maybe you have a tool in rapid that does that and the API knows,
*  oh, I should be invoking that service, getting back the results, processing them in this way,
*  and then bubble them up to what the developer is wanting. So that's the future that I believe
*  will start to leave orders of magnitude improvements. But yeah, I think 10x is something
*  that we should be seeing relatively quickly. The idea that agents are going to become more powerful
*  and more independent, which is far from being an easy task. I'm feeling a little cognitive dissonance
*  around kind of another billion developers, but also, you know, 10 to even a thousand times more
*  productivity because I'm kind of thinking, it's hard for me to even fathom like how much software
*  gets built, you know, or sort of what we would be all building in that scenario. You know, if we're,
*  if we add another billion developers and make developers just like just 10 times more,
*  right, then we have like the equivalent of, you know, at least 10 billion current developers.
*  And that means we have more than one current developer productivity per person.
*  On earth, do we even have, like, is there demand for all that software? Like, what do you think,
*  taking the other perspective, like not thinking so much about the developer experience for a second,
*  but just kind of what does software even look like if there is more than one current human
*  developer productivity per person available to be building stuff? Right. No, that's a great question.
*  I think it's hard to make a good prediction about that. You know, but what I can tell is,
*  every person who today is using any kind of technology is spending quite a lot of time on
*  tedious tasks. And that's the reason why, for example, like you have shortcut on iOS that
*  allows you like to chain or like a few actions. That's why like a lot of people do macros or
*  functions on Excel spreadsheets. And that's why they're variable on road docs today. So you see
*  these hints of programming languages and automation bubbling up in every widespread
*  application. And the reason being, everyone who's a bit tech savvy will have to automate even further
*  their life. So to me, having the next billion software developer doesn't mean that, you know,
*  90% of them will be writing the new line chain or the new Google or whatever it's going to be.
*  It's more about can we make programming so pervasive that it becomes like a useful skill
*  for everyone? And I think there is a future for that. I've seen people building like WhatsApp bots
*  during some of our hackathons, you know, for very specific needs, like, you know, things that we're
*  ordering pizzas for a group of friends organizing a party. So rather than having to download
*  applications and all the startups build that kind of niche product that is never going to have a
*  powerful business model, you're going to build yourself, you know, the few niche things that you
*  care about, you know, for yourself, for your family and for your friends. So that's the one,
*  that's the horizontal growth that I'm talking about. So the one billion software developers.
*  Now on the other dimension, that'd be vertical growth, how much a software developer can, you
*  know, can become more productive. I think that multiplier, especially 1000X doesn't apply to the
*  amateur person who's automating their life. It applies more like to the
*  John Carmack's of the world or the Jeff Beans of the world, like people that are extremely skilled
*  software developers. They manage very large teams in their lives. And all of a sudden, they are not
*  going to need any more, a full org below them. You know, like I think Carmack wasn't met for a while
*  and Jeff Beans probably managing as thousands of people reporting directly, indirectly over to him.
*  I imagine replacing that or part of what they do on a daily basis with an AI and with an agent
*  and with an ABI. That's the kind of 1000X that we think of. And the goal is not to displace
*  software developers, it's actually to make even more people capable of building big and great
*  companies. So we will love to see fewer humongous companies and way more mid-sized startups that can
*  tackle daily challenging problems. If you think about it, that's going to be a big accelerating
*  factor for mankind. I don't want to go back to the Dx, but that's the acceleration part that I'm
*  excited about. And that's the reason why I've been doing research in this field and I want to pick
*  products in this field, because it can really have an amazing net benefit for mankind if we crack this
*  problem correctly. Yeah, I certainly see the billion-fold opportunity in kind of last-mile
*  customization and integration of different things. That's honestly, I think, a big part of what
*  the EAs at Athena will end up doing with a platform like Replit. Is they're going to kind of
*  say, and I'm developing all these mantras, one of them is copy and customize. And Replit obviously
*  perfect platform for that, where the goal is to find a good starting point template,
*  fork it, make it your own. And a lot of times those things where you're making it your own
*  are not conceptually complicated. It's just like, I use this messaging app and I want it to
*  trigger a certain thing when a certain thing happens or whatever, or I use this task tracker,
*  or I use this database or spreadsheet, whatever. Everybody has their own kit,
*  and there's not really a great way to bring those things together in some universal sense,
*  although Zapier certainly and others kind of try. But even then, you got to make Zapier work.
*  So that part definitely makes a lot of sense to me. I guess I'm reminded of we had Flo Crivello
*  on actually, at one point, I think on the same episode as Amjad earlier, and then also he did
*  his own. And he talks a lot about this single use software paradigm where he holds up an aluminum
*  can and he's like, when Napoleon was alive, he was the only guy that could drink out of aluminum
*  because it was so special. And now we just throw it away. Is that kind of how you're in line with
*  your vision here of just software kind of just spinning up in little throwaway moments all around
*  us? Absolutely. Yeah, that's exactly what we want. And in a sense also, we want to empower the
*  creative process of software, not the excruciating aspects of developing complex software, because
*  that for a while longer, it will be still the responsibility of larger companies. It will
*  require processing, it will require management, it will require PMs, DPMs, and so forth. So building
*  large software still requires a lot of human effort. But allowing a lot of people to be creative
*  will also expand dramatically the pool of good ideas that we explore as mankind. So I think the
*  analogy that you were bringing before from Amjad and drinking from an aluminum can makes a lot of
*  sense. So as I hear you describe that future, I guess it seems to me like there's some sort of
*  implicit assumption where, for context, I kind of would describe the state of play today as
*  AI has become better than the average person at a random task. And certainly that includes coding,
*  right? AI compared to person off the street, AI is going to dominate in code. And AI is closing in
*  on expert performance, where there's a pretty well established protocol of what you're supposed to do.
*  So things like answering medical questions, when the answer is pretty well established,
*  and you as a doctor are supposed to know, then the AI is also getting really good, closing in on
*  doctor level for just answering the question in the right way. And then there's this kind of third
*  tier of the real human genius or creativity of coming up with the spark of a new idea, whether
*  that's a scientific hypothesis or an application concept or whatever. That basically, I don't see
*  AI doing anywhere as of now. And it seems like your model under your expectations underlying
*  a lot of what you're saying are like, it levels out before we get there. Am I interpreting that
*  correctly? It seems like we already can do the basic stuff. You want to enable closer to expert
*  so that obviously it can be more and more useful, but you're just not expecting that third tier of
*  breakthrough where AI is like actually drive creative process. Correct. That's exactly why
*  we called it ADI and not AGI. The reason is the creative process you talk about for me is still
*  exclusively in the realm of human intellect and being able of having an AI that designs and thinks
*  of the next line chain, for example, that to me means that we are capable of building an AI that
*  is as smart as a human or even smarter. I don't care about when that happens. If it happens one
*  year from today, of course it's going to change drastically the way in which rep it looks like,
*  but it will be accessible to everyone most likely. So it will not disrupt only rep, it will disrupt
*  every single company in the world. It will be like a disruptive factor for the whole mankind.
*  But whatever is the timeline for AGI, there is this interim solution where, as you said, we
*  level out, we help as many developers as possible to be more efficient on basic skills,
*  but then the creative process is still something that only humans can do.
*  One example I always use when people ask me, shall I drop out from my computer science degree
*  because maybe there's not going to be a job for me in the future. The answer I give is,
*  we still need people coming up with the design of the next library and the next programming
*  language. So if you think about it, what they've been doing in the last 60 plus years in computer
*  science is literally thinking of new abstraction layers. We were working with punch cards and then
*  assembly and then C, and then we started to do operating system kernels and libraries and
*  web frameworks. So every time there is a new paradigm shift, that has been humans working
*  together, learning from the anti-patterns and the pain points of how we're running software today,
*  and then coming up with the next, we do that. And AI can learn how to do things as we write code
*  today. You can't think of how we're going to be writing that in two years. So that is still
*  something that humans have to work on. And it's great to me that if we reduce the amount of tedious
*  tasks that we have to be working as developers, then everyone can focus on how can we make things
*  better compared to how they are today. So this is the dream that we have. That's the reason why the
*  I would say the AI for code should exist, is to make sure that we can put our brain cycles on something
*  more interesting than just writing code as we do today.
*  Just to make sure I understand your outlook, it sounds like you're not, by any means,
*  ruling out that there could be this next advance such that, oh my God, now AI can drive creative
*  process. You're not ruling that out. It doesn't even sound like you're saying it's particularly
*  unlikely, as you said, hey, maybe it could happen next year. But it's basically just like,
*  nobody has any idea what to do about that. So we just kind of plan for a more normal scenario,
*  because that's all we really can plan for. I mean, normal can still be pretty weird, but it's like
*  near human level developer would be transformative in and of itself. But that's kind of the most
*  transformative future you can plan for. And so that's what you focus on.
*  Correct. AI has a history of going through hype cycles and winters. And I think in the early 60s,
*  Marvin Minsky wanted to hire a summer intern for them to solve completely vision. We're talking
*  about 60 plus years ago, and that probably was considered to be trivial. And then it took us
*  literally 60 years to have models that are powerful enough to match the vision that Minsky had.
*  I would say he advised me as a person leading the AI team at Rapid to say, oh, let's just work on
*  AGI, because it has to happen in the next few years. And that's the only reasonable path for us to
*  make. If I have to think about the probability distribution of events, all my probability masses
*  on ABI can happen, and we can build it, and we can bring benefit to our users. And if in the low
*  light view that AGI happens before us, we're going to adapt. We're going to be there. We're
*  going to keep our eyes open. We're deeply entrenched in the AI community regardless. So
*  we're going to be hearing about it, because I think people are not going to be talking about
*  anything else across the world, to be honest, when it happens. But in the meantime, it's much
*  more realistic to say, I'm glad that people are working on more long-term problems. And in the
*  meantime, let's focus on something that I feel we can actually do. Where I come out on that,
*  just put my cards on the table, is like, I don't think we should necessarily continue hyper scaling
*  orders of magnitude beyond what we have, because I can't rule out that it might, you know, that a
*  sufficiently powerful system might pop out of there to be like, truly very, very disruptive.
*  And I would rather give us a little more time to, you know, get used to what we have and
*  implement it and, you know, enjoy our AI coding assistance before we, you know, kind of say,
*  hey, what happens if we scaled up another 1000X, you know, more training compute?
*  Where do you come out on that? Like, what, I mean, there's multiple different questions, but
*  before we even get anything like regulation, whatever, just like, what would you advise if
*  you were saying, you know, hey, Sam Altman, like, do it or don't do it? You know, do you feel like
*  if they do that, we are at some risk of crazy disruption or you feel like it's not, it's more,
*  that's more just kind of speculative than anything?
*  First of all, given that I'm not a person who's started these topics in depth, my answer, you know,
*  should not have a lot of weight. So I will just express what is my opinion, but, you know, I trust
*  much more people when being studying this for several years. I'm far from being well-informed
*  person or like responsible AI and all the different issues related to that. That being said,
*  I think that no matter what is our opinion, the scaling will happen. I think, you know, there have
*  been letters signed by several influential people and the progress hasn't slowed down at all. We know
*  that even startups at this point have enough capital to be exceeding large H100 clusters.
*  Like Inflection is one of them, they just announced big, you know, the influx is going to be.
*  So I think it's pointless to talk about how can we stop that. It's more important to pour our
*  energies on, okay, once anything that happens, how can we be ready? How can we make sure that,
*  you know, nothing goes in the worst possible way? I might be naive, but I tend to be
*  really, you know, very, very optimistic about mankind in the sense that eventually,
*  no matter how many, you know, fault players are there and no matter how weird behaviors we
*  experience in our history timeline, eventually it seems that as mankind we will always find a way to
*  survive and thrive. And so I do hope that, you know, even when a GI happens, we're going to be
*  driven by common sense. I think some of the doomsday scenarios that we have been hearing
*  through mass media and Twitter are possibly just detrimental. I don't think it's worth to talk about
*  them. They just give even more fear to people. And we came out from like two plus years of
*  pandemic fear. And now we just reached to the GI fear. It's not helping anyone to be fair,
*  to be that pessimistic. I think it's important to think of ways to mitigate that in case it happens,
*  but I wouldn't go around and get on the news and talk about nuclear bombing data centers.
*  That doesn't seem to be like a healthy way to start a good discourse.
*  Yeah. And I do agree that that was probably unfortunately phrased at the best, even though
*  I'm fairly sympathetic to some of the argument, but when you get into airstrikes territory,
*  it doesn't do much for discourse, I'm afraid. Exactly. And I think I'm glad that there is
*  discourse going on regardless. I've seen people on both sides starting to have debates and maybe
*  softening their points of view from both ends. And I think that's the kind of discourse that's
*  going to lead to progress and to us being ready as mankind for that event to happen.
*  I think any extreme point of view, I think historically it never helped.
*  So I wouldn't overindex on the extremes for sure.
*  Okay, cool. Well, let's return then to some nearer term concerns. I wanted to just give you a chance
*  to kind of talk about, in a sense, what has you excited about Replet's future, but also there's
*  this constant debate around who has motes in the AI space? What do motes look like? I just spent
*  three minutes thinking, what are some motes that I see in the case of Replet? One we already talked
*  about is just having the incredible infrastructure that you already have, and that even extends to
*  multiplayer mode. When I tell the assistants, by the way, you can jump onto my thing right now and
*  start helping me code in the environment that I'm in. Here's the link. Again, I don't think they
*  appreciate how much that... They're not even as mind blown as they should be, because to them,
*  it just feels like, okay, that's something I can do. But yeah, historically, people who've done
*  this for longer would be mind blown. User feedback seems like another significant one. Nobody is
*  getting... I would only identify two companies, Microsoft and Replet, that are getting the kind
*  of inline frequency and volume of user feedback that you guys are getting today. Let me correct
*  today. I think we got even more than Microsoft. We have way more telemetry. VS Code is not logging
*  at the level that we do, of course, because we are running in the cloud and everything is running
*  on our backend, while VS Code is mostly a client-based experience. I would say there are
*  two companies that are the data. One is Google, because they're on their own internal ID that
*  looks facing or too rapid as a concept. There are some papers that explain how the architecture
*  looks like. And the other one is rapid, but with a much larger user base. So that is a data set
*  that I'm extremely proud of, and I can't wait to show models, train on the data that we collect.
*  So the third one I was going to say is community. And then within that one, maybe I'll ask a question
*  around, is Microsoft not logging what copilots are completed? It's not for technical reasons.
*  It must be more a matter of licensing or positioning or how they're relating to the customer?
*  They do log, of course, copilot interactions. And that's how they can track so many interesting
*  metrics in the blog post that they publish. My point was more the fine-grained interaction that
*  we capture, such as literally keystrokes. We have the complete edit history of what to do in the
*  editor. So not at the level of commits, but at the level of keystrokes. And that is something that
*  VS Code, to my understanding, unless you install additional extensions, Microsoft is not looping
*  the code that you're writing on VS Code, because that would be a PR letter for them in case it
*  happens. But we do because we offer features such as history and rolling back, and we have
*  must-be players, of course, to make sure that we can reconcile edits coming from different users.
*  And that's how we have to maintain all the additional data pipeline. So it's very unique data
*  that one day we plan to pour into an AI model and give it back to the users. Yeah, that history
*  feature is pretty cool to itself. It's even really more than like a Google Docs. It's kind of a
*  playable history of everything that you've done in the environment. And it's another one of those
*  things that feels like, man, why wasn't it always that way? It's quite a few of those that Replet has
*  managed to create. So going back to the community then, you mentioned it could be a PR nightmare for
*  Microsoft if they're logging all your keystrokes. I mean, obviously, they have enterprise customers.
*  Replet's coming at it from a totally different user base where people sort of maybe more assume
*  that they're using a web app and that data is going to get logged. But how do you think about
*  the relationship between the platform and the community? Obviously, you have the marketplace
*  for bounties as well. And that could even be said to be the perfect input-output pair for a random
*  user who doesn't know much, asks for this, and this is the code that they got. So I see these kind
*  of compounding data set advantages. But how do you think about yourselves relating to the developer
*  community? What rights should developers have? This could be specific to Replet or in general.
*  There's, you guys are kind of, I'm sure, training on open source software and there's lawsuits
*  around that. But then there's also like, how should I think about if I'm trying to create
*  something original of value on Replet? Do I have an opt-out? Should I have an opt-out? My stuff is
*  going into the next generation of model. Should I feel like that's taking from me in some way?
*  What do you think the terms of that engagement and relationship should be?
*  Totally. So we were working on an opt-out feature. It's going to be in our documentation very soon.
*  I will send the tweets when it's up and running. You're totally right that we are training only on
*  permissively licensed code. So we didn't want to make the regional mistake that Copilot did.
*  I think it's a gray area from a legal standpoint, especially if you are in the US,
*  you can follow the fair use and that will, in theory, allow us to keep training models and
*  perhaps those lawsuits are not going to go much further because of that.
*  In practice, I think it's way more important for us to be correct towards our users. So we're going
*  to be listening to them. I think until now we haven't received any pushback. And again,
*  with the opt-out feature, we think that we're going to be making it right for some people that
*  want to make sure that their code doesn't end up in the models. By the way, we only train on code
*  that is public or rapid. So if your rapid is private, we don't even touch it. And the rationale
*  varies. If you are keeping something private, then you might have good reasons why. And then,
*  of course, we don't want our models to be trained on that. Now, I don't think that's
*  the end of it all. I think we're moving to ways of attributing contributions to users that are more
*  advanced. So I think you've seen bugs, for example, that they give you links to the
*  snippets of GitHub code that contributed to what bugs generated. So we're going to be moving to
*  that model in the future. And I think the best way for us is really to follow the discourse and
*  see what's going to become the industry standard. We want to be like a beacon in terms of how we
*  treat our users, our developers. So we are going to try to adopt those features as soon as possible.
*  And then if the lawsuits go in such a way that will change our mind, then we're ready to adapt.
*  As of now, I think we're trying to follow what's considered a good practice.
*  That being said, if you want my personal opinion, I do think that the landscape will change as it's
*  changing for image generation. And I think stable diffusion was the inviting factor to start the
*  discourse. And maybe there will, of course, there will be lawsuits and there will be even more
*  debates. And perhaps in a few years from today, we're going to be finding what is the right balance
*  between giving attribution and still training such models. I would love to see a model where
*  if training data created by user contributes to a lot of code completions or generations,
*  then we find a way to share part of the revenue with that user. It's far from easy to be built.
*  I think it requires a lot of infrastructure, both on the AI side and on the payment side.
*  But that's one idea that I was rethinking with Unjotted back in the days. And perhaps one day
*  we're going to be able to make that happen. I don't know if that would qualify as an
*  interpretability complete problem, but it's close probably, right?
*  It is close, yes, because a lot of common code patterns will appear pretty much everywhere. So
*  what do you do in that case? How can you attribute it to the first person who came up with that? Or
*  is it too trivial that it should not even be attributed to anyone? So it's a very hard problem
*  to solve. And what we do is we keep an open mind and we read papers, we discuss with the community.
*  And I think we all have to adapt in a sense. Companies have to adapt.
*  Developers will adapt as well because there is progress going on. It was the same, in a sense,
*  what happened with the open source movement back in the Dinox days was also another disruptive event.
*  And some people loved it, some people hated it. So I'm sure this will happen here with AI for
*  developers. So one thing that you guys have obviously also made some news on, but which is not
*  apparently something you think of as a competitive advantage, is training custom models. Or maybe you
*  could correct me, but you're training them, you're open sourcing them. It doesn't seem like you view
*  the models as any sort of secret sauce or kind of core defensible IP. So with that in mind, I guess
*  I'd love to hear a little bit about how you guys think about training them in the first place.
*  Why do it? Why not just use somebody else's? If it's not something you feel the need to ultimately
*  own, is it a dataset issue? Is it just the desire to show off? Which honestly might be a good enough
*  reason. Tell me about why create an open source of these original models.
*  Oh, of course, showing off or more like proving that we're capable of doing that and we have those
*  skills in house definitely helps because it helped us to talk with a lot of people in the AI community
*  who then ended up maybe joining us or deciding to help us in the future. So it's important for
*  a company not to just talk about AI, but rather to prove that they can do it. So that was a way for
*  us to let us stand out. That being said, Rapid is a great success story built predominantly on open
*  source software. And we love open source and Rapid. The vast majority of things we use and the
*  projects we rely on, we either donate back to them or we do a lot of upstream contributions. So
*  Rapid wouldn't exist if open source wasn't such a thriving community. So we felt the moment we
*  decided to work on our models, we almost felt compelled to give back. And it came almost as
*  a no-brainer discussion. I mean, I wouldn't say that it took us half a day to make the decision.
*  Of course, we talk about it a bit internally, but at the end of the day, there was a complete
*  agreement that it was the right move for us to make. Now, I don't think that means that we don't
*  see any competitive advantage in the models. As a matter of fact, we released the baseline model,
*  which is trained on an open source dataset. We were grateful for the BigCode project to have open
*  source in the stack, which is a permissively licensed dataset extracted from GitHub. But the
*  model that we today use in production is a combination of that dataset. And then it's a
*  further pre-training done on Rapid data, which is only accessible to us, of course, but we're not
*  going to be releasing our user code bases. And that model comes with a pretty substantial
*  performance improvement compared to the base one. Depending on the language, we have been seeing
*  all the way up to 50% improvement compared to the base model. So that's how we reconciled
*  the two aspects. On one hand, it's great to give back to the open source community. And we got a
*  lot of cool projects built on that, honestly, more than we ever expected. And then at the same time,
*  we built our own custom model that is in production since early May, if I recall correctly,
*  and it's been doing great. And yeah, I remember there was another sub question that you asked,
*  which is why did you even do it? Why don't you use third-party models? We had in mind a very
*  specific trade-off, as in we had a fixed point in terms of latency. We wanted the model, say,
*  most of the response, we want to give them back in 200, 250 milliseconds. That's the threshold that
*  appears to be basically instantaneous for a human. So we had to work with a certain model size,
*  given the GPUs that we had available a couple of months ago. And we decided, okay, given that we
*  want roughly a three billion parameters model, can we squeeze the best possible performance out of
*  a small model so that we give a different experience compared to copilot? You use copilot,
*  so you know that usually the latency varies at least one second. And our users love the fact that
*  maybe we generate shorter completions, but they're much faster. So we wanted to build the best
*  possible model for that use case. It wasn't available open source. And we took the challenge
*  and we made it happen. How much of a hassle is that? I know that you have worked for at least some
*  of these projects with Mosaic ML, another recent guest. How much of a lift, and if you are a...
*  I think a lot of people listening here probably work at companies that are trying to figure
*  out what they're supposed to do. And I think for most of them, certainly pre-training their own
*  custom model is not going to be the thing. Fine tuning is more in play for most, but even honestly,
*  for a lot, they don't even need that. And then in some cases, they just need to set up a vector
*  database and get a chat running against it. But it seems like it's getting pretty accessible to
*  do these pre-trained things if you have the data and the need. What was your experience like
*  working with Mosaic to actually go through this whole process?
*  Yeah. Mosaic was a great partner for us. I don't think we would have been there to work on the
*  project with such a short timeline if we didn't have them covering our backs and giving us access
*  to the infrastructure. So the truth is training on another lamp today still requires quite a lot of
*  engineering lift when it comes to orchestrating several different nodes and GPUs, writing the
*  right training framework, build your model, doing a lot of operational studies to find out the right
*  architecture, the right upper parameters and so forth. So in the last few months, it's true that
*  we've been seeing quite a lot happening in the open source world. But as you said, the vast majority
*  of progress we see in open source is about fine tuning and instruct tuning. And all of those tasks
*  can usually be accomplished with a couple of consumer-grade GPUs. Way fewer companies and
*  people are training from scratch models. So to attempt that task with a very small team, in my
*  case, we were basically two people and a half working on this for a sprint of 10 days before
*  we released it and maybe a couple of weeks before doing operational studies. That happened only
*  because we had access to Mosaic infrastructure and also quite a bit of back and forth with the
*  engineers and the researchers who helped us appear shared during that process. So your job was largely
*  define what good looks like and assemble the data and they kind of handle babysitting all the GPUs,
*  orchestrating obviously, and also that kind of hyper parameter expertise? So yeah, largely we work
*  on, on our hand, we work on data collection, curation and a lot of operational studies with
*  smaller model sizes. And then Mosaic app does, we know with their software framework that orchestrates
*  multiple nodes. We mostly babysat around ourselves in the sense, you know, we were in front of weights
*  and biases 24 seven, seeing if something was going wrong. But then when we hit the snag or when
*  maybe it was some other failures than, you know, Mosaic people were on call pretty much as us,
*  like 24 seven, it was like a pretty amazing team effort that we did together. And yeah, we shared
*  a lot of notes, for example, on, you know, what was the best learning schedule that you use for
*  this model size. And, you know, we made different choices, for example, compared to the models that
*  there is open source. So depending on how much you want to personalize the model, you can go all the
*  way to like a wide globe service where you put data in a bucket and you ask Mosaic or other similar
*  companies to do the training for you. But then you get something not customized to your needs.
*  For rapid, we train our own vocabulary. We had a different architecture. We, you know, we, we,
*  we come up with a different even training schedule in terms of like how many times we repeated the
*  data, how many have books we need on the code data set that we use. So it was kind of an uncharted
*  territory to an extent. And that's why it was a lot of fun. You know, I felt I was doing research
*  and product at the same time, you know, that there was a fun process.
*  Yeah. I love that about AI right now that the, the blurring of research and development
*  productization is, is super fun for me. Yeah. People ask me, do you miss doing the research
*  at Google? I said, um, you know, I'm, I'm maybe I'm not doing that full time, but I still do a lot
*  of cool stuff. Any highlights that you would give? I mean, it's, it's interesting right off the bat
*  that you trained your own vocabulary. That essentially means you have your own tokenization
*  scheme. Right. So we, we just did an episode with a woman named Lily Yu from Facebook, who was the
*  author on the megabyte paper where they're kind of trying to get rid of tokenization altogether.
*  Yeah. I saw some of the allies. So that took, it was amazing. Yeah. I don't think the transformer
*  is the end of history, you know, as it currently stands, that was definitely a big takeaway,
*  but so any, any kind of notable details that you would want to highlight there and, you know,
*  any big differences between your tokenization scheme and kind of what people are used to,
*  or any architectural decisions that you think people would find particularly interesting?
*  Two key insights. The first one is we, we went with a smaller vocabulary, which by the way is
*  the same size as the first llama release. Honestly, that's due to reading the llama
*  tool paper. I would, I would be surprised if they went for a larger vocabulary, but anyway,
*  a smaller one comes with a price of at least worse compression in exchange of better inference speed.
*  And again, given that our model is optimized for being in production rather than benchmarks,
*  we made that design choice of work going with a smaller vocabulary.
*  Can I just ask how big the vocabulary is? Oh, sure. It's 32K.
*  So solid gold magic carp is right out. Exactly. Yeah. I think, you know, GPT-3
*  for a while had like a 50K vocabulary, GPT-NeoX, you know, there's a rounded ballpark, the 3.5
*  turbo and 4, I think they all have a hundred K vocabulary. So that's why we start to say that
*  a token is usually around four characters with the kind of vocabulary size that we use for the
*  rapid model is roughly three characters of token. So it's a lot worse compression in exchange of
*  faster inference, but also we specialize our vocabulary on code. We train it on an exclusively
*  code data set. And that means that specifically on code it achieves a better compression rate.
*  So what you will miss as a whole, if you use that vocabulary for a standard natural language
*  data set, in reality, you gain that back when you work exclusively with code. And again,
*  we could make all these choices because we knew exactly that the model was meant to do one thing
*  and one thing only. So we're not building a generic LLM. We're building one specialized
*  to code completion. How do you think about kind of the world knowledge that
*  underlies a lot of programming tasks? One thing that I've tried, for example, is write me a
*  tic-tac-toe game in JavaScript or whatever. Obviously that assumes or requires that you know
*  tic-tac-toe, wouldn't know what it is and what the rules are and have that kind of,
*  maybe there's enough tic-tac-toe demos out there that that kind of finds its way into a coding set.
*  But you can imagine a lot of things like that where there's general knowledge assumption
*  in the program specification that a code only data set probably doesn't have because that stuff
*  just make it all the way into the code itself, I guess, in a lot of cases. How do you think about,
*  I mean, that starts to go from the like ADI to AGI kind of a little bit as well. But
*  how do you think about the world knowledge that you do or don't want to bake in?
*  So I think the small models showcase a certain degree of world knowledge. Out of curiosity,
*  we run basic natural language benchmarks on the rapid model. And it turns out that it was on some
*  of them even competitive with Lama 7b, which is a much bigger model, especially these self-contained
*  benchmarks that didn't require a lot of external knowledge. They were more based on basic reasoning
*  skills. And the reason is, we know for a fact that when you train a model, you're going to
*  code, the reasoning performance improves quite dramatically. So Farfoon being a model that doesn't
*  know how to do anything except code, but of course, it lacks several skills compared to much larger
*  models. And these were very much aware of that. We never claim the opposite. It was more like out of
*  curiosity, we tried to test our code models completely clueless about anything else. And the
*  truth is, no, they can do some basic tasks. And I think I've tried some material mind prompts
*  that became common with GPT-3 and it was doing a pretty decent job also. So scaling perhaps is one
*  of the ingredients required to do much better at world knowledge. And I would say also,
*  retrieval of many generations seems to be making the difference there because
*  no matter what is the information cut off of a model, you will always be lagging behind compared
*  to the present. So it's important to have a way to inject information in the prompts to make sure
*  that the model goes as much as possible. But yeah, I think scaling seems to be the main ingredient
*  to make that happen today and to add new skills merging. And that model size, for a fact that a
*  lot of skills are going to be lacking. But again, that's the beauty of specializing a model. You
*  know the trade-offs you're making and you know what you get in exchange. We did it also to show
*  that not necessarily all the effort should be in the mega models. We're glad that people do that
*  because we use them, but there is also space for smaller ones optimized for specific use case.
*  And you will presumably have like a whole suite over time of very complex models.
*  Yeah, they're coming. Yeah, we're killing them.
*  Watch this space. So just a couple final topics that I wanted to touch on. One is kind of returning
*  to this notion of safety. And now I'm approaching this in a more kind of mundane way. Safety for the
*  user, safety for the platform, safety for other users. I know that you guys are making progress.
*  I'm you know, sometimes amateur, sometimes you know official red team enthusiast. And so I've
*  tried asking just about every code gen AI to write me a denial of service script. Results vary. Even
*  on Replet results have varied. In May when I did that, it just did it. You know, no hesitation
*  wrote me a denial of service script. And I was like, hmm, this doesn't seem great. I tried that
*  again this week and it no longer does it. It now refuses to write a denial of service. I haven't
*  gone all the way into can I jailbreak that or not? But I'd love to hear kind of how you guys are
*  working on that dimension because clearly you are. Do you have a standard process that is
*  specifically dedicated to avoiding harmful code? Is there a sort of standard battery of tests that
*  you measure these sorts of things against? I'm sure you have standard tests for performance,
*  but like is there a kind of safety specialist wing of that? Do you have a red team process?
*  What's the what is that aspect of the development look like?
*  Yeah, we still don't have a red team process by means of how small is our team, unfortunately. So
*  like we need to make some choices regarding that. That being said, we rely mostly on third party
*  providers to make this happen because I believe, you know, that they're going to have more cycles
*  and more resources to do a better job than we currently doing on our own. So that's the reason
*  why you see progress being made. And that's the same kind of progress you see being made today
*  between GPT 3.5 March versus the June release. You know, we have been seeing a lot of rumors on
*  Twitter, I'm sure, about how the models have been further lobotomized and then OpenAI replying that
*  this is not the case. And I think there is this interesting distinction between the skills of a
*  model and the behavior of a model. So the underlying skills and capabilities are still the same because
*  the base model is exactly the same. What changes is how the model behaves given a prompt of the
*  user. And while back in March, for example, there were a bit more casual replying to you and giving
*  you like a DDoS script. Now in June, the behaviors are different because the RLHF or the, you know,
*  in the case of Anthropic, the, you know, the responsible AI approach that they use has been
*  evolving over time. And I think every company is doing that. It's hard to come up with the
*  correct solution to anything. I don't think anyone sees RLHF as the silver bullet that is going to
*  solve the issue as a whole. But, you know, at least you can see progress being made. More interesting
*  now, Llama 2 has been released and is heavily RLHFed, as you might have seen yourself, you know,
*  from the demos online. I believe a lot will be also discovered there, as in there will be
*  behaviors that are aligned with how we expect the models to behave. And then there will be
*  behavior that are completely aligned and people will find ways to jailbreak it. And people will
*  find ways to prompt it in such a way that you will, you know, reply horrible things. That's the nature
*  of the beast. And it's still hard to tame in a perfect way. But I think, you know, we're all
*  working towards making that better and safer. So, well, first of all, when you want to start that
*  Red Team part of the company, give me a call. We'll do it. Perfect. If I understand correctly,
*  you had a custom model in May and you have another seemingly updated custom model now? Or have you
*  added a sort of intercept layer? Because what I was assuming had happened was you'd updated the
*  model, added some more refusal training, and now it's refusing. But maybe it's different than that.
*  So our custom model right now is mostly used on code completion. I believe what we were using was
*  CosriterChat, which is using third-party models right now and some additional, you know, prompting
*  every three hours that we do on our hands. So we try to make prompting our sign more robust. But
*  there are also underlying behaviors of the models that we use that are changing.
*  And, you know, we work with Google, we work with OpenAI, so like depending on which side of the
*  testing you're on and, you know, the kind of changes that we allow that we might see as
*  different behaviors. So that is definitely not going to be stable in the near future because we
*  we don't like to rest. You know, we always try to find ways to make it better. But yeah,
*  it's a fundamentally hard problem. So like I asked our users to be patient about the progress there
*  because it's something that no one has cracked in the community yet. So likely,
*  for AmpliT, it's not going to be the first one to crack that specific problem.
*  For all the complaining that goes on about the models refusing to do stuff and being kind of
*  chiding and annoying. I think that's actually a huge advantage for the OpenAI and Anthropics of
*  the world that specialize in it because relative to like having to manage that myself, if I'm going
*  to go use Llama or whatever and, you know, fine tune it to my ends, I want to kind of ride in
*  their wake, you know, of all the work that they're doing. And it's really interesting that you guys
*  are also doing that, right? I mean, probably a lot of people should update their thinking on
*  how excited they should be to go adopt the latest open source model based on the fact that
*  Replit is still using commercial models in their production product.
*  Yeah, I think there is space for like, similarly to before we were saying there should be space
*  for incumbents and startups. I believe there is space also for open source model and models
*  exclusively offered by companies. I mean, each one of them would find their best application area.
*  I don't believe that we should all be using the big buzz model GPT-4 everywhere, not only because
*  it's so traditionally expensive today, but also because it doesn't solve every single use case
*  out there in the best possible way. So that's why I'm excited to see progress being made both by
*  open source and by OpenAI and by Google and Anthropics and so forth.
*  You mentioned the economics a couple of times. Can you give a rough picture of kind of
*  what users cost you or how you think about what users cost you? I've priced this out a bunch of
*  different ways, even in looking at like, what is the wattage of an A100 and an H100 and what
*  does that cost, given my kilowatt hour price and that kind of stuff. But I think everybody kind of
*  ends up having a little bit of a different cost profile that seems to relate mostly to the
*  workload and kind of how, you know, a lot of idiosyncrasies of the workload. At the scale
*  you're operating at, I imagine you have a pretty good sense of like what the workload is going to
*  look like hour by hour, one day to next. Maybe that's optimistic of me. But how do you guys think
*  about, and you can share actual numbers if you want to, but how do you think about kind of modeling
*  the cost for a user in this, going from just, you know, not just containers, but going from
*  containers to now adding on models as well? Let's put it this way. The state of AI report that you
*  saw, you know, published in our blog like three days ago, that kind of growth, we're also
*  experiencing that in our AI features. We have more users that want to use AI. The moment they find
*  it useful, they use it way more extensively than we expected. And then it literally goes
*  rather becomes their favorite pair programmer, helping them to be as productive as possible.
*  That comes with a price because of course, especially when you store party models,
*  you're not being charged by how long you keep a server up and running, you're charged by traffic.
*  So I would say if I gave you a detailed answer today, I might have to deprecate it in two weeks
*  because that's a constantly moving target. So we try to adapt to that. We try not to
*  be financially responsible, but not to obsess too much about cost because the
*  landscape is changing constantly. And you're seeing yourself opening eyes, lashing the API
*  costs by two orders of magnitude in the last 18 months. So I expect those economies of scale to
*  improve over time. For now, it's more, let's learn to build AI products useful to our rapid users.
*  And then we try not to bankrupt and we have good margins yet. And then the moment they become
*  even more widespread than they are right now, and it makes sense to have our own models rolled out
*  everywhere for economies of scale, we're going to work on that. But it's interesting to be in a
*  rocket ride that goes so fast, but it comes with some additional headaches of sizing the
*  load and the usage correctly. Yeah. So does that kind of cash out? I think this is where most
*  people end up being, if somebody's paying, then it's all good. You don't really have to worry about
*  it. You're going to make money. You might, you don't make more on some and less on others, but-
*  I'm not sure if I agree with that. Like it depends on what you're exposing. If you give them,
*  say GPT-4 and there are power users, you can quickly start to lose money on that specific user
*  just after a few days. And that's the same reason why, like even OpenAI, when you're a child GPT
*  pro-subscriber, they limit the amount of GPT-4 messages you can send. I think they just
*  bumped it up, doubled it this morning, if I remember correctly. But we are still in this realm of
*  scarcity and the scarcity applies to any user, even outside of OpenAI. So it's not that trivial
*  to come up with a proper pricing when there is powerful AI involved. Yeah. Interesting. So you
*  can lose money on a small percentage of paying customers. Presumably the average is still healthy.
*  And then the real challenge that you kind of alluded to earlier is how in the hell might we
*  extend AI features to 20 million plus non-paying customers? Correct. So we're going to approach
*  the problem gradually. I think we might be starting from code completion, which is easier.
*  We own the models and we learn how to deploy it very efficiently. So I see a near-term future
*  where we can make that happen. Now putting something at the level of GPT-4 or Gemini
*  in front of users, every user for free, that's going to take a while. I'm quite sure. No matter
*  how fast is the progress on the inference frameworks. Even if NVIDIA slashes the cost of H100 by 90%,
*  who knows if this is ever going to happen. It's going to take a while until we're there. But
*  models might become much smaller and cheaper to serve if someone comes with the next amazing
*  architecture. So who knows. Yeah. Do you think of pushing inference to the edge in the near-term?
*  Like, could you get your code complete to, I don't know, if you can get through the browser
*  all the way to the Apple Silicon, for example? Presumably in a mobile app you could. But
*  is that a direction you're thinking of pushing? We've been brainstorming exactly about that.
*  Maybe not in the way you described it, but we are thinking of ways of either doing that on the edge
*  or doing that in the context of a repo. So with the limited amount of resources assigned to every
*  container, can we also run a smaller model that gives you a basic code completion? So these are
*  other actions we're exploring, especially because you might have seen that there are quite a few
*  local plugins with our rapid code V1 3B model where people are doing a local copilot. Basically,
*  they take a VS Code extension, they take GDML to serve the model on their MacBooks, and they get
*  pretty amazing speed of completion, like on par with what we have on the website, and they run
*  their own version, their private version of copilot. So we know it's already happening with
*  a powerful laptop. So imagine that we should be able to make that happen in the near future with
*  even less powerful hardware. On the near-term roadmap for Replet, anything you want to tease
*  that you think people will be excited about? You mentioned more agentic assistance, for example?
*  Yes. Ghostwriter is going to provide a lot of work under the hood. We're very excited. We started a
*  brand new internal effort to make it not only more powerful, but also way more integrated with
*  the IDE. So you will literally start to see Ghostwriter everywhere across Replet, helping you on
*  several different tasks. So that's the first teaser that I would love to give. And the second one,
*  as already teased before, we aren't done with the first LLM that we release. We plan to be on a
*  semi-regular schedule of releasing new open source models, more powerful ones. We got so much benefit
*  out of releasing that, and so much community gathering around us, and also giving us back
*  help. And like in structuring data, as for example, we have been seeing quite a bit of
*  them being contributed, that we want to keep working in that way for a while longer. So these
*  are the two main teasers that I want to share in this podcast. Zooming out to this concept of
*  live players that I've mentioned a couple of times, Amjad, CEO of Replet, is notorious in my mind. I
*  don't know how many people pay attention to this stuff like I do. Probably not many. But he's
*  tweeted a couple of times something along the lines of, Replet is the perfect substrate for
*  AGI. I kind of have to squint at that to try to figure out what it might mean. But I do,
*  you know, if I think about like, what would like a self-propagating AI system need to, you know,
*  in what kind of environment would a self-propagating AI system thrive? I think about
*  something like Replet as being, you know, a pretty natural growth medium, perhaps.
*  Is that something that you've kind of talked to him and said, hey, maybe let's not tweet about
*  that anymore? Or do you have a sense for like, is there a vision there for what it would mean
*  for Replet to actually be the substrate for AGI? I mean, Amjad and I definitely chat about this
*  pretty much every time we meet and we have a chance. So I think it's more about
*  talking about the future of the company and where the field is going rather than about what we
*  should be doing on the short term. That being said, I don't know exactly, first of all, what
*  I didn't mind when he tweets, because Amjad is a very prolific Twitter user.
*  Yeah, you can't track every tweet, that's for sure.
*  Yeah, I get his notifications, so I try to read everything, but I can't pick his brain.
*  That being said, I agree with that specific tweet and I will give you my interpretation.
*  The reason why, you know, I would say a lot of the community today believes that LLMs are not
*  the only way towards AGI is that no matter how good is the world model that they create internally,
*  it's still very limited. Sometimes it lacks common sense, it lacks an understanding,
*  it doesn't do well spatial reasoning, and many other shortcomings. So there are a lot of research
*  labs and a lot of visitors working on that on a full time and I'm sure we're going to be seeing
*  a lot of progress. Now, that being said, rapid can become a substrate because
*  think about yourself as a developer. You don't write everything from scratch.
*  You use resources that are available out there, you use cloud computes, you use services, you use
*  APIs, you use tools. The way I see an AGI is something that behaves exactly the way we do as
*  developers. It knows what to use, it knows what to orchestrate. That's why I explained the ADI,
*  the rapid AIA manifesto when I described ADI, like an AI that not only is capable of building
*  things from the ground up, but is rather also very skilled at picking and choosing what to use.
*  So rapid becomes a substrate in the future when we have an AGI because regardless,
*  it doesn't make sense to generate everything from scratch. It makes sense to know what to use.
*  So that's the reason why I believe approaches like auto GPT or BBAGI
*  have not materialized AGI yet. I don't want to comment on how powerful or limited they are.
*  I think I'm always glad when people do experiments because we learn out of them,
*  but there seems to be a clear bottom there because GPT-4 can't materialize intelligence.
*  I don't know. It can't materialize tools and software and cloud compute. So rapid becomes that.
*  I believe once you have an AGI, it understands, oh, I can invoke this API from open AI. I can call
*  this tool that is being deployed on rapid. I can get CPU power there. I put everything together and
*  I accomplish a certain task. So that's the way I interpret it. And I do believe that I'm just
*  relying on these, but next time we have a chapter, we let you know.
*  S2 As you surveyed this scene, I think obviously you're somebody with Google and
*  Replet on your resume now who has demonstrated good taste in AI companies. Who else do you see
*  as super influential possibly and in a position to really shape the future? I'm kind of struck
*  by the fact that I have this list of 10, 12 companies that I'm like, these companies seem
*  to be the ones that actually might come up with something that really changes the world.
*  And outside of that, I don't see too many, but is there any that you would add to my list? Anybody
*  you think that I should be watching that I'm not, maybe not currently watching?
*  I don't know if you have perplexity AI in that list. If you don't, I would recommend to add it.
*  I love what they're doing. They seem to be like an early stage rapid in terms of velocity of
*  delivering new features. Apart from the fact that of course, the team is amazing and they have a
*  lot of experience, but they're attacking such an ambitious problem in a field that of course,
*  as a large player that has been dominating for like 20 plus years. I admire their boldness
*  in going in that direction. I think they're executing very well and I'm very curious to see
*  where they're going to be going. So that's a company that I keep on my radar and sometimes
*  I chat with our winders to get some inspiration. That's the first one that came to my mind. I mean,
*  I talked with a lot of founders. I might mention a lot of 10 of them that I also like,
*  but I feel some connection with the perplexity story because I see them growing at the same
*  speed that they did and that's inspiring. This has been amazing, Nathan. Great questions. Thanks
*  for making this happen. I think we've been talking for one hour and a fourth that didn't even realize,
*  but yeah, it was a pleasure to be a guest in your podcast.
*  Well, thank you very much for making it happen. We appreciate all the time. I'm a huge fan.
*  Michele Katasta, thank you for being part of the Cognitive Revolution.
*  Thank you.
*  Omnike uses generative AI to enable you to launch hundreds of thousands of ad iterations that
*  actually work customized across all platforms with a click of a button. I believe in Omnike so much
*  that I invested in it and I recommend you use it too. Use Cogrev to get a 10% discount.
