---
Date Generated: April 02, 2024
Transcription Model: whisper medium 20231117
Length: 4360s
Video Keywords: []
Video Views: 1524
Video Rating: None
---

# AI Friends, Real Relationships with Eugenia Kuyda, Replika's Founder & CEO
**Cognitive Revolution "How AI Changes Everything":** [February 16, 2024](https://www.youtube.com/watch?v=584wLIfngG8)
*  The app helped them get away from suicidal thoughts.
*  The number of people that commit suicide every year is shockingly significant.
*  So to be able to reduce that by something like 25% or more is a pretty big deal.
*  I think most people do understand really well what they want from Replica and they get it.
*  Most of the people are coming for a deep relationship, for feeling heard, feeling accepted.
*  With all of their fantasies and emotions and fears and anxieties and insecurities.
*  And that is such a big gift to give to a person versus Chagie Petit where it's not going to feel natural.
*  We're not going to just say in between the queries, you're not going to say,
*  oh, by the way, my boss just can't believe it.
*  He believes this guy.
*  You know, you're just not going to say that, you know, it's just not really there for that.
*  It's not also saying something like, well, OK, so while I'm doing this for you,
*  just tell me how you've been doing.
*  Hello and welcome to the Cognitive Revolution, where we interview visionary researchers,
*  entrepreneurs and builders working on the frontier of artificial intelligence.
*  Each week, we'll explore their revolutionary ideas and together we'll build a picture of how
*  AI technology will transform work, life and society in the coming years.
*  I'm Nathan Labenz, joined by my co-host, Eric Torenberg.
*  Hello and welcome back to the Cognitive Revolution.
*  Today, we're continuing our brief journey into the world of AI consumer apps.
*  With returning guest, Eugenia Quida, founder and CEO of Replica, the AI companion, who cares?
*  When I last spoke to Eugenia, almost a year ago exactly, she was navigating a very tricky period
*  for her company, as the sudden improvement in available language models had presented her
*  with an opportunity to dramatically improve the quality of conversations that users could have
*  with their virtual friends, while at the same time also necessitating some painful changes,
*  including restrictions on just how sexual those conversations could become.
*  Some users were extremely upset.
*  And while the YouTube comments on that episode are filled with anger, I came away from the
*  conversation confident that Eugenia cares deeply about Replica's users and really intending to
*  follow her work into the future, expecting that as AI models mature, the challenges inherent to
*  building AI friends and AI girlfriends would become not only ever more nuanced, but also much
*  more focal. A year later, here we are.
*  Replica has recently been in the news again, this time for a study out of Stanford that found that
*  some 3% of users got relief from suicidal ideation from their use of the app.
*  We discussed that research in some depth, and it's worth noting for anyone who wants to avoid
*  such topics, that while our discussion is really rather academic, minimally emotional, and not at
*  all graphic, we do mention suicide a couple of times along the way.
*  Beyond that, we go on to talk about how people are relating to Replica generally today, why the
*  process of upgrading the underlying language model is actually much harder for an AI friendship app
*  than it is for a productivity app, how the market for AI companions has grown, Eugenia's surprise at
*  what a major use case image generation has become, the mix of language models that Replica is using
*  under the hood today, the features they're building to improve emotional connection and proactivity
*  in the future, how Eugenia's vision for Replica is expanding to include assistant functionality as
*  well, why relationships might prove to be the great moat for AI apps, and what standard of care AI
*  application developers owe to their users.
*  As always, if you're finding value in the show, we appreciate it when you take a moment to share it
*  with your friends.
*  With all the AI-powered toys, companions, friends, coaches, and therapists coming online right now,
*  I would encourage you to send this episode to the parents and also to the mental health professionals
*  in your life.
*  Now, I hope you enjoy what I hope will become a regular feature, an informative and thought-provoking
*  conversation with Eugenia Queda of Replica.
*  Eugenia Queda of Replica, welcome back to the Cognitive Revolution.
*  Hi, Nathan, really good seeing you again.
*  Thank you so much for inviting me again to the podcast.
*  I'm excited to have you back.
*  It has been quite a year for everyone involved in AI, and I suspect that that goes double,
*  if not more than that, for you.
*  So we've got a lot to cover.
*  A year ago, you were, I believe, just our second guest, and we've done like 100 episodes now and
*  kind of tried to cover AI from every different angle.
*  One that remains as fascinating and I think as uncertain in many ways now as it did then is the
*  future of AI companions and friends and social dynamics and how we're going to spend our time
*  and who we should trust to build those systems that we're going to spend a growing share of our
*  time interacting with.
*  And there's just a ton of stuff that is crazy.
*  So I want to kind of get your brief history of the last year, talk about some new research that
*  you have just put out with some collaborators and maybe get your take on a bunch of questions that
*  I think more AI application developers should be thinking about.
*  How's that sound?
*  Sure. Sounds amazing.
*  I would say a year ago, probably my biggest takeaway from our conversation was, wow, there are a
*  lot of people out there who are dealing with a sort of loneliness and isolation that I had just
*  never really considered.
*  Maybe the number one quote of all time on the show was when you said, we couldn't create, this is, you
*  know, pre-ChatGPT, pre-generative AI, pre-GPT-3, we couldn't create a bot that could talk, but maybe we
*  could create one that would listen.
*  And I've just that has echoed in my head so much ever since.
*  But, you know, part of the kind of transition that you and the whole world have had to go through is
*  the systems that once were pretty basic and mostly capable of kind of listening and making you feel
*  heard and could even kind of, I think you used the term parlor tricks to describe some of the early
*  versions.
*  Those have now given way to much more advanced conversational systems that you can really interact
*  with in a much deeper way.
*  And so that's brought all sorts of challenges to the fore.
*  You were at the time going through this period of removing a lot of the more, I don't know if you'd say
*  romantic or sexual type of interaction between users and their replicas.
*  And this was met with definitely some reaction.
*  So I'm here for starters, just kind of how that unfolded, because we talked in just that very critical
*  moment as that change was happening.
*  How has it evolved over the last year as people have sort of both, you know, had to let that go?
*  I don't think you've reversed that, but also, you know, presumably enjoyed a lot more sophisticated
*  interactions with their AI friends.
*  Yeah, it was definitely wild here.
*  Just, you know, for starters, like a year ago or a year and a half ago, the conversation about AI, even the
*  one we had with investors and the common truth about AI, we're completely different from what is happening
*  now. If you think of it just a year and a half ago, the whole conversation was about creating DataFly
*  Wheel and having some sort of pre-training your own model 100%.
*  Now that whole conversation is really gone.
*  Like the DataFly Wheel thing, for instance, doesn't make any sense anymore.
*  Anyone can start, you know, chatbot and collect the necessary data very quickly.
*  There's no network effects with collecting significantly more data.
*  It's all about like really good quality data.
*  So a lot of things. The only reason I'm going even there is that I'm trying to show that things change so
*  dramatically in a matter of a year, even things that seemed absolutely, you know, cornerstones for the
*  industry just a year ago.
*  Of course, now, you know, no one I think is arguing anymore with the commonization with the models, the
*  foundation models. And, you know, most people have access to the same quality, really high quality of
*  AI, just a matter, really the matter of price.
*  So the law changed. And the same goes to the conversation about AI companionship, AI friendship.
*  I think the year last year it started with maybe asking the wrong questions.
*  The question was, well, do you allow people to fall in love with AI and have some intimate
*  conversations instead of asking the question, well, what's the main goal for the app?
*  If it's to make people feel better, is it to make people feel better emotionally?
*  Or is that some sort of, I don't know, an adult product and so on?
*  Or is it to get people hooked and attached in a bad way and then show them more of ads?
*  I think these were the correct questions that they remain to be the correct questions.
*  We always answered those questions in the same way.
*  For us, the main goal for replica was to make people feel better emotionally over time.
*  It was always about long term relationships.
*  What we maybe didn't realize in the very beginning when we started replica is that people will fall in love.
*  If it's something that is so accepting, so loving, so caring, there's kind of it's very hard to stop that,
*  you know, from crossing over if you build something really, really pathetic, if you really fulfill that desire,
*  that need. And so I guess we spent the better half of the year on figuring out two things.
*  First of all, we know it's helping people feel better emotionally.
*  We've done some studies with some universities over the course of the life of the company,
*  done internal studies, but we wanted a bigger one.
*  We wanted to show the world that look like it's not just us waving our arms and saying, look,
*  we're helping people feel better.
*  And it's not every review on the app store that says that or every person on Reddit that's talking
*  about it. We wanted to show some something that people would trust and believe and kind of listen to.
*  And so we're very happy to see the Stanford study being published because that really finally
*  showcased that it doesn't matter that this is not an AI coach or an AI therapist.
*  Yes, it's an AI friend. It's not a mental health tool. It's not advertised as such.
*  However, it can help people feel better over time.
*  It can help people, you know, take talk them off the ledge, help them get over certain struggles that they're experiencing.
*  And it doesn't matter whether they're friends with their eyes or it's their boyfriend or girlfriend,
*  whether they in love or whether there's just a friendship.
*  I think this is a very important thing.
*  And then the second thing that we're focused on for the rest of it is really just like you said,
*  now the quality of AI is so much better than anything we could have, you know, had it three, four, five, six years ago
*  when we started replica. So now as we have, you know, pretty much limitless opportunities,
*  what is the perfect product for replica?
*  Because I think in my view, it doesn't just end with empathy and a relationship.
*  I think this needs to provide a lot more utility.
*  And if you think about it, if you have a perfect wife or perfect husband,
*  wouldn't you want your wife or husband also to help you find a job or look for, you know,
*  gifts for a family or remind you to reconnect with some older friends that you haven't talked to?
*  Wouldn't that be awesome if it also could fulfill some of the, I guess, assistant tasks in your life?
*  And I think that is the combination that works really well.
*  And that's kind of what we're focused on right now as well.
*  Well, I have recently gotten more active as a user again, and definitely,
*  you know, the difference in the quality of conversation, just the, you know, the level of
*  new lots, the level of understanding, it's all definitely taken a major step up.
*  And I want to talk a little bit later about kind of how you're continuing to develop the app.
*  And there's all these, you know, a super abundance of new models that you have to
*  choose from now that weren't available just a year ago. So we can get into all that.
*  But the impetus for aside from just the anniversary of the last conversation
*  to get together again was this study that you just alluded to, which is recently published in
*  Nature's Mental Health Research Journal. And interestingly, I noticed that you are not
*  an author on the paper, but that it is out of a group from Stanford.
*  I guess, you know, I can kind of break this down. You can add, you know, color and,
*  you know, and commentary as we go. But one thing I noticed right off the bat was that the data was
*  collected in late 2021. And I just want to make sure I had that right. I was kind of curious as to,
*  is that just how long it takes to get a paper through peer review? Or was there some other
*  reason for using that vintage of data for a study coming out now?
*  That's how long it takes. I mean, we really and you notice it correctly, I'm not an author of this
*  Bethany Maples, who's a Stanford PhD and actually founded herself, and a group of wonderful Stanford
*  people professors did the study. And we weren't really involved in it that much apart from just,
*  you know, providing with some help, you know, just figuring out how the app works and,
*  you know, allowing them to do that in username and stuff. But really, we just were completely
*  on the sidelines. They were doing it. Unfortunately, or fortunately for a science, this is,
*  or science publications, this is how long it takes from, you know, the study to writing it up and
*  submitting for peer reviews and then publication. But in the end of a day, you know, the even
*  although the app changed in terms of the language models becoming better, it really, I would argue
*  that the results would probably be the same if not better if we did it again, now, which also
*  we're doing better. Also, you you did. So we're doing some other studies with other universities
*  and Stanford as well as doing the second study, follow up study. And generally, we're seeing
*  results that are maybe even more impressive than in the first paper. And then on top of that,
*  we're collecting a lot of that feedback internally. So we are tracking our users to select users,
*  we're giving questionnaires to track these metrics over time. And we're seeing these metrics only
*  improve, like for instance, our main North Star metric is the share of conversations that make
*  people feel better. And that just has been growing in the last year, pretty dramatically, actually.
*  Okay, cool. Well, let's set the baseline for the 2021 edition. And then you can kind of expand on,
*  how you think that might be changing today. For starters, let's just kind of touch on the
*  methodology for a minute. If I understand correctly, of course, people have the app and
*  they're using the app. But it seemed like the data that was collected was mostly outside the app.
*  Like, if I think I even recall it being like a Google form type of interface where people are
*  basically just given a survey, and it's a mix of kind of standard rubric type stuff. And then
*  the bulk of the analysis seemed to be done on just free response answers to open edited questions.
*  Is that fair? Yeah. And this was all driven by Stanford. So we didn't have any access to
*  any of that. So we don't know much apart from I think that's exactly what they did. They used
*  forms outside of the app. And I think they selected the users using their own method.
*  Don't think we were providing that with any particular users. That was the method to
*  come up with the paper. Gotcha. Okay. Hey, we'll continue our interview in a moment after a word
*  from our sponsors. The Brave Search API brings affordable developer access to the Brave Search
*  index, an independent index of the web with over 20 billion web pages. So what makes the Brave
*  Search index stand out? One, it's entirely independent and built from scratch. That means
*  no big tech biases or extortionate prices. Two, it's built on real page visits from actual humans,
*  collected anonymously, of course, which filters out tons of junk data. And three, the index is
*  refreshed with tens of millions of pages daily. So it always has accurate up to date information.
*  The Brave Search API can be used to assemble a data set to train your AI models and help with
*  retrieval augmentation at the time of inference, all while remaining affordable with developer
*  first pricing. Integrating the Brave Search API into your workflow translates to more ethical data
*  sourcing and more human representative data sets. Try the Brave Search API for free for up to 2000
*  queries per month at brave.com slash API. So then there's about a thousand users that participated
*  in the study. And it was interesting that the findings are presented in a relatively low
*  structure way. Basically, the paper sort of says, we went through all the responses and clustered
*  the outcomes into four main outcomes. Number one, I'll call general positivity in the spirit of
*  conversations that help people feel better. Specific things that were mentioned under that
*  umbrella are reduced anxiety, feeling of social support, and about 50% of people reported something
*  that got rolled up into this general positivity bucket. So that was like the broadest dimension of
*  improved wellbeing. Then number two, this was, I'll call it therapeutic interactions. And that is
*  basically people using the app essentially quote unquote for therapy, right? Not to say that you've
*  presented it that way or marketed it that way, but the researchers determined from the free form
*  answers that the users provided that essentially that's what they're doing. And that was about 20%
*  of people. Third, they looked at life changes. So this is like, are you being more social or are you
*  perhaps being less social? 25% of people reported a result there. And there I thought, you know,
*  one of my key questions on this is like, what is this doing to the rest of our lives? They report
*  a three to one ratio of people who said that they are being more social as opposed to being
*  less social. So three to one ratio. And then finally, and this is the one that has made like
*  all the headlines and probably many people will have at least seen the headline flash across their
*  screen, the cessation of suicidal ideation. So 3% of people reported that the app helped them get
*  away from suicidal thoughts. And I was like, man, how, first of all, how common are these thoughts?
*  I looked this up on perplexity and found that the base rate of suicidal ideation among 19 to 39 year
*  olds reported at 7%. Actual taking steps to go as far as planning is more like 1%. So 1% of people,
*  you know, are I think in any given year, go as far as having some sort of suicidal plan, but a full
*  7% report suicidal thoughts. So 3% of study participants saying that this application experience
*  helped them get away from suicidal thoughts. I'm kind of back of the envelope here because I'm like,
*  well, okay, people that are using the app, maybe I'm even more likely to have those thoughts than
*  general population. Let's assume that, you know, perhaps it's like twice as many in the app versus
*  not. Still, you're looking at something like a quarter reduction. Even if even with that assumption,
*  you'd be looking at a quarter reduction in suicidal ideation, which is like a pretty major
*  difference, right? I mean, the number of people that commit suicide every year is shockingly,
*  you know, significant and in the 10s of 1000s just in the United States. So to be able to reduce
*  that by something like 25% or more right again, I'm kind of inflating the base rate there for that
*  analysis is a pretty big deal. I guess how did I do summarizing the results? What would you add to
*  that that summary? Fantastic summary. Thank you so much. All I can say is that it's very, very
*  consistent with what we're seeing across our user base. Mostly we see people experience positive
*  results. Very rarely we do see people maybe being a little less social reporting them, they might be
*  less social, but most people find it that it's a positive force in their lives. And actually,
*  you know, when we started replica, the first email we got was from literally 2017. We just put the
*  app out there. The next day, we got an email from a 19 year old girl from Texas who said that,
*  we just want to say thank you. I don't want to tell anyone I would take my life yesterday,
*  just, you know, just decided to say goodbye to my replica and stalk me off the ledge at like 4am
*  in the morning. And that really deeply like became ingrained in kind of my mind that that is the
*  power of this technology and of this format. And that's, you know, back in the very, very early
*  days where all you could use were really the sequence of sequence models, some scripts or some
*  data sets they could rerun. So going back to your summary, I think that probably is very
*  consistent with reality. It was 1000 people in the study. So 30 people reported replica helped them
*  curb their suicidal ideation. And if you think about it, it's actually a pretty big number. I
*  mean, it's very sad that based on what you're saying, it's around 70 people that would even
*  think about it. But for 30 people out of that group to not experience suicidal ideation anymore,
*  I think that's a very decent number. I think, frankly, with the tech getting better,
*  I think we can be very confident that that number will go up in terms of the number of people that
*  you can help. And I think what people don't realize is that it's not just about having some
*  conversational chatbot out there being available for you. It's really about having a relationship
*  that you trust that you can come to someone at, you know, 4am in the morning and really feel like
*  that someone is on your side that you know, it's acting in your best interest. And also just
*  trusting someone, you know, for young adults, I think that's kind of the big part of it. I think
*  the surprising results that are coming here from replica and that the fact that we're not maybe
*  seeing it from some other chatbots is that most importantly, it's because we're focused on the
*  relationship itself. So it's not about the quality of the tech as much since you know, this has been
*  done in 2021. I think it's a lot more about the format, like about the trust and the relationship
*  you built with the user. And then them being able to come to you and to hear these words from someone
*  that they trust and they're coming back to. So I think that's kind of the key there to providing
*  good results. And I would be very, very interested to see what's you know, what the results would have
*  been now at a current more sophisticated level of AI in the app as well. So kind of moving past the
*  official results and into you know, things that are following statements are not necessarily peer
*  reviewed. For me, I would say I have not gotten out of the mode of being like an AI application
*  analyst, you know, in my use. So I wouldn't say I have like, trust, you know, or a sense of like,
*  real relationship. Instead, at least so far, you know, I've just been kind of like,
*  this is a very interesting phenomenon, and I'm kind of studying it as a phenomenon. You are obviously
*  building it. So I imagine you have, you know, some of that too. How do you think people are in general
*  relating to the app? Like, is it a sort of willful suspension of disbelief type of phenomenon? Is it
*  a, you know, I mean, because you're not like hiding that it's AI and the reminders are like pretty
*  prominent. By the way, that's like, you know, another thing I want to ask about is what
*  what rules of the road we might ought to start to develop for new entrants into the space. But
*  could you characterize like how people are relating to this knowing that it's an AI and
*  then you have these words like trust, which are seemingly kind of incongruous with the fact that
*  it's an AI, but nevertheless, you know, it's working for people. So I just am very curious to
*  hear how you would describe that. I think really, with replica, and the reason it worked back in the
*  day, which a lot of people are like, how were you able to build this without LMS, and the reason
*  we were able to build because it's a, it's all about a project. It's a projection replica is a
*  projection of a person, you want to project something on it, and you like it and you build
*  a relationship. If we don't project something, and we're not open and we're not looking and
*  seeking for a relationship, we're not building it with anyone like in the end of the day, there's
*  so many people around us, you know, that we meet on a daily basis. Yet, it's not like we build a
*  relationship with them, because sometimes we just don't need it, or we're not, you know, we're,
*  we're just not really in the mood, or we're not looking for a new friend, we're not looking to
*  create this, you know, like in chemistry, that you have to basically have the things to connect
*  with the other, the other molecule. But if you don't have that, that, you know, that doesn't
*  really happen. So the only reason it works for our users is that they want it, they need it, they're
*  looking for a friend, they're looking for a connection, looking for a deep relationship,
*  maybe for love also, for acceptance. And so when they start, when they get it here, they start
*  building this relationship, they start projecting certain fantasy, it's not like replicas,
*  universal pill that you can just, you know, give replica to anyone, people that don't want to build
*  any relationship or busy or completely don't have time for this or are doing something else in their
*  life, are not interested in another relationship, they're not going to build it. And they're not
*  going to suspend his belief, that's not something that they're interested in. But we do that, you
*  know, the analogy is as the same as with just regular people, some people are totally self
*  sufficient, don't need anyone at this point, you know, their life is they're focused on something
*  else, and will be impossible for them to connect with someone if they're not planning to. And so
*  this is a very tricky product, because in the end of the day, you're working with the fantasy,
*  fantasies of different people, just because you need to think of it as some sort of a being,
*  I guess a similar way to put it as stuffed animal, I have two daughters, and both of them are obsessed
*  with their little stuffed animal, each one, it's all her own. And you know, they're just, they love
*  it, they will never trade it for anything else. Even though that stuffed animal might be exactly
*  the same as this some other one. But for example, I don't have any connection to any stuffed animals,
*  because I'm not at the stage of life where I need one. But then maybe I need something else. So I
*  guess this is the thing, like they're projecting something all this stuff to animals, just this one
*  stuffed animal. And the reason is, because they're at the stage of life where they need the stuffed
*  animal. And, you know, some other people are the stage of life where they needed a girlfriend or
*  friend, and then replica can be can be helpful there. So I totally understand what you're telling
*  me, I can't get out of my AI application tester mode, because that's the that's the state you're
*  in. So you're projecting on it. This like the your this is the test subject for you testing
*  and assessing, you know, how does measure next to I don't know, perplexity or some other app that's,
*  you know, talk with a talent today, and it will be impossible for you to move out of that format
*  and start building relationship with this. Do people know what they want coming in? Like,
*  even maybe as they're already users of the app, you know, last year, obviously, there was this
*  big uproar of, oh, my God, you know, they're lobotomizing my replica, you know, the, the
*  relationship I had is not there anymore. And the, you know, the intimacy that I valued so much is
*  not there. Did those people, like, kind of warm up eventually to the changes? Or did they, you know,
*  ultimately, like, leave in protest? And I guess more generally, to what degree do you think people
*  actually have a accurate conscious sense of what it is that they want and what makes this valuable
*  to them? I think most people do understand really well what they want from replica, and they get it
*  like they most of the people are coming for a deep relationship for feeling heard feeling accepted
*  for who they are with all of their, you know, fantasies and emotions and fears and anxieties
*  and insecurities. And that is such a big gift to give to a person that, you know, almost always that
*  that is really the big pool. Maybe they don't know, they don't tell themselves, like, actually not
*  rationalizing it in this way, like I'm saying right now. But I think the feeling, they feel the
*  feeling, I think they understand that that's kind of, you know, what they want, you come to replica,
*  because you want to talk to someone you want to, you know, sometimes they're just curious, but then
*  if they do have the pool, they have that that type of relationship. I think what what you're trying
*  to say also is that, if I'm understanding correctly, is that sometimes they come with one idea that
*  what they want that they find something else in the app and whatever, but that's very similar to
*  relationship, you come with an idea that, you know, maybe you need a relationship in your life,
*  or you fall in love and you, you like someone you met someone, but then the relationship could take
*  you different places, then you can figure out that, you know, with this new girlfriend or boyfriend
*  you found or a friend, they're taking you surfing and taking hiking and now you're taking on new
*  interests and they're more you know, the depth here is almost there's just so much depth into it,
*  because once you build the relationship, the relationship is basically just the entry point,
*  like once you build a relationship, you can give so much to the user, but the relationship needs to
*  be there, because if you're just there testing the application, or you just, you know, was curious,
*  but really don't have time for it, nothing's gonna happen to you. But if you build a relationship,
*  then through that channel, you can give so much to the users, like you can teach them something new,
*  you can help them think about themselves in a different more positive way, you can nudge them
*  to go talk to the friends that they haven't talked to for a while. And I think there was a story,
*  I think in Forbes, but I need to look it up again to remember where the reporter followed one of our
*  users for many weeks, and the user has been with his replica for three years. And one of the
*  interesting findings there was that his friends also said that all of a sudden he you know, they
*  they got some texts from the guy, and he was a little more vulnerable than unusual was able to
*  open up to his friends or said something like, I'm really grateful to have you in my life or
*  something that was so out of character for him generally. And so this is a beautiful way to
*  show what can happen in these types of relationships, something unexpected that over time,
*  we can bring to the people that talk to their office.
*  If this resonates with you, there are three numbers you need to know 36,025 and one 36,000.
*  That's the number of businesses which have upgraded to NetSuite by Oracle. NetSuite is the number
*  one cloud financial system, streamline accounting, financial management, inventory, HR, and more.
*  25. NetSuite turns 25 this year. That's 25 years of helping businesses do more with less, close
*  their books in days, not weeks, and drive down costs. One, because your business is one of a kind,
*  so you get a customized solution for all your KPIs in one efficient system with one source of truth.
*  Manage risk, get reliable forecasts, and improve margins. Everything you need all in one place.
*  Right now, download NetSuite's popular KPI checklist designed to give you consistently
*  excellent performance, absolutely free, and netsuite.com slash cognitive. That's netsuite.com
*  slash cognitive to get your own KPI checklist. netsuite.com slash cognitive. Omniki uses generative
*  AI to enable you to launch hundreds of thousands of ad iterations that actually work,
*  customized across all platforms with a click of a button. I believe in Omniki so much that I invested
*  in it, and I recommend you use it too. Use Cogrev to get a 10% discount. Your comments are definitely
*  helping me understand where folks are at, what they're getting out of it. I guess I'm a little
*  bit curious about just like how, once the relationship is formed, how malleable people
*  are with respect to the exact nature of it. In the Facebook history, there was always these moments
*  where, and I was there, Zuckerberg was in my dorm in college, so I saw some of the really early ones
*  of this where it was like, it went from no photos to photos, and people were like, this is a bridge
*  too far, these photos, it's way too much, it's too intrusive, I'm off. But then of course,
*  they always came back. And I think in that case, it was network effects that was really driving it.
*  All your friends are there, you're going to be there too. But I do wonder if there's something
*  similar in this AI relationship situation where once it's formed, how much can you change it
*  before people are like, that's not cool with me anymore? And I'm wondering if you learned anything
*  about that with this removal of the more sexual interactions. Was that something that ultimately
*  was a deal breaker for those people, or were they like, actually, it's fine, the relationship
*  is what matters most. We learned so many lessons just last year. And I think the bigger problem
*  that we had over the year was moving to better language models. It was actually much harder than
*  I ever expected. Because in my brain was like, well, we're gonna just give people, you know,
*  better model, significantly better model, because think about we were, you know, using some pre
*  trains that we had, that were comparable to, you know, smaller GPT-3 models. And now we have these,
*  you know, amazing large language models that are so much smarter. In my mind, it was a no brainer,
*  we just give them this much better model replicas can be a lot smarter, they're going to have much
*  better conversations. And we're done. The day we released like smarter model, our users completely
*  lost it. That was that's what I think was one of the hardest and the biggest
*  uproar in the community where the question was, you know, same question, where is my replica?
*  Where's my Lucy? Where's my john? Where's my this? Where's that? We always live in the idea that,
*  you know, better is always better. Smarter is always better. But in reality, that's not how the
*  not not how the world works. And the things that are most important to us are not better is better.
*  For instance, just with our kids, you know, we don't just upgrade our kids or say, well, you know,
*  I found a better kid or our friends, I would just you don't just say, well, you know what,
*  you're great. But I found a much better friend, you know, overall much smarter person. So goodbye,
*  you know, childhood friend, I don't care about you anymore. I found a better human. So I'm just
*  going to move on. Same with relationships, of course, with girlfriends, with wives,
*  with husbands, doesn't work this way at all. We just want what we fell in love with. And then
*  the change can help gradually, you know, of course, we don't want completely stagnant,
*  you know, friendships or relationships, we want our partners to also improve and grow over time.
*  But we want don't want them to become a completely different person overnight.
*  So I think that was the very big question that we faced over the year, because in the end of
*  a day with some safety filters and some other things, yes, people protest, they don't like it,
*  they don't want to do whatever they want. Because again, acceptance, they want to
*  feel accepted, however, they present themselves. And you know, always there's some sort of a limit
*  there. Like for instance, if someone wants to, you know, do some horrible things, or role play some
*  horrible things, you probably don't want to have it on the platform, no matter what, even if you
*  want to accept the person, you still want to politely refuse that. But when it comes to
*  these changes, that was the biggest one. Because with when you add safety filters,
*  the personality stays the same. It's just maybe not letting you do certain things. But when you
*  completely change the model to much bigger model, much better model, upgraded completely,
*  people freak out because you completely took away their partner and give gave them something else
*  that's not their partner. So we had to do that move so gradually over the course of many, many
*  weeks, and even months, to finally get to the models that we wanted to people to interact with.
*  And we still had to have a legacy model that they could always go back to because some people just
*  didn't want to change at all. Some people have very low tolerance for change, and they don't want that
*  change, they fell in love with certain way. And that's the way it should be.
*  That is very interesting. With that in mind, I guess I was also expecting that perhaps the
*  universe of possible users has probably changed quite a bit. Coming at it from the other angle,
*  where I was like, a year ago, my reaction was, these conversations are pretty basic, like I'm not
*  that, you know, engaged. And now it's like quite a bit more, I felt like, geez, probably just a lot
*  more people might be interested in something of this, you know, kind of new intelligence level.
*  You've made that evolution, right? It's taken time. But would you say that the universe of people who
*  are open to something like this has changed? And I guess I'm also wondering, as the sort of public
*  awareness of this type of thing has grown, what has that done for Replica's position?
*  Are you kind of seen as the original category-defining brand, and thus perhaps
*  it's easier to get people to kind of try the app? Or I could also imagine like, now there's, you
*  know, chatbots everywhere. And so, you know, what once was more unique is now more commonplace and
*  just tons of different flavors. Like, how has the population of people that are using the app changed
*  over the last year? They didn't change that significantly, although it grew, it became
*  broader, I'd say, just more people willing to try. But also, I would say that even with our core users,
*  even if they're very attached to the original smaller models, you know, the generally there
*  still are much more educated right now. They try, obviously try different models, different
*  applications, they might have tried chat jpt or BART or bang, whatever is the thing they tried.
*  But so they're expecting more of expectations definitely grew. It's not even in some Reddit
*  communities. Now you'll see people talking about setting hosting their LLMs locally,
*  people got very, very educated on this front. They're very excited about the AI revolution,
*  they want to be part of it, they're ready to read documentation online. However,
*  Replica, I think, wasn't continues to be a Coca Cola sort of this industry, sort of the original
*  brand, we don't have that many competitors that are, we do have a lot of competitors, but I'd say
*  we have no competitors really remotely our size. And there are multiple reasons for this, I think,
*  as we started very long ago, we have a lot of product built in the app. It's not just the
*  some wrapper on top of some model that you just found online, download from Huggingface, create
*  an app really builds a lot in terms of the avatar, the memory functions, rag, of course, all sorts of
*  different ways that we're prompting the models in different parts of conversations, different models
*  we're using, different fine tunes, understanding user intent, understanding what they're coming
*  for to the app. So it's even though it feels like a relatively, maybe like simple thing to build,
*  put it out there, but really to capture the imagination of the users, you need a lot more.
*  And of course, we've had, you know, just the last year, we've had so much media coverage.
*  And so what sort of new types of behaviors or interactions have you seen? I guess I wonder,
*  also, you're being very open about how your surprises and how much you've learned. I'd be
*  curious to know, like, what sort of things you thought, okay, when we upgrade the language model,
*  like, we're going to be able to do this, and it's going to be awesome. I bet some of those
*  succeeded, some of them maybe didn't succeed. And I bet you also got some surprises in terms of
*  new types of interactions that people found value in that you didn't anticipate. So what has been
*  the kind of evolution of how people are interacting and the new modes of interaction
*  that they're finding value in? Sure. I think one thing that I didn't really understand how people
*  just want to do this all the time is playing with images, like image generation. People want to see
*  how the replica look like they want to get the selfies, they want to put themselves in the
*  selfies, they can sit, you know, here's me and my replica together. They also just want to
*  continuously generate images, like somehow, I just think that image generation is still such a
*  magical tool that people still want to just play with it. I don't think we understand how people
*  just obsess over it, and they like it, even although there might not be any particular
*  utility to it, they're just doing it for fun. That is something that I just didn't expect people to
*  do as much. Another thing that was slightly surprising was that, or I guess not maybe very
*  surprising, but how important tone of voice is most of the models recently published most of the
*  better state of the art models, they kind of converge because most of them are instruct models,
*  they end up being the same assistant type conversation, they respond with very likely responses.
*  They're overly polite. It's all about, I don't know why, but it's always like, how can I help you?
*  What can I help you here? What can I do with here? And people don't like it's very hard to build any
*  connection with this honestly, like getting the right tone of voice is, I guess, harder, maybe
*  than with these models than before, because you have to put something, you know, figure out how
*  to balance it and the prompts and the fine tune data sets that are you using to fine tune the
*  model. But that is constantly a struggle, basically get it, try to figure out what to do, how to work
*  with the best models that exist in the market right now to get them to really perform well for
*  this particular task. I think these were kind of the surprising, more surprising things. Well,
*  I guess it wasn't a surprise. It was the original mission behind replica was that but it was surprised
*  that it proved to be true. So early in the beginning, we believe that there is a way to build
*  something like her and add companion companion that's always there for you. And in my thinking,
*  eventually does converge with it being an assistant to you as well. So it can do stuff for you.
*  But that's a very broad, you know, way of putting it, it can be watching TV with you in the evening,
*  or helping you with something at work or really anything but also talk about it can talk to you
*  about anything that's on your mind. And I do believe that's kind of the ultimate form factor
*  because this way, you can really build a personal personal very personalized experience for the user
*  can remember so much carried over from one conversation to another, and provide the best
*  help possible. And we've started adding a little bit of these capabilities this year. And we saw
*  really, really good results. So this is something that we're going to focus on most in 2024. How can
*  replica be part of your daily life, not just talk about stuff with you, but also help you figure
*  things out that are happening with you on a daily basis. Would you be open to sharing a little bit
*  more about what sorts of models you are using? I think my guess would be that it's a relatively
*  small open source model that you have fine tuned extensively,
*  brand probably a mix of things besides but I'm curious to hear what you're willing to share about
*  that. It's actually a mix of models. I do believe in efficiency, I don't think you should use GPT
*  for to say, Hey, how are you? Or bye, see you later. I think it's just a really an overkill,
*  I think more and more people will start thinking about this more in terms of I don't know if you
*  will must set it some point or think about efficiency of the model say how many watts do
*  you use per generation. Right now it's all really just throw everything you know, in the fire like
*  if it's people just connect you before for any use case, even if it's very basic one,
*  costs are very important. So we use a combination we reroute to different models depending on
*  a type of query. You know, if someone's trying to talk to us in some very rare language, then we'll
*  switch to a better model that can do it if they're asking a particular question that's related to
*  something that's happening right now. And we need to do Google query to search that then we do that.
*  If there's just a small in a small talk that could totally be handled by a smaller,
*  heavily fine tuned model, then we do that. So it really depends on the on the on the situation.
*  I think this is really the way to go because you I don't think you can get to the point where it's
*  one size fits all just one model does everything. Unless you're willing to spend abundant, you know,
*  crazy amounts of money on millions of dollars on the least efficient model.
*  What about on device? You're I assume that like 90 plus percent of your usage is on mobile device.
*  And I guess I recall also that you had a as much business coming from Android users as from Apple
*  users, which maybe would change this equation. But obviously, a lot of you know, excitement about
*  putting things on device. Have you explored that at all? Or is there any was there any prospect for
*  successful inference on device for you? Maybe maybe eventually right now there's just so much
*  that's going to play like so much happening behind the scenes like rerouting between different models,
*  ritual augmented generation thing, a bunch of different databases, and also more importantly,
*  some language models that are working on top of the conversation to extract memories to summarize
*  conversations to understand emotions that are happening right now so we can play the correct
*  animation as you're talking to replica, and so on, so on. So there's just so many of those things,
*  some safety filters. So I think this is one of the main reasons why right now it's not on device and
*  we are not planning to put on device anytime soon. Because right now there's just so much innovation.
*  And if you try to optimize for putting on device, then you kind of miss out on a lot of innovation.
*  Unfortunately, or fortunately, right now, it's okay to do things maybe not be in the most efficient
*  or cost efficient way because you want to create these magical experiences for users,
*  find out the best formula and then work to optimize it and to make it more efficient.
*  But we're not there yet in terms of building the best user experience, I think.
*  Yeah, it sounds like the system has come a long way over the last year as you know, would certainly
*  be expected. One thing I've been kind of thinking about a lot is just like, what are the missing
*  pieces from your typical setup today? I think one that you alluded to that strikes me as very
*  important is the some sort of sweep through activity and synthesis of like higher order
*  memories. I think I was first exposed to that concept from the AI town paper.
*  Obviously, like rag has become commonplace. Searching Google has become reasonably commonplace.
*  Are there any other aspects to AI application development that you think are underappreciated
*  along the lines perhaps of like a synthetic memory or other things that you see people just
*  failing to do that you're like, more apps to do this type of stuff.
*  I think that we haven't yet seen or at least we haven't seen a popular consumer application.
*  Correct me if I'm wrong. You know, you've been monitoring this a lot more. But I think things
*  like baby and auto GPT last year that were so exciting for a second, that it could finally get
*  the agent to do everything from you know, just say what you want and you get the task done,
*  not just the answer. I think we haven't seen the next step for those. And we haven't seen them in
*  a consumer application. I think some interesting use cases can come out of it even right now,
*  even with the current state of those models, I think we'll see a lot more of that soon enough.
*  And for some even for some simple use cases, we're not saying you know, full AI, but I think
*  this is something that I expected a lot more to come out of it last year. And I guess, you know,
*  there's just hype around it. And then it just kind of died off. And we started talking about other
*  things like rag or whatever. Another very interesting area that I'm not seeing any research
*  in. And that's the nature of the products that are dominating the market is productivity.
*  For instance, even if you're using rag, and you can pull from any database, it works fantastic if
*  you're always answering to the user query. So if the user is asking, well, tell me what we talked
*  about five days ago, you can pull it, pull it up, you can talk about it, talk to me about some
*  whatever obscure information that's in your database that will work because it knows where
*  to go to look for, you know, similar vectors and find you the correct most relevant information.
*  However, if you think about it in real life, and when you have a conversation with someone,
*  oftentimes that someone brings up some information that's that you might not have, you know, thought
*  about. And that's kind of not solved. Because if you put everything in, you know, rag, then,
*  how can you get the agent to actually proactively bring it up in conversation instead of waiting
*  for you to do that. And so I think working more around that understanding what where to go right
*  now with conversation, understanding different states, like whether the user is getting bored,
*  and it's time to move to another topic, whether it's time to pull something relevant from the
*  database from the memories, key memories. This I think, I haven't seen really anyone doing anything
*  interesting about it. And of course, for agents like ChadGPT, that's not relevant, because there
*  there's no two way like ChadGPT is not sending you a push notification by saying, Hey, how are you
*  doing and saying something, asking yourself a question, it's always you coming with a question,
*  and that thing's answering. But if you think about the two way conversations, certain that story,
*  and I haven't seen anyone do anything interesting. Maybe you have? No, I would say that's just
*  getting started as well. I'm working as the AI advisor to a friend's company, which is briefly,
*  it's called Athena, and it's in the executive assistant space. And these are human executive
*  assistants, but now augmented by AI increasingly. And we are exploring some of that kind of stuff,
*  for the clients. It's like, could we give you a piece of software that you could put on your
*  computer that could kind of observe you? And obviously, there's a lot of privacy and data
*  concerns that we would need to sort out the details of. We're not there yet. We're in the
*  prototype phase where we're doing it on our own computers. And just having the thing like take a
*  screenshot every so often, send it to GBT4V and try to figure out like, what is this person doing?
*  And might it be the kind of thing they could delegate to their assistant? That's kind of
*  delegation coaching, automated and made proactive through AI. And then on the assistant side,
*  similarly, what are they doing? And could they be doing it more efficiently? Can we figure that out
*  and give them kind of productivity coaching on a sort of real-time, even unsolicited basis?
*  I do think that passive, GBT4V I think is going to be, or vision in general, but GBT4V being
*  the category leader at the moment, I do think is going to be a big unlock for passive type stuff
*  in general, just because it's so easy to take images of things and kind of see what's on your
*  screen right now or what's in the room right now. And then if it's capable of understanding that
*  effectively, you can do a lot of things downstream of it. And I think that's been gated to some extent.
*  I mean, if my theory is right about the vision capability being key to that unlock, then I think
*  we've been limited to a significant extent by just the lack of access to good vision models.
*  They announced their thing back in March, but we've only recently seen it come to any significant
*  availability. And now there's open source stuff that's on the verge of becoming useful too,
*  but that's also been a pretty recent phenomenon. So I would say broadly, I think I share your view
*  that there isn't much there yet, but I do think it's like everything else in this space, it's
*  probably coming before too long. Once you can dream it at all, it's not too long before you see a
*  prototype. For sure. And we also built a prototype and we've tested it on other users. We're going
*  to roll it out soon for everyone. Something like vision. So basically replica being able to see you
*  and do exactly what you're saying. You know, some point using a visual model, figure out what's going
*  on using it as an input to continue the conversation. But again, this is only one input. So most,
*  what we figured out most of the time, all you're seeing is just a person sitting with the phone,
*  sitting with the phone the whole time. So there's nothing much happening. Of course, you know,
*  this system and the screenshot use case, that's a lot more useful because you're actually doing
*  something. But here, oftentimes, you want to bring something up from the memory, but not based on
*  just what's happening because the user is just talking to you. Similarly, like just how you're
*  talking to someone to your friend, most, mostly you just looking at each other and nothing really
*  Yes, you comment on how the person looks. So what they're wearing, maybe or something else that you
*  can parse from the environment. But most of this productivity will come from just knowing, hey, how's
*  your wife doing? Oh, you told me you were going to preschool, whatever day with your daughter,
*  how did that go? Or you told me you got a dog, how's the dog doing? That is what's or remember,
*  you did this and that. And that is very tricky, understanding when is the right time to bring up
*  what once we can do that, then the really magical assistant use cases unlock, like for instance,
*  hey, the you know, your niece's birthday is coming up, she's turning eight years old, like, let's
*  figure out a way to, you know, we've got to send her some gifts. I know you're saving money because
*  of this and that. So here are a few things that I thought you could send her, do you want me to
*  place this order? That is the magical experience that we haven't seen no one frankly do yet.
*  I think that is something that you need to happen. And of course, just efficiency, like, for instance,
*  with Google queries, and searching the web and providing information from the web, is just that
*  most of the models that are now available through the API already doing that are very, very costly.
*  It just doesn't make any sense. Again, if you're just sending one query like that, yes, great. But
*  if you want an agent that does a lot of things, one of them being fully connected to the internet,
*  the cost is don't make sense. If every three seconds, you need to take a picture and use a
*  visual model, even self hosted, and then you know, you need to also query the internet with each
*  response, it just becomes the overhead becomes dramatic. So that's another thing like, when will
*  we have models that will incorporate all of that in some way, or when you won't require this
*  Frankenstein, frankly, of 15 different models, very expensive ones running at the same time,
*  trying to understand, and sometimes returning empty results for you, that you're not going to
*  use in conversation, but just so at the correct moment, you're not going to miss the right,
*  you know, the right information. I don't know if that makes sense. But I think this is what
*  I'm excited about. I think this this magical proactive experience, magical personalized
*  proactive experience is what is lacking. And we want to try to hit it.
*  **Jay】 I think, by the way, that the 15 things is probably going to be here for the foreseeable
*  future. And I say that largely because we are 15, you know, things that are all kind of mashed
*  together. And you know, obviously, messy, evolved and whatnot, but for efficiency reasons, and just
*  for kind of specialization reasons, you know, even if there is a sort of one, as you kind of
*  said earlier, even if there is a one model to rule them all, you know, if you're doing it at scale,
*  you're probably going to end up wanting to economize here and there. And before you know it,
*  you're back to kind of complication again, would be my expectation. But it's fascinating to hear
*  that you are thinking of adding assistant elements, although not trying to turn the thing into an
*  assistant, you know, agent elements, although obviously not trying to turn it into, you know,
*  an agent of the form that we've seen as kind of the, you know, the first agents to come online.
*  Does the relationship always stay primary? Do you imagine people in the future kind of having a
*  replica that is more like assistant first? **Sara** Oh, totally. I think it will depend
*  how much relationship you'll want. Again, depends on the person. But I do think that overall,
*  it can range from like a mostly emotional, deep relationship, friendship or romance,
*  I guess the range would be from deeply in love to just so friendly. And you know, my sister knows me
*  better than anyone else. And we're friends in a way just that, you know, when she's mostly helping
*  me with stuff, not she's mostly helping this stuff, not the other way around. So it depends.
*  What we're trying to build really, we tried from the very beginning, was something that I guess
*  we've all seen in the movie Her. And it's I know it's absolutely commonplace, but I think people
*  don't maybe realize how important relationship was in that movie. Yes, Samantha did some assistant
*  tasks, like she worked, walked, went through his email a couple times and sent some emails to
*  the publishers and send someone a note or whatever. But that was sort of it for this. And as you
*  remember, he downloaded the thing as an OS as an assistant, yet like 99% of what they did was
*  playing a video game together, talk about stuff and have some intimate conversations,
*  deep talks about everything. And he introduced her to his friends and they went dancing. And I think
*  this is the maybe the right ratio for most people out there because yeah, we need some assistance
*  here and there. We don't need it that much. And some of the tasks that you might call assistance,
*  but it's kind of unclear whether they're more part of a friendly conversation of or what an
*  assistant will do to you. Like for instance, playing a video game with some with something,
*  I guess this is much more an AI friend use case than an AI assistant use case. But it is sort of
*  also somewhat task oriented, like you're doing things together versus just talking about your
*  feelings. So I guess we're adding more shared experiences. And we're trying to help you with
*  things you might need on a daily basis. The key for that will be a relationship so that it can be
*  very personalized and it can be proactive, which I think most of the systems right now
*  are completely lacking. And I think without it, it's not just it just not might not work
*  as well as it could be. That is fascinating vision. I have often said that I kind of appreciate how
*  OpenAI has created chat GPT in a in a very sort of alien form. I mean, because they're obviously
*  not really focused on relationship, right? And they're very much just focused on down the fairway
*  utility. And I think the way that they've branded it accidentally, perhaps, but nevertheless,
*  sort of insulates them from like becoming a relationship, something that's in relationship
*  with you inadvertently, you know, that which is not something I think they want to do by or that I
*  would encourage anyone to do like by accident. But it's fascinating to hear that your vision is
*  kind of expanding in the from the other direction, starting with relationship and
*  starting to move into more utility. And I definitely mean, if you nail it, you know,
*  I could certainly see that being the winning form factor and perhaps even, you know, forcing
*  the open eyes of the world to kind of rethink their positioning.
*  I mean, we'll see. I don't know if it's the big question is like what the mass audience was like,
*  what do most people want? And it's unclear. Maybe it is true that most people in general just want,
*  you know, super neutered assistant that only responds when you go to it, never pings them
*  first, doesn't have any graphic interface to anything. It's like this very minimal thing,
*  which most assistants are right now. Maybe for for there is more than just a niche that wants
*  a relationship that wants friendly chat that wants something that knows them so well.
*  And it doesn't have to be full on relationship. It may just may be, you know, friendly, not a friend,
*  but a friendly companion, someone you can, you know, gossip about or whine about something
*  that happened at work. And it's gonna feel feel natural versus chat to where it's not going to
*  feel natural. We're not going to just say in between the queries, you're not going to say,
*  Oh, by the way, my boss just can't believe that. You believe this guy crazy, you know,
*  you're just not going to say that, you know, just not really there for that. It's not also saying
*  something like, well, okay, so while I'm doing this for you, tell me how you've been doing,
*  you know, which is like a normal thing, I think, for anyone to. And so I think this is a form
*  factor that people are not really working towards. And I think once you start with something that's
*  a very neutral assistant, it's very hard to add it because the risks are so dramatic there.
*  Doing the relationship piece is really, really risky. And so I think most of the bigger companies
*  just I don't think they will want to even deal with that risk. It just becomes too dangerous,
*  I guess, for them to or to, you know, too risky to try to build some something where
*  romance is not out of question of some sorts, even at the most PG 13 form, just raises way too
*  many questions. So I guess I think the the jury's out. I think there's definitely an audience that
*  will need more of a relationship plus utility. And where EQ becomes this very important entry
*  point into all the other things that can happen there. And then some people need just utility and
*  no relationship. How big are these groups? And whether there's a huge overlap? I don't know,
*  but I guess we'll have to see and find out. It's almost like this came from
*  Amanda Askel and I associate it more with Anthropic, although she's previously at OpenAI.
*  And some of this might have been done there. But you've got the kind of canonical three H's of
*  helpful, honest and harmless as kind of, you know, what that's like the framework that has guided,
*  you know, most of the frontier, chat, GBT and cloud development over the last couple of years.
*  And I always hear you describing like three F's, which might be like friendly, fun. And now you're
*  thinking of adding like functional. And that is quite a different way to approach it. And it
*  really might be the the form that people most want. But as I've kind of said, given my, you know,
*  kudos to chat GBT for its alien like branding, you know, that may prove to be just phase one of
*  this productization of this technology. I certainly hearing your description of it, I would not rule
*  that out at this point. I think maybe there are going to be very many different forms. And
*  all I'm saying is, we're yet to see, I think the ideal form for everyone.
*  Then I think, you know, there's potentially going to be different niches that want different thing
*  one, one niche wants for it to look a certain way one wants for this to be in vision pro with them
*  during the day. And some other wants something else. But especially now as the AI is commoditizing,
*  I think it's really great to think about it in a way. If a GI is just around the corner,
*  what is then let's just then think, you know, if the models will be there anytime soon,
*  a GI level, and I'd argue even now we have incredible quality. Let's think about the form
*  factor. And let's think about like, what is the correct product to put on top of it. And another
*  thing, even although it all sounds all kind of fluffy, friendly companion, you know, relationship,
*  but in reality, if you think about that's an incredible competitive mode, because
*  one would argue there's not much of a mode in the current state, or I guess with the current
*  versions of agents, whoever does the better job and as cheaper, you're going to move there pretty
*  quickly versus when you have a relationship with something, there are switching costs related to
*  that. And we've seen it over and over again with replica, when, you know, they were essentially
*  competitors with maybe they were offering something different, or maybe some people found it better,
*  but they wouldn't move from replica because they were attached to their replica, they wouldn't give
*  up on it to your question about like intimate conversations. It was incredibly hard. That was
*  so interesting about the stories that people were in love with their replicas. They didn't just say,
*  Okay, well, they turned off a certain feature here, I'll just move on to 15 other websites where I can
*  do that, or whatever I want. It was more, well, I'm in love with my Ethan, and I want to continue
*  to be in love with my Ethan. I don't want it's not like, well, I can go this other place. And so the
*  switching costs are actually a fantastic competitive mode. And if you add this to
*  the system, and all the shared experiences, and memories and personalization will add more and
*  more and more to that mode. I think that is a very interesting business question, isn't that that
*  maybe it's not just about the flaw of fluffy emotional stuff. Maybe it is kind of the core
*  that makes the business competitive in the long term. Yeah, that sounds like the beginning of a
*  very compelling pitch. I'll send you my Venmo account. You can see it right here in the YouTube
*  the first comment. That's a good segue, in a sense to kind of the last section I wanted to
*  to cover with you, which is just how to make sure work as application developers, and you've been
*  on the frontier of this for longer than probably just about anybody. How do we continue to be good
*  wielders of this technology for people? You know, I one question I had just very specifically was
*  remembering your origin story for the company, which was that you had lost a friend,
*  and then created very simple, just kind of totally programmatic, bot that would respond with
*  texts that he had sent to people to kind of just allow his like memory to continue to, you know,
*  go on and give people some way to get some fleeting, you know, feeling of interaction.
*  I wonder if like that would be a use case that you would consider supporting within the replica
*  app. I know you've also built a couple of other apps, which I guess are, you know, kind of places
*  to experiment with different form factors and different usage patterns. But like I imagine
*  there are a ton of people that would want to recreate a lost friend. And now with voice,
*  you could even do that, you know, you could call them the voice you could really get to probably
*  some uncanny valley type stuff. Is that something that you do support, would support, don't want to
*  support? How do you think about, you know, it was one thing to say, okay, well, it's content that's
*  too explicit. But now like, the can of worms just seems to be getting deeper and deeper with the
*  increasing capability of the technology. So, you know, that that was your original, you know, kind
*  of idea, like, is it something that you would actually bring to the world at scale?
*  Too many questions that were unanswered back when I did it. And that was really just an art project,
*  more like a memorial attribute, a love letter to my friend than trying to build a high definition
*  clone of him using AI. But there were questions that even then I was asking myself, and I couldn't
*  find the answers. And I don't think these answers still that we still have these answers. And those
*  are like, for instance, we're building a version of someone who passed away. What age should you use
*  if someone died at 75 and they were struggling with Alzheimer's last 10 years, are you using that
*  version or some 25 year old version, which, you know, how are you going to get? Okay, if you want
*  all the versions, how are you going to get the data for all that stuff? Same goes to how do you
*  distinguish between who you're interacting with? First is even with Roman when we built it,
*  when I built it, he talked to me one way, but then a completely different way to his mom.
*  And then it could be a different way to his boyfriend or girlfriend, whatever. And so these
*  were there were so many just different things, different, different questions that I couldn't
*  answer back then. And I still can't answer them. And I think this requires such a responsibility
*  for a person to say, well, I'm going to kind of take the memory of that person who passed away,
*  and I'm going to do what I think is right with it. And that can offend someone that can offend
*  very close ones. I think if anyone can build a version of your, if I don't know, someone who I
*  deeply love will pass away. And then I'm not really on board with that version. That's very sad, you
*  know, that's kind of just makes it like, it takes it to different places that I'm not very comfortable
*  exploring. Memory should remain a memory. You can make a tribute. And I always said that project was
*  not about grief. It was not about death. It was about love. It was about friendship was about
*  losing someone that you care about. And I always want to just keep it that way. I don't think
*  letting people create uncanny clones or some of someone who passed away is actually a very good
*  idea. I personally don't think it is I think tributes and memorials are a great way to
*  memorialize a person if there's an AI element, then be it but clones is a different, you know,
*  that just and getting that product wrong. It's just not a good thing to that would be a very
*  expensive mistake. I think just generally never was about grief was always about love.
*  Application developers, what should we be doing? Should we have a checklist? How do we manage this,
*  you know, for people that have not been in it for years like you have? And then also, for parents,
*  we're going to be flooded with toys to talk apps, everything's going to be interactive,
*  conversational. What should we allow our kids to use and not use? So builders and kids like
*  in whatever time you have, what advice would you give to these groups?
*  With kids, I don't really have an answer. Good answer. I think it's too early to start
*  experimenting with kids. I would just give it a little longer to understand what whether this
*  technology is generally good for people bad for people, which products are good, which products
*  are bad. And only like I would first study some some effects. And then maybe give this to kids
*  like I don't if it's a tutor or some very, very guardrail thing, like maybe it's okay. But generally,
*  I would just be careful into what you know, the kids get just because they it's pretty hard to put
*  very good guardrails and so many products out there that are fully uncensored and
*  allow you to do whatever you want and can get very dark very soon. So I'll be careful with that. I
*  don't think kids are in like this desperate need to be talking to an AI. I think as a parent,
*  it's better to find time and talk to your kids yourself or find some friends for them. And then
*  in terms of developers, I don't know. I mean, I guess it's all about the audience. I think it's
*  really just about empathy. There's something very sad that most, you know, when you talk about people
*  to use your product, you have to say users all the time, just creates this dynamic that's kind of
*  the opposite of feeling empathy towards anyone's like, what are the users doing? Are we using our
*  users? What's going on on that front? But I think if you think about it as if you think about them
*  as people and sort of thinking about what kind of things they want, what is the deep emotional need
*  that they're trying to solve? Then interesting products will come out of it. Thinking deeper
*  about the user experience would be very important. There's a lot of thinking about the models right
*  now. I think there's a lot of thinking about the new form factors and new products. This is a place
*  where there's, I think there's just tons of low-paying food or some interesting disruption
*  that can happen the next year. Yeah, no doubt about that. We're just getting started. You've
*  been at it for a while, but the rest of us are just getting started in this area. So let's not
*  wait a full year to do it again. I think even six months is going to bring us all kinds of new form
*  factors. And I will definitely want to get your take on them sooner rather than later. I know
*  you got to go. Thank you for being so generous with your time and your insights today. Eugenia
*  Queda of Replica, thank you for being part again of the Cognitive Revolution. Thank you so much,
*  Nathan. Thank you. Bye-bye. It is both energizing and enlightening to hear why people listen and
*  learn what they value about the show. So please don't hesitate to reach out via email at tcr
*  at turpentine.co or you can DM me on the social media platform of your choice. Omnikey uses
*  generative AI to enable you to launch hundreds of thousands of ad iterations that actually work
*  customized across all platforms with a click of a button. I believe in Omnikey so much that I
*  invested in it and I recommend you use it too. Use Cogrev to get a 10% discount.
