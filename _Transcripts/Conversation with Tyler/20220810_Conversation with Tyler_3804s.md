---
Date Generated: September 17, 2024
Transcription Model: whisper medium 20231117
Length: 3804s
Video Keywords: ['Conversations with Tyler', 'Tyler Cowen', 'Will MacAskill', 'Effective Altruism', 'Philosophy', 'Economics', 'Policy']
Video Views: 3440
Video Rating: None
Video Description: When Tyler is reviewing grants for Emergent Ventures, he is struck by how the ideas of effective altruism have so clearly influenced many of the smartest applicants, particularly the younger ones. And William MacAskill, whom Tyler considers one of the world’s most influential philosophers, is a leading light of the community.

William joined Tyler to discuss why the movement has gained so much traction and more, including his favorite inefficient charity, what form of utilitarianism should apply to the care of animals, the limits of expected value, whether effective altruists should be anti-abortion, whether he would side with aliens over humans, whether he should give up having kids, why donating to a university isn’t so bad, whether we are living in “hingey” times, why buildering is overrated, the sociology of the effective altruism movement, why cultural innovation matters, and whether starting a new university might be next on his slate.

Recorded July 7th, 2022

Transcript and links: https://conversationswithtyler.com/episodes/william-macaskill/

Will's latest book: https://whatweowethefuture.com/

Stay connected:
Follow us on Twitter, IG, and Facebook: @cowenconvos
https://www.twitter.com/cowenconvos
https://www.facebook.com/cowenconvos
https://www.instagram.com/cowenconvos

https://conversationswithtyler.com

https://mercatus.org
---

# William MacAskill on Effective Altruism, Moral Progress, and Cultural Innovation  Convos with Tyler
**Conversation with Tyler:** [August 10, 2022](https://www.youtube.com/watch?v=-tdt-qca4RM)
*  Hello, everyone, and welcome back to Conversations with Tyler.
*  Today I'm here with Will MacAskill, who is one of the most important and influential
*  philosophers period.
*  And August 16th is the publication date of his new and excellent book, What We Owe the
*  Future.
*  Will, most of all, is known for being a leader, perhaps the intellectual leader, of the Effect
*  of Altruism movement.
*  Will, welcome.
*  Thanks so much for having me on.
*  Of all the inefficient things, which is the one you love most?
*  Of all the inefficient things?
*  I mean, there are some amazing examples of inefficient charities that I love in the
*  sense that they give me pleasure to think about.
*  One of my favorites is a charity called Scotts Care, which was set up in the early 17th century.
*  And it's called the Charity for Scots in London.
*  And it's dedicated to help Scottish people in poverty in London.
*  And you might think that's a pretty strange aim, but it made more sense when the charity
*  was set up in the early 17th century.
*  There was just recently the personal union of England and Scotland.
*  There was migration from Scotland to London.
*  So Scottish people were kind of poor immigrants.
*  And so it made sense for there to be a charity.
*  400 years on, perhaps doesn't make quite as much sense.
*  And I like it partly, especially given that London is pretty affluent, especially compared
*  to many areas of Scotland.
*  And so I like it as an example of what gets called the dead hand problem in philanthropy,
*  where the founding of non-profits can have a very specific mission.
*  And that mission can become increasingly absurd over time.
*  And I think it's a good and heartening lesson for people who are trying to do good looking
*  into the future, too, that you want to have aims that can be sufficiently flexible that,
*  as in the environment where things change, they still keep making sense.
*  Does liking the inefficient mean that you are a pluralist and not a utilitarian?
*  Well I'm not.
*  I think I'm neither a pluralist nor a utilitarian.
*  A pluralist is someone who thinks that there are many sources of moral reasons.
*  A utilitarian thinks there's only one.
*  I say I'm not a utilitarian because though it's the view I'm most inclined to argue
*  for in seminar rooms, because I think it's most underappreciated by the academy, I think
*  we should have some degree of belief in a variety of moral views and take a compromise
*  between them.
*  But that is pluralism, right?
*  No pluralism would be saying there is one true moral view, and that view says there
*  are multiple competing sources of reasons.
*  Whereas a true moral view can be a probabilistic assemblage of the different things you think
*  might matter, right?
*  So if you go just a little bit more meta with pluralism, it encompasses what you think.
*  Yes, so that is my view, and it ends up looking very similar to what is known as pluralism,
*  because I end up paying attention to different sorts of moral reasons.
*  But I think that the actual moral truth might be quite simple, whereas the pluralist in
*  terms of first order model theorizing rather than the kind of meta theorizing that I'm
*  proposing says that, you know, model reality might be really very complex or messy.
*  If we're assessing the well-being of non-human animals, should we use preference utilitarianism
*  or hedonistic utilitarianism?
*  Because it will make a big difference.
*  We're not sure all these animals are happy, right?
*  They may live lives of terror, but we're pretty sure they want to stay alive.
*  It makes a huge difference.
*  I mean, I think the arguments for hedonism as a theory of well-being, where that's saying
*  that well-being consists only in conscious experiences.
*  Positive ones contribute positively, negative conscious experiences contribute negatively.
*  I think the arguments for that as a theory of well-being and the theory of what's good
*  are very strong.
*  And it does mean that when you look to the animal lives of animals in the wild, I mean,
*  My view is just very non-obvious whether those lives are good or not.
*  That's me being a little bit more optimistic than other people that have looked into this,
*  but the optimism is mainly drawing from just lack of, I think we know very little about
*  the conscious lives of fish, let alone invertebrates.
*  But yes, if you have a preference satisfaction view, then I think the world looks a lot better
*  because beings in general want to keep living.
*  And that's true when we look to the future as well, I think.
*  If you assess how good is the future going to be on a hedonist view, well, maybe it's
*  quite fragile.
*  You could imagine lots of future ways that civilization could go where they just don't
*  care about consciousness at all, or perhaps the beings that rule are not conscious.
*  But probably beings in the future will have preferences and those preferences will be
*  being satisfied.
*  And so in general, the kind of moral reality looks a lot more rosy, I think, if you're
*  a preference satisfactionist.
*  But it's possible, say, in your view, that human beings should spend a lot of their time
*  in resources going around destroying nature, since it might have negative net expected
*  utility value.
*  I think it's a possible implication.
*  I think it'd be very unlikely to be the best thing we could be doing because...
*  But there's a lot of nature.
*  We have very effective bombs, weapons.
*  We could develop animal killing weapons if we set our minds to it, right?
*  There's a lot of nature, but there's far more future.
*  And so if we're willing to take philosophical reasoning far enough that where we'd be seriously
*  considering removing nature, then you should be taking much more seriously the fact that
*  we can have this enormous impact over our long-run future.
*  And I will caveat and say, I really don't know whether animals in the wild have lives
*  that are good or bad.
*  It gets really determined.
*  When you look at the world as a whole, it gets incredibly determined by where's the
*  cutoff for conscious experiences?
*  Because is it ants conscious?
*  They have an awful lot of the total neuron count of animals in the wild.
*  And if they are, do they have lives that are good or not?
*  I'm like, I have no idea.
*  But I worry a bit this is verging into the absurd.
*  And I'm aware that word is a bit question begging.
*  But if we think about the individual level, like what do you well value?
*  You value in part the inefficient.
*  It's very hard to give people just pure utilitarian advice because they're necessarily partial.
*  At the big macro level, like the whole world of nature versus humans, ethics of the infinite
*  and so on, it also seems to me utilitarianism doesn't perform that well.
*  So the utilitarian part of our calculations, isn't that like only a mid-scale theory?
*  So you can ask us, right, control work or tariffs good.
*  Utilitarianism is fine there.
*  But otherwise, it just doesn't make sense.
*  OK.
*  So there is what we might call the train to crazy town.
*  So we have these all of these starting moral intuitions.
*  What I see is the project of moral philosophy is reconciling them using theory and careful
*  reasoning to make kind of moral progress, which often involves creating simpler and
*  explanatory powerful theories that move away from your common sense intuitions.
*  And then the question is, how far are we willing to move?
*  Very difficult kind of methodological question.
*  You brought up infinite ethics.
*  And that is something where I certainly in practice do not bite that bullet or follow
*  that implication where for the listeners, the argument is that, OK, the utilitarian
*  wants to maximize the good, understanding that it's total well-being.
*  Now perhaps in my book, I argue there's enormous amounts of value at stake when we consider
*  the long term.
*  So we should reduce the risk of extinction and promote good values so that we make the
*  most of all the value in the long term.
*  However, someone could respond, reply and say, well, that's just piddling finite amounts
*  of value.
*  What about the possibility of creating infinite amounts of value?
*  Because religious traditions say that one can create infinite amounts of value, that
*  heaven is infinitely good, and you're a good Bayesian.
*  You don't have credence zero in the idea of there being a god that could produce infinite
*  amounts of value.
*  I would say, no, I don't.
*  And if so, well, tiny, even if I put it, what an a trillion that there's such a god, multiply
*  one in a trillion by infinite positive value, then that overall expectation is infinitely
*  great and that's what we should be focusing on.
*  And I will acknowledge I get off the train to crazy town before I'm at that point.
*  But why not get off the train a bit earlier and just say, well, the utilitarian part of
*  our calculations, it's embedded within a particular social context, like how do we
*  arrange certain affairs of society?
*  But if you try to shrink it down to too small, how should you live your life or to too large?
*  How do we deal with infinite ethics and all of nature?
*  That it just doesn't work and it has to stay embedded in this context.
*  Universal domain as an assumption doesn't really work anywhere.
*  So why should it work for the utilitarian part of our ethics?
*  Get off the train and stop too, not stop 17.
*  So I agree there's a hard choice there and certainly as someone who takes action in the
*  real world as well, it's very notable to me how much you end up just infusing your
*  action with common sense, moral reasoning.
*  And it's always unclear like, is that on sophisticated consequentialist grounds or is it just that
*  one is acting pluralistically?
*  I think you should take it on a case by case basis.
*  I think that actually the issue of wild animal suffering, it sounds completely wild when
*  you first, no pun intended, completely wild when you first hear it.
*  But I think it's not that many steps away from common sense moral reasoning.
*  So I don't have a pet, my friends have pets, they care greatly about the lives of their
*  pets, that's like in their wellbeing.
*  And that's just a very non, like very standard common sense kind of moral view.
*  And next, does that wellbeing of your pet change?
*  Does the moral worth of a creature change whether it happens to be your pet or it is
*  born in the wild?
*  And I think it's a good argument for thinking no.
*  And then the question of, okay, well, if you think it's good to invest some resources to
*  improve the wellbeing of your pets, then yeah, maybe it's good to invest resources in improving
*  the wellbeing of animals in the wild.
*  And then I think the reaction that people have, which is like, this is just so crazy,
*  partly it's not really thinking about it, but partly also is just worries about interfering
*  with nature, having negative backfiring consequences.
*  And I think those arguments are just good.
*  Maybe you think, oh, predation is bad, so we're going to stop predators.
*  But then that leads to other worse consequences.
*  I think it is true that you're dealing with an environment that we don't fully understand.
*  So from the wild animal suffering perspective, I'd maybe be very pro kind of more research
*  or more thinking about this.
*  I'd be kind of pretty wary of just paving over the jungle because on the basis of our
*  very non-robust evaluation, we think that animal lives are on average negative.
*  Let me ask you the question.
*  I asked Sam Bankman Fried, let's say we take the known world of living beings, however
*  large that may be, and a demon offers us a bet.
*  We can double that world with probability 51%, but with 49%, it all goes away and disappears
*  and everything's gone.
*  Now in expected value terms, that's a good bet, right?
*  Should we do that?
*  Sam said yes.
*  He's like, I'm going to bite that bullet.
*  I want to bite this bullet, he said.
*  What's your view?
*  Yeah, so I mean, one first thing is we've got to carefully state the question in that
*  it's, if you're just giving me a doubling of the world as it is, well, I think again,
*  almost all value is in the future, it's to come.
*  And so instead, the thought is that it has to be-
*  Well, that doubles too, right?
*  The future, the double, everything.
*  So spell it out all carefully, but it's a double or nothing bet at 51% odds.
*  I say no way should you do it.
*  Yeah.
*  So I also admit, intuitively, I have very rapid diminishing returns to value.
*  So intuitively, I think that you take a galaxy and it's got full of bliss, best possible
*  galaxy, 50-50 for that versus all accessible galaxies are like flourishing and so on.
*  There's 20 billion of them.
*  And I'm like, no, don't want to take that bet.
*  And I also think that this is an issue.
*  There are issues for expected value theory in general where, I mean, it comes in, in
*  particular with like low probabilities of large amounts of value.
*  Sure.
*  Pascal's Lager, St. Peter's-
*  Pascal's Lager.
*  Exactly, yeah.
*  We're getting into like all sorts of messes there.
*  And then in this case, it's not an example of very low probabilities of very large amounts
*  of value.
*  You might even-
*  And then your view would have to argue that, well, the future as it is, is like close to
*  the upper bound of value in order to make sense of the idea that you shouldn't flip
*  50-50.
*  I think that actually that position would be like pretty hard to defend, is my guess.
*  And so my thought is that probably we're in a situation where any view you say ends up
*  having pretty bad, pretty implausible consequences.
*  Your response sounds very ad hoc to me.
*  Why not just say in matters of the very large utilitarian kinds of moral reasoning just
*  don't apply?
*  They're always embedded in some degree of partiality.
*  The 51-49% bet is not great for our partiality toward ourselves.
*  And we just can't go there.
*  So it's not that there's some other theory that's going to tie up all the conundrums
*  in a nice bundle.
*  But simply that there are limits to moral reasoning and we cannot fully transcend the
*  notion of being partial because moral reasoning is embedded in that context of being partial
*  about some things.
*  I think we should be more ambitious than that with our moral reasoning, where I think if
*  we did moral reasoning at many times in the past that were simply saying, look, there
*  are many of these different considerations.
*  It's all kind of pluralist at some point, just even though I can't give it a good argument,
*  you know, the utilitarian-esque reasoning that seems so compelling when you're talking
*  about saving one life versus 10.
*  And we think, oh, clearly the 10 is more important, including a 50-50 chance of saving 10 lives
*  versus one.
*  You should still, you should go with the math.
*  Like over time, that would save more lives.
*  And then you say, oh, no, at some scale, that sort of reasoning breaks.
*  That's what seems ad hoc to me.
*  So if you're saying, oh, well, these arguments pushing you in a certain direction, but then
*  at some scale, I mean, what exactly is the scale?
*  Is it a thousand lives, a million lives, a billion lives?
*  Then I'm suddenly, you know, it seems like nothing qualitatively different has happened.
*  Whereas the thing that I want to say is a qualitative difference is something to do
*  with when we're juggling probability against value.
*  That's where, OK, maybe the pure just like multiplication or like there's something
*  going along with expected value theory.
*  And I can kind of constrain the issues to that.
*  Whereas if I'm just saying in general, when think the scales get big, drop kind of utilitarian-esque
*  reasoning, that seems like unmotivated to me.
*  But I don't think it's just probabilistic question.
*  So you're very familiar with the repugnant conclusion.
*  You know we haven't solved it.
*  There's nothing probabilistic there.
*  It just seems to be another case where when you stretch the limits far enough, nothing
*  And that you have Pascal's wager, the 51-49 gamble, the repugnant conclusion, many other
*  paradoxes in moral philosophy.
*  They all seem to kick in.
*  And in my view, that's not an accident.
*  There's no reason to ad hoc try to address every one.
*  We just need to downgrade where we think a certain kind of consequentialist reasoning
*  could apply.
*  OK.
*  I mean, I think these paradoxes show something much more further going than an issue for
*  pluralism.
*  So we'll take also just briefly on the 51-49 because of the pluralism that I talked about,
*  although again, it's kind of meta-pluralism of putting weight on many different moral
*  views.
*  I would at least need the probabilities to be quite a bit wider in order to take the
*  gamble because again, I can give you 90-10.
*  We play it 200 times, right?
*  You're still in a lot of trouble.
*  Yeah.
*  I was kind of wanting to clarify that for the listeners more than I was.
*  I didn't say it earlier because I was quite aware that you could pull me back with just,
*  OK, give me some probability or something.
*  But then I think it starts to get more defensible.
*  OK.
*  Anyway, yeah.
*  So there are these paradoxes.
*  So take the paradoxes of population ethics.
*  Again, for the listeners, any view that you have in population ethics has extremely unintuitive
*  implications.
*  And that's actually been formally proven.
*  The repugnant conclusion is the idea that a world consisting of a very, very, very,
*  very large number of beings, all with lives that are just barely above zero, just barely
*  That, because it has more aggregate wellbeing, is better than 10 trillion lives of wonderful
*  bliss.
*  It turns out actually that's, in my view, the least bad of the bullets that you have
*  to bite within population ethics.
*  And sometimes this is taken as a problem for consequentialism, and that's what you're
*  suggesting.
*  But every moral view has to have a view on population ethics.
*  Every moral view has to decide under what conditions should we think it's a good thing
*  to bring a new flourishing life into existence, not just consequentialist moral views.
*  And so the view you'd have to be promoting is something much more further going, which
*  is just that there's limits to moral reasoning.
*  Perhaps that we should be OK with just inconsistent moral views.
*  I'm not quite sure exactly what your view would be there.
*  But it's not just that we have to throw consequentialism out the way.
*  It's like we actually have to throw moral consistency out the window or something.
*  Should the EA movement be anti-abortion?
*  I don't think so.
*  Why not?
*  If you look at hedonistic utility, if you have more people, we're not at repugnant
*  conclusion margins.
*  You'd have somewhat more people, not that many more.
*  Right?
*  Yeah.
*  So I think a few things.
*  So the first is if you think that it's good to have more happy flourishing people.
*  And I think if people have sufficiently good lives, then I think that's true.
*  I argue for it in what we are the future.
*  Then by far and by overwhelming amounts, the focus should be on how many people might exist
*  in the future rather than now.
*  Perhaps you have a really good fertility program and you can increase the world population
*  by 10%.
*  That's like an extra billion people or so.
*  But the loss of future life and future very good life if we go extinct, that's being measured
*  in the trillions upon trillions of lives.
*  And so the question of just how many people should be alive today is really driven by,
*  well how would that impact on the long-term flourishing of humanity?
*  That being said, like all things considered, I think there's this idea at the moment that
*  it's bad to have kids because of the carbon footprint.
*  I think that only looks at one side of the ledger like, yep, people emit carbon dioxide
*  and that has negative effects, but they also do a lot of good things.
*  They innovate.
*  And there's an intrinsic benefit too.
*  They have happy lives.
*  Well, if you can bring up people to live good lives, then they will flourish.
*  That's making the world better.
*  They also might be like moral change makers and so on.
*  But then the question, so even suppose you think, okay, yeah, like larger family sizes
*  are good.
*  What's the best way of achieving that?
*  Would seem very unlikely to me that banning abortions or like very heavily restricting
*  women's reproductive rights is the best way of going about that.
*  It doesn't have to be the best way of achieving that.
*  It might be the 37th best way.
*  But if it were still positive expected utility value, at least in your framework, like you're
*  fine with subsidising births, right?
*  Yeah.
*  So taxing non-births just seems to be the opposite of that, right?
*  It's like the dual.
*  Yeah.
*  I mean, again, you've got to fully take into account like different moral perspectives
*  where in the same way, like, I think it's good for people to donate to charity.
*  I think that makes the world a better place.
*  But having that view is like a far cry from saying, therefore, we should go and like lock
*  people up who don't donate to charity.
*  That could easily be like very bad counterproductive.
*  And I think that's probably very similar could be said about early stage abortion, for example.
*  If there are smart sentient space aliens out there, say in pretty large numbers, should
*  we then worry much less about existential risk on earth?
*  I think someone will continue the tradition.
*  Maybe they don't love Beethoven.
*  But yeah, you know, 400 years from now, maybe people won't anyway.
*  It's a great question.
*  And among people I know views are divided on, you know, should you think that a human
*  originating future is going to be better than an alien originating civilisation?
*  My honest view is more like it's a toss up.
*  I don't see a particular reason for thinking that a civilisation that comes from human
*  beings is going to be like much greater in value than from aliens.
*  Whether this undermines existential risk, though, is dependent very crucially on whether
*  we actually expect aliens to come and build a flourishing civilisation.
*  And I think the best guess from the Fermi paradox, that is the paradox that we don't
*  in fact see advanced intelligent life, is that probably we're just alone, at least
*  in a very, very large section of space.
*  So as an empirical fact, I think it's really quite likely that if humanity dies off, no
*  one else will take our place to build some flourishing civilisation.
*  Just contingent on the space aliens being out there.
*  If just someone asked me to bet, well, which side in the war would Wilma Caskell fight
*  on?
*  Like I would bet a thousand to one you'd fight for the humans.
*  But in your moral theory, the humans being better than the aliens, it's kind of a toss
*  up.
*  And this notion that you can't actually escape some pre-existing degree of partiality in
*  the normative framework seems to resurface.
*  And I think you want to have it both ways, unless you feel my bet on you to fight for
*  the humans is wrong.
*  Like is there really a 50% chance you'll fight for the aliens?
*  So, well, here's the argument.
*  I mean, either, so I have two kind of moral perspectives that I'm putting some weight
*  on.
*  One says it's just, you know, aliens have as good a chance of producing a great civilisation
*  as humans do.
*  The second is like the more partial view, which would weight humans above aliens.
*  If I'm putting weight on both of them, which again, I think we should, I don't think you
*  should be super confident in any more moral worldview, then that will favour, to some
*  extent, favouring the humans.
*  I think it would be a mistake to favour the humans by like, you know, 10,000 to 1.
*  Supposing you could do some very risky thing that could like wipe out both, you know, it's
*  50-50 chance of wiping out both aliens and humans, but 50% chance of saving the humans.
*  And that increases your odds of humanity surviving.
*  Then I'm like, no, don't do that thing.
*  But do we give, would I give some extra weight to human originating civilisation, all things
*  considered?
*  Then yes.
*  Now you're super influential.
*  I'd say you're one of the five most influential philosophers in the world, which is great.
*  Does that mean you should personally give up having children?
*  Wow, what a great question.
*  And I want you to to be clear, but I'm asking what you think.
*  So it's obviously something I've thought deeply about.
*  And I do want to say that like people in general should, you know, make their own reproductive
*  choices.
*  I think in my own case, like it is pretty striking that I am now engaged in like all of these
*  projects that bring me very large amounts of meaning.
*  And then when I think about like, would I have the like argument, the reason that many people I
*  think are drawn to having kids is like to have additional meaning in their lives is not
*  something that like appeals to me.
*  And so it's like, like, like really motivates me.
*  And so I think I do have this like extra responsibility when thinking about major decisions
*  in my life as to like, if I have kids, like what is the impact of that on the world?
*  On the one hand, it would like take time away from other things I could be doing.
*  On the other hand, perhaps it's good.
*  You know, I do think it's good to have a family.
*  Perhaps that's a good signalling thing.
*  I do think those are relevant considerations.
*  In my own case, it's like at least having a family is like never something I've been like
*  particularly drawn to and excited about.
*  And so it's currently not my plan.
*  And I think the fact that like that will help me do more good in the world is like a benefit to.
*  Here's a very simple practical question.
*  Let's say I'm a skilled lawyer and I'm more or less a generalist.
*  I could do a lot of different things and I want to do some pro bono work for effective altruism.
*  What should I actually do?
*  If you're a skilled lawyer, skilled lawyer in the United States.
*  OK, yeah. Then I think it is.
*  There's two obvious options.
*  I mean, there are volunteering opportunities like high impact nonprofits, both within the
*  effective altruism, organizations or organizations we recommend like Malaria Consortium.
*  Do you want the world? But the alternative is to work over time and donate the profits as well.
*  And I could sue someone.
*  Right. So I don't have to do malaria work or give well or bed nets.
*  I'm a lawyer. I could try to change laws by suing people.
*  Right. I have the special leverage.
*  Oh, yeah. So what should I target?
*  So I think people potentially doing dangerous like biotechnology research, things that could have like
*  large negative externalities.
*  I don't know about the law there.
*  People who are patent trolls seem like that's like particularly harmful, it seems to me, you know,
*  like slowing down innovation, perhaps kind of legal work there could be very helpful as well.
*  I'm kind of curious on this is a little bit more theoretical, but and depends on the nature of the
*  lawyer. But, you know, it's plausible to me that at some point in our lifetimes, there will be a
*  world government set up that will government will have a constitution very little.
*  The forming of the Constitution of the United States was enormously impactful from a very long
*  term perspective and yet was done over the course of about four months.
*  So, you know, we can think in terms of these plastic moments that have a real impact over the
*  future. I think that whoever's writing the Constitution of the world government, that is going
*  to be a very influential moment.
*  And so you could be one of the weird lawyers who are working on this that no one is currently
*  working on, but would turn out to be very impactful if it did occur over the next century.
*  Now, you have a PhD from Oxford, right?
*  That's right.
*  Given how much innovation comes out of top schools, why is it crazy to make big donations to
*  them? But I see EA people criticize this fairly often, like, oh, don't give your money to
*  Harvard, give it to Bednets.
*  But given the power of innovation, including your own, right, Peter Singer, has been connected to
*  all these schools.
*  Why not make big donations to top universities?
*  Yeah, I think two things.
*  One is that, yeah, the standard line of criticism of donations to big universities.
*  I don't actually think that's among charitable gifts.
*  I don't really think that's one of the ones we should be criticizing for being enormously
*  ineffective compared to, I don't know, Scott's care or things that are promoting the
*  OPCTA or something, or the US golf society.
*  On the other hand, like, if I'm going to promote research, I think a generic gift to
*  Harvard is going to look pretty unleavered.
*  Like, I don't think universities are in general in a great state in terms of how they
*  could promote research compared to, say, independent research-focused organizations.
*  And in particular, when you're donating to these universities with these enormous existing
*  endowments, looking at the kind of, you know, what in principle happens there, where maybe
*  even you're trying to target the donation to some like focused thing.
*  Now, sometimes that can work and then it's good.
*  And we have funded a bunch of things that research institutes at major universities,
*  including Oxford.
*  But if you're just giving like a generic gift, then probably you're just giving to like
*  Harvard as a whole.
*  And like, that's fine.
*  I do think universities have produced enormous amounts of value, but probably you're missing
*  out on opportunity to do something more focused that pays off sooner as well.
*  We'll take gifts to the opera, which you mentioned.
*  Why should we not build monuments to what has been our greatest and most profound
*  creations just to show people like we did this?
*  This is really important.
*  We still think it's important, right?
*  There's it's a kind of elitism.
*  But nonetheless, isn't it important to keep those traditions alive and highly visible?
*  Yeah, it could be important.
*  Is it going to pass the benefit cost test?
*  I mean, at least, you know, I'm open to anything.
*  You've got to just show me the numbers ultimately.
*  But there are not going to be numbers, right?
*  We're just kind of guessing.
*  Well, you hear Beethoven in the symphony.
*  Do you do something great 30 years later?
*  We're not going to have an RCT on that, right?
*  Well, we're not going to have an RCT, but you can still at least say like, OK,
*  at best, this will this message will reach this many people.
*  At best, this message reaching people will, let's say,
*  increase the impact of their lives by a certain percentage.
*  And then you could at least get kind of upper bound
*  where you think, OK, with the most optimistic assumptions,
*  how much benefit would be being created by this extra run of the opera?
*  And I think
*  even with those optimist, my guess, my strong guess would be that even with
*  those optimistic assumptions, it would not look comparable to other good things
*  that one could be doing.
*  But it's like Parfit's paradoxes and moral arithmetic, right?
*  So the single action doesn't seem that important.
*  If you're a single marksman in a firing squad, well, you didn't kill the person.
*  But in a way, you still did.
*  So no single performance of a great opera is really going to matter much, in my view.
*  But the fact that we have a network of operas performing the magic flute,
*  Fidelio, keeping alive these 18th, 19th century ideals of liberty, freedom,
*  you know, the sonic temple, glorious music,
*  the importance of the exalted and the divine, that seems to me
*  intuitively a super high return, though I don't ever think I'll be able to measure it.
*  So I actually think that.
*  If if you think that even an expectation,
*  your additional project, let's say one of the opera, is not making a difference,
*  then that actually suggests that this class of projects is being overfunded.
*  You should just take that at face value.
*  The value doesn't get inherited from the fact that it's already done like
*  a lot of
*  has done like a lot of good.
*  So taking another example of voting, let's say there's kind of
*  evil candidate and good candidate will suppose, should I vote in the election?
*  If I think like, oh, maybe it's like actually could go either way.
*  Then I think often the answer is yes, because there's some chance that your vote
*  will be decisive. That's worth enormous amounts of
*  and with enormous amounts of value.
*  If, however, it's already kind of 95 percent towards the good candidate
*  in terms of votes and you're just absolutely sure that
*  voting for the good candidate will not make a difference,
*  then I think the main argument and by far the main argument is kind of undermined
*  because
*  you're you know, it's already overdetermined that this good thing is going to happen.
*  And so you adding your extra weight is not making the world any better.
*  How should it matter if for our moral calculations, if we think we might be
*  living in a simulation?
*  I think it potentially matters in a lot of ways.
*  It gets into very
*  what seemed like esoteric topics in decision theory.
*  So two different views of decision theory, causal decision theory and ever
*  non-causal decision theory. Causal decision theory says I care what I cause
*  about. I care I should care about what I cause.
*  If so, then if I'm living in a simulation, the argument for taking the very long
*  term future seriously gets, you know, a massive penalty at least
*  because those people in the future who are simulating us, who
*  you know, are interested in how did things go down at this crucial moment in
*  history when human level artificial intelligence gets built and so on.
*  Once they've got that information, it's much less likely that they're going to
*  keep simulating things like things might get things would get like a lot more
*  boring and computational computation is expensive.
*  So if we're living in a simulation, the future is probably going to be a lot
*  shorter and therefore the causal impact of my actions is much lower.
*  If, however, you've got a non-causal decision theory where I don't take into
*  account just the causal effects of my actions, but also what evidence do I get
*  about how other people will behave, then I should think, well, even if I'm in a
*  simulation, if I do such and such thing, that is also giving me evidence that
*  the will who is in the real world, the non-simulated will with all of this
*  important, you know, huge consequences in front of him, he will do such and such
*  an action too.
*  And so for non-causal decision theory, it makes much less of a difference.
*  Now, I'm someone who tends to prefer causal decision theory.
*  So I think that I guess I think two things.
*  One, if we're in a simulation, kind of all bets are off because like who knows
*  now like what implications you're having.
*  But secondly, maybe you also just are much more likely to favour near-term
*  actions rather than long-term actions because, you know, helping the simulated
*  suffering person now.
*  Okay, well, that's a good thing that you're doing.
*  Trying to positively impact the long-term future is not something that will
*  actually occur because the simulation is kind of likely to get shut off.
*  But couldn't it be this convex returns to time that the simulation might be
*  likely to run for much longer, at least in terms of subjective time, then like
*  if all we have is the so-called real physical universe and you should care
*  about the long run much more.
*  But there's this insuperable epistemic problem.
*  You don't know what the simulators want or even people in other simulations.
*  And there's quite possibly lots and lots and lots of them.
*  So you're paralysed for this other reason.
*  And so you're paralysed for this other reason.
*  So you're paralysed for this other reason.
*  Just you don't know anything.
*  And what you want seems to now be smaller than if like it's just us, Mars and Venus.
*  Yeah.
*  So I think that's pretty plausible.
*  This, if you're in a simulation, it's just, like I said, all bets are off and
*  we don't really know.
*  And maybe that means that no matter how confident you are, that you're in a
*  simulation, you should act as if you're not because 99% you're in a simulation.
*  It's like nihilism.
*  It's like, well, who knows what the impact of any of our actions are.
*  1% that you're not in a simulation and then act on, you know, you just do the
*  kind of things that seem best.
*  On this issue of like, oh, maybe it's convex.
*  So maybe the simulation goes even longer.
*  That's in this kind of category of things that again, feel to me like the kind of
*  low, but also like extremely speculative probabilities that feel like crazy town.
*  So, cause it's not just the kind of simulation ones that are other, you know,
*  other thoughts you might have.
*  And we've mentioned infinite ethics as well.
*  Other thoughts you might have that would lead to even more value in the future,
*  but seem like extremely implausible.
*  So here's another one, which is, you know, you are in favor of speeding up economic
*  growth because that has many benefits for not just now, but like many centuries to
*  come. My kind of response to that would be, well, at some point, economic growth
*  would plateau, you know, not in a few centuries, but certainly by 10,000 years
*  time, we can't just keep growing.
*  And so the more important thing is to either change the values that guide the
*  future or ensure that we have a future at all.
*  Because that's a different difference that really persists for all time.
*  But here's a response you could make, Tyler, which is, well, we shouldn't be
*  confident. We shouldn't be certain, not 100% certain that economic growth will
*  plateau. Maybe it just keeps going forever and ever and ever until 100 trillion years
*  when the last stars burn out.
*  And what's more, that's where all the value is.
*  Because if economic growth can keep going for so long, then that's huge amounts of
*  value, way more than if we merely get a few thousand years of growth.
*  And my response to that is, man, this just seems like super brittle, low
*  probability, because it seems so implausible to me that we could get hundreds
*  of trillions of years of technological progress and improving well-being.
*  And so I have to admit that this just...
*  What about the simple response that higher economic growth today gives you better
*  institutions, and that also serves to minimise existential risk.
*  You look at the countries with poor growth records, none of them seem to have the
*  institutions to fight off a real threat to humanity, right?
*  So is it a simple argument for growth being a priority?
*  OK, so there's a different argument.
*  Then I would focus less on growth per se.
*  But there is something that I do buy.
*  And at some margin, I think it is what we should be doing.
*  And I've done a bit of it so far, which is just like, OK, it's hard to predict the
*  future. We're going to get lots of unexpected events.
*  There are some things that just correlate pretty well.
*  We're onto a good thing.
*  Like, yeah, improved, yeah, technologically driven growth, good
*  institutions, democracy, liberalism, more cooperation, higher trust in societies.
*  These are just generically good.
*  I mean, innovation.
*  And let's just like, from the sheer track record of how helpful these have been over
*  the last 200 years, let's just kind of keep pushing on that.
*  That's the kind of view that I think I'm most sympathetic to in terms of the kind of
*  more progress studies worldview.
*  Because I do think that's like good for the long term future.
*  And it's kind of like, can you beat the market?
*  And I think probably we can actually.
*  But at some margin, that's what I think long termism turns into.
*  It looks kind of more commonsense like building a flourishing society.
*  Now, I'm going to use the word hingy to describe the quality of living in a time that
*  is highly influential, where that influence may very well persist for a long period of
*  time. Do people in their own eras know when they are living in especially hingy eras,
*  or are they clueless?
*  I think we know a lot more now than we did in the past.
*  So I think people in the past would have been pretty clueless.
*  They didn't have a good sense of how long.
*  I mean, the fact that the universe is so truly enormous, so big and yet uninhabited is
*  actually a very recent idea, like a little over 100 years that we've really appreciated
*  that. So people in previous times may have thought.
*  That they were living in extraordinarily hingy times, the early Christians, extraordinarily
*  hingy time. Kingdom was going to come in.
*  It was one or two generations.
*  Oh, you think they were right?
*  And we've just been lucky.
*  But Christianity has proven extremely important and it's still with us, right?
*  It's a foundation for Western prosperity.
*  Yeah, I think they actually I do agree they were the very hingy time, just not for the
*  reasons they thought.
*  But that gets at the epistemic problem.
*  Do people ever know, like people in 1720, how many of them were sitting around saying,
*  well, we're on the cusp of an industrial revolution, right?
*  That was a hingy event.
*  I'm not sure no one knew.
*  The founding fathers were aware.
*  So John Adams has this great quote of the last importance that we build the institutions
*  of America correctly because they may well not wear out for thousands of years.
*  And if they are built incorrectly, then they will not return except by accident to the
*  right path. So I think they actually did seem pretty aware of the importance of what they
*  were doing. But I think this is so I think two things.
*  One, I think this should give us a lot of humbleness or humility in terms of taking
*  actions today. I think perfectly plausible to me, maybe even more likely than not, that
*  in a hundred years time, people would look back and say, oh, wow, these are the people
*  they cared about, AI and worried about bioweapons in the same way as I look back at John Stuart
*  Mill at the end of the 19th century, who was fighting for future generations by trying
*  to keep coal in the ground because he thought that we were going to run out of coal very
*  quickly and that would impoverish future generations.
*  You know, I think that's actually quite likely.
*  I think that gives a good argument for trying to build up, do much more robustly good actions
*  or trying to build up resources that will be very useful in a hundred years time, increasing
*  the number of impartially concerned and altruistically motivated and carefully reasoning
*  thinkers, for example. But I also think we have much better evidence than those people in the
*  past. We have like a much better understanding of physics, a better understanding of social
*  science, of probability, even of ethics.
*  And I think that's a foreign policy.
*  Like we can't predict anything.
*  I don't know anyone good at predicting foreign policy outcomes.
*  Those are maybe the most important issues in the world.
*  So if we can't predict foreign policy two years out, like how well can we understand our
*  own hinginess?
*  Well, I think one reason.
*  So here's a general argument for thinking that we're at least plausibly a very influential
*  time. And I think this argument works.
*  I don't think it means like we're at the most influential time.
*  That's like a substantially harder argument to make.
*  But it's just that level of technological progress is very high compared to history and
*  also very high, like the rate of technological progress and very high compared to what will
*  happen and what must happen in the future.
*  And the argument is just that if we had economic growth of 2% per year for 10,000 years, then
*  we would be producing 10 to the 87
*  world's worth of economic output.
*  And that doesn't seem and not only.
*  Yeah, sorry. We'll be producing that.
*  There are 10 to 67 atoms in within 10,000 light years.
*  So we would be producing about a trillion, trillion
*  civilizations worth like current civilizations worth of economic output for every atom
*  within 10,000 years.
*  And that just seems like, OK, that can happen.
*  So economic growth, technologically driven economic growth is going to have to decrease
*  when we look to the future.
*  And so that actually suggests we're living at a time of unusually high technological
*  change. And this is actually a very tiny window.
*  10,000 years.
*  So it's only been like 200 years that we've gotten anything close to this level of tech
*  progress. 10,000 years is also a very tiny window compared to hundreds of thousands of
*  years that we've been around so far.
*  The millions, billions or trillions of years we could be around in the future.
*  That just seems like a really pretty good reason for thinking, OK, we might like there's at
*  least decent probability for thinking with them at an unusually hingey time.
*  Again, maybe not the most influential time, but something that's like pretty distinctive
*  if you kind of tell the story of the whole of civilization, not just the past, but also
*  the future.
*  Now, on this question, I'm looking for a sociological answer.
*  It's really striking to me how many very smart young people right now are attracted to
*  the effect of Altruism movement.
*  I think way more than a lot of outsiders realize.
*  I'm sure you've seen this.
*  Why is that the case?
*  And the mere fact that you all might be correct, I do not consider a satisfactory answer to
*  be clear, because in general, it's not the case that the correct movements are always
*  attracting the smartest people.
*  So why is this happening now?
*  I was absolutely going to say, well, maybe we've just got the best arguments.
*  I think, OK, fine, I've got to give an entirely sociological explanation.
*  One is, I think there just was an untapped market of altruistically minded people.
*  I think that especially like this kind of effect of Altruism is much broader than
*  consequentialism, but consequentialism flavored ethical views, I think, correlate with being
*  very high educationally performing.
*  I think it's also the case that something that correlates with being high educationally
*  performing is just being kind of secure about your material needs, whether that's because
*  you come from a better family or but also because you're like a career prospect, looking
*  pretty good.
*  I think of Altruism like a luxury good.
*  So the more secure you are, the more you can focus on that.
*  And so one thing is just we've tapped into this market that I think wasn't otherwise
*  being tapped into.
*  Then a second thing I could say is just that the topics are just very intellectually
*  interesting.
*  You can and like, you know, a very unusual intersection of like intellectually interesting
*  and extremely impactful and important for one's own life and in fact, how the world
*  should be.
*  So, you know, you're making arguments about paradoxes in population ethics and moral
*  philosophy and what the resolution there is like really going to make a difference to
*  what you what you should do.
*  So perhaps that's kind of more attractive to the nerds of the world, too.
*  Let me make a sociological observation of my own.
*  If I think about making the world a better place, I think so much about so many things
*  being downstream from culture that we need to think about culture.
*  This is quite a messy topic.
*  It's not easily amenable to what you might call optimization kinds of reasoning.
*  And then when I hear EA discussions, they seem very often to be about optimization.
*  So many chats online in person, like how many chickens are worth a cow, the bed that
*  versus the anti-malaria program.
*  And I often think that this is maybe my biggest difference with EA, that EA has the wrong
*  emphasis pushing people into the optimization discussions when it should be more about
*  improving the quality of institutions and management everywhere in a way that depends
*  on culture, which is this harder thing to manage.
*  And this may even get back to, you know, subsidizing Mozart's magic flute.
*  But there's something about the sociology of EA that strongly encourages, especially
*  online, what I would call the optimization mindset.
*  What's your response to that?
*  I think I'm going to surprise you and agree with you, Tyler, where, yeah, there's I'm
*  not sure it's about optimization, but here's a certain critique that one could make of
*  EA in general or traditionally, it's like, hey, you're a bunch of nerds, bunch of kind
*  of STEM people.
*  You're like the way your brains work will be inclined to focus on like technology or
*  technological fixes and not on mushy things like institutions and culture, but they're
*  like super important.
*  And yeah, I'm at least I at least think that that criticism has like a lot going for
*  them. And one of the things I mean, I don't want to wholesale endorse it because often
*  you just can have technological fixes to what are even sociological problems where, you
*  know, the risk of an engineered pandemic causing, you know, killing hundreds of millions
*  of people. That is in part a sociological or political problem because it's going to be
*  an individual that builds it and does it.
*  We could just solve it with technology, though.
*  Early warning detection systems, far UVC lighting that kind of sterilizes rooms.
*  So there doesn't need to be a match between political or sociological problems and
*  political or cultural responses.
*  But I do think the culture is just enormously important.
*  That's something I've kind of changed my view on and appreciated a lot over the last
*  few years. Just as I started to learn more about history, about the cultural evolution
*  literature, about Joseph Henrik's work and our understanding of humanity as a species.
*  So Nathan Dunn actually, it's one of my favorite and most underrated articles is by Nathan
*  Dunn. It's called History as Evolution.
*  I think it's extremely good.
*  Yes.
*  And actually, my understanding of like human beings, rather than like homo economists,
*  which are mainly motivated by self-interest.
*  You understand that in terms of income, at least when you're looking at much broader
*  scale. I think we're much more like homo culturalis, where people have a view of how
*  the world should be and they go out and try and like make that vision happen.
*  And I think that can have like, yeah, hard to measure and very long run, but important
*  effect. And I actually see effective altruism as a whole as kind of cultural innovation.
*  It's creating this new subculture, a culture of people who are impartially impartial and
*  altruistically motivated, extremely concerned about the truth and having accurate beliefs.
*  And that is a way in which I think effective altruism could have a big impact in the same
*  way as kind of, you know, the scientific revolution was like primarily a cultural
*  revolution, I think. I shouldn't use that term.
*  Primarily a revolution in culture, where people suddenly started innovating and they
*  started to think in a certain way.
*  It was like, oh, we can do experiments and we can test things and we can tinker.
*  So I actually see effective altruism as like a cultural innovation that could drive great
*  kind of moral progress in the future.
*  Yeah. And then like, should we be doing more in terms of cultural change?
*  I guess one thing I'll say is like, people are doing quite a lot of it in terms of, I mean,
*  myself promoting concern for future generations in this book, What We Are the Future is doing
*  that. An awful lot of people are going to promote cultural change around attitudes to
*  non-human animals.
*  It is hard to measure, but I think there's a very big difference between having an
*  optimization mindset to then do the best and having a mindset that's like, therefore, we
*  always need to be able to measure what we're doing and have some metric that we're
*  optimizing towards, where that latter thing, I think, is a bit of a straw man against the
*  EA.
*  Will the EA movement avoid conquest second law, namely that institutions not explicitly
*  designed to be right wing end up becoming left wing?
*  All these major foundations, right? Rockefeller, Ford, Pew, you can go all the way down
*  the list, whether you like that or not, right? It seems to be an empirical regularity.
*  So will it happen to EA?
*  Yeah, I think I'd be curious about what the underlying mechanism is there for those
*  other foundations. It's not something I know about.
*  It's interesting that if you look at the demographics and political views of people in
*  effective altruism, even though we've really not been selecting for that at all, we've
*  been selecting for people who care about things like, does it make sense to spend your
*  money to pay for bed nets to save lives in poor countries? That's certainly not a
*  politically hot button issue. There does tend to be a pretty systematic tendency towards being
*  very socially liberal and being economically moderate or something. There's still obviously
*  a range on both of those cases, but there certainly is a particular tendency. My guess is that
*  that's the bigger factor and inertia would keep effective altruism broadly in that category.
*  But perhaps you could convince me otherwise if I understood what's the mechanism by which these
*  other foundations are shifting left-wing.
*  For our final segment, do you have time for a quick round of underrated versus overrated?
*  Of course.
*  Okay. Bishop Barclay, the philosopher, overrated or underrated?
*  Underrated because idealism in general, I think, is underrated as a metaphysical view.
*  And that's related to thinking we might be living in a simulation.
*  Or not.
*  Yeah, just in general, the fact that it's our experiences that we have direct awareness of and
*  the idea that, well, maybe there is no external world. I think there's more on the table than
*  maybe modern philosophers give it credit for.
*  You're from Scotland. Adam Smith's theory of moral sentiments as a book, over or underrated?
*  I'll have to confess I haven't read it. My guess is that it's underrated because, well,
*  from people I know I respect, they think of it very highly.
*  Quine, the philosopher, over or underrated?
*  Overrated, I'm afraid.
*  Why?
*  Two Dogmas of Empiricism, for example, is his most famous article.
*  He has this and there's the analytic synthetic distinction, views that are true in virtue of
*  meaning, views that are true empirically. And he's like, I don't believe in this distinction.
*  His argument is just, well, can you define what it means for something to be true in virtue of
*  meaning? And he's like, this definition is circular, this definition is circular. I don't
*  think it's a very good argument. I think you can clearly have arguments that are,
*  you know, you can clearly have positions that involve primitive concepts without being able
*  to define them in non-circular terms. And that was his like, you know, that's regarded as one of the
*  great papers of analytic philosophy over the last century. And I think the arguments are pretty weak.
*  And then more generally, he has this tendency of writing these articles that,
*  where the arguments aren't very good, but he ends with some like vivid,
*  yeah, vivid picture or metaphor. And people don't really understand the arguments because
*  they're often quite technical. But then really like the metaphor and people think, oh, he's great.
*  Buildering, overrated or underrated? And you need to tell us what it is.
*  Buildering is also known as urban climbing. So it's where just basically climb buildings in
*  urban environments. It's something I used to do as a younger man. And it is very dangerous. And so
*  I'm going to say it's overrated. In the book, I talk about how I nearly killed myself doing that.
*  And so a lesson from that to the long term future of humanity.
*  Thus we need to worry about existential risk.
*  Exactly.
*  Last question to close this out. What is it you will do next? But just to remind our readers what
*  we owe the future. Will's new book, excellent, one of the most important books of the year.
*  Will is one of the most influential and important philosophers in the world.
*  So please do buy it and read it. But tell us also, what will you do next?
*  Thanks so much, Tyler. So I have a few options on the table. I've been helping Sam Bankman-Flead
*  launch his foundation, the Future Fund, which has been going well. And we've been able to move a
*  lot of money, about $140 million this year. Possible I will keep working more on that.
*  That's one option. Second is just doubling down on books and promotion of ideas.
*  That's why I really love, I enjoy having back and forth with people like yourself.
*  And there's plenty more I'd be interested in writing. Another book that's kind of a follow
*  up to Doing Good Better that is really explaining kind of what is the effect of autism community
*  and actually taking a more, an introduction into that that's less from abstract principles and
*  arguments and more just via what are actually the people in that community, what are they doing.
*  And then a final option that I consider is some sort of new college or university.
*  So really trying to take some of the brightest people from all around the world, especially in
*  countries where very bright, promising, morally motivated people are being missed. So if you're,
*  you know, you could be extremely intellectually talented in rural India and it's maybe you can
*  get, maybe you can make your way out, but it's a challenge at least. And then just trying to give
*  the kind of very best kind of all around education possible. Hiring people who are dedicated as
*  teachers rather than having their attention split between research and teaching, which is the
*  standard university model and also using certain techniques within that we have kind of discovered
*  that aren't very widely used within education to try and accelerate people's learning as fast as
*  possible. And if that worked well in this one instance, then perhaps it could become a much
*  wider idea. Will McCaskill, congratulations again on the book and thank you very much.
*  Great. Thank you so much, Taylor.
