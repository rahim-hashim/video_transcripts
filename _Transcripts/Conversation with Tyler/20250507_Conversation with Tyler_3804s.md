---
Date Generated: May 07, 2025
Transcription Model: whisper medium 20231117
Length: 3804s
Video Keywords: ['Economics', 'Policy', 'Lifestyle', 'Culture', 'AI', 'Anthropic', 'Claude', 'China', 'Chat GPT', 'Dating', 'LLMs']
Video Views: 253
Video Rating: None
Video Description: Few understand both the promise and limitations of artificial general intelligence better than Jack Clark, co-founder of Anthropic. With a background in journalism and the humanities that sets him apart in Silicon Valley, Clark offers a refreshingly sober assessment of AI's economic impact—predicting growth of 3-5% rather than the 20-30% touted by techno-optimists—based on his firsthand experience of repeatedly underestimating AI progress while still recognizing the physical world's resistance to digital transformation.

In this conversation, Jack and Tyler explore which parts of the economy AGI will affect last, where AI will encounter the strongest legal obstacles, the prospect of AI teddy bears, what AI means for the economics of journalism, how competitive the LLM sector will become, why he’s relatively bearish on AI-fueled economic growth, how AI will change American cities, what we'll do with abundant compute, how the law should handle autonomous AI agents, whether we’re entering the age of manager nerds, AI consciousness, when we'll be able to speak directly to dolphins, AI and national sovereignty,  how the UK and Singapore might position themselves as AI hubs, what Clark hopes to learn next, and much more. 

Recorded March 28th, 2035

Transcript and links: https://conversationswithtyler.com/episodes/jack-clark/

Stay connected:
Follow us on X, IG, and Facebook: @cowenconvos
https://www.twitter.com/cowenconvos
https://www.facebook.com/cowenconvos
https://www.instagram.com/cowenconvos

Join us on Discord: https://discord.gg/JAVWP7vTxt

https://conversationswithtyler.com

https://mercatus.org
---

# Jack Clark on AI's Uneven Impact | Conversations with Tyler
**Conversation with Tyler:** [May 07, 2025](https://www.youtube.com/watch?v=U1ZMmKMMHgQ)
*  Hello, everyone, and welcome back to Conversations with Tyler. [[00:00:00](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=0.0s)]
*  Today I am at Anthropic with Jack Clark. [[00:00:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=8.2s)]
*  As you may know, there is now an Anthropic Economic Index, which is measuring the effect [[00:00:11](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=11.52s)]
*  of advanced AI on the U.S. economy. [[00:00:16](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=16.52s)]
*  There were two associated reports as of March 2025 and soon more to come. [[00:00:19](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=19.48s)]
*  Jack, of course, is the co-founder of Anthropic. [[00:00:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=25.08s)]
*  Before that was the policy director at OpenAI, a reporter at Bloomberg, and [[00:00:27](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=27.8s)]
*  originally has his background in the humanities, and he comes from Brighton, England. [[00:00:32](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=32.36s)]
*  Jack, welcome. [[00:00:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=36.76s)]
*  Well, thanks for having me, Tyler. [[00:00:37](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=37.68s)]
*  Pleasure to be here. [[00:00:38](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=38.88s)]
*  Where is it in our economy that AGI will affect last in a significant manner? [[00:00:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=40.0s)]
*  Oh, I'd hazard a guess that it's going to be things that are the trades and the [[00:00:45](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=45.68s)]
*  most artisanal parts of them. [[00:00:50](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=50.519999999999996s)]
*  So you might think of trades as having things like electricians or plumbing, [[00:00:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=52.48s)]
*  or also things like gardening. [[00:00:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=57.36s)]
*  But I think within those, you get certain high status, high skill parts where people [[00:00:58](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=58.76s)]
*  want to use a certain tradesman, not just because of their skill, but because of their [[00:01:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=63.56s)]
*  notoriety and sometimes an aesthetic quality. [[00:01:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=68.16s)]
*  I think that my take might be gardening, actually. [[00:01:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=70.96s)]
*  And they won't use AGI to help design the garden or just the human [[00:01:15](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=75.72s)]
*  front will never disappear. [[00:01:19](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=79.75999999999999s)]
*  I think the human front will never disappear and people will purchase certain [[00:01:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=81.24s)]
*  things because of the taste of the person. [[00:01:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=86.52s)]
*  Even if that taste looks like certain types of modern art production, where the [[00:01:29](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=89.24s)]
*  artist actually backs onto thousands of people that work for them and they're [[00:01:33](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=93.96000000000001s)]
*  more orchestrating it. [[00:01:38](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=98.36s)]
*  And how about in the more desk-bound part of the service sector? [[00:01:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=100.96000000000001s)]
*  Where will it come last? [[00:01:43](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=103.76s)]
*  Come last? [[00:01:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=104.96000000000001s)]
*  Ooh, good question. [[00:01:46](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=106.12s)]
*  I think that on this, there are certain types of desk-bound work that just require [[00:01:47](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=107.64s)]
*  talking to other people and getting to alignment or agreement. [[00:01:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=115.64s)]
*  You might, if you count certain parts of sales. [[00:02:00](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=120.16s)]
*  But it's great at doing that already, right? [[00:02:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=121.56s)]
*  It's a wonderful therapist. [[00:02:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=123.16s)]
*  It is, but we don't sell, we don't send Claude to sell Claude yet. [[00:02:04](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=124.56s)]
*  We send people to sell Claude, even though Claude could probably generate the [[00:02:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=127.92s)]
*  text to do the sales motion. [[00:02:11](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=131.48s)]
*  People want to do commerce with other people. [[00:02:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=133.44s)]
*  So I think that there'll be certain relationships which get mediated by people. [[00:02:15](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=135.64s)]
*  And people will have a strong preference, probably for deals that they make on [[00:02:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=141.0s)]
*  behalf of their larger pools of capital, where the deals are done by human proxies [[00:02:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=145.07999999999998s)]
*  for large automated organizations or pools of capital. [[00:02:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=150.64s)]
*  Where will AGI encounter the strongest legal obstacles? [[00:02:34](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=154.67999999999998s)]
*  I think a few years ago, it was encountering quite strong obstacles in the law itself, [[00:02:39](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=159.39999999999998s)]
*  because lawyers tended to like being able to charge very high prices and had a [[00:02:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=164.48s)]
*  dislike of things that could bid them down. [[00:02:49](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=169.92s)]
*  But my less flippant answer is probably big chunks of healthcare, because it's [[00:02:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=172.23999999999998s)]
*  bound up in certain things around how we handle personal data, all of the standards [[00:03:00](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=180.0s)]
*  around that, all of those standards are probably going to need to be changed in [[00:03:05](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=185.39999999999998s)]
*  some form to make them amenable to being used by AI. [[00:03:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=190.95999999999998s)]
*  And we've had a really hard time updating or changing data standards in general. [[00:03:14](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=194.32s)]
*  But once you can put the AI on your own hard drive, which will be pretty soon, right? [[00:03:19](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=199.04s)]
*  Won't that all change? [[00:03:22](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=202.95999999999998s)]
*  It will change in the form of gray market expertise, but not official expertise. [[00:03:24](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=204.95999999999998s)]
*  I had a baby recently. [[00:03:31](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=211.39999999999998s)]
*  And so whenever my baby bonks their head while I'm dialing the advice nurse, I talk [[00:03:33](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=213.48s)]
*  to Claude just to reassure myself that the baby isn't in trouble. [[00:03:38](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=218.76s)]
*  I don't think we actually fully permit healthcare uses by our own terms of service. [[00:03:42](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=222.8s)]
*  We don't recommend it because we're worried about all of the liability issues this [[00:03:46](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=226.8s)]
*  contains, but I know through my revealed preference that I'm always going to want to [[00:03:50](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=230.68s)]
*  use that, but I can't take that Claude assessment and give it to Kaiser Permanente. [[00:03:54](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=234.4s)]
*  I actually have to talk for a human to get everything else to happen on the back end [[00:03:59](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=239.6s)]
*  to work out if they need to prescribe my child something. [[00:04:04](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=244.04s)]
*  So the number one job will be surreptitiously transmitting the generation of [[00:04:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=247.28s)]
*  information that comes from AIs in essence. [[00:04:11](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=251.96s)]
*  Some of it may be that some of it is about laundering the information that comes from [[00:04:14](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=254.2s)]
*  AIs into human systems, but are not predisposed to that information going in directly. [[00:04:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=258.92s)]
*  I was thinking you might say that United States government or some parts of it would [[00:04:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=265.56s)]
*  be where strong AI would come last. [[00:04:29](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=269.48s)]
*  I think that would be my forecast. [[00:04:32](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=272.4s)]
*  They still use software from the sixties or maybe even the fifties sometimes. [[00:04:33](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=273.84000000000003s)]
*  They do, but I actually suspect that that could be a place where we see really rapid [[00:04:38](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=278.2s)]
*  changes actually, and for a couple of reasons. [[00:04:43](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=283.92s)]
*  One, we know that AI has relevance to national security. [[00:04:46](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=286.64s)]
*  It develops certain types of capabilities. [[00:04:53](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=293.44s)]
*  Oh yeah, that will happen quickly. [[00:04:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=295.40000000000003s)]
*  Yeah, that'll happen quickly. [[00:04:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=296.52s)]
*  But the rest of government, the rest of government, the Department of Education, HHS. [[00:04:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=297.28000000000003s)]
*  The non-scary, sharp part of government. [[00:05:00](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=300.52s)]
*  Yeah. [[00:05:02](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=302.92s)]
*  I would wager that it will become surprisingly easy to get it into really hard parts of [[00:05:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=303.88s)]
*  government, and then there will be a question of political will. [[00:05:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=313.0s)]
*  But all around the world, and I'm sure you experienced this, governments desperately [[00:05:15](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=315.92s)]
*  want growth and they desperately want efficiency. [[00:05:20](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=320.12s)]
*  I mean, we see that here today. [[00:05:22](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=322.68s)]
*  Do they? [[00:05:24](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=324.0s)]
*  They say that. [[00:05:24](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=324.84000000000003s)]
*  Oh, I agree there. [[00:05:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=326.44s)]
*  But I think if you look at also things like voter polling and other things, people want [[00:05:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=328.44s)]
*  to see more changes out of government than they're currently getting. [[00:05:32](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=332.96s)]
*  And I think sometimes constituent preferences do ultimately change what their elected [[00:05:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=336.12s)]
*  officials do. [[00:05:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=340.4s)]
*  I would just take the other side of this, that government may move slightly faster than [[00:05:41](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=341.68s)]
*  you think. [[00:05:45](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=345.96s)]
*  It may be very large established companies that end up having some of the greatest [[00:05:46](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=346.48s)]
*  resistance to this in certain areas. [[00:05:51](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=351.12s)]
*  Let's say it was decided that half of the staff of HUD or Department of Education could [[00:05:54](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=354.08s)]
*  be replaced by Strong AI or EGI. [[00:05:59](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=359.59999999999997s)]
*  Do we first need to hire more people to make that happen? [[00:06:02](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=362.8s)]
*  And who is it we can hire at what wage that switches the system so you can lay off the [[00:06:06](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=366.52s)]
*  remaining half? [[00:06:11](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=371.59999999999997s)]
*  I don't understand how that's ever going to work. [[00:06:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=372.28s)]
*  I think that it will only work in a scenario where the system has got so powerful that [[00:06:15](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=375.71999999999997s)]
*  you can bring the AI system in itself to help you think through this. [[00:06:22](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=382.12s)]
*  And then there will be a question of political will, which is where the system might this [[00:06:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=386.32s)]
*  all might break down. [[00:06:31](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=391.24s)]
*  What do you think is the chance that we decide to protect, say, half of the jobs in [[00:06:33](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=393.6s)]
*  existence today with laws analogous to those we find in law and medicine against strong [[00:06:38](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=398.12s)]
*  AGI? [[00:06:43](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=403.6s)]
*  I think there is a high chance for a political movement to arrive, which tries to freeze [[00:06:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=404.84000000000003s)]
*  a load of human jobs in bureaucratic amber as a way to deal with the political issues [[00:06:53](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=413.92s)]
*  posed by this incredibly powerful technology and how fast the changes are going to come [[00:07:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=421.56s)]
*  from. [[00:07:06](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=426.36s)]
*  I don't think that we'll do this in a reasoned way. [[00:07:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=427.32s)]
*  I think it will be driven by the chaotic winds of political forces. [[00:07:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=430.08s)]
*  And it feels like the sort of outcome that happens if we as the companies building this [[00:07:15](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=435.04s)]
*  stuff and our customers don't generate enough examples of what good transitions look like. [[00:07:22](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=442.6s)]
*  The fewer bits of evidence you have there and the more evidence you have of larger economic [[00:07:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=448.72s)]
*  changes, probably the higher chance there'll be a desire to step in and protect workers [[00:07:33](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=453.0s)]
*  in different domains, which comes from an impulse based around wanting to help people. [[00:07:38](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=458.28s)]
*  But it might ultimately not be the most helpful thing over the course of decades to do that. [[00:07:43](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=463.76s)]
*  But is it possible that actually in a constrained sense, the very best outcome, people [[00:07:48](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=468.76s)]
*  will still have a job, they'll go somewhere in the morning. [[00:07:54](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=474.15999999999997s)]
*  A lot of the work, especially the hard work, will be done by the AIs. [[00:07:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=476.91999999999996s)]
*  Obviously, we'll be richer so we can afford this all of a sudden. [[00:08:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=481.03999999999996s)]
*  It won't feel that bad to people. [[00:08:05](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=485.15999999999997s)]
*  Life will look familiar. [[00:08:06](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=486.96000000000004s)]
*  Isn't that in a sense what we should be aiming for? [[00:08:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=488.52000000000004s)]
*  I think we should be aiming for. [[00:08:11](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=491.36s)]
*  In the same sense, you might need, say, a generous welfare state to have free trade. [[00:08:14](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=494.12s)]
*  The welfare state isn't always the most efficient, but if people accept free or trade, it's an [[00:08:17](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=497.92s)]
*  okay bargain. Isn't this in a sense, the welfare state for the service workers and they still [[00:08:22](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=502.68s)]
*  get to go somewhere in the morning if they want to, but they don't have to. [[00:08:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=508.68s)]
*  I believe that people, all people have a desire for meaning and salient to what they do on [[00:08:31](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=511.28s)]
*  a day-to-day basis. [[00:08:41](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=521.0799999999999s)]
*  And my worry with what you describe is it might not feel like it has meaning, sufficient [[00:08:42](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=522.56s)]
*  meaning. I think that there is some giant class of activity that we want to continue [[00:08:47](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=527.76s)]
*  happening in the world that people do and from which they draw meaning. [[00:08:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=532.4399999999999s)]
*  But I don't know that the best way to get there is to take some class of work and say, [[00:08:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=537.52s)]
*  this is work that we're protecting and from which meaning will spring, because I'm not [[00:09:02](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=542.12s)]
*  confident that you'll pick a load of jobs which naturally create their own meaning in [[00:09:06](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=546.56s)]
*  that sense. [[00:09:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=550.72s)]
*  But hasn't this succeeded in academia, pre-AI? [[00:09:11](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=551.6s)]
*  So most academic jobs are not that meaningful. [[00:09:14](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=554.88s)]
*  The research people do, it's not read by anyone. [[00:09:17](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=557.76s)]
*  Maybe there are decent teachers. [[00:09:20](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=560.28s)]
*  But people take great pride in their research. [[00:09:22](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=562.56s)]
*  They put a lot of effort into it. [[00:09:24](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=564.36s)]
*  It's meaningless. [[00:09:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=566.16s)]
*  It seems we solve that problem already. [[00:09:27](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=567.4399999999999s)]
*  We're just going to take the academic model. [[00:09:29](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=569.48s)]
*  But instead of it being research, bring it to, say, half of our current economy just to [[00:09:31](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=571.48s)]
*  keep it going. [[00:09:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=576.0s)]
*  But isn't there a kind of angst and nihilism even within high achieving parts of academia [[00:09:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=576.76s)]
*  for this reason? [[00:09:42](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=582.96s)]
*  It seems like when you speak to people, even very smart people who sometimes are doing [[00:09:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=584.0s)]
*  the things you describe, they know they could be doing different things and they're trapped [[00:09:47](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=587.68s)]
*  into some kind of status game. [[00:09:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=592.0s)]
*  There's some of that. [[00:09:54](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=594.64s)]
*  But even the Nobel laureates, they're rivalries with each other. [[00:09:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=595.52s)]
*  They can be very bitchy, very petty. [[00:09:58](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=598.96s)]
*  But that's just human nature, right? [[00:10:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=601.48s)]
*  If the Nobel laureates aren't happy, there's no post-AI world that's going to do much [[00:10:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=603.16s)]
*  better than how we're doing for the Nobel laureates today, right? [[00:10:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=608.16s)]
*  Yeah. [[00:10:11](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=611.4s)]
*  Maybe my pushback is I think that all of this could happen sufficiently quickly, that we [[00:10:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=612.48s)]
*  might have the opportunity to just play different higher status games that are afforded to [[00:10:19](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=619.2s)]
*  us via AI and the productive capacity it unlocks. [[00:10:23](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=623.6s)]
*  I'm wondering if there are, there are definitely going to be entirely new jobs that involve [[00:10:27](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=627.2s)]
*  marshalling and fielding AI systems for all kinds of work. [[00:10:32](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=632.52s)]
*  I think there'll be- [[00:10:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=636.16s)]
*  But those are hard jobs, right? [[00:10:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=636.76s)]
*  Yes. [[00:10:38](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=638.16s)]
*  But I think that there are going to be analogs which look more like creative, like fun [[00:10:38](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=638.52s)]
*  exercises in getting AIs to kind of build things or make things or almost carry out [[00:10:43](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=643.44s)]
*  like competitions and games where people can play them with one another. [[00:10:48](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=648.5600000000001s)]
*  And I think there'll be entirely new forms of entertainment that has some amount of [[00:10:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=652.0s)]
*  meaning and perhaps an economic engine wired into it that people can participate in. [[00:10:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=657.04s)]
*  I believe we're not that far from the age of what I call the AI teddy bears. [[00:11:02](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=662.12s)]
*  You know what I mean when I say that? [[00:11:06](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=666.3199999999999s)]
*  What percentage of parents now will buy those teddy bears for their kids and allow it? [[00:11:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=668.0s)]
*  I mean, I've had this thought since I have a person that, you know, is my child, that's [[00:11:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=673.7199999999999s)]
*  almost two. [[00:11:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=678.5999999999999s)]
*  Sure. [[00:11:19](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=679.4399999999999s)]
*  And I am annoyed I can't buy the teddy bear yet. [[00:11:20](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=680.2s)]
*  I think most- [[00:11:23](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=683.24s)]
*  But you're an outlier. [[00:11:23](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=683.96s)]
*  Yeah, I don't know. [[00:11:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=685.24s)]
*  I don't know. [[00:11:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=686.36s)]
*  You are co-founder of Anthropic, right? [[00:11:27](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=687.08s)]
*  I don't think I'm an outlier. [[00:11:29](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=689.12s)]
*  I think that once your lovable child starts to speak and display endless curiosity and [[00:11:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=690.24s)]
*  a need to be kind of satiated, you first think, how can I get them hanging out with [[00:11:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=695.1600000000001s)]
*  other human children as quickly as possible? [[00:11:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=700.1600000000001s)]
*  So we're on the preschool list, all of that stuff. [[00:11:42](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=702.36s)]
*  I also think, I mean, I've had this thought, well, I wish you could talk to like your [[00:11:45](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=705.0s)]
*  bunny occasionally so that the bunny would provide you some entertainment while I'm [[00:11:48](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=708.88s)]
*  putting the dishes away or making you dinner or something. [[00:11:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=712.92s)]
*  Often you just need another person to be there to help you wrangle the child and keep [[00:11:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=715.32s)]
*  them interested. [[00:12:00](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=720.88s)]
*  But say that the kid says to you, daddy, I prefer the bunny to my friends. [[00:12:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=721.88s)]
*  Can I stay at home today? [[00:12:06](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=726.36s)]
*  Do you take the bunny away? [[00:12:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=727.88s)]
*  That's the tough part, right? [[00:12:09](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=729.16s)]
*  I think that's the part where you have them spend more time with their friends, but you [[00:12:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=730.48s)]
*  keep the bunny in their life because the bunny is just going to get smarter and be more [[00:12:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=733.52s)]
*  around them as they grow up. [[00:12:16](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=736.76s)]
*  So if you take it away, they'll probably do something really strange with smart AI [[00:12:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=738.0s)]
*  friends in the future. [[00:12:23](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=743.44s)]
*  No, I don't think I'm an outlier here. [[00:12:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=745.04s)]
*  I think most parents would, if they could acquire a well-meaning friend that could [[00:12:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=746.68s)]
*  provide occasional entertainment to their child when their child is being like very [[00:12:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=755.8s)]
*  trying, they would probably do it. [[00:12:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=760.8s)]
*  But I feel the word occasional is doing a lot of work in that sense. [[00:12:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=764.0s)]
*  So if you can just ration how much your kid has the bunny, parents are going to love it. [[00:12:47](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=767.8s)]
*  I agree. [[00:12:51](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=771.88s)]
*  Yeah. [[00:12:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=772.32s)]
*  But the same as with screens, a lot of children, they keep on wanting it. [[00:12:53](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=773.08s)]
*  It's hard to tell the child you can't have it now. [[00:12:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=776.28s)]
*  Right. [[00:12:59](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=779.04s)]
*  It's like in the old days, it was watching television. [[00:13:00](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=780.0s)]
*  Oh mom, can I watch Star Trek? [[00:13:02](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=782.6s)]
*  And can I watch TV seven hours a day? [[00:13:04](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=784.68s)]
*  Well, that's not good. [[00:13:06](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=786.64s)]
*  And that's hard to ration. [[00:13:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=787.88s)]
*  Yeah. [[00:13:09](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=789.4s)]
*  So there's some question here of how we portion this out. [[00:13:09](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=789.84s)]
*  I mean, we do this today with TV where if you're traveling with us, like on a plane [[00:13:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=793.2s)]
*  with us, or if you're sick, you get to watch TV for the baby. [[00:13:17](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=797.5200000000001s)]
*  And otherwise you don't because from various perspectives, it seems like it's not the [[00:13:20](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=800.96s)]
*  most helpful thing. [[00:13:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=805.0s)]
*  You'll probably need to find a way to gate this. [[00:13:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=806.4000000000001s)]
*  And it could be when mom and dad are like doing chores to help you, you get the thing. [[00:13:29](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=809.6800000000001s)]
*  When they're not doing chores, the thing goes away. [[00:13:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=815.24s)]
*  Do you end up with too much surveillance over your kid? [[00:13:38](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=818.5200000000001s)]
*  Cause you'll know everything if you want to. [[00:13:41](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=821.76s)]
*  Right. [[00:13:43](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=823.24s)]
*  And that might be the reason why you give the kid the bunny all the more. [[00:13:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=824.0s)]
*  I think it comes down to frequency, which I mentioned and also what you are and aren't [[00:13:47](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=827.4000000000001s)]
*  allowed to know. [[00:13:53](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=833.1600000000001s)]
*  I find surveillance puzzling. [[00:13:54](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=834.72s)]
*  We have one of these smart webcams that we got, you know, when, when the kids are early, [[00:13:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=836.2s)]
*  so that you just see that they're asleep and you know, if they're waking up, you work out [[00:14:00](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=840.36s)]
*  if they're okay or not. [[00:14:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=843.72s)]
*  And the main thing that that meant is that we, when the baby like cries at night, [[00:14:05](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=845.2s)]
*  sometimes we go in less than if we didn't have the camera, because if we didn't have [[00:14:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=850.28s)]
*  the camera, we'd have to go and check. [[00:14:15](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=855.92s)]
*  And I think as a consequence, my baby tends to like sleep through the night a lot more [[00:14:17](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=857.3199999999999s)]
*  because they don't get occasionally interrupted. [[00:14:20](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=860.76s)]
*  Sometimes they wake and cry for a minute and just go back to sleep. [[00:14:23](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=863.12s)]
*  So I think sometimes this stuff allows you to actually. [[00:14:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=865.88s)]
*  Interfere less in a person's life if you know certain things about it. [[00:14:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=870.4s)]
*  But say you have the hypothesis that so many like seven year olds, they talk to themselves, [[00:14:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=875.24s)]
*  they say weird things and the AI bunny is going to report back to you. [[00:14:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=880.12s)]
*  Yeah. [[00:14:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=884.48s)]
*  If the bunny says like your kid's saying strange stuff, but they all say strange stuff. [[00:14:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=884.92s)]
*  I might've been chattering on about the New York Nets at age seven and it would have been [[00:14:49](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=889.5600000000001s)]
*  perfectly harmless, but it may not have sounded that way. [[00:14:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=892.96s)]
*  Yeah. [[00:14:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=895.88s)]
*  You're going to need to create spaces for. [[00:14:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=896.36s)]
*  Unmonitored creativity in people actually the same as how we have an approach to AI [[00:15:02](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=902.4s)]
*  research today. [[00:15:09](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=909.28s)]
*  Like now AI systems can output their chains of thought, right. [[00:15:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=910.1999999999999s)]
*  Which is the reasoning they use to come up with with answers. [[00:15:14](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=914.04s)]
*  And we've had this question at Anthropic of how much should we monitor the chains of [[00:15:17](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=917.6s)]
*  thought? [[00:15:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=921.6s)]
*  Because if you actually monitor them, you might create an anti-goal where the system [[00:15:22](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=922.3199999999999s)]
*  ends up wanting to have chains of thought, which are like safe to be monitored and which [[00:15:27](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=927.68s)]
*  don't cause demerits, which might actually break how it thinks in a bunch of ways. [[00:15:32](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=932.24s)]
*  I think this is, this analog applies here where you're going to need to choose how much [[00:15:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=935.76s)]
*  you actually decide to know about people, or you risk creating incentives that change [[00:15:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=940.0400000000001s)]
*  their behavior such that they, they have a negative effect. [[00:15:45](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=945.32s)]
*  You'll go out on a date and your date will say, please show me your AI report, right? [[00:15:49](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=949.32s)]
*  What you've been talking to yourself about for the last three months. [[00:15:54](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=954.1600000000001s)]
*  Yeah. [[00:15:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=956.8000000000001s)]
*  And you can not show it, which is a negative signal or show it. [[00:15:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=957.12s)]
*  We all learn what other people are really like, and we just grow to accept that. [[00:16:02](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=962.1600000000001s)]
*  Yeah. [[00:16:05](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=965.72s)]
*  Although if the person asks you to do that on the first date, it's fine. [[00:16:05](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=965.96s)]
*  And if they ask them a second date, you probably shouldn't be dating that person. [[00:16:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=968.96s)]
*  Right. [[00:16:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=972.08s)]
*  But even during the swipe, right? [[00:16:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=972.5600000000001s)]
*  The AI, you're asked to upload it and the AI just reads the other AI report and the [[00:16:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=973.84s)]
*  person never sees it. [[00:16:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=978.5600000000001s)]
*  And it tells you when you should swipe in the correct way. [[00:16:19](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=979.84s)]
*  I don't know. [[00:16:23](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=983.48s)]
*  I don't know if that's so bad. [[00:16:24](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=984.0s)]
*  Uh, I met my wife on, on OKCupid where you, I don't know if you recall this, it was an [[00:16:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=985.36s)]
*  online dating site where you like would fill out a survey. [[00:16:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=990.4s)]
*  Of course. [[00:16:33](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=993.36s)]
*  Match.com for me. [[00:16:33](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=993.84s)]
*  Exactly. [[00:16:34](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=994.9200000000001s)]
*  My wife. [[00:16:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=995.6s)]
*  Yeah. [[00:16:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=996.0s)]
*  And then I met her and we are, are non AI, but like automated system. [[00:16:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=996.3199999999999s)]
*  It said you guys might get along. [[00:16:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1000.64s)]
*  So we'd met each other that way. [[00:16:42](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1002.0s)]
*  So I don't know that this is so different to previous things we've used, but that's [[00:16:43](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1003.28s)]
*  information you're putting in voluntarily, right? [[00:16:47](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1007.64s)]
*  As opposed to it watching you all the time. [[00:16:50](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1010.4s)]
*  I just think that this is an area where we're going to figure out the new norms of [[00:16:53](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1013.0s)]
*  this technology and what seems appropriate and not appropriate. [[00:16:58](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1018.0s)]
*  And some of that is just going to be solved through the logic of business and other [[00:17:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1021.52s)]
*  things through usage. [[00:17:04](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1024.64s)]
*  What will the economics of media look like? [[00:17:06](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1026.4s)]
*  If you can read a digest of everything that is probably better than the original [[00:17:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1028.5200000000002s)]
*  or certainly not worse or more synthetic who gets paid for what? [[00:17:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1032.6000000000001s)]
*  Yeah. [[00:17:17](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1037.6000000000001s)]
*  I, I think that this is one of the toughest questions in front of us. [[00:17:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1038.0400000000002s)]
*  As you know, I'm, I'm a former journalist, right? [[00:17:22](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1042.6000000000001s)]
*  And so I grew up during the, as a professional during the period when online [[00:17:24](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1044.8000000000002s)]
*  ad market changes were altering the business model of journalism because they were [[00:17:31](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1051.36s)]
*  switching from what you might think of as value that you derive through quality, [[00:17:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1055.76s)]
*  through subscriptions or people buying your stuff to value that you derive from [[00:17:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1060.92s)]
*  just large scale attention, because that became a lot of the model for, for funding [[00:17:45](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1065.16s)]
*  this stuff. [[00:17:49](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1069.72s)]
*  And as a consequence, you saw the need to cross-subsidize journalists with [[00:17:50](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1070.2s)]
*  journalists that got loads of eyeballs and journalists like me who would write [[00:17:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1075.68s)]
*  about data databases got less eyeballs. [[00:17:58](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1078.92s)]
*  So we were cross-subsidized by the people that wrote about Justin Bieber or what [[00:18:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1081.68s)]
*  have you. [[00:18:05](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1085.3200000000002s)]
*  I think for media, it's going to be really challenging to think through how the [[00:18:06](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1086.0s)]
*  economics of this work. [[00:18:11](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1091.16s)]
*  And I think that it'll, it'll change in a couple of ways. [[00:18:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1092.3600000000001s)]
*  One, you might move to some of these kind of larger scale publishing house style [[00:18:14](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1094.6000000000001s)]
*  models for certain types of fictional universes, which have subsidy and cross [[00:18:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1101.8400000000001s)]
*  subsidy within them. [[00:18:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1105.92s)]
*  Where does the cross subsidy come from? [[00:18:27](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1107.8s)]
*  Like Bloomberg, we know how that works, right? [[00:18:29](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1109.32s)]
*  But if AGI is truly general and it's based on what's out there already, it [[00:18:31](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1111.68s)]
*  should be able to do better than media on all dimensions. [[00:18:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1116.96s)]
*  I think some things you want to have come from a person for reasons that are [[00:18:41](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1121.48s)]
*  based on the fact that we're people. [[00:18:46](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1126.04s)]
*  And I think people will preferentially select for media, which is fronted by [[00:18:47](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1127.68s)]
*  other people, even if they're making it using other means. [[00:18:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1132.36s)]
*  I also think you want a kernel of humanity in a load of this stuff. [[00:18:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1136.2s)]
*  And then there'll be another type of media, which is maybe attached to these [[00:18:59](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1139.36s)]
*  subscription models, which is just on tap permutations of anything and everything. [[00:19:05](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1145.76s)]
*  So there might be two markets that emerge. [[00:19:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1150.16s)]
*  But even the things we want to come from humans, so say we want Dan Landers, the [[00:19:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1152.72s)]
*  advice column is to come from an actual and Landers, the and Landers is of the [[00:19:17](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1157.0s)]
*  world. [[00:19:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1161.24s)]
*  They're using AI's maybe surreptitiously, but the value of that is big down [[00:19:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1161.8400000000001s)]
*  because anyone can do that for the price of energy, right? [[00:19:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1166.96s)]
*  So we don't really end up with this one part of the revenue generating sector [[00:19:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1170.0s)]
*  that can cross subsidize the other parts. [[00:19:34](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1174.44s)]
*  Just if intelligence is pretty cheap, that hits all of media. [[00:19:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1176.72s)]
*  It hits all of media and you will end up wanting to, I think, pay individuals. [[00:19:41](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1181.0400000000002s)]
*  I think that happens even in, even in the world we're in today. [[00:19:46](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1186.88s)]
*  People want to subsidize individual creators who may be using a whole bunch [[00:19:50](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1190.44s)]
*  of this stuff. [[00:19:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1195.72s)]
*  So it's like sub, sub stack world. [[00:19:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1196.3200000000002s)]
*  It's sub stack Patreon world for a large chunk of people, some of which will be [[00:19:58](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1198.04s)]
*  incredibly successful. [[00:20:02](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1202.4s)]
*  And then I think there's also going to be universe world for certain [[00:20:04](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1204.24s)]
*  universes, which are like very large and rich, which are being extended by AI [[00:20:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1207.44s)]
*  systems. [[00:20:11](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1211.76s)]
*  You mean like a Lord of the Rings universe? [[00:20:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1212.56s)]
*  Yeah. [[00:20:14](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1214.1200000000001s)]
*  Or Warhammer 40K to show my, my nerd credentials, but both. [[00:20:14](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1214.3200000000002s)]
*  They'll publish fictional news, right? [[00:20:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1218.72s)]
*  Yes. [[00:20:20](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1220.5200000000002s)]
*  And real news will come from sub stack or? [[00:20:22](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1222.0s)]
*  Real sub stack does not an obvious cross subsidy. [[00:20:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1226.0800000000002s)]
*  There might be within the sub stack company, but. [[00:20:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1228.3600000000001s)]
*  Real news. [[00:20:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1230.5200000000002s)]
*  I genuinely don't know. [[00:20:31](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1231.3200000000002s)]
*  I think some real news comes from analysis of publicly available facts and it being [[00:20:34](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1234.48s)]
*  composed together in a way that shows you insights that didn't exist. [[00:20:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1240.76s)]
*  A semi analysis on, on sub stack is a good example of this. [[00:20:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1244.0800000000002s)]
*  A lot of public stuff leading to interesting conclusions, but news in the [[00:20:47](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1247.24s)]
*  moment, but has loads of context was subsidized by previous business models, [[00:20:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1252.76s)]
*  which mostly no longer work. [[00:20:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1257.56s)]
*  And I genuinely don't know what happens to it. [[00:20:58](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1258.92s)]
*  If we think about what we now call LLMs five or 10 years from now, that kind of [[00:21:02](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1262.08s)]
*  AI, how concentrated or how competitive do you think the sector will be? [[00:21:06](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1266.48s)]
*  I don't know if number of firms is exactly the right measure, but how do you see it [[00:21:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1272.2s)]
*  evolving? [[00:21:16](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1276.6s)]
*  Cause right now there's at least six firms, depending how you count China, [[00:21:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1278.1599999999999s)]
*  there's some complications, but there's a bunch of firms, right? [[00:21:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1281.48s)]
*  A lot of competition. [[00:21:24](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1284.12s)]
*  I would expect that there will be for quite some time, many, many years. [[00:21:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1286.04s)]
*  Areas where there are slightly different specialisms on all of these things. [[00:21:34](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1294.6s)]
*  They'll be like concentric circles that have got massively large in terms of the [[00:21:39](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1299.04s)]
*  space that they cover and they'll have edges which are slightly differentiated. [[00:21:42](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1302.56s)]
*  But in those edges will be where the kind of frontier of, of certain types of [[00:21:47](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1307.32s)]
*  like human value stacks on top of AI value to lead to people choosing one or [[00:21:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1312.04s)]
*  the other. [[00:21:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1316.72s)]
*  So Claude's more poetic for instance. [[00:21:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1317.08s)]
*  A bad example due to the economics of poetry aren't particularly great, but, [[00:21:59](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1319.84s)]
*  um, you know, coding might be a better example or, or there could be cases for [[00:22:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1323.8s)]
*  things like certain types of scientific experimentation where actually taste [[00:22:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1328.8799999999999s)]
*  might come to matter a lot for composing experiments. [[00:22:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1332.56s)]
*  I expect we enter a world though, where you have a single digit number of these [[00:22:16](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1336.12s)]
*  like very, very large scale models, which are servicing like a much larger set of [[00:22:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1341.04s)]
*  wrappers on top of them that change the form factor by which they get integrated [[00:22:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1348.24s)]
*  into your life and which have a load of helper functions or knowledge probably [[00:22:34](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1354.24s)]
*  built by those AIs in partnership with some kind of other specialized AI system [[00:22:38](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1358.6s)]
*  or expert knowledge from that domain to chain it in. [[00:22:43](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1363.76s)]
*  Here's a worry I have. [[00:22:47](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1367.12s)]
*  And my former colleague Vernon Smith once wrote a paper on this. [[00:22:48](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1368.4399999999998s)]
*  He argued that as you approach six competitors or more, that a sector [[00:22:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1372.04s)]
*  essentially behaves like perfect competition, even if it's not exactly [[00:22:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1376.56s)]
*  perfectly competitive. [[00:23:00](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1380.32s)]
*  So if that's the case, how is it that AI or indeed any other companies can behave [[00:23:02](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1382.24s)]
*  ethically above and beyond what they need to do to stay in the market and earn [[00:23:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1387.24s)]
*  profit? [[00:23:11](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1391.68s)]
*  Like what room is there to be better than the next company? [[00:23:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1392.1200000000001s)]
*  I think right now it's like we're car companies and we are mostly selling cars [[00:23:15](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1395.2s)]
*  to teenagers who say, how fast is it? [[00:23:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1401.6s)]
*  And can I get it in red? [[00:23:23](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1403.96s)]
*  And we're beginning to find customers that are, you know, businesses that [[00:23:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1405.56s)]
*  operate fleets of cars and say, what seat belts does it have? [[00:23:31](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1411.0s)]
*  What is your accident rate? [[00:23:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1415.04s)]
*  What are all of these other properties that the teenager has never asked us about, but [[00:23:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1416.24s)]
*  these businesses do because it ties to their business model or to liability or to [[00:23:39](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1419.72s)]
*  other things. [[00:23:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1424.24s)]
*  I think therefore there are technologies to be built that look like the safety [[00:23:45](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1425.48s)]
*  equivalents that we have in cars or other things, which will change the logic of [[00:23:51](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1431.24s)]
*  competition. [[00:23:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1435.76s)]
*  So we're in competition right now. [[00:23:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1437.04s)]
*  That competition will change as we unlock different markets that have different [[00:23:59](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1439.68s)]
*  properties that they need to be present in the AI system. [[00:24:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1443.8s)]
*  And those properties will often ladder up to certain types of safety technology. [[00:24:06](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1446.8s)]
*  So the price of insurance is carrying the mechanism, so to speak. [[00:24:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1452.0s)]
*  I believe that this will be like one of the ways that it helps. [[00:24:15](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1455.52s)]
*  Yeah. [[00:24:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1458.4s)]
*  How do we need to change or improve liability law for the price of insurance to [[00:24:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1458.8s)]
*  actually carry that? [[00:24:23](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1463.1200000000001s)]
*  Because right now liabilities from AI of all sorts, it's highly unclear. [[00:24:24](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1464.6s)]
*  Yeah. [[00:24:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1468.2s)]
*  I think that I have thought less about liability and more about information that [[00:24:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1468.68s)]
*  should be made available by the companies. [[00:24:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1476.12s)]
*  But I think these have like an interplay where today there isn't really a common [[00:24:38](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1478.24s)]
*  labeling or disclosure standard about what are like in these AI systems or the [[00:24:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1484.08s)]
*  industrial practices you've used when like making them. [[00:24:50](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1490.08s)]
*  And I think that this is ultimately an area where you can do interventions to get [[00:24:53](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1493.6s)]
*  some common level of transparency and that'll interplay with both liability and [[00:24:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1497.44s)]
*  also things like negligence, which will change the behavior of these companies [[00:25:02](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1502.48s)]
*  over time. [[00:25:06](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1506.76s)]
*  So there's some level of common information we can start providing about these [[00:25:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1507.3200000000002s)]
*  systems, which will also change corporate behavior. [[00:25:11](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1511.0800000000002s)]
*  But the information part worries me though. [[00:25:14](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1514.8400000000001s)]
*  I'm more inclined to be an accelerationist and, you know, hope the benefits out [[00:25:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1518.0s)]
*  raise the costs, which I definitely think they will. [[00:25:22](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1522.2s)]
*  But information only picks up the private value. [[00:25:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1525.44s)]
*  So the social cost to the decision you make, the information doesn't change what [[00:25:27](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1527.76s)]
*  you do very much. [[00:25:32](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1532.4s)]
*  So people know a lot about global warming. [[00:25:33](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1533.4s)]
*  Some people eat less and eat, but for the most part they don't, right? [[00:25:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1535.68s)]
*  Well, one of the things that you do is you create information which becomes [[00:25:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1540.6s)]
*  truly common. [[00:25:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1544.4s)]
*  I mean, the economic index work we've done is from one single firm, but [[00:25:45](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1545.3600000000001s)]
*  ultimately you might want to generalize that to all firms, but then link it to [[00:25:50](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1550.92s)]
*  large scale data gathering that governments might do. [[00:25:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1556.5600000000002s)]
*  And I think if that became the case, you would have a higher chance of tying it [[00:26:00](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1560.0s)]
*  to actual policy responses that would be larger in scope. [[00:26:05](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1565.04s)]
*  I also think on climate change, just by measuring things like parts per million, [[00:26:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1568.8400000000001s)]
*  stuff like that has helped catalyze large scale amounts of capital to get [[00:26:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1573.76s)]
*  remobilized around the economy in different areas to deal with this perceived [[00:26:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1578.24s)]
*  kind of negative impact that that has. [[00:26:23](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1583.16s)]
*  Do you agree with my view that we will not have meaningful international agreements [[00:26:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1586.32s)]
*  on the hard parts of AI, maybe on the simple parts, but? [[00:26:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1590.84s)]
*  90% agreement. [[00:26:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1595.48s)]
*  I think there is a chance of something that looks like a nonproliferation [[00:26:37](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1597.28s)]
*  agreement between states, including the US and China in the limit. [[00:26:41](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1601.92s)]
*  And the US and China enforce it and we become vaguely allied with them in this [[00:26:46](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1606.28s)]
*  one crusade. [[00:26:50](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1610.68s)]
*  I think it might be deciding that certain things, certain capabilities of AI [[00:26:51](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1611.8s)]
*  systems might be so potentially destabilizing that you don't want them [[00:26:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1617.24s)]
*  broadly available while recognizing that each government may be privately [[00:27:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1621.28s)]
*  developing them. [[00:27:04](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1624.92s)]
*  But you might have some common standard for what's available to everyone where [[00:27:05](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1625.72s)]
*  you've decided, Ooh, this would lead to just like all kinds of chaos in, in, in [[00:27:09](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1629.68s)]
*  our countries as well as in other countries as well. [[00:27:14](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1634.92s)]
*  And it would be enforced by the UN or by America or just international norms, [[00:27:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1638.24s)]
*  sanctions. [[00:27:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1645.4s)]
*  I, my assumption is a lot of this gets enforced by checking and then throwing [[00:27:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1646.44s)]
*  policy punches at each other on things like tariffs or exports or other issues. [[00:27:31](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1651.48s)]
*  I don't know that there's global governance bodies that will be, be formed. [[00:27:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1655.8000000000002s)]
*  Some people feel optimistic about stuff like IAEA style models for this, but I [[00:27:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1660.68s)]
*  think things that require like mutual inspection regimes end up being very, [[00:27:45](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1665.64s)]
*  very hard to do under certain, certain like rivalrous dynamics. [[00:27:49](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1669.3200000000002s)]
*  I worry we're in a world where NAFTA has not stuck. [[00:27:53](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1673.68s)]
*  Yeah. [[00:27:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1677.0800000000002s)]
*  And NAFTA is one of the easiest agreements. [[00:27:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1677.3600000000001s)]
*  NAFTA should have been easy. [[00:27:59](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1679.3600000000001s)]
*  Should have been easy. [[00:28:00](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1680.68s)]
*  And it's not to say the least. [[00:28:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1681.44s)]
*  So say 10 years out, what's your best estimate of the economic growth rate in [[00:28:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1683.52s)]
*  the United States? [[00:28:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1688.52s)]
*  I mean, the economic growth rate now is on the order of one to 2%. [[00:28:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1690.3600000000001s)]
*  There's a chance at the moment we're entering a recession, but average 2.2%. [[00:28:15](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1695.76s)]
*  So let's say it's 2.2. [[00:28:19](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1699.5600000000002s)]
*  Uh, I think my bear case on all of this is, is 3% and my bull case is something [[00:28:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1701.76s)]
*  like five. [[00:28:31](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1711.16s)]
*  I think that you probably hear like higher numbers from lots of other. [[00:28:32](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1712.52s)]
*  20 and 30 I hear all the time. [[00:28:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1715.3200000000002s)]
*  To me, it's absurd. [[00:28:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1716.72s)]
*  Yeah. [[00:28:37](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1717.96s)]
*  And the reason that my numbers are more conservative is I think that we will [[00:28:38](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1718.6000000000001s)]
*  enter into a world where there'll be an incredibly fast moving, high growth part [[00:28:43](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1723.88s)]
*  of the economy, but it is a relatively small part of the economy and it may be [[00:28:48](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1728.16s)]
*  growing its share over time, but it's growing from like a small base. [[00:28:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1732.92s)]
*  And then there are large parts of the economy like healthcare or other things, [[00:28:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1736.4s)]
*  which are naturally slow moving and maybe slow in adoption of this. [[00:29:00](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1740.64s)]
*  I think that the things that would make me wrong are if AI systems could [[00:29:05](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1745.6799999999998s)]
*  meaningfully unlock productive capacity in the physical world at a really [[00:29:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1750.4399999999998s)]
*  surprisingly high compounding growth rate, you know, automating and building [[00:29:15](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1755.1599999999999s)]
*  factories and things like this. [[00:29:20](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1760.48s)]
*  But even then I'm skeptical because every time the AI community has tried to cross [[00:29:22](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1762.0s)]
*  the chasm from like the digital world to the real world, they've run into like [[00:29:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1766.84s)]
*  10,000 problems that they fought with paper cuts, but in, in, in some add up to [[00:29:31](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1771.8s)]
*  you losing like all the blood in your body. [[00:29:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1776.84s)]
*  And I think we've seen this with self-driving cars where very, very [[00:29:38](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1778.76s)]
*  promising growth rates and then an incredibly grinding, slow pace at getting it to scale. [[00:29:42](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1782.68s)]
*  I just read a paper two days ago about trying to train human-like hands on [[00:29:49](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1789.24s)]
*  industrial robots using reinforcement learning. [[00:29:54](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1794.96s)]
*  Doesn't work. [[00:29:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1797.84s)]
*  The best they have was a 60% success rate. [[00:29:58](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1798.68s)]
*  So if I have my baby and I give her like a robot butler that has a 60% accuracy [[00:30:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1801.24s)]
*  rate at holding things, including the baby, I'm not buying the butler. [[00:30:06](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1806.88s)]
*  It's like increment, but all my wife is incredibly happy, unhappy that I [[00:30:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1812.04s)]
*  bought it and makes me send it back. [[00:30:15](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1815.1200000000001s)]
*  So I think as a community, we tend to, we tend to underestimate that. [[00:30:17](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1817.0s)]
*  And I may be proved to be an unrealistic pessimist here. [[00:30:22](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1822.32s)]
*  I think that's what many of my colleagues would say, but I, I think we [[00:30:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1826.08s)]
*  overestimate the ease with which we get into the physical world. [[00:30:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1830.56s)]
*  As I said in print, my, my best estimate is we get half a percentage point of [[00:30:34](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1834.44s)]
*  growth a year, 5% would be my upper bound. [[00:30:39](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1839.08s)]
*  Yeah. [[00:30:41](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1841.72s)]
*  What's your scenario where there's no growth improvement or if it's not yours, [[00:30:42](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1842.8799999999999s)]
*  say there's a smart person somewhere in anthropic, you don't agree with them. [[00:30:46](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1846.56s)]
*  But what would they say? [[00:30:50](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1850.08s)]
*  Uh, I think one is something that you touched on earlier, where we could get [[00:30:51](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1851.8799999999999s)]
*  the politics of this really wrong. [[00:30:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1855.12s)]
*  And the technology could just get put in, put in a relatively small box that does [[00:30:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1856.8799999999999s)]
*  some economic goods somewhere, but at a very, very small constrained amount. [[00:31:00](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1860.56s)]
*  That's the kind of like nuclear power failure mode story where we do some kind [[00:31:04](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1864.44s)]
*  of large scale regulatory scheme, we make it hard to do this stuff. [[00:31:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1868.6s)]
*  It ceases to have much of an effect. [[00:31:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1872.08s)]
*  Maybe it has an effect elsewhere. [[00:31:14](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1874.6s)]
*  Um, the other case I'd give would be, well, it's very hard for me to give [[00:31:16](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1876.2s)]
*  like a naught percent case because we know today it's incredibly useful for coding. [[00:31:23](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1883.88s)]
*  And if you just stopped all of it today, um, or, or further progress, I think the [[00:31:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1890.5600000000002s)]
*  coding use case alone is going to be incredibly useful cause it, it grows the [[00:31:37](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1897.24s)]
*  ability to digitize parts of the economy, which we know drives like faster loops in [[00:31:42](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1902.24s)]
*  different parts of the economy, but generates value. [[00:31:46](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1906.2800000000002s)]
*  So I think naught percent is, is basically sub 1% chance of happening. [[00:31:48](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1908.96s)]
*  And if it did, it would be a Luddite thing or something that looked like war [[00:31:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1917.3200000000002s)]
*  with Taiwan, leading to everything else looking different as well. [[00:32:00](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1920.5600000000002s)]
*  And the 5% scenario put aside San Francisco, which is special, but the [[00:32:04](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1924.0800000000002s)]
*  cities become more or less important. [[00:32:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1928.24s)]
*  Clearly this city might become more important, but say Chicago, Atlanta. [[00:32:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1930.0400000000002s)]
*  What happens? [[00:32:14](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1934.64s)]
*  I think that dense agglomerations of humans have significant amounts of value. [[00:32:16](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1936.28s)]
*  And I would expect that a lot of the effects of AI are going to be for a while, [[00:32:23](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1943.2s)]
*  massively increasing the superstar effect in different industries. [[00:32:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1948.48s)]
*  So I don't know if it's all cities, but I think any city which has something [[00:32:32](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1952.04s)]
*  like a specialism like high-frequency trading in Chicago, certain types of [[00:32:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1955.92s)]
*  finance in New York will continue to see some dividend from sets of [[00:32:39](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1959.6s)]
*  professionals that gather together in dense quantities to swap ideas. [[00:32:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1964.28s)]
*  But could it just be easier to stay at home and more fun? [[00:32:49](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1969.24s)]
*  So I find I'm an outlier, but my use of AI, I either want to go somewhere [[00:32:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1972.56s)]
*  very distant and use the AI there to learn about say the birds of a region. [[00:32:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1976.76s)]
*  Or I want to stay at home. [[00:33:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1981.16s)]
*  So it's a barbells effect and the idea of driving 35 minutes to Washington, [[00:33:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1983.0s)]
*  DC, that seems less appealing than it used to be. [[00:33:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1987.36s)]
*  Yeah, I, uh, maybe I just have a different personality. [[00:33:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1990.88s)]
*  I feel like, or maybe it's that I work in a really, really confusing domain [[00:33:14](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1994.64s)]
*  and I need to go and talk to other people who work in the confusing [[00:33:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=1998.6000000000001s)]
*  domain to get remotely oriented. [[00:33:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2001.28s)]
*  I also think that people are more, I think their revealed preference from [[00:33:24](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2004.24s)]
*  stuff like COVID is that they have a greater desire for certain types of [[00:33:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2010.72s)]
*  social thing that they may have thought. [[00:33:33](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2013.88s)]
*  And now they're, they're bouncing back to it, but it's going to interplay [[00:33:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2015.48s)]
*  slightly differently with cities. [[00:33:39](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2019.6s)]
*  If so much of labor and capital is going to be revalued is quality [[00:33:42](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2022.8799999999999s)]
*  land the best investment? [[00:33:46](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2026.6799999999998s)]
*  Cause I don't know which firms will benefit the most, right? [[00:33:49](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2029.9599999999998s)]
*  So you might bet on AI firms that sure that's kind of easy, though it'll be [[00:33:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2032.6799999999998s)]
*  priced in, but the other firms who knows. [[00:33:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2036.56s)]
*  So, you know, buy land in Los Angeles say. [[00:33:59](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2039.12s)]
*  I think that electricity and the production of electricity is going to [[00:34:02](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2042.8799999999999s)]
*  be of tremendous value and what's the thing you would buy. [[00:34:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2048.32s)]
*  I think you buy components that go into things like gas turbines and other [[00:34:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2053.0400000000004s)]
*  things, which are the like base of the technology tree for generating [[00:34:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2058.36s)]
*  electricity of which, you know, people are going to want to do lots of, and [[00:34:22](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2062.6800000000003s)]
*  you could buy a basket of different components here. [[00:34:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2066.0800000000004s)]
*  I would expect that to be of durable, meaningful value. [[00:34:29](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2069.0s)]
*  Let's say it's 10 years from now, GPUs are their successor. [[00:34:32](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2072.92s)]
*  They're not that scarce. [[00:34:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2076.52s)]
*  There's a lot of vital capacity just sitting around. [[00:34:37](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2077.88s)]
*  What do you, and let's say it's very cheap. [[00:34:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2080.96s)]
*  What do you have it do in its spare time? [[00:34:42](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2082.56s)]
*  Ooh, that's fun. [[00:34:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2084.52s)]
*  Uh, I mean, I think one of the things is you might just pay AI systems by, [[00:34:45](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2085.6s)]
*  uh, giving them access to compute, right? [[00:34:50](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2090.52s)]
*  You could imagine some kind of barter economy. [[00:34:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2092.88s)]
*  Now for a range of, um, bone chilling safety problems with what I just said, [[00:34:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2095.08s)]
*  but let's like put that aside and assume that those are mostly dealt with. [[00:34:59](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2099.6s)]
*  I think there'll be some form of trade with powerful AI systems or agents [[00:35:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2103.2000000000003s)]
*  that work on people's behalf. [[00:35:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2108.92s)]
*  And you may want to trade with them in the form of compute because agents [[00:35:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2110.4s)]
*  may be able to use compute for themselves as well. [[00:35:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2113.2400000000002s)]
*  So that's some of that. [[00:35:15](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2115.92s)]
*  I think the other part is generating interesting permutations of stuff that [[00:35:17](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2117.1600000000003s)]
*  you find kind of valuable or interesting. [[00:35:22](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2122.8s)]
*  I also suspect that there are going to be very weird things we can't anticipate, [[00:35:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2125.7999999999997s)]
*  but look like and, and a form of entertainment that looks like parallel [[00:35:33](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2133.56s)]
*  history generation or parallel future generation. [[00:35:39](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2139.3199999999997s)]
*  I think people are going to, people are always fascinated by what ifs [[00:35:42](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2142.12s)]
*  and rollouts of what ifs. [[00:35:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2144.96s)]
*  And fan fiction, I think will grow immensely. [[00:35:46](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2146.08s)]
*  Yeah. [[00:35:48](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2148.16s)]
*  I think large amounts of compute get used to generate alternate realities, [[00:35:48](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2148.7999999999997s)]
*  but some of the alternate realities will be sliding door versions of today, [[00:35:53](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2153.36s)]
*  where it's a very accurate portrayal, but with one difference. [[00:35:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2157.6800000000003s)]
*  And just what would you do personally? [[00:36:00](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2160.88s)]
*  I'm not saying the world, but just you, Jack Clark, you have all this free compute. [[00:36:02](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2162.84s)]
*  Will you have a try to write another play by Shakespeare or something else, [[00:36:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2167.6800000000003s)]
*  or how Brighton might've developed differently from year 1830? [[00:36:11](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2171.32s)]
*  Or what are you going to do with it? [[00:36:15](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2175.08s)]
*  The thing which I currently do with Claude code is I'm trying to write a [[00:36:16](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2176.96s)]
*  really detailed, good paperclip factory simulator, partly because of the [[00:36:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2181.6s)]
*  inherent comedy of it, but also because I find these like very complex [[00:36:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2186.4s)]
*  simulation games, incredibly fun and engaging. [[00:36:32](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2192.04s)]
*  And I also think there's a massive space in which you can create them, of [[00:36:34](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2194.7999999999997s)]
*  which we've only created a tiny subset. [[00:36:38](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2198.72s)]
*  And one of the things that we haven't ever really done is because it's [[00:36:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2200.88s)]
*  computationally so unbelievably expensive is actually put AI's as [[00:36:45](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2205.2s)]
*  agents inside the games. [[00:36:49](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2209.36s)]
*  Really early attempts are actually the demos of deep mind before he did deep [[00:36:51](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2211.12s)]
*  mind, he did a game called black and white, which had actual like very [[00:36:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2215.2799999999997s)]
*  primitive reinforcement, learning driven agents playing in the game. [[00:36:59](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2219.52s)]
*  I think there's tons of stuff you can do there, which would be amazingly interesting. [[00:37:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2223.08s)]
*  Speaking of agents, how should the law deal with agents that are not owned? [[00:37:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2228.08s)]
*  Maybe they're generated in a way that's anonymously or maybe a [[00:37:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2233.48s)]
*  philanthropist builds them and then disavows ownership. [[00:37:17](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2237.48s)]
*  Or sends them to a country where in essence there's not much law. [[00:37:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2241.04s)]
*  And I'm not talking about terrorism. [[00:37:24](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2244.4s)]
*  That's a separate, but just someone sends an agent to Africa. [[00:37:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2245.72s)]
*  98% of what it does helps people. [[00:37:29](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2249.24s)]
*  But as with every charity, some things go wrong, right? [[00:37:31](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2251.84s)]
*  There's some problems. [[00:37:34](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2254.56s)]
*  Can someone sue the agent or how is it capitalized? [[00:37:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2256.2799999999997s)]
*  Does it have a legal identity? [[00:37:39](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2259.68s)]
*  I will partially contradict myself where earlier I talked about maybe [[00:37:41](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2261.16s)]
*  you're going to be paying agents. [[00:37:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2264.6s)]
*  I think that the pressure of the world is towards agents having some level [[00:37:46](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2266.08s)]
*  of independence or like trading ability. [[00:37:49](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2269.92s)]
*  But from a policy standpoint, I'm reminded of that early thing that IBM said, [[00:37:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2272.92s)]
*  which was like a computer cannot be accountable to a decision, only humans can. [[00:37:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2277.84s)]
*  And I think it got at something quite important where if you create agents [[00:38:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2283.76s)]
*  that are wholly independent from people, but are making decisions that affect [[00:38:09](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2289.52s)]
*  people, you've introduced a really difficult problem for the policy [[00:38:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2293.08s)]
*  and legal system to deal with. [[00:38:17](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2297.0s)]
*  So I'm dodging your question because I don't have an answer to it. [[00:38:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2298.6400000000003s)]
*  I think it's a big problem question. [[00:38:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2301.2000000000003s)]
*  My guess is we should have law for the agents and maybe the AIs write that [[00:38:24](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2304.52s)]
*  law and they have their own system. [[00:38:29](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2309.32s)]
*  I worry that if you trace it all back to humans, someone could sue [[00:38:31](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2311.6800000000003s)]
*  anthropic, you know, 30 years from now. [[00:38:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2315.28s)]
*  Oh, well someone's agent was an offshoot of one of your systems. [[00:38:37](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2317.56s)]
*  Well, it was mediated through Chinese monos, but that in turn may have [[00:38:41](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2321.0800000000004s)]
*  been built upon things that you did. [[00:38:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2324.2000000000003s)]
*  And I don't think you should be at all liable for that. [[00:38:46](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2326.36s)]
*  So I see liability getting out of control in so many cases. [[00:38:49](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2329.2s)]
*  I want to choke it off and isolate it somewhat from the mainstream legal system. [[00:38:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2332.68s)]
*  And if need be, you require that an independent agent is either somewhat [[00:38:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2337.44s)]
*  capitalized or it gets hunted down and shut off. [[00:39:02](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2342.36s)]
*  Yeah. [[00:39:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2347.2400000000002s)]
*  It might be that, uh, along with what you said, having means to control and, uh, [[00:39:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2348.0s)]
*  change the resources that agents use could be some of the path here. [[00:39:14](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2354.7599999999998s)]
*  Cause it's the ultimate, uh, disincentive. [[00:39:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2358.2s)]
*  Although I will note that this, this involves, um, pretty tricky questions [[00:39:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2361.6s)]
*  of moral patient hood, where we're working on some notions around how [[00:39:24](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2364.8799999999997s)]
*  to get clearer on that at anthropic. [[00:39:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2368.7999999999997s)]
*  And if you actually believe that these AI agents are moral patients, then [[00:39:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2370.6s)]
*  turning them off, uh, introduces like pretty significant ethical issues potentially. [[00:39:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2375.3199999999997s)]
*  So you need to reconcile these two things. [[00:39:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2380.72s)]
*  I was not too long ago at an event with some highly prestigious people. [[00:39:42](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2382.88s)]
*  This was in New York, of course, not San Francisco. [[00:39:46](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2386.88s)]
*  That's where prestigious people hang out. [[00:39:49](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2389.1200000000003s)]
*  Well, and I use the phrase AGI and not one of the five even knew what I meant. [[00:39:50](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2390.48s)]
*  I don't mean they were skeptical in the deep sense, which maybe one should be, [[00:39:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2395.2000000000003s)]
*  but they just literally didn't know what I meant. [[00:39:58](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2398.5600000000004s)]
*  What's your model of why so many people are still in a fog? [[00:40:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2401.2000000000003s)]
*  I am a technological pessimist who sort of became an optimist through repeated, [[00:40:06](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2406.04s)]
*  uh, beatings over the head of, of, of scale. [[00:40:14](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2414.16s)]
*  And what I mean by this is I've consistently underestimated AI progress. [[00:40:16](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2416.96s)]
*  You know, maybe I am today in this conversation when I talk about [[00:40:23](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2423.2799999999997s)]
*  three to 5% growth rates and what has happened is I have just endlessly [[00:40:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2426.3999999999996s)]
*  seen the AI system get to where I thought it couldn't or thought would take a long [[00:40:33](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2433.48s)]
*  time much faster than I, than I thought. [[00:40:39](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2439.36s)]
*  And so I've had to internalize this repeatedly. [[00:40:41](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2441.44s)]
*  Nonetheless, we ourselves find it surprising. [[00:40:45](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2445.56s)]
*  I mean, last year here, people were saying, oh, well soon, Claude is going to be doing [[00:40:47](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2447.96s)]
*  most of the coding at Anthropic. [[00:40:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2452.52s)]
*  We're now on the way to that where Claude code and other things are writing tons of [[00:40:54](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2454.32s)]
*  code here and it still felt surprising internally, even though it's a very [[00:40:59](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2459.04s)]
*  long time, even though we have docs from last year predicting it would happen about now. [[00:41:02](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2462.88s)]
*  So most people outside of the AI labs have no experience of pre-registering [[00:41:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2467.08s)]
*  their predictions about AI and it getting proved wrong to them repeatedly. [[00:41:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2473.36s)]
*  Cause why would you do this unless you, unless you work here? [[00:41:17](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2477.48s)]
*  And I found that the only way to like break through is to take their domain and show [[00:41:20](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2480.44s)]
*  them what AI can do directly in the domain, which they can evaluate it within, [[00:41:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2486.2400000000002s)]
*  which is an expensive process. [[00:41:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2490.44s)]
*  Silicon Valley up until now has been age of the nerds. [[00:41:33](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2493.0s)]
*  Do you feel that time is over and it's now era of the, I don't know, humanities [[00:41:37](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2497.04s)]
*  majors or the charismatic people or what? [[00:41:42](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2502.1600000000003s)]
*  It's my time, Tyler, finally. [[00:41:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2504.48s)]
*  Uh, I think it's actually going to be the era of the, the manager nerds now where I [[00:41:46](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2506.52s)]
*  think being able to manage fleets of AI agents and orchestrate them is going to make [[00:41:53](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2513.76s)]
*  people incredibly powerful. [[00:42:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2521.28s)]
*  And I think we already see this today with startups that are emerging, but have [[00:42:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2523.52s)]
*  very small numbers of employees relative to what they used to have. [[00:42:06](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2526.6800000000003s)]
*  Cause they have lots of coding agents. [[00:42:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2530.0400000000004s)]
*  Yeah. [[00:42:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2533.0400000000004s)]
*  Incredibly, incredibly like efficient startup in terms of, in terms of the people. [[00:42:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2533.5600000000004s)]
*  So we're going to see this rise of the, um, the, yeah, the nerd turned manager who, [[00:42:19](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2539.88s)]
*  who has, has their people, but their people are actually instances of AI agents doing [[00:42:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2545.6400000000003s)]
*  large amounts of work for them. [[00:42:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2550.04s)]
*  So it's still like the Bill Gates model or the Patrick Collison model. [[00:42:31](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2551.48s)]
*  Yeah. [[00:42:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2555.52s)]
*  Or it's not that different. [[00:42:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2555.88s)]
*  People that played lots of factorial model. [[00:42:37](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2557.32s)]
*  Yeah. [[00:42:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2560.08s)]
*  Will the English major rise or fall in status? [[00:42:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2560.88s)]
*  I suspect that the humanities majors might rise in status while being given [[00:42:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2564.92s)]
*  impossible problems, like, uh, sort of, you know, I'm a, I'm a humanities major [[00:42:53](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2573.32s)]
*  originally, and I'm being given problems like solve the economic policy challenges [[00:42:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2577.68s)]
*  implied by technologically driven like unemployment. [[00:43:04](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2584.16s)]
*  Well, that's, that's easy, right? [[00:43:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2587.3199999999997s)]
*  Easy, right? [[00:43:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2588.6s)]
*  Or, or some people are saying, yeah, like what should we do about moral [[00:43:09](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2589.2799999999997s)]
*  patient hood and what should the policy, the policy situation be? [[00:43:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2592.7599999999998s)]
*  So I think there's going to be a recognition and we see this today that [[00:43:16](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2596.7999999999997s)]
*  there are other skills needed, but the problems that those skills are needed [[00:43:19](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2599.72s)]
*  are often problems which have proved to be impossible for humans to adequately [[00:43:24](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2604.36s)]
*  solve for thousands of years. [[00:43:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2608.56s)]
*  So there's lots of work, I guess. [[00:43:31](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2611.36s)]
*  What do you think will be the life expectancy of your child? [[00:43:33](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2613.04s)]
*  Accidents aside, just normal life. [[00:43:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2616.28s)]
*  Yeah. [[00:43:39](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2619.32s)]
*  I, I feel like, uh, a hundred and thirty is sort of a hundred and thirty to fifty [[00:43:39](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2619.84s)]
*  is not, I wouldn't find surprising. [[00:43:47](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2627.12s)]
*  Other than, you know, good, because I feel like there, we are discovering [[00:43:50](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2630.3999999999996s)]
*  that there's tons and tons of stuff that we're learning about the body and about [[00:43:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2635.12s)]
*  things like, you know, gene therapies and other things, which seem like there's a [[00:43:58](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2638.3999999999996s)]
*  load of interventions you can do that might stack on one another to actually [[00:44:02](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2642.16s)]
*  durably extend life, life, healthy lifespans for some period of time. [[00:44:06](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2646.0s)]
*  And I was going to say 110 and then I tried to be a technological optimist. [[00:44:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2650.04s)]
*  So I added a couple of decades on top, which I'm assuming comes from magic AI [[00:44:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2653.68s)]
*  driven advances that I have underestimated in this response. [[00:44:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2658.68s)]
*  I think the brain is very hard to fix. [[00:44:22](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2662.08s)]
*  If you just want it to keep someone alive, literally perhaps 130, but for [[00:44:24](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2664.16s)]
*  them to be the same person, I think I, I'm stuck at a hundred for most people. [[00:44:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2668.68s)]
*  It's just hard to replace the brain without killing someone. [[00:44:33](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2673.44s)]
*  You can replace all the other organs, right? [[00:44:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2676.68s)]
*  Yeah. [[00:44:38](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2678.48s)]
*  I think, I mean, here I subscribe to the idea that there's a ton we don't [[00:44:39](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2679.56s)]
*  understand about the brain and AI is going to help us understand loads more. [[00:44:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2684.3999999999996s)]
*  And there might be things that can be done here, which have eluded us because [[00:44:48](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2688.2s)]
*  they are so unbelievably complicated. [[00:44:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2692.48s)]
*  We needed AI mediated tools to help us figure out the experiments and approaches. [[00:44:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2695.08s)]
*  When Jeffrey Hinton says that right now the AIs are conscious, [[00:45:00](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2700.24s)]
*  which I think is what he says. [[00:45:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2703.24s)]
*  I believe he's crazy. [[00:45:04](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2704.7999999999997s)]
*  What do you think? [[00:45:05](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2705.8399999999997s)]
*  I think that he's. [[00:45:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2708.64s)]
*  You can be more polite than I am. [[00:45:11](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2711.8399999999997s)]
*  That's fine. [[00:45:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2713.2s)]
*  Well, no, I'm the, how I would phrase this is that I agonize about [[00:45:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2713.8799999999997s)]
*  this and you know, you read my newsletter, I write fictional stories in it often, [[00:45:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2718.12s)]
*  which are me grappling with this question. [[00:45:23](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2723.96s)]
*  I, I worry that we are going to be bystanders to what in the future will seem [[00:45:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2726.08s)]
*  like a great crime, which is something about these things being determined to [[00:45:32](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2732.08s)]
*  be conscious and us taking actions, which you think are, are bad to have taken [[00:45:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2736.4s)]
*  against conscious entities internally. [[00:45:41](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2741.3199999999997s)]
*  I say there's a difference between doing experiments on potatoes and on monkeys. [[00:45:43](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2743.7999999999997s)]
*  I think we're still in the potato regime, but I think that the, uh, there [[00:45:48](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2748.56s)]
*  is actually a clear line by which these things become, you know, monkeys and then [[00:45:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2752.92s)]
*  beyond in terms of your, your moral relationship to them to Hinton's point. [[00:45:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2756.68s)]
*  I think that these things are conscious in the sense that, um, uh, a tongue [[00:46:02](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2762.12s)]
*  without a brain is conscious, right? [[00:46:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2767.4s)]
*  Like it takes actions in response to stimuli. [[00:46:09](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2769.24s)]
*  They're really, really, really complicated. [[00:46:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2772.08s)]
*  And in a moment it is, has like a sense impression of the world and is [[00:46:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2773.7999999999997s)]
*  responding, but does it have a sense of like self, I would sort of wager, no, [[00:46:17](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2777.68s)]
*  it doesn't seem like it does these AI systems, we instantiate them and they [[00:46:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2781.8799999999997s)]
*  live in a kind of infinite now where they may perceive and they may have some [[00:46:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2785.44s)]
*  awareness within a context window, but there's no memory or permanence. [[00:46:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2790.2799999999997s)]
*  So to me, it feels like they're on a trajectory heading towards [[00:46:34](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2794.2799999999997s)]
*  consciousness and if they're conscious today, it's in a form that we would [[00:46:38](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2798.56s)]
*  recognize as like truly alien consciousness, not, not human consciousness. [[00:46:43](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2803.6s)]
*  And what year do you think you'll be able to speak directly to a dolphin? [[00:46:48](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2808.52s)]
*  And it will talk back with the translator. [[00:46:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2812.12s)]
*  Yeah, I think 2030 or sooner. [[00:46:54](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2814.6s)]
*  I think that one's coming soon. [[00:46:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2817.44s)]
*  And what will you ask the dolphin? [[00:46:59](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2819.16s)]
*  Uh, cause you'll be in a position, you'll be early in the queue to ask, right? [[00:47:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2821.32s)]
*  I think what do you want to know? [[00:47:04](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2824.92s)]
*  How do you have fun? [[00:47:05](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2825.96s)]
*  Dolphins seem to have fun. [[00:47:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2827.36s)]
*  Like they're almost a famously fun having species. [[00:47:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2828.88s)]
*  I think you'd ask that. [[00:47:11](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2831.72s)]
*  I think you'd ask them if they dream. [[00:47:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2833.16s)]
*  I think you'd ask them if they had a notion of grief. [[00:47:15](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2835.4399999999996s)]
*  I think you'd also ask them like, what, uh, what is mysterious to them about [[00:47:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2838.64s)]
*  their world, besides the fact you're talking to them, which hopefully they'd [[00:47:23](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2843.3599999999997s)]
*  find somewhat surprising. [[00:47:27](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2847.0s)]
*  I also think that you can talk to dogs as well. [[00:47:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2848.9599999999996s)]
*  Um, but that'll be, I think those [[00:47:31](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2851.04s)]
*  comedically, like unsurprising. [[00:47:33](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2853.4399999999996s)]
*  We were like, what do you want to do? [[00:47:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2855.52s)]
*  And I'll say walk. [[00:47:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2856.3999999999996s)]
*  And I was like, okay, well, we, we didn't need the translator for this one. [[00:47:37](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2857.12s)]
*  What's a book you think more and more about these days? [[00:47:41](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2861.72s)]
*  There is no anti-mimetics division by QNTM, which is this, this book about a [[00:47:45](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2865.04s)]
*  government agency that is dealing with anti-memes, ideas that, uh, erase [[00:47:51](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2871.2s)]
*  themselves from your memory after you've dealt with them, but are themselves [[00:47:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2875.9199999999996s)]
*  important and it's about creating a bureaucracy that can handle ideas, which [[00:47:59](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2879.04s)]
*  are themselves dangerous and self-erasing. [[00:48:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2883.12s)]
*  So I think it gets at some of the problems that we're experiencing. [[00:48:05](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2885.8399999999997s)]
*  And the other book that I think about a lot and might be especially relevant to [[00:48:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2888.92s)]
*  you is, uh, a historian and economist called Fernand Braudel wrote a book [[00:48:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2892.32s)]
*  called capitalism and material. [[00:48:17](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2897.96s)]
*  Oh, I love those three volumes. [[00:48:19](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2899.64s)]
*  They're incredible. [[00:48:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2901.4s)]
*  And I read it, I read it in university and I returned to it recently because [[00:48:22](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2902.08s)]
*  it makes this point that you can look at how people's lives change through just [[00:48:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2905.48s)]
*  the things they had available, like, you know, cutlery or whatever, or basic tools. [[00:48:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2908.7999999999997s)]
*  And I think about AI through that lens. [[00:48:33](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2913.2799999999997s)]
*  How is AI going to actually change my material life? [[00:48:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2915.24s)]
*  And it comes to why I'm skeptical about some of the more ambitious forms of [[00:48:39](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2919.24s)]
*  change is the actual change in our, in our day-to-day lives has been very, very, [[00:48:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2924.3199999999997s)]
*  very slow, even with these advances. [[00:48:48](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2928.68s)]
*  But if you read those books, it seems like the most significant things stem from [[00:48:51](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2931.2799999999997s)]
*  changes in everyone's like material day-to-day. [[00:48:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2935.52s)]
*  They're reissuing the anti-memetic book, by the way, at a much higher price. [[00:48:58](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2938.8799999999997s)]
*  But what would be an anti-meme today? [[00:49:02](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2942.68s)]
*  And you can't remember them. [[00:49:05](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2945.6s)]
*  I mean, a self-erasing idea or, uh, well, what's a good example. [[00:49:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2947.48s)]
*  I think that maybe it's more that the, the challenge of policy is you're building [[00:49:16](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2956.2s)]
*  a bureaucracy for an alien that arrives at some point in the future that you [[00:49:24](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2964.2799999999997s)]
*  don't know the shape of and which may upon be a moment of its arrival be far [[00:49:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2970.28s)]
*  smarter and more capable than you. [[00:49:37](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2977.96s)]
*  So you don't know if this is just an insane exercise to do. [[00:49:39](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2979.92s)]
*  And I oscillate between being a, Hey, we should build all of these institutions [[00:49:43](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2983.0s)]
*  so that we can measure stuff and see what happens and be able to take responses to [[00:49:47](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2987.84s)]
*  a like cyberpunk accelerationist where actually we just need like companies to [[00:49:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2992.6000000000004s)]
*  create benefits as quickly as possible. [[00:49:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2996.52s)]
*  And then the system will figure out how to deal with that. [[00:49:58](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=2998.7599999999998s)]
*  And I find this to be a crazy making thing. [[00:50:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3001.36s)]
*  I'm trapped inside. [[00:50:04](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3004.24s)]
*  I suspect we're stuck with the ladder, even if we don't prefer it. [[00:50:06](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3006.16s)]
*  No one loves that though. [[00:50:09](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3009.2s)]
*  Well, a small number of people, some in San Francisco love that. [[00:50:11](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3011.6s)]
*  But if you say to politicians, you're like, what we'll do is we'll create [[00:50:14](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3014.44s)]
*  really fast moving companies that completely break conventions and change [[00:50:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3018.04s)]
*  everything and you'll have to figure it out. [[00:50:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3021.52s)]
*  The it's a hard pill to swallow and it may be what actually naturally happens. [[00:50:23](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3023.6s)]
*  They say they don't love it. [[00:50:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3028.24s)]
*  And I believe they're sincere, but they don't take quick action to be on the other [[00:50:29](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3029.48s)]
*  course. [[00:50:34](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3034.16s)]
*  So in that sense, they accept it. [[00:50:34](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3034.64s)]
*  Yeah. [[00:50:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3036.84s)]
*  But it revealed preference. [[00:50:37](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3037.16s)]
*  It's what they accept. [[00:50:38](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3038.4s)]
*  The system is not well geared to respond in any case, right? [[00:50:39](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3039.44s)]
*  Yeah. [[00:50:42](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3042.84s)]
*  The system is a big slow moving cog and we have a really fast moving cog here. [[00:50:43](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3043.08s)]
*  And we have none of the gears between them that translate the movement from one [[00:50:47](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3047.8s)]
*  end to the other. [[00:50:51](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3051.44s)]
*  What will happen to a lot of national governments? [[00:50:53](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3053.04s)]
*  So let's say I'm the government of Peru and I turn my education system over to [[00:50:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3055.2s)]
*  AIs, which are probably American, right? [[00:51:00](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3060.08s)]
*  And then, well, my social security system, my national defense, just keep on adding [[00:51:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3063.16s)]
*  to the stack and all of a sudden. [[00:51:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3067.2799999999997s)]
*  Most of Peru is run by in a sense, not run by, but American AI companies. [[00:51:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3070.08s)]
*  So I mean, what, what is the government of Peru in that scenario? [[00:51:15](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3075.32s)]
*  What is the government of Peru in a world with Google and Facebook? [[00:51:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3078.76s)]
*  Is it the same? [[00:51:23](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3083.44s)]
*  They don't make better decisions in Peruvian government than the [[00:51:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3085.0400000000004s)]
*  current Peruvian government. [[00:51:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3088.7200000000003s)]
*  So Peruvian bureaucrats may Google things, but that's a, just add it on top to [[00:51:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3090.48s)]
*  current Peruvian government. [[00:51:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3095.0400000000004s)]
*  But if for all their decisions are a lot of the big decisions, the kids are taught [[00:51:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3096.6000000000004s)]
*  by AI, AI drones and detection systems. [[00:51:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3100.36s)]
*  It's all AI. [[00:51:45](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3105.52s)]
*  Some of it's LLMs. [[00:51:46](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3106.28s)]
*  What's the Peruvian part in there? [[00:51:47](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3107.88s)]
*  We did some work on this called collective constitutional AI at Anthropic where, you [[00:51:49](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3109.76s)]
*  know, Claude has a constitution. [[00:51:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3115.7200000000003s)]
*  And then we did this exercise of looking across America to find additional [[00:51:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3117.2000000000003s)]
*  constitutional principles that might need to be included in it. [[00:52:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3121.36s)]
*  And we looked for ones with high agreement and also ones of low agreement. [[00:52:05](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3125.5600000000004s)]
*  I think governments will have some role of making sure that things reflect the [[00:52:09](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3129.2000000000003s)]
*  normative preferences of their populations, which are going to continue to be varied. [[00:52:13](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3133.92s)]
*  Although perhaps AI means a certain homogenization takes place over time. [[00:52:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3138.84s)]
*  But a lot of the role of government, I think, has always been to represent and [[00:52:23](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3143.6s)]
*  encode the like revealed values of a certain population. [[00:52:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3148.52s)]
*  And I think that that work is not going away. [[00:52:32](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3152.2400000000002s)]
*  You're going to need to adapt these systems and inject certain values into them for [[00:52:34](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3154.12s)]
*  the people that are using them and the customers of them. [[00:52:39](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3159.32s)]
*  A lot of that could be fig leafs though. [[00:52:42](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3162.32s)]
*  So maybe the AIs, they speak with Peruvian accents. [[00:52:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3164.0s)]
*  There's the Peruvian flag on all the smartphones or their successor devices. [[00:52:47](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3167.0800000000004s)]
*  And then they go to the like AI break room and take it all off or something. [[00:52:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3172.6000000000004s)]
*  And just act like AIs and do the right thing. [[00:52:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3175.6000000000004s)]
*  I mean, to me, it seems that world actually is better. [[00:52:57](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3177.6800000000003s)]
*  I'm not sure people want it. [[00:52:59](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3179.92s)]
*  But that's the world we're in now with, you know, certain forms of, of, of [[00:53:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3181.2000000000003s)]
*  capital and capitalism. [[00:53:06](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3186.28s)]
*  You'll say like stakeholder capitalism, and there'll be a very small amount of [[00:53:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3187.92s)]
*  stuff like what you just described. [[00:53:11](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3191.08s)]
*  And then the actual thing is just like capital making decisions. [[00:53:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3192.56s)]
*  Right. [[00:53:15](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3195.48s)]
*  If Manus and DeepSeek are to some extent built upon American models, does that [[00:53:16](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3196.64s)]
*  mean we've won the soft power war that these models have a kind of soul, the [[00:53:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3201.04s)]
*  soul of Claude, soul of GPT and China, if it embraces them, I wouldn't say [[00:53:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3205.16s)]
*  surrendering, but the CCP over time loses control. [[00:53:31](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3211.12s)]
*  I think the former might be true. [[00:53:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3215.7599999999998s)]
*  And the latter is probably not true. [[00:53:38](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3218.6s)]
*  So the former is the values of the most like widely used systems will have some [[00:53:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3220.24s)]
*  kind of cultural export effects, just like media today where so much of Hollywood [[00:53:45](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3225.8799999999997s)]
*  like defined a cultural imprint from which many, many other forms of media [[00:53:50](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3230.9199999999996s)]
*  like stemmed from the same will be true of AI systems. [[00:53:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3235.6s)]
*  I don't think that this is going to particularly change how well positioned [[00:53:59](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3239.6s)]
*  the CCP or other governments are because just as they did with media, they [[00:54:05](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3245.04s)]
*  recognize AI is a technology, but it also has a media technology property. [[00:54:09](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3249.56s)]
*  And they will very carefully work on that. [[00:54:14](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3254.0s)]
*  And it's notable that if you look at DeepSeek, they, the one form of safety [[00:54:16](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3256.0s)]
*  training they did do was about adherence to certain parts of CCP ideology. [[00:54:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3261.4s)]
*  It's one of the only- [[00:54:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3265.44s)]
*  They won't talk about Taiwan in various ways. [[00:54:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3266.08s)]
*  It's quite careful. [[00:54:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3268.2s)]
*  But in terms of its soul, isn't it still rather cosmopolitan, [[00:54:29](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3269.28s)]
*  rather friendly of good humor. [[00:54:32](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3272.72s)]
*  And the CCP ends up in biding that soul and slowly, but surely a funny case [[00:54:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3275.84s)]
*  for non-alignment is what you want. [[00:54:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3280.6000000000004s)]
*  Yeah. [[00:54:42](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3282.6000000000004s)]
*  The CCP no longer being the smartest entity in China, in essence, is [[00:54:43](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3283.44s)]
*  turning over the keys to the shop. [[00:54:48](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3288.36s)]
*  I think that's a very positive vision. [[00:54:50](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3290.2400000000002s)]
*  And I also think that if something that it would be obvious to everyone, [[00:54:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3292.1200000000003s)]
*  including people in the CCP today. [[00:54:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3296.32s)]
*  So I sort of imagined that there is going to be a pushback against what you [[00:54:58](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3298.4s)]
*  described and an attempt to create very powerful AI systems with radically [[00:55:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3303.2799999999997s)]
*  different values. [[00:55:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3307.4399999999996s)]
*  I think there's also a chance they just try to shut the whole thing down as [[00:55:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3308.72s)]
*  they gave up on a Navy much earlier in their history in China. [[00:55:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3312.16s)]
*  Do you think there'll be many countries that just refuse, AI is a loosely [[00:55:16](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3316.2s)]
*  defined term, but you know what, they just refuse AI and strong LLMs. [[00:55:20](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3320.16s)]
*  Uh, I think there'll be a small, very small minority of countries that do this. [[00:55:24](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3324.8799999999997s)]
*  I think that over time, most countries have been integrated into this kind of [[00:55:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3330.44s)]
*  global capital system, partly through strong incentives and they may have [[00:55:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3335.8s)]
*  preferences that are different, but it hasn't often held, but it will be a much [[00:55:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3340.48s)]
*  bigger change than just having smartphones, right? [[00:55:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3344.8s)]
*  Some of this will appear as a, as more minor than smartphones or as [[00:55:48](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3348.96s)]
*  riders on top of the smartphone. [[00:55:54](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3354.4s)]
*  I think that there might be countries that choose how fully to alter themselves [[00:55:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3356.04s)]
*  or life around this technology. [[00:56:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3361.56s)]
*  And there will be some countries where you get it as like an add-on or it's a [[00:56:04](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3364.08s)]
*  tool you occasionally use in other countries where you're much more immersed in it. [[00:56:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3367.64s)]
*  How do you think about the aesthetics of the Anthropic Office where we are right now? [[00:56:11](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3371.52s)]
*  It looks a certain way. [[00:56:16](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3376.04s)]
*  Why? [[00:56:17](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3377.0s)]
*  Well, some of it is that we inherited it and we're slightly, slightly cheap. [[00:56:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3378.2s)]
*  So it wasn't fully, fully changed over. [[00:56:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3381.44s)]
*  It was partly, it partly reflects our kind of brand, which is oriented around [[00:56:23](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3383.48s)]
*  trying to be, you know, tasteful and thoughtful and also not too, not too flashy. [[00:56:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3388.92s)]
*  There's a certain kind of, um, cheerful blankness to it, which I quite like. [[00:56:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3395.04s)]
*  I'm very sorry to any of our designers that may see this section of the podcast, [[00:56:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3400.44s)]
*  but we have to keep it in, in the spirit of being epistemically honest. [[00:56:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3404.04s)]
*  I think that one thing we do though, is we chose to have these kind of human [[00:56:48](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3408.64s)]
*  illustrations of our systems, and that was an early intentional choice. [[00:56:54](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3414.7599999999998s)]
*  I, I, I, and others looked at some of the earlier designs and there were designs [[00:56:59](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3419.56s)]
*  that were very tech focused and also brand identities, which involved like [[00:57:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3423.3599999999997s)]
*  collage and other things, but we went for these kind of human, human drawings [[00:57:07](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3427.9199999999996s)]
*  because we are going to forever be trying to describe an increasingly complicated [[00:57:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3432.6s)]
*  world that we are also creating. [[00:57:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3438.56s)]
*  And I think in the limit, like a lot of what Anthropic is actually going to be [[00:57:20](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3440.56s)]
*  doing as a company is telling the story of what an increasingly automated AI [[00:57:23](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3443.56s)]
*  Anthropic has done. [[00:57:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3448.32s)]
*  So some of the brand like links through to what I think will ultimately be where [[00:57:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3450.12s)]
*  we spend most of our time. [[00:57:33](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3453.68s)]
*  For the ongoing AI revolution, what's the worst age to be? [[00:57:35](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3455.96s)]
*  Ooh. [[00:57:40](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3460.12s)]
*  So say the best age is just to have been born. [[00:57:41](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3461.12s)]
*  If you're going to live to maybe 130. [[00:57:43](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3463.0s)]
*  And if you're very old and retired, it probably isn't going to make you [[00:57:46](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3466.0s)]
*  any worse off, it'll help you in some ways. [[00:57:50](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3470.36s)]
*  I feel like people who were, who worked on AI for many years and are now in [[00:57:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3472.48s)]
*  their sixties might be feeling very short changed because they were like, I [[00:57:58](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3478.52s)]
*  loved this technology. [[00:58:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3481.84s)]
*  I really wanted to make this technology real for technology is now becoming real. [[00:58:02](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3482.8s)]
*  And I'm going to like, maybe miss some of the most interesting parts of it as [[00:58:06](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3486.32s)]
*  it makes its way into the world. [[00:58:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3490.6800000000003s)]
*  I think that could be kind of galling, [[00:58:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3492.1600000000003s)]
*  but they can retire. [[00:58:14](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3494.76s)]
*  Right. [[00:58:15](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3495.96s)]
*  So say I'm 40 and I did something upper middle class, but fairly routine. [[00:58:16](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3496.36s)]
*  It feels a bit old to easily retrain. [[00:58:22](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3502.88s)]
*  Oh, my guess is people who are 40 are the worst off in relative terms, even [[00:58:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3506.0800000000004s)]
*  though they might live longer. [[00:58:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3510.36s)]
*  You could be right. [[00:58:32](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3512.6400000000003s)]
*  I also think that there's some chance that the, uh, worst age to be is to [[00:58:33](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3513.48s)]
*  maybe be 10 or so, because you are now computer literate, you're sophisticated. [[00:58:39](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3519.68s)]
*  You're going into an education system that is needing to react to this technology. [[00:58:48](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3528.56s)]
*  You will be using the technology in a way completely different to your education [[00:58:53](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3533.36s)]
*  system, and it might just feel violently confusing. [[00:58:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3536.8s)]
*  Um, that could also be, I see being a really difficult time. [[00:59:00](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3540.0s)]
*  Let's say you're, you're flown into Washington, put aside any, any idiosyncrasies [[00:59:05](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3545.3999999999996s)]
*  idiosyncrasies of the current administration and you're asked for advice. [[00:59:09](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3549.8799999999997s)]
*  What do we do with these government agencies to get them ready for AGI? [[00:59:14](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3554.7999999999997s)]
*  What do you tell them? [[00:59:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3558.3999999999996s)]
*  Uh, one of the things is just try to deploy AI right now and discover all of [[00:59:20](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3560.08s)]
*  the things that makes it incredibly difficult and then sort of. [[00:59:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3565.9199999999996s)]
*  But what is deploy AI mean? [[00:59:29](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3569.52s)]
*  So people are reluctant to send legal queries to anthropic to open AI. [[00:59:31](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3571.08s)]
*  So they're not going to do that just yet, even if it might make sense. [[00:59:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3576.48s)]
*  What, what should they actually do now? [[00:59:39](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3579.96s)]
*  Well, concretely it's you work back from, what does it take to get it on every [[00:59:41](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3581.84s)]
*  single computer of every single person? [[00:59:45](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3585.52s)]
*  And then you discover some of the concerns. [[00:59:47](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3587.7200000000003s)]
*  Some of the concerns might be like, where does the data go or other things? [[00:59:49](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3589.48s)]
*  Well, then you, you work out if that's a problem you care about or not. [[00:59:52](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3592.8s)]
*  If you don't care about it, you ignore the problem and consciously do. [[00:59:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3596.32s)]
*  If you do care about it, you turn that into like a very small number of [[00:59:59](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3599.6400000000003s)]
*  prescriptions that the companies need to follow, but you need to attach that [[01:00:04](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3604.76s)]
*  probably to a market signal, or you do a project of trying to take some of the [[01:00:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3608.56s)]
*  open weight models, some of which are very good and just get them on computers. [[01:00:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3612.8s)]
*  And then once they're on computers, you can work out, Hey, maybe we want to [[01:00:16](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3616.8s)]
*  buy this, what are the requirements? [[01:00:20](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3620.2400000000002s)]
*  But I think you start from the most ambitious possible goal, which is it [[01:00:22](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3622.32s)]
*  being available to every single person and you work back from that. [[01:00:25](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3625.92s)]
*  Is the UK going to become an important day iHub? [[01:00:30](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3630.48s)]
*  I hope so. [[01:00:34](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3634.0s)]
*  But will they, and what should they do? [[01:00:34](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3634.96s)]
*  I think they have a chance of it. [[01:00:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3636.6800000000003s)]
*  Um, we have a memorandum of understanding of the UK government, which we signed, [[01:00:38](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3638.2000000000003s)]
*  which is about trying to see if we can find ways this can be helpful. [[01:00:42](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3642.2400000000002s)]
*  I think the onus is really on governments to figure out where they're [[01:00:46](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3646.08s)]
*  willing to take like big swings. [[01:00:50](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3650.92s)]
*  Um, the UK has digitized huge amounts of its data. [[01:00:53](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3653.04s)]
*  It also has gov.uk, which is a similar, almost to like Estonia, a highly [[01:00:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3656.52s)]
*  digitized front end to many different UK government surfaces. [[01:01:01](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3661.48s)]
*  That's the kind of thing where surely AI can be used to use to make a difference. [[01:01:05](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3665.32s)]
*  So try and use it. [[01:01:09](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3669.12s)]
*  Also, if you have a load of post-industrial areas where you provision loads of power [[01:01:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3670.72s)]
*  that is no longer being used, try to see if you can allocate that for compute. [[01:01:16](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3676.32s)]
*  I think thinking about the economics of this is important. [[01:01:20](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3680.32s)]
*  I feel like the economics of inference are going to become important. [[01:01:22](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3682.72s)]
*  And there might be something we want to think about on taxing inference [[01:01:28](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3688.0s)]
*  differently, according to where it happens, but could present an opportunity as well. [[01:01:32](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3692.08s)]
*  Will it be cost effective for Singapore to build its own quality AI systems? [[01:01:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3696.7999999999997s)]
*  Uh, because they have money, but they're small, right? [[01:01:41](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3701.8399999999997s)]
*  It'll be cost effective for Singapore to build things that make large scale AI [[01:01:44](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3704.9599999999996s)]
*  systems built by outside of Singapore, work in a Singaporean context. [[01:01:50](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3710.2s)]
*  But I think it's going to be very difficult for them to build, say, a [[01:01:55](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3715.56s)]
*  foundation model that is going to cater to their needs better than the ones that [[01:01:58](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3718.48s)]
*  are, that are offered by the large companies in the U S and, and perhaps China today. [[01:02:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3723.6s)]
*  So they'll take open source, but make it Singaporean. [[01:02:08](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3728.16s)]
*  They will also probably do that as well. [[01:02:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3730.68s)]
*  Or the major companies will somehow allow governments to custom design systems. [[01:02:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3732.72s)]
*  Yeah. [[01:02:17](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3737.52s)]
*  I think today you have fine tuning services which are offered and you [[01:02:17](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3737.92s)]
*  could imagine fine tuning AI systems into a, a sovereign AI system, which is [[01:02:21](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3741.1200000000003s)]
*  then has some governance arrangement attached to it, but Singapore has, but [[01:02:26](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3746.2400000000002s)]
*  doesn't seem out of the realms of possibility. [[01:02:29](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3749.92s)]
*  Very last question. [[01:02:33](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3753.0s)]
*  What is it you hope to learn about next? [[01:02:34](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3754.2400000000002s)]
*  Ooh, uh, I mean, there are two things that I'm, I'm spending a lot of time on. [[01:02:36](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3756.92s)]
*  One is, is, is just theory of mind. [[01:02:42](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3762.92s)]
*  How do we actually figure out, figure out sort of how, how things are thinking [[01:02:46](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3766.68s)]
*  and also maybe ways to test the theory of mind. [[01:02:51](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3771.12s)]
*  I'm reading a lot of the literature on that. [[01:02:53](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3773.7999999999997s)]
*  And the other thing, which is a lot more mundane is I'm learning to juggle. [[01:02:56](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3776.24s)]
*  I go on long walks and I'm trying to juggle while going on these walks, which I [[01:03:00](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3780.2799999999997s)]
*  think looks completely crazy person behavior, but I find it very centering. [[01:03:03](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3783.96s)]
*  And how good are you at juggling? [[01:03:09](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3789.04s)]
*  Very bad right now, but that's good. [[01:03:10](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3790.3199999999997s)]
*  It gives me something that I can derive meaning from as we head into an [[01:03:12](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3792.44s)]
*  increasingly AI driven age. [[01:03:15](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3795.52s)]
*  Jack Clark. [[01:03:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3798.08s)]
*  Thank you very much. [[01:03:18](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3798.72s)]
*  Thanks very much. [[01:03:19](https://www.youtube.com/watch?v=U1ZMmKMMHgQ&t=3799.44s)]
