---
Date Generated: May 30, 2025
Transcription Model: whisper medium 20231117
Length: 2126s
Video Keywords: ['peter diamandis', 'longevity', 'xprize', 'abundance']
Video Views: 27330
Video Rating: None
Video Description: In this episode, recorded during Abundance360 2024, Peter and Eric discuss AI policy, government struggles, and AI’s global impact.  
Eric Schmidt is best known as the CEO of Google from 2001-2011, including as the Executive Chairman of Google, Alphabet, and later as their Technical Advisor until 2020. He was also on the board of directors at Apple from 2006-2009 and is currently the Chairman of the board of directors at the Broad Institute. From 2019 to 2021, Eric chaired the National Security Commission on Artificial Intelligence. He’s also a founding partner at Investment Endeavors, a VC firm. 
Learn more about Abundance360: https://www.abundance360.com/summit 
—******--
This episode is supported by exceptional companies:
Get started with Fountain Life and become the CEO of your health: https://fountainlife.com/peter/
AI-powered precision diagnosis you NEED for a healthy gut: https://www.viome.com/peter 
—******--
Topics:
0:00 - Intro
1:12 - Google and the AI Revolution
6:33 - AI's Power and Impact Today
10:12 - China's Race to Language Dominance
11:20 - TikTok's Growth and Election Interference
12:45 - AD: The Future of Healthcare: Fountain Life
15:03 - AI and the Fight Against Misinformation
21:28 - The Future of AI Safety
25:30 - AD: Viome 
27:12 - Government Struggles with Rapid Tech Growth
31:49 - AI's Potential Impact on Science
33:38 - Quantum Simulations Revolutionize Drug Development
******************************************--
I send weekly emails with the latest insights and trends on today’s and tomorrow’s exponential technologies. Stay ahead of the curve, and sign up now: https://www.diamandis.com/subscribe
My new book with Salim Ismail, Exponential Organizations 2.0: The New Playbook for 10x Growth and Impact, is now available on Amazon: https://bit.ly/3P3j54J 
Get my new Longevity Practices book for free: https://www.diamandis.com/longevity
Connect with Peter:
Twitter: https://bit.ly/40JYQfK
Instagram: https://bit.ly/3x6UykS
Listen to the show:
Apple: https://apple.co/3wLXeV3
Spotify: https://spoti.fi/3DwLzgs
---

# Ex-Google CEO on Government AI Policy & Deepfakes w/ Eric Schmidt | EP #99
**Moonshots - Peter Diamandis:** [May 02, 2024](https://www.youtube.com/watch?v=eHy68Jy-qwQ)
*  Something is about to change that I don't think people have clocked yet. [[00:00:00](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=0.0s)]
*  We're going to have a very different world and it's going to happen very quickly for the following reason. [[00:00:04](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=4.0s)]
*  People tend to think of AI as language to language and we're going to move from language to action. [[00:00:13](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=13.0s)]
*  What are we going to do when super intelligence is broadly available to everyone? [[00:00:18](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=18.0s)]
*  Well, in this case, I'm both optimistic and also fearful. [[00:00:23](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=23.0s)]
*  And obviously there's evil people and they'll use it in evil ways. [[00:00:26](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=26.0s)]
*  But I'm going to bet that the good people will route out evil. [[00:00:29](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=29.0s)]
*  That's historically been true in human society. [[00:00:33](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=33.0s)]
*  The systems will get so good that you and I, everyone in this audience will have access to a polymath. [[00:00:35](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=35.0s)]
*  Let's be a little proud that we are inventing a future that will accelerate physics, science, chemistry and so forth. [[00:00:42](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=42.0s)]
*  Eric, first of all, thank you for your friendship and for your partnership. [[00:00:51](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=51.0s)]
*  You've been an incredible friend, mentor, supporter for XPRIZE, for Singularity, for all the things that we've been working on. [[00:00:56](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=56.0s)]
*  And I just want to say a heartfelt thank you for that. [[00:01:05](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=65.0s)]
*  Thank you. [[00:01:08](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=68.0s)]
*  So the conversation we've had over the past 24 hours has been what we call the great AI debate, which is a obviously a challenge. [[00:01:11](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=71.0s)]
*  It's not truly a debate, but it's been the conversation around as we evolve digital super intelligence, that's a million times and a billion times faster. [[00:01:28](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=88.0s)]
*  How do we think about it? Is it our greatest hope or our gravest existential threat? [[00:01:38](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=98.0s)]
*  And how do we steer the course there? [[00:01:44](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=104.0s)]
*  We had Rick Kurzweil and Jeffrey Hinton on stage with us yesterday, as well as many people that you know, Imad Mustak and Nat Friedman and Guillaume Verdun. [[00:01:46](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=106.0s)]
*  I'm curious how you're steering this in your mind. [[00:01:58](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=118.0s)]
*  I mean, you have been Google from its earliest roots has been an AI first company. [[00:02:04](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=124.0s)]
*  I don't think people realize that it's always been the fundamental of the organization. [[00:02:11](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=131.0s)]
*  And Google actually developed all this technology way before anybody else, but it chose not to release it just to make sure it's safe, which was the responsible thing to do. [[00:02:16](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=136.0s)]
*  But your hand was forced. [[00:02:30](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=150.0s)]
*  How do you think about fear versus optimism in your own mind? [[00:02:32](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=152.0s)]
*  Well, in this case, I'm both optimistic and also fearful. [[00:02:37](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=157.0s)]
*  And, you know, Larry Page's PhD research was on AI. [[00:02:43](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=163.0s)]
*  So you're correct that Google was founded in the umbrella, if you will, of what AI was going to do. [[00:02:48](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=168.0s)]
*  And for a while, it seemed like about two thirds of the world's AI resources were well hosted within Google, which is a phenomenal achievement on the part of the founders and the leadership. [[00:02:53](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=173.0s)]
*  I think that people tend to think of AI from what they've seen in the movies, which is typically, you know, the sort of female scientist kills the killer robot kind of scenarios. [[00:03:03](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=183.0s)]
*  And first, we haven't figured out how to get robotics to work yet, but we certainly understand how to get information to work. [[00:03:19](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=199.0s)]
*  I wrote a book with Dr. Kissinger called The Age of AI, and we have our second one, his last one. [[00:03:25](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=205.0s)]
*  He died, unfortunately, last year called Genesis, coming out later this year, which is precisely on this topic. [[00:03:30](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=210.0s)]
*  I think the thing that to understand is that we're going to have a very different world and it's going to happen very quickly for the following reason. [[00:03:36](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=216.0s)]
*  The systems will get so good that you and I, everyone in this audience and everyone in the world through their phone or what have you, will have access to essentially a polymath, as in the historic polymaths of old. [[00:03:44](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=224.0s)]
*  So imagine if you had Aristotle to consult with you on logic and you had Oppenheimer to consult with you on physics and not the person, but rather the knowledge and that kind of scaling intelligence. [[00:03:59](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=239.0s)]
*  These sort of truly brilliant people who were historically incredibly rare, their equivalents would become generally available in the shorter term. [[00:04:12](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=252.0s)]
*  So that's the long term answer is what are we going to do when superintelligence is broadly available to everyone? [[00:04:22](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=262.0s)]
*  And obviously, there's evil people and they'll use it in evil ways. [[00:04:28](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=268.0s)]
*  But I'm going to bet that the good people will route out evil. [[00:04:31](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=271.0s)]
*  That's historically been true in human society. [[00:04:34](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=274.0s)]
*  The thing I would emphasize for this audience is that something is about to change that I don't think you're able to clock yet, which is people tend to think of AI as language to language. [[00:04:38](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=278.0s)]
*  And we're going to move from language to action specifically and technically. [[00:04:49](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=289.0s)]
*  It means that your text will be essentially computed into a program that can be then used. [[00:04:54](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=294.0s)]
*  So in your case, you're doing a conference, started all the potential conference members, call them up, figure out if they're going to come, lock them in, figure out who the most important ones and do the seating chart. [[00:05:02](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=302.0s)]
*  Right. And do it all by program. [[00:05:13](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=313.0s)]
*  Right. That's something that humans do all day. [[00:05:16](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=316.0s)]
*  Right. In what you do and many of you do many things, but that was one that will become automatic just by verbal command. [[00:05:19](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=319.0s)]
*  Somebody else will say, you know, I really like to see a competitor to Google. [[00:05:26](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=326.0s)]
*  So build a search engine, sort the ranking, but do it using my algorithm, not the one that Google is, which I don't like. [[00:05:29](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=329.0s)]
*  And the system won't do the same thing. [[00:05:35](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=335.0s)]
*  So you're going to see this explosion in digital power on a per person basis. [[00:05:38](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=338.0s)]
*  And no one's quite set it this way. [[00:05:44](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=344.0s)]
*  You're very good at marketing. [[00:05:46](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=346.0s)]
*  Maybe you can come up with a name for this. [[00:05:48](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=348.0s)]
*  It's an abundance of intelligence, but it's also in your format. [[00:05:50](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=350.0s)]
*  It's a bunch of action. [[00:05:53](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=353.0s)]
*  Yeah, it's intentional AI. [[00:05:55](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=355.0s)]
*  Yeah. [[00:05:58](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=358.0s)]
*  Making things happen. [[00:05:59](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=359.0s)]
*  And this is going to everything is going to become. [[00:06:01](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=361.0s)]
*  I think we're heading towards the trillion sensor economy where everything is knowable or AI can then take actions based upon the information out there and execute through robotics and such. [[00:06:04](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=364.0s)]
*  You've been very active in guiding national leaders on security, and that's been a really important work at this at this stage in your life. [[00:06:17](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=377.0s)]
*  And I want to hit on three of these. [[00:06:31](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=391.0s)]
*  We have such a short period of time. [[00:06:33](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=393.0s)]
*  So let me let me mention the three and then weave them as you as you would. [[00:06:35](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=395.0s)]
*  The first is AI and US national security. [[00:06:40](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=400.0s)]
*  The second is AI and competitiveness with China. [[00:06:45](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=405.0s)]
*  And the third is the impact of AI on the upcoming US elections, which many people have said could be patient zero and a lot of concerns. [[00:06:49](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=409.0s)]
*  So how do you think about these three things? [[00:07:00](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=420.0s)]
*  How should we think about them? [[00:07:02](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=422.0s)]
*  So I'm a part of a group that has looked very carefully at the real dangers of the current LLMs, and they're scary. [[00:07:04](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=424.0s)]
*  The conclusion of our group, which is roughly 20 people who are basically scientists, is we think we're OK now and we're worried about the future. [[00:07:11](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=431.0s)]
*  And the point at which you really want to get worried is called recursive self-improvement. [[00:07:20](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=440.0s)]
*  So recursive self-improvement means go learn everything, start now and don't stop until you know everything. [[00:07:26](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=446.0s)]
*  And this could allow this recursive self-improvement could eventually allow self-invocation of things. [[00:07:34](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=454.0s)]
*  And imagine a recursive self-improvement system which gets access to weapons. [[00:07:40](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=460.0s)]
*  So you can imagine doing things in biology that we cannot currently understand. [[00:07:45](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=465.0s)]
*  So there is a threshold. [[00:07:50](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=470.0s)]
*  Now, my standard joke about that is that when that thing starts learning on its own, you know what we're going to do? [[00:07:52](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=472.0s)]
*  We're going to unplug it because you can't have these things running randomly around, if you will, in the information space and not understanding at all what they're doing. [[00:07:57](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=477.0s)]
*  Another threshold point is when two different agentic systems, agents as a computer science point, are today defined as LLMs with state. [[00:08:07](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=487.0s)]
*  In other words, not only do they know how to go from input to output, but they can also they know what they did in the past and they can make judgments based on that. [[00:08:17](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=497.0s)]
*  So they accumulate knowledge. [[00:08:24](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=504.0s)]
*  So there's a scenario where your agent and my agent learn how to speak to each other and they start and they stop talking in English and they start talking in a language that they have invented. [[00:08:26](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=506.0s)]
*  What do we do in that case? Unplug the things. [[00:08:36](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=516.0s)]
*  You see that? [[00:08:39](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=519.0s)]
*  We've seen that. [[00:08:41](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=521.0s)]
*  So these scenarios, these threshold points, and we'll know when they're happening. [[00:08:42](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=522.0s)]
*  Another example will be when the system can start doing math on its own at a level that's incredibly advanced math. [[00:08:47](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=527.0s)]
*  That's another threshold point. [[00:08:57](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=537.0s)]
*  Now, will these things occur? When will they occur? [[00:08:59](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=539.0s)]
*  There's a debate in the industry. [[00:09:01](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=541.0s)]
*  Some people think five years. [[00:09:03](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=543.0s)]
*  I think it's going to be longer. [[00:09:04](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=544.0s)]
*  But people, you know, that's the clear threshold. [[00:09:06](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=546.0s)]
*  Now, with respect to AI safety in general, I was heavily involved with the UK Act in November, the executive order from the White House. [[00:09:10](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=550.0s)]
*  And we started a series of Track Two dialogues with China. [[00:09:19](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=559.0s)]
*  So I kind of roughly understand Europe, of course, as its usual hopeless self. [[00:09:22](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=562.0s)]
*  So I roughly know what everybody's doing. [[00:09:28](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=568.0s)]
*  And the governments are trying to tread lightly at the moment by doing essentially various forms of notification and self-regulation. [[00:09:31](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=571.0s)]
*  So if you look at the US Act, for example, you're not required to tell them what you're doing, but you're required above 10 to the 26 flops, which is an arbitrary measure that we frankly just invented, that you have to notify that the training event begins. [[00:09:38](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=578.0s)]
*  That seems like a reasonable compromise. [[00:09:53](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=593.0s)]
*  We don't know what the Chinese are going to do in this area, but you have to assume that they're going to fear the broad scale impacts of AI more than democracies will because it will be used to disempower the state. [[00:09:55](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=595.0s)]
*  And so we have to assume that the government will ultimately restrict it more than the West will. [[00:10:07](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=607.0s)]
*  Eric, how do you benchmark China today in terms of their capabilities in large language models and neural nets against the US? [[00:10:12](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=612.0s)]
*  I think as the audience knows, the government did something good, which is a restricted access to ASML and H100, now H800 chips from Nvidia, although Nvidia is doing just fine without all that revenue. [[00:10:24](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=624.0s)]
*  And so China is now stuck at the A100 level. [[00:10:36](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=636.0s)]
*  They're roughly limited at five nanometers. [[00:10:40](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=640.0s)]
*  So I'll just say broadly speaking, seven nanometers lower is better. [[00:10:42](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=642.0s)]
*  The chips that we're using now are three nanometers going down to two and then one point four or so. [[00:10:47](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=647.0s)]
*  So it looks like the gap hardware gap is going to increase. [[00:10:52](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=652.0s)]
*  And it also looks like the the Chinese will be forced to do scalable software with lesser hardware. [[00:10:56](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=656.0s)]
*  Can they pull it off? Absolutely. [[00:11:04](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=664.0s)]
*  How will they do it? They'll spend more money. [[00:11:07](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=667.0s)]
*  So if it costs us a billion dollars to do training, they'll spend five billion. [[00:11:09](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=669.0s)]
*  So it's it's a temporary gap. [[00:11:14](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=674.0s)]
*  It's not a crippling gap, if you will, in the competition. [[00:11:17](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=677.0s)]
*  You asked about the elections. [[00:11:20](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=680.0s)]
*  One way to understand this is that people now, and it's sad, don't really get their information out of the traditional news sources. [[00:11:24](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=684.0s)]
*  They get it out of let's think about it. [[00:11:32](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=692.0s)]
*  YouTube, which is, in my view, well managed, Instagram and Twitter and Facebook and TikTok. [[00:11:35](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=695.0s)]
*  Now, TikTok is not really social media. [[00:11:46](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=706.0s)]
*  TikTok is really television. [[00:11:48](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=708.0s)]
*  Remember, it's not really a function of what your friends are doing. [[00:11:51](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=711.0s)]
*  It uses a different algorithm, which is super impressive. [[00:11:54](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=714.0s)]
*  And it's growing like like crazy. [[00:11:56](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=716.0s)]
*  And of course, the U.S. is busy trying to ban it, which is probably not a very good idea. [[00:11:58](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=718.0s)]
*  But in any case, with Twitter, with TikTok's growth, you should expect regulation of content because we regulate every country regulates television in one form or another. [[00:12:03](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=723.0s)]
*  We're precisely this issue of election interference. [[00:12:12](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=732.0s)]
*  So I think you're going to see the decisions that are made by the social media companies with respect to how they present content will determine how badly regulated they're going to be in this election, because most people will encounter misinformation not because they built it, but because they saw it through social media. [[00:12:15](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=735.0s)]
*  So the secret that the social media companies understand the peril that they're in with respect to the downside, if they screw this up on either side. [[00:12:34](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=754.0s)]
*  Everybody want to take a short break from our episode to talk about a company that's very important to me and could actually save your life or the life of someone that you love. [[00:12:44](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=764.0s)]
*  Company is called Fountain Life, and it's a company I started years ago with Tony Robbins and a group of very talented physicians. [[00:12:53](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=773.0s)]
*  You know, most of us don't actually know what's going on inside our body. [[00:13:01](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=781.0s)]
*  We're all optimists. [[00:13:05](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=785.0s)]
*  Until that day when you have a pain in your side, you go to the physician in the emergency room and they say, listen, I'm sorry to tell you this, but you have this stage three or four going on. [[00:13:07](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=787.0s)]
*  And, you know, it didn't start that morning. [[00:13:17](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=797.0s)]
*  It probably was a problem that's been going on for some time. [[00:13:20](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=800.0s)]
*  But because we never look, we don't find out. [[00:13:23](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=803.0s)]
*  So what we built at Fountain Life was the world's most advanced diagnostic centers. [[00:13:27](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=807.0s)]
*  We have four across the US today and we're building 20 around the world. [[00:13:32](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=812.0s)]
*  These centers give you a full body MRI, a brain, a brain vasculature, an AI enabled coronary CT looking for soft plaque, a DEXA scan, a grail blood cancer test, a full executive blood workup. [[00:13:37](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=817.0s)]
*  It's the most advanced workup you'll ever receive. [[00:13:50](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=830.0s)]
*  150 gigabytes of data that then go to our AIs and our physicians to find any disease at the very beginning when it's solvable. [[00:13:54](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=834.0s)]
*  You're going to find out eventually. [[00:14:04](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=844.0s)]
*  Might as well find out when you can take action. [[00:14:06](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=846.0s)]
*  Fountain Life also has an entire side of therapeutics. [[00:14:08](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=848.0s)]
*  We look around the world for the most advanced therapeutics that can add 10, 20 healthy years to your life. [[00:14:11](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=851.0s)]
*  And we provide them to you at our centers. [[00:14:16](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=856.0s)]
*  So if this is of interest to you, please go and check it out. [[00:14:20](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=860.0s)]
*  Go to fountainlife.com backslash Peter. [[00:14:24](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=864.0s)]
*  When Tony and I wrote our New York Times bestseller Life Force, we had 30,000 people reached out to us for Fountain Life memberships. [[00:14:28](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=868.0s)]
*  You go to fountainlife.com backslash Peter. [[00:14:37](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=877.0s)]
*  We'll put you to the top of the list. [[00:14:40](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=880.0s)]
*  Really, it's something that is for me one of the most important things I offer my entire family, the CEOs of my companies, my friends. [[00:14:42](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=882.0s)]
*  It's a chance to really add decades onto our healthy lifespans. [[00:14:51](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=891.0s)]
*  Go to fountainlife.com backslash Peter. [[00:14:56](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=896.0s)]
*  It's one of the most important things I can offer to you as one of my listeners. [[00:14:59](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=899.0s)]
*  All right, let's go back to our episode. [[00:15:03](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=903.0s)]
*  I saw recently some limitations put on Gemini and talking about elections and politics. [[00:15:05](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=905.0s)]
*  Is this our other companies doing this or is it just Google that's stepping up? [[00:15:12](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=912.0s)]
*  So, well, Google, again, my view and I'm obviously biased has always been at the forefront of this. [[00:15:17](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=917.0s)]
*  In 2016, when Google faced the question of elections and the Trump interference, we did not have trouble because we had done the advertising with a white list. [[00:15:22](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=922.0s)]
*  In other words, you had to be approved, whereas the others, in particular Facebook, that was ultimately the biggest casualty of this, had not put a white list in. [[00:15:33](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=933.0s)]
*  Since then, Facebook has put a white list in. [[00:15:41](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=941.0s)]
*  So there's hope that the companies who have a vested interest in their own survival will manage this. [[00:15:44](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=944.0s)]
*  I'll let you speculate on X and Elon. [[00:15:50](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=950.0s)]
*  But the important thing here is that I didn't fully understand this until the last few years. [[00:15:53](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=953.0s)]
*  When you run a large social network, there are well-funded information transparency opponents who, for whatever reason, misinformation, disinformation, national security, what have you, they want their information out there. [[00:16:00](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=960.0s)]
*  I got in trouble one day because I announced, why would we ever source from RT, which is Russia Today? [[00:16:18](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=978.0s)]
*  And people yelled at me at the time, Russia Today, an RT after the Crimea invasions was in fact banned for precisely this reason. [[00:16:24](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=984.0s)]
*  So you really do have to be careful about the power of misinformation at scale. [[00:16:32](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=992.0s)]
*  The misinformer is guilty, but so are the platforms if they spread it without checking. [[00:16:37](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=997.0s)]
*  Right. And that's damaging to a democracy. [[00:16:43](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1003.0s)]
*  It really does put democracies at threat. [[00:16:46](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1006.0s)]
*  And this problem will only get worse. [[00:16:48](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1008.0s)]
*  There are a gazillion videos now where, I'll give an example, you can have chat CPT or equivalent generate a text. [[00:16:50](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1010.0s)]
*  You can generate the mouth movements. [[00:17:00](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1020.0s)]
*  You can move the face and so forth to the average person. [[00:17:02](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1022.0s)]
*  They're indistinguishable from real. [[00:17:05](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1025.0s)]
*  If you look at what happened with Taylor Swift and the deep fakes about her, there were plenty of systems that were trying to prevent the creation of the deep fakes. [[00:17:08](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1028.0s)]
*  But people were so motivated to create these images that they managed to get around all of the checks and balances. [[00:17:19](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1039.0s)]
*  So it is a war between the locks and the lock pickers and lock makers and the lock makers need to win with disinformation for the nation, frankly, for democracy. [[00:17:25](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1045.0s)]
*  You have been involved in the inner workings of US national defense policy. [[00:17:36](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1056.0s)]
*  How will AI change the business of war? [[00:17:44](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1064.0s)]
*  Is it ultimately a positive right now? [[00:17:49](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1069.0s)]
*  Helping us be more accurate? [[00:17:53](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1073.0s)]
*  I'll say this. [[00:17:56](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1076.0s)]
*  It'll sound cynical, but I'll say it. [[00:17:58](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1078.0s)]
*  I genuinely mean it. [[00:18:00](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1080.0s)]
*  The best thing about the Western military is they're not at war. [[00:18:02](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1082.0s)]
*  And so they're incredibly slow. [[00:18:05](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1085.0s)]
*  Right. [[00:18:07](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1087.0s)]
*  There is a real war in the West, and that's in Ukraine and Russia. [[00:18:09](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1089.0s)]
*  And I've now been many times to Ukraine and I've provided some advice. [[00:18:13](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1093.0s)]
*  And I obviously want, I think that however imperfect we want to preserve democracies in our world, they're just better and safer to have democracies than autocracies. [[00:18:17](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1097.0s)]
*  It's certainly not ones that are busy invading the neighboring country. [[00:18:29](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1109.0s)]
*  So what's really going on in Ukraine is a vision of what's happening in the future. [[00:18:33](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1113.0s)]
*  You now have, and again, I'll avoid my own history with respect to this, but a year ago I could go to the front and I could hang out and joke and so forth. [[00:18:38](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1118.0s)]
*  The weather was nice. [[00:18:50](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1130.0s)]
*  The food was good kind of a thing. [[00:18:51](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1131.0s)]
*  Now you cannot walk during the day or the night because there's a traffic jams of your drones and enemy drones for both sides on top. [[00:18:53](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1133.0s)]
*  And it's essentially a death zone. [[00:19:02](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1142.0s)]
*  So the ubiquity of drones means, in my view, that tanks and artillery and mortars go away as weapons of war. [[00:19:05](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1145.0s)]
*  I'm a sufficient optimist that I believe that once countries figure out a way to make this ubiquitous notion of drones for their own defense, it'll become impossible to invade an adjacent country. [[00:19:15](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1155.0s)]
*  Because once the tanks roll, what you could do is just bomb them with drones and a drone costs $5,000 or less and the tank costs $5 million or less. [[00:19:26](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1166.0s)]
*  So the kill ratio is such that the tanks just don't make it and you can make enough drones to pull it off. [[00:19:34](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1174.0s)]
*  The current drones are not particularly AI sophisticated. [[00:19:40](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1180.0s)]
*  But if the U.S. government in its infinite stupidity were actually to do something right and approve the Ukraine aid pact, it would give us another another year. [[00:19:44](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1184.0s)]
*  So my current phrase publicly is, let's get another year here. [[00:19:54](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1194.0s)]
*  And in that year, you're going to see asymmetric asymmetric innovation that can allow a smaller government, which is a new democracy, trying hard to counter the moves of a large and established of invading power. [[00:19:58](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1198.0s)]
*  I suppose the cynic would say, well, that means it's going to get harder for the U.S. to invade neighboring countries. [[00:20:13](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1213.0s)]
*  And I said, well, that may be true, too. [[00:20:18](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1218.0s)]
*  But when having now seen real war as opposed to what you see in the movies, and I have lots of drone death videos that I will not show anybody, it's really horrific. [[00:20:20](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1220.0s)]
*  And we want everything we can to stop war. [[00:20:32](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1232.0s)]
*  And I think that there's a scenario where I makes it actually much less likely. [[00:20:35](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1235.0s)]
*  They'll certainly with a I am empowered weapons be far fewer collateral damage because of the targeting. [[00:20:40](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1240.0s)]
*  And again, this is this is lost in the various critics of what I and others are doing. [[00:20:47](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1247.0s)]
*  The biggest casualties of war are not actually the soldiers, but the civilians. [[00:20:51](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1251.0s)]
*  So war is horrific. [[00:20:56](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1256.0s)]
*  And it should be if you have to have a deal with the professionals and don't kill kids and women and old ladies and bomb the buildings like the Russians have been doing with their tanks, which upsets me knowing those are called war crime. [[00:20:58](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1258.0s)]
*  I had Palmer lucky on this stage last year describing what he's doing with Andrew. [[00:21:11](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1271.0s)]
*  And that was his key point that precision is everything. [[00:21:15](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1275.0s)]
*  I'm being able to and Palmer's company has done a fantastic job. [[00:21:20](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1280.0s)]
*  They're one of the great U.S. leaders in this space. [[00:21:24](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1284.0s)]
*  Yeah, for sure. [[00:21:26](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1286.0s)]
*  Let's talk about safety. [[00:21:28](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1288.0s)]
*  Safety. [[00:21:30](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1290.0s)]
*  You know, the point's been made over and over again in the last 24 hours that these A.I. models are progeny. [[00:21:31](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1291.0s)]
*  They're built on our digital exhaust. [[00:21:40](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1300.0s)]
*  How should we be training models? [[00:21:43](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1303.0s)]
*  How should we be trying to maximize? [[00:21:48](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1308.0s)]
*  Is containment ever an issue? [[00:21:51](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1311.0s)]
*  Is how do you think about safety in our super advanced A.I. models? [[00:21:54](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1314.0s)]
*  I mean, the first the first rules were don't put it on the open Internet and don't allow it to self referentially improve itself. [[00:22:00](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1320.0s)]
*  And we've put it out in the open Internet and and we've had, you know, software coding software. [[00:22:10](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1330.0s)]
*  So where do we go from here? [[00:22:17](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1337.0s)]
*  Well, let's understand the structure of the future Internet at the moment. [[00:22:20](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1340.0s)]
*  The hyperscalers, the big ones, which essentially are Microsoft, Microsoft Open Eyes, kind of a pair. [[00:22:25](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1345.0s)]
*  Google, Anthropic, Inflection. [[00:22:32](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1352.0s)]
*  There's a couple in China that are coming. [[00:22:35](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1355.0s)]
*  These are closed models. [[00:22:38](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1358.0s)]
*  And when I say closed, that means that you you don't know how they work internally. [[00:22:40](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1360.0s)]
*  The source code is not available. [[00:22:44](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1364.0s)]
*  The weights are not available and the APIs are limited in some way. [[00:22:46](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1366.0s)]
*  And I there's been a debate in the industry for a long time is open versus closed models. [[00:22:50](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1370.0s)]
*  If you look at the open models that have come out, if you look at the Mistral most recent models, if you look at Lama 3, each of these models are incredibly powerful. [[00:22:56](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1376.0s)]
*  They get to roughly 80 percent. [[00:23:05](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1385.0s)]
*  But so the debate that's going on in the industry is will the open source and closed models, will they track? [[00:23:07](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1387.0s)]
*  In other words, will open source lag a year or two or will the hyperscalers get much bigger? [[00:23:13](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1393.0s)]
*  That is essentially a question of dollars, right? [[00:23:18](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1398.0s)]
*  And time dollars and so forth. [[00:23:21](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1401.0s)]
*  And we're talking about two hundred and fifty million dollars for a training run, five hundred thousand five hundred million dollars for a training run escalating quite quickly. [[00:23:24](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1404.0s)]
*  And you see this in Nvidia's stock price, et cetera. [[00:23:32](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1412.0s)]
*  So so the first question is, do you think that there'll be a small number or large number of such things? [[00:23:35](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1415.0s)]
*  My own view is there'll be a small number of incredibly powerful AGI systems, which will be heavily regulated because they're so powerful. [[00:23:41](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1421.0s)]
*  This is my personal view. [[00:23:48](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1428.0s)]
*  And then a much larger number of what I'm going to call middle sized models, which will be open source. [[00:23:50](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1430.0s)]
*  And people would just plug in and out. [[00:23:55](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1435.0s)]
*  I looked very carefully at this question of could you selectively train? [[00:23:57](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1437.0s)]
*  In other words, if you could delete the bad part of the information in the world and just only train on good information, would you get a better model? [[00:24:01](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1441.0s)]
*  Unfortunately, it appears that it doesn't actually work that way. [[00:24:08](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1448.0s)]
*  When you restrict training data, you actually get a more brittle model. [[00:24:11](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1451.0s)]
*  So it looks like you're better off, at least today with the current algorithms to build a large model and then restrict it with guardrails with the so-called red teams and so forth. [[00:24:15](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1455.0s)]
*  And the red teaming is clever because what they do is they have humans who think that they test something. [[00:24:26](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1466.0s)]
*  They say if it knows something, it must know something else. [[00:24:32](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1472.0s)]
*  And that seems to be working. [[00:24:35](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1475.0s)]
*  Eventually, the consensus of the groups that I have been working with is that the red teaming will become its own business. [[00:24:37](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1477.0s)]
*  I have a thing about how to fund this philanthropically, because if you think about it, how do you know what an AI is doing unless an AI is watching it? [[00:24:44](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1484.0s)]
*  Well, how can the AI that's watching it know what the AI discovered it unless the AI tells it, but it doesn't know how to tell you what it knows? [[00:24:52](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1492.0s)]
*  So this conundrum is to be worked on. [[00:24:59](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1499.0s)]
*  There are plenty of people working on this problem. [[00:25:01](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1501.0s)]
*  I think we'll get this solved. [[00:25:03](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1503.0s)]
*  But I think it's important to say that these very large models are ultimately going to get regulated. [[00:25:05](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1505.0s)]
*  And the reason is they're just too powerful. [[00:25:10](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1510.0s)]
*  And they're going to be regulated because they need to be. [[00:25:12](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1512.0s)]
*  They know too many ways of harm as well as enormous, enormous power of gain, right? [[00:25:15](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1515.0s)]
*  The ability to cure cancer and fix our energy problems and do new materials and on and on and on. [[00:25:21](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1521.0s)]
*  I mean, I can go on and on and on about what they'll be able to do because they're polymaths. [[00:25:27](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1527.0s)]
*  Did you see the movie Oppenheimer? [[00:25:31](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1531.0s)]
*  If you did, did you know that besides building the atomic bomb at Los Alamos National Labs, [[00:25:33](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1533.0s)]
*  that they spent billions on bio-defense weapons, the ability to accurately detect viruses and microbes by reading their RNA? [[00:25:40](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1540.0s)]
*  Well, a company called Viome exclusively licensed the technology from Los Alamos Labs [[00:25:48](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1548.0s)]
*  to build a platform that can measure your microbiome and the RNA in your blood. [[00:25:54](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1554.0s)]
*  Now, Viome has a product that I've personally used for years called Full Body Intelligence, [[00:25:59](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1559.0s)]
*  which collects a few drops of your blood, spit, and stool and can tell you so much about your health. [[00:26:04](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1564.0s)]
*  They've tested over 700,000 individuals and used their AI models to deliver members critical health guidance, [[00:26:09](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1569.0s)]
*  like what foods you should eat, what foods you shouldn't eat, as well as your supplements and probiotics, [[00:26:15](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1575.0s)]
*  your biological age and other deep health insights. [[00:26:21](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1581.0s)]
*  And the results of the recommendations are nothing short of stellar. [[00:26:25](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1585.0s)]
*  As reported in the American Journal of Lifestyle Medicine, [[00:26:28](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1588.0s)]
*  after just six months of following Viome's recommendations, members reported the following. [[00:26:31](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1591.0s)]
*  A 36% reduction in depression, a 40% reduction in anxiety, a 30% reduction in diabetes, and a 48% reduction in IBS. [[00:26:36](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1596.0s)]
*  Listen, I've been using Viome for three years. I know that my oral and gut health is one of my highest priorities. [[00:26:47](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1607.0s)]
*  Best of all, Viome is affordable, which is part of my mission to democratize health. [[00:26:54](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1614.0s)]
*  If you want to join me on this journey, go to Viome.com slash Peter. [[00:26:59](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1619.0s)]
*  I've asked Naveen Jain, a friend of mine who's the founder and CEO of Viome, to give my listeners a special discount. [[00:27:03](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1623.0s)]
*  You'll find it at Viome.com slash Peter. [[00:27:09](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1629.0s)]
*  We had two political leaders on stage with us yesterday. [[00:27:13](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1633.0s)]
*  And, you know, the question is, can the government possibly keep up with this? [[00:27:17](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1637.0s)]
*  From your own experiences inside the hallowed halls of this, our government, and others, how are you seeing it? [[00:27:23](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1643.0s)]
*  Are they, is there enough attention? Is there enough awareness, enough fear? [[00:27:31](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1651.0s)]
*  Well, fear is a heavy motivator for political leaders, especially if they're worried about their own jobs. [[00:27:37](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1657.0s)]
*  What I found in the Senate was that there's a group of four, two Republicans and two Democrats, who really got it. [[00:27:43](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1663.0s)]
*  And I worked very closely with them. We had a series of Senate hearings on this subject, which were well attended. [[00:27:51](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1671.0s)]
*  There's a similar initiative now in the House. And this is largely, it's happening so quickly. [[00:27:58](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1678.0s)]
*  In fairness to our political leaders, most of us have trouble understanding what's going on. [[00:28:04](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1684.0s)]
*  Can you imagine a normal person who's got like political problems to deal with? [[00:28:08](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1688.0s)]
*  So I think this is a situation where America, and I think it's important to say that we should be very proud of our country. [[00:28:14](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1694.0s)]
*  We spend all of our time complaining. [[00:28:21](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1701.0s)]
*  But the fact of the matter is the future is being invented in the United States and in the UK, our closest ally. [[00:28:23](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1703.0s)]
*  And the fact of the matter is that the Chinese, for example, every Chinese training run starts with an open source event and then moves on. [[00:28:30](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1710.0s)]
*  Right. So they get it. Right. And they start with our great work. [[00:28:37](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1717.0s)]
*  So let's be a little proud that we are inventing a future that will accelerate physics, science, chemistry and so forth. [[00:28:41](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1721.0s)]
*  I'm working with people who are busy reading science journals, reading chemistry journals, [[00:28:49](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1729.0s)]
*  generating hypotheses and then labeling proteins and so forth in new ways and doing it all automatically and then using robotic farms to do it. [[00:28:54](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1734.0s)]
*  The scale of innovation that this notion of read everything, take an action, write a program and run the program is profound. [[00:29:01](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1741.0s)]
*  And by the way, the innovation is not being done by the faculty. It's being done by the graduate students. [[00:29:10](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1750.0s)]
*  And by the way, guess what? The engine of growth in our society is the graduate students who are trying to get their PhDs and they invent whole industries. [[00:29:14](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1754.0s)]
*  And then when they get their PhDs, we kick them out of the country and send them home. [[00:29:22](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1762.0s)]
*  Perhaps we should try to keep them in the US. [[00:29:26](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1766.0s)]
*  Perhaps you should staple a green card on the back of the doctoral degree. [[00:29:29](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1769.0s)]
*  I think my point here is that everyone spends all their time with these sort of concerns about how society will adapt. [[00:29:32](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1772.0s)]
*  This is going to happen first. The systems are not prepared for this. [[00:29:44](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1784.0s)]
*  So the government's not prepared for it. [[00:29:48](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1788.0s)]
*  The companies who are doing the majority of the work have an enormous responsibility to maintain human values, to maintain decency, [[00:29:50](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1790.0s)]
*  to deal with some of the abuses that occur online. And they need to do it on their own. [[00:30:00](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1800.0s)]
*  They need to clean up their own act if they don't have it cleaned up now. [[00:30:05](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1805.0s)]
*  And if they don't, they'll get regulated. [[00:30:08](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1808.0s)]
*  And hopefully the industry as a group, which is what we're trying to do, can present a coherent structure that manages the downside correctly, [[00:30:10](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1810.0s)]
*  but gives us this incredible upside, both for national security, which I work on most of the time, but also for health science and education. [[00:30:18](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1818.0s)]
*  You know, back, I remember when I was a gene jockey in the labs at MIT and Harvard Med School in the 80s when the first restriction enzymes came out. [[00:30:28](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1828.0s)]
*  And there was a huge fear about that, of what that could mean. [[00:30:39](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1839.0s)]
*  The biotech industry got together in a series of Asilomar conferences to self-regulate. [[00:30:43](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1843.0s)]
*  Is that same sort of regulation? And it worked, by the way. [[00:30:50](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1850.0s)]
*  Is that same conversation going on now in the AI leadership world? [[00:30:54](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1854.0s)]
*  And in fact, we had a meeting in December, which was an attempt at that. It was not at Asilomar. [[00:30:59](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1859.0s)]
*  There was a meeting a week ago at Asilomar. There's another meeting at Stanford in two weeks on the same subject. [[00:31:05](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1865.0s)]
*  All of us are participating in it. And we're talking about all these things precisely. [[00:31:11](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1871.0s)]
*  If you go back to your training way back when you were a doctor, the RAG, right, [[00:31:15](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1875.0s)]
*  which is the sort of group that managed all of this, was actually created out of the scientists, not out of the government. [[00:31:21](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1881.0s)]
*  And eventually the RAG was put under what is now HSS. [[00:31:27](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1887.0s)]
*  So there is a history here of the scientists who really do understand what this thing can do, but are otherwise clueless on its impact, typically. [[00:31:31](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1891.0s)]
*  Can basically get the structure right. And then the government can figure out what is the human impact of it. [[00:31:39](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1899.0s)]
*  And that's the right partnership, in my view. [[00:31:46](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1906.0s)]
*  Last question, Eric, and again, thank you for your time. [[00:31:49](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1909.0s)]
*  The work that you do with Schmidt Features Foundation, you're a very curious individual in across a multitude of different areas. [[00:31:52](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1912.0s)]
*  I imagine that AI to discovering new physics, and new math, and new biology, and new materials has to be just an extraordinary candy for you. [[00:32:01](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1921.0s)]
*  What are you most excited about there? [[00:32:13](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1933.0s)]
*  Well, I've gone to a series of conferences in physics and chemistry, which I did not really understand a word of it. [[00:32:15](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1935.0s)]
*  But here's my report. They're doing, they're taking the LLMs and more importantly, diffusion models. [[00:32:21](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1941.0s)]
*  And diffusion model is essentially this strange thing where you take something, you add noise to it, and then you denoise it, and you get a more accurate version of the same thing. [[00:32:28](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1948.0s)]
*  They're using these tools in very complicated ways in physics to solve problems that are not, that are just not solved. [[00:32:38](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1958.0s)]
*  A typical example is that using physics equations or chemistry equations, we know precisely how the forces work. [[00:32:45](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1965.0s)]
*  We just can't, they're incomputable by computers in the next 100,000 years. [[00:32:51](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1971.0s)]
*  Right. But you can use these techniques to get approximations. [[00:32:55](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1975.0s)]
*  And these approximations are good enough to solve the problem that you have in front of you, which is an estimation problem or an annealing problem or something like that. [[00:32:59](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1979.0s)]
*  I think the biggest area of impact is going to be biology because biology is so vast and so unknown. [[00:33:08](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1988.0s)]
*  And the way you do it is you basically you do math solving through a thing called lean, and then you do all this chemistry work, and then it builds on top of that. [[00:33:14](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=1994.0s)]
*  In physics, there are people who are working on partial differential equation solvers, which are at the base of everything. [[00:33:22](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2002.0s)]
*  And again, they're using variants of LLMs, but they're not actually LLMs. [[00:33:27](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2007.0s)]
*  And the math is impossible to understand, but that's okay. [[00:33:31](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2011.0s)]
*  I wasn't good enough to do physics. [[00:33:34](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2014.0s)]
*  You know, I should have mentioned you're the, are you still the chairman of Sandbox AQ? [[00:33:36](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2016.0s)]
*  I am. Yeah. [[00:33:41](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2021.0s)]
*  We had Jack, we had Jack Hittery here last year. [[00:33:43](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2023.0s)]
*  He's phenomenal and brilliant. [[00:33:46](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2026.0s)]
*  And congratulations on the success of Sandbox AQ. [[00:33:48](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2028.0s)]
*  Jack will come back with us again next year. [[00:33:51](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2031.0s)]
*  I have to imagine that as explosive and exciting as AI is, that quantum compute and quantum technologies are going to make that look like it's standing still. [[00:33:53](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2033.0s)]
*  Is that a fair statement? [[00:34:04](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2044.0s)]
*  Yeah, I've been waiting for quantum computing to arrive for about 20 years. [[00:34:06](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2046.0s)]
*  The physical problem with quantum computing is the error rate. [[00:34:11](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2051.0s)]
*  And so for one qubit, you need a thousand real qubit, one accurate qubit, you need a thousand and so forth. [[00:34:15](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2055.0s)]
*  People are working on this. [[00:34:21](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2061.0s)]
*  That stuff remains very hard. [[00:34:22](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2062.0s)]
*  What Jack's company, Sandbox AQ, did is said, we're not going to work on that. [[00:34:24](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2064.0s)]
*  We're going to basically build simulations of quantum and apply them to real world problems. [[00:34:29](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2069.0s)]
*  An interesting, I assume I can talk about this a little bit in public, the interesting new thing that they figured out is that they can take a drug, if you will, [[00:34:33](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2073.0s)]
*  and using quantum effects, but using a simulator of quantum because they don't have a quantum computer, they can perturb it. [[00:34:44](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2084.0s)]
*  And in the perturbations, they can make the drugs more effective, longer lasting, longer shelf life, what have you. [[00:34:52](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2092.0s)]
*  That turns out to be an incredibly powerful and big industry. [[00:34:58](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2098.0s)]
*  And it's an example of a short term impact of quantum that I, for one, never occurred to me. [[00:35:01](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2101.0s)]
*  I assume we had to wait for quantum computers. [[00:35:07](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2107.0s)]
*  But the quantum simulation is so good now that you can make wins now. [[00:35:09](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2109.0s)]
*  And that's what he's doing. [[00:35:14](https://www.youtube.com/watch?v=eHy68Jy-qwQ&t=2114.0s)]
