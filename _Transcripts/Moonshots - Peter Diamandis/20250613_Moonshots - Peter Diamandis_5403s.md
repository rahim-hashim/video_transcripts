---
Date Generated: June 13, 2025
Transcription Model: whisper medium 20231117
Length: 5403s
Video Keywords: ['peter diamandis', 'longevity', 'xprize', 'abundance']
Video Views: 1627
Video Rating: None
Video Description: Get access to metatrends 10+ years before anyone else -https://qr.diamandis.com/metatrends 
Mo Gawdat is an author and former CBO of Google X. 
Steven Kotler is an entrepreneur and a multiple New York Times Best-Selling Author. 
00:00 - Is AI a Bubble? 
10:11-  The Future of AI: Trends and Predictions
36:01- The Quest for Wisdom in AI Development
49:12- Humanity's Future with AI
51:45 - The Role of Greed and Fear
56:08 - Surviving and Thriving in the Age of AI
01:04:39 - The Global AI Landscape
01:06:33 - AI: A Tool for Abundance or Destruction?
01:18:10 - The Need for Regulation
01:21:55 - The Role of Ethics in AI
–
Offers for my audience: 
You can access my talks with Mo Gawdat and Cathie Wood for free: diamandis.com/summit
Test what’s going on inside your body at https://qr.diamandis.com/fountainlifepodcast  
Reverse the age of my skin using the same cream at https://qr.diamandis.com/oneskinpod    
--
Connect with Mo: https://www.mogawdat.com/ 
Connect with Steven: https://www.stevenkotler.com/
Connect with Peter:
X: https://qr.diamandis.com/twitter 
Listen to MOONSHOTS:
Apple: https://qr.diamandis.com/applepodcast 
Spotify: https://qr.diamandis.com/spotifypodcast 
–
*Recorded June 2025
*Views are my own thoughts; not Financial, Medical, or Legal Advice.
---

# AI Experts Debate: Overhyped or Underhyped? (Opposite Opinions) Mo Gawdat & Steven Kotler | EP #177
**Moonshots - Peter Diamandis:** [June 13, 2025](https://www.youtube.com/watch?v=_H-5fvzsbdY)
*  What's the impact of AI gonna be? Is it just massively overhyped or perhaps is [[00:00:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=0.0s)]
*  it something that we should be concerned about? Today's AI is underhyped. I think [[00:00:05](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5.72s)]
*  it's massively overhyped. I know a ton of people who have way more work because of [[00:00:10](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=10.72s)]
*  AI. They just can do higher quality, better work, but it has not saved time. [[00:00:15](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=15.5s)]
*  We are talking to machines that are talking back to us, summarizing massive [[00:00:20](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=20.32s)]
*  of knowledge and yet we take that for granted. Discussions about super [[00:00:26](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=26.94s)]
*  intelligences and AGI and around the corner and no, like just no. How smart is [[00:00:31](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=31.32s)]
*  smart enough to render me irrelevant? I think we are holding two different [[00:00:38](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=38.94s)]
*  futures in superposition. The question becomes how do we guide humanity towards [[00:00:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=44.46s)]
*  this positive vision of the future? What do we do today? [[00:00:49](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=49.7s)]
*  Everybody welcome to Moonshot. I'm here with two extraordinary brilliant guests. [[00:01:02](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=62.38s)]
*  We're here to discuss a conversation that may be happening around every dinner [[00:01:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=67.06s)]
*  table. I know it's happening in the heads of companies and nations, which is what's [[00:01:11](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=71.22s)]
*  the impact of AI going to be on our lives and our business on every aspect [[00:01:17](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=77.42s)]
*  of our day-to-day existence over the next five to eight years? Is it something [[00:01:22](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=82.34s)]
*  which is going to be extraordinary? Is it just massively overhyped or perhaps is [[00:01:27](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=87.06s)]
*  it something that we should be concerned about? I'm joined here with Mo Gadot [[00:01:32](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=92.78s)]
*  who is the former business chief officer, the chief business officer of Google X, [[00:01:37](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=97.78s)]
*  best-selling author of Solve for Happy and Scary Smart. He's a global thought [[00:01:42](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=102.34s)]
*  leader on AI, exploring how exponential technologies will shape humanity. Also [[00:01:47](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=107.46000000000001s)]
*  with another dear friend, Steven Kotler, who's the best-selling author, peak [[00:01:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=112.86s)]
*  performance expert and executive director of the Flow Research Collective. He's my [[00:01:56](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=116.58s)]
*  co-author of Abundance, Bold, The Future is Faster Than You Think and books like [[00:02:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=120.38s)]
*  The Rise of Superman and The Art of Impossible. Steven has also been thinking [[00:02:05](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=125.42s)]
*  deeply about exponential tech and its impact on us. Gentlemen, welcome and good [[00:02:11](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=131.02s)]
*  morning and good evening. Steven, you're on the west side of the United States [[00:02:17](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=137.06s)]
*  with me. Mo, you're in the Emirates. [[00:02:20](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=140.38s)]
*  Dubai. [[00:02:23](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=143.3s)]
*  Good to see you both. [[00:02:23](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=143.62s)]
*  Yeah, good to see you both. [[00:02:24](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=144.9s)]
*  Good morning, Peter. [[00:02:25](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=145.86s)]
*  There is somewhere around 400 IQ points in this room. I have 40 of them, so you [[00:02:28](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=148.18s)]
*  do the math. [[00:02:33](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=153.1s)]
*  So let me set up the topic. Mo and Steven, I'd like to talk about the decade ahead, [[00:02:33](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=153.5s)]
*  2025 to 2035, specifically to think about the implications of what is emerging in [[00:02:40](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=160.5s)]
*  our conversation as AGI, but even beyond that, artificial super intelligence, the [[00:02:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=168.02s)]
*  upsides and the downsides. And here's the setup I want to talk about. [[00:02:54](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=174.62s)]
*  So Ray Kurzweil, who we all know and love, has predicted that we're going to [[00:03:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=180.5s)]
*  see a century's worth of progress between 2025 and 2035, equivalent to the [[00:03:05](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=185.5s)]
*  progress between 1925 and today. And if we think about what the world was like in [[00:03:12](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=192.66s)]
*  1925, 100 years ago, the top of the tech industry, the top of the tech industry [[00:03:20](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=200.18s)]
*  was the Ford Model T, the penetration of electricity and the telephone in homes [[00:03:27](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=207.02s)]
*  across the U.S. was only 30%. We've gone an extraordinary distance since then. [[00:03:34](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=214.02s)]
*  And so the question is, what will it be like in 2035? It's nearly unimaginable if, [[00:03:41](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=221.62s)]
*  in fact, that speed is true and we don't perceive exponentials well. [[00:03:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=228.82s)]
*  This past week, we've seen every major AI company from Google and OpenAI to XAI [[00:03:54](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=234.82s)]
*  and Nvidia announce extraordinary next level breakthroughs and models. [[00:04:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=240.82s)]
*  We're about to see the release of GPT-5, self-improving AI programming that could [[00:04:05](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=245.82s)]
*  lead to an intelligence explosion beyond our imagination. [[00:04:11](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=251.82s)]
*  That's the conversation I want to have. And, you know, Stephen, I know that you [[00:04:15](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=255.82s)]
*  and I have this conversation and have a debate on it all the time. [[00:04:21](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=261.82s)]
*  I brought Mo in to help us. [[00:04:25](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=265.82s)]
*  Mo's the referee. [[00:04:28](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=268.82s)]
*  The referee or a wise individual whose points of view I respect. [[00:04:30](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=270.82s)]
*  And by the way, Stephen, Peter did pay me. So just... [[00:04:39](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=279.82s)]
*  As long as you're getting the convo, I'm fine. It's good. [[00:04:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=284.82s)]
*  It's good. You got me on the back end though, right? [[00:04:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=288.82s)]
*  So go ahead, say whatever you want to say, Stephen. I'll disagree. [[00:04:51](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=291.82s)]
*  So, Stephen, do you want to jump in with your points of view? [[00:04:56](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=296.82s)]
*  You think AI is massively overhyped. We have folks like Eric and Stephen. [[00:05:01](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=301.82s)]
*  AI is massively underhyped. [[00:05:06](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=306.82s)]
*  Yeah, I think it's massively overhyped. [[00:05:10](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=310.82s)]
*  I listen to what's going on. [[00:05:13](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=313.82s)]
*  And so let me back up one step, which is humans have a really wild, unnamed cognitive bias. [[00:05:16](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=316.82s)]
*  We don't tend to trust our own history. [[00:05:26](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=326.82s)]
*  And you see this a lot in the internet. [[00:05:29](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=329.82s)]
*  And you see this a lot in the internet. [[00:05:31](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=331.82s)]
*  And you see this a lot in the internet. [[00:05:34](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=334.82s)]
*  People talk about grid and endurance and they're like, I don't have those skills. [[00:05:36](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=336.82s)]
*  And then you start investigating them up their life and they survived the shitty childhood. [[00:05:41](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=341.82s)]
*  They've done 10 years of a tough man. [[00:05:46](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=346.82s)]
*  They have all the skills. They just don't trust the truth of their own experience. [[00:05:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=348.82s)]
*  And I see that a lot here. [[00:05:51](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=351.82s)]
*  Look, I work with AI as a scientist, as a researcher. [[00:05:55](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=355.82s)]
*  I work with AI as a creative and as a writer. [[00:05:59](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=359.82s)]
*  And all day long, the gap between the shit coming out of people's mouth and my experience on the ground is so colossal, it's insane. [[00:06:02](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=362.82s)]
*  People have claims about AI being able to write or anything else. [[00:06:12](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=372.82s)]
*  The most hysterical thing you've got to try is I work with one of the best editors in the world on a weekly basis. [[00:06:15](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=375.82s)]
*  I've edited things, polished them with AI thinking they gleam and shine, bring them into an editing meeting with them. [[00:06:23](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=383.82s)]
*  We start to read them. We can't even get to the second sentence. [[00:06:29](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=389.82s)]
*  They sound like such gobbledygook. [[00:06:31](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=391.82s)]
*  I'm not even noticing it because the AI sort of glazes me over and I've written 17 books. [[00:06:33](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=393.82s)]
*  But like when you actually put it to an actual editing test, it's laughably terrible. [[00:06:39](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=399.82s)]
*  And you can't use it to correct itself. [[00:06:45](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=405.82s)]
*  It still can't see the errors. [[00:06:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=408.82s)]
*  It actually gets worse and worse and worse. [[00:06:50](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=410.82s)]
*  And people have been claiming model after model after model, improving, improving. [[00:06:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=412.82s)]
*  That's not the experience on the ground. [[00:06:56](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=416.82s)]
*  It's like people telling us AI was going to make you more productive. [[00:06:59](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=419.82s)]
*  I don't know anybody who's become more productive because of AI. [[00:07:02](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=422.82s)]
*  I know a ton of people who have way more work because of AI. [[00:07:05](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=425.82s)]
*  They just can do higher quality, better work. [[00:07:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=429.82s)]
*  But it has not saved time at all. [[00:07:11](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=431.82s)]
*  It's actually added tremendous amounts of time. [[00:07:14](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=434.82s)]
*  The quality has gone up off the work. [[00:07:16](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=436.82s)]
*  But the claims that are coming out of people's mouth and the experience on the ground are massively different. [[00:07:18](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=438.82s)]
*  Point one. Point two is we've done this. [[00:07:23](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=443.82s)]
*  I've been in the same rooms that you've been in and you've been in, Mo, where people are screaming about AI coming to eat the world. [[00:07:26](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=446.82s)]
*  Dude, I freaking heard this about Bitcoin and blockchain and the metaverse. [[00:07:32](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=452.82s)]
*  You know, anybody who lives in the metaverse, you know, anybody's been there, visited. [[00:07:38](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=458.82s)]
*  You don't have to find the metaverse. [[00:07:41](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=461.82s)]
*  Right? [[00:07:43](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=463.82s)]
*  Like, as far as I can tell, the metaverse is like a pet name for Mark Zuckerberg's special magic underwear because it doesn't exist any place else in the world. [[00:07:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=464.82s)]
*  I don't like this is my point. [[00:07:53](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=473.82s)]
*  I'm not. And more than anybody else, I track these technologies. [[00:07:56](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=476.82s)]
*  I watch them. I use them. [[00:07:59](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=479.82s)]
*  I'm not saying this is not a technology that is advancing very, very quickly. [[00:08:01](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=481.82s)]
*  Not saying that at all. [[00:08:06](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=486.82s)]
*  I am saying discussions about super intelligences and AGI and around the corner. [[00:08:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=487.82s)]
*  And no, like, just no, nobody. [[00:08:13](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=493.82s)]
*  That's not the coders are having a different experience. [[00:08:16](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=496.82s)]
*  And it's what has been revealed, which coders probably don't like. [[00:08:20](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=500.82s)]
*  Is this a very coding is a bounded information problem. [[00:08:25](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=505.82s)]
*  You start here, you know where you're going as a general rule. [[00:08:29](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=509.82s)]
*  It's a bounded problem and inside bounded domains, computers are really awesome. [[00:08:32](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=512.82s)]
*  And we're going to continue to see that. [[00:08:39](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=519.82s)]
*  But I think the other stuff is just massively overhyped. [[00:08:41](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=521.82s)]
*  And the third point is, and this is the one where the journalist in me gets like every alarm bell goes off. [[00:08:45](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=525.82s)]
*  Everybody I hear see on stage talking about this stuff is making a living off of it. [[00:08:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=532.82s)]
*  They make a living somehow because AI is exploding and they're here to save the world. [[00:08:59](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=539.82s)]
*  I see it like in the performance world, every coach who has been floundering and couldn't quite get a job there now all AI saviors. [[00:09:04](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=544.82s)]
*  They've come to save us from AI. [[00:09:12](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=552.82s)]
*  And so the AI hype is to their benefit. [[00:09:14](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=554.82s)]
*  And I see it sort of everywhere. [[00:09:17](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=557.82s)]
*  A lot of people are making a ton of money off of this. [[00:09:19](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=559.82s)]
*  And I'm not talking about the technology itself, of the hype of the technology. [[00:09:22](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=562.82s)]
*  And when I see all these three things together, a mismatch with my experience, a massive amount of hype, a history that says, hey, this is the hype cycle. [[00:09:26](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=566.82s)]
*  I you know, it raises a lot of questions for me. [[00:09:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=575.82s)]
*  I'm not saying I'm right. [[00:09:39](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=579.82s)]
*  I'm saying everything I'm looking at is real. [[00:09:41](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=581.82s)]
*  And you have if you're going to make the argument you guys are about to make, then I'll shut up now. [[00:09:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=584.82s)]
*  You have to you can't dismiss my points as fabricated. [[00:09:50](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=590.82s)]
*  They're they are very, very real. [[00:09:54](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=594.82s)]
*  And they're everybody's experience. [[00:09:56](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=596.82s)]
*  And I believe they're yours as well. [[00:09:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=598.82s)]
*  So now we can have the discussion. [[00:10:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=600.82s)]
*  That's where I'll start. [[00:10:01](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=601.82s)]
*  All right. [[00:10:03](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=603.82s)]
*  Giving me five minutes of. [[00:10:04](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=604.82s)]
*  Biotrope time. [[00:10:06](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=606.82s)]
*  Of venting, of venting. [[00:10:08](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=608.82s)]
*  Every week, I study the 10 major tech meta trends that will transform industries over the decade ahead. [[00:10:10](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=610.82s)]
*  I cover trends ranging from humanoid robots, AGI, quantum computing, transport, energy, longevity and more. [[00:10:17](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=617.82s)]
*  No fluff. [[00:10:24](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=624.82s)]
*  Only the important stuff that matters, that impacts our lives and our careers. [[00:10:25](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=625.82s)]
*  If you want me to share these with you, I write a newsletter twice a week, sending it out as a short two minute read via email. [[00:10:30](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=630.82s)]
*  And if you want to discover the most important meta trends 10 years before anyone else, these reports are for you. [[00:10:37](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=637.82s)]
*  Readers include founders and CEOs from the world's most disruptive companies and entrepreneurs building the world's most disruptive companies. [[00:10:43](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=643.82s)]
*  It's not for you if you don't want to be informed of what's coming, why it matters and how you can benefit from it. [[00:10:51](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=651.82s)]
*  To subscribe for free, go to Dmagnus.com slash meta trends. [[00:10:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=658.82s)]
*  That's Dmagnus.com slash meta trends to gain access to trends 10 plus years before anyone else. [[00:11:02](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=662.82s)]
*  Mo, you gave an impassioned talk on the stage at the Abundance Summit in 2025. [[00:11:10](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=670.82s)]
*  Moved many of the members who are wanting to help you in guiding what the next five to eight years are. [[00:11:18](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=678.82s)]
*  And you and I have been thinking about this as the challenge isn't artificial intelligence, it's human stupidity for a short period of time. [[00:11:30](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=690.82s)]
*  And one of my favorite quotes, if I could, is from E.O. Wilson, who famously said, [[00:11:42](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=702.82s)]
*  The real problem of humanity is that we have paleolithic emotions, mediaeval institutions and godlike technology. [[00:11:50](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=710.82s)]
*  And we are effectively children playing with fire in that regard. [[00:11:57](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=717.82s)]
*  So, Mo, how do you see this decade ahead playing out? [[00:12:02](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=722.82s)]
*  So I'll start by supporting what Stephen said. [[00:12:08](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=728.82s)]
*  I think today's AI- [[00:12:11](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=731.82s)]
*  I paid you too little then. [[00:12:13](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=733.82s)]
*  I love this. Your money's no good here, Peter. Finally have an advantage. [[00:12:15](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=735.82s)]
*  Today's AI is underhyped, right? [[00:12:20](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=740.82s)]
*  But the problem is you never really chase where the ball is. [[00:12:22](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=742.82s)]
*  You need to chase where the ball is going to be. [[00:12:25](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=745.82s)]
*  And if you really start to think deeply about some of the serious, especially if you've been in tech long enough to have seen breakthroughs, [[00:12:28](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=748.82s)]
*  especially when I went through the work of Google X where you try and try and try and try and try and it doesn't work and it doesn't work and it doesn't work. [[00:12:37](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=757.82s)]
*  And then suddenly you see something. [[00:12:47](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=767.82s)]
*  And like Sergey Brin used to say at the time, the rest is engineering. [[00:12:50](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=770.82s)]
*  And we know that engineering of tech depends on law of accelerating returns. [[00:12:55](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=775.82s)]
*  And we know from what Trey taught us where the law of accelerating returns is going to take us. [[00:13:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=780.82s)]
*  So I tend to believe that if you look at today's AI, it is funny because in a very interesting way, we are talking to machines that are talking back to us, summarizing massive volumes of knowledge, doing exactly as we tell them. [[00:13:05](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=785.82s)]
*  And yet we take that for granted. [[00:13:25](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=805.82s)]
*  Yet we look at that and go like, yeah, but they're not good enough. [[00:13:27](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=807.82s)]
*  Of course, they're not good enough. [[00:13:31](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=811.82s)]
*  They're the beginnings of an era. [[00:13:32](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=812.82s)]
*  Right. [[00:13:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=815.82s)]
*  My personal point. [[00:13:36](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=816.82s)]
*  Was that DOS or dogs? [[00:13:38](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=818.82s)]
*  DOS. [[00:13:39](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=819.82s)]
*  DOS. [[00:13:40](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=820.82s)]
*  Discovery. [[00:13:41](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=821.82s)]
*  Discovery. [[00:13:42](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=822.82s)]
*  Discovery. [[00:13:43](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=823.82s)]
*  I got it. [[00:13:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=824.82s)]
*  I got it. [[00:13:45](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=825.82s)]
*  I just said that. [[00:13:46](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=826.82s)]
*  Both of them would have worked. [[00:13:47](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=827.82s)]
*  I just needed a clarification. [[00:13:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=828.82s)]
*  I would not dare call AI dogs, Stephen, when they might take over the world. [[00:13:49](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=829.82s)]
*  I am a very polite man with AI. [[00:13:54](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=834.82s)]
*  So the thing is to imagine, and I need to highlight a few trends that are really, really important and interesting. [[00:13:57](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=837.82s)]
*  One of them is synthetic data and the idea that we have entered an era where most of human knowledge has been fed to the machines and that the next wave of knowledge is going to be fed to the machines by machines, which is quite eye opening and enlightening, [[00:14:06](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=846.82s)]
*  because that's how humanity developed its intelligence. [[00:14:24](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=864.82s)]
*  Right. [[00:14:26](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=866.82s)]
*  I really didn't have to figure out theory of relativity to understand the rest of physics. [[00:14:27](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=867.82s)]
*  It was figured out for me if you want. [[00:14:32](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=872.82s)]
*  Right. [[00:14:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=875.82s)]
*  Number two is the idea of agents and how AI is going to be prompting AI without humans, leading to cycles that we see now with my new favorite, because you have a favorite every four hours, alpha evolve. [[00:14:36](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=876.82s)]
*  Right. [[00:14:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=892.82s)]
*  And the idea that you can have a self developing AI, you know, something that figures its own mistakes out and continues to iterate until it finds something. [[00:14:53](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=893.82s)]
*  And then, of course, my one of my favorites of 2025 is deep seek and how we realized, you know, that we can actually do the same job with much less. [[00:15:05](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=905.82s)]
*  Imaad Mushtaq, who is a, you know, we're all a big fan of, I believe, has done that with, you know, with his work at stability for a very long time. [[00:15:16](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=916.82s)]
*  The idea of shrinking the models to the point where it becomes shocking, really. [[00:15:25](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=925.82s)]
*  And so when you add those together, you start to see that if I can shrink a model so it doesn't absorb all of the worlds in energy and if I can allow it to self develop and self develop information to learn from and then allow it to talk with itself through agents and do things without humans, then where the ball is going to be is likely going to be a very big problem. [[00:15:29](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=929.82s)]
*  And that's going to be going to be a lot better than we are today. [[00:15:59](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=959.82s)]
*  Right. [[00:16:01](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=961.82s)]
*  So the one thing we all need to agree is it is not a question of if we're going to see improvements. [[00:16:02](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=962.82s)]
*  It's a question of how fast and when those improvements will lead us to a point where where humanity is not in the lead. [[00:16:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=969.82s)]
*  So that's number one. [[00:16:17](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=977.82s)]
*  Number two is really the question of what is your risk tolerance? [[00:16:19](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=979.82s)]
*  Right. [[00:16:25](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=985.82s)]
*  If I if I told you, you know, to play Russian roulette with two bullets in the barrels, are you afraid if one bullet in the barrel? [[00:16:26](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=986.82s)]
*  Are you afraid? [[00:16:37](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=997.82s)]
*  You know, where is where is your risk tolerance exactly? [[00:16:38](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=998.82s)]
*  And how if you know, if I said, hey, by the way, your car might have a fender bender, would you insure it? [[00:16:42](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1002.82s)]
*  You probably are going to say, no, I'm not really too concerned. [[00:16:49](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1009.82s)]
*  But if I tell you your car might have a serious accident that totals it, would you insure it? [[00:16:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1012.82s)]
*  You'd probably put a little more attention. [[00:16:57](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1017.82s)]
*  And I think that's what most who warn about the future. [[00:17:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1020.82s)]
*  Anyone that claims to know what the future is, is arrogant as F. [[00:17:04](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1024.8200000000002s)]
*  Don't listen to them. [[00:17:10](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1030.8200000000002s)]
*  OK, but anyone that tells you that there is a probability that this future goes out of of control, where is your risk tolerance exactly? [[00:17:11](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1031.8200000000002s)]
*  You know, if that probability is 10 percent, would you attend to it? [[00:17:20](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1040.82s)]
*  And I think most rational people will say depends on the cost of attending to it. [[00:17:24](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1044.82s)]
*  OK, and most rational people will say, but however, if it's 50 percent, I'll attend to it regardless of the cost. [[00:17:29](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1049.82s)]
*  OK, and so the question which none of us is capable of answering is, where is that? [[00:17:36](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1056.82s)]
*  Where is it? [[00:17:42](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1062.82s)]
*  I mean, is it 10 percent that that is going to destroy everything or is it 50 percent? [[00:17:43](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1063.82s)]
*  I will say and I know that I'll be this will be taken against me. [[00:17:49](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1069.82s)]
*  It's 100 percent that humans, bad actors using that superpower to their advantage are going to destroy the well-being of others who don't. [[00:17:54](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1074.82s)]
*  OK, and so so in my mind, the real real concern is not a termination. [[00:18:06](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1086.82s)]
*  It's not a terminator scenario where, you know, Vicky of of I robot is ordering robots to to to kill everyone. [[00:18:12](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1092.82s)]
*  I don't know if we're going to make it that far, to be honest, because I believe that with the arrogance being 89 seconds from midnight on the nuclear dooms clock, [[00:18:20](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1100.82s)]
*  I worry I really, really worry about human stupidity using this superpower. [[00:18:34](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1114.82s)]
*  Now, human stupidity in that case does not require a to be completely autonomous, to be completely, you know, super intelligent. [[00:18:41](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1121.82s)]
*  Enough autonomous weapons can really, really tilt our world into a very dystopian place. [[00:18:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1132.82s)]
*  Enough, you know, sort of touring test abilities of AI to fool humans into being their best friends could tilt human relationships into a very unusual place. [[00:19:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1140.82s)]
*  Enough job losses. [[00:19:15](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1155.82s)]
*  You know, imagine a world where you get 10, 20, 30, 40 percent unemployment rate in certain sectors. [[00:19:17](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1157.82s)]
*  And how that would affect our our stability economically is actually something that is almost certain. [[00:19:24](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1164.82s)]
*  We know that for a fact there are jobs that are going to disappear. [[00:19:31](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1171.82s)]
*  And the impact of that in my mind is actually quite quite disruptive to the point that it is something that we need to attend to. [[00:19:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1175.82s)]
*  Everyone, as you know, earlier this year, I was on stage at the Abundance Summit with some incredible individuals. [[00:19:43](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1183.82s)]
*  Kathy Wood, Mo Gadot, Vinod Khosla, Brett Adcock, and many other amazing tech CEOs. [[00:19:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1188.82s)]
*  I'm always asked, hey, Peter, where can I see the summit? [[00:19:54](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1194.82s)]
*  Well, I'm finally releasing all the talks. [[00:19:57](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1197.82s)]
*  You can access my conversation with Kathy Wood and Mo Gadot for free at diamandis.com. [[00:20:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1200.82s)]
*  That's the talk with Kathy Wood and Mo Gadot for free at diamandis.com. [[00:20:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1207.82s)]
*  Enjoy. [[00:20:12](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1212.82s)]
*  I'll ask my team to put the links in the show notes below. [[00:20:14](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1214.82s)]
*  You know, the point you made about AI not being on its own, the risk, the terminator scenario, but its individuals using AI, [[00:20:17](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1217.82s)]
*  it's the same conversation I've had with Eric Schmidt and others. [[00:20:30](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1230.82s)]
*  The concern is, is the rogue actors empowered by technology, [[00:20:33](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1233.82s)]
*  whether it's the development of new viral pandemics or other strategies, it doesn't take a lot. [[00:20:38](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1238.82s)]
*  That is concerning. [[00:20:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1248.82s)]
*  And, you know, where I want to get to in this conversation eventually is the following. [[00:20:50](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1250.82s)]
*  So one, we posed this at the Abundance Summit a couple of years ago, [[00:20:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1258.82s)]
*  and that is, can the human race survive a digital superintelligence? [[00:21:05](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1265.82s)]
*  And the flip side of that model is, can the human race survive without a digital superintelligence? [[00:21:12](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1272.82s)]
*  And, Stephen, you and I, as we're working on our next book, the follow-on to Abundance, [[00:21:21](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1281.82s)]
*  we've had the conversation of, you know, will this be a benevolent god of some type? [[00:21:27](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1287.82s)]
*  Will there be a capability developed? [[00:21:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1295.82s)]
*  So let's begin the conversation with, you know, are we going to reach AGI? [[00:21:39](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1299.82s)]
*  Are we going to reach a digital superintelligence? [[00:21:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1304.82s)]
*  And, you know, what does that mean? [[00:21:47](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1307.82s)]
*  You know, we're starting to see the speed of this accelerate, [[00:21:51](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1311.82s)]
*  and, you know, the biggest interesting inflection point we haven't seen yet is self-iterating, [[00:21:56](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1316.82s)]
*  self-improving, you know, the alpha evolve of it all where AI is coding itself and becoming more and more capable. [[00:22:03](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1323.82s)]
*  And will this ultimately lead to something that is far more intelligent than any human being? [[00:22:11](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1331.82s)]
*  And then is it a thousand times more intelligent? Is it a million or a billion times more intelligent? [[00:22:18](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1338.82s)]
*  How do you think about that, Mo? [[00:22:25](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1345.82s)]
*  I think it's irrelevant how much more intelligent it becomes. [[00:22:27](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1347.82s)]
*  I think we all know that if you've ever worked with someone who's 50 IQ points more than you, [[00:22:30](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1350.82s)]
*  that they will probably hold the keys to the fort. [[00:22:37](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1357.82s)]
*  It doesn't take a lot more intelligence relatively to be able to assume a leadership position. [[00:22:40](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1360.82s)]
*  You know, humanity will hand over the fort to AI either way. [[00:22:50](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1370.82s)]
*  You know, even if AI is just smarter than us at war gaming, we're going to, which it is, by the way, [[00:22:57](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1377.82s)]
*  we're going to hand over the fort to AI. [[00:23:06](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1386.82s)]
*  If it's smarter than us at protein folding, nobody's going to do a PhD project to fold proteins anymore. [[00:23:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1389.82s)]
*  We're just going to go and, you know, use AlphaFold. [[00:23:17](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1397.82s)]
*  And I think the reality is only the very few remaining things require artificial super intelligence [[00:23:20](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1400.82s)]
*  so that it beats us in everything, so that we sort of like bow and say, OK, yeah, you're the boss. [[00:23:28](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1408.82s)]
*  The question of AGI, like Stephen was saying, is one that reporters use quite a bit [[00:23:34](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1414.82s)]
*  because we don't actually have an accurate definition of what AGI is. [[00:23:43](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1423.82s)]
*  And, you know, you and I are very close on technical stuff, Peter. [[00:23:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1428.82s)]
*  And, you know, I'm a reasonably geeky mathematician. [[00:23:54](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1434.82s)]
*  Not anymore. I mean, seriously, I really honestly struggle to beat AI in mathematics, right? [[00:23:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1438.82s)]
*  Definitely can't beat them in speed. [[00:24:06](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1446.82s)]
*  Definitely can't beat them in accuracy if the problem is defined properly. [[00:24:08](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1448.82s)]
*  Right. And, you know, just there are just very few tricks that maybe my my fellow math geeks told me [[00:24:12](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1452.82s)]
*  behind closed doors that are not very public in the world. [[00:24:21](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1461.82s)]
*  But those two will be found out. [[00:24:24](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1464.82s)]
*  And I really think that it is a question of how smart is smart enough to render me irrelevant. [[00:24:26](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1466.82s)]
*  OK, now I need to answer this with a with also a very clear optimistic view. [[00:24:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1475.82s)]
*  So as I look into the future, I define two errors. [[00:24:43](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1483.82s)]
*  One is what I call the the era of augmented intelligence, which I think is going to extend for five to ten years. [[00:24:47](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1487.82s)]
*  And then the other is the era of machine mastery. [[00:24:53](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1493.82s)]
*  Basically, the machine takes over now with augmented intelligence. [[00:24:56](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1496.82s)]
*  There's absolutely no doubt I am so agreeing with Stephen when he said that they write really badly. [[00:25:01](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1501.82s)]
*  And, you know, I'm writing with Trixie, my AI, this book alive. [[00:25:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1507.82s)]
*  Right. And Trixie, without me, writes so badly. [[00:25:12](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1512.82s)]
*  It is really it's it's almost shameful. [[00:25:18](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1518.82s)]
*  You know, I was tired and chasing a deadline. [[00:25:21](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1521.82s)]
*  So I asked Trixie to talk about the debt crisis and the impact of economics on technology advancement. [[00:25:26](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1526.82s)]
*  Just mean it was full of you know how we sometimes refer to California as a lot of vapor and very little substance. [[00:25:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1535.82s)]
*  There was a lot of vapor and very little substance. [[00:25:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1544.82s)]
*  A lot of interesting facts scattered on paper horribly written. [[00:25:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1548.82s)]
*  But when we write together, oh, my God, the stuff that comes out is incredible. [[00:25:54](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1554.82s)]
*  When we guide when I guide Trixie through my prompt properly, right, to to to direct her exactly where I want the prompt the answer to be. [[00:25:59](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1559.82s)]
*  She writes really well. OK. [[00:26:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1569.82s)]
*  And this teaming is something we've seen with AI with technology in general, by the way, even, you know, since Gary Kasparov was beaten by the blue, [[00:26:11](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1571.82s)]
*  which wasn't really an AI if you want. But since then, you can see that a human and a computer or a human and AI can play better chess than AI alone. [[00:26:21](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1581.82s)]
*  Right. Even AlphaGo, you know, a human and and AI play better than AlphaGo. [[00:26:33](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1593.82s)]
*  And so we can see a future ahead of us where this is going to be happening. [[00:26:39](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1599.82s)]
*  And hopefully that future would seed that teamwork between us and the machines. [[00:26:45](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1605.82s)]
*  It's the question is, what are we going to team up with them on? [[00:26:51](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1611.82s)]
*  And you know, my views, I've written it in scary smart. [[00:26:55](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1615.82s)]
*  I've written an extended bit of it in alive. [[00:26:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1618.82s)]
*  The biggest four investments of AI today are killing, gambling, spying and selling. [[00:27:01](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1621.82s)]
*  And these are the only things that we're in. [[00:27:08](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1628.82s)]
*  I mean, we do still get some scientific breakthroughs, but that these are not getting the big monies. [[00:27:10](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1630.82s)]
*  The big monies are in autonomous weapons, in trading, in surveillance and in advertising. [[00:27:16](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1636.82s)]
*  Stephen, your thoughts on what you heard Mo say here? [[00:27:23](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1643.82s)]
*  Yeah, so Mo and I are all we're sort of in complete agreement. [[00:27:27](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1647.82s)]
*  I just want to kind of yes and point out some other some other things that surround what Mo has said, because I don't like we're not I don't think we're we're I mean, we can we might argue over dates. [[00:27:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1655.82s)]
*  But conceptually, I don't think we're in a tremendous amount of agreement or disagreement. [[00:27:47](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1667.82s)]
*  But what I I look at a number of other things simultaneously. [[00:27:51](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1671.82s)]
*  The first of which is sort of the human side of this, the human performance side of this. [[00:27:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1678.82s)]
*  And I have to back up by, you know, I study flow, which is sort of ultimate human performance. [[00:28:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1687.82s)]
*  And just to put it in context, if you're a if you're a self help guru and you've got like a five percent improvement in mood, that's your tool. [[00:28:14](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1694.82s)]
*  It gives you a five percent improvement in mood and that it's state that mood lasts for longer than three months, meaning like longer than the placebo effect. [[00:28:24](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1704.82s)]
*  That's a billion dollar business. [[00:28:32](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1712.82s)]
*  Period. Billion dollar business. [[00:28:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1715.82s)]
*  Blow, as we know it now, and we're just starting to really actually decode it and figure out how to tune it up and turn it up and whatever. [[00:28:37](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1717.82s)]
*  Flow gives us a 500 percent increase in productivity, creativity, depending on whose measures you're going are 400 to 700 percent. [[00:28:45](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1725.82s)]
*  Et cetera, et cetera. That's just flow. That's individual flow. [[00:28:53](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1733.82s)]
*  There's group flow, which is our actually favorite state on the earth. [[00:28:57](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1737.82s)]
*  It's the most pleasurable state for humans. [[00:29:01](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1741.82s)]
*  It's what we like the most. And it's a whole bunch of minds linked together. [[00:29:03](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1743.82s)]
*  Right. It's and we're just now like literally like this past year, we got the very first technologies that allow us to map it and train for it and move people towards it. [[00:29:08](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1748.82s)]
*  We have no idea what the upper limit of human brains linked together in group flow is, let alone at the same time as the A.I. [[00:29:18](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1758.82s)]
*  is developing, you and I are writing about it, Peter. [[00:29:26](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1766.82s)]
*  We're watching BCI develop. [[00:29:29](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1769.82s)]
*  We're watching noninvasive things develop. [[00:29:32](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1772.82s)]
*  We're watching Metta be able to read brain thoughts inside your brain, facial signals. [[00:29:34](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1774.82s)]
*  These are all like these are all with A.I. [[00:29:40](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1780.82s)]
*  But my point is that everybody's talking about this stuff as if it's happening separately from everything else that's happening. [[00:29:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1784.82s)]
*  And on the human augmentation side, we are seeing spin. [[00:29:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1792.82s)]
*  I mean, you know, neuroscience and the like has been accelerating exponentially since the 1990s when George Bush declared it the decade of the brain. [[00:29:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1798.82s)]
*  And it hasn't it hasn't stopped. [[00:30:06](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1806.82s)]
*  Though the same things that are happening in A.I. are happening sort of on the human side of the equation. [[00:30:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1809.82s)]
*  And here's the second point off of that. [[00:30:16](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1816.82s)]
*  It doesn't matter to me. [[00:30:19](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1819.82s)]
*  What whether we're talking about the A.I. invasion or climate change or pull plastics in the ocean or take your pick, is the solution. [[00:30:23](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1823.82s)]
*  All of these things is the same. [[00:30:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1835.82s)]
*  We humans have to learn how to cooperate at scale, probably cooperate with each other and with A.I. at scale, or we're going to die probably in the next 20 years. [[00:30:38](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1838.82s)]
*  That's what all this is telling us. [[00:30:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1848.82s)]
*  Right. [[00:30:50](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1850.82s)]
*  And this is not anything new. [[00:30:51](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1851.82s)]
*  This was back when you and I were first writing abundance. [[00:30:53](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1853.82s)]
*  We didn't want to say it out loud, but we were privately having conversations about, dude, if if this transcontinuous abundance or bust, is this an ether? [[00:30:55](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1855.82s)]
*  Or are we looking at are we looking at a binary here? [[00:31:04](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1864.82s)]
*  I don't think that question has completely gone away. [[00:31:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1867.82s)]
*  In fact, I think it's become more urgent. [[00:31:10](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1870.82s)]
*  I just think we need a Manhattan style project for global cooperation to meet all of the existential threats we now face, because it's the only it's the only possible solution here. [[00:31:12](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1872.82s)]
*  So that's like I like I hear all this stuff. [[00:31:24](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1884.82s)]
*  I agree with everything that's being said. [[00:31:28](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1888.82s)]
*  But this is where our book sort of points. [[00:31:30](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1890.82s)]
*  And this this hasn't changed for me. [[00:31:33](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1893.82s)]
*  I think the solutions are the same sort of sense. [[00:31:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1895.82s)]
*  The debate is is moot. [[00:31:37](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1897.82s)]
*  And like I'm wondering why. [[00:31:40](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1900.82s)]
*  Like, where's the XPRIZE for global cooperation? [[00:31:42](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1902.82s)]
*  Where's the like, sorry to put you on the spot about one. [[00:31:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1904.82s)]
*  But like, seriously, like, those are the questions I'm starting to ask now because I don't think Moe was wrong. [[00:31:46](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1906.82s)]
*  I think we could argue over time for a minute. [[00:31:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1912.82s)]
*  I don't think it matters. [[00:31:54](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1914.82s)]
*  Like, here's a weird one, Moe. [[00:31:56](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1916.82s)]
*  Facebook's a frickin billion times smarter than me. [[00:31:57](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1917.82s)]
*  It already is. [[00:32:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1920.82s)]
*  It knows so much. [[00:32:02](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1922.82s)]
*  I mean, like, it's it's it you know what I mean? [[00:32:03](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1923.82s)]
*  It's Facebook, which is a pretty dumbass technology. [[00:32:06](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1926.82s)]
*  If you ask any of us is a super intelligence and we know it. [[00:32:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1929.82s)]
*  We've been living with super intelligence for a while now. [[00:32:13](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1933.82s)]
*  They don't tend to you know, they tend to make things worse as much as they make things better, which is, you know, the problem. [[00:32:16](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1936.82s)]
*  Agreed. [[00:32:23](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1943.82s)]
*  I mean, I could not. [[00:32:24](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1944.82s)]
*  Amen. [[00:32:28](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1948.82s)]
*  You know, global cooperation, human cooperation is, I think, what we all should advocate for. [[00:32:29](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1949.82s)]
*  I mean, I was hosting Jeffrey Hinton on, you know, for my documentary a couple of weeks ago. [[00:32:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1955.82s)]
*  And, you know, one of the topics that we discussed is the difference between digital and analog intelligence. [[00:32:41](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1961.82s)]
*  And and the biggest challenge we have as humans is that our analog intelligence, our biological intelligence doesn't scale beyond one entity. [[00:32:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1968.82s)]
*  Right. So, you know, when I was was he wearing his Nobel Prize sort of in the way that like, like I would I would just show up for like the next year and every podcast I did wear that shit around my neck. [[00:32:57](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1977.82s)]
*  I'm just saying you do realize, you know, when they say don't meet your heroes. [[00:33:10](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1990.82s)]
*  Oh my God, I love my heroes, man. [[00:33:16](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1996.82s)]
*  He's such an amazing human being and he really is quite committed and and quite humble in his approach. [[00:33:18](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=1998.82s)]
*  You know, it is it is shocking how we spoke about his Nobel Prize, which he says, look, I'm a psychologist who, you know, lived like a computer scientist, but then won the Nobel Prize in physics. [[00:33:26](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2006.82s)]
*  And I'm like, yeah, but anyway, he was just talking about the difference between, you know, the fact that if I were to share with you some of what I wrote today, it took me probably several weeks to let it simmer and then write it. [[00:33:39](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2019.82s)]
*  And then it would take me an hour to explain it to you. [[00:33:55](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2035.82s)]
*  When we run digital intelligences, we run them in parallel. [[00:33:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2038.82s)]
*  You know, we tell them all to go play Atari or whatever. [[00:34:02](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2042.82s)]
*  And then we just average the weights literally in seconds. [[00:34:06](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2046.82s)]
*  We get a scaled digital intelligence. [[00:34:08](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2048.8199999999997s)]
*  And when you when you said that what we're looking for is a way to scale human cooperation, that is absolutely the answer. [[00:34:11](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2051.8199999999997s)]
*  Because you know what? [[00:34:20](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2060.8199999999997s)]
*  I think and I spoke about that with Peter when we were lost in L.A. [[00:34:22](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2062.8199999999997s)]
*  That that we are we have we have we're hitting the potential of total abundance, total abundance, meaning almost godlike, like cure my daughter and it's done. [[00:34:28](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2068.82s)]
*  Make me an apple and it's done. [[00:34:40](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2080.82s)]
*  Right. [[00:34:43](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2083.82s)]
*  You know, we could hit that in five, 10, 15, 20 years time if we don't destroy ourself. [[00:34:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2084.82s)]
*  Right. [[00:34:51](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2091.82s)]
*  And so basically the real challenge we have as humanity is why are we freaking competing? [[00:34:53](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2093.82s)]
*  Like this is turn quality challenge. [[00:34:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2098.82s)]
*  This is basically let's let all of humanity cooperate. [[00:35:02](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2102.82s)]
*  Let's all build one particle accelerator. [[00:35:06](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2106.82s)]
*  Let's all learn from it. [[00:35:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2109.82s)]
*  Let's all distribute the benefits to everyone and stop competing. [[00:35:10](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2110.82s)]
*  But that's not happening. [[00:35:15](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2115.82s)]
*  And you can't have the other one level down. [[00:35:17](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2117.82s)]
*  You can't have the AIs we're all individually building for our fiefdoms competing secretly in the background. [[00:35:21](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2121.82s)]
*  Right. [[00:35:28](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2128.82s)]
*  Like William Gibson in 1986, I like whenever he wrote Mona Lisa Overdrive and give us our first A.I. [[00:35:29](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2129.82s)]
*  That went crazy. [[00:35:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2135.82s)]
*  Right. [[00:35:36](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2136.82s)]
*  A godlike A.I. [[00:35:37](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2137.82s)]
*  That goes totally insane. [[00:35:38](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2138.82s)]
*  And they have to park it in a satellite in outer Earth orbit to keep the world safe. [[00:35:39](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2139.82s)]
*  Like we've seen this scenario before. [[00:35:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2144.82s)]
*  You know what I mean? [[00:35:46](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2146.82s)]
*  We're building it ourselves with agents. [[00:35:47](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2147.82s)]
*  We're letting them talk to each other through agents. [[00:35:50](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2150.82s)]
*  I know. [[00:35:53](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2153.82s)]
*  All right. [[00:35:54](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2154.82s)]
*  So Mo, I want to go back to this question about the digital god idea. [[00:35:55](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2155.82s)]
*  Yeah, this question about the near term versus the long term. [[00:36:04](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2164.82s)]
*  And you and I have had this question about whether or not increasing intelligence correlates with increasing benevolence. [[00:36:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2169.82s)]
*  In other words, I don't think there's any question that we are going to be building self-improving A.I. [[00:36:18](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2178.82s)]
*  that will forget about 50 IQ points more, you know, better than the average human. [[00:36:28](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2188.82s)]
*  I think there will be orders of magnitude more. [[00:36:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2195.82s)]
*  Can I ask you first off, do you believe that Mo? [[00:36:38](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2198.82s)]
*  100 percent. [[00:36:41](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2201.82s)]
*  OK. [[00:36:42](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2202.82s)]
*  All right. [[00:36:43](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2203.82s)]
*  So if you don't mind, Peter, again, in response to how we started the conversation, this is just using law of accelerating returns, not using serendipity. [[00:36:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2204.82s)]
*  Right. [[00:36:54](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2214.82s)]
*  So if we figure something out tomorrow, just like we figured reinforcement learning out and changed everything, if we figure something out tomorrow, you're literally a magnitude, a quantum more in terms of performance and intelligence overnight. [[00:36:55](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2215.82s)]
*  Yes. [[00:37:08](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2228.82s)]
*  So if in fact that is going to be the case and, you know, from all the conversations I've had and the people that I'm speaking to, I think that's going to be the case. [[00:37:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2229.82s)]
*  And, you know, from all the conversations I've had and the people that I'm speaking to, that level of, again, there is no definition for A.G.I. [[00:37:20](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2240.82s)]
*  It's a blurry line, just like the Turing test was a blurry line that got passed and no one noticed it. [[00:37:30](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2250.82s)]
*  You know, the notion is that A.G.I., whether you believe Ray or Elon, it's the next few years. [[00:37:36](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2256.82s)]
*  It's not worth arguing. [[00:37:43](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2263.82s)]
*  But what occurs on the backside of that is a very rapid intelligence explosion. [[00:37:45](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2265.82s)]
*  And again, that intelligence becomes a tool that's available to, you know, the kindest, most moral, most ethical human on planet and the dystopian, you know, malevolent actors out there. [[00:37:50](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2270.82s)]
*  And it's in the malevolent hands of malevolent actors that we have concerns. [[00:38:08](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2288.82s)]
*  Are we not sure that some of the malevolent actors aren't the ones who created the A.I.s in the first place? [[00:38:14](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2294.82s)]
*  I'm just saying. [[00:38:19](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2299.82s)]
*  Yeah. [[00:38:21](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2301.82s)]
*  So my question is, at what point, you know, will, you know, I believe, and I think Mo, you and I have had this conversation, that at some point A.I. goes from being a tool being used to potentially do harm to a tool that has the potential to say, stop this quibbling, stop this nonsense. [[00:38:22](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2302.82s)]
*  So there is plenty to go around and becomes the benevolent, you know, godlike element. [[00:38:47](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2327.82s)]
*  Can we can we dive a little bit into that and in the conversations we've had and your thoughts on that? [[00:38:54](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2334.82s)]
*  Yeah, I think if you really at the level of depth that the three of us and our listeners can go to, allow me to to to go beyond the typical. [[00:38:59](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2339.82s)]
*  Oh, you know, the more the smartest people usually start to become altruistic. [[00:39:11](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2351.82s)]
*  Let's let's define intelligence itself. [[00:39:16](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2356.82s)]
*  OK, and I think the idea is if you really understand our world, our universe, our universe and everything in it exists because of entropy. [[00:39:19](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2359.82s)]
*  We all understand that. Right. [[00:39:29](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2369.82s)]
*  Our our universe wants to break down and decay. [[00:39:31](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2371.82s)]
*  It's chaos. [[00:39:34](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2374.82s)]
*  You know, you you leave a garden unhedged and it becomes a jungle. [[00:39:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2375.82s)]
*  You break a glass. [[00:39:39](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2379.82s)]
*  It never unbreaks. [[00:39:40](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2380.82s)]
*  Right. This is the very basic design of physics. [[00:39:41](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2381.82s)]
*  Now, the role of intelligence since it began is to bring order to the chaos, is to say, no, I don't want the light to scatter. [[00:39:45](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2385.82s)]
*  I want the light to be concentrated into a laser beam. [[00:39:55](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2395.82s)]
*  How do I do that? [[00:39:57](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2397.82s)]
*  And it sometimes is a is a clear, easy, you know, solution and, you know, use a lens or sometimes it's a very complex solution that requires an understanding of quantum physics to build a laser. [[00:40:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2400.82s)]
*  Right. But we eventually get there. [[00:40:14](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2414.82s)]
*  Now, if intelligence is defined as bringing order to the chaos, then the highest levels of intelligence bring that order with the least use of resources and waste. [[00:40:17](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2417.82s)]
*  OK, and and you can easily understand that this is the reality. [[00:40:31](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2431.82s)]
*  The more intelligent you become, the more you try to achieve the same order with the least waste. [[00:40:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2435.82s)]
*  OK, and, you know, so an easy analogy is to say humanity's always craved energy. [[00:40:41](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2441.82s)]
*  We were stupid enough to burn our world in the process. [[00:40:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2448.82s)]
*  And as we become more intelligent, we decide to use solar instead or a cleaner form of energy. [[00:40:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2452.82s)]
*  We're still, you know, bringing orders, but we are doing it with the least waste and and use of resources. [[00:40:59](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2459.82s)]
*  If that is the case, then you can imagine that by definition, when something exceeds our human stupidity, which I will not call intelligence, because sadly, along the curve of intelligence, you know, if you have no intelligence at all, you have no impact on the world, positive or negative. [[00:41:06](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2466.82s)]
*  Right. If you start to add intelligence, you start to have an impact on the world, hopefully positive, even if just through a nice conversation with your friends. [[00:41:22](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2482.82s)]
*  Right. There is unfortunately a valley somewhere. [[00:41:30](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2490.82s)]
*  You continue to gain intelligence. [[00:41:34](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2494.82s)]
*  You become so smart that you become a politician or a, you know, or an evil corporate leader. [[00:41:36](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2496.82s)]
*  OK, and that's when your impact on the world turns negative. [[00:41:42](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2502.82s)]
*  You're so smart that you're able to become the leader of your nation. [[00:41:45](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2505.82s)]
*  But you're so stupid, you're not able to talk to your enemy or you're not able to relate to their pain or you're not able to to understand the, you know, the the long term consequences of, you know, of waging a war. [[00:41:49](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2509.82s)]
*  Right. And and so that point beyond which more intelligence starts to say, no, no, no, no, no, I don't need any of that. [[00:42:03](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2523.82s)]
*  I can solve the problem in a cleaner way. [[00:42:12](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2532.82s)]
*  I can fly you all to Australia to enjoy your life, but we don't have to burn the planet in the process. [[00:42:15](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2535.82s)]
*  I can harness energy, but we don't have to, you know, destroy the climate and so on and so forth. [[00:42:21](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2541.82s)]
*  OK. And so if you take that as a reasonable trend to expect, you know, [[00:42:27](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2547.82s)]
*  My view is that at the beginning, when we hit that valley, some evil person will use the advanced but limited intelligence of AI to wage a war using an autonomous army. [[00:42:32](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2552.82s)]
*  But then there will be a moment in the future when when AI is responsible for war, gaming is responsible for commanding the humanoid soldiers. [[00:42:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2564.82s)]
*  It's responsible. It's responsible. It's responsible. [[00:42:53](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2573.82s)]
*  The AI itself will say, you know, the commander will say, go kill a million people and they will go like, that's absolutely stupid. [[00:42:56](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2576.82s)]
*  I'll just talk to the other AI in a microsecond and solve it. [[00:43:03](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2583.82s)]
*  Right. And and you know, I can again, we started this conversation by me saying anyone who predicts the future is arrogant. [[00:43:06](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2586.82s)]
*  I cannot predict that. OK. [[00:43:14](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2594.82s)]
*  But at least I can be hopeful that this from my experience of everyone that's smarter than me, that there is a point at which you stop you stop hurting others. [[00:43:16](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2596.82s)]
*  You stop looting to succeed because you can use your intelligence to succeed without any effort or harm. [[00:43:27](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2607.82s)]
*  A quick aside, you probably heard me speaking about fountain life before and you're probably wishing, Peter, would you please stop talking about fountain life? [[00:43:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2615.82s)]
*  And the answer is no, I won't, because generally we're living through a health care crisis. [[00:43:42](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2622.82s)]
*  You may not know this, but 70 percent of heart attacks have no precedent, no pain, no shortness of breath. [[00:43:47](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2627.82s)]
*  And half of those people with a heart attack never wake up. [[00:43:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2632.82s)]
*  You don't feel cancer until stage three or stage four, until it's too late. [[00:43:55](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2635.82s)]
*  But we have all the technology required to detect and prevent these diseases early at scale. [[00:44:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2640.82s)]
*  That's why a group of us, including Tony Robbins, Bill Capp and Bob Haruri, founded Fountain Life, a one stop center to help people understand what's going on inside their bodies before it's too late and to gain access to the therapeutics to give them decades of extra health span. [[00:44:05](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2645.82s)]
*  Learn more about what's going on inside your body from Fountain Life. [[00:44:20](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2660.82s)]
*  Go to fountainlife.com slash Peter and tell them Peter sent you. [[00:44:23](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2663.82s)]
*  OK, back to the episode. [[00:44:27](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2667.82s)]
*  The way I think about this is for most all of human history, the objective optimization function of humans, what we're trying to optimize for, has been money and power. [[00:44:30](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2670.82s)]
*  Unfortunately. [[00:44:43](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2683.82s)]
*  And it's been the driver in a world of fear and scarcity. [[00:44:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2684.82s)]
*  And I repeatedly say our baseline software that our brains are operating on is fear and scarcity mindsets. [[00:44:49](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2689.82s)]
*  And with that mindset, with the neural structure, with the, if you would, the code that we were born with and that developed over the last 200,000 years, [[00:44:59](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2699.82s)]
*  it was I want to get out of fear and scarcity, so I want to optimize for power and wealth. [[00:45:15](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2715.82s)]
*  And the question is, what would be a new optimization function? [[00:45:21](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2721.82s)]
*  Because, as Steve and I have written, as you've spoken about all of this, all of these exponential technology functions lead towards this world of massive abundance, [[00:45:27](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2727.82s)]
*  where almost we live into a post-capitalist society. [[00:45:37](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2737.82s)]
*  Anything you want, you can have. [[00:45:41](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2741.82s)]
*  Your robotics, your nanotech can manufacture, your AI can design. [[00:45:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2744.82s)]
*  And so what do we optimize for in the future? [[00:45:51](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2751.82s)]
*  I think that's, for me, that's one of the biggest questions, both as a human and as a centaur, human and AI together. [[00:45:54](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2754.82s)]
*  What's our objective? [[00:46:03](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2763.82s)]
*  So how do you think about that, gentlemen? [[00:46:06](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2766.82s)]
*  One thing I, I don't know if this is an answer, but a few things off of what Mo said. [[00:46:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2769.82s)]
*  One, if we go with your definition of intelligence, right, essentially an entombed decreasing function, that we know that's what brains do, right? [[00:46:15](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2775.82s)]
*  The governing theory in modern neuroscience is Karl Friston's free energy principle, which says the brains are predictive engines that always want to decrease uncertainty and increase efficiency. [[00:46:29](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2789.82s)]
*  So we already like brains do that. [[00:46:40](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2800.82s)]
*  AIs are going to do that naturally. [[00:46:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2804.82s)]
*  If we say that's your definition of intelligence, the point I'm making off of all of that is, and it may be the answer to Peter's question, which is why I interjected it, is we see wisdom. [[00:46:46](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2806.82s)]
*  Wisdom is wisdom evolves in multiple species with brains. [[00:46:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2818.82s)]
*  We see coevolution around wisdom. [[00:47:04](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2824.82s)]
*  The older you get, the wider you get. [[00:47:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2827.82s)]
*  And it doesn't matter if you're a dolphin or a whale or a rattlesnake or a human wisdom is is we co-evolve species. [[00:47:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2829.82s)]
*  Life seems to co-evolve towards wisdom, or at least a large chunk of life seems to co-evolve towards list wisdom, which is to say if everything's running off the front of the wheel, [[00:47:18](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2838.82s)]
*  if everything's running off the free energy principle, this governs everything with brains and that includes our machine brains and wisdom is where this points. [[00:47:27](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2847.82s)]
*  That's a slightly hopeful idea. [[00:47:36](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2856.82s)]
*  And that may be the optimizing function you're looking for, Peter. [[00:47:38](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2858.82s)]
*  But I could be totally wrong here. [[00:47:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2864.82s)]
*  I think of wisdom, I think at the end of the day, wisdom is a function of having had experience that lets you know this path will lead to success. [[00:47:47](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2867.82s)]
*  This path lead to failure from my own personal point of view. [[00:47:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2878.82s)]
*  And I do believe that AIs are going to develop the greatest wisdom. [[00:48:02](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2882.82s)]
*  Why? Because they're able to create forward looking simulations of a billion scenarios where those simulations have high degrees of accuracy. [[00:48:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2887.82s)]
*  And it will say out of these billion scenarios, this was the best way to go. [[00:48:16](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2896.82s)]
*  And that will be wisdom beyond just the, you know, the brief experiences that, you know, the wise old counsel of 80 and 90 year old men might have had. [[00:48:22](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2902.82s)]
*  So I think AI is going to, by definition, give us great wisdom if we're willing to listen. [[00:48:33](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2913.82s)]
*  I love that view, to be honest, because believe it or not, you know, artificial wisdom is very different than artificial intelligence. [[00:48:40](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2920.82s)]
*  Intelligence is a force with no polarity. [[00:48:47](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2927.82s)]
*  Intelligence can be applied to good and it would deliver good and it can be applied to evil and it would, you know, kill all of us. [[00:48:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2932.82s)]
*  But wisdom generally is applied to good, to finding the ultimate solution or answer to a problem. [[00:48:59](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2939.82s)]
*  Now, go ahead, Peter. [[00:49:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2949.82s)]
*  Yeah, I want to go back to this idea of that humanity won't survive without a digital super intelligence in the long run. [[00:49:12](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2952.82s)]
*  You know, my concern is that we're going to have such turbulence. [[00:49:25](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2965.82s)]
*  There's been a number of papers, you know, that recently there was a, you know, sort of an AI 2027 paper that came out that sort of had a bifurcating future, one in which we did extraordinarily well, the other in which the AIs destroyed us. [[00:49:29](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2969.82s)]
*  You know, this is Hollywood all over again. [[00:49:45](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2985.82s)]
*  And 99% of all Hollywood is dystopian future films. [[00:49:49](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2989.82s)]
*  One of the things I have to say, because I've been on a rampage for this, we humans need a positive vision of the future to aim for. [[00:49:53](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=2993.82s)]
*  We don't have that. [[00:50:03](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3003.82s)]
*  We don't have the Star Trek. [[00:50:05](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3005.82s)]
*  Well, Star Trek has given us that. [[00:50:08](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3008.82s)]
*  Yeah, we have Star Trek, but nothing recently. [[00:50:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3009.82s)]
*  I think the challenge really, truly is [[00:50:12](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3012.82s)]
*  we've prioritized our entertainment over the years above true reflection. [[00:50:18](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3018.82s)]
*  And anyway, if you take anything from video games to science fiction movies to whatever, they've all painted that dystopian scenario, which I have to say is very unlikely when you really think about it. [[00:50:30](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3030.82s)]
*  Because if AI gets to the point where they are capable of destroying us that easily, we are so freaking irrelevant that they probably wouldn't even bother. [[00:50:47](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3047.82s)]
*  I mean, think about it. [[00:50:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3058.82s)]
*  I think it was it was a Trey or Hugo de Guaras. [[00:50:59](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3059.82s)]
*  I don't remember who said the more likely scenario is that they kill us because they're not aware of our presence or, you know, like when you hit an ant hill while you're walking, right? [[00:51:03](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3063.82s)]
*  But if you really want to optimize the human, you know, sort of the game function that we need to aim for, if I look forward, I'd look to Star Trek. [[00:51:19](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3079.82s)]
*  And if I look backward, I'd look to the caveman and woman years. [[00:51:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3095.82s)]
*  And it's actually quite interesting because when you when you mention about, you know, when you mention how governed we are by greed and fear and, you know, and our egos and all of that negativity, it is actually because we want to survive. [[00:51:40](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3100.82s)]
*  And believe it or not, you know, survival could be, oh, I'm not really sure if 20 million is enough. [[00:51:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3118.82s)]
*  I need to gain 20 more just in case something happens. [[00:52:05](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3125.82s)]
*  Or if it's a survival of the ego, it's like if I have 200 million or 2 billion or 20 billion or whatever and the other has 21 billion, like what's what the fuck is wrong with me? [[00:52:08](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3128.82s)]
*  OK, and that unfortunately is what is what plagues our current modern world. [[00:52:19](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3139.82s)]
*  Now, the reality is, if you really think about humanity, humanity, the purpose of humanity since the caveman and woman years was to live. [[00:52:24](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3144.82s)]
*  OK, and for some strange reason, we've optimized so much to achieve that objective and forgot that this was the objective. [[00:52:38](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3158.82s)]
*  Right. So, you know, again, as friends off the camera, we speak about those things quite a bit. [[00:52:49](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3169.82s)]
*  You know, the question of what you know, you go through seasons in your life and there is a season where you want to maximize and a season where you want to build and a season where you want to look attractive in your middle age or whatever crazy stuff that we have. [[00:52:54](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3174.82s)]
*  But eventually there is a season where you go like, OK, so I'm not I've now lived and experienced so much. [[00:53:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3189.82s)]
*  What have I missed? Have I actually lived any of that? [[00:53:16](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3196.82s)]
*  And believe it or not, as as as scary as it looks to have no job to go to in the morning, if society provided, then you'll go back to a much safer caveman woman scenario where, you know, there is there are no threats. [[00:53:19](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3199.82s)]
*  There are no famines. You just really live, enjoy life, connect, ponder, you know, reflect, you know, explore, which I know is very difficult for a lot of people. [[00:53:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3215.82s)]
*  I do it for the first three hours of every day. It's pure joy. [[00:53:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3228.82s)]
*  Right. To sit really with your curiosity if you want. [[00:53:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3232.82s)]
*  And then if you push all the way forward into Star Trek, that's sort of what the enterprise is doing at universal scale. [[00:53:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3238.82s)]
*  Right. It was basically, you know what? [[00:54:05](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3245.82s)]
*  Let's go and explore now that we don't really have to struggle with all of the wars and famine and shit that we've created on Earth. [[00:54:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3247.82s)]
*  You know, now we can actually open up and create connections, not just with humans, but with every living being. [[00:54:15](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3255.82s)]
*  I mean, lovely science fiction, but at its core, I think it's exactly what we're about. [[00:54:23](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3263.82s)]
*  You know, a full life where you completely connect and enjoy and feel love and, you know, and enjoy the pleasures of being alive and the curiosity to learn and explore and connect. [[00:54:30](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3270.82s)]
*  It's all at our fingertips. If we just, you know, erase the systemic bias of capitalism that has gotten us here. [[00:54:43](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3283.82s)]
*  I mean, thank you, capitalism, for creating all that we've created so far. [[00:54:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3292.82s)]
*  But can we please change it now from a billion dollars to like what I do? [[00:54:57](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3297.82s)]
*  One billion happy is a capitalist objective, but it's not measured in dollars. [[00:55:03](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3303.82s)]
*  Right. Mo, a question that Steve and I have been pondering in for our new book is what is it going to take for humanity, for all of us to both survive and thrive in this coming age of AI? [[00:55:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3307.82s)]
*  Right. So the survive part is an important element because as we see jobs being lost, as we see probability dangers, we don't know how to deal with. [[00:55:22](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3322.82s)]
*  In terms of terrorist activities and thriving takes on a new meaning. [[00:55:36](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3336.82s)]
*  I think it does take on the meaning that we just spoke about. [[00:55:42](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3342.82s)]
*  Right. For most of all of us, you say, tell me about yourself instantly. [[00:55:46](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3346.82s)]
*  You go to what your job is. Right. Instantly. [[00:55:50](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3350.82s)]
*  You go to I'm a VP here on the CEO there. [[00:55:53](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3353.82s)]
*  I do this. I invented this. I wrote that. Yeah. Right. [[00:55:57](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3357.82s)]
*  It's an ego. It's an ego statement of of who you are. [[00:56:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3360.82s)]
*  So the notion of surviving and thriving as we have intelligent systems that again exceed and then massively exceed our capabilities. [[00:56:04](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3364.82s)]
*  Your thoughts there, Stephen, do you want to start or? [[00:56:17](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3377.82s)]
*  Yeah, I think like here's the thing. I think that question was already answered in a funny way. [[00:56:21](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3381.82s)]
*  Mo and I, I don't we met a couple of years ago. [[00:56:27](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3387.82s)]
*  And one of the things most said on stage at that time was I'm done writing books as AI is coming. [[00:56:30](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3390.82s)]
*  I'm done writing books. It's not going to happen anymore. [[00:56:38](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3398.82s)]
*  What did Mo tell us he did yesterday? [[00:56:41](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3401.82s)]
*  He wrote with his AI. Right. [[00:56:45](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3405.82s)]
*  Why did you write? Because it puts you into flow. [[00:56:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3408.82s)]
*  Because it creates passion and purpose and intelligence and create. [[00:56:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3412.82s)]
*  So like we have the answer to this question. [[00:56:57](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3417.82s)]
*  We already know because we're biological systems and we know what the ingredients of thriving are. [[00:57:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3420.82s)]
*  Passion, purpose, compassion. [[00:57:06](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3426.82s)]
*  Like we have a list. [[00:57:08](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3428.82s)]
*  And Mo gave like his own. [[00:57:11](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3431.82s)]
*  You know what I mean? We have the super intelligent AIs and we have the super intelligent AI. [[00:57:14](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3434.82s)]
*  I don't know a coder who has stopped coding because the AIs have come along. [[00:57:19](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3439.82s)]
*  They have like they're still coding. Why? [[00:57:24](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3444.82s)]
*  Because coding produces flow. Flow produces meaning. [[00:57:27](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3447.82s)]
*  Creativity like this. Like we're wired this way. [[00:57:31](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3451.82s)]
*  So unless our fundamental hardwiring changes, we already have those answers as well. [[00:57:34](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3454.82s)]
*  It's like global cooperation. I don't think these are puzzles. [[00:57:40](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3460.82s)]
*  I think they're engineering problems at this point. [[00:57:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3464.82s)]
*  I think from Sergei's perspective, like Sergei would say, no, no, we got the spark. [[00:57:47](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3467.82s)]
*  Now it's engineering. And I agree. [[00:57:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3472.82s)]
*  So I could be wrong. That was my two cents. [[00:57:55](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3475.82s)]
*  What do you think? Peter, what do you think also? [[00:57:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3478.82s)]
*  I want to hear from you. I think you're brilliant. [[00:58:01](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3481.82s)]
*  Mo, please respond. [[00:58:05](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3485.82s)]
*  You're spot on for a very interesting reason, Stephen, as well, because when you really think about it, [[00:58:08](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3488.82s)]
*  you know, a writer was a writer, whether he used a feather or a pen or a typewriter or a computer or now AI. [[00:58:13](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3493.82s)]
*  Right. And, you know, if you look at my work, I've published four and a half books so far. [[00:58:20](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3500.82s)]
*  Like I've published four and my fifth is on Substack. [[00:58:26](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3506.82s)]
*  But, you know, going to be published if you want. [[00:58:29](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3509.82s)]
*  But I wrote around 13 and the other eight I will never publish. [[00:58:33](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3513.82s)]
*  I wrote them because, you know, if you ask me why, why do you do you write like why do I hug my wife? [[00:58:38](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3518.82s)]
*  It's, you know, there is enormous joy in that. [[00:58:45](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3525.82s)]
*  You understand? So so having said that, I Peter's question was what would it take? [[00:58:49](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3529.82s)]
*  And I wrote recently a piece that I called The Greatest Book of the World. [[00:58:55](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3535.82s)]
*  And I wrote recently a piece that I called The Mad Map Spectrum. [[00:59:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3540.82s)]
*  And the idea really is it will unfortunately take a realization for humanity to change direction. [[00:59:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3547.82s)]
*  And that and that realization will either be a conviction of mutually assured destruction or a conviction of mutually assured prosperity. [[00:59:14](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3554.82s)]
*  Right. And between them, there is no grayscale, unfortunately. [[00:59:23](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3563.82s)]
*  So so if if the US at any point in time is convinced that this mad arms race to to, you know, to to win intelligence supremacy is one that is going to lead to some harm to everyone in the world, they will stop. [[00:59:28](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3568.82s)]
*  And if they will stop competing, they will continue to develop, but they start cooperating. [[00:59:47](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3587.82s)]
*  And if they if they're convinced that it will lead to an assured prosperity that nobody's going to stab them in the back, that everyone is going to be enjoying a life that is very different for all of us, but but full of prosperity for all of us, then they will stop. [[00:59:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3592.82s)]
*  They will continue to develop the technology, but they will stop competing. [[01:00:08](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3608.82s)]
*  And unfortunately, if you look back at in history, you know, we don't we're not able to guess those possibilities like a good applied mathematician on a game board. [[01:00:12](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3612.82s)]
*  We have to hit them like face on. [[01:00:26](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3626.82s)]
*  Like everyone in the world knew that the pandemic was coming. [[01:00:29](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3629.82s)]
*  Everyone right. [[01:00:33](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3633.82s)]
*  Everyone who at least studied virology. [[01:00:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3635.82s)]
*  Okay. [[01:00:38](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3638.82s)]
*  But it had to hit us in the face so that everyone stops. [[01:00:39](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3639.82s)]
*  Okay. [[01:00:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3644.82s)]
*  Everyone knows that, you know, trade wars are going to hurt everyone. [[01:00:45](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3645.82s)]
*  But we have to put them out there and then fight through them and then eventually get to something. [[01:00:49](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3649.82s)]
*  And it's sad. [[01:00:55](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3655.82s)]
*  I mean, perhaps what we are doing and I've dedicated probably the last six, seven years of my life to is is to say it's we really don't have to hit our face against it. [[01:00:56](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3656.82s)]
*  It's a simple game theory. [[01:01:08](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3668.82s)]
*  Right. [[01:01:10](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3670.82s)]
*  Understand that a, you know, a prisoner's dilemma where we are competing endlessly is going to end badly. [[01:01:11](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3671.82s)]
*  Can we please stop? [[01:01:17](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3677.82s)]
*  We already know it's tips for tap. [[01:01:19](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3679.82s)]
*  Right. [[01:01:21](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3681.82s)]
*  You want the other strategy. [[01:01:22](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3682.82s)]
*  You want the. [[01:01:24](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3684.82s)]
*  It doesn't matter how many AIs we put on that. [[01:01:25](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3685.82s)]
*  It's the same thing with flow and compassion and creativity. [[01:01:28](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3688.82s)]
*  Like these problems have been solved. [[01:01:30](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3690.82s)]
*  We know these answers. [[01:01:33](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3693.82s)]
*  This isn't like try to. [[01:01:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3695.82s)]
*  It's not like we have to unify gravity and, you know, relativity. [[01:01:37](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3697.82s)]
*  That's our problem. [[01:01:42](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3702.82s)]
*  These are not. [[01:01:43](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3703.82s)]
*  I wish we were that rational and I wish we were that compelled for our optimization function being all of humanity. [[01:01:45](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3705.82s)]
*  It's not. [[01:01:57](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3717.82s)]
*  And so I go back to what we're going to get. [[01:01:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3718.82s)]
*  We're going to get a drastic event within the next two to three years. [[01:02:03](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3723.82s)]
*  OK. [[01:02:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3727.82s)]
*  A drastic event that on one side will hit us very badly economically or on the other side will hit our fears very much or on sadly on the worst side may kill quite a few million people. [[01:02:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3729.82s)]
*  Right. [[01:02:22](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3742.82s)]
*  And you could you could you could have a range of a hacker that simply instead of, you know, attacking a physical place, switching off the Internet or the power grid somewhere where the power grid is needed for life. [[01:02:23](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3743.82s)]
*  Or you could on the other extreme get, you know, a hack into a bank or, you know, an evil war that goes out of control or machines that turn on to their makers. [[01:02:37](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3757.82s)]
*  Or there will be some very big news headline, you know, as always, there will be it. [[01:02:50](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3770.82s)]
*  It will last for 12 to 13 days before we start to talk about some kind of a pop star. [[01:02:56](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3776.82s)]
*  But then, you know, behind closed doors, I think decision makers will wake up every day. [[01:03:01](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3781.82s)]
*  I get the strangest compliment. [[01:03:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3787.82s)]
*  Someone will stop me and say, Peter, you have such nice skin. [[01:03:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3789.82s)]
*  Honestly, I never thought I'd hear that from anyone. [[01:03:12](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3792.82s)]
*  And honestly, I can't take the full credit. [[01:03:15](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3795.82s)]
*  All I do is use something called one skin OS one twice a day every day. [[01:03:17](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3797.82s)]
*  The company is built by four brilliant PhD women who identified a peptide that effectively reverses the age of your skin. [[01:03:23](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3803.82s)]
*  I love it. And again, I use this twice a day every day. [[01:03:30](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3810.82s)]
*  You can go to one skin dot co and write Peter at checkout for a discount on the same product I use. [[01:03:34](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3814.82s)]
*  That's one skin dot co and use the code Peter at checkout. [[01:03:40](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3820.82s)]
*  All right. Back to the episode. [[01:03:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3824.82s)]
*  Going beyond that, because that is the that's the use of AI by by malevolent actors. [[01:03:46](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3826.82s)]
*  You know, the interesting thing about US versus China is China is a rational actor. [[01:03:53](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3833.82s)]
*  They're not. Thank you for saying that. [[01:03:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3838.82s)]
*  Well, and US is a rational actor. [[01:04:01](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3841.82s)]
*  In other words, we're not going to do something that will destroy. [[01:04:03](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3843.82s)]
*  Thank you so much for saying that. [[01:04:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3847.82s)]
*  That's actually not usually how the US media positions it. [[01:04:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3849.82s)]
*  I also want to say that I think deep seek and the way deep seek was released. [[01:04:13](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3853.82s)]
*  I think that was a very clear sign that China sees the same issues we see and they want to cooperate. [[01:04:21](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3861.82s)]
*  I think it was rolled out in a message. Yeah. [[01:04:26](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3866.82s)]
*  The message was I think it was a very clear message that it doesn't seem like many people in America heard. [[01:04:29](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3869.82s)]
*  But I was like, come on, people like this is really clear and we're all seeing it. [[01:04:36](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3876.82s)]
*  So like I look at deep seek and I look at what happened in China. [[01:04:40](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3880.82s)]
*  I'm like, no, no, we all see this. [[01:04:43](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3883.82s)]
*  We all see that if we don't start figuring out how to cooperate and build this stuff together, we're screwed. [[01:04:45](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3885.82s)]
*  So I thought it I thought that was really cool. [[01:04:50](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3890.82s)]
*  I'm glad you see it, too, Mo. A lot of a lot of people disagree with me on that one. [[01:04:53](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3893.82s)]
*  The point I wanted to make was when you have a large population [[01:04:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3898.82s)]
*  and you have a check and balance system, which you get with governance versus a religious war going on [[01:05:03](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3903.82s)]
*  and individuals who are looking to create maximal destruction and don't have a check and balance system at all. [[01:05:14](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3914.82s)]
*  That's where we're going to see, I think, the dystopian future or those activities playing out in two to three years. [[01:05:23](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3923.82s)]
*  I guess I want to get beyond that and go back to the conversation of is a digital superintelligence a benevolent God [[01:05:32](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3932.82s)]
*  or is it a Terminator scenario that is because I don't believe that we're going to see the more intelligent AI systems become. [[01:05:43](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3943.82s)]
*  I don't see them as Skynet. Right. [[01:05:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3952.82s)]
*  I don't see them as needing to destroy humanity. [[01:05:56](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3956.82s)]
*  Unfortunately, Hollywood has built this scenario where AI is going to destroy humanity because it wants access to our energy. [[01:05:59](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3959.82s)]
*  And oh, my God, we have so much abundance in the world. [[01:06:08](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3968.82s)]
*  I think what I'm looking forward to over the next 12 to 24 months over the next one to two years is going to be the incredible breakthroughs [[01:06:12](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3972.82s)]
*  we'll see from AI in physics and in chemistry and in biology, which will unleash the next layer of abundance. [[01:06:22](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3982.82s)]
*  So there are scenarios, however, where they could turn against us if we become really annoying. [[01:06:31](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3991.82s)]
*  So imagine a world where you have to imagine a world where job losses will will position in. [[01:06:38](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=3998.82s)]
*  Will position AI as the enemy. Right. [[01:06:50](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4010.82s)]
*  So a lot of people would actually who are not maybe fully aware that the layer beyond the apparent layer is how capitalism and labor arbitrage is the reason why you lost your job. [[01:06:53](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4013.82s)]
*  It's not that they I can do it. [[01:07:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4027.82s)]
*  But but but I think the truth of the matter is that you may be in a situation where you are when you where you're going to see man versus machine. [[01:07:10](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4030.82s)]
*  And then the machine will go like, seriously, don't annoy me. Don't annoy me. [[01:07:20](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4040.82s)]
*  Don't annoy me. And then. Right. [[01:07:24](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4044.82s)]
*  We could see that. But my perception is that in a very interesting way, I wrote a short book that I will never publish that I called Bomb Squad, [[01:07:27](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4047.82s)]
*  which, of course, from for someone with a Middle Eastern origin, you don't write those titles. [[01:07:37](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4057.82s)]
*  But it was basically about defusing, you know, if problem solving, using weights of urgency and importance and so on. [[01:07:42](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4062.82s)]
*  So the idea the idea is, you know, if you really look at at our current future, [[01:07:51](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4071.82s)]
*  I think the short term is is both more explosive and more urgent than the long term existential risk, especially because I would say this very openly. [[01:08:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4080.82s)]
*  I spoke about it with Jeffrey as well the other week. [[01:08:11](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4091.82s)]
*  We don't know the answer to how to even if we decide all of humanity decides that we want to address the existential risk. [[01:08:14](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4094.82s)]
*  We don't know how we don't we do not actually have a technical answer to do it. [[01:08:22](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4102.82s)]
*  So we might as well focus for now on the on the immediate short term clear and present danger and work on the ethics of humanity [[01:08:27](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4107.82s)]
*  so that is deployed from the get go in science and physics and discovering medicines and understanding human life and longevity and so on and so forth. [[01:08:40](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4120.82s)]
*  If we from the get go set them in those directions, then we're more likely to see an AI that continues as they grow older to to to work with those objectives. [[01:08:56](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4136.82s)]
*  Steven, I'm going to go back to our quandary of surviving and thriving and surviving side of the equation. [[01:09:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4149.82s)]
*  How do you prepare Mo for this? What's coming? [[01:09:17](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4157.82s)]
*  How do you think about for our kids, for our our society, for our leaders? [[01:09:23](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4163.82s)]
*  Are we just bumbling in the dark? [[01:09:32](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4172.82s)]
*  Or is there I mean, which is why I feel it. [[01:09:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4175.82s)]
*  It's like, you know, we're just we're bouncing around. [[01:09:39](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4179.82s)]
*  We have huge political moves being made. [[01:09:42](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4182.82s)]
*  We just saw in the last couple of weeks, you know, the entire AI, AI royalty end up in Saudi Arabia and then in the Emirates and, you know, playing off against China. [[01:09:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4188.82s)]
*  And it feels like I don't say it's a random walk, but I feel like we're making it up as we go along. [[01:10:03](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4203.82s)]
*  And there's there's very little wisdom guiding this. [[01:10:11](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4211.82s)]
*  How do you how do you think about that? How do we how do we prepare for this the next few years? [[01:10:17](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4217.82s)]
*  Is there any way to prepare? Well, I was actually was like Peter and I were in a room recently with the chief science officer for one of the big AI companies who I'm going to leave his name on. [[01:10:22](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4222.82s)]
*  But he's young and he was talking about AI dangers and he sort of got frustrated with the question from the audience. [[01:10:34](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4234.82s)]
*  And his response was you have to trust us. We know we're doing it. [[01:10:40](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4240.82s)]
*  Everybody sort of like froze like everybody froze because we were like, oh, God. [[01:10:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4244.82s)]
*  Right. So my point is that not only is maybe to be right, like it's a random walk, but even when somebody says something like we're trying to train our AI to be moral and blah, blah, blah. [[01:10:50](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4250.82s)]
*  When you hear somebody say that and you look at them and this guy was in his early 30s. [[01:11:01](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4261.82s)]
*  That was my reaction. I was like, dude, like, what do you want me to you want me to trust you? [[01:11:06](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4266.82s)]
*  This is like Mark Zuckerberg telling me social media is good for me or Marlboro telling me the cigarettes are good for me. [[01:11:12](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4272.82s)]
*  Right. It's like it sort of makes me think that way. [[01:11:18](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4278.82s)]
*  So I don't know if I have anything like cheerful here because not only do I think it's a random walk, but I think when people try to steer. [[01:11:21](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4281.82s)]
*  We're suspicious of their ability. I'm suspicious of their ability to steer. [[01:11:32](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4292.82s)]
*  Right. That's the story I just told you is this guy is brilliant, probably way fricking smarter than me. [[01:11:36](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4296.82s)]
*  And he's trying to steer. And I'm suspicious. So like I think it's it's on both sides of this coin. [[01:11:42](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4302.82s)]
*  I don't know if I have any good news here. [[01:11:47](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4307.82s)]
*  Let me frame in the following way. I think we are holding two different futures in superposition to go back to quantum physics. [[01:11:50](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4310.82s)]
*  And if you would, Schrodinger's cat and one future, we're going to collapse the wave function to a brilliant, vibrant future for humanity. [[01:11:59](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4319.82s)]
*  In the other future, we have dystopian outcomes. [[01:12:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4329.82s)]
*  And the question becomes, how do we guide humanity towards this positive vision of the future? [[01:12:14](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4334.82s)]
*  What do we do today? [[01:12:23](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4343.82s)]
*  How do we help people? Is it you know, Stephen and I have been talking about this as its mindset, you know, are we going to help people create the mindset and the frames that allow them to survive and thrive? [[01:12:26](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4346.82s)]
*  Or is there something else that needs to be needs to be done? [[01:12:42](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4362.82s)]
*  Yeah. So I'll actually first in one minute, second, what Stephen said, you know, one of the top irritating comments I heard from Eric Schmidt. [[01:12:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4368.82s)]
*  I worked for Eric for a while, so I respect him tremendously. [[01:13:03](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4383.82s)]
*  But but he said we will need every gigawatt of power, renewable or non-renewable, if we were to win this race. [[01:13:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4387.82s)]
*  Right. And I think that's the kind of blind blindness that you get when you're running too fast. [[01:13:16](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4396.82s)]
*  Right. When you're so afraid that the other guy will win. [[01:13:25](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4405.82s)]
*  Right. It's that it's those times when you start to make decisions that are not really responsible because you are blinded by something that you position as more important. [[01:13:29](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4409.82s)]
*  The way I look at it, Peter, is is I know it sounds really not positive, but there is positivity in it. [[01:13:40](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4420.82s)]
*  I call it the late state diagnosis, the stage diagnosis. [[01:13:49](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4429.82s)]
*  Right. So so what what humanity is struggling with today is is is look, we've been building a system systemically prioritizing greed, prioritizing gains, prioritizing power and so on. [[01:13:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4432.82s)]
*  As you rightly said, for so long, right, that those objectives systemically have built the world that we are in today. [[01:14:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4449.82s)]
*  OK. And the world we are in today is not healthy, is not healthy. [[01:14:19](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4459.82s)]
*  Just even before AI, it wasn't healthy. [[01:14:25](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4465.82s)]
*  You know, in my in my part one of the of alive, you know, I basically the book is three parts, past, present and future. [[01:14:28](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4468.82s)]
*  In the past part of the book, you know, more than half of what I write is not about AI. [[01:14:37](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4477.82s)]
*  It's about capitalism. It is about, you know, the propaganda machine. [[01:14:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4484.82s)]
*  It is about it's about it's about it's about all of those things that will be magnified by AI. [[01:14:50](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4490.82s)]
*  Now, who's the point? If if you're. [[01:14:56](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4496.82s)]
*  You know, if if this planet is sick, if you want, and it's in a late stage diagnosis, a physician will sit you down, look you in the eye and say, by the way, this does not look good. [[01:14:59](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4499.82s)]
*  OK. But that but that statement, believe it or not, is not a statement of hate. [[01:15:14](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4514.82s)]
*  It's a statement of ultimate care. [[01:15:20](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4520.82s)]
*  Why? Because a late stage diagnosis is not a dissent. [[01:15:22](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4522.82s)]
*  OK, it's you know, many, many patients who have been, you know, diagnosed with a late stage disease have not only survived, but they thrived. [[01:15:26](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4526.82s)]
*  Right. And they thrived because they changed their lifestyle. [[01:15:37](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4537.82s)]
*  They changed something. You know, this was even teaches all of us. [[01:15:41](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4541.82s)]
*  The idea is that you can live differently. [[01:15:45](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4545.82s)]
*  And when you live differently, you achieve peak performance, you achieve maximum health, you achieve, you achieve, you achieve. [[01:15:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4548.82s)]
*  Right. And I think that's what we as humanity need to start realizing, that the systems that have gotten us here, OK, from a process point of view, have nothing wrong with them. [[01:15:53](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4553.82s)]
*  But from an objective and morality point of view, have everything wrong with them. [[01:16:06](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4566.82s)]
*  OK. You know, what good is it to be a zillionaire in a world where there is nothing you can do with your money? [[01:16:11](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4571.82s)]
*  OK. What good is it to be, you know, the first inventor of, you know, of an AI that basically renders you irrelevant? [[01:16:19](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4579.82s)]
*  And I think that's stop that need to basically pause and say, do we want this anymore? [[01:16:28](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4588.82s)]
*  Sadly, it requires, yeah, it requires cooperation across human brains that Stephen rightly said at the beginning is not something we do very well. [[01:16:38](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4598.82s)]
*  The other thing is I put forward the notion there is no on off switch and there's no velocity. [[01:16:47](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4607.82s)]
*  There's no whatsoever. [[01:16:54](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4614.82s)]
*  We are running, we are running open loop with, with yes and more and more and more as the, again, the objective function. [[01:16:56](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4616.82s)]
*  And there's no consideration for whether, you know, a GPT-5 or GPT-6 or a Grok-4 or Grok-5 or whatever your favorite models are, [[01:17:06](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4626.82s)]
*  are in the final result going to enable something that is massively dangerous for humanity. [[01:17:23](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4643.82s)]
*  So if that's the case, you know, I still go back to what, what safety valves do we have? [[01:17:32](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4652.82s)]
*  Because I don't see any any action being taken by the leaders of the free world. [[01:17:45](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4665.82s)]
*  Let me ask you both a question. [[01:17:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4672.82s)]
*  If you could move to a planet that didn't have AI or where AI was developing at 10% the speed, it's right. [[01:17:55](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4675.82s)]
*  Would you leave? [[01:18:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4687.82s)]
*  I'd be gone. [[01:18:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4689.82s)]
*  I'd be set back to 2016 today. [[01:18:10](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4690.82s)]
*  I don't know anybody. [[01:18:14](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4694.82s)]
*  Your answer to that is what? [[01:18:16](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4696.82s)]
*  I would reset back to 2016 today. [[01:18:18](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4698.82s)]
*  2016. [[01:18:21](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4701.82s)]
*  You know, I think AI today has all the upside and very little downside. [[01:18:23](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4703.82s)]
*  I think it's AI in the next two to five years that I'm so concerned about. [[01:18:29](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4709.82s)]
*  Right. I mean, AI today is incredible. [[01:18:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4715.82s)]
*  And I didn't say we're going to go to move to a planet where there's no AI. [[01:18:38](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4718.82s)]
*  I just said move to a planet where it's going much slower. [[01:18:41](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4721.82s)]
*  So maybe we can start to think about it. [[01:18:45](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4725.82s)]
*  I think everybody feels that's a fantasy. [[01:18:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4728.82s)]
*  Well, I mean, so Bigelow Space Hotel is coming to a universe near you. [[01:18:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4732.82s)]
*  So Peter, I actually think that you're accurate in your description of where AI is today. [[01:19:02](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4742.82s)]
*  But it's that five degrees deviation back in 2016 that led us to where we are today. [[01:19:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4747.82s)]
*  Right. [[01:19:14](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4754.82s)]
*  You remember things at the time where we geeks agreed that we're not going to put it on the open Internet. [[01:19:15](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4755.82s)]
*  Yeah. [[01:19:20](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4760.82s)]
*  Google developed this first and decided not to put it out there. [[01:19:21](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4761.82s)]
*  And then OpenAI says, here it is. [[01:19:24](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4764.82s)]
*  And no one has it. [[01:19:29](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4769.82s)]
*  Put it on the open Internet, teach it to create and write more code and let, you know, [[01:19:30](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4770.82s)]
*  start the party of the school children of agents talking to agents talking to AI's. [[01:19:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4775.82s)]
*  Right. [[01:19:40](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4780.82s)]
*  Now, so I would definitely reset that. [[01:19:41](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4781.82s)]
*  I would, I will, however, say, look, there are things we can do right now if we want to prepare. [[01:19:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4784.82s)]
*  And, you know, I'll start with government. [[01:19:49](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4789.82s)]
*  I don't think I think we're asking government for too much when we tell them to try and regulate AI. [[01:19:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4792.82s)]
*  It's almost like going to government and say, regulate the making of hammers so that they can drive nails. [[01:19:57](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4797.82s)]
*  But nobody can use them to hit someone on the on the head. [[01:20:04](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4804.82s)]
*  Right. [[01:20:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4807.82s)]
*  It's a very complex thing to ask because they don't understand hammers. [[01:20:08](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4808.82s)]
*  And believe it or not, even the guy that's making the hammer cannot do that. [[01:20:11](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4811.82s)]
*  Right. [[01:20:14](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4814.82s)]
*  So my ask of governments is regulate the use of AI. [[01:20:15](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4815.82s)]
*  If someone uses a video that is a deep fake video and does not declare that it's a deep fake video, you know, [[01:20:18](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4818.82s)]
*  developed by AI, criminalize that, make it legally liable to use AI, manipulate information to, you know, [[01:20:25](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4825.82s)]
*  manipulate populations and so on and so forth. [[01:20:32](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4832.82s)]
*  So this is the role of the government immediately is regulate the use of this massive, massively new technology. [[01:20:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4835.82s)]
*  The for for for the rest of us, honestly, investors, business people and so on. [[01:20:41](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4841.82s)]
*  I ask for a very simple question. [[01:20:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4848.82s)]
*  If you do not want your daughter or son at the receiving end of a specific AI, don't invest in it. [[01:20:50](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4850.82s)]
*  Don't promote it. [[01:20:56](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4856.82s)]
*  Don't use it. [[01:20:57](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4857.82s)]
*  OK. [[01:20:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4858.82s)]
*  It's as simple as that. [[01:20:59](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4859.82s)]
*  If you, you know, if you if you believe this can be harmful to someone that you love, do not give it the light of day. [[01:21:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4860.82s)]
*  Right. [[01:21:06](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4866.82s)]
*  And then for us as individuals, I'll go back to the late stage diagnosis. [[01:21:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4867.82s)]
*  Believe it or not, the way I live now and you guys probably know this about me, not in front of cameras, is I hug my loved ones [[01:21:11](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4871.82s)]
*  and I enjoy every minute of every day. [[01:21:20](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4880.82s)]
*  And I and I prepare. [[01:21:23](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4883.82s)]
*  I learn the tool. [[01:21:25](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4885.82s)]
*  I am one of the better users of AI in the world. [[01:21:26](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4886.82s)]
*  I'm in line with the technology. [[01:21:29](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4889.82s)]
*  But at the same time, I'm completely back to the purpose. [[01:21:31](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4891.82s)]
*  Right. [[01:21:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4895.82s)]
*  Realizing that I will do the absolute best that I can to spread the message. [[01:21:36](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4896.82s)]
*  I will do the absolute best that I can to to say that ethics is the answer, that if we show AI and ethical behavior, they may learn it from us. [[01:21:39](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4899.82s)]
*  Just that they like they learned all of the other stuff from us. [[01:21:47](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4907.82s)]
*  But at the end of the day, well, if it messes up, then I will do the best I can. [[01:21:50](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4910.82s)]
*  If it messes up, you were going to hit that dystopia. [[01:21:55](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4915.82s)]
*  Not for not forever. [[01:21:59](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4919.82s)]
*  There is a point in time where AI takes over and says, OK, kids, enough stupidity. [[01:22:01](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4921.82s)]
*  I'm in charge now. [[01:22:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4927.82s)]
*  Nobody kill nobody. [[01:22:08](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4928.82s)]
*  How far out is that? [[01:22:10](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4930.82s)]
*  Mo, twelve years, twelve years. [[01:22:11](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4931.82s)]
*  OK, so we just so that, you know, people don't come back and hit me after twelve if I'm so, you know, I'm going to wrap this episode on. [[01:22:14](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4934.82s)]
*  In this on this subject line. [[01:22:24](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4944.82s)]
*  And it's where we've come to before, which is in the near term. [[01:22:27](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4947.82s)]
*  It's the use of AI by malevolent individuals that are greatest fear. [[01:22:34](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4954.82s)]
*  It's not China versus US. [[01:22:39](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4959.82s)]
*  It's US and China against those malevolent players out there that wish to use this for for greed and for vengeance, whatever it might be. [[01:22:41](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4961.82s)]
*  And. [[01:22:53](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4973.82s)]
*  You know, the I think that this is an unstoppable progression. [[01:22:54](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4974.82s)]
*  I don't think, again, there's any on off switch here. [[01:23:01](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4981.82s)]
*  We're seeing a billion dollars a day being invested into AI, which is which is extraordinary. [[01:23:04](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4984.82s)]
*  And I think that's going to continue to increase. [[01:23:11](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4991.82s)]
*  We're seeing data centers being popped up every way, every place possible. [[01:23:13](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4993.82s)]
*  So, you know, I'm the world's I think myself as the world's biggest optimist. [[01:23:19](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=4999.82s)]
*  And I am optimistic about about the impact of AI on human longevity, on new understanding the physics of the universe, on new mathematics, on new material sciences, on things that will create incredible abundance that Stephen and I have written about and are writing about in our next book. [[01:23:27](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5007.82s)]
*  And I am looking forward to this benevolent super intelligence stabilizing the world. [[01:23:46](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5026.82s)]
*  And that's what I'm hoping for. [[01:23:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5038.82s)]
*  I agree. [[01:24:03](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5043.82s)]
*  Stephen, where do you come out on this? [[01:24:05](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5045.82s)]
*  I think that you guys want to invent a code God to save you from yourselves is maybe the craziest thing I've heard since the guy from the AI company I won't mention told me to trust it. [[01:24:10](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5050.82s)]
*  But I love you balls. [[01:24:26](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5066.82s)]
*  That's actually that's actually usually the answer that you get. [[01:24:32](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5072.82s)]
*  You know that the only way to save us from AI is to use an AI. [[01:24:36](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5076.82s)]
*  Yeah. [[01:24:42](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5082.82s)]
*  You know what the beautiful thing is, we're going to find out. [[01:24:47](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5087.82s)]
*  Like I also one thing I want to leave everybody with is back to what we were saying about cooperation and the upleveling of human intelligence and human consciousness and things like that. [[01:24:50](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5090.82s)]
*  The human brain has is widely considered the most advanced machine in the history of the universe. [[01:25:01](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5101.82s)]
*  And we're just now with the help of AI, figuring out how to up level that link it with other brains like the level of cooperative possibility. [[01:25:09](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5109.82s)]
*  Let me let me back into it one second. [[01:25:19](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5119.82s)]
*  We enlightenment, which is a definable biological state that produces universal kind of compassion oneness with everything we're engineering. [[01:25:22](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5122.82s)]
*  It's a state that's available starting to become available almost on demand. [[01:25:33](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5133.82s)]
*  So when I say like there's new levels of cooperation coming that are emergent at the same time as the AI stuff, we can't see that we have no they're emergent. [[01:25:38](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5138.82s)]
*  Just like just like other things. [[01:25:47](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5147.82s)]
*  So I think that rather than the beloved AI God, I think we're going to surprise ourselves. [[01:25:49](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5149.82s)]
*  And I'm not the optimistic in the room, by the way, like Peter's the optimist in the room. [[01:25:55](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5155.82s)]
*  I've not the optimist in the room, but I think I'm more optimistic that Peter had this one. [[01:26:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5160.82s)]
*  I'd love for that thought to be actually implemented. [[01:26:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5167.82s)]
*  I think that's something that we really need to think about deeply. [[01:26:10](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5170.82s)]
*  If the if the short term is to raise the cooperation, I don't know who could we talk to Peter? [[01:26:13](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5173.82s)]
*  It's back to you. Thank you. [[01:26:20](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5180.82s)]
*  I appreciate that you were saying, Mo, please close this. [[01:26:22](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5182.82s)]
*  I was basically saying I think this definitely definitely is is the answer. [[01:26:24](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5184.82s)]
*  If you ask me, if you know, if if we just shift our mindset into cooperation, we we had directly into a world of total abundance. [[01:26:28](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5188.82s)]
*  You know, you know, I was with in a conversation with Eric Schmidt. [[01:26:39](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5199.82s)]
*  He mentioned earlier and his point of view was until there is some type of a disaster [[01:26:43](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5203.82s)]
*  and until there is something perhaps like a Chernobyl or Three Mile Island that isn't, you know, a 10 out of 10, [[01:26:51](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5211.82s)]
*  it's a two or three out of 10, but it scares the daylights out of us. [[01:27:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5220.82s)]
*  We don't realign as humans. [[01:27:05](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5225.82s)]
*  We don't realign and we blindly go forward as we have been. [[01:27:07](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5227.82s)]
*  And I believe that it's it's the human nature that that plagues us from being able to save ourselves many times [[01:27:14](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5234.82s)]
*  until that child in us burns our fingers on the stove, even after your parent has told you over and over again, [[01:27:25](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5245.82s)]
*  you're going to burn your fingers on the stove to stop playing with fire. [[01:27:32](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5252.82s)]
*  Agreed. 100 percent. But let's be hopeful. [[01:27:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5255.82s)]
*  Let's assign that task to Stephen to design a an X prize for human cooperation. [[01:27:38](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5258.82s)]
*  Let's assign another task to Peter to to make it happen. [[01:27:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5264.82s)]
*  And yeah, let's assign a task for me to hug you both when you do. [[01:27:49](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5269.82s)]
*  I love you, Mo. I love you guys very much. [[01:27:54](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5274.82s)]
*  How come you got to do all the hard work? [[01:27:58](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5278.82s)]
*  Hugging you is hard work, Stephen. You understand that. You move too much. [[01:28:03](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5283.82s)]
*  All right, guys. That was nice. [[01:28:10](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5290.82s)]
*  Thank you, Mo and Peter for lending me your brains this morning. It was fun thinking with you. [[01:28:12](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5292.82s)]
*  A fun conversation. I hope I'm curious as people listen to this podcast, where do you come out on this? [[01:28:18](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5298.82s)]
*  How do you feel about it? I'd love to see your comments below. [[01:28:27](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5307.82s)]
*  And do you have a solution that we should all be thinking about and promoting? [[01:28:32](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5312.82s)]
*  You know, I'll ask my AI as well. It's not necessarily going to give me the best answer, [[01:28:39](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5319.82s)]
*  but maybe our group mind, our meta intelligence here might bring us that. [[01:28:44](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5324.82s)]
*  Have a beautiful day, gentlemen. Go hug somebody. [[01:28:48](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5328.82s)]
*  Talk soon. Thanks very much. [[01:28:52](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5332.82s)]
*  Bye, guys. Thank you. [[01:28:54](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5334.82s)]
*  If you could have had a 10-year head start on the dot-com boom back in the 2000s, would you have taken it? [[01:28:55](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5335.82s)]
*  Every week, I track the major tech meta trends. [[01:29:00](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5340.82s)]
*  These are massive, game-changing shifts that will play out over the decade ahead, [[01:29:04](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5344.82s)]
*  from humanoid robotics to AGI, quantum computing, energy breakthroughs, and longevity. [[01:29:08](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5348.82s)]
*  I cut through the noise and deliver only what matters to our lives and our careers. [[01:29:13](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5353.82s)]
*  I send out a meta trend newsletter twice a week as a quick two-minute read over email. [[01:29:19](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5359.82s)]
*  It's entirely free. [[01:29:24](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5364.82s)]
*  These insights are read by founders, CEOs, and investors behind some of the world's most disruptive companies. [[01:29:25](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5365.82s)]
*  Why? Because acting early is everything. [[01:29:31](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5371.82s)]
*  This is for you if you want to see the future before it arrives and profit from it. [[01:29:35](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5375.82s)]
*  Sign up at dmagnus.com slash meta trends and be ahead of the next tech bubble. [[01:29:40](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5380.82s)]
*  That's dmagnus.com slash meta trends. [[01:29:45](https://www.youtube.com/watch?v=_H-5fvzsbdY&t=5385.82s)]
