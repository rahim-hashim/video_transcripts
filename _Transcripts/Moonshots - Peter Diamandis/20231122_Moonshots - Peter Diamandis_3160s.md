---
Date Generated: May 30, 2025
Transcription Model: whisper medium 20231117
Length: 3160s
Video Keywords: ['peter diamandis', 'longevity', 'xprize', 'abundance']
Video Views: 36718
Video Rating: None
Video Description: Amidst all the recent Tech news, Peter and Emad hopped on Twitter (X) Spaces to discuss AGI governance, the future of AI, open-source models, and more. 
Emad Mostaque is the CEO and Founder of Stability AI, a company funding the development of open-source music- and image-generating systems such as Dance Diffusion and Stable Diffusion.

Get started on Stability AI: https://stability.ai/ 
Sign up for the launch of the $101M XPRIZE – the largest in history: https://www.xprize.org/health#get-involved 

Topics:
0:00 - Intro
1:34 - AI Governance and Open Source
7:46 - Separating Governance and Safety
9:12 - Containing AI in an Open World
12:08 - AI Governance: Hard Rules
15:03 - OpenAI's Democratic Woes
18:05 - The Openness of AI Transformed
20:36 - Transparency and AI Survival
23:02 - Speed of Change: Frightening or Awesome?
26:33 - Digital Nation-Wide Infrastructure
32:45 - Thriving In an AI World
40:17 - The Pace of Progress Quickens
42:48 - Navigating a High-Tech World
48:32 - Announcing an X Prize of $101M
50:28 - Building Economic Stability Together

******************************************--
I send weekly emails with the latest insights and trends on today’s and tomorrow’s exponential technologies. Stay ahead of the curve, and sign up now: https://www.diamandis.com/subscribe
My new book with Salim Ismail, Exponential Organizations 2.0: The New Playbook for 10x Growth and Impact, is now available on Amazon: https://bit.ly/3P3j54J 
Get my new Longevity Practices book for free: https://www.diamandis.com/longevity

Connect with Peter:
Twitter: https://bit.ly/40JYQfK
Instagram: https://bit.ly/3x6UykS
Listen to the show:
Apple: https://apple.co/3wLXeV3
Spotify: https://spoti.fi/3DwLzgs
---

# Who Will Govern the Future of AGI? with Emad Mostaque (Stability AI Founder) | X (Twitter) Spaces
**Moonshots - Peter Diamandis:** [November 22, 2023](https://www.youtube.com/watch?v=ZOJoPG9wqvI)
*  Hey, Amad, good to hear you. [[00:00:00](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=0.0s)]
*  It was a pleasure. [[00:00:11](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=11.96s)]
*  Yeah. [[00:00:14](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=14.0s)]
*  So where are you today? [[00:00:15](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=15.0s)]
*  I'm in London. [[00:00:16](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=16.0s)]
*  Good. [[00:00:17](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=17.0s)]
*  Other side of the planet. [[00:00:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=18.0s)]
*  I'm in Santa Monica. [[00:00:19](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=19.400000000000002s)]
*  It's been quite the extraordinary game of ping pong out there these last four or five [[00:00:23](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=23.0s)]
*  days. [[00:00:29](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=29.68s)]
*  I didn't think the first thing that AI would disrupt would be the reality TV, right? [[00:00:30](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=30.68s)]
*  Yeah. [[00:00:37](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=37.44s)]
*  It's been fascinating how X has become sort of the go-to place to find out the latest [[00:00:38](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=38.44s)]
*  of where Sam is working and what's going on with the AI industry. [[00:00:46](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=46.92s)]
*  You found the notifications in the way it goes. [[00:00:51](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=51.4s)]
*  I think that's the thing. [[00:00:55](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=55.2s)]
*  What else will move at the speed of this? [[00:00:57](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=57.040000000000006s)]
*  I was saying to someone recently, AI research doesn't really move the speed of conferences [[00:00:59](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=59.36s)]
*  or even PDFs anymore, right? [[00:01:04](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=64.08s)]
*  You just wake up and you're like, oh, it's 10 times faster. [[00:01:05](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=65.44s)]
*  I think that's why X is quite good. [[00:01:08](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=68.0s)]
*  I actually unfollow just about everyone. [[00:01:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=70.92s)]
*  I just let the AI algorithms find the most interesting things for me. [[00:01:13](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=73.12s)]
*  So I've got 10 people that I follow and it's actually working really well. [[00:01:17](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=77.80000000000001s)]
*  It's getting better. [[00:01:21](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=81.32000000000001s)]
*  Well, it has been. [[00:01:22](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=82.44s)]
*  I've been enjoying the conversation. [[00:01:25](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=85.03999999999999s)]
*  It really feels like you're inside a intimate conversation among friends as this is going [[00:01:27](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=87.0s)]
*  back and forth. [[00:01:33](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=93.39999999999999s)]
*  I think this entire four or five days has been an extraordinary up close intimate conversation [[00:01:34](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=94.39999999999999s)]
*  around governance and around what's the future of AI because honestly, as it gets faster [[00:01:42](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=102.8s)]
*  and more powerful, the cost of missteps is going to increase exponentially. [[00:01:51](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=111.64s)]
*  Let's begin here. [[00:02:00](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=120.72s)]
*  You've been making the argument about open source as one of the most critical elements [[00:02:02](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=122.76s)]
*  of governance for a while now. [[00:02:08](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=128.08s)]
*  Let's hop into that. [[00:02:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=130.48000000000002s)]
*  Yeah, I think that open source is a difficult one because it means a few different things. [[00:02:11](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=131.48000000000002s)]
*  Is it models you can download and use? [[00:02:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=138.32s)]
*  Do you make all the data available and free? [[00:02:20](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=140.35999999999999s)]
*  And then when you actually look at what all these big companies do, all their stuff is [[00:02:23](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=143.79999999999998s)]
*  built on open source basis. [[00:02:28](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=148.32s)]
*  It's built on the transformer paper. [[00:02:31](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=151.79999999999998s)]
*  It's built on the new model by Kai-Fu Lee and real one.ai is basically llama. [[00:02:33](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=153.64s)]
*  It's actually got the same variable names and other things like that plus a gigantic [[00:02:40](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=160.44s)]
*  supercomputer. [[00:02:45](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=165.48s)]
*  The whole conversation has been how important is openness and transparency and what are [[00:02:46](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=166.56s)]
*  the governance models that are going to allow the most powerful technology on the planet [[00:02:52](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=172.51999999999998s)]
*  to enable the most benefit for humanity and the safety? [[00:02:58](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=178.51999999999998s)]
*  You've been thinking about this and speaking to transparency, openness, governance for [[00:03:03](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=183.67999999999998s)]
*  a while. [[00:03:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=190.2s)]
*  What do you think is going to be, what do you think we need to be focused on? [[00:03:13](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=193.0s)]
*  Where do we need to evolve to? [[00:03:17](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=197.96s)]
*  It's a complicated topic. [[00:03:19](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=199.96s)]
*  I think that most of the infrastructure of the internet is open source, Linux, everything [[00:03:23](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=203.12s)]
*  like that. [[00:03:28](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=208.84s)]
*  I think these models, it's unlikely that our governments will be run on GPT-7 or Bard [[00:03:29](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=209.96s)]
*  or anything like that. [[00:03:36](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=216.64s)]
*  How are you going to have black boxes that run these things? [[00:03:37](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=217.64s)]
*  I think a lot of the governance debate has been hijacked by the AI safety debate where [[00:03:41](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=221.64s)]
*  people are talking about AGI killing us all and then there's this precautionary principle [[00:03:46](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=226.04s)]
*  that kicks in. [[00:03:49](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=229.84s)]
*  It's too dangerous to let out because what if China gets it? [[00:03:51](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=231.04s)]
*  What if someone builds an AGI that kills us all? [[00:03:53](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=233.92s)]
*  It'd be great to have this amazing board that could pull the off switch. [[00:03:56](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=236.24s)]
*  Whereas in reality, I think that you're seeing a real social impact from this technology [[00:04:00](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=240.72s)]
*  and it's about who advances forward and who's left behind if we're thinking about risk. [[00:04:08](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=248.16000000000003s)]
*  Because governance is always about finding, as you said, the best outcomes and also mitigating [[00:04:13](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=253.68s)]
*  against the harms. [[00:04:19](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=259.2s)]
*  There's some very real amazingly positive outcomes that are now emerging that people [[00:04:21](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=261.2s)]
*  can agree on, but also some very real social impacts that we have to mitigate against. [[00:04:24](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=264.88s)]
*  So, let's begin. [[00:04:30](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=270.72s)]
*  How is stability governed? [[00:04:34](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=274.24s)]
*  Stability is basically governed by me. [[00:04:38](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=278.72s)]
*  So, I looked in foundations and DAOs and everything like that and I thought to take it to where [[00:04:43](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=283.68s)]
*  we are now, it needed to have very singular governance. [[00:04:47](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=287.6s)]
*  But now we're looking at other alternatives. [[00:04:50](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=290.48s)]
*  And where would you head in the future? [[00:04:52](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=292.24s)]
*  Let's actually jump away from this in particular. [[00:04:56](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=296.56s)]
*  What do you recommend the most powerful technologies on the planet? [[00:04:59](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=299.36s)]
*  How should they be governed? [[00:05:03](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=303.92s)]
*  How should they be owned? [[00:05:05](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=305.6s)]
*  Where should we be in five years? [[00:05:09](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=309.68s)]
*  I think they need to be public goods that are collectively owned and then individually [[00:05:12](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=312.72s)]
*  owned as well. [[00:05:17](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=317.36s)]
*  So, for example, there was the tweet kind of storm, the kind of I am Spartacus or his [[00:05:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=318.56s)]
*  name is Robert Boulson from the OpenAI team saying OpenAI is nothing without its people. [[00:05:24](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=324.8s)]
*  Stability, we have amazing people, 190 and 65 top researchers. [[00:05:32](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=332.24s)]
*  Without its people, we're open models used by hundreds of millions. [[00:05:37](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=337.2s)]
*  It continues. [[00:05:40](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=340.56s)]
*  And if you think about where you need to go, you can never have a choke point on this [[00:05:42](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=342.32s)]
*  technology, I think, if it becomes part of your life. [[00:05:45](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=345.36s)]
*  Like the phrase I have is not your models, not your minds. [[00:05:49](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=349.44s)]
*  So these models, again, are just such interesting things. [[00:05:53](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=353.28s)]
*  Take billions of images or trillions of words and you get this file out that can do magic, [[00:05:57](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=357.36s)]
*  right? [[00:06:02](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=362.08s)]
*  Trade or magic sand. [[00:06:02](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=362.71999999999997s)]
*  I think that you will have pilots that gather our global knowledge on various modalities [[00:06:05](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=365.04s)]
*  and you'll have co-pilots that you individually own that guide you through life. [[00:06:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=370.71999999999997s)]
*  And I can't see how that can be controlled by any one organization. [[00:06:14](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=374.96s)]
*  You've been on record talking about having models owned by the citizens of nations. [[00:06:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=378.0s)]
*  Can you speak to that a little bit? [[00:06:25](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=385.44s)]
*  Sure. [[00:06:28](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=388.08s)]
*  So we just released some of the top Japanese models from visual language to language to [[00:06:28](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=388.56s)]
*  Japanese SDXL as an example. [[00:06:34](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=394.71999999999997s)]
*  So we're training for half a dozen different nations and models now. [[00:06:37](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=397.59999999999997s)]
*  And the plan is to figure out a way to give ownership of these data sets and models back [[00:06:40](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=400.32s)]
*  to the people of that nation. [[00:06:46](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=406.8s)]
*  So you get the smartest people in Mexico to run a stability Mexico or maybe a different [[00:06:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=408.56s)]
*  structure that then makes decisions for Mexicans with the Mexicans about the data and what [[00:06:53](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=413.68s)]
*  goes in it. [[00:06:58](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=418.8s)]
*  Because everyone's been focusing on the outputs. [[00:07:00](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=420.16s)]
*  The inputs actually are the things that matter the most. [[00:07:02](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=422.96000000000004s)]
*  The best way I've thought about thinking of these models is very enthusiastic about it. [[00:07:06](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=426.48s)]
*  So hallucinations isn't just trying too hard. [[00:07:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=430.32s)]
*  A lot of the things about like, oh, what about these bad things the models can output? [[00:07:13](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=433.2s)]
*  It's about what you've input. [[00:07:16](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=436.88s)]
*  And so what you put into that Mexican data set or the Chinese or Vietnamese one will [[00:07:19](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=439.04s)]
*  impact the outputs. [[00:07:23](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=443.84000000000003s)]
*  And there's a great paper in Nature Human Behavior today about that, about how foundational [[00:07:25](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=445.36s)]
*  models are cultural technologies. [[00:07:31](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=451.36s)]
*  So again, how can you outsource your culture and your brains to other countries, to people [[00:07:33](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=453.68s)]
*  that are from a different place? [[00:07:41](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=461.44s)]
*  I think it eventually has to be localized. [[00:07:43](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=463.68s)]
*  Yeah, I think one of the points you said originally is we have to separate the issue [[00:07:45](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=465.68s)]
*  of governance versus safety and alignment. [[00:07:49](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=469.36s)]
*  Are they actually different? [[00:07:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=474.96000000000004s)]
*  So I think that a lot of the safety discussion or this AGI risk discussion is because the [[00:08:00](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=480.72s)]
*  future is so uncertain because it is so powerful. [[00:08:06](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=486.8s)]
*  Right? [[00:08:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=490.08s)]
*  And we didn't have a good view of where we're going. [[00:08:11](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=491.28s)]
*  So when you go on a journey and you don't know where you're going, you'll minimize [[00:08:15](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=495.03999999999996s)]
*  for max and regret, you'll have the precautionary principle. [[00:08:19](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=499.03999999999996s)]
*  And then that means you basically go towards authority, you go towards trying to control [[00:08:22](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=502.8s)]
*  this technology when it's so difficult to control. [[00:08:27](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=507.28s)]
*  And you end up not doing much, you know, because everything could go wrong. [[00:08:30](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=510.8s)]
*  When you have an idea of where we're going, like you should have all the cancer knowledge [[00:08:34](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=514.8s)]
*  in the world at your fingertips or climate knowledge, or anybody should be able to create [[00:08:38](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=518.08s)]
*  whole worlds and share them. [[00:08:42](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=522.08s)]
*  Then you align your safety discussions against the goal, against the location that you're [[00:08:44](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=524.24s)]
*  going to. [[00:08:49](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=529.6s)]
*  Again, just like setting out on a journey, I think that's a big change. [[00:08:50](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=530.8000000000001s)]
*  Similarly, most of the safety discussions have been on outputs, not inputs. [[00:08:53](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=533.9200000000001s)]
*  If you have a high quality data set without knowledge about anthrax, your language model [[00:08:58](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=538.48s)]
*  is unlikely to tell you how to build anthrax, you know? [[00:09:02](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=542.48s)]
*  So I think that's it. [[00:09:06](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=546.4000000000001s)]
*  And transparency around that will be very useful. [[00:09:07](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=547.44s)]
*  So let's dive into that safety alignment issue for a moment, because [[00:09:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=550.08s)]
*  it's an area you and I have been talking a lot about. [[00:09:16](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=556.8000000000001s)]
*  So Mustafa Suleimanis wrote a book called The Coming Wave, in which he talks about containment [[00:09:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=558.96s)]
*  as the mechanism by which we're going to be making sure we have safe AI. [[00:09:26](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=566.72s)]
*  You and I have had the conversation of it's really how you educate and raise and train [[00:09:33](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=573.04s)]
*  your AI systems in making sure that there's full transparency and openness on the data [[00:09:38](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=578.32s)]
*  sets that are utilized. [[00:09:44](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=584.96s)]
*  Do you think containment is an option for safety? [[00:09:45](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=585.92s)]
*  No, not at all. [[00:09:50](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=590.48s)]
*  Mike, a number of leaders say, what if China gets open source AI? [[00:09:52](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=592.16s)]
*  The reality is that China, Russia, everyone already has the weights for GPT-4, because [[00:09:58](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=598.64s)]
*  they just downloaded it on a USB stick. [[00:10:03](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=603.0400000000001s)]
*  You would know that they're being compromised, right? [[00:10:07](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=607.2s)]
*  There's no way they couldn't. [[00:10:09](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=609.12s)]
*  The rewards are too great. [[00:10:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=610.24s)]
*  And there is an absolutely false dichotomy here. [[00:10:12](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=612.1600000000001s)]
*  A lot of the companies want you to believe that giant models are the main thing, and [[00:10:17](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=617.6s)]
*  you need to have these gigantic, ridiculous supercomputers that only they can run. [[00:10:22](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=622.4000000000001s)]
*  We run gigantic supercomputers. [[00:10:26](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=626.56s)]
*  But the reality is this. [[00:10:28](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=628.88s)]
*  The supercomputers and the giant trillion, zillion data sets are just a shortcut for [[00:10:31](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=631.52s)]
*  bad quality data. [[00:10:37](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=637.76s)]
*  It's like using a hot pot or sous vide-ing a steak that's bad quality. [[00:10:40](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=640.3199999999999s)]
*  You cook it for longer and it organizes the information. [[00:10:44](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=644.9599999999999s)]
*  With stable diffusion, we did a study and we showed that basically 92% of the data isn't [[00:10:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=648.64s)]
*  used 99% of the time. [[00:10:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=654.0s)]
*  You know, because now you're seeing this with, for example, Microsoft's PHY release, [[00:10:57](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=657.12s)]
*  it's trained entirely on synthetic data. [[00:11:01](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=661.4399999999999s)]
*  Dali 3 is trained on RVAE and entirely synthetic data. [[00:11:04](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=664.24s)]
*  You are what you eat. [[00:11:09](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=669.12s)]
*  And again, we cooked it for longer to get past that. [[00:11:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=670.48s)]
*  But the implications of this are that I believe within 12 to 18 months, you'll see GPT-4 [[00:11:14](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=674.0799999999999s)]
*  level performance on a smartphone. [[00:11:22](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=682.0799999999999s)]
*  How do you contain that? [[00:11:24](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=684.56s)]
*  And how do you contain it when China can do distributed training at scale and release [[00:11:26](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=686.3199999999999s)]
*  open source models? [[00:11:31](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=691.12s)]
*  Google recently did 50,000 TPU training run on their V5Es. [[00:11:32](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=692.88s)]
*  The new V5Es, their TPUs, are very low powered relative to what we've seen. [[00:11:39](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=699.92s)]
*  But again, you can do distributed dynamic training. [[00:11:44](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=704.56s)]
*  Similarly, we funded Five Mind and we've seen Google DeepMind just did a new paper [[00:11:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=708.0799999999999s)]
*  on localization through distributed training. [[00:11:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=714.0s)]
*  The models are good at fast stuff and cheap enough that you can swarm them. [[00:11:56](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=716.48s)]
*  And you don't need to run supercomputers anymore. [[00:12:00](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=720.48s)]
*  And that has a lot of implications. [[00:12:03](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=723.36s)]
*  And how are you going to contain that? [[00:12:05](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=725.36s)]
*  So coming back to the question of do you mandate training sets? [[00:12:08](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=728.4s)]
*  Does the government set out what all companies should be utilizing and mandate if you're [[00:12:15](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=735.04s)]
*  going to have a aligned AI? [[00:12:21](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=741.28s)]
*  It has to be trained on these sets. [[00:12:24](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=744.8s)]
*  How do we possibly govern that? [[00:12:27](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=747.92s)]
*  Look, we have food standards, right, for ingredients. [[00:12:32](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=752.0s)]
*  Why don't you have data standards for the ingredients that make up a model? [[00:12:34](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=754.9599999999999s)]
*  It's just data compute and some algorithms, right? [[00:12:39](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=759.68s)]
*  And so you should say there are the standards and then you can make it [[00:12:43](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=763.12s)]
*  impulsory. [[00:12:47](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=767.3599999999999s)]
*  That will take a while. [[00:12:47](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=767.92s)]
*  You can just have an ISO type standard. [[00:12:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=768.7199999999999s)]
*  This is good quality model training, good quality data. [[00:12:51](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=771.52s)]
*  You know, and people will naturally gravitate towards that and it becomes the default. [[00:12:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=774.64s)]
*  Are you working towards that right now? [[00:12:59](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=779.52s)]
*  Yeah, I mean, look, we spun out a Luther AI as an independent 5.1c3 so they could look [[00:13:02](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=782.56s)]
*  at data standards and things like that independently of us. [[00:13:07](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=787.1999999999999s)]
*  And the opposite of open AI. [[00:13:09](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=789.76s)]
*  And this is something I've been talking to many people about. [[00:13:12](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=792.7199999999999s)]
*  And we're getting national data sets and more so that hopefully we can implement good standards. [[00:13:14](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=794.88s)]
*  Similar to how we offered Optout and had a billion images opted out of our image [[00:13:20](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=800.16s)]
*  data set because everyone was just training on everything. [[00:13:23](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=803.68s)]
*  Is required? [[00:13:26](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=806.3199999999999s)]
*  No. [[00:13:27](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=807.1999999999999s)]
*  But is it good? [[00:13:27](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=807.52s)]
*  Yes. [[00:13:28](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=808.3199999999999s)]
*  And everyone will benefit from better quality data. [[00:13:29](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=809.28s)]
*  So there's no reason that for these very large model training runs, the data sets [[00:13:32](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=812.2399999999999s)]
*  should not be transparent and logged. [[00:13:35](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=815.8399999999999s)]
*  So again, we want to know what goes into that. [[00:13:38](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=818.2399999999999s)]
*  Again, if we have the graduate analogy, what was the curriculum that the graduate was taught at? [[00:13:40](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=820.7199999999999s)]
*  Which university did they go to? [[00:13:45](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=825.76s)]
*  It's something that we'd want to know. [[00:13:47](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=827.92s)]
*  But then why do we talk to GPT-4 where we don't know where it went to university [[00:13:49](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=829.76s)]
*  or where it's been trained on? [[00:13:53](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=833.36s)]
*  It's a bit weird. [[00:13:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=834.88s)]
*  What do you think the lesson is going to be from the last four days? [[00:13:57](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=837.2s)]
*  I'm just confused. [[00:14:04](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=844.96s)]
*  I don't know who was against who or what. [[00:14:05](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=845.6800000000001s)]
*  I think I just posted, are we against misalignment or MOLOC? [[00:14:07](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=847.6s)]
*  I think probably the biggest lesson is it's very hard to align humans. [[00:14:12](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=852.0s)]
*  And the stakes are very large. [[00:14:16](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=856.4s)]
*  Why is this so interesting to us? [[00:14:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=858.32s)]
*  Because the stakes are so high. [[00:14:20](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=860.24s)]
*  You tweeted something that was serious and unfortunately funny, [[00:14:21](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=861.76s)]
*  which was how can we align AI with humanity's best interests if we can't align [[00:14:25](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=865.68s)]
*  our company's board with its employees' best interests? [[00:14:30](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=870.72s)]
*  Well, the thing is it's not the employees' best interests. [[00:14:36](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=876.24s)]
*  It's like the board was set up as a lever to ensure the charter of OpenAI. [[00:14:39](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=879.12s)]
*  So if you look at the original founding document of OpenAI from 2015, [[00:14:47](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=887.4399999999999s)]
*  it is a beautiful document talking about open collaboration, everything. [[00:14:51](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=891.76s)]
*  And then it kind of changed in 2019. [[00:14:55](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=895.76s)]
*  But the charter still emphasizes cooperation, safety, and fundamental. [[00:14:57](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=897.76s)]
*  I posted about this back in March when I said the board and the [[00:15:02](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=902.88s)]
*  government structure of OpenAI is weird. [[00:15:05](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=905.12s)]
*  Like, what is it for? [[00:15:08](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=908.48s)]
*  What are they trying to do? [[00:15:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=910.24s)]
*  Because if you say you're building AGI, in their own road to AGI, [[00:15:12](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=912.24s)]
*  they say this will end democracy. [[00:15:16](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=916.3199999999999s)]
*  I remember reading that. [[00:15:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=918.8s)]
*  Because democracy, there's no way democracy survives AGI. [[00:15:20](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=920.16s)]
*  Because either obviously it'll be better and you get it to do it, [[00:15:25](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=925.52s)]
*  or it can persuade everyone, or we all die, or it's utopia forever, right? [[00:15:28](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=928.24s)]
*  Abundance baby. [[00:15:33](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=933.12s)]
*  But then regardless, there's no way it survives AGI. [[00:15:37](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=937.1999999999999s)]
*  There's no way capitalism survives AGI. [[00:15:39](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=939.8399999999999s)]
*  The AGI will be the best trader in the world, right? [[00:15:42](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=942.0s)]
*  And it's like, who should be making the decisions on the AGI? [[00:15:45](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=945.4399999999999s)]
*  Assuming that they achieve those things. [[00:15:51](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=951.12s)]
*  And that's in their own words. [[00:15:52](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=952.88s)]
*  So I think that people are kind of waking up to, [[00:15:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=954.56s)]
*  oh, there's no real way to do this properly. [[00:15:57](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=957.92s)]
*  And previously we were scared of Open and being transparent, [[00:16:01](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=961.1999999999999s)]
*  everyone getting this, which was the original thing of OpenAI. [[00:16:06](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=966.3199999999999s)]
*  And now we're scared of who are these clowns? [[00:16:09](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=969.1999999999999s)]
*  And put it in the nicest way, because this was ridiculous. [[00:16:13](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=973.68s)]
*  Like, you see better politics in a teenage sorority, right? [[00:16:16](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=976.64s)]
*  And it's fundamentally scary that unelected people, no matter how great they are, [[00:16:20](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=980.16s)]
*  and I think some of the new board members are great, [[00:16:27](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=987.68s)]
*  should have a say in something that could literally upend our entire society [[00:16:30](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=990.48s)]
*  according to their own words. [[00:16:35](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=995.28s)]
*  I find that inherently anti-democratic and illiberal. [[00:16:38](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=998.16s)]
*  LARSON At the end of the day, [[00:16:41](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1001.04s)]
*  capitalism has worked and it's the best system that we have thus far. [[00:16:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1008.9599999999999s)]
*  And it's built on self-interest and built on continuous optimization and maximization. [[00:16:53](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1013.76s)]
*  I'm still wondering where you go in terms of governing these companies at one level [[00:17:03](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1023.4399999999999s)]
*  internal governance and then governing the companies at a national and global level. [[00:17:11](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1031.44s)]
*  Has anybody put forward a plan that you think is worth highlighting here? [[00:17:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1038.08s)]
*  Not really. [[00:17:25](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1045.28s)]
*  I mean, organizations are a weird artificial intelligence, right? [[00:17:26](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1046.1599999999999s)]
*  They have the status of people and they're slow dumb AI. [[00:17:30](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1050.56s)]
*  And they eat up hopes and dreams, that's what they feed on, I think. [[00:17:34](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1054.8799999999999s)]
*  But this AI can upgrade them, it can make them smarter, [[00:17:38](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1058.1599999999999s)]
*  they can have you coordinate. [[00:17:41](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1061.84s)]
*  And from a mechanism design perspective, it's super interesting. [[00:17:43](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1063.28s)]
*  Like in markets, I think we will have AI market makers that can tell stories. [[00:17:46](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1066.7199999999998s)]
*  Like the story of Silicon Valley Bank went around the world in two seconds, [[00:17:51](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1071.84s)]
*  the story of OpenAI goes around. [[00:17:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1074.7199999999998s)]
*  AI can tell better stories than humans, it's inevitable. [[00:17:56](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1076.32s)]
*  I think that gives hope for coordination, but then also it's dangers of disruption. [[00:17:59](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1079.6799999999998s)]
*  I want to double click one second on the two words that you use most, openness and transparency, [[00:18:04](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1084.48s)]
*  and understand fully what those mean one moment because, [[00:18:11](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1091.52s)]
*  and the question is not only what they mean, but how fundamental they need to be. [[00:18:15](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1095.2s)]
*  So openness right now in your definition in terms of AI means what? [[00:18:21](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1101.52s)]
*  It means different things, but different things unfortunately. [[00:18:27](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1107.6s)]
*  I don't think it means open source. [[00:18:31](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1111.52s)]
*  I think for me, open means more about access and ownership of the models, [[00:18:33](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1113.52s)]
*  so that you don't have a lockstep, like you can hire your own graduates as opposed to [[00:18:38](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1118.56s)]
*  really relying on consultants. [[00:18:42](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1122.96s)]
*  Transparency comes down to, I think for language models in particular, [[00:18:45](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1125.28s)]
*  I don't think this falls to media models, you really need to know what it's been taught. [[00:18:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1128.96s)]
*  That's the only way to safety. [[00:18:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1134.08s)]
*  Like you should not engage with something or use something if you don't know what its [[00:18:56](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1136.16s)]
*  credentials are and how it's been taught, because I think that's inherently dangerous [[00:18:59](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1139.52s)]
*  as these get more and more capabilities. [[00:19:03](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1143.44s)]
*  I don't know if we get to HGI, if we do I think it'll probably be like Scarlett Johansson and her, [[00:19:05](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1145.52s)]
*  just to give thanks to Judy. [[00:19:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1150.72s)]
*  I'm assuming we don't. [[00:19:11](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1151.84s)]
*  You still need transparency. [[00:19:14](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1154.0s)]
*  So again, how can any government or regulated industry not run on a transparent model? [[00:19:14](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1154.96s)]
*  They can't run on black boxes. [[00:19:22](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1162.72s)]
*  I get that and I understand the rationale for it, [[00:19:24](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1164.6399999999999s)]
*  but now the question is, can you prove transparency? [[00:19:28](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1168.8s)]
*  I think that again, a model is only three things really. [[00:19:33](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1173.84s)]
*  It's the data, the algorithm and the compute. [[00:19:37](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1177.36s)]
*  And then they come and the binary file pops out. [[00:19:41](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1181.28s)]
*  Then you can tune it with RLHF or DPO or genetic algorithms or whatever. [[00:19:43](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1183.44s)]
*  But that's really the recipe, right? [[00:19:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1188.3999999999999s)]
*  And so the algorithms, you don't need algorithmic transparency here versus classical AI, [[00:19:50](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1190.56s)]
*  because they're very simple. [[00:19:55](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1195.2s)]
*  One of our fellows recreated the Palm 540 billion parameter model. [[00:19:57](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1197.1200000000001s)]
*  This is Lucid Rains on GitHub. [[00:20:01](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1201.2s)]
*  You look at that if you're a developer and you want to cry, it's GitHub, it's crazy. [[00:20:03](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1203.1200000000001s)]
*  In 206 lines of PyTorch. [[00:20:06](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1206.88s)]
*  And that's it. [[00:20:09](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1209.3600000000001s)]
*  The algorithms are not very complicated. [[00:20:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1210.5600000000002s)]
*  Running a gigantic super computer is complicated. [[00:20:13](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1213.2s)]
*  And this is why they freaked out when Greg Brockman kind of stepped down, [[00:20:15](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1215.8400000000001s)]
*  because he's one of those talented engineers of our time. [[00:20:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1218.88s)]
*  Built this amazing gigantic clusters. [[00:20:21](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1221.92s)]
*  And then the data and how you structure data is complicated. [[00:20:24](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1224.88s)]
*  So I think you can have transparency there. [[00:20:28](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1228.4s)]
*  Because if the data is transparent, then who cares about the super computer? [[00:20:30](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1230.48s)]
*  Who really cares about the algorithm? [[00:20:33](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1233.1200000000001s)]
*  Now let's talk about the next term alignment here. [[00:20:36](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1236.48s)]
*  Alignment's thrown around in lots of different ways. [[00:20:39](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1239.68s)]
*  How do you define alignment? [[00:20:44](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1244.8000000000002s)]
*  I define alignment in terms of objective function. [[00:20:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1248.0800000000002s)]
*  So YouTube was used by the extremists to serve ads for their nastiness. [[00:20:51](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1251.44s)]
*  Right? Why? [[00:20:59](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1259.3600000000001s)]
*  Because the algorithm optimized for engagement, [[00:21:00](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1260.4s)]
*  which then optimized for extreme content, [[00:21:03](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1263.92s)]
*  which then optimized for the extremists. [[00:21:06](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1266.32s)]
*  Did YouTube mean that? [[00:21:08](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1268.72s)]
*  No. [[00:21:09](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1269.68s)]
*  But they're just trying to serve ads up, right? [[00:21:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1270.56s)]
*  But it meant it wasn't aligned with its users' interests. [[00:21:12](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1272.8s)]
*  And so for me, if you have these technologies [[00:21:16](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1276.32s)]
*  that we're going to outsource more of our mind, our culture, [[00:21:19](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1279.12s)]
*  our children's futures, to you that are very persuasive, [[00:21:21](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1281.52s)]
*  we have to ensure they're aligned with our individual community and societal best interests. [[00:21:25](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1285.28s)]
*  I think this is where the tension with corporations will come in. [[00:21:31](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1291.4399999999998s)]
*  Because whoever licenses Scarlett Johansson's voice will sell a lot of ads. [[00:21:36](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1296.8799999999999s)]
*  You know? [[00:21:41](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1301.04s)]
*  They can be very, very persuasive. [[00:21:41](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1301.6799999999998s)]
*  But then what are the controls on that? [[00:21:44](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1304.0s)]
*  No one talks about that. [[00:21:46](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1306.32s)]
*  The bigger question of alignment is not killerism, [[00:21:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1308.08s)]
*  making sure the AI doesn't kill us. [[00:21:52](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1312.6399999999999s)]
*  But again, I feel that if we build AI that is transparent, [[00:21:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1314.0s)]
*  that we can test that people can build mitigations around, [[00:21:58](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1318.56s)]
*  we are more likely to survive and thrive. [[00:22:03](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1323.6s)]
*  And also, I think there's a final element to here, which is who's alignment. [[00:22:06](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1326.32s)]
*  Yes. [[00:22:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1330.48s)]
*  Different cultures are different. [[00:22:11](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1331.04s)]
*  Different people are different. [[00:22:12](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1332.24s)]
*  What we found with stable diffusion is that when we merge together the models [[00:22:14](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1334.16s)]
*  that different people around the world have built, the model gets so much better. [[00:22:17](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1337.92s)]
*  I think that makes sense because a monoculture will always be less fragile than the diversity. [[00:22:22](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1342.56s)]
*  Again, I'm not talking about in the DEI kind of way. [[00:22:27](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1347.84s)]
*  I'm talking about it in the actual logical way. [[00:22:29](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1349.84s)]
*  So we have a paper from our reinforcement learning lab called CARPA called QDHF, [[00:22:33](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1353.44s)]
*  QDAIF, Quality and Diversity Through Artificial Intelligence Feedback. [[00:22:38](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1358.56s)]
*  Because you find these models do get better with high quality and diverse inputs, [[00:22:43](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1363.44s)]
*  just like you will get better if you have high quality and diverse experiences. [[00:22:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1368.24s)]
*  And I think that's something that's important that [[00:22:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1374.96s)]
*  we'll get lost if all these models are centralized. [[00:22:57](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1377.52s)]
*  You and I have had a lot of conversations about timelines here. [[00:23:02](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1382.16s)]
*  We can get into a conversation of when and if we see AGI, [[00:23:06](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1386.56s)]
*  but we're seeing more and more powerful capabilities coming online right now [[00:23:12](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1392.8000000000002s)]
*  that are going to cause a lot of amazing progress and disruption. [[00:23:16](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1396.8000000000002s)]
*  How much time do we have, Emad? [[00:23:23](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1403.28s)]
*  And we had a conversation when we were together at FII about [[00:23:26](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1406.0s)]
*  the disenfranchised youth coming off COVID. [[00:23:33](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1413.44s)]
*  So let's talk one second about timelines of how long do we have to get our shit together, [[00:23:38](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1418.64s)]
*  both as AI companies and investors and governors of society. [[00:23:45](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1425.1200000000001s)]
*  The speed here is awesome and frightening. [[00:23:58](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1438.72s)]
*  How long do we have? [[00:24:03](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1443.3600000000001s)]
*  Almost everything, everywhere, all at once, right? [[00:24:04](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1444.3999999999999s)]
*  We don't have long. [[00:24:06](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1446.24s)]
*  AGI timelines, for whatever definition of AGI, I have no idea. [[00:24:07](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1447.6s)]
*  It will never be less than 12 months, right? [[00:24:11](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1451.6799999999998s)]
*  Because it's such a step change. [[00:24:14](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1454.3999999999999s)]
*  So let's put that to the side. [[00:24:16](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1456.3999999999999s)]
*  Right now, everyone that's listening, [[00:24:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1458.96s)]
*  are you all going to hire the same amount of graduates that you hired before? [[00:24:21](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1461.84s)]
*  The answer is no. [[00:24:26](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1466.3999999999999s)]
*  Some people might not hire any because this is a productivity enhancement. [[00:24:27](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1467.76s)]
*  This is a productivity enhancement. [[00:24:31](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1471.84s)]
*  We have the data for that across any type of knowledge industry. [[00:24:33](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1473.6799999999998s)]
*  You just had a great app that you can sketch. [[00:24:38](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1478.0s)]
*  It does a whole iPhone app for you, right? [[00:24:40](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1480.6399999999999s)]
*  I got on record in saying there are no programmers we didn't know in five years. [[00:24:43](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1483.52s)]
*  Why? [[00:24:47](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1487.4399999999998s)]
*  Where would there be? [[00:24:47](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1487.6799999999998s)]
*  What are interfaces? [[00:24:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1488.8s)]
*  You had a 50% drop. [[00:24:50](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1490.3999999999999s)]
*  I just posted on my Twitter in hiring from Indian IITs. [[00:24:52](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1492.0s)]
*  That's crazy. [[00:24:56](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1496.9599999999998s)]
*  So what you're going to have in a couple of years is [[00:24:58](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1498.32s)]
*  around the world at the same time, [[00:25:00](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1500.72s)]
*  these kids that have gone through the trauma of COVID, [[00:25:03](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1503.28s)]
*  highly educated STEM, programming, accountancy law, [[00:25:06](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1506.64s)]
*  simultaneously people will hire massively less of them because productivity enhances. [[00:25:12](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1512.64s)]
*  You don't need as many of them. [[00:25:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1518.32s)]
*  Why would you need as many paralegals? [[00:25:20](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1520.16s)]
*  That for me is a gigantic societal issue. [[00:25:22](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1522.48s)]
*  The only thing I can think of is to stoke open innovation [[00:25:26](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1526.0s)]
*  and the generative jobs of the future through open source technology. [[00:25:28](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1528.96s)]
*  I don't know how else we're going to mitigate that because, [[00:25:33](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1533.3600000000001s)]
*  Peter, you're a student of history. [[00:25:36](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1536.08s)]
*  What happens when you have large amounts of intelligent, [[00:25:37](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1537.8400000000001s)]
*  disenfranchised youth? [[00:25:40](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1540.24s)]
*  We've had that happen a few times. [[00:25:41](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1541.76s)]
*  We just had Arab Spring not long ago. [[00:25:43](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1543.2s)]
*  Revolt. [[00:25:46](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1546.0s)]
*  Civil war, if not international law. [[00:25:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1548.48s)]
*  War is a good way to soak up the excess youth. [[00:25:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1554.08s)]
*  Yep. [[00:25:56](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1556.8s)]
*  But it's not pleasant. [[00:25:57](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1557.92s)]
*  It's not pleasant for society. [[00:25:59](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1559.2s)]
*  Fundamentally, the cost of information gathering, [[00:26:01](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1561.12s)]
*  organization has collapsed. [[00:26:04](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1564.88s)]
*  You look at Stable Video that we released yesterday. [[00:26:06](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1566.8s)]
*  It's going to get so much better so quickly, just like Stable Divu. [[00:26:09](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1569.92s)]
*  The cost of creating movies increases. [[00:26:12](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1572.88s)]
*  The demand for quality stuff increases. [[00:26:15](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1575.2s)]
*  But there's a few years where demand and supply don't match. [[00:26:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1578.08s)]
*  And that's such a turbulent thing to navigate. [[00:26:23](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1583.3600000000001s)]
*  That's one of the reasons I'm creating Stabilities for different countries. [[00:26:25](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1585.8400000000001s)]
*  So the best and brightest from each can help navigate them. [[00:26:28](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1588.5600000000002s)]
*  I loved your idea that the stability models and systems will be owned by the nation. [[00:26:31](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1591.2s)]
*  In fact, the one idea that I heard you say, which I thought was fantastic, [[00:26:41](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1601.92s)]
*  was you graduate college in India. [[00:26:45](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1605.3600000000001s)]
*  You're an owner in that system. [[00:26:47](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1607.68s)]
*  You graduate in Nigeria. [[00:26:51](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1611.2s)]
*  You're an owner in that system. [[00:26:52](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1612.96s)]
*  Basically, to incentivize people to complete their education [[00:26:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1614.6399999999999s)]
*  and to have them have ownership in what is ultimately the most important asset that nation has. [[00:26:57](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1617.76s)]
*  And talk about it as infrastructure as well. [[00:27:03](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1623.9199999999998s)]
*  I think that's an important analogy that people don't get. [[00:27:06](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1626.48s)]
*  This is the knowledge infrastructure of the future. [[00:27:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1630.24s)]
*  It's the biggest leap forward we have. [[00:27:12](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1632.48s)]
*  Because you'll always have a co-pilot that knows everything in a few years. [[00:27:14](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1634.1599999999999s)]
*  And it can create anything in any time. [[00:27:17](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1637.76s)]
*  But it must be embedded to your cultural values. [[00:27:19](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1639.92s)]
*  And you can't let anyone else own that. [[00:27:22](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1642.64s)]
*  So it is the infrastructure of the mind. [[00:27:25](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1645.04s)]
*  And who would outsource the infrastructure to someone else? [[00:27:27](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1647.3600000000001s)]
*  So that's why I think Nigerians should own the models of Nigerians for Nigerians. [[00:27:30](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1650.24s)]
*  It should be the next generation that does that. [[00:27:34](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1654.3200000000002s)]
*  That's why you give the equity to the graduates. [[00:27:37](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1657.28s)]
*  That's why you list it. [[00:27:39](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1659.52s)]
*  That's why you make national champions. [[00:27:40](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1660.4s)]
*  Because again, that has to be that way. [[00:27:42](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1662.88s)]
*  This is far more important than 5G. [[00:27:46](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1666.24s)]
*  And this gives you an idea of the scale. [[00:27:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1668.1599999999999s)]
*  We're just at the start, the early adopter phase. [[00:27:49](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1669.4399999999998s)]
*  A trillion dollars was spent on 5G. [[00:27:51](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1671.76s)]
*  This is clearly more important. [[00:27:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1674.56s)]
*  More than a trillion dollars will be spent on this. [[00:27:56](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1676.6399999999999s)]
*  And again, it flips the world. [[00:27:58](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1678.32s)]
*  And so there is huge threat for our societal balance. [[00:28:00](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1680.6399999999999s)]
*  And again, I think open is a potential antidote to create the jobs of the future. [[00:28:05](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1685.76s)]
*  And there's huge opportunity on the side because no one will ever be alone. [[00:28:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1690.1599999999999s)]
*  And we can use this to coordinate our systems. [[00:28:13](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1693.76s)]
*  Give everyone all the knowledge that they need at their fingertips. [[00:28:16](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1696.16s)]
*  And help guide everyone if we build this infrastructure correctly. [[00:28:19](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1699.2s)]
*  And I don't see the highlight can be closed. [[00:28:23](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1703.3600000000001s)]
*  AGI. [[00:28:26](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1706.64s)]
*  The conversation and the definition of AGI has basically been all over the place. [[00:28:29](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1709.92s)]
*  Ray Kurzweil's prediction has been for 30 years that it's 2029. [[00:28:36](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1716.4s)]
*  Again, that's a blurry line of what we're trying to target. [[00:28:42](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1722.16s)]
*  But Elon's talked about anywhere from 2025 to 2028. [[00:28:45](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1725.76s)]
*  What are you thinking? [[00:28:51](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1731.68s)]
*  What's your timeline for even digital super intelligence? [[00:28:55](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1735.36s)]
*  I honestly have no idea. [[00:29:02](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1742.4s)]
*  People are looking at scaling laws and applying it. [[00:29:04](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1744.56s)]
*  But as I've said, data is the key. [[00:29:06](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1746.56s)]
*  And it's clear that we already have... [[00:29:09](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1749.76s)]
*  You could build a board GPT and it'd be better than most corporate boards, right? [[00:29:12](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1752.24s)]
*  So I think we're already seeing improvements over the existing. [[00:29:16](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1756.24s)]
*  One of the complications here is swarm AI. [[00:29:20](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1760.56s)]
*  So even like it's the whole thing, like a duck-sized human or a 100 human-sized duck. [[00:29:23](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1763.6s)]
*  We're just at the start of swarm intelligence. [[00:29:29](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1769.2s)]
*  And that reflects and represents how companies are organized. [[00:29:31](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1771.44s)]
*  Andre Carpathy has some great analogies on this in terms of the new knowledge OS. [[00:29:35](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1775.28s)]
*  And that could take off at any time. [[00:29:39](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1779.84s)]
*  But the functional format of that may not be this whole [[00:29:42](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1782.5600000000002s)]
*  Western, un-compromised consciousness that we think of. [[00:29:46](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1786.64s)]
*  But just incredibly efficient systems that displace existing human decision making, right? [[00:29:49](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1789.8400000000001s)]
*  And so there's an entire actual range of different AGI outcomes, depending on your definition. [[00:29:55](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1795.52s)]
*  And I just don't know. [[00:30:01](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1801.44s)]
*  But I feel again, like I wake up and I'm like, oh, look, it's fed up 10 times the model. [[00:30:03](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1803.92s)]
*  I'm just like, no one can predict this. [[00:30:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1810.08s)]
*  But there is a point at which, I mean, we're heading towards an AI singularity, [[00:30:12](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1812.3999999999999s)]
*  using the definition of a singularity as a point after which you cannot predict what's coming next. [[00:30:16](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1816.96s)]
*  And that isn't far away. [[00:30:22](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1822.48s)]
*  I mean, how far out is it for you? [[00:30:24](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1824.72s)]
*  A year, two years? [[00:30:27](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1827.6s)]
*  I think you're heading towards it in the next few years. [[00:30:30](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1830.32s)]
*  But like I said, every company, organization, individual has an objective function. [[00:30:32](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1832.8s)]
*  My objective function is to allow the next generation to navigate what's coming in the [[00:30:38](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1838.4s)]
*  optimal way and achieve their potential. [[00:30:44](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1844.64s)]
*  So I don't want to build an AGI. [[00:30:47](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1847.2s)]
*  I don't want to do any of this. [[00:30:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1848.48s)]
*  Amplified human intelligence is my preference. [[00:30:49](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1849.76s)]
*  And trying to mitigate against some of the harms of these agentic things through data [[00:30:53](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1853.28s)]
*  transparency, good standards, and making it so people don't need to build gigantic models [[00:30:58](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1858.3200000000002s)]
*  on crap, which I think is a major danger, if even if not for major. [[00:31:04](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1864.24s)]
*  But again, we just don't understand because it's difficult for us to comprehend [[00:31:08](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1868.88s)]
*  superhuman capabilities. [[00:31:13](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1873.6s)]
*  But again, we're already seeing that in narrow fields. [[00:31:15](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1875.1200000000001s)]
*  We already know that it's a better writer than us. [[00:31:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1878.48s)]
*  So now we already know that it can make better pictures than us. [[00:31:21](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1881.1200000000001s)]
*  And a better physician and a better educator and a better surgeon and a better everything. [[00:31:24](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1884.32s)]
*  Yeah. [[00:31:29](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1889.36s)]
*  And again, I think it's this mythos of these big labs being AGI focused, [[00:31:30](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1890.8799999999999s)]
*  whereas you can be better than us in like 5% of the stuff that humans can do. [[00:31:37](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1897.04s)]
*  And that's still a massive impact on the world. [[00:31:42](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1902.0s)]
*  And it can still take over companies and things like that. [[00:31:43](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1903.52s)]
*  Right? [[00:31:46](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1906.0s)]
*  Like if you take over a company, then you can impact the world. [[00:31:46](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1906.96s)]
*  And there's clearly with a GPT-4 or a thousand of them orchestrated correctly, [[00:31:50](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1910.48s)]
*  that can call out people and stuff. [[00:31:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1914.8s)]
*  You wouldn't know it's not the CEO. [[00:31:56](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1916.24s)]
*  I can make an MRI GPT and then they won't have to make all these tough decisions. [[00:31:58](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1918.0800000000002s)]
*  And nearly there. [[00:32:01](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1921.3600000000001s)]
*  And most of my decisions aren't that good. [[00:32:02](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1922.72s)]
*  So it'd probably be better. [[00:32:04](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1924.64s)]
*  So I think that we're getting to that point. [[00:32:07](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1927.44s)]
*  It's very difficult. [[00:32:09](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1929.76s)]
*  And the design patterns are going fast. [[00:32:11](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1931.2s)]
*  We're at the iPhone 2G, 3G stage. [[00:32:13](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1933.28s)]
*  It's got copy and paste. [[00:32:15](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1935.52s)]
*  And we've just got the first stage as well of this technology, [[00:32:17](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1937.3600000000001s)]
*  which is the creation step. [[00:32:20](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1940.3200000000002s)]
*  It creates stuff. [[00:32:21](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1941.6000000000001s)]
*  The next step is control and then composition, [[00:32:22](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1942.96s)]
*  where they're annoyed because chat GPT doesn't remember all the stuff that you've written. [[00:32:26](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1946.0s)]
*  That won't be the case in a year. [[00:32:30](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1950.32s)]
*  And the final bit is collaboration, [[00:32:31](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1951.9199999999998s)]
*  where these AIs collaborate together and with humans [[00:32:33](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1953.52s)]
*  to build the information superstructures of the future. [[00:32:36](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1956.8799999999999s)]
*  And I don't feel that's more than a few years away. [[00:32:39](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1959.36s)]
*  And it's completely unpredictable what that will create. [[00:32:41](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1961.52s)]
*  Let's talk about responsibility that AI companies have [[00:32:44](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1964.8s)]
*  for making sure that their technology is used [[00:32:49](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1969.3600000000001s)]
*  in a pro-human and not a disruptive fashion. [[00:32:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1974.3200000000002s)]
*  Do you think that is a responsibility of a company, [[00:32:56](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1976.88s)]
*  of a company's board, of a company's leadership? [[00:33:00](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1980.4s)]
*  How do you think about that? [[00:33:02](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1982.88s)]
*  Again, with the corporatist capitalist system, [[00:33:06](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1986.4s)]
*  it typically isn't because you're maximizing shareholder value [[00:33:08](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1988.24s)]
*  and there aren't laws and regulations, [[00:33:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1990.96s)]
*  which is why I think there's a moral, a social, [[00:33:12](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1992.96s)]
*  and legal slash regulatory aspect to this. [[00:33:15](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1995.1200000000001s)]
*  Companies will just look at the legal slash regulatory [[00:33:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=1998.32s)]
*  and in some cases, they'll just ignore them, right? [[00:33:20](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2000.32s)]
*  But I do think, again, we have a bigger moral [[00:33:23](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2003.52s)]
*  and social obligation to this. [[00:33:25](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2005.76s)]
*  This is why I don't subscribe to EA or EAC or any of these things. [[00:33:27](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2007.9199999999998s)]
*  I think it's complicated and it's hard, [[00:33:31](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2011.52s)]
*  given the uncertainty of how this technology proliferate. [[00:33:34](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2014.3999999999999s)]
*  And you've got to do your best and be as straight as possible [[00:33:37](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2017.12s)]
*  to people about doing your best. [[00:33:39](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2019.76s)]
*  Because none of us have qualified to understand or do this. [[00:33:42](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2022.96s)]
*  And none of us should be trusted to have the power [[00:33:46](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2026.8s)]
*  over this technology, right? [[00:33:49](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2029.36s)]
*  You should be questioned. [[00:33:51](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2031.04s)]
*  You should be challenged with that. [[00:33:51](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2031.92s)]
*  And again, if you're not transparent, [[00:33:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2034.08s)]
*  how are you going to challenge? [[00:33:55](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2035.84s)]
*  When I think of the most linear organizations on the planet, [[00:33:58](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2038.4s)]
*  I think of governments, maybe religions, [[00:34:01](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2041.28s)]
*  but governments, let's leave it there. [[00:34:03](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2043.84s)]
*  How can, you know, let's talk about western government, [[00:34:06](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2046.88s)]
*  at least the US, I would have said Europe, [[00:34:12](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2052.0s)]
*  but I'll say the UK and Europe. [[00:34:15](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2055.2s)]
*  What steps should they be taking right now? [[00:34:20](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2060.96s)]
*  You know, if you were given the reins to say, [[00:34:23](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2063.9199999999996s)]
*  how would you regulate? [[00:34:26](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2066.8799999999997s)]
*  What would you want them to do or not do? [[00:34:28](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2068.64s)]
*  I believe it's a complicated one. [[00:34:34](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2074.16s)]
*  So I signed the first FLI letter. [[00:34:36](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2076.0s)]
*  I think I was the only AI CEO to do that back before it was cool. [[00:34:38](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2078.0s)]
*  Because I said, I don't think AGI will kill us all, [[00:34:41](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2081.8399999999997s)]
*  but I just don't know. [[00:34:44](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2084.08s)]
*  I think it's a conversation that deserves to be had. [[00:34:45](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2085.2799999999997s)]
*  And it's a good way to have that conversation. [[00:34:47](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2087.2799999999997s)]
*  And then we flipped the wrong way, [[00:34:49](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2089.12s)]
*  where we went overly AI death risk and other things like that. [[00:34:50](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2090.4s)]
*  And governments were doing that at the AI safety summit in the UK. [[00:34:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2094.16s)]
*  And then we had the King of England come out [[00:34:57](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2097.7599999999998s)]
*  and say, this is the biggest thing since fire. [[00:34:59](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2099.68s)]
*  I was like, okay, that's a big change. [[00:35:01](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2101.68s)]
*  The King of England said it, so I must be on the right track. [[00:35:05](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2105.84s)]
*  But I think if you look at it, regulation doesn't move fast enough. [[00:35:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2110.08s)]
*  Even the executive order will take a long time. [[00:35:12](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2112.56s)]
*  The EU things will come in. [[00:35:14](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2114.48s)]
*  Instead, I think that governments have to focus on the tangibles. [[00:35:16](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2116.24s)]
*  AI killerism, again, it can be addressed by considering this as infrastructure. [[00:35:20](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2120.56s)]
*  What infrastructure do we need to give our people to survive and thrive? [[00:35:24](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2124.72s)]
*  The US is in a good initial place with the CHIPS Act. [[00:35:27](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2127.7599999999998s)]
*  But I think you need national data sets. [[00:35:30](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2130.56s)]
*  You need to provide open models to stoke innovation. [[00:35:32](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2132.64s)]
*  And think about what the jobs of the future are, [[00:35:36](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2136.08s)]
*  because things are never the same again. [[00:35:38](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2138.0s)]
*  You don't need all those programmers. [[00:35:40](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2140.56s)]
*  When copilot is so good and you're moving copilot to a level above, [[00:35:42](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2142.08s)]
*  which is compositional copilot and then collaborative copilot, [[00:35:45](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2145.7599999999998s)]
*  you would be able to talk and computers can talk to computers [[00:35:50](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2150.0s)]
*  better than humans can talk to computers. [[00:35:52](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2152.4s)]
*  So we need to articulate the future on that side, but then the other side. [[00:35:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2154.56s)]
*  One of the examples I give is a loved one had a recent misdiagnosis of pancreatic cancer. [[00:35:58](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2158.7999999999997s)]
*  Vida and I talked about this. [[00:36:04](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2164.64s)]
*  And the loss of agency you feel, and many of you in this call [[00:36:06](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2166.56s)]
*  will have had that diagnosis, the new India is huge. [[00:36:09](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2169.68s)]
*  And then I had a thousand AI agents finding out every piece of information [[00:36:13](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2173.2799999999997s)]
*  about pancreatic cancer. [[00:36:16](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2176.24s)]
*  And then after that, I felt a bit more control. [[00:36:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2178.08s)]
*  Why don't we have a global cancer model that gives you all the knowledge about cancer [[00:36:20](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2180.64s)]
*  and helps you talk to your kids and connects with people like you, [[00:36:24](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2184.64s)]
*  not for diagnosis or research, but for humans. [[00:36:27](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2187.44s)]
*  This is the Google MedPalm 2 model, for example, [[00:36:31](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2191.68s)]
*  that outperforms humans and diagnosis, but also empathy. [[00:36:34](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2194.56s)]
*  And what if we armed our graduates? [[00:36:37](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2197.6s)]
*  To go out and give support to the humans that are being diagnosed in this way. [[00:36:39](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2199.8399999999997s)]
*  That makes society better and it's valuable. [[00:36:45](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2205.2s)]
*  You know, and that's an example of a job of the future. [[00:36:47](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2207.8399999999997s)]
*  I think I don't believe in UVI. [[00:36:50](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2210.8799999999997s)]
*  I think even universal basic jobs. [[00:36:52](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2212.96s)]
*  Yes, universal basic opportunity, right? [[00:36:55](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2215.2s)]
*  You know, space opportunity, you know, space jobs, but then [[00:36:59](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2219.68s)]
*  policymakers need to think about it now because the graduate unemployment wave [[00:37:02](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2222.96s)]
*  is literally a few years away. [[00:37:07](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2227.3599999999997s)]
*  That is, I mean, when I think about what I parse the challenges we're going to be facing in society [[00:37:09](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2229.2000000000003s)]
*  into a few different elements. [[00:37:16](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2236.7200000000003s)]
*  I think, you know, what we have today is amazing. [[00:37:19](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2239.6800000000003s)]
*  And if if generative AI froze here, we'd have an incredible [[00:37:22](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2242.1600000000003s)]
*  set of tools to help humanity across all of its areas. [[00:37:26](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2246.6400000000003s)]
*  And then we've got what's coming in the next, you know, zero to five years. [[00:37:30](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2250.2400000000002s)]
*  We've talked about patient zero, perhaps being the US elections [[00:37:34](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2254.0800000000004s)]
*  and the, you know, I think you had said it was Cambridge Analytica that required interference. [[00:37:37](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2257.92s)]
*  Now it's any kid in the garage that could could play with the elections. [[00:37:43](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2263.28s)]
*  That's a challenging period of time. [[00:37:47](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2267.84s)]
*  And this graduate unemployment wave, as you mentioned, coming right on its heels. [[00:37:50](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2270.48s)]
*  That's, you know, the question becomes, is the only thing that can create alignment and help us [[00:37:57](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2277.92s)]
*  overcome this AGI at the highest level, meaning it is causing challenges, but ultimately is a [[00:38:03](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2283.92s)]
*  tool that will allow us to enable to solve these challenges as well. [[00:38:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2290.8s)]
*  I mean, that's a crazy thought, right? [[00:38:15](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2295.6s)]
*  Like, all this stuff is crazy, but the sheer scale and impact of it. [[00:38:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2298.32s)]
*  And, you know, these discussions, we had them last year, Peter. [[00:38:22](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2302.56s)]
*  And now everyone's like, yeah, that makes sense. [[00:38:26](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2306.7200000000003s)]
*  I'm like, oh, wow. [[00:38:28](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2308.2400000000002s)]
*  Right. [[00:38:30](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2310.16s)]
*  It may be AGI, it may be these coordinating automated story makers and balances for the [[00:38:30](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2310.56s)]
*  market, right? [[00:38:35](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2315.04s)]
*  Next year, there's 56 elections with 4 billion people heading to the polls. [[00:38:36](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2316.16s)]
*  What could possibly go wrong? [[00:38:41](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2321.2799999999997s)]
*  Possibly go wrong, you know? [[00:38:44](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2324.4s)]
*  Oh my God. [[00:38:46](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2326.0s)]
*  But again, the technology isn't going to stop. [[00:38:46](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2326.88s)]
*  Like, even if stability puts down things, if open AI puts down things, [[00:38:49](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2329.12s)]
*  it will continue from around the world because you don't need much to train these models. [[00:38:53](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2333.6s)]
*  Again, the supercomputer thing is a myth. [[00:38:58](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2338.8s)]
*  You've got another year or two where you need them, you don't need them after that. [[00:39:01](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2341.1200000000003s)]
*  And that is insane to think about. [[00:39:04](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2344.0800000000004s)]
*  You just released Stability Video. [[00:39:06](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2346.6400000000003s)]
*  Congratulations on our stable video diffusion. [[00:39:08](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2348.1600000000003s)]
*  And I'm enjoying some of the clips. [[00:39:12](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2352.6400000000003s)]
*  How far are we away from me telling a story to my kids and saying, let's make that into a movie? [[00:39:15](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2355.36s)]
*  I'm 22 years away. [[00:39:26](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2366.96s)]
*  So this is a building block. [[00:39:29](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2369.76s)]
*  It's the first creation step. [[00:39:31](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2371.1200000000003s)]
*  And then like, so you have the control step, composition, and then collaboration [[00:39:32](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2372.4s)]
*  and self-learning systems around that. [[00:39:37](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2377.6800000000003s)]
*  So we have Comfy UI, which is our node-based system where you have all the logic that makes [[00:39:39](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2379.52s)]
*  up an image, like you can take a dress and a pose and a face that combines them all. [[00:39:44](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2384.4s)]
*  And it's all encoded in the image because you can move beyond files to intelligent [[00:39:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2388.88s)]
*  workflows that you can collaborate with. [[00:39:53](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2393.6800000000003s)]
*  If I send you that image file and you put it into your Comfy UI, [[00:39:55](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2395.52s)]
*  it gives you all the logic that made that up. [[00:39:59](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2399.28s)]
*  How insane is that? [[00:40:01](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2401.2000000000003s)]
*  So we're going to step up there. [[00:40:03](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2403.44s)]
*  And what's happened now is that people are looking at this AI like instant versus, again, [[00:40:05](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2405.36s)]
*  the huge amount of effort it took to take this information and structure it before. [[00:40:11](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2411.92s)]
*  But the value is actually in stuff that takes a bit longer. [[00:40:16](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2416.96s)]
*  Like when you're shooting a movie, you don't just say, do it all in one shot, right? [[00:40:20](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2420.08s)]
*  Unless you are a very talented director and actor, you know? [[00:40:24](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2424.64s)]
*  You have mise en place, you have staging, you have blocking, you have cinematography. [[00:40:29](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2429.12s)]
*  It takes a while to composite the scenes together. [[00:40:33](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2433.2s)]
*  It will be the same for this, but a large part of it will then be automated [[00:40:36](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2436.3199999999997s)]
*  for creating a story that can resonate with you. [[00:40:40](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2440.0s)]
*  And then you can turn it into Korean or whatever. [[00:40:42](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2442.48s)]
*  And there'll still be big blockbusters like Oppenheimer and Barbie. [[00:40:44](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2444.7999999999997s)]
*  But again, the flaw will be raised overall. [[00:40:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2448.4s)]
*  Similarly, we had a music video competition, check it out on YouTube with Peter Gabriel. [[00:40:52](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2452.08s)]
*  He allows us to use kindly his songs. [[00:40:56](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2456.96s)]
*  And people from all around the world made amazing music videos to his thing, [[00:40:59](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2459.2000000000003s)]
*  but they took weeks. [[00:41:02](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2462.56s)]
*  So I think that's somewhere in the middle here. [[00:41:04](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2464.2400000000002s)]
*  Where again, we're just at that early stage because [[00:41:05](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2465.92s)]
*  ChatGPT isn't even a year old. [[00:41:09](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2469.2000000000003s)]
*  You know, StableDiffusion is only 14, 15 months. [[00:41:11](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2471.6800000000003s)]
*  And I think you'd agree that neither of them is the end all and be all. [[00:41:13](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2473.76s)]
*  It's the earliest days of this field. [[00:41:19](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2479.84s)]
*  It's the tiniest building block. [[00:41:23](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2483.6s)]
*  Yeah, I had this conversation with Ray Kurzweil two weeks ago. [[00:41:24](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2484.88s)]
*  We were just after a Singularity Board meeting we had. [[00:41:29](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2489.36s)]
*  We were just hanging on a Zoom and chatted. [[00:41:31](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2491.76s)]
*  And the realization is that unfortunately, the human mind is awful at exponential projections. [[00:41:34](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2494.64s)]
*  And despite the convergence of all these technologies, [[00:41:42](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2502.16s)]
*  we tend to project the future as a linear extrapolation of the world we're living in right now. [[00:41:45](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2505.6s)]
*  But the best I can say is that we're going to see in the next decade, [[00:41:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2514.16s)]
*  between now and 2033, we're going to see a century worth of progress. [[00:41:58](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2518.64s)]
*  But it's going to get very weird very fast, isn't it? [[00:42:03](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2523.04s)]
*  I mean, there's two way doors and there's one way doors, right? [[00:42:07](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2527.8399999999997s)]
*  In December of last year, multiple headmasters called me and said, [[00:42:11](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2531.92s)]
*  we can't set essays for our homework anymore. [[00:42:16](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2536.16s)]
*  And every headmaster in the world had to say that same thing. [[00:42:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2538.4s)]
*  It's a one way door. [[00:42:20](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2540.64s)]
*  Yes. [[00:42:21](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2541.92s)]
*  And this is the scary part, the one way doors, right? [[00:42:22](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2542.64s)]
*  Like when you have an AI that can do your taxes. [[00:42:26](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2546.08s)]
*  What does that mean for accountants? [[00:42:30](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2550.4s)]
*  All the accountants at the same time? [[00:42:33](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2553.36s)]
*  It's kind of crazy, right? [[00:42:36](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2556.4s)]
*  It is. [[00:42:37](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2557.52s)]
*  And the challenge, I mean, one of my biggest concerns, so listen, I'm the eternal optimist. [[00:42:39](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2559.12s)]
*  I'm not the guy whose the glass is half full. [[00:42:44](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2564.08s)]
*  It's the glass is overflowing. [[00:42:46](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2566.0s)]
*  And one of the challenges I think through when I think about where AI, AGI, ASI, [[00:42:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2568.3199999999997s)]
*  however you want to project it to you is the innate importance of human purpose. [[00:42:56](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2576.96s)]
*  And unfortunately, most of us derive our purpose from the work that we do. [[00:43:04](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2584.64s)]
*  I ask you, tell me about yourself and you jump into your work and what you do. [[00:43:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2590.48s)]
*  And so when AI systems are able to do most everything we do, not just a little bit better, [[00:43:16](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2596.3199999999997s)]
*  but orders of magnitude better, redefining purpose and redefining my role in achieving [[00:43:21](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2601.8399999999997s)]
*  a moonshot or transformation is the impedance mismatch between human societal [[00:43:31](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2611.2s)]
*  growth rates and tech growth rates. [[00:43:44](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2624.7999999999997s)]
*  What are your thoughts there? [[00:43:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2628.0s)]
*  Yeah, I mean, I think again, exponentials are hard. [[00:43:50](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2630.56s)]
*  Like if I say GPT-4 in 12 to 18 months on a smartphone, you'd be like, well, that's not [[00:43:52](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2632.72s)]
*  possible. Why? You know, like GPT-4 is impossible. Stable diffusion is impossible, right? [[00:43:59](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2639.12s)]
*  Like now they've almost become commonplace, but why would you need supercomputers and these things? [[00:44:05](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2645.52s)]
*  I do agree there's this mismatch and that's why we're in for five years of [[00:44:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2650.56s)]
*  chaos. And that's why I call this stability. [[00:44:15](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2655.3599999999997s)]
*  I saw this coming a few years ago and I was like, holy crap, we have to build this company. [[00:44:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2658.08s)]
*  And now we have the most downloads of any models of any company, [[00:44:24](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2664.08s)]
*  like 50 million last month versus 700,000 from Estral, for example. [[00:44:27](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2667.76s)]
*  And we will have the best model of every type except for very large language models by the end [[00:44:33](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2673.1200000000003s)]
*  of the year. So we have audio, 3D, video, code, everything, and a lovely, amazing community. [[00:44:37](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2677.44s)]
*  Because it's just so hard again for us to imagine this mismatch. There's a period of chaos. [[00:44:44](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2684.6400000000003s)]
*  But then on the other side, like there's this P-Doom question, right? The probability of a doom. [[00:44:50](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2690.0s)]
*  I can say something with this technology, the probability of doom is lower than without this [[00:44:55](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2695.2799999999997s)]
*  technology, because we're killing ourselves. And this can be used to enhance every human and [[00:44:59](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2699.2s)]
*  coordinate us all. And I think what we're aiming for is that Star Trek future versus that Star Wars. [[00:45:05](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2705.4399999999996s)]
*  Yes, I'm into that. And I think that's an important point that the level of complexity that we have [[00:45:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2710.16s)]
*  in society, we don't need AI to destroy the planet. We're doing that very well, thank you. [[00:45:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2718.56s)]
*  But the ability to coordinate. So one of the things I think about is a world in which [[00:45:28](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2728.72s)]
*  everyone has access to all the food, water, energy, healthcare, education that they want. [[00:45:34](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2734.32s)]
*  Really a world of true abundance, in my mind, is a more peaceful world. Why would you want [[00:45:39](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2739.84s)]
*  to destroy things if you have access to everything that you need? And that kind of a world of abundance [[00:45:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2748.48s)]
*  is on the backside of this kind of awesome technology. [[00:45:54](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2754.56s)]
*  You have to navigate the next period. I believe we'll see it within our lifetimes, [[00:46:00](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2760.4s)]
*  particularly if we get longevity songs, right? And that's so amazing, right? But then we think [[00:46:04](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2764.16s)]
*  about, as you said, why peace? A child in Israel is the same as a child in Gaza. And then something [[00:46:10](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2770.08s)]
*  happens. A lie is told that you are not like others, and the other person is not human like you. [[00:46:16](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2776.64s)]
*  All wars are based on that same lie. And so again, if we have AI that is aligned with the [[00:46:22](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2782.64s)]
*  potential of each human that can help mitigate those lies, then we can get away from war because [[00:46:28](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2788.56s)]
*  the world is not scarce. There is enough food for everyone. It's a coordination failure. [[00:46:35](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2795.8399999999997s)]
*  I agree. One of the most interesting and basic functions and capabilities of [[00:46:41](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2801.04s)]
*  generative AI has been the ability to translate my ideas into concepts that someone who has a [[00:46:48](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2808.7200000000003s)]
*  different frame of thought can understand. But that's what this generative AI is. It's a [[00:46:55](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2815.12s)]
*  universal translator. It does not have facts. The fact that it knows anything is insane. [[00:47:02](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2822.32s)]
*  Hallucinations is a crazy thing to say. It's just like a graduate trying so hard. GPT-4 with 10 [[00:47:08](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2828.4s)]
*  trillion words and 100 gigabytes is insane. Stable diffusion has like 100,000 gigabytes [[00:47:14](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2834.7200000000003s)]
*  and a 2 gigabyte file. 50,000 to 1. Compression is something else. It's learned principles. [[00:47:22](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2842.1600000000003s)]
*  It's knowledge versus data. [[00:47:29](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2849.44s)]
*  It's knowledge versus data. And you apply some experience, you get the wisdom, right? [[00:47:33](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2853.2000000000003s)]
*  Because it's learned the principles and contexts and it can map them to transform data. [[00:47:37](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2857.52s)]
*  Because that's how you navigate. You don't navigate based on [[00:47:43](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2863.84s)]
*  logical flow. We have those two parts of our brain. Navigate sometimes based on instinct, [[00:47:49](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2869.7599999999998s)]
*  based on the principles you've learned. So test this new self-driving model. It's entirely based [[00:47:53](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2873.36s)]
*  on a concept which I'll potentially make this set up publicly. It's based on this technology. [[00:47:59](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2879.6s)]
*  It doesn't have any rules. It's just learned the principles of how to drive a massive amount of [[00:48:03](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2883.84s)]
*  Tesla data that now fits on the hardware without internet. And so they went from self-driving being [[00:48:08](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2888.6400000000003s)]
*  impossible to now, hey, it clicks pretty well. Because it's learned the principles. And so that's [[00:48:15](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2895.2000000000003s)]
*  why this technology can help solve the problems. This is why it can help us amplify our intelligence [[00:48:21](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2901.28s)]
*  and innovation. Because it's the missing part, the second part of the brain. [[00:48:27](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2907.1200000000003s)]
*  Next, I can't give more details yet, but next week we're announcing the largest X prize ever. [[00:48:31](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2911.6s)]
*  It's $101 million. Elon had the $100 million prize that got him to fund a few years ago for [[00:48:36](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2916.8799999999997s)]
*  carbon sequestration. And the first funder of this prize wanted to be larger than Elon's. I said, [[00:48:47](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2927.52s)]
*  okay, you add the extra million. That's for luck. We did our seed rounds with $101 million. [[00:48:53](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2933.4399999999996s)]
*  Oh, really? Okay. That's great. That's a new popular number. Anyway, [[00:48:59](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2939.12s)]
*  and it's in the field of health. I'll leave it at that. Folks can go to xprize.org to register to [[00:49:07](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2947.04s)]
*  see the live event on November 29th. We're going to be debuting the prize, what it is. It's going [[00:49:12](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2952.64s)]
*  to impact 8 billion people. Long story short, it's a non-linear future because we are able to [[00:49:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2958.3199999999997s)]
*  utilize AI and make things that were seemingly crazy before likely to become inevitable. [[00:49:27](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2967.36s)]
*  It's an amazing future we have to live into. Yeah. I mean, again, because it's one way doors. [[00:49:37](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2977.2000000000003s)]
*  The moment we create a cancer GPT, this is something that we're building. We have trillions [[00:49:44](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2984.2400000000002s)]
*  of tokens and then you Google TPUs and things like that, that organizes global cancer knowledge [[00:49:49](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2989.1200000000003s)]
*  and makes it accessible and useful. Even if it's just for guiding people that have been diagnosed, [[00:49:55](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=2995.04s)]
*  the world changes. The 50% of people that have a cancer diagnosis in their lives, [[00:50:01](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3001.2s)]
*  in every language, in every level, will have someone to talk to and connect them with the [[00:50:05](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3005.6s)]
*  resources they need and other people like them and talk to their families. How insane is that? [[00:50:09](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3009.84s)]
*  These positive stories of the future need to be told because that will align us [[00:50:17](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3017.44s)]
*  to where we need to go as opposed to a future full of uncertainty and craziness and doom. [[00:50:22](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3022.16s)]
*  In our last couple minutes here, buddy, what can we look forward to from stability [[00:50:28](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3028.24s)]
*  in the months and years ahead? We have every model of every type and we'll build it for [[00:50:34](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3034.64s)]
*  every nation and we'll give back control to every nation. Coming back to governance here, [[00:50:40](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3040.48s)]
*  again, is the nation state the unit of control? [[00:50:45](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3045.92s)]
*  No, my thinking is the stability of every nation should have the best and brightest of each [[00:50:52](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3052.4s)]
*  because what you've seen is there are amazing people in this sphere. [[00:50:59](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3059.2s)]
*  The best and brightest in the world know this is the biggest thing ever and they all want to [[00:51:02](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3062.96s)]
*  work in it and it's just finding the right people with the right intention. The brightest people go [[00:51:07](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3067.36s)]
*  back to Singapore or Malaysia or others because of the future of their nations. Again, now we're [[00:51:12](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3072.16s)]
*  doing a big change and we don't talk about all the cool stuff we do. We've just taken it because we [[00:51:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3078.08s)]
*  need to articulate that positive vision of the future because the only space resource that actually [[00:51:22](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3082.64s)]
*  this is human capital. It's not GPUs. It's not data. It's about the humans that can see this [[00:51:26](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3086.64s)]
*  technology and realize that they can play a part in guiding it for the good of everyone, [[00:51:32](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3092.3999999999996s)]
*  their own societies and more. That's again what I thought stability can mean. [[00:51:39](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3099.04s)]
*  Well, I wish you the best of luck, pal. Thank you for joining me in this conversation. It's [[00:51:43](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3103.52s)]
*  been a crazy four or five days and wish Sam and Greg and the entire OpenAI team [[00:51:49](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3109.28s)]
*  stability in their lives. [[00:51:58](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3118.88s)]
*  I hope they have a nice Thanksgiving. They're an amazing team building world-changing technology. [[00:52:02](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3122.2400000000002s)]
*  It's such a concentration of talent. I think, again, I really felt for them over the last few [[00:52:07](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3127.92s)]
*  days, much as I post memes and everything. I posted that as well. I think this will bring [[00:52:13](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3133.28s)]
*  them closer together and hopefully they can solve the number one problem that I've asked them to [[00:52:18](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3138.56s)]
*  solve, which is email. Solve email and they will crack on from there. All right. Cheers, my friend. [[00:52:22](https://www.youtube.com/watch?v=ZOJoPG9wqvI&t=3142.8s)]
