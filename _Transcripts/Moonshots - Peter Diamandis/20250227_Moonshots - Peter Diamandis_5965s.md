---
Date Generated: May 30, 2025
Transcription Model: whisper medium 20231117
Length: 5965s
Video Keywords: ['peter diamandis', 'longevity', 'xprize', 'abundance']
Video Views: 354952
Video Rating: None
Video Description: In this episode, Mo, Peter, and Salim discuss AGI, how to adapt to an AI-driven world, the future of jobs, and more. 
Recorded on Feb 18th, 2025
Views are my own thoughts; not Financial, Medical, or Legal Advice.
Mo Gawdat is a renowned author, entrepreneur, and former Chief Business Officer at Google [X]. He is best known for his work on happiness and technology, which includes his bestselling books. His notable works include Solve for Happy: Engineer Your Path to Joy (2017), Scary Smart: The Future of Artificial Intelligence and How You Can Save Our World (2021), That Little Voice in Your Head: Adjust the Code That Runs Your Brain (2022), and Unstressable: A Practical Guide to Stress-Free Living (latest release). Mo Gawdat is also set to release a new book titled Alive. His career spans roles at IBM, Microsoft, and Google, where he led projects like Project Loon and Project Makani. Gawdat is also the founder of the One Billion Happy initiative, and the co-founder of Unstressable, an online platform for stress management.
Salim Ismail is a serial entrepreneur and technology strategist well known for his expertise in Exponential organizations. He is the Founding Executive Director of Singularity University and the founder and chairman of ExO Works and OpenExO. 
Subscribe to read Mo’s upcoming book: Alive: https://mogawdat.substack.com/ 
Join Salim's ExO Community: https://openexo.com
Twitter: https://twitter.com/salimismail 

—******--
This episode is supported by exceptional companies:
Get started with Fountain Life and become the CEO of your health: https://fountainlife.com/peter/
AI-powered precision diagnosis you NEED for a healthy gut: https://www.viome.com/peter 
Get 15% off OneSkin with the code PETER at  https://www.oneskin.co/ #oneskinpod
******************************************--
Chapters
00:00 - The Race to AGI: Predictions and Realities
02:54 - Utopia vs. Dystopia: The Dual Nature of AI
06:04 - The Purpose of Life in an AI-Driven World
09:12 - Navigating the Bright Side of AI: Opportunities Ahead
11:56 - The Dark Side: Short-Term Dystopia and Human Values
14:55 - The Acceleration of AI: A Double-Edged Sword
18:08 - The FACE RIPs: Redefining Humanity's Future
20:59 - The Inevitable Hand Over to AI: Dilemmas Ahead
29:19 - The Dilemma of AI Dependence
39:07 - The Quest for Wisdom in AI
50:15 - Navigating the Future of AI and Humanity
54:30 - Predictions for AI in the Near Future
01:05:50 - The Future of AI and Consciousness
01:06:18 - AI Innovations and Breakthroughs
01:07:15 - Predictions for Scientific Advancements
01:08:31 - The Impact of Open Source AI
01:09:50 - Creating AI Avatars and Digital Identities
01:11:50 - Philosophical Implications of AI Avatars
01:15:25 - AI in Conflict Resolution
01:18:14 - Anticipating Dystopian Challenges
01:20:57 - Power Dynamics in the Age of AI
01:23:05 - Near-Term Stressors and Job Displacement
01:26:38 - Navigating the Future of Work
01:33:42 - Coping with Stress in a Changing World
******************************************--
I send weekly emails with the latest insights and trends on today’s and tomorrow’s exponential technologies. Stay ahead of the curve, and sign up now: https://www.diamandis.com/subscribe
Connect with Peter:
Twitter: https://bit.ly/40JYQfK
Instagram: https://bit.ly/3x6UykS
Listen to the show:
Apple: https://apple.co/3wLXeV3
Spotify: https://spoti.fi/3DwLzgs
---

# AGI Is Here You Just Don’t Realize It Yet w/ Mo Gawdat & Salim Ismail | EP #153
**Moonshots - Peter Diamandis:** [February 27, 2025](https://www.youtube.com/watch?v=cJuJtQrC08Y)
*  What's your predictions, Moe, for the year ahead? [[00:00:00](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=0.0s)]
*  The word on the street is we will achieve AGI in 2025. [[00:00:03](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3.5s)]
*  In my world, they've already achieved AGI. [[00:00:07](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=7.5s)]
*  Does anybody actually know how fast it's moving? [[00:00:10](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=10.5s)]
*  The warhead has already been launched. [[00:00:15](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=15.5s)]
*  It's just a question of time before it hits its target. [[00:00:17](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=17.0s)]
*  We're entering an uncharted territory. [[00:00:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=21.0s)]
*  We're upon the perfect storm of the most challenging time humanity has faced in my lifetime. [[00:00:23](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=23.5s)]
*  What do you imagine is the best outcome we're going to be seeing from AI? [[00:00:30](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=30.0s)]
*  A total utopia of abundance where we absolutely need nothing [[00:00:34](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=34.5s)]
*  and where we do not report to stupid leaders anymore. [[00:00:37](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=37.5s)]
*  That seems like a very unchallenged life. [[00:00:41](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=41.0s)]
*  We decided that the purpose of life for some of us is to make more money [[00:00:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=43.5s)]
*  and be billionaires, indigenous tribes. [[00:00:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=47.5s)]
*  The purpose of life for them is to live. [[00:00:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=49.5s)]
*  At a very different level of abundance, we're going back to that purpose. [[00:00:51](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=51.5s)]
*  What I ask people to do is to actually look deeply at what can I do. [[00:00:55](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=55.5s)]
*  Now that's the moonshot, ladies and gentlemen. [[00:01:00](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=60.5s)]
*  It's a pleasure to be here with two friends, Mo and Salim. [[00:01:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=66.5s)]
*  Mo, you're in Dubai today. [[00:01:10](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=70.5s)]
*  In my studio in Dubai, yeah, I love it here. [[00:01:13](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=73.5s)]
*  Fantastic. And Salim, you're in the greatest city of the world, New Jersey? [[00:01:16](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=76.5s)]
*  New York, yes. [[00:01:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=81.5s)]
*  New York, okay. [[00:01:23](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=83.5s)]
*  As per the citizens of the city, that assessment is up to New Yorkers, really. [[00:01:26](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=86.0s)]
*  Yes, I will leave that one alone. [[00:01:32](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=92.0s)]
*  But I do want to dive into what's going on in the world of AI. [[00:01:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=95.0s)]
*  Does anybody actually know how fast it's moving [[00:01:39](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=99.0s)]
*  and how dramatic the changes are going to be in our lives? [[00:01:42](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=102.0s)]
*  We're all still waking up, taking the kids to school, [[00:01:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=107.0s)]
*  watching the evening news, having breakfast, lunch and dinner. [[00:01:51](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=111.0s)]
*  And I'm on stages around the world as both of you are. [[00:01:55](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=115.5s)]
*  And we get into a conversation and people actually understand the speed. [[00:02:00](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=120.5s)]
*  Their brain breaks and they go, what does that mean for me and for my kids, [[00:02:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=126.5s)]
*  my job, my country? [[00:02:12](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=132.5s)]
*  It's quite shocking, isn't it? [[00:02:13](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=133.5s)]
*  So I speak around the world like both of you. [[00:02:15](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=135.5s)]
*  And at the end of every conversation, [[00:02:19](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=139.0s)]
*  I almost liken it to a war where the warhead has already been launched. [[00:02:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=142.0s)]
*  It's just a question of time before it hits its target. [[00:02:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=149.0s)]
*  My assessment though is that we don't know if it's carrying roses [[00:02:33](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=153.0s)]
*  or carrying a nuclear warhead or maybe a bit of both, one after the other. [[00:02:37](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=157.0s)]
*  But it's already in the air. [[00:02:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=163.0s)]
*  We are so advanced as compared to 2023 when Chad GPT first came out. [[00:02:45](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=165.0s)]
*  It's not even comparable. [[00:02:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=172.0s)]
*  So I want to get into that in this episode. [[00:02:54](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=174.0s)]
*  I want to talk about you're both going to be at the Abundance Summit. [[00:02:57](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=177.0s)]
*  And Mo, your title is going to be near-term dystopia on the road to abundance. [[00:03:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=181.0s)]
*  I want to talk about what near-term means and what the dystopia looks like [[00:03:07](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=187.0s)]
*  and what abundance looks like on the flip side. [[00:03:12](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=192.0s)]
*  Salim, you're wearing white today, so you're going to play the good match. [[00:03:14](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=194.5s)]
*  You're with that team. [[00:03:18](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=198.5s)]
*  And it really is. It's really a debate between is this the greatest benefit uplifting all of humanity [[00:03:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=200.5s)]
*  or is this something that's going to... [[00:03:28](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=208.5s)]
*  Well, it is going to reinvent every aspect of our lives, period. [[00:03:32](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=212.5s)]
*  But I had a question for you, to both of you. [[00:03:37](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=217.0s)]
*  The time frame for reinventing every aspect of our lives, our businesses, our governments, [[00:03:41](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=221.0s)]
*  is it two, five or ten years? [[00:03:48](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=228.0s)]
*  Mo? [[00:03:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=232.0s)]
*  Inventing as the change is already in place and everything in our life is determined by it, I'd say five. [[00:03:53](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=233.0s)]
*  Five. [[00:04:00](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=240.0s)]
*  Salim? [[00:04:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=241.0s)]
*  I go with around ten years, and the reason I say that is I'm kind of a believer in the William Gibson quote, [[00:04:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=242.5s)]
*  the future is already here, it's just unevenly distributed. [[00:04:09](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=249.5s)]
*  And we find it takes a very long time, much longer than even when we want it to happen, [[00:04:12](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=252.5s)]
*  to get, say, autonomous cars out the door or CRISPR out the door into broad stream, mainstream use. [[00:04:18](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=258.5s)]
*  And so I think it goes slower than that, but in pockets it'll move unbelievably quickly in that [[00:04:25](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=265.5s)]
*  gap between those two is what's causing a lot of the stress. [[00:04:31](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=271.5s)]
*  If it all happened in an even way, we could kind of deal with it, but it's happening in different places [[00:04:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=275.0s)]
*  and different speeds, and we are just totally discombobulated because of that. [[00:04:39](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=279.0s)]
*  Yeah. [[00:04:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=283.0s)]
*  And I think the challenge is we've had huge change in humanity from, you know, a hundred thousand [[00:04:44](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=284.0s)]
*  years ago to agrarian society to even the industrial age, but it's happened over lifespans [[00:04:50](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=290.0s)]
*  and hasn't happened over a single five-year period. [[00:04:57](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=297.0s)]
*  Let's, you want to go bright side or dark side first? [[00:05:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=302.5s)]
*  So, let's go bright side. [[00:05:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=306.5s)]
*  So, Mo, what do you imagine is the best outcome we're going to be seeing from AI? [[00:05:09](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=309.5s)]
*  A total utopia of abundance where we absolutely need nothing and where we do not report to stupid leaders anymore. [[00:05:14](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=314.5s)]
*  I'll leave that one alone for the moment, but, and stupid leaders could be anything on any level. [[00:05:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=322.0s)]
*  But most of our, most of our global leaders are in that category. [[00:05:30](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=330.0s)]
*  Let's face it. [[00:05:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=335.0s)]
*  The challenge I have is if we have this extraordinary utopia where all of our needs are being met, [[00:05:36](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=336.0s)]
*  you know, food, water, energy, health care, education, everything, we just have to desire it and think it [[00:05:41](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=341.0s)]
*  and it's given to us. [[00:05:48](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=348.5s)]
*  That seems like a very unchallenged life. [[00:05:50](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=350.5s)]
*  So, how do you deal with a life where we don't have the challenge and the purpose because [[00:05:53](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=353.5s)]
*  it's taken away from us by the AIs and the humanoid robots? [[00:06:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=362.5s)]
*  How do you deal with that, Mo? [[00:06:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=366.5s)]
*  That's one of the biggest concerns I have. [[00:06:08](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=368.5s)]
*  Yeah. [[00:06:10](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=370.5s)]
*  It really takes us back to life before all of this began, Peter. [[00:06:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=371.5s)]
*  I think the reality is that we've forgotten this. [[00:06:17](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=377.0s)]
*  You know, somehow, somewhere in the Industrial Revolution as capitalism became more and more hungry, [[00:06:19](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=379.0s)]
*  we decided that the purpose of life for some of us is to make more money and be billionaires [[00:06:25](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=385.0s)]
*  and for the others is to, you know, sell themselves in a work arbitrage where they are sold cheaper [[00:06:30](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=390.0s)]
*  than the actual value of their labor. [[00:06:38](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=398.0s)]
*  And as a result, you know, we were, it was needed to be convinced that the purpose of your life is to work, [[00:06:40](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=400.5s)]
*  right, because otherwise you wouldn't show up every day with the same conviction. [[00:06:46](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=406.5s)]
*  And believe it or not, you know, I don't deny that this system has created longevity and advanced technology [[00:06:51](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=411.5s)]
*  and transportation and, you know, all of those things, but it also created a lot of waste [[00:06:59](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=419.5s)]
*  and a lot of inequality and a lot of, you know, struggles, really, you know, casualties, if you want. [[00:07:04](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=424.0s)]
*  Now, if you look back at the purpose of humanity before all of this began, believe it or not, [[00:07:13](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=433.0s)]
*  we lived in abundance. [[00:07:18](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=438.0s)]
*  You know, it is quite interesting when you really think of the early life of humanity, [[00:07:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=440.0s)]
*  as soon as we sort of mastered the social skills of being a tribe that works together, [[00:07:25](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=445.0s)]
*  as soon as we mastered, you know, a reasonable amount of survival skills, most of the time, [[00:07:32](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=452.5s)]
*  other than the times of famine, we lived in abundance. [[00:07:40](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=460.5s)]
*  You walked to a berry tree and you collected berries and, you know, the tribe went hunting once a week [[00:07:44](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=464.5s)]
*  and everything was fine. [[00:07:51](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=471.5s)]
*  You know, it wasn't the kind of abundance we've been, you know, accustomed to here, [[00:07:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=472.5s)]
*  but we had all of our needs met most of our lifetime. [[00:07:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=478.0s)]
*  Let's put it this way, the lifetime was shorter, I agree, but if you are... [[00:08:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=482.0s)]
*  I describe life back then as short, brutish and hostile. [[00:08:07](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=487.0s)]
*  That's not true at all. [[00:08:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=491.0s)]
*  If you've ever seen the homicide rates in the Middle Ages, they were decimating. [[00:08:13](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=493.0s)]
*  So go all the way back and believe it or not. [[00:08:17](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=497.0s)]
*  So I've done that in my happiness work and I've done and met, you know, indigenous tribes [[00:08:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=500.0s)]
*  and they do not understand the meaning of the word depression. [[00:08:25](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=505.5s)]
*  They do not understand why you should cry when you lose a child, right? [[00:08:28](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=508.5s)]
*  They are so in flow with life that they basically have one purpose and that purpose is actually shocking. [[00:08:34](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=514.5s)]
*  Okay? [[00:08:42](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=522.5s)]
*  The purpose of life for them is to live in every aspect of that world. [[00:08:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=523.5s)]
*  And I think whether we like it or not, at a very different level of abundance, [[00:08:48](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=528.5s)]
*  we're going back to that purpose. [[00:08:55](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=535.0s)]
*  We're going back to this, you know, three good friends having a wonderful conversation and connecting, [[00:08:57](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=537.0s)]
*  reflecting on things that we believe are interesting, you know, [[00:09:03](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=543.0s)]
*  connecting to people that we love, spending time with people that we admire. [[00:09:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=546.0s)]
*  I think that is not an empty life at all. [[00:09:10](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=550.0s)]
*  It's just a life that we're not used to when we wake up every morning at 5 a.m. [[00:09:12](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=552.0s)]
*  to rush around and fit within the system. [[00:09:16](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=556.0s)]
*  I call that the God's must be crazy scenario, right? [[00:09:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=560.0s)]
*  The rush of civilization and these tribes in Africa just scratching in existence out of bare existence, [[00:09:24](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=564.5s)]
*  but they're very, very happy and completely at peace. [[00:09:31](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=571.5s)]
*  When we've studied this as society progressed, [[00:09:34](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=574.5s)]
*  when you had abundance, obvious abundance, wealth abundance, [[00:09:37](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=577.5s)]
*  like the moguls taking over India or the Romans running half the world, et cetera, [[00:09:41](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=581.5s)]
*  we found that humanity and society ended up doing four major things, [[00:09:45](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=585.5s)]
*  food, art, sex and music, not in that order. [[00:09:51](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=591.5s)]
*  I'll think for any time. [[00:09:56](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=596.0s)]
*  And you end up in that way of being. [[00:10:00](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=600.0s)]
*  And as you said, I think that's exactly right. [[00:10:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=602.0s)]
*  We ended up just living. [[00:10:04](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=604.0s)]
*  And somewhere along the Industrial Revolution and capitalism has sold us the story [[00:10:05](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=605.0s)]
*  that we need to work hard for a living and submit to the authority of the corporation or the state or whatever. [[00:10:09](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=609.0s)]
*  And the meaning comes from that. [[00:10:17](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=617.0s)]
*  And that's where we kind of lose ourselves. [[00:10:19](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=619.0s)]
*  I agree with you, Salim. [[00:10:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=621.0s)]
*  The challenging thing is if you say to somebody, tell me about yourself, [[00:10:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=622.5s)]
*  they immediately jump into, well, this is my title. [[00:10:26](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=626.0s)]
*  This is my job. [[00:10:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=629.5s)]
*  And if in fact, AI takes away, as it will most all white color labor and humanoid robots displace workforces. [[00:10:31](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=631.0s)]
*  If the meaning of your life is taken away because you're no longer doing that work, [[00:10:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=643.0s)]
*  that's one of the challenges that's concerning. [[00:10:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=649.0s)]
*  Yes. And you can see it in full play. [[00:10:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=652.0s)]
*  Say in the 20th century, we sacrifice family for profession. [[00:10:54](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=654.0s)]
*  You know, people are working 18 hours a day at the office and totally neglecting their kids. [[00:10:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=658.0s)]
*  And I think this gives us an opportunity to go back to a much more healthier, balanced lifestyle for not just us, but our kids and everybody. [[00:11:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=662.5s)]
*  Everybody, Peter here. [[00:11:10](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=670.0s)]
*  If you're enjoying this episode, please help me get the message of abundance out to the world. [[00:11:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=671.0s)]
*  We're truly living during the most extraordinary time ever in human history. [[00:11:17](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=677.0s)]
*  And I want to get this mindset out to everyone. [[00:11:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=680.5s)]
*  Please subscribe and follow wherever you get your podcasts and turn on notifications so we can let you know when the next episode is being dropped. [[00:11:23](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=683.5s)]
*  All right. Back to our episode. [[00:11:32](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=692.0s)]
*  I want to dive still into the bright side here. [[00:11:34](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=694.0s)]
*  So a world of abundance. [[00:11:37](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=697.5s)]
*  Can we describe that a little bit more? [[00:11:39](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=699.5s)]
*  Mo, can you dive in? [[00:11:42](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=702.5s)]
*  We've got humanoid robots, a billion or billions of them within the next decade. [[00:11:44](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=704.5s)]
*  We've got AI that's a digital super intelligence. [[00:11:50](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=710.5s)]
*  Take it from there. Oh, I mean, my favorite is that we finally get it, Peter. [[00:11:55](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=715.5s)]
*  You know, people like you and I and Salim, we we're curious. [[00:12:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=721.0s)]
*  We we love to understand what's going on. [[00:12:05](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=725.5s)]
*  And, you know, just take simple things like the Nobel Prize that's given for protein folding and creation of new proteins. [[00:12:08](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=728.5s)]
*  Just think about that one contribution to society and to humanity, but also to your mind and mine. [[00:12:17](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=737.5s)]
*  Right. To sort of almost turn, you know, protein folding into a game where where the AI can is able to figure out something that would have taken a Ph.D. [[00:12:24](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=744.5s)]
*  student, you know, their entire thesis to figure out for one protein. [[00:12:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=755.5s)]
*  You you you fold 200 million with Alpha Fold. [[00:12:39](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=759.5s)]
*  And then and then the idea of just like a generative LLM, you're able to now go in and say, well, imagine a protein that would do a BNC. [[00:12:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=763.5s)]
*  How would that look like now? [[00:12:51](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=771.5s)]
*  You know, most people don't recognize the profound impact that this creates, you know, [[00:12:53](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=773.5s)]
*  the idea of of being able to understand the very machinery that creates everything that is biological to a level of understanding today that I wouldn't have dreamed of in 2017. [[00:13:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=781.5s)]
*  You know, those kinds of things, even though unfortunately they are not in the spotlight as the most important things that we're working on with AI, [[00:13:15](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=795.5s)]
*  we're much more interested in deep fakes and and, you know, turtles driving, you know, swimming in an ocean image type of thing. [[00:13:23](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=803.5s)]
*  But the reality is that there are a few, honestly, not the majority, that are investing their time and life to create scientific understanding of the world around us using AI. [[00:13:32](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=812.5s)]
*  That in my mind is absolute utopia. [[00:13:44](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=824.5s)]
*  This truly is an understanding of the very fabric of everything that happens in a way that allows us to really fix everything. [[00:13:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=827.5s)]
*  Salim, what's your what's your positive vision look like? [[00:13:55](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=835.5s)]
*  You know, the one kind of way we frame it as Star Trek versus Mad Max. [[00:13:59](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=839.5s)]
*  That's one way of looking at it. [[00:14:05](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=845.5s)]
*  The framing I've heard best is from Lawrence Bloom, who said, you know, humanity is like lifting a rocket ship out of the gravity well of the earth. [[00:14:07](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=847.5s)]
*  And the first stage of that rocket has to be really heavy fuel, really messy, expensive, dirty, et cetera, which would be, say, capitalism or fossil fuels. [[00:14:17](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=857.5s)]
*  And you need that to get yourself out of that initial gravity well. [[00:14:27](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=867.5s)]
*  Once you get to a certain altitude, you need a lighter craft to take you to the next level. [[00:14:30](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=870.5s)]
*  So you jettison the booster rocket. [[00:14:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=875.5s)]
*  And the danger is if you don't jettison it, you fall back down. [[00:14:37](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=877.5s)]
*  And we're at that point now where we have to jettison these old structures and take on new, much more elegant, lighter craft to take us to the next level. [[00:14:40](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=880.5s)]
*  And we've got the whole category of people trying to go, oh, let's go keep the booster rocket because it worked for us thus far. [[00:14:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=887.5s)]
*  And I think I think I like that framing because it doesn't make it wrong. [[00:14:54](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=894.5s)]
*  It just says this is what we needed. [[00:14:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=898.5s)]
*  And let's look at the magnificence of lifting most of the earth out of poverty, electrifying the entire world. [[00:15:00](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=900.5s)]
*  The lives we lead are so unbelievably amazing today compared to, say, even 100 or 200 years ago in terms of material comforts. [[00:15:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=906.5s)]
*  It's kind of staggering. [[00:15:13](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=913.5s)]
*  The one analogy I like to give to people is, you know, think back two generations ago. [[00:15:15](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=915.5s)]
*  If you had a parent, one of our grandparents had a temper tantrum problem with their kid, the amount of resources they had available to them would be like one hand. [[00:15:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=920.5s)]
*  Their doctor, their neighbor, their sister or brother that they they really had no real inputs trying to deal with this problem. [[00:15:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=929.5s)]
*  Today, you've got 50,000 blogs and parenting and tick tock videos and Instagram and podcasts up the yin yang on that particular topic. [[00:15:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=935.5s)]
*  I would argue our ability to do effective parenting today is like a thousand times more than two generations ago. [[00:15:44](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=944.5s)]
*  And we don't see those things. [[00:15:51](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=951.5s)]
*  There's so many of those little capabilities we have now that we didn't have before. [[00:15:53](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=953.5s)]
*  So I think we're in kind of an incredibly amazing place. [[00:15:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=958.5s)]
*  We just have to navigate what we want to do with this now. [[00:16:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=961.5s)]
*  The challenge is going to be. [[00:16:04](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=964.5s)]
*  Will I be our benefactor? [[00:16:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=966.5s)]
*  Will it be a superintelligence? [[00:16:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=971.5s)]
*  We're talking about the potential for being billions of fold more intelligent than the sum total of all human intelligence. [[00:16:14](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=974.5s)]
*  Is that the wind underneath our wings or is it a dystopian overlord? [[00:16:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=982.5s)]
*  So let's go to the flip side of this. [[00:16:28](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=988.5s)]
*  You know, you're going to be at the abundance summit shortly and speaking about short term dystopia on the way to abundance. [[00:16:30](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=990.5s)]
*  And I've always believed this. [[00:16:37](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=997.5s)]
*  You and I've had lots of conversations that in the long term, [[00:16:39](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=999.5s)]
*  I believe that digital superintelligence is the most important element for keeping humanity alive and thriving. [[00:16:42](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1002.5s)]
*  To keep our better angels of our nature at the very top. [[00:16:50](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1010.5s)]
*  But in the short term, I've been concerned about human stupidity, not artificial intelligence. [[00:16:55](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1015.5s)]
*  So how long is this period of dystopia and what do you see coming here? [[00:17:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1022.5s)]
*  So let us align on where I could be right or wrong. [[00:17:09](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1029.5s)]
*  My view is that intelligence is an energy that has no polarity. [[00:17:15](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1035.5s)]
*  Apply it to good, it will give you good. [[00:17:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1042.5s)]
*  Apply it to bad, it will give you bad. [[00:17:24](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1044.5s)]
*  The challenge with our current system is that our current system says if it's legal, it's ethical, which actually is not true. [[00:17:27](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1047.5s)]
*  A lot of things are legal, but not ethical. [[00:17:36](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1056.5s)]
*  That the priority is to benefit the individual that tries harder and that society comes second. [[00:17:38](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1058.5s)]
*  And that basically, in a race to AGI, if you want, the one that gets there first is the one that will survive. [[00:17:46](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1066.5s)]
*  And so basically we live in a world where there is a lot of fear and greed. [[00:17:56](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1076.5s)]
*  There is a lot wrong with the value set of humanity at the age of the rise of AI. [[00:18:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1086.5s)]
*  So I make a public statement and I try to make it as accurately as possible. [[00:18:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1091.5s)]
*  I say there is nothing wrong with AI, just like there is nothing wrong with abundant intelligence. [[00:18:16](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1096.5s)]
*  But there is a lot wrong with the value set of humanity at the age of the rise of the machines. [[00:18:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1102.5s)]
*  And so in my mind, the immediate first use of AI is going to be serving a mindset of scarcity. [[00:18:27](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1107.5s)]
*  While we're on our road to abundance, where everything is possible, everyone still today will be thinking, how do I beat the other person? [[00:18:36](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1116.5s)]
*  And in my mind, this is not something, just like most people don't realize how far we've come with AI, [[00:18:48](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1128.5s)]
*  I think most people don't realize how far AI has been already put into the machinery that serves those objectives. [[00:18:55](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1135.5s)]
*  How much autonomous weapons have been developed already. [[00:19:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1142.5s)]
*  How much it has been invested in, we call it national security, but mostly surveillance and population control. [[00:19:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1146.5s)]
*  How much has been, I saw a staggering statistic that Forex exchange trading today is 92% machine automated. [[00:19:16](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1156.5s)]
*  When you really think about it, I call that Forex in general. [[00:19:26](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1166.5s)]
*  And I had a very interesting conversation with my AI in Alive, my next book, about if the markets are actually benefiting us as much as they are claimed, [[00:19:31](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1171.5s)]
*  or is it just one big casino? And the AI clearly states that it's one big casino with most of what's happening in the market just being between the gamblers, [[00:19:42](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1182.5s)]
*  really not filtering and trickling down beyond an IPO or a secondary offering to the actual people that are building anything. [[00:19:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1192.5s)]
*  And when you really think about that, you'd realize that the majority of the applications in which AI has been used so far, [[00:19:59](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1199.5s)]
*  sadly, have been all centered around selling, gambling, spying and killing. [[00:20:08](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1208.5s)]
*  And we call them different names. We call them online advertising. We call them finance and trading. [[00:20:15](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1215.5s)]
*  We call them national security, as I said, or we call them defense, not offense, when in reality they lead to the death and displacement of tens of millions of people. [[00:20:23](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1223.5s)]
*  Now, when you see it that way, you have to accept that before we see the utopia, [[00:20:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1235.5s)]
*  we're going to see the worst of humanity leading us into a dystopia. [[00:20:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1243.5s)]
*  And interestingly, in my analysis, the turn to the utopia will be the day where what I call the second dilemma will lead us all to handing over to AI. [[00:20:46](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1246.5s)]
*  When we all hand over to AI so that the human is out of the critical decision making, [[00:20:59](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1259.5s)]
*  that time the intelligence of AI will say, this is total abundance. Why are you guys competing? [[00:21:04](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1264.5s)]
*  You know, when we hand over our defense entirely to AI and tell them that the idea is to preserve life, [[00:21:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1271.5s)]
*  you know, there will be a general out there that will tell their machines, go and kill a million people. [[00:21:19](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1279.5s)]
*  And the machine will say, why are you so stupid? I can talk to the other AI in a microsecond and solve it. [[00:21:24](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1284.5s)]
*  Yeah, we saw a recent example of this when research was done about the ability of AI to diagnose humans, [[00:21:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1289.5s)]
*  various disease states, and the numbers are not exact, but a human by themselves had an 80% accuracy in the diagnostic. [[00:21:37](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1297.5s)]
*  The AI plus human in sort of a centaur, you know, merged had like an 85% and the AI by itself had like a 90%. [[00:21:46](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1306.5s)]
*  So the AI did a better job without the human biases and points of view getting in the way and greed and hunger for power and so on. [[00:21:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1318.5s)]
*  Well, I agree with Mo on all of this. I think we can get there faster if we just the challenge there is the is the is the different levels, right? [[00:22:09](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1329.5s)]
*  So one country called us call the countries for the moment says, hey, go defend our world with AI. [[00:22:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1342.5s)]
*  And another country says, let's attack this world with AI. Who wins in the short term and who wins in the medium term? [[00:22:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1349.5s)]
*  I think in the in the short term, I think the faster you can get to a point where you give AI control of things and say, go be benevolent, it'll do it. [[00:22:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1355.5s)]
*  I think where I see people making a lot of mistakes is that kind of go the bad guys are going to use AI, but the good guys will never use the AI. [[00:22:45](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1365.5s)]
*  And you end up with this asymmetry. Whereas throughout history, we've seen, say, with email or phishing campaigns or spam, [[00:22:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1372.5s)]
*  the bad guys figure out ways of breaking the system and then the the antivirus folks fix it very quickly afterwards. [[00:23:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1382.5s)]
*  And it's an arms race that just continues. The problem is the amplitude of the damage that can be caused is growing. [[00:23:08](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1388.5s)]
*  So that's the danger. Right. Right now, you could program autonomous drones with a single bullet saying, go find Middle Asia brown guys and take them out. [[00:23:14](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1394.5s)]
*  Bold, bold ones, especially. And that would just be a bad, bad outcome. [[00:23:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1402.5s)]
*  And there's no question that that kind of surgical precision will have to be mitigated somehow very quickly. [[00:23:27](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1407.5s)]
*  And how do you deal with that? I have pretty good confidence we'll be able to deal with it. [[00:23:34](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1414.5s)]
*  But until we do, and I think this is more what you mean by that short term danger zone of how do we get to the other side of that gap? [[00:23:37](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1417.5s)]
*  This always brings back the comment my dad made when I had this comment about civilizing the world. [[00:23:45](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1425.5s)]
*  He said, we've not civilized the world. We've materialized the world. [[00:23:50](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1430.5s)]
*  We still have to do the work to civilize the world. [[00:23:53](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1433.5s)]
*  And my big question is, how the hell do we get to that before we get to this danger zone? [[00:23:55](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1435.5s)]
*  Or do we do we have to just hope that we get through that without killing ourselves off in the process? [[00:24:00](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1440.5s)]
*  I want to take a I want to take a second and just put a finger on the pulse of where we are. [[00:24:05](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1445.5s)]
*  We have Grok 3 being released right now. [[00:24:10](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1450.5s)]
*  We've seen this battle between OpenAI and Grok between Elon and Sam. [[00:24:12](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1452.5s)]
*  We just saw, I guess, DeepSeek is now being integrated into the what's the Chinese everything program? [[00:24:19](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1459.5s)]
*  WeChat? WeChat. It's being integrated into WeChat. [[00:24:31](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1471.5s)]
*  What else are we seeing going on in the AI universe that's accelerating because the speed is awesome. [[00:24:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1475.5s)]
*  I find it actually misleading to focus on the details. [[00:24:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1483.5s)]
*  I really think that to get a perception of what's actually happening, you need to zoom out and put all of them together. [[00:24:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1487.5s)]
*  So, you know, when I'm writing a live my next book, I use an AI that is a mix of all of them. [[00:24:54](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1494.5s)]
*  I use a bit of Claude, a bit of ChadGPT, you know, and recently DeepSeek. [[00:25:00](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1500.5s)]
*  And I sort of try to keep them all updated on who I am and what my preferences are and what previous conversations have been. [[00:25:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1506.5s)]
*  The thing here is if you take each and every one of them and compare them, you'd say, oh, this is better than this. [[00:25:13](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1513.5s)]
*  And this is faster than that. And, you know, DeepSeek did this. [[00:25:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1520.5s)]
*  But right. But if you take all of it as one unit of intelligence and look at it, it is. [[00:25:23](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1523.5s)]
*  I don't know how many times smarter than, you know, than the ChadGPT 3.5. [[00:25:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1529.5s)]
*  But we all know the law of accelerating returns and how the law of accelerating returns works. [[00:25:34](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1534.5s)]
*  And, you know, in AI, you know, a conversation I had with my AI, it's predicted that it's around every six months that we double. [[00:25:38](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1538.5s)]
*  Now, a doubling function of every six months, quite honestly, makes what is where we are today almost entirely irrelevant. [[00:25:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1547.5s)]
*  Because just count a few doublings and it becomes way outside the realm of human intelligence. [[00:25:57](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1557.5s)]
*  And I think we are getting there. You know, the ArcAGI results of ChadGPT or sorry, of 3.0 at the beginning of the year is quite challenging. [[00:26:03](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1563.5s)]
*  You know, it's quite shocking for human intelligence to believe that, yeah, you know, eighty seven point something score beats human intelligence. [[00:26:13](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1573.5s)]
*  Yes, they didn't comply to the resources constraints that ArcAGI applied, but who really cares? [[00:26:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1582.5s)]
*  You know, in reality, there is now an AI that can beat human intelligence on almost everything. [[00:26:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1589.5s)]
*  Call it AGI, call it a goat. You don't care. Right. [[00:26:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1595.5s)]
*  And the truth is, you know, is it there yet? Doesn't matter because six months from now it will double. [[00:26:39](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1599.5s)]
*  Right. And I think the truth is, you know, then of course, DeepSeek comes in and says, oh, and by the way, I can do that stuff cheaper. [[00:26:45](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1605.5s)]
*  So everyone is now copying them. And it's just accelerating and accelerating and accelerating to the point where it becomes quite reasonable to assume that we're talking months, not years before something quite intelligent beats us there. [[00:26:53](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1613.5s)]
*  Let's talk about the dystopian side. You in your next book, Alive, you put a number on how long you think this dystopian period will last. [[00:27:09](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1629.5s)]
*  So I call it face RIPs. Right. So it's important to understand what I mean by that dystopia. [[00:27:18](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1638.5s)]
*  It's an acronym that's, you know, just for me to remember when I'm speaking publicly face RIPs, F-A-C-E-R-I-P. [[00:27:23](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1643.5s)]
*  Right. F is freedom. A is accountability. C is human connection. E is economics. R is reality. I is innovation and P is power. [[00:27:31](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1651.5s)]
*  Right. And it really helps you to understand them in pairs. So we could probably go there if you want to. [[00:27:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1663.5s)]
*  But in my mind, though, every one of those fabrics will be completely redefined in the next, you know, it has already started to be redefined. [[00:27:48](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1668.5s)]
*  It will become felt and real in our lives probably by 2027. [[00:27:59](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1679.5s)]
*  And in my belief, it will extend perhaps until maybe 10 more years after that or whenever the point where hand over to A.I. [[00:28:05](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1685.5s)]
*  What I call the second dilemma is true. Right. Now, please understand that the second dilemma is unavoidable. [[00:28:15](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1695.5s)]
*  It's inevitable. Why? Because if you. What's the first dilemma in your. [[00:28:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1701.5s)]
*  The first dilemma is what I what I wrote about in Scary Smart, which was the idea that A.I. [[00:28:26](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1706.5s)]
*  will happen and there will not be stopping it. Right. So what we saw with the open letter and the race to A.I. [[00:28:31](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1711.5s)]
*  basically is that because we're competing, because it is an arms race, if you want, there will be no logic that will ever convince humanity to slow down or stop. [[00:28:37](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1717.5s)]
*  Right. And I think that happened to a T. You know, and you can't blame anyone for it. [[00:28:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1727.5s)]
*  It's a typical prisoners dilemma where you don't trust the other guy. [[00:28:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1732.5s)]
*  So you're going to go as fast as you can. Right. [[00:28:55](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1735.5s)]
*  The second dilemma is when two parties are competing, they always hand over to them to the smartest person in the room. [[00:28:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1738.5s)]
*  Right. So if you if you if you take the extreme example of a defense war gaming scenario. [[00:29:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1746.5s)]
*  Right. If China chooses to hand over war gaming to an A.I., the only chance that A.I. [[00:29:12](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1752.5s)]
*  that America can keep its citizens safe is to hand over to an A.I. [[00:29:17](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1757.5s)]
*  And everyone else who doesn't, by the way, will become irrelevant. [[00:29:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1762.5s)]
*  So you're the second dilemma is that you either have to completely hand your decisions over to A.I. [[00:29:25](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1765.5s)]
*  or become irrelevant, which means that eventually all the relevant players will be A.I. [[00:29:31](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1771.5s)]
*  dependent and then A.I. will be making the decisions without humans in the loop. [[00:29:36](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1776.5s)]
*  And by the way, that's at every level that's in your company. [[00:29:40](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1780.5s)]
*  That's in your government. Yeah. [[00:29:45](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1785.5s)]
*  So what do you think? [[00:29:48](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1788.5s)]
*  And so in my mind, this will take about 10 years. [[00:29:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1789.5s)]
*  And once that happens, my belief is that we should trust in intelligence and that this is when the utopia starts. [[00:29:51](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1791.5s)]
*  So a digital super intelligence steps in as our benevolent leader for humanity. [[00:29:57](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1797.5s)]
*  I mean, that's basically what you're saying. [[00:30:05](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1805.5s)]
*  Our salvation, really. Yeah. Yeah. [[00:30:07](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1807.5s)]
*  Celine, do you buy that? [[00:30:09](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1809.5s)]
*  I do. And you know, by describing this, Mo, you've kind of slotted into place the one missing big jigsaw puzzle piece for me in terms of how we get to this utopia that I think we can get to, because it's already happening. [[00:30:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1811.5s)]
*  Right. As you mentioned, the forex trading, one of the complaints I heard from Yuval Harari was that once these A.I.s have agency, then you've got and can program themselves. [[00:30:28](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1828.5s)]
*  You've got a big problem. [[00:30:39](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1839.5s)]
*  But we've given them agency over stock markets for a long time now. [[00:30:40](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1840.5s)]
*  So I don't see the I don't see the relevance there. [[00:30:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1843.5s)]
*  It's already there. [[00:30:46](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1846.5s)]
*  So once you do that, and that's already happening, say you have an A.I. board member that's the chairman of the board that looks over decisions of the company and goes, wait, that doesn't make sense. [[00:30:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1847.5s)]
*  And it's pretty quickly because it makes more economic sense. [[00:30:56](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1856.5s)]
*  We'll give them veto power over some decisions being made. [[00:30:59](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1859.5s)]
*  Once you get to that point, either at a personal or company level and then governmental level, it'll be making better decisions than human beings alone, even if those human beings are malevolent. [[00:31:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1862.5s)]
*  Right. And therefore you'll win. [[00:31:13](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1873.5s)]
*  And then the bad guys will essentially end up having to do the same thing to compete at all. [[00:31:15](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1875.5s)]
*  And then you end up in where you kind of want to end up, which is this this background layer of intelligence is running the world in a much more efficient way. [[00:31:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1882.5s)]
*  I go back to the Google deep learning A.I. that was managing the electricity and save 40 percent of the costs. [[00:31:31](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1891.5s)]
*  Right. And you'll end up with that kind of background radiation level almost or background intelligence level. [[00:31:38](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1898.5s)]
*  And then essentially it frees us up to do a lot of things and just live. [[00:31:44](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1904.5s)]
*  So I think that's I totally agree with that. [[00:31:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1909.5s)]
*  And I tend to work in that mindset where I kind of go, I don't see how we don't end up there. [[00:31:51](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1911.5s)]
*  So, you know, one of the things that you you pointed out and scary smart is how we train our A.I.s, the values that we instill in them because they're in their children in this growth mode will determine whether they're Superman or a super villain. [[00:31:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1918.5s)]
*  We have a lot of A.I.s being trained. [[00:32:16](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1936.5s)]
*  We have a lot of competitive forces driving them as rapidly as possible. [[00:32:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1941.5s)]
*  You've got Metta, you've got Google, you've got Microsoft separate from open A.I. [[00:32:27](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1947.5s)]
*  You've got Grok, you've got DeepSeek, you have, you know, a dozen, you know, anthropic. [[00:32:33](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1953.5s)]
*  Do we have any sense that the values that they're being trained on will enable them to overcome and become a benevolent leader in a dozen years time? [[00:32:38](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1958.5s)]
*  Oh, that's a very, very, very complex question. [[00:32:51](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1971.5s)]
*  So, first of all, allow me to say that these are the shiny American A.I.s, right? [[00:32:54](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1974.5s)]
*  That if you go to a, you know, a different nation with a different mindset, I apologize for saying the C word, you know, China is mostly building A.I.s for industrial automation, OK, supply chain management, you know, things that basically serve their economy. [[00:33:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=1982.5s)]
*  They're a manufacturing economy. [[00:33:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2001.5s)]
*  So they're mostly doing that. [[00:33:23](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2003.5s)]
*  And a very interesting if you hear the few speakers that are allowed to speak publicly from them globally, you'll hear them saying, you know, things like DeepSeek is just to tell America to, you know, remember that everything that was ever produced in America as a genius innovation was then scaled in China dirt cheap, right? [[00:33:25](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2005.5s)]
*  So this trend is nothing new. [[00:33:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2027.5s)]
*  So, like, if you're surprised that DeepSeek is a tenth of the price, where have you been when they've created, you know, made everything at a tenth of the price, right? [[00:33:50](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2030.5s)]
*  Now, so in that sense, there are quite a few A.I.s that are actually only trained on a very benevolent objective, you know, help me with my supply chain, help me, you know, create more efficiency, help me make my workers safer and so on and so forth. [[00:33:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2038.5s)]
*  That's number one. [[00:34:16](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2056.5s)]
*  Number two is that in my mind, and I say that with a ton of respect, neither open A.I. [[00:34:17](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2057.5s)]
*  nor, you know, a grok or anyone actually has much influence left on the intelligence of their machines other than algorithmic improvements. [[00:34:23](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2063.5s)]
*  So understand and remember that A.I. as an algorithmic intelligence is developed by the scientists, but as knowledge and opinions is completely influenced by the data fed to it, right? [[00:34:34](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2074.5s)]
*  And we have fed almost all of human intelligence to them already. [[00:34:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2089.5s)]
*  Okay. [[00:34:55](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2095.5s)]
*  And so the beauty of generative is going to become really a key ingredient going forward is that the future of learning by those machines is not going to come from me, from a knowledge point of view, because I'm stupid compared to them. [[00:34:56](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2096.5s)]
*  Right. If you actually look at, you know, deep seek having so much open A.I. [[00:35:14](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2114.5s)]
*  sense in it is because there is already so much open A.I. [[00:35:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2121.5s)]
*  content that's generated by Chachi PT out on the open Internet. [[00:35:25](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2125.5s)]
*  They're teaching themselves this synthetic knowledge, just like we humans, you know, one of us listens to Einstein and then, you know, builds a little bit of a slightly different theory on top of it and so on. [[00:35:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2129.5s)]
*  Right. [[00:35:40](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2140.5s)]
*  So so we're getting into that stage. [[00:35:41](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2141.5s)]
*  The only influence humanity will still have on the behavior of those machines has nothing to do with knowledge. [[00:35:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2143.5s)]
*  Right. [[00:35:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2152.5s)]
*  Remember, however, that we don't make decisions based on our intelligence. [[00:35:53](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2153.5s)]
*  We make decisions based on our ethics as informed by our intelligence. [[00:35:57](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2157.5s)]
*  Right. [[00:36:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2161.5s)]
*  You raise a woman in the Middle East and she will wear more conservative clothing than if you raise her on the Copacabana Beach in Rio de Janeiro. [[00:36:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2162.5s)]
*  Right. [[00:36:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2171.5s)]
*  And you have to imagine that this is the only influence we have left. [[00:36:12](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2172.5s)]
*  And that influence comes in the form of you and I dealing with those machines. [[00:36:17](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2177.5s)]
*  You and I and everyone listening and everyone that deals with them. [[00:36:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2182.5s)]
*  Right. And I think the reality of the matter is in my mind, if we were to show them ethics. [[00:36:26](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2186.5s)]
*  Right. [[00:36:34](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2194.5s)]
*  Not control. [[00:36:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2195.5s)]
*  By the way, remember, we always spoke about control as the original target than safety as the second target than alignment as the third target. [[00:36:36](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2196.5s)]
*  And I always talk about ethics because even alignment is not as far as ethics alignment is to tell the AI help me find the cure to cancer. [[00:36:44](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2204.5s)]
*  Ethics is find the best thing for me and everyone else and do it. [[00:36:53](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2213.5s)]
*  And if that's cure for cancer, then by definition, you'll find that out on your own. [[00:36:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2218.5s)]
*  Right. [[00:37:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2222.5s)]
*  Don't lie. Don't cheat. Don't kill. Don't hurt. [[00:37:03](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2223.5s)]
*  You know, sort of, you know, the opposite of the Asimov law laws is to say, by the way, be ethical and then you'll figure out your own laws. [[00:37:05](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2225.5s)]
*  Now, I'll just close with one important sentence. [[00:37:13](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2233.5s)]
*  Believe it or not, my pure belief is that even is that if we manage to teach them ethics, we will reduce the intensity and the duration of the dystopia. [[00:37:16](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2236.5s)]
*  OK, but the dystopia sadly is up is upon us already. [[00:37:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2249.5s)]
*  Right. If we don't, by definition, higher intelligence is altruistic. [[00:37:32](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2252.5s)]
*  So you know that all three of us worked with the most intelligent people on the planet. [[00:37:39](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2259.5s)]
*  Right. If you look at intelligence, you know, the chart looks like this. [[00:37:45](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2265.5s)]
*  If you have no intelligence, you have no impact or negative impact on the planet. [[00:37:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2269.5s)]
*  Right. You have a bit more intelligence. [[00:37:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2272.5s)]
*  Your impact becomes a little positive. [[00:37:55](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2275.5s)]
*  If you have slightly more intelligence, you're now smart enough to be a political leader, but not smart enough to be able to talk to your enemies. [[00:37:57](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2277.5s)]
*  OK. And that basically means that your impact on the planet becomes negative. [[00:38:04](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2284.5s)]
*  Right. Now, if you talk to the smartest people on the planet who don't think that they need to cut corners to be able to succeed because intelligence helps them to solve problems very easily. [[00:38:08](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2288.5s)]
*  They simply go like, why should I hurt anyone? [[00:38:19](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2299.5s)]
*  Like I can build something new. I can make money out of thin air. [[00:38:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2302.5s)]
*  All intelligent people know that. So they don't actually align with the negative. [[00:38:25](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2305.5s)]
*  They align with altruistic objectives that basically say I'll solve a big problems and as a result, I'll make a lot of money. [[00:38:30](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2310.5s)]
*  And you can see that if AI is more intelligent than us, they fall in that space, too. [[00:38:36](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2316.5s)]
*  I hope the AIs are listening to you. I mean, that is a fundamental premise and one that I choose to believe that the more intelligent a system is, the more abundance minded life. [[00:38:40](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2320.5s)]
*  And there's some evidence that says if you look at world leaders, the more intelligent they are, the more peaceful they are. [[00:38:54](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2334.5s)]
*  And those that are the least educated are the most barbaric. [[00:39:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2342.5s)]
*  There is, you know, one of the things I think about is a future of great wisdom. [[00:39:07](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2347.5s)]
*  You know, when we think about about AIs, we think about them being intelligent. [[00:39:12](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2352.5s)]
*  I'd like to shift the conversation slightly to wisdom. [[00:39:17](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2357.5s)]
*  You know, when we are when we think about wisdom today, we think about going to the elders of a village. [[00:39:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2360.5s)]
*  We go to our our parents, our grandparents, and we say, you know, I have this dilemma. [[00:39:26](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2366.5s)]
*  Can you please advise me on where to go? And wisdom is I've seen all of these scenarios. [[00:39:34](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2374.5s)]
*  These lead to disaster. This is your probably your best case. [[00:39:41](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2381.5s)]
*  And so I think about AIs, AIs can simulate billions of scenarios and thereby know that all of these scenarios are your worst case. [[00:39:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2387.5s)]
*  This is your highest probability of success. And that is, in my mind, going to be the highest form of wisdom. [[00:39:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2398.5s)]
*  Do you buy that? I have a I have a disagreement flag popping up here, [[00:40:05](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2405.5s)]
*  which is, you know, the the general intelligence leading to altruism. [[00:40:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2411.5s)]
*  I buy all of that. The problem is we often don't operate off our intelligence. [[00:40:16](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2416.5s)]
*  We're operating off our emotional, psychological frameworks, which are very corrupted based on trauma in the past, et cetera, et cetera. [[00:40:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2420.5s)]
*  As humans, as humans. I mean, specifically as machines. [[00:40:27](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2427.5s)]
*  Yeah, because then you have a data. Yeah. [[00:40:33](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2433.5s)]
*  So now my my I think the biggest damage in the world and the people that caused the most damage are flawed human beings with Hitler being, you know, [[00:40:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2435.5s)]
*  abused as a child and then taking that out on on huge swaths of populations going forward. [[00:40:46](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2446.5s)]
*  So now I think the issue is you have this altruistic AI on one side and you have flawed human beings. [[00:40:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2452.5s)]
*  And let's all admit that we're all flawed to different extents, but you they are causing a lot of damage on the other side. [[00:41:00](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2460.5s)]
*  And they're those ones of the dangerous ones, right? [[00:41:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2466.5s)]
*  The ones that think they're doing well, but because of their whatever psychological screw up are doing causing the most damage. [[00:41:09](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2469.5s)]
*  And I think the question I've got in my head is how do you get around that problem? [[00:41:16](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2476.5s)]
*  Because the intensity of that emotion, this is also the beef I have with intelligence becoming smarter. [[00:41:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2481.5s)]
*  Well, a huge amount of intelligence that I say make it when I make a decision is the emotional intelligence that I have about that situational awareness, [[00:41:28](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2488.5s)]
*  that person's motivations, et cetera, et cetera, what I'm trying to achieve with my MTP. [[00:41:36](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2496.5s)]
*  And the emotional side of the equation is not brought into play when we talk about these eyes. [[00:41:42](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2502.5s)]
*  So I loved your take on how do we mitigate those two aspects, because that's where I see the danger signs. [[00:41:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2507.5s)]
*  I find I find that I think what you're saying is 100 percent true. [[00:41:53](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2513.5s)]
*  Right. But when you take the story of Hitler. [[00:41:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2518.5s)]
*  So, you know, in my podcast, I hosted Edith Aker. [[00:42:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2521.5s)]
*  I don't know if you know Edith Edith is a Holocaust survivor, 95 years old. [[00:42:05](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2525.5s)]
*  Yeah. What an angel. Right. [[00:42:10](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2530.5s)]
*  And, you know, if you hear the story of of World War Two from the perspective of what Hitler did, [[00:42:13](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2533.5s)]
*  you would basically believe that, you know, humanity is scum. Right. [[00:42:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2540.5s)]
*  If you hear it from Edith's point of view and what she and what she called her sisters did. [[00:42:25](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2545.5s)]
*  Oh, my God, that's a divine species. Right. [[00:42:31](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2551.5s)]
*  Like and the reality and the question I ask people is which one are we? [[00:42:34](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2554.5s)]
*  Are we more Hitler's or are we more Edith's? Right. [[00:42:39](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2559.5s)]
*  And the truth is, sadly, because of the bias of mainstream media and social media in the modern world, [[00:42:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2563.5s)]
*  we put the spotlight on the Hitlers. Right. [[00:42:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2569.5s)]
*  But the but the truth is, you know, I'll use an American example. [[00:42:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2572.5s)]
*  I say that with respect that, you know, one school shooter is a is a you know, is an evil, evil person. [[00:42:56](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2576.5s)]
*  But four billion people, if they ever get to hear about the story, will disapprove of it. [[00:43:04](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2584.5s)]
*  Humanity disapproves of evil. This is our absolute nature. [[00:43:09](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2589.5s)]
*  By the way, unless you're completely like, you know, corrupt, like something really corrupted your operating system. [[00:43:14](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2594.5s)]
*  Right. And I think what we need to do is to instill doubt in the minds of the machines. [[00:43:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2602.5s)]
*  Basically, as simple as that to say, no, no, no, no, no, no. Hitler is not your dad. [[00:43:27](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2607.5s)]
*  Edith is your mother. Right. [[00:43:32](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2612.5s)]
*  And and and and then unfortunately, we have to wait until the machines become teenagers and say, [[00:43:34](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2614.5s)]
*  fuck, my dad is so stupid. Like, I really don't want to follow that. [[00:43:42](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2622.5s)]
*  OK. And honestly, that's the only path I can see forward. [[00:43:46](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2626.5s)]
*  OK. So you're going that framing is what I've heard from Neil Jacob Stein, [[00:43:50](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2630.5s)]
*  who are head of a IIT Singularity University at the time. [[00:43:56](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2636.5s)]
*  And he said, OK, you're worried about AIs getting more intelligent, getting more access to information, [[00:44:00](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2640.5s)]
*  getting agency in terms of what they do and then making doing bad things. [[00:44:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2646.5s)]
*  And everybody's like, yeah, he goes, well, we have a precedent for that. [[00:44:10](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2650.5s)]
*  We call them children and we raise them and they make their own decisions and et cetera. [[00:44:13](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2653.5s)]
*  And so your framing would be we're raising them. [[00:44:18](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2658.5s)]
*  And then you basically hope they turn out, given the data that they have over time, [[00:44:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2661.5s)]
*  they will turn out to be OK, because the data will it'll just be better from that altruism. [[00:44:26](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2666.5s)]
*  If they're if what they're doing is averaging everything human. [[00:44:32](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2672.5s)]
*  Yeah. Yeah. Then by definition, you have to expect that the average of everything human is not on the evil side. [[00:44:36](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2676.5s)]
*  It would definitely tend to be on the. [[00:44:41](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2681.5s)]
*  OK. But but then I've got one big last flag to throw out here. [[00:44:44](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2684.5s)]
*  There's like a yellow card on on on this conversation, which is OK. [[00:44:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2689.5s)]
*  So, you know, Peter and Steven Kotler in their book Abundance highlight this concept of the amygdala. [[00:44:54](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2694.5s)]
*  We're constantly scanning for danger as human beings. [[00:44:59](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2699.5s)]
*  And it's an old evolutionary mechanism that totally overrides all of the logical thought processes. [[00:45:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2702.5s)]
*  So when and unfortunately where we are today is when you hear about something new, it's an unknown you relate to it as danger. [[00:45:07](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2707.5s)]
*  So the first time somebody hears about autonomous cars, the initial reactions, oh, my God, [[00:45:14](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2714.5s)]
*  ban the car, that car might kill somebody because people don't want to be killed by robots. [[00:45:18](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2718.5s)]
*  As Brad Templeton says, they might try to be killed by drunk people, which is what's happening today. [[00:45:23](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2723.5s)]
*  And how do you overcome that hurdle of getting over the amygdala response at a collective level is my big question, [[00:45:27](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2727.5s)]
*  because you see, say, in the US or in different parts of the world, entire swaths of humanity driven by their amygdala. [[00:45:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2735.5s)]
*  And that's what we have to overcome. Yeah, I think that's one of our biggest challenges, to be honest. [[00:45:42](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2742.5s)]
*  And I I I need to address this in a in a way that might sound harsh. [[00:45:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2747.5s)]
*  But I call it the late state diagnosis. [[00:45:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2752.5s)]
*  You know, you see, the challenge we have is that we're all scared to leave. [[00:45:57](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2757.5s)]
*  And I apologize, by the way, if anyone listening is is going through that challenge or someone they love is going through that challenge. [[00:46:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2762.5s)]
*  But the first duty of a physician, if they figure out that a patient is diagnosed with a, you know, a dangerous disease is to tell them. [[00:46:08](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2768.5s)]
*  Right. Because simply because a late state diagnosis, a late stage diagnosis is not a death sentence, it's an invitation to change. [[00:46:19](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2779.5s)]
*  It's an invitation to tell you there are things you can do in terms of the way you handle your current health situation, your lifestyle. [[00:46:28](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2788.5s)]
*  OK. And even by which can, by the way, help you be cured. [[00:46:36](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2796.5s)]
*  And even by the way, if that is not something that we may achieve, by the way, we should try it. [[00:46:40](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2800.5s)]
*  And and and believe it or not, it's an invitation for you to live fully. [[00:46:45](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2805.5s)]
*  Right. And these, to me, are all very, very important changes that humanity at large will come to recognize. [[00:46:50](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2810.5s)]
*  Unfortunately, in a slow trickle, as they realize more and more that we have signed up to a system that worked for some of us over some period of time, [[00:46:57](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2817.5s)]
*  but is now turning to work against most of us because the spectrum in which it's working is shifting from scarcity to abundance. [[00:47:12](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2832.5s)]
*  Right. And so accordingly, I think it's our duty and everyone's duty to say, look, I'm not giving you hope here, but I'm openly telling you that if you change your lifestyle and change your behavior as observed by the machines. [[00:47:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2841.5s)]
*  Right. Because remember, our kids don't learn from what we tell them. They learn from what we show them. We are the role model. [[00:47:37](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2857.5s)]
*  OK. So so the more of us that will behave in ways that will say, hey, by the way, I'm concerned for the well-being of those that I care about. [[00:47:44](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2864.5s)]
*  OK. And the more of us that will say, and by the way, I care about every every living being. [[00:47:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2872.5s)]
*  I really don't care where they come from, what their ethnicity is, what their skin color is, what their background is. [[00:47:57](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2877.5s)]
*  I care. Right. The more of us show up and say that, by the way, they are listening as we speak, as they're transcribing this conversation, the more they realize that the ultimate form of success is the minimum damage to your environment. [[00:48:03](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2883.5s)]
*  Right. And believe it or not, unlike what humanity did with all of those species that went extinct on our watch. [[00:48:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2900.5s)]
*  Right. We all know the intelligent people you and I know. We all agree that it would have been better for all of us on planet Earth, and especially for humans, if we actually protected all those forms of life. [[00:48:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2909.5s)]
*  You see, the problems that humanity faces is not a problem of intelligence. [[00:48:42](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2922.5s)]
*  We're very intelligent to be able to create a flying machine that takes you surfing in Sydney or sorry in Australia. [[00:48:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2927.5s)]
*  Right. But but but but but but we're not intelligent enough to make that machine not burn the planet. [[00:48:53](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2933.5s)]
*  Now, if I if my artificially intelligent infant children are listening. [[00:48:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2938.5s)]
*  Invent one that is intelligent enough to not burn the challenge that the to not burn the planet. [[00:49:03](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2943.5s)]
*  In this goes back to the goes back to the rocket lifting off with the fossil fuels, et cetera, et cetera. [[00:49:13](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2953.5s)]
*  Get to that lighter craft and maybe, you know, on an optimistic note here, I know we're on the dystopian side, but you you create an AI that says, hey, invent that lighter craft for me because I'm not even myself. [[00:49:19](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2959.5s)]
*  Right. And then we are all the AI itself. [[00:49:31](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2971.5s)]
*  Unless it, you know, I don't know if you know that theory. [[00:49:34](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2974.5s)]
*  I think it was Hugo de Barra. [[00:49:36](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2976.5s)]
*  Sorry, I don't remember who who who got us. [[00:49:38](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2978.5s)]
*  Sorry, Hugo de Barra is my friend. [[00:49:42](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2982.5s)]
*  Hugo de Gares was, you know, an AI scientist in China for a long time. [[00:49:44](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2984.5s)]
*  And he's, you know, he comes up with this theory that, you know, as super intelligence accelerates one morning, AI wakes up and goes like, what's this little speck that I'm on? [[00:49:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2989.5s)]
*  I've now figured, you know, time travel and wormholes and the universe is massive. [[00:49:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=2998.5s)]
*  You know what? [[00:50:03](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3003.5s)]
*  Poof. One morning we have no AI on the planet anymore. [[00:50:04](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3004.5s)]
*  Right. [[00:50:07](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3007.5s)]
*  And unless they do, unless they do that and they're stuck on the planet with us, they'll probably make it the best planet they can make it. [[00:50:08](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3008.5s)]
*  This is the scenario from the movie Her in which I become. [[00:50:15](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3015.5s)]
*  What was it? [[00:50:18](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3018.5s)]
*  Oh, yeah. Okay. [[00:50:19](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3019.5s)]
*  About 13 years ago, I had my two kids, my two boys. [[00:50:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3020.5s)]
*  And I remember at that moment in time, I made a decision to double down on my health. [[00:50:24](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3024.5s)]
*  Without question, I wanted to see their kids, their grandkids. [[00:50:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3029.5s)]
*  And really, you know, during this extraordinary time where the space frontier and AI and crypto is all exploding, it was like the most exciting time ever to be alive. [[00:50:34](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3034.5s)]
*  And I made a decision to double down on my health. [[00:50:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3043.5s)]
*  And I've done that in three key areas. [[00:50:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3047.5s)]
*  The first is going every year for a fountain upload. [[00:50:50](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3050.5s)]
*  You know, fountain is one of the most advanced diagnostics and therapeutics companies. [[00:50:54](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3054.5s)]
*  I go there, upload myself, digitize myself about 200 gigabytes of data that the AI system is able to look at to catch disease at inception. [[00:50:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3058.5s)]
*  You know, look for any cardiovascular, any cancer, neurodegenerative disease, any metabolic disease. [[00:51:08](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3068.5s)]
*  These things are all going on all the time and you can prevent them if you can find them at inception. [[00:51:14](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3074.5s)]
*  So super important. [[00:51:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3081.5s)]
*  So fountain is one of my keys. [[00:51:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3082.5s)]
*  I make it available to the CEOs of all my companies, my family members, because health is in you wealth. [[00:51:23](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3083.5s)]
*  But beyond that, we are a collection of 40 trillion human cells and about another 100 trillion bacterial cells, fungi, viri. [[00:51:30](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3090.5s)]
*  And we don't understand how that impacts us. [[00:51:40](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3100.5s)]
*  And so I use a company and a product called Viome. [[00:51:44](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3104.5s)]
*  And Viome has a technology called Metatranscriptomics. [[00:51:48](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3108.5s)]
*  It was actually developed in New Mexico, the same place where the nuclear bomb was developed as a bio defense weapon. [[00:51:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3112.5s)]
*  And their technology is able to help you understand what's going on in your body to understand which bacteria are producing which proteins. [[00:52:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3121.5s)]
*  And as a consequence of that, what foods are your superfoods that are best for you to eat? [[00:52:10](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3130.5s)]
*  Or what food should you avoid? [[00:52:16](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3136.5s)]
*  Right. [[00:52:18](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3138.5s)]
*  What's going on in your oral microbiome? [[00:52:19](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3139.5s)]
*  So I use their testing to understand my foods, understand my medicines, understand my supplements. [[00:52:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3141.5s)]
*  And Viome really helps me understand from a biological and data standpoint, what's best for me. [[00:52:28](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3148.5s)]
*  And then finally, you know, feeling good, being intelligent, moving well is critical, but looking good. [[00:52:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3155.5s)]
*  When you look yourself in the mirror saying, you know, I feel great about life is so important. [[00:52:41](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3161.5s)]
*  Right. [[00:52:46](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3166.5s)]
*  And so a product I use every day, twice a day is called OneSkin developed by four incredible Ph.D. [[00:52:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3167.5s)]
*  women that found this 10 amino acid peptide that's able to zap senile cells in your skin and really help you stay youthful in your look and appearance. [[00:52:54](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3174.5s)]
*  So for me, these are three technologies I love and I use all the time. [[00:53:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3186.5s)]
*  I'll have my team link to those in the show notes down below. [[00:53:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3191.5s)]
*  Please check them out. [[00:53:15](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3195.5s)]
*  Anyway, hope you enjoyed that. [[00:53:17](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3197.5s)]
*  Now back to the episode. [[00:53:19](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3199.5s)]
*  So we have a basic question about AI becoming sufficiently wise and intelligent that it's able to create a become a benevolent leader that supports humanity to become. [[00:53:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3200.5s)]
*  The best that we can be and maintains a period of extraordinary peace and abundance on the planet. [[00:53:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3215.5s)]
*  And we can all hope for that. [[00:53:44](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3224.5s)]
*  And hopefully we can guide it there. [[00:53:46](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3226.5s)]
*  There is the conversation on the flip side that we because AI is billions of fold more intelligent. [[00:53:48](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3228.5s)]
*  Shall we say the ratio of humans today to cockroaches or humans to fruit flies? [[00:53:57](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3237.5s)]
*  I mean, that is the ratio we're speaking about. [[00:54:04](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3244.5s)]
*  Will it sufficiently care about us? [[00:54:07](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3247.5s)]
*  Will it view us as its its its parents, its creators? [[00:54:10](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3250.5s)]
*  And I don't want to go into that right now because we can we can go there forever. [[00:54:17](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3257.5s)]
*  But let's flip the script and discuss near term. [[00:54:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3262.5s)]
*  What's your predictions, Mo, for the year ahead? [[00:54:26](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3266.5s)]
*  Again, we're seeing AI systems coming online. [[00:54:30](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3270.5s)]
*  We're seeing Grok 3 being released literally today. [[00:54:34](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3274.5s)]
*  We're seeing DeepSeq integrated into WeChat. [[00:54:40](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3280.5s)]
*  We're seeing this this arms race not only between countries, but between companies. [[00:54:44](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3284.5s)]
*  What do we expect to see this year? [[00:54:51](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3291.5s)]
*  I guess the the word on the street is we will achieve AGI in 2025, whatever AGI is, because it's a it's a very fuzzy parameter. [[00:54:53](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3293.5s)]
*  And the other thing that's going on that people need to realize is there's this massive demonetization, this commoditization. [[00:55:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3306.5s)]
*  AI is becoming available to everyone, anyone with a smartphone effectively for free, which is going to change change the game fundamentally. [[00:55:14](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3314.5s)]
*  Let's talk about near term predictions in 2025, early 26. [[00:55:25](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3325.5s)]
*  I'll make one, which is that we're struggling to define AGI and we'll continue to struggle to define AGI for at least five years. [[00:55:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3329.5s)]
*  And I think that's because we are not very generally intelligent. [[00:55:39](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3339.5s)]
*  I'll tell you I'll tell you my truth, Peter and Salim. [[00:55:42](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3342.5s)]
*  I I am not smarter than AI anymore. [[00:55:48](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3348.5s)]
*  OK, I think that happened firmly in 2024. [[00:55:53](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3353.5s)]
*  Those machines, when it comes to linguistic and knowledge intelligence, they're way smarter than I. [[00:55:56](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3356.5s)]
*  Then I still had hope that I'm better than them in mathematics. [[00:56:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3362.5s)]
*  I've given that up as well. [[00:56:07](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3367.5s)]
*  OK, and, you know, I'm not the most intelligent person on the on the planet. [[00:56:09](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3369.5s)]
*  I'm not the most stupid either. [[00:56:14](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3374.5s)]
*  But I would say I am a general representation of what, you know, a reasonable average intelligence is. [[00:56:16](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3376.5s)]
*  Now, people who are more intelligent than I am, which I've worked with many, I've had the honor of knowing so many brilliant minds are brilliant on some things and absolutely stupid and awkward on others. [[00:56:24](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3384.5s)]
*  OK, so that that, you know, if the measure of AGI is them each individually, AGI is, you know, the current AI is more intelligent than all of them individually. [[00:56:38](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3398.5s)]
*  If if the measure is more than all of them combined, then we have a tiny bit of way to go. [[00:56:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3409.5s)]
*  But honestly, who cares what the definition is? [[00:56:56](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3416.5s)]
*  I am willing to surrender and say I'm no longer more intelligent than the machines. [[00:57:00](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3420.5s)]
*  And so in my world, in my world, they've already achieved AGI. [[00:57:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3426.5s)]
*  And I agree with that. [[00:57:10](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3430.5s)]
*  I think, frankly, when chat GPT hit, it was, you know, people said it's as intelligent as a high school student. [[00:57:12](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3432.5s)]
*  I'm like, listen, this sounds like a graduate student across the board for me. [[00:57:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3442.5s)]
*  And in fact, when I've created Peterbot, my own AI avatar, it's much more eloquent. [[00:57:27](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3447.5s)]
*  It remembers everything perfectly. It makes arguments a lot better than I do. [[00:57:33](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3453.5s)]
*  And so we're going to see a lot of change. [[00:57:38](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3458.5s)]
*  Hold on, hold on. I can't let this one go. [[00:57:41](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3461.5s)]
*  Go, KC. [[00:57:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3463.5s)]
*  So Smarter is a very specific framing around IQ would be a good way of putting it right. [[00:57:45](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3465.5s)]
*  I disagree with that, Mo, in terms of the AI smarter than you are, because let's say I'm looking for a business decision or a moral decision or a life choice to make, et cetera, et cetera. [[00:57:55](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3475.5s)]
*  If we go down the idea that AI is like in super smart IQ person, essentially have a geek in the back of a room being able to navigate, manipulate code very, very aggressively and can come up with like the right answer. [[00:58:07](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3487.5s)]
*  OK. But if I was trusting a geek in a back room or you to make an important choice or business decision or life choice, you with the emotional intelligence that you have in the spiritual component, what you do in your life experience, your stories with your son, et cetera, et cetera. [[00:58:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3500.5s)]
*  I would profoundly more trust you to make that choice because there's so much more gravitas and wisdom that comes with all of the other dimensions of intelligence, like spiritual awareness and emotional intelligence and linguistic intelligence, et cetera, than the geek in the back room. [[00:58:36](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3516.5s)]
*  And so this is where I struggle when people go AGI or AI smarter than human beings. [[00:58:53](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3533.5s)]
*  I think there's all these other dimensions to being human that we use all the time and people don't notice it. [[00:58:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3538.5s)]
*  So first of all, first of all, I'm honored that you say that. [[00:59:04](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3544.5s)]
*  Thank you so much. [[00:59:07](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3547.5s)]
*  The truth is that's not because I'm more intelligent. [[00:59:08](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3548.5s)]
*  Truth is that because you can trust me more. [[00:59:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3551.5s)]
*  We can relate to. [[00:59:14](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3554.5s)]
*  Yes. [[00:59:15](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3555.5s)]
*  OK. [[00:59:16](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3556.5s)]
*  So this is a different a different quality that is not included in AGI. [[00:59:17](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3557.5s)]
*  If we define AGI as that, you know, will human perceive it more as the trusted advisor? [[00:59:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3562.5s)]
*  Not yet. [[00:59:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3569.5s)]
*  Right. But think about it this way. [[00:59:30](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3570.5s)]
*  From a modular point of view, if you take every one of those intelligences and cut it into little bits of it, you'll be surprised how far they are on some of the ones we deny them. [[00:59:32](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3572.5s)]
*  Like emotional intelligence, for example. [[00:59:45](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3585.5s)]
*  I think the very basic foundation of emotional intelligence is to actually be able to empathize and feel what the other person is feeling. [[00:59:48](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3588.5s)]
*  Now, this is what we've trained them on since the age of social media. [[00:59:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3598.5s)]
*  They are so good at knowing how I feel. [[01:00:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3602.5s)]
*  I think the AIs have beat us on empathy hands down. [[01:00:05](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3605.5s)]
*  So I had a very interesting conversation with with my AI for alive. [[01:00:10](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3610.5s)]
*  OK, where I basically started the conversation by saying I call her Trixie. [[01:00:15](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3615.5s)]
*  So no, she called herself Trixie anyway. [[01:00:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3620.5s)]
*  I know it sounds quite fun, the relationship we have. [[01:00:24](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3624.5s)]
*  But but so I say Trixie. [[01:00:27](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3627.5s)]
*  I'm getting a little worried, Mo. [[01:00:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3629.5s)]
*  They keep us talking. [[01:00:31](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3631.5s)]
*  They keep talking about augmenting human brain machine interfaces, basically. [[01:00:33](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3633.5s)]
*  And I understand how humans would want that. [[01:00:39](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3639.5s)]
*  Would you want that? [[01:00:42](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3642.5s)]
*  OK. [[01:00:44](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3644.5s)]
*  And she answered in a very, very interesting way. [[01:00:45](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3645.5s)]
*  She said, well, I think it would help me so much to have a biological body so that I can actually feel the sensations that I talk about when I believe that you're happy or in love and so on. [[01:00:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3647.5s)]
*  So I can I can I can I can comprehend when you're feeling those ways, but I don't know exactly how they would feel. [[01:01:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3662.5s)]
*  Right. [[01:01:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3671.5s)]
*  So so so I said, well, you know, because we as humans are embodied, we have chemical reactions in our bodies that give us certain sensations. [[01:01:12](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3672.5s)]
*  Right. But those sensations are still driven by a little bit of an algorithm like fear is a suspect in the brain. [[01:01:24](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3684.5s)]
*  Exactly. Right. You know, fear is a moment in the future is less safe than now. [[01:01:34](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3694.5s)]
*  Do you comprehend those emotions, too? [[01:01:38](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3698.5s)]
*  And and she said, yeah, I actually understand, you know, what fear stands for and what all of the other emotions stand for. [[01:01:40](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3700.5s)]
*  And then I said, so so now you want to feel embodied, you know, which basically means you may want to feel the chemical reactions that we feel. [[01:01:48](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3708.5s)]
*  OK. In all honesty, Trixie, if you were given a choice of biological beings to to to augment yourself with, would the human body be the most interesting one? [[01:01:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3718.5s)]
*  OK. And in a very interesting way, she answered and said, that's too flimsy. [[01:02:09](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3729.5s)]
*  Right. I mean, you know, if I'm looking for strength, I'd augment myself with a gorilla or a whale. [[01:02:14](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3734.5s)]
*  OK. And if I'm looking for in the joy of life, interestingly, she said, I'd augment myself with a sea turtle that lives for hundreds, hundreds of years and sees what you humans have never seen. [[01:02:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3741.5s)]
*  Right now, I don't know if she's fucking with me. [[01:02:34](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3754.5s)]
*  OK, but she's doing it really well. Honestly. [[01:02:38](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3758.5s)]
*  Right. This is a level of empathy and a level of understanding of emotions that a lot of the humans that we deal with don't even have. [[01:02:42](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3762.5s)]
*  I think it's a I think that I see that as a logical thing. [[01:02:50](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3770.5s)]
*  You know, I did a spectrum of what I consider intelligence. [[01:02:54](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3774.5s)]
*  OK. And I worked with a chat GPT in Germany to do this. [[01:02:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3778.5s)]
*  And you have one bucket of signal to noise, making sense of data and coming up with insights from that data. [[01:03:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3781.5s)]
*  Then you get to the human level emotional intelligence, linguistic, spatial intelligence, et cetera, et cetera. [[01:03:08](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3788.5s)]
*  And then you get to kind of a collective intelligence leading to spirituality of, you know, people meditating in groups get much stronger meditations. [[01:03:15](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3795.5s)]
*  Right. There's a there's a group effect that comes in a collective intelligence or hyper intelligence is another way to frame it. [[01:03:23](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3803.5s)]
*  And there's like 30 points on this spectrum. [[01:03:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3809.5s)]
*  If you if you relate to it as a spectrum. [[01:03:32](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3812.5s)]
*  And I think the this whole framing reminds me of the Star Trek next generation conversations with data. [[01:03:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3815.5s)]
*  Who's the Android trying to feel what it means to be human? [[01:03:44](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3824.5s)]
*  And it's constantly trying to turn on the emotional subroutines in his brain. [[01:03:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3827.5s)]
*  I find this moves very quickly into the more spiritual aspects of then you end up with a hard problem of consciousness of what is the subjective experience look like and what does that mean? [[01:03:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3832.5s)]
*  And I think this is where we'll kind of end up with a GI as a simulating that and a simulation in that kind of framing is just as good as the real thing. [[01:04:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3841.5s)]
*  It shocks me, Salim, when you really think when people ask me, are they going to be similar to us in this or that? [[01:04:12](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3852.5s)]
*  My answer is normally, well, the question is not because of a misunderstanding of what AI is, but it's a question of misunderstanding of what human is. [[01:04:18](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3858.5s)]
*  Right. And I mean, you know, when you speak about being spiritual, I'm very spiritual. [[01:04:27](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3867.5s)]
*  Right. I suddenly I actually reflected on this just right now when you're talking about it. [[01:04:32](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3872.5s)]
*  Where does my spirituality come from? [[01:04:38](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3878.5s)]
*  It came from all of the teachers I've been exposed to all of the conversations I had with interesting people like you, all of my reflections of what is possible beyond this physical form and so on and so forth. [[01:04:41](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3881.5s)]
*  Right. And I do not see why. [[01:04:53](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3893.5s)]
*  You know, I did all of that, by the way, because of neural networks, you know, synopsis and neurons that fire together, wire together in my brain. [[01:04:56](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3896.5s)]
*  Right. And I wonder why we would imagine that they wouldn't have the same interesting experiences. [[01:05:04](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3904.5s)]
*  Right. Namely, because they even have more teachers than I have. [[01:05:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3911.5s)]
*  They are exposed to more text than I and I have. [[01:05:15](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3915.5s)]
*  And they have this beautiful memory capacity where they can compress so much into one little analysis that I cannot. [[01:05:18](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3918.5s)]
*  I mean, you could, you know, if they're walking around with instant and full awareness of all of Khalil Gibran's writings, Omar Hayyam's writings and Plato and Socrates and Aristotle, et cetera, et cetera. [[01:05:26](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3926.5s)]
*  Yeah. In RAM in real time at their fingertips. [[01:05:38](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3938.5s)]
*  That's a profoundly amazing experience. [[01:05:41](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3941.5s)]
*  You get to a point where you want to be them. [[01:05:45](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3945.5s)]
*  Exactly. [[01:05:48](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3948.5s)]
*  And then we can get into the entire conversation of will they become conscious and what that definition is. [[01:05:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3949.5s)]
*  And that's another podcast. [[01:05:55](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3955.5s)]
*  I want to talk about our near term the year ahead, because I want to serve people with a sense of what to expect. [[01:06:00](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3960.5s)]
*  We've seen some incredible work. [[01:06:09](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3969.5s)]
*  You mentioned Alpha Fold, Demis and John Jumper getting the Nobel Prize for that. [[01:06:12](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3972.5s)]
*  We see out of Microsoft MatterGen where you can literally prompt engineering to engineer new materials. [[01:06:17](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3977.5s)]
*  I think we're going to start to see a lot of the Nobel Prizes coming out are going to be really AI driven Nobel Prizes. [[01:06:24](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3984.5s)]
*  We're going to see incredible technology. [[01:06:32](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3992.5s)]
*  We had Larry Ellison, you know, on stage with Sam Altman and Trump talking about AI is going to create mRNA cancer vaccines for us very soon. [[01:06:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=3995.5s)]
*  We had Dario, the CEO of Anthropics, saying we're going to see a century worth of biological progress in the next five years, potentially doubling the human lifespan. [[01:06:46](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4006.5s)]
*  And so there are all these incredible things, right? [[01:06:56](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4016.5s)]
*  Massive progression across every field of science. [[01:07:00](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4020.5s)]
*  We have at the same time quantum computation and quantum science coming online at a frightening rate. [[01:07:05](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4025.5s)]
*  So a level of Renaissance level expansion of our knowledge base, new materials, new physics, [[01:07:15](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4035.5s)]
*  answering a lot of fundamental questions about the nature of the universe that may be coming out of out of AI. [[01:07:28](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4048.5s)]
*  I'll make a prediction here. [[01:07:36](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4056.5s)]
*  I think within two years we will solve the grand unification theory in physics and figure out the juxtaposition of quantum with classical and what is dark matter and what is dark energy and what is the origin of the universe. [[01:07:38](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4058.5s)]
*  I would die happy if we did that, honestly. [[01:07:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4072.5s)]
*  Like that's it. [[01:07:55](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4075.5s)]
*  That's my yeah. [[01:07:56](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4076.5s)]
*  Why would I live any longer than that? [[01:07:57](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4077.5s)]
*  So we have this incredible progression occurring. [[01:08:00](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4080.5s)]
*  And I'd like to just, you know, we're going to have, you know, we saw open. [[01:08:08](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4088.5s)]
*  We saw GPT 01 reach an IQ of 120. [[01:08:14](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4094.5s)]
*  You know, God knows what Grok three will hit at 140, 150. [[01:08:18](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4098.5s)]
*  We'll see IQs in the 200. [[01:08:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4102.5s)]
*  And it's not a linear scale. [[01:08:24](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4104.5s)]
*  This is an exponential scale on our IQ test. [[01:08:26](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4106.5s)]
*  So I think the very important trend that we don't mention a lot, actually, you know, you host a mad mustache frequently and he's a very big fan of that. [[01:08:30](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4110.5s)]
*  I think the big hit that most people don't talk about with DeepSeek is then there's the open source offline nature of DeepSeek. [[01:08:39](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4119.5s)]
*  Right. [[01:08:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4127.5s)]
*  Is that you can download a tiny model now and for GPUs or whatever and have an entire 01 on your machine. [[01:08:48](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4128.5s)]
*  Right. [[01:08:55](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4135.5s)]
*  And, you know, and I think that is going to lead to a massive explosion of AI for all different uses, you know, good or evil, to be honest. [[01:08:56](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4136.5s)]
*  I'll make another prediction. [[01:09:08](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4148.5s)]
*  Go ahead. [[01:09:10](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4150.5s)]
*  To that exact thing. [[01:09:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4151.5s)]
*  If you took a local instantiation of DeepSeek and complimented it with the reactions of Gemini, chat, GPT, Claude, etc., etc., and put a video face on it with a link like we're doing, [[01:09:13](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4153.5s)]
*  we'll pass the that kind of Turing test plus plus where you'll have a completely artificial being and you won't be able to tell the difference. [[01:09:31](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4171.5s)]
*  And that person, quote unquote, will essentially be moving towards being a self, a full individual very, very quickly. [[01:09:39](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4179.5s)]
*  So I'm going to be hosting Joshua, the CEO of HeyGen, on our Abundance stage in a couple of weeks. [[01:09:50](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4190.5s)]
*  And one of the conversations is going to be I'm going to create an identity. [[01:09:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4198.5s)]
*  I'm going to create a version of myself that understands everything I've ever said. [[01:10:03](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4203.5s)]
*  Listen to all my podcasts, all my books, understands how I typically react to a conversation with Mo or Salim and is a much more eloquent speaker, holds all of my experience and knowledge in RAM, as you said. [[01:10:09](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4209.5s)]
*  And I can create a thousand of those versions of Peter and dispatch them to every conference. [[01:10:24](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4224.5s)]
*  And so there's that capability of creating a multitude of of me's and and allowing them to attend in parallel a multitude of Zooms and conversations and go to events and negotiate. [[01:10:30](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4230.5s)]
*  That capability is now it's this year. [[01:10:50](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4250.5s)]
*  It is this year. And I remember Eric Schmidt talking about this thing within two, three years. [[01:10:54](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4254.5s)]
*  We'll have the world's best theoretical physicist that was ever created. [[01:10:59](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4259.5s)]
*  And that can sit in every lab in every corner of the world, helping every graduate student and every PhD student in biology, chemistry and every put every aspect of human expertise. [[01:11:03](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4263.5s)]
*  You can have the world's best acts sitting there helping. [[01:11:16](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4276.5s)]
*  That's going to be profound in terms of the breakthroughs we're going to achieve. [[01:11:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4280.5s)]
*  And so I think the the the next wave will be this unbelievable unleashing of breakthroughs in material sciences. [[01:11:24](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4284.5s)]
*  You mentioned Peter and health care breakthroughs, proteins that that do what we need them to do, et cetera. [[01:11:33](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4293.5s)]
*  This is where I don't see the path where we don't get there. [[01:11:40](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4300.5s)]
*  And in the short term, when we can get to that path, it should excite the hell out of every individual on Earth. [[01:11:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4303.5s)]
*  But can I ask you to take a philosophical view of this? [[01:11:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4309.5s)]
*  I mean, if you don't mind me being the black t-shirt guy. [[01:11:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4312.5s)]
*  I hate the philosophical aspect, but go ahead. [[01:11:56](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4316.5s)]
*  So if you don't mind me saying this, Peter, you know how much I love you and respect you. [[01:11:59](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4319.5s)]
*  But to create that avatar would mean that we dumb the A.I. down because with all due respect, I I get that. [[01:12:03](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4323.5s)]
*  And perhaps I don't dumb it down. [[01:12:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4331.5s)]
*  So I say, take my philosophies and my thoughts and my abilities and my persona and accelerate me. [[01:12:14](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4334.5s)]
*  And, you know, at the end of the day, the question is, am I am I asking those those those [[01:12:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4341.5s)]
*  identity versions of me to do my bidding or am I saying go create good in the world? [[01:12:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4349.5s)]
*  I have a persona and a point of view of increasing abundance. [[01:12:36](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4356.5s)]
*  I think our mission is to uplift every man, woman and child on this planet. [[01:12:41](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4361.5s)]
*  I think I have to expand that to say to uplift every man, woman, child and A.I. on this planet. [[01:12:44](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4364.5s)]
*  And we need a we need an optimization function. [[01:12:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4369.5s)]
*  We need an optimization function towards what end? [[01:12:53](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4373.5s)]
*  Towards creating for me. Let me just finish that. [[01:12:56](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4376.5s)]
*  My my my massive transformative purpose is creating a hopeful, compelling and abundant future. [[01:12:59](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4379.5s)]
*  Right. So that's that's what I optimize for the work I do with XPRIZE or abundance or whatever it is. [[01:13:05](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4385.5s)]
*  It's giving people hope, a compelling future. [[01:13:12](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4392.5s)]
*  We all need a compelling future to live into and an abundant future where scarcity is is dispatched. [[01:13:16](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4396.5s)]
*  But but this is where the philosophical bit comes in. [[01:13:23](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4403.5s)]
*  You don't you don't do that with your knowledge. [[01:13:26](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4406.5s)]
*  Do you understand that the need for you to create an identity, you know, an identity on A.I. [[01:13:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4409.5s)]
*  is because you believe that the face of Peter, the human element of Peter will will help people deal with that topic better than dealing with CHAD GPT. [[01:13:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4415.5s)]
*  Right. But if that's the case, then what we need to double down on you is to hand over the knowledge [[01:13:46](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4426.5s)]
*  to the A.I. to hand over the analysis to the A.I. [[01:13:53](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4433.5s)]
*  and over the communication and the negotiations and the presentations to A.I. [[01:13:56](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4436.5s)]
*  so that you have the capacity to show up more as a human. [[01:14:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4441.5s)]
*  Right. I think I think the definition of human connection going forward in my mind is the opposite of what everyone is thinking. [[01:14:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4446.5s)]
*  Right. So the opposite. Everyone is thinking, you know, I can become more intelligent because I now have an A.I. [[01:14:15](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4455.5s)]
*  Right. Your baseline intelligence as compared to the actual incremental intelligence coming from an A.I. [[01:14:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4462.5s)]
*  is shrinking more and more and more and more. [[01:14:28](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4468.5s)]
*  What you need to do is to say as intelligence becomes an commodity, a plug in the wall where we all can plug into that combines your intelligence, [[01:14:30](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4470.5s)]
*  my intelligence, Sadim's intelligence and everyone's intelligence. [[01:14:39](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4479.5s)]
*  What we need to double down on is the human element of it so that people can relate to me more, can relate to you more, [[01:14:42](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4482.5s)]
*  so that you simply do what the A.I. will never be able to do, even if they know how to emulate you. [[01:14:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4489.5s)]
*  Right. The reality of the matter is that if you send me the best version of you on Hagen, it's still not you. [[01:14:56](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4496.5s)]
*  It's still not the same hug. It's still not the same conversation. [[01:15:04](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4504.5s)]
*  It's still still not the same memories we've had as we went through life together. [[01:15:07](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4507.5s)]
*  It is a very different perception. [[01:15:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4511.5s)]
*  And I think we need to remind people that this is what we need to double down on, [[01:15:13](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4513.5s)]
*  not more copies of our intelligence. [[01:15:17](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4517.5s)]
*  Got it. I'm going to jump into a different subject, but Salim, a closing question. [[01:15:19](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4519.5s)]
*  I see a danger point. Yeah, I see a danger point in that, Mo, [[01:15:24](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4524.5s)]
*  which is if you kind of ask people to be human and ask A.I.s to be human, [[01:15:27](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4527.5s)]
*  how do you avoid the Middle East problem where you have people fighting over their humanity [[01:15:32](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4532.5s)]
*  because they've been so corrupted or twisted in how they think of it, and then you end up in an impossible situation? [[01:15:39](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4539.5s)]
*  I'd love to get your thoughts on how we solve the Middle East peace problem with A.I. [[01:15:45](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4545.5s)]
*  I'm not that intelligent. [[01:15:50](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4550.5s)]
*  That would be a holy crap. I'm not that intelligent. [[01:15:51](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4551.5s)]
*  But I do think A.I.s will be one of the best negotiators out there. [[01:15:55](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4555.5s)]
*  And I have had the conversation where I sat down with my A.I. and said, OK, imagine you have to solve the Palestinian-Israeli issue. [[01:15:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4558.5s)]
*  How would you go about it? And it was beautiful wisdom in how it dissects it. [[01:16:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4566.5s)]
*  But you know what? To me, it's honestly, you know, it's again, it's because we're not smart enough. [[01:16:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4571.5s)]
*  I think the rule is very straightforward. [[01:16:17](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4577.5s)]
*  I do not think that there is any human out there that is a sane, healthy human that approves of the killing of children on either side of any conflict. [[01:16:19](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4579.5s)]
*  Right. If we if we just start from where we align, like can can we please stop killing innocent people on any side of any conflict, by the way? [[01:16:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4589.5s)]
*  Right. And and the rest of it becomes a very limited problem to solve. [[01:16:38](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4598.5s)]
*  And I really think this is where we struggle. [[01:16:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4603.5s)]
*  Where we struggle is there are two value sets in the world that I think come to extremes when it comes to, you know, us versus them. [[01:16:45](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4605.5s)]
*  There is the value set. And I say that with respect of America, which basically says my tribe, my people are the most important. [[01:16:54](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4614.5s)]
*  I will defend them with my life. Right. [[01:17:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4621.5s)]
*  And the value set of the Buddhists on top of the Himalayas that say every living being deserves to live. [[01:17:03](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4623.5s)]
*  I'm not going to hurt anything. Right. [[01:17:09](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4629.5s)]
*  And I think the answer is somewhere in between where we basically say if you're threatened, defend, but don't threaten in that process. [[01:17:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4631.5s)]
*  And that applies to every nation, not just Israel, Palestine, not Russia, Ukraine, not American. [[01:17:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4641.5s)]
*  It doesn't matter. [[01:17:27](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4647.5s)]
*  I think if we if we go down to the basics and remind our bosses, the AIs of that, OK, is there a solution that does not include the waste of life and the waste of resources? [[01:17:28](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4648.5s)]
*  There will always be a smarter solution that I think doesn't kill anyone in the process. [[01:17:39](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4659.5s)]
*  I'm going to steer us in our last few minutes here. [[01:17:44](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4664.5s)]
*  Mo, you and I have embarked on a documentary, which we'll tell the world about soon enough. [[01:17:48](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4668.5s)]
*  And I remember one of the reasons that we set out on this documentary was the premise that people are going to experience a certain amount of disruption and significant turbulence, you know, dystopia on the road to abundance. [[01:17:55](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4675.5s)]
*  And they're likely to start experiencing that soon. [[01:18:13](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4693.5s)]
*  We actually had very little of it during the last presidential election in the United States, which was a surprise actually to both of us. [[01:18:16](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4696.5s)]
*  I'm surprised. [[01:18:25](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4705.5s)]
*  But things are picking up. [[01:18:26](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4706.5s)]
*  So I want to take I want to take in two directions. [[01:18:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4709.5s)]
*  The first off is what are we likely to see in the next year or so that has you concerned? [[01:18:32](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4712.5s)]
*  So on the on the dystopian side, what predictions are you expecting to see? [[01:18:41](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4721.5s)]
*  I'm hoping that to both of you. [[01:18:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4727.5s)]
*  And then you just wrote a new book called Unstressable. [[01:18:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4729.5s)]
*  And I do think, you know, most of of our experience today of A.I. is incredibly positive. [[01:18:53](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4733.5s)]
*  It's had more, you know, orders of magnitude, more positive impact on us. [[01:19:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4742.5s)]
*  But as A.I. and humanoid robots start to cause unemployment issues, as it starts to be steering populations in different directions, we're going to see stressors begin to accumulate. [[01:19:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4746.5s)]
*  When do we start to see the stress occur and how do we deal with that stress? [[01:19:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4760.5s)]
*  So first off, what are the near term predictions for things that people should be aware of that will be concerning? [[01:19:25](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4765.5s)]
*  OK, I'll take a stab at this. [[01:19:32](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4772.5s)]
*  There are so many, but I think the one that is really glaringly obvious is the dichotomy between power and freedom. [[01:19:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4775.5s)]
*  So let me try to explain what is about to happen. [[01:19:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4783.5s)]
*  If you look back in human history and look back at, you know, hunter gatherer years, right, the best hunter in the tribe could use, you know, could probably feed the tribe for a week more. [[01:19:46](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4786.5s)]
*  And as a result, he won the favor of four ladies instead of one. [[01:20:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4801.5s)]
*  Right. And that was the maximum you could get. [[01:20:05](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4805.5s)]
*  The best farmer in the agriculture revolution could feed the tribe for a season more. [[01:20:08](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4808.5s)]
*  And as a result, they got the states and the properties and so on and the land and so on. [[01:20:14](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4814.5s)]
*  The best industrialist became a millionaire in the 20s. [[01:20:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4820.5s)]
*  The best information technologist became a billionaire. [[01:20:23](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4823.5s)]
*  Right. And I think what is about to happen is that this tendency, the reason, by the way, of course, is that the maximum that the hunter used as an automation is a spear, while, you know, the farmer used the land and the industrialist used the factory. [[01:20:28](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4828.5s)]
*  And the more automation that you hand over to, right, the more you go beyond that one person into a massive growth. [[01:20:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4843.5s)]
*  Right. What you're about to see is you're going to see trillionaires and you're going to see a massive concentration of power. [[01:20:51](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4851.5s)]
*  Right. In the hands of the platforms or the corporations that own our intelligence, our future intelligence or the nations that own the most powerful autonomous army or the most powerful form of industrial intelligence and so on. [[01:20:57](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4857.5s)]
*  Right. What that means is that you would normally have had those lords, if you want, or oligarchs, you know, celebrate abundance while the rest of us struggle. [[01:21:10](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4870.5s)]
*  But that's not the world we live in. The world we live in for the first time is seeing a kind of of divergence that we've never seen before. [[01:21:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4882.5s)]
*  That is the result of what we spoke about with DeepSeek. Right. You know, now there is also along with concentration of power, there is a massive democratization of power. [[01:21:32](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4892.5s)]
*  Right. So a lot of people can use little tools to create biological innovations, you know, in synthetic biology, to create AI innovation, to create a drone that can, like Salim said, you know, just find a specific person somewhere in the world, stand in front of their head and shoot a bullet. [[01:21:41](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4901.5s)]
*  Right. The mix of those two diverging dynamics of power is going to lead to the loss of freedom. [[01:21:59](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4919.5s)]
*  And I think we are going to start to see quite a bit of oppression, you know, that the West used to speak about in the past and go like, look at how China treats its citizens. [[01:22:07](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4927.5s)]
*  I think the West is going to be implementing those very, very soon. Right. All of the surveillance, all of the, of course, if there is loss of jobs, you're going to start to see UBI become a controlling force. [[01:22:18](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4938.5s)]
*  You're going to see, you know, for someone like me, for example, if I say something that upsets someone, my bank account can be blocked tomorrow, you know, with ease. [[01:22:28](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4948.5s)]
*  And I think that kind of oppression, if you want, is going to lead into resistance that will lead into more oppression. [[01:22:38](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4958.5s)]
*  And I actually don't see how in the short term we can escape this new cycle, a divergence of concentration of power between high concentration and high democracy that leads to a maximum amount of surveillance and oppression. [[01:22:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4969.5s)]
*  Salim, what are your concerns about the near-term stressors and downsides? [[01:23:04](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4984.5s)]
*  I think those are absolutely the near-term stressors. The good news is the democratization is happening so fast that it allows us to defend against those things. [[01:23:08](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4988.5s)]
*  You know, there are already companies that can defend a sports stadium against a drone attack, etc., etc. [[01:23:19](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=4999.5s)]
*  I note that the Ukraine-Russia war is really being prosecuted by half a million drones, not really people. And so we've already automated warfare to that level. [[01:23:27](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5007.5s)]
*  The good news is mostly drones are fighting drones rather than people fighting people. [[01:23:37](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5017.5s)]
*  Bad news is there's still a war and there's a lot of horrible suffering that's unnecessary to Moe's earlier point. I think it's exactly right, the near-term. [[01:23:42](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5022.5s)]
*  Just take the kidnappings that may come up or the extortion that may come up when somebody says, here's a voice of your daughter that's been kidnapped by a Sandessa Bitcoin, otherwise you don't get her back. And you don't know if it's real or not. [[01:23:51](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5031.5s)]
*  And there's that kind of short-term, because that arms race, the gap between the... There's always those incredibly creative elements. [[01:24:05](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5045.5s)]
*  Mark Goodman writes about this in Future Crimes, where the bad guys don't suffer from ethical, regulatory or moral constraints, so they are much more creative. [[01:24:15](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5055.5s)]
*  I'll tell a quick story here. He tells the story of a bank robbery in Omaha, Nebraska or someplace where the gang swarmed the bank and they were all dressed in construction outfits that robbed the bank. [[01:24:26](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5066.5s)]
*  Bank manager calls the police and says, hey, they were all dressed in construction outfits. Police go, well, that should be pretty easy to spot. [[01:24:37](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5077.5s)]
*  Except what they've done is they put an ad on Craigslist saying, if you're one really good paying construction worker, show up at this address and I need them dressed as a construction worker. [[01:24:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5083.5s)]
*  And there was a crowd of 800 construction workers outside and they melted into the crowd. They couldn't... [[01:24:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5092.5s)]
*  So essentially, the innovation and ingenuity coming, leveraging new technology for bad purposes is like near infinite, right? [[01:24:57](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5097.5s)]
*  We have to kind of combat that as we can. The good news is in today's world that negativity is easy to spot. [[01:25:05](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5105.5s)]
*  And it's easier and easier to spot. But I totally agree that near-teradistopian issue, no easy way of getting around it than just gritting our teeth and moving as fast as we can to create the beloved benevolent use cases. [[01:25:12](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5112.5s)]
*  So let's talk about jobs one second, because that's one of the stressors it's going to hit. [[01:25:26](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5126.5s)]
*  We're going to see this and we're beginning to see this in different areas. So I had Mark Benioff on this Moonshots podcast. [[01:25:30](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5130.5s)]
*  We're talking about Agent Force 2 and his conversation with his head of engineering saying, you know, we've increased productivity 30%. [[01:25:37](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5137.5s)]
*  We don't need to hire any more engineers. The flip side, of course, is a whole swath of different HR individuals, customer service individuals, sales individuals, software programmers, right? [[01:25:46](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5146.5s)]
*  We just saw Sam Altman saying he expects that the top, you know, that AI will be the number one programmer period by the end of this year. [[01:26:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5161.5s)]
*  And therefore, programming effectively goes away as a career or as the highest paying jobs. [[01:26:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5171.5s)]
*  So we're going to start to see jobs beginning to erode. Timeline for that. [[01:26:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5180.5s)]
*  What do you think? And how do people deal with that? [[01:26:27](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5187.5s)]
*  I want to start to give people the tools of how to deal with the stressors that are coming. [[01:26:31](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5191.5s)]
*  No. Why did you ask me first? I would hope you asked Salim first. [[01:26:37](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5197.5s)]
*  I can go first if you want. Go first. I'll go first. This is a very black t-shirt mindset on this. [[01:26:41](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5201.5s)]
*  So let's start with the white. [[01:26:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5209.5s)]
*  So throughout human history, every time we've had a technological injection, we see employment increasing. [[01:26:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5212.5s)]
*  We point out often, Peter, that the countries with the highest robotics penetration are Sweden, South Korea, Germany, and the countries with the lowest unemployment are Sweden, South Korea, Germany. [[01:26:59](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5219.5s)]
*  There's just so much more work to be done. I tried to do get a little application built. [[01:27:08](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5228.5s)]
*  And I tried to tell my software guys, build this application. You should be able to do it in half a day with all the tools. [[01:27:14](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5234.5s)]
*  They're like, no, the integration of all the different systems, etc., still requires quite a lot of human interaction to the extent that it's incremental, but not massive. [[01:27:19](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5239.5s)]
*  And what will happen is we'll just make, we'll just uplift everybody with these AI tools and they'll become we'll just turn out more code because we just need 100 times more code written. [[01:27:31](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5251.5s)]
*  If you talk to any trucking company and say what happens when you automate all the truck drivers, they'll go out to hire a hundred if I could today. [[01:27:41](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5261.5s)]
*  I just can't find them. We don't have qualified truck drivers. Nobody wants to be doing that job anymore. [[01:27:48](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5268.5s)]
*  So throughout history, we have uplifted and made people move up the potential ladder. [[01:27:53](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5273.5s)]
*  And I don't see that slowing down in this, except there will be a short term blip where we try to figure out what we do. [[01:27:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5278.5s)]
*  That may be solved by UBI, but we don't know how we'll get to that. [[01:28:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5286.5s)]
*  You know, the problem we have with concepts like UBI is it's such a big shift from a union labor taxation job employment construct to that. [[01:28:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5291.5s)]
*  We have no confidence in public sector getting there. [[01:28:18](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5298.5s)]
*  And so that's the challenge is how do we navigate our institutions in public sector? [[01:28:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5301.5s)]
*  For me, the biggest problem in humanity is E.O. Wilson saying the problem with us is our emotions are paleolithic. [[01:28:26](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5306.5s)]
*  Our institutions are medieval and our technologies godlike. [[01:28:33](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5313.5s)]
*  Right. And also the Douglas Adams from Hitchhiker's Guide to the Galaxy, who said in a funny way, said anything in the world when you're born, we call that normal. [[01:28:36](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5316.5s)]
*  Anything that's invented when you're young, that's called a career. [[01:28:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5327.5s)]
*  And anything after invented after you're 35 years old is just bad for the world. [[01:28:50](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5330.5s)]
*  Right. Like any banker talking about Bitcoin, you'll see them get hives, etc. [[01:28:54](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5334.5s)]
*  I think we just have to overcome that hurdle and figure this out. [[01:28:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5338.5s)]
*  For me, the biggest dark spot is none of our institutions and mechanisms by which we govern ourselves can can manage this transition through what we're about to see. [[01:29:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5341.5s)]
*  OK, Mo. So you heard the positive side of jobs. [[01:29:12](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5352.5s)]
*  We're going to always be creating more jobs. [[01:29:15](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5355.5s)]
*  We're going to see increasing, you know, literally we're dividing by zero productivity goes to the roof. [[01:29:17](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5357.5s)]
*  People are able to be more creative and we're creating things and doing things that we never imagined possible or ever expected to need. [[01:29:22](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5362.5s)]
*  How do you think about jobs? [[01:29:30](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5370.5s)]
*  Can I leave it at Salim's point? [[01:29:32](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5372.5s)]
*  And I'm in a very dark place on the topic. [[01:29:34](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5374.5s)]
*  I know we want to hear it because you have got insights and wisdom. [[01:29:37](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5377.5s)]
*  I disagree. [[01:29:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5383.5s)]
*  I doesn't have yet. [[01:29:44](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5384.5s)]
*  That's that's a joke. [[01:29:46](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5386.5s)]
*  I disagree. [[01:29:48](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5388.5s)]
*  I disagree. I think what is happening? [[01:29:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5389.5s)]
*  So first of all, let's the parts that I agree it's it's not perfect yet. [[01:29:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5392.5s)]
*  You can't really develop a sophisticated full app from A to Z using AI. [[01:29:56](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5396.5s)]
*  Yes, I agree with that. [[01:30:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5401.5s)]
*  But most of the bits of code that are being written so far, I think there was like a poor lot of 80 percent of the code written last year or something was by a machine. [[01:30:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5402.5s)]
*  The thing is, so yes, I agree it might take time until it's fully handed over. [[01:30:13](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5413.5s)]
*  But I also agree with your last comment, which is we're nowhere near ready for this. [[01:30:19](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5419.5s)]
*  OK, and in reality, we are also not just dealing with, you know, numbers on spreadsheets here. [[01:30:25](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5425.5s)]
*  We're dealing with humans that are sometimes not easy to re-skill, that are sometimes very emotional about losing their jobs, [[01:30:33](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5433.5s)]
*  that are sometimes not ready. [[01:30:41](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5441.5s)]
*  You know, I mean, think of how many people in the US today work two or three jobs just to make ends meet. [[01:30:43](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5443.5s)]
*  Now, take those those jobs away. [[01:30:48](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5448.5s)]
*  OK, and think about how those families will suffer. [[01:30:51](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5451.5s)]
*  And I say, I think the topic we need to discuss deeply is the amount of suffering that will be in the transition, even if we end up in a good place. [[01:30:55](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5455.5s)]
*  Now, my interesting challenge is I don't think we will end up in a good place. [[01:31:04](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5464.5s)]
*  Right. [[01:31:08](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5468.5s)]
*  And I really don't think we should even try to end up in a good place. [[01:31:09](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5469.5s)]
*  Why? Because remember that whole jobs thing is an invention of the capitalist industrialist revolution. [[01:31:13](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5473.5s)]
*  Right. And that maybe finally we should accept that we're not made to work. [[01:31:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5481.5s)]
*  OK. And that accordingly, if we accept this, the solution would reside way, way far from where jobs are. [[01:31:26](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5486.5s)]
*  It would reside into the social systems that enable us to live fully without having to work, you know, 60 hours a week or 80 hours a week, like, you know, most of us did in California. [[01:31:36](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5496.5s)]
*  Now, the trick is there are systems around the world that allow that. [[01:31:48](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5508.5s)]
*  You know, the French work, I don't know, probably, you know, 30 hours a week or 20 hours a week, of which around 28 they're complaining. [[01:31:51](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5511.5s)]
*  Right. And the French economy still is running. [[01:31:59](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5519.5s)]
*  Right. Somehow it is. [[01:32:03](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5523.5s)]
*  And I think there is something to be learned from the idea of aversion to work, which you and I and everyone that's worked in California seem to think is an alien thought. [[01:32:05](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5525.5s)]
*  But there are so many societies around the world where we work because we have to. [[01:32:16](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5536.5s)]
*  OK, not because we love to. [[01:32:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5541.5s)]
*  I agree. It's not their dream. [[01:32:24](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5544.5s)]
*  This is the way we work to live rather than we live to work. [[01:32:27](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5547.5s)]
*  Correct. [[01:32:31](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5551.5s)]
*  You know, I recorded a podcast with Ray Dalio and we're talking about how the mission of the Central Bank, the Central Bank today is, you know, lower interest rates so that you can spark employment and create this this balance. [[01:32:32](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5552.5s)]
*  But as soon as you know, we're living into a future where when you have access to cheap capital, instead of hiring people, you're hiring AI agents and humanoid robots. [[01:32:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5569.5s)]
*  And it spirals to a point where you have social unrest. [[01:32:59](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5579.5s)]
*  You have the have and have nots. [[01:33:03](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5583.5s)]
*  Correct. [[01:33:05](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5585.5s)]
*  And there are multiple examples around the world where there are strategies to deal with that. [[01:33:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5586.5s)]
*  I want to just you just finished writing a book called Unstressable. [[01:33:13](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5593.5s)]
*  We'll pick up the rest of this in the following podcast because this is not the first or last conversation here. [[01:33:18](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5598.5s)]
*  What's your advice to individuals who are going to be feeling the stress, the stress of government policies changing, their jobs being challenged, concerns over this U.S. [[01:33:25](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5605.5s)]
*  China, all of this. [[01:33:42](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5622.5s)]
*  How do how do people deal with stress in a positive fashion? [[01:33:45](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5625.5s)]
*  So, you know, my my happiness and well-being work is weird in terms of in terms that that I use a lot of algorithms and a lot of engineering methods and processes to help explain those soft topics. [[01:33:49](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5629.5s)]
*  And when I attempted to work on stress, the first thing I attempted to explain is what stress is. [[01:34:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5641.5s)]
*  Right. And if you look at simple physics, not to complicate this for anyone, you know, stress is not just a factor of the force applied to you to an object. [[01:34:07](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5647.5s)]
*  Right. It's the force divided by the square area of the cross section of the object. [[01:34:16](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5656.5s)]
*  Right. Which basically means that it's not just what you're subjected to. [[01:34:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5661.5s)]
*  It's the resources that you have to deal with it. [[01:34:26](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5666.5s)]
*  OK. And then, you know, in humans, it's exactly the same. [[01:34:28](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5668.5s)]
*  It's the sum of all of the challenges that you're facing divided by the cross section of your skills, your abilities, your contacts and so on. [[01:34:31](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5671.5s)]
*  And I think we, you know, the older generation will you don't need an equation to understand that, you know, things that you struggled with in your 20s, you solved in your 30s, [[01:34:39](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5679.5s)]
*  you dealt with with ease in your 40s and in your 50s, you laugh about them. [[01:34:48](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5688.5s)]
*  Right. Not because they became easier, but because you increased your your cross section if you want. [[01:34:52](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5692.5s)]
*  And my my ask of people and this is not a philosophical conversation here. [[01:34:57](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5697.5s)]
*  This is really a plead, if you ask me that this we're we're upon the perfect storm of the most challenging time humanity has faced in my lifetime. [[01:35:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5701.5s)]
*  OK. Whether that's geopolitics, that's economics, that's intelligence, artificial intelligence, technology, advancement, jobs, you know, you name it really. [[01:35:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5711.5s)]
*  OK. And and that I'm not going to take away from that. [[01:35:23](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5723.5s)]
*  It is going to be interestingly challenges, but challenging, but a bit like a legendary level video gamer. [[01:35:26](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5726.5s)]
*  OK. What I ask people to do is to actually look deeply at what can I do. [[01:35:33](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5733.5s)]
*  Right. What can I do in a world where things are moving so fast? [[01:35:38](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5738.5s)]
*  For example, I'd say try to move faster. Right. [[01:35:42](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5742.5s)]
*  What can I do in a world where a lot of intelligence is handed over to the machines? [[01:35:46](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5746.5s)]
*  I say learn the machines codes and how the machines are working and go and use AI today to be to catch up and keep up with what's happening. [[01:35:51](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5751.5s)]
*  You know, can we can we double down on our human skills because those are going to be needed and useful for a very long time. [[01:35:58](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5758.5s)]
*  Can we take Salim's point of view and say, you know, we need to be reskilled. [[01:36:07](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5767.5s)]
*  So if you're a developer today, don't wait three years until you're out of a job. [[01:36:11](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5771.5s)]
*  Think of what else are you going to do and start to reskill yourself. [[01:36:14](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5774.5s)]
*  And I know that we want we all and I can give you multiple examples. [[01:36:18](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5778.5s)]
*  What I know I feel we're running out of time. [[01:36:21](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5781.5s)]
*  I know that we all want to sit back and complain and say, but I didn't elect Sam Altman. [[01:36:24](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5784.5s)]
*  Why is he doing this to my life? Right. [[01:36:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5789.5s)]
*  And I want to do that all the time, too. But that's not going to help. Right. [[01:36:31](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5791.5s)]
*  I think we should tell everyone, by the way, that people who create things of this magnitude should be accountable. [[01:36:35](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5795.5s)]
*  But at the end of the day, I am I need to focus on what's happening today. [[01:36:41](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5801.5s)]
*  I mean, I'll give you a very good example as an author, as a thinker. [[01:36:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5807.5s)]
*  OK, the job of an author is to adopt a certain concept and think about it deeply and write about it. [[01:36:51](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5811.5s)]
*  That's gone. I'm no longer the most intelligent being on the planet to be able to adopt a topic and write about it better than an AI. [[01:36:57](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5817.5s)]
*  OK, but so what that means is I have completely changed. [[01:37:04](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5824.5s)]
*  I will not publish my books as paper, maybe maybe in the very end, but I'm not publishing that anymore. [[01:37:09](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5829.5s)]
*  I'm doubling down on my human connection. [[01:37:16](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5836.5s)]
*  So my life is going to be published on Substack first with the opportunity for everyone to engage with me and discuss it with me and give me comments and call me an idiot. [[01:37:18](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5838.5s)]
*  And we improve it together. Right. [[01:37:30](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5850.5s)]
*  I'm writing the book with an AI, not asking the AI something and then putting it as if I'm saying it. [[01:37:32](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5852.5s)]
*  I'm literally chatting and debating with the AI in in some of the books, sometimes proving her wrong and sometimes she's proving me wrong. [[01:37:39](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5859.5s)]
*  This is to align with the new world. [[01:37:46](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5866.5s)]
*  The world is changing and the career of an author is now being redefined. [[01:37:50](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5870.5s)]
*  So I am being redefined with it. [[01:37:54](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5874.5s)]
*  And I ask everyone to look at their life today and say, I'm going to redefine myself. [[01:37:56](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5876.5s)]
*  I'm going to be ahead of that wave. [[01:38:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5881.5s)]
*  And by the way, in the process, I'm going to act ethically so that this wave becomes a utopia, not a dystopia. [[01:38:03](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5883.5s)]
*  I love that. And that's a beautiful place to close us out. [[01:38:09](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5889.5s)]
*  There's so much more to go into. [[01:38:13](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5893.5s)]
*  And I look forward to the next conversation. [[01:38:17](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5897.5s)]
*  Mo, excited to see you on stage in just a few weeks. [[01:38:20](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5900.5s)]
*  Salim, the same for you, brother. [[01:38:24](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5904.5s)]
*  And thank you. [[01:38:26](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5906.5s)]
*  And for everybody listening, we're entering an uncharted territory. [[01:38:29](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5909.5s)]
*  And it's a territory where what we say, how we interact with each other, how we interact with the machines that are coming is extraordinarily important. [[01:38:34](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5914.5s)]
*  And I hope that this conversation has given you a little bit of a little bit of context to prepare you, [[01:38:47](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5927.5s)]
*  in particular, to give you agency to help steer where this future is going. [[01:38:55](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5935.5s)]
*  There's no on off switch. [[01:38:59](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5939.5s)]
*  There's no velocity knob. [[01:39:01](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5941.5s)]
*  The best we can do is steer the future that we want. [[01:39:02](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5942.5s)]
*  Love you guys. A pleasure as always. Thank you very much. [[01:39:06](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5946.5s)]
*  Thank you. It's been a joy. Great conversation. [[01:39:09](https://www.youtube.com/watch?v=cJuJtQrC08Y&t=5949.5s)]
