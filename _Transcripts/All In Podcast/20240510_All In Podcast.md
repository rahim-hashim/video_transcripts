---
Date Generated: January 11, 2025
Transcription Model: whisper medium 20231117
Length: 6182s
Video Keywords: ['chamath', 'david sacks', 'david friedberg', 'jason calacanis', 'all in podcast', 'tech', 'news', 'politics', 'big tech', 'antitrust', 'election', 'covid', 'quarantine', 'stocks', 'stock market', 'tech stocks', 'palihapitiya', 'government']
Video Views: 440125
Video Rating: None
Video Description: (0:00) Welcoming Sam Altman to the show!
(2:28) What's next for OpenAI: GPT-5, open-source, reasoning, what an AI-powered iPhone competitor could look like, and more
(21:56) How advanced agents will change the way we interface with apps
(33:01) Fair use, creator rights, why OpenAI has stayed away from the music industry
(42:02) AI regulation, UBI in a post-AI world
(52:23) Sam breaks down how he was fired and re-hired, why he has no equity, dealmaking on behalf of OpenAI, and how he organizes the company
(1:05:33) Post-interview recap
(1:10:38) All-In Summit announcements, college protests
(1:19:06) Signs of innovation dying at Apple: iPad ad, Buffett sells 100M+ shares, what's next?
(1:29:41) Google unveils AlphaFold 3.0


Follow Sam:
https://twitter.com/sama

Follow the besties: 
https://twitter.com/chamath
https://twitter.com/Jason
https://twitter.com/DavidSacks
https://twitter.com/friedberg

Follow on X:
https://twitter.com/theallinpod

Follow on Instagram:
https://www.instagram.com/theallinpod

Follow on TikTok:
https://www.tiktok.com/@theallinpod

Follow on LinkedIn: 
https://www.linkedin.com/company/allinpod

Intro Music Credit:
https://rb.gy/tppkzl
https://twitter.com/yung_spielburg

Intro Video Credit:
https://twitter.com/TheZachEffect

Referenced in the show:
https://twitter.com/EconomyApp/status/1622029832099082241
https://sacra.com/c/openai
https://twitter.com/tim_cook/status/1787864325258162239
https://openai.com/index/introducing-the-model-spec
https://twitter.com/SabriSun_Miller/status/1788298123434938738
https://www.archives.gov/founding-docs/bill-of-rights-transcript
https://twitter.com/ClayTravis/status/1788312545754825091
https://www.inc.com/bill-murphy-jr/warren-buffett-just-sold-more-than-100-million-shares-of-apple-reason-why-is-eye-opening.html
https://www.youtube.com/watch?v=snbTCWL6rxo
https://www.digitimes.com/news/a20240506PD216/apple-ev-startup-genai.html
https://www.theonion.com/fuck-everything-were-doing-five-blades-1819584036
https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model

#allin #tech #news
---

# In conversation with Sam Altman
**All In Podcast:** [May 10, 2024](https://www.youtube.com/watch?v=nSM0xd8xHUM)
*  I first met our next guest, Sam Altman, almost 20 years ago when he was working on a local [[00:00:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=0.0s)]
*  mobile app called Looped. [[00:00:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5.12s)]
*  We were both backed by Sequoia Capital and, in fact, we were both in the first class of [[00:00:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6.96s)]
*  Sequoia Scouts. [[00:00:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=12.040000000000001s)]
*  He did investment in a little unknown fintech company called Stripe. [[00:00:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=13.32s)]
*  I did Uber. [[00:00:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=16.48s)]
*  And in that tiny experiment... [[00:00:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=17.88s)]
*  You did Uber? [[00:00:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=19.240000000000002s)]
*  I've never heard that before. [[00:00:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=20.240000000000002s)]
*  Yeah, I think so. [[00:00:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=21.240000000000002s)]
*  It's possible. [[00:00:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=22.240000000000002s)]
*  It's starting already. [[00:00:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=23.240000000000002s)]
*  You should write a book, Jacob. [[00:00:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=24.240000000000002s)]
*  Maybe. [[00:00:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=25.240000000000002s)]
*  That tiny experimental fund that Sam and I were part of as Scouts is Sequoia's highest [[00:00:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=26.24s)]
*  multiple returning fund. [[00:00:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=49.599999999999994s)]
*  A couple of low digit millions turned into over 200 million, I'm told. [[00:00:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=51.08s)]
*  Really? [[00:00:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=54.480000000000004s)]
*  Yeah, that's what I was talking about. [[00:00:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=55.72s)]
*  Rulof, yeah. [[00:00:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=56.72s)]
*  And he did a stint at Y Combinator, where he was president from 2014 to 2019. [[00:00:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=57.72s)]
*  In 2016, he co-founded OpenAI with the goal of ensuring that artificial general intelligence [[00:01:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=61.8s)]
*  benefits all of humanity. [[00:01:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=67.64s)]
*  In 2019, he left YC to join OpenAI full-time as CEO. [[00:01:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=69.6s)]
*  Things got really interesting on November 30th of 2022. [[00:01:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=73.75999999999999s)]
*  That's the day OpenAI launched ChatGPT. [[00:01:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=77.75999999999999s)]
*  In January 2023, Microsoft invested 10 billion. [[00:01:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=81.04s)]
*  In November 2023, over a crazy five-day span, Sam was fired from OpenAI. [[00:01:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=84.08000000000001s)]
*  Everybody was going to go work at Microsoft. [[00:01:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=90.24000000000001s)]
*  A bunch of heart emojis went viral on X slash Twitter, and people started speculating that [[00:01:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=91.80000000000001s)]
*  the team had reached artificial general intelligence. [[00:01:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=97.2s)]
*  The world was going to end, and suddenly, a couple days later, he was back to being [[00:01:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=100.44000000000001s)]
*  the CEO of OpenAI. [[00:01:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=104.72s)]
*  In February, Sam was reportedly looking to raise $7 trillion for an AI chip project. [[00:01:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=107.4s)]
*  This after it was reported that Sam was looking to raise a billion from Masayoshi San to create [[00:01:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=112.52000000000001s)]
*  an iPhone killer with Johnny Ive, the co-creator of the iPhone. [[00:01:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=117.12s)]
*  All of this while ChatGPT has become better and better and a household name. [[00:02:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=120.80000000000001s)]
*  It's having a massive impact on how we work and how work is getting done, and it's reportedly [[00:02:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=125.08000000000001s)]
*  the fastest product to hit 100 million users in history in just two months. [[00:02:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=130.78s)]
*  Check out OpenAI's insane revenue wrap-up. [[00:02:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=135.95999999999998s)]
*  They reportedly hit $2 billion in ARR last year. [[00:02:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=138.64s)]
*  Welcome to the All In Podcast, Sam Ullman. [[00:02:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=142.16s)]
*  Thank you. [[00:02:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=144.95999999999998s)]
*  Thank you, guys. [[00:02:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=145.95999999999998s)]
*  Sachs, you want to lead us off here? [[00:02:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=146.95999999999998s)]
*  Okay, sure. [[00:02:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=147.95999999999998s)]
*  I mean, I think the whole industry is waiting with bated breath for the release of GPT-5. [[00:02:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=148.95999999999998s)]
*  I guess it's been reported that it's launching sometime this summer, but that's a pretty [[00:02:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=154.23999999999998s)]
*  big window. [[00:02:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=158.0s)]
*  Can you narrow that down? [[00:02:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=159.0s)]
*  I guess where are you in the release of GPT-5? [[00:02:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=160.67999999999998s)]
*  We take our time on releases of major new models, and I don't think we... [[00:02:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=164.64s)]
*  I think it will be great when we do it, and I think we'll be thoughtful about how we do [[00:02:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=171.76s)]
*  it. [[00:02:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=177.16s)]
*  We may release it in a different way than we've released previous models. [[00:02:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=178.16s)]
*  Also, I don't even know if we'll call it GPT-5. [[00:03:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=181.04s)]
*  What I will say is a lot of people have noticed how much better GPT-4 has gotten since we've [[00:03:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=185.48s)]
*  released it, and particularly over the last few months. [[00:03:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=191.35999999999999s)]
*  I think that's a better hint of what the world looks like, where it's not the one, [[00:03:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=194.0s)]
*  two, three, four, five, six, seven, but you use an AI system, and the whole system just [[00:03:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=201.72s)]
*  gets better and better fairly continuously. [[00:03:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=208.04s)]
*  I think that's both a better technological direction. [[00:03:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=212.4s)]
*  I think that's easier for society to adapt to, but I assume that's where we'll head. [[00:03:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=215.24s)]
*  Does that mean that there's not going to be long training cycles and it's continuously [[00:03:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=221.72s)]
*  retraining or training sub-models, Sam? [[00:03:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=225.82s)]
*  Maybe you could just speak to us about what might change architecturally going forward [[00:03:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=229.56s)]
*  with respect to large models. [[00:03:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=233.32s)]
*  Well, one thing that you could imagine is just that you keep training a model. [[00:03:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=235.6s)]
*  That would seem like a reasonable thing to me. [[00:04:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=242.96s)]
*  Do you think that... [[00:04:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=245.0s)]
*  And when you talk about releasing it differently this time, are you thinking maybe releasing [[00:04:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=246.48s)]
*  it to the paid users first or a slower rollout to get the red teams tight since now there's [[00:04:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=251.06s)]
*  so much at stake? [[00:04:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=258.06s)]
*  You have so many customers actually paying and you've got everybody watching everything [[00:04:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=259.18s)]
*  you do. [[00:04:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=263.2s)]
*  You have to be more thoughtful now, yeah? [[00:04:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=264.2s)]
*  Well, GPT-4 is still only available to the paid users, but one of the things that we [[00:04:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=266.9s)]
*  really want to do is figure out how to make more advanced technology available to free [[00:04:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=270.98s)]
*  users too. [[00:04:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=276.18s)]
*  I think that's a super important part of our mission and this idea that we build AI tools [[00:04:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=277.3s)]
*  and make them super widely available, free or not that expensive, whatever it is, so [[00:04:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=283.82s)]
*  that people can use them to go kind of invent the future rather than the magic AGI in the [[00:04:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=289.34000000000003s)]
*  sky inventing the future and showing it down upon us. [[00:04:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=293.82s)]
*  That seems like a much better path. [[00:04:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=298.1s)]
*  It seems like a more inspiring path. [[00:04:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=299.22s)]
*  I also think it's where things are actually heading. [[00:05:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=300.22s)]
*  So it makes me sad that we have not figured out how to make GPT-4 level technology available [[00:05:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=302.66s)]
*  to free users. [[00:05:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=308.06s)]
*  It's something we really want to do. [[00:05:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=309.06s)]
*  It's just very expensive. [[00:05:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=310.66s)]
*  I take it. [[00:05:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=311.66s)]
*  It's very expensive. [[00:05:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=312.66s)]
*  Yeah. [[00:05:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=313.66s)]
*  Chema, your thoughts? [[00:05:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=314.66s)]
*  I think maybe the two big vectors, Sam, that people always talk about is that underlying [[00:05:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=315.66s)]
*  cost and sort of the latency that's kind of rate limited, a killer app. [[00:05:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=321.32s)]
*  And then I think the second is sort of the long-term ability for people to build in an [[00:05:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=326.56s)]
*  open source world versus a closed source world. [[00:05:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=333.28000000000003s)]
*  And I think the crazy thing about this space is that the open source community is rabid. [[00:05:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=335.66s)]
*  So one example that I think is incredible is we had these guys do a pretty crazy demo [[00:05:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=342.08s)]
*  for Devin, remember, like even like five or six weeks ago that looked incredible. [[00:05:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=347.7s)]
*  And then some kid just published it under an open MIT license like OpenDevin. [[00:05:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=352.76s)]
*  And it's incredibly good and almost as good as that other thing that was closed source. [[00:05:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=357.78s)]
*  So maybe we can just start with that, which is tell me about the business decision to [[00:06:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=364.82s)]
*  keep these models closed source. [[00:06:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=369.5s)]
*  And where do you see things going in the next couple of years? [[00:06:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=371.46s)]
*  So on the first part of your question, speed and cost, those are hugely important to us. [[00:06:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=375.84s)]
*  And I don't want to like give a timeline on when we can bring them down a lot because [[00:06:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=383.16s)]
*  research is hard, but I am confident we'll be able to. [[00:06:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=387.4s)]
*  We want to like cut the latency super dramatically. [[00:06:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=391.08s)]
*  We want to cut the cost really, really dramatically. [[00:06:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=393.6s)]
*  And I believe that will happen. [[00:06:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=397.28s)]
*  We're still so early in the development of the science and understanding how this works. [[00:06:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=398.56s)]
*  Plus, we have all the engineering tailwinds. [[00:06:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=403.76s)]
*  So I don't know like when we get to intelligence too cheap to meter and so fast that it feels [[00:06:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=405.94s)]
*  instantaneous to us and everything else. [[00:06:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=412.9s)]
*  But I do believe we can get there for a pretty high level of intelligence. [[00:06:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=414.86s)]
*  It's important to us. [[00:07:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=424.2s)]
*  It's clearly important to users and it'll unlock a lot of stuff. [[00:07:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=425.4s)]
*  On the sort of open source, closed source thing, I think there's great roles for both. [[00:07:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=428.76s)]
*  I think, you know, we've open sourced some stuff, we'll open source more stuff in the [[00:07:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=432.8s)]
*  future. [[00:07:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=438.12s)]
*  But really, like our mission is to build towards AGI and to figure out how to broadly distribute [[00:07:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=439.12s)]
*  its benefits. [[00:07:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=444.26s)]
*  We have a strategy for that. [[00:07:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=445.46000000000004s)]
*  Seems to be resonating with a lot of people. [[00:07:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=447.34000000000003s)]
*  It obviously isn't for everyone. [[00:07:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=449.2s)]
*  And there's like a big ecosystem and there will also be open source models and people [[00:07:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=450.54s)]
*  who build that way. [[00:07:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=454.48s)]
*  One area that I'm particularly interested personally in open source for is I want an [[00:07:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=456.58000000000004s)]
*  open source model that is as good as it can be that runs on my phone. [[00:07:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=461.38s)]
*  And that I think is going to, you know, the world doesn't quite have the technology for [[00:07:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=466.0s)]
*  a good version of that yet. [[00:07:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=471.26s)]
*  But that seems like a really important thing to go do at some point. [[00:07:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=472.9s)]
*  Will you do that? [[00:07:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=475.74s)]
*  Will you release it? [[00:07:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=476.74s)]
*  I don't know if we will or someone will. [[00:07:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=477.74s)]
*  What about llama three? [[00:07:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=478.74s)]
*  Llama three running on a phone? [[00:07:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=479.74s)]
*  Well, I guess maybe there's like a seven billion. [[00:08:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=480.74s)]
*  Yeah, yeah. [[00:08:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=485.74s)]
*  I don't know if that's if that will fit on a phone or not. [[00:08:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=488.1s)]
*  That should be fitable on a phone. [[00:08:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=491.22s)]
*  But I don't I'm not sure if that one's like I haven't played with it. [[00:08:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=493.78000000000003s)]
*  I don't know if it's like good enough to kind of do the thing I'm thinking about here. [[00:08:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=496.86s)]
*  So when llama three got released, I think the big takeaway for a lot of people was, [[00:08:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=500.34000000000003s)]
*  oh, wow, they've like caught up to GPT four. [[00:08:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=504.38000000000005s)]
*  I don't think it's equal in all dimensions, but it's like pretty, pretty close or pretty [[00:08:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=506.5s)]
*  in the ballpark. [[00:08:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=511.3s)]
*  I guess the question is, you know, you guys released for a while ago, you're working on [[00:08:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=512.3000000000001s)]
*  five or, you know, more upgrades to four. [[00:08:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=517.26s)]
*  I mean, I think to Jamal's point about Devon, how do you stay ahead of open source? [[00:08:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=520.46s)]
*  I mean, it's just that's like a very hard thing to do in general, right? [[00:08:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=524.7s)]
*  I mean, how do you think about that? [[00:08:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=528.26s)]
*  What we're trying to do is not make the sort of smartest set of weights that we can, but [[00:08:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=531.6600000000001s)]
*  we're trying to make is like this useful intelligence layer for people to use. [[00:09:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=540.1s)]
*  And a model is part of that. [[00:09:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=545.7s)]
*  I think we will stay pretty far ahead of, I hope, we'll stay pretty far ahead of the [[00:09:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=547.1400000000001s)]
*  rest of the world on that. [[00:09:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=552.0200000000001s)]
*  But there's a lot of other work around the whole system that's not just that, you know, [[00:09:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=554.46s)]
*  the model weights and we'll have to build up enduring value the old fashioned way like [[00:09:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=561.1s)]
*  any other business does. [[00:09:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=565.74s)]
*  We'll have to figure out a great product and reasons to stick with it and, you know, deliver [[00:09:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=566.74s)]
*  it at a great price. [[00:09:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=571.34s)]
*  When you founded the organization, you the stated goal or part of what you discussed [[00:09:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=572.42s)]
*  was, hey, this is too important for any one company to own it. [[00:09:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=578.42s)]
*  So it definitely needs to be open. [[00:09:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=581.5799999999999s)]
*  Then there was the switch hates too dangerous for anybody to be able to see it. [[00:09:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=583.2199999999999s)]
*  And we need to lock this down because you had some fear about that. [[00:09:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=588.24s)]
*  I think is that accurate? [[00:09:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=591.18s)]
*  Because the cynical side is like, well, this is a capitalistic move. [[00:09:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=593.42s)]
*  And then the I think, you know, I'm curious what the decision was here in terms of going [[00:09:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=596.84s)]
*  from open. [[00:10:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=603.5600000000001s)]
*  The world needs to see this. [[00:10:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=605.2800000000001s)]
*  It's really important to close. [[00:10:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=606.52s)]
*  Only we can see it. [[00:10:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=607.9200000000001s)]
*  Well, how did you come to that conclusion? [[00:10:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=609.2800000000001s)]
*  What were the reasons that we released chat GPT was we want the world to see this and [[00:10:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=611.9200000000001s)]
*  we've been trying to tell people that AI is really important. [[00:10:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=616.0s)]
*  And if you go back to like October of 2022, not that many people thought I was going to [[00:10:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=618.52s)]
*  be that important or that it was really happening. [[00:10:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=623.4000000000001s)]
*  A huge part of what we try to do is put the technology in the hands of people. [[00:10:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=626.96s)]
*  Now, again, there's different ways to do that. [[00:10:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=633.64s)]
*  And I think there really is an important role to just say, like, here's the weights have [[00:10:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=635.4s)]
*  added. [[00:10:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=638.76s)]
*  But the fact that we have so many people using a free version of chat GPT that we don't, [[00:10:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=639.76s)]
*  you know, we don't run ads on, we don't try to like make money on, we just put out there [[00:10:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=645.0600000000001s)]
*  because we want people to have these tools, I think has done a lot to provide a lot of [[00:10:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=647.94s)]
*  value and teach people how to fish, but also to get the world really thoughtful about what's [[00:10:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=653.4s)]
*  happening here. Now, we still don't have all the answers and we're fumbling our way through [[00:10:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=659.6s)]
*  this like everybody else. [[00:11:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=664.0799999999999s)]
*  And I assume we'll change strategy many more times as we learn new things. [[00:11:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=665.1999999999999s)]
*  You know, when we started OpenAI, we had really no idea about how things were going to go, [[00:11:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=669.4s)]
*  that we'd make a language model, that we'd ever make a product. [[00:11:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=675.4s)]
*  We started off just I remember very clearly that first day where we're like, well, now [[00:11:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=677.88s)]
*  we're all here. That was, you know, it's difficult to get this set up. [[00:11:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=682.8s)]
*  But what happens now? Maybe we should write some papers. [[00:11:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=685.3599999999999s)]
*  Maybe we should stand around a whiteboard. [[00:11:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=687.8s)]
*  And we've just been trying to like put one foot in front of the other and figure out what's next [[00:11:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=689.64s)]
*  and what's next and what's next. [[00:11:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=694.0s)]
*  And I think we'll keep doing that. [[00:11:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=695.7199999999999s)]
*  Can I just replay something and just make sure I heard it right? [[00:11:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=698.92s)]
*  I think what you were saying on the open source, closed source thing is if I heard it right, all [[00:11:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=701.1999999999999s)]
*  these models, independent of the business decision you make, are going to become asymptotically [[00:11:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=708.28s)]
*  accurate towards some amount of accuracy, like not all, but like let's just say there's four or [[00:11:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=713.3199999999999s)]
*  five that are well capitalized enough. [[00:11:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=718.6s)]
*  You guys, Meta, Google, Microsoft, whomever. [[00:12:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=722.12s)]
*  Right. So let's just say four or five, maybe one startup and on the open web. [[00:12:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=725.16s)]
*  And then quickly. [[00:12:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=730.64s)]
*  The accuracy or the value of these models will probably shift to these proprietary sources [[00:12:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=733.28s)]
*  of training data that you could get that others can't or others can get that you can't. [[00:12:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=738.24s)]
*  Is that how you see this thing evolving where the open web gets everybody to a certain [[00:12:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=742.5600000000001s)]
*  threshold and then it's just an arms race for data beyond that? [[00:12:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=748.36s)]
*  Doesn't. So I definitely don't think it'll be an arms race for data, because when the models get [[00:12:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=752.52s)]
*  smart enough at some point, it shouldn't be about more data, at least not for training. [[00:12:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=756.76s)]
*  It may matter data to make it useful. [[00:12:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=760.88s)]
*  Look, the one thing that I have learned most throughout all of this is that it's hard to make [[00:12:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=763.32s)]
*  confident statements a couple of years in the future about where this is all going to go. [[00:12:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=770.4s)]
*  And so I don't want to try now. [[00:12:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=774.04s)]
*  I will say that I expect lots of very capable models in the world. [[00:12:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=775.4s)]
*  And, you know, like it feels to me like we just like stumbled on a new fact of nature or [[00:13:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=780.76s)]
*  science or whatever you want to call it, which is like we can create. [[00:13:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=787.36s)]
*  You can like, I mean, I don't believe this literally, but it's like a spiritual point. [[00:13:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=792.28s)]
*  No intelligence is just this emergent property of matter. [[00:13:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=797.92s)]
*  And that's like a that's like a rule of physics or something. [[00:13:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=800.4s)]
*  So people are going to figure that out. [[00:13:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=804.48s)]
*  But there will be all these different ways to design the systems. [[00:13:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=805.44s)]
*  People will make different choices, figure out new ideas. [[00:13:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=808.08s)]
*  And I'm sure like, you know, like any other industry, I would expect there to be multiple [[00:13:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=811.24s)]
*  approaches and different people like different ones. [[00:13:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=821.0s)]
*  You know, some people like iPhones, some people like an Android phone. [[00:13:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=823.2s)]
*  I think there'll be some effect like that. [[00:13:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=826.2s)]
*  Let's go back to that first section of just the cost and the speed. [[00:13:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=828.0s)]
*  All of you guys are sort of a little bit rate limited on literally Nvidia's throughput. [[00:13:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=834.28s)]
*  Right. [[00:13:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=839.4s)]
*  And I think that you and most everybody else have sort of effectively announced how much [[00:13:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=839.68s)]
*  capacity you can get just because it's as much as they can spin out. [[00:14:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=844.4799999999999s)]
*  What needs to happen at the substrate so that you can actually compute cheaper, compute [[00:14:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=848.2399999999999s)]
*  faster, get access to more energy? [[00:14:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=853.76s)]
*  How are you helping to frame out the industry solving those problems? [[00:14:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=856.52s)]
*  Well, we'll make huge algorithmic gains for sure. [[00:14:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=861.4s)]
*  And I don't want to discount that. [[00:14:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=864.0s)]
*  I'm very interested in chips and energy, but if we can make our if we can make a same [[00:14:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=865.4s)]
*  quality model twice as efficient, that's like we had twice as much compute. [[00:14:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=870.96s)]
*  And I think there's a gigantic amount of work to be done there. [[00:14:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=875.04s)]
*  And I hope we'll start really seeing those results. [[00:14:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=881.44s)]
*  Other than that, the whole supply chain is like very complicated. [[00:14:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=886.16s)]
*  You know, there's there's logic, fab capacity. [[00:14:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=888.64s)]
*  There's how much HBM the world can make. [[00:14:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=892.04s)]
*  There's how quickly you can get permits and pour the concrete, make the data centers and [[00:14:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=893.8399999999999s)]
*  then have people in there, wiring them all up. [[00:14:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=897.52s)]
*  There's finding the energy, which is a huge bottleneck. [[00:14:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=899.4s)]
*  But I think when there's this much value to people, the world will do its thing. [[00:15:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=902.04s)]
*  We'll try to help it happen faster. [[00:15:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=908.8s)]
*  And there's probably like I don't know how to give it a number, but there's like some [[00:15:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=911.88s)]
*  percentage chance where there is, as you were saying, like a huge substrate breakthrough. [[00:15:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=915.8s)]
*  And we have like a massively more efficient way to do computing. [[00:15:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=921.2s)]
*  But I don't I don't like bank on that or spend too much time thinking about it. [[00:15:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=924.12s)]
*  What about the device side and sort of you mentioned sort of the models that can fit [[00:15:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=928.5600000000001s)]
*  on a phone. So obviously, whether that's an LLM or some SLM or something, I'm sure [[00:15:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=934.4s)]
*  you're thinking about that. But then does the device itself change? [[00:15:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=938.6s)]
*  I mean, is it does it need to be as expensive as an iPhone? [[00:15:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=941.44s)]
*  I'm super interested in this. [[00:15:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=948.1600000000001s)]
*  I love like great new form factors of computing. [[00:15:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=949.9200000000001s)]
*  And it feels like with every major technological advance, a new thing becomes possible. [[00:15:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=953.76s)]
*  Phones are unbelievably good. [[00:16:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=961.24s)]
*  So I think the threshold is like very high here. [[00:16:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=962.8000000000001s)]
*  Like, like, I think like I personally think iPhone is like the greatest piece of [[00:16:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=965.12s)]
*  technology humanity has ever made. [[00:16:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=971.88s)]
*  It's really a wonderful product. [[00:16:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=973.84s)]
*  What comes after it? [[00:16:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=975.32s)]
*  Like, I don't know. [[00:16:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=976.52s)]
*  I mean, I was going to say that was what I was saying. [[00:16:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=977.32s)]
*  It's so good to get beyond it. [[00:16:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=978.84s)]
*  I think the bar is like quite high. [[00:16:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=980.96s)]
*  Well, you've been working with Johnny Ive on something, right? [[00:16:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=983.96s)]
*  We've been discussing ideas, but I don't like if I knew. [[00:16:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=986.5600000000001s)]
*  Is it that that it has to be more complicated or actually just much, much cheaper and [[00:16:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=991.36s)]
*  simpler? [[00:16:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=995.1600000000001s)]
*  Well, almost everyone's willing to pay for a phone anyway. [[00:16:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=996.36s)]
*  So if you could like make a way cheaper device, I think the barrier to carry a second [[00:16:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=999.52s)]
*  thing or use a second thing is pretty high. [[00:16:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1003.8000000000001s)]
*  So I don't think given that we're all willing to pay for phones or most of us are, I [[00:16:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1007.0s)]
*  don't think cheaper is the answer. [[00:16:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1011.2s)]
*  Different is the answer then. [[00:16:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1014.16s)]
*  Would there be like a specialized chip that would run on the phone that was really good [[00:16:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1016.4399999999999s)]
*  at powering a phone size AI model? [[00:17:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1020.04s)]
*  Probably. [[00:17:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1023.36s)]
*  But the phone manufacturers are going to do that for sure. [[00:17:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1023.96s)]
*  That doesn't that doesn't necessitate a new device. [[00:17:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1026.12s)]
*  I think you'd have to like find some really different interaction paradigm that the [[00:17:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1028.52s)]
*  technology enables. [[00:17:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1033.44s)]
*  And if I knew what it was, I would be excited to be working on it right now. [[00:17:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1034.48s)]
*  But you have you have voice working right now in the app. [[00:17:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1038.2s)]
*  In fact, I set my action button on my phone to go directly to chat GPT's voice app and [[00:17:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1041.08s)]
*  I use it with my kids and they love it. [[00:17:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1046.16s)]
*  Talking to latency issues, but it's really great. [[00:17:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1047.88s)]
*  We'll get that. [[00:17:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1050.8s)]
*  We'll get that better. [[00:17:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1051.32s)]
*  And I think voice is a hint to whatever the next thing is. [[00:17:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1052.2s)]
*  Like if you can get voice interaction to be really good, it feels I think that feels [[00:17:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1056.4s)]
*  I think that feels like a different way to use a computer. [[00:17:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1063.8s)]
*  But again, like that, by the way, like why is it not responsive? [[00:17:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1066.2s)]
*  And it feels like a CB, you know, like over over really annoying to use, you know, in [[00:17:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1070.8799999999999s)]
*  that way. But it's also brilliant when it gives you the right answer. [[00:17:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1078.3999999999999s)]
*  We are working on that. [[00:18:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1081.04s)]
*  It's it's so clunky right now. [[00:18:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1083.32s)]
*  It's slow. It's like kind of doesn't feel very smooth or authentic or organic. [[00:18:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1084.96s)]
*  Like we'll get all that to be much better. [[00:18:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1090.6s)]
*  What about computer vision? [[00:18:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1094.08s)]
*  I mean, they have glasses or maybe you could wear a pendant. [[00:18:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1095.12s)]
*  I mean, you take the combination of visual or video data, combine it with voice. [[00:18:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1099.2s)]
*  And now super I know everything that's happening around you. [[00:18:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1105.3999999999999s)]
*  Super powerful to be able to like the multimodality of saying like, hey, chat GPT, what am I [[00:18:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1108.28s)]
*  looking at? Or like what kind of plant is this? [[00:18:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1115.04s)]
*  I can't quite tell. [[00:18:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1117.24s)]
*  That's obvious. That's like that's another, I think, like hint. [[00:18:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1120.48s)]
*  But whether people want to wear glasses or like hold up something when they want that, [[00:18:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1123.2s)]
*  like, there's a bunch of just like the sort of like societal interpersonal issues here [[00:18:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1127.76s)]
*  are all very complicated about wearing a computer on your face. [[00:18:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1135.64s)]
*  We saw that with Google Glass. [[00:18:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1139.64s)]
*  People got punched in the face in the mission. [[00:19:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1140.92s)]
*  Started a lot of fighting. [[00:19:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1143.16s)]
*  I forgot about that. I forgot about that. [[00:19:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1143.96s)]
*  So I think it's like. [[00:19:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1145.24s)]
*  What are the apps that could be unlocked if AI was sort of ubiquitous on people's phones? [[00:19:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1148.88s)]
*  Do you have a sense of that or what would you want to see built? [[00:19:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1155.96s)]
*  I think what I want is just this always on like super low friction thing where I can. [[00:19:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1162.68s)]
*  Either by voice or by text or ideally like some other, it just kind of knows what I want. [[00:19:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1170.32s)]
*  Have this like constant thing helping me throughout my day that's got like as much [[00:19:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1175.0s)]
*  context as possible. [[00:19:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1178.76s)]
*  It's like the world's greatest assistant. [[00:19:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1180.8s)]
*  And it's just this like thing working to make me better and better. [[00:19:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1183.1599999999999s)]
*  There's there's like a I know you hear people like talk about the AI future there. [[00:19:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1187.4399999999998s)]
*  Imagine they imagine there's sort of two different approaches and they don't sound that [[00:19:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1191.3999999999999s)]
*  different. But I think they're like, yeah, I'm going to go with that. [[00:19:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1197.32s)]
*  But I think they're like very different for how we'll design the system in practice. [[00:19:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1199.72s)]
*  There's the. I want an extension of myself. [[00:20:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1202.76s)]
*  I want like a ghost or an alter ego or this thing that really like is me is acting on my [[00:20:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1207.28s)]
*  behalf is responding to emails not even telling me about it is sort of like it becomes [[00:20:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1213.84s)]
*  more me and is me. [[00:20:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1222.24s)]
*  And then there's this other thing which is like I want a great senior employee. [[00:20:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1224.56s)]
*  It may get to know me very well. [[00:20:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1229.4s)]
*  I may delegate it. [[00:20:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1230.76s)]
*  You know, you can like have access to my email and I'll tell you the constraints. [[00:20:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1231.96s)]
*  But but I think of it as this like separate entity. [[00:20:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1235.2s)]
*  And I personally like the separate entity approach better and think that's where we're [[00:20:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1239.3200000000002s)]
*  going to head. And so in that sense. [[00:20:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1244.56s)]
*  The thing is not you, but it's it's like a always available, always great, super capable [[00:20:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1249.56s)]
*  assistant. [[00:20:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1255.88s)]
*  Exactly. [[00:20:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1256.8400000000001s)]
*  It's an agent in a way like it's out there working on your behalf and understands what [[00:20:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1257.16s)]
*  you want and anticipates what you want is what I'm reading into what you're saying. [[00:21:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1261.88s)]
*  I think that the agent like behavior, but there's like a difference between. [[00:21:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1265.3600000000001s)]
*  A senior employee and an agent. [[00:21:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1271.76s)]
*  Yeah. [[00:21:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1273.2s)]
*  And like I want it. [[00:21:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1273.68s)]
*  You know, I think of like my. [[00:21:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1275.68s)]
*  I think like a bit. [[00:21:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1277.8400000000001s)]
*  Like one of the things that I like about a senior employee is they'll. [[00:21:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1279.68s)]
*  They'll push back on me. [[00:21:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1287.3600000000001s)]
*  They will sometimes not do something I ask, or they're sometimes will say like, I can [[00:21:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1288.8000000000002s)]
*  do that thing if you want. [[00:21:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1292.8400000000001s)]
*  But if I do it, here's what I think would happen. [[00:21:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1293.8400000000001s)]
*  And then this and then that. [[00:21:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1295.88s)]
*  And are you really sure? [[00:21:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1296.76s)]
*  I definitely want that kind of vibe, which not not just like this thing that I ask and [[00:21:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1300.0s)]
*  it does. [[00:21:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1304.2s)]
*  It can reason. [[00:21:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1305.0400000000002s)]
*  Yeah. [[00:21:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1306.0400000000002s)]
*  Yeah. [[00:21:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1306.5600000000002s)]
*  And push back. [[00:21:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1306.76s)]
*  It has like the kind of relationship with me that I would expect out of a really [[00:21:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1307.24s)]
*  competent person that I worked with, which is different from like a sycophant. [[00:21:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1311.92s)]
*  Yeah. [[00:21:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1316.0800000000002s)]
*  The thing in that world where if you have this like Jarvis like thing that can reason, [[00:21:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1316.92s)]
*  what do you think it does to products that you use today where the interface is very [[00:22:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1323.6000000000001s)]
*  valuable? [[00:22:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1329.72s)]
*  So, for example, if you look at an Instacart or if you look at an Uber or if you look at [[00:22:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1330.24s)]
*  a DoorDash, these are not services that are meant to be pipes that are just providing [[00:22:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1334.72s)]
*  a set of APIs to a smart set of agents that ubiquitously work on behalf of eight [[00:22:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1339.92s)]
*  billion people. [[00:22:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1344.88s)]
*  What do you think has to change in how we think about how apps need to work, of how [[00:22:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1346.96s)]
*  this entire infrastructure of experiences need to work in a world where you're [[00:22:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1350.8000000000002s)]
*  agentically interfacing to the world? [[00:22:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1354.68s)]
*  I'm actually very interested in designing a world that is equally usable by humans [[00:22:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1356.5200000000002s)]
*  and by AIs. [[00:22:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1362.5200000000002s)]
*  So I like the interpretability of that. [[00:22:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1363.92s)]
*  I like the smoothness of the handoffs. [[00:22:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1369.48s)]
*  I like the ability that we can provide feedback or whatever. [[00:22:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1371.24s)]
*  So, you know, DoorDash could just expose some API to my future AI assistant and they [[00:22:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1374.0s)]
*  could go put the order in and whatever. [[00:23:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1381.84s)]
*  Or I could say like, I could be holding my phone and I could say, OK, AI assistant, [[00:23:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1383.8s)]
*  like you put in this order on DoorDash, please. [[00:23:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1387.6399999999999s)]
*  And I could like watch the app open and see the thing clicking around and I could say, [[00:23:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1391.12s)]
*  hey, no, not this. [[00:23:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1394.76s)]
*  Like, there's something about designing a world that is usable equally well by humans [[00:23:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1396.12s)]
*  and AIs that I think is an interesting concept. [[00:23:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1405.28s)]
*  And it's always more excited about humanoid robots than sort of robots of like very [[00:23:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1407.9599999999998s)]
*  other shapes. The world is very much designed for humans. [[00:23:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1412.7199999999998s)]
*  I think we should absolutely keep it that way. [[00:23:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1415.0s)]
*  And a shared interface is nice. [[00:23:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1417.0s)]
*  So you see voice chat, that modality kind of gets rid of apps. [[00:23:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1419.36s)]
*  You just ask it for sushi. [[00:23:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1423.56s)]
*  It knows sushi you like before. [[00:23:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1425.0s)]
*  It knows what you don't like and does its best shot at doing it. [[00:23:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1426.16s)]
*  It's hard for me to imagine that we just go to a world totally where you say like, hey, [[00:23:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1430.2800000000002s)]
*  chat, you order me sushi and it says, OK, do you want it from this restaurant? [[00:23:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1435.24s)]
*  What kind of time? Whatever. [[00:23:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1438.92s)]
*  I think user, I think visual user interfaces are super good for a lot of things. [[00:24:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1440.4s)]
*  And it's hard for me to imagine like a world where you never look at a screen. [[00:24:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1447.5600000000002s)]
*  And just use voice mode only. [[00:24:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1455.1200000000001s)]
*  But I can imagine that for a lot of things. [[00:24:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1457.0s)]
*  I mean, Apple tried with Siri, like supposedly you can order an Uber automatically with [[00:24:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1460.16s)]
*  Siri. I don't think anybody's ever done it because why would you take the risk of not [[00:24:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1464.68s)]
*  putting the quality to your point? [[00:24:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1469.16s)]
*  The quality is not good. [[00:24:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1470.5600000000002s)]
*  But when the quality is good enough, you're you'll actually prefer it just because it's [[00:24:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1471.64s)]
*  just lighter weight. You don't have to take your phone out. [[00:24:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1476.0800000000002s)]
*  You don't have to search for your app and press it. [[00:24:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1478.0400000000002s)]
*  And, oh, it automatically logs you out. [[00:24:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1480.6000000000001s)]
*  Oh, hold on. Log back in. [[00:24:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1482.6000000000001s)]
*  Oh, TFA. It's a whole pain in the ass. [[00:24:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1483.92s)]
*  You know, it's like setting a timer with Siri. [[00:24:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1485.92s)]
*  I do every time because it works really well and it's great. [[00:24:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1488.0s)]
*  I need more information. [[00:24:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1491.3200000000002s)]
*  But ordering an Uber, like I want to see the prices for a few different options. [[00:24:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1493.24s)]
*  I want to see how far away it is. [[00:24:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1498.0s)]
*  I want to see, like, maybe even where they are on the map, because I might walk somewhere. [[00:24:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1499.44s)]
*  I get a lot more information by I think in less time by looking at that order, the [[00:25:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1503.68s)]
*  Uber screen than I would if I had to do that all through the audio channel. [[00:25:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1508.5600000000002s)]
*  I like your idea of watching it happen. [[00:25:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1512.24s)]
*  That's kind of cool. [[00:25:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1514.0800000000002s)]
*  I think there will just be like, yeah, different. [[00:25:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1515.48s)]
*  There are different interfaces we use for different tasks, and I think that'll keep going. [[00:25:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1519.2s)]
*  Of all the developers that are building apps and experiences on OpenAI, [[00:25:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1522.6000000000001s)]
*  are there a few that stand out for you where you're like, OK, this is directionally [[00:25:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1528.04s)]
*  going in a super interesting area, even if it's like a toy app. [[00:25:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1531.3200000000002s)]
*  But are there things that you guys point to and say this is really important? [[00:25:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1534.96s)]
*  I met with a new company this morning or barely even a company. [[00:25:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1538.68s)]
*  It's like two people that are going to work on a summer project trying to actually [[00:25:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1543.3600000000001s)]
*  finally make the AI tutor. [[00:25:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1547.04s)]
*  Like, hmm. [[00:25:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1548.48s)]
*  And I've always been interested in this space. [[00:25:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1549.48s)]
*  A lot of people have done great stuff on our platform. [[00:25:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1551.48s)]
*  But if someone can deliver, like, the way that you actually like, they used a phrase I love, [[00:25:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1553.48s)]
*  which is this is going to be like a Montessori level reinvention for how people work. [[00:26:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1563.3600000000001s)]
*  But if you can find this new way to let people explore and learn in new ways on their own, [[00:26:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1566.88s)]
*  I'm personally super excited about that. [[00:26:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1571.5600000000002s)]
*  A lot of the coding related stuff you mentioned, Devon, earlier, I think that's like a super [[00:26:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1574.0s)]
*  cool vision of the future. [[00:26:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1578.8000000000002s)]
*  The thing that I am, health care, I believe, should be pretty transformed by this. [[00:26:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1580.8000000000002s)]
*  But the thing I'm personally most excited about is the sort of do-it-yourself approach. [[00:26:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1587.8000000000002s)]
*  The thing I'm most excited about is the sort of doing faster and better scientific discovery. [[00:26:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1593.8s)]
*  GPT-4 clearly not there in a big way, although maybe it accelerates things a little bit by [[00:26:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1599.8s)]
*  making scientists more productive. [[00:26:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1604.08s)]
*  But Alpha 43, yeah. [[00:26:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1606.9199999999998s)]
*  That's like... [[00:26:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1609.6s)]
*  But Sam... [[00:26:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1610.6s)]
*  That will be a triumph. [[00:26:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1611.6s)]
*  Those are not... [[00:26:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1612.6s)]
*  Like these models are trained and built differently than the language models. [[00:26:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1615.6s)]
*  I mean, to some, obviously, there's a lot that's similar. [[00:27:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1622.84s)]
*  But there's a lot... [[00:27:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1626.12s)]
*  There's kind of a ground-up architecture to a lot of these models that are being applied [[00:27:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1627.12s)]
*  to these specific problem sets, these specific applications, like chemistry interaction modeling, [[00:27:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1630.76s)]
*  for example. [[00:27:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1637.4599999999998s)]
*  You'll need some of that for sure. [[00:27:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1638.4599999999998s)]
*  But the thing that I think we're missing across the board for many of these things we've been [[00:27:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1641.12s)]
*  talking about is models that can do reasoning. [[00:27:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1645.28s)]
*  And once you have reasoning, you can connect it to chemistry simulators or whatever. [[00:27:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1649.1599999999999s)]
*  That's the important question I wanted to kind of talk about today, was this idea of [[00:27:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1653.28s)]
*  networks of models. [[00:27:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1660.3999999999999s)]
*  People talk a lot about agents as if there's kind of this linear set of call functions [[00:27:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1661.3999999999999s)]
*  that happen. [[00:27:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1666.24s)]
*  But one of the things that arises in biology is networks of systems that have cross interactions [[00:27:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1667.72s)]
*  that the aggregation of the system, the aggregation of the network produces an output rather than [[00:27:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1675.84s)]
*  one thing calling another, that thing calling another. [[00:28:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1681.96s)]
*  Do we see an emergence in this architecture of either specialized models or network models [[00:28:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1684.48s)]
*  that work together to address bigger problem sets, use reasoning? [[00:28:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1689.56s)]
*  There's computational models that do things like chemistry or arithmetic, and there's [[00:28:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1693.72s)]
*  other models that do, rather than one model to rule them all that's purely generalized. [[00:28:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1697.24s)]
*  I don't know. [[00:28:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1705.0s)]
*  I don't know how much reasoning is going to turn out to be a super generalizable thing. [[00:28:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1706.0s)]
*  I suspect it will, but that's more just like an intuition and a hope, and it would be nice [[00:28:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1715.24s)]
*  if it worked out that way. [[00:28:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1720.4s)]
*  I don't know if that's like. [[00:28:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1722.08s)]
*  But let's walk through the protein modeling example. [[00:28:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1726.6s)]
*  There's a bunch of training data, images of proteins, and then sequence data, and they [[00:28:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1730.84s)]
*  build a model, predictive model, and they have a set of processes and steps for doing [[00:28:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1736.08s)]
*  that. [[00:28:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1739.9199999999998s)]
*  Do you envision that there's this artificial general intelligence or this great reasoning [[00:29:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1740.9199999999998s)]
*  model that then figures out how to build that sub model that figures out how to solve that [[00:29:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1745.56s)]
*  problem by acquiring the necessary data and then resolving it? [[00:29:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1749.24s)]
*  There's so many ways where that could go. [[00:29:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1753.48s)]
*  Maybe it is it trains a literal model for it, or maybe it just knows the one big model. [[00:29:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1755.64s)]
*  It can go pick what other training data it needs and ask a question and then update on [[00:29:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1762.1200000000001s)]
*  that. [[00:29:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1765.96s)]
*  I guess the real question is, are all these startups going to die? [[00:29:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1766.96s)]
*  Because so many startups are working in that modality, which is go get special data and [[00:29:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1771.0s)]
*  then train a new model on that special data from the ground up, and then it only does [[00:29:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1774.96s)]
*  that one sort of thing, and it works really well at that one thing, and it works better [[00:29:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1778.88s)]
*  than anything else at that one thing. [[00:29:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1782.72s)]
*  There's like a version of this I think you can already see. [[00:29:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1784.32s)]
*  When you were talking about biology and these complicated networks of systems, the reason [[00:29:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1791.8799999999999s)]
*  I was smiling, I got super sick recently, and I'm mostly better now, but it was just [[00:29:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1795.2s)]
*  body got beat up one system at a time. [[00:30:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1801.52s)]
*  You can really tell, okay, it's this cascading thing. [[00:30:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1804.08s)]
*  And that reminded me of you talking about biology is just these, you have no idea how [[00:30:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1809.6s)]
*  much these systems interact with each other until things start going wrong. [[00:30:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1813.44s)]
*  And that was sort of like interesting to see. [[00:30:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1816.24s)]
*  But I was using, I was like using chat GPT to try to like figure out like what was happening, [[00:30:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1818.8400000000001s)]
*  whatever, and would say, well, I'm unsure of this one thing. [[00:30:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1826.92s)]
*  And then I just like posted a paper on it without even reading the paper, like in the [[00:30:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1830.8400000000001s)]
*  context. [[00:30:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1835.44s)]
*  And it says, oh, that was the thing I was unsure of. [[00:30:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1836.44s)]
*  Like now I think this instead. [[00:30:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1837.8000000000002s)]
*  So there's like a that was like a small version of what you're talking about, where you can [[00:30:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1839.76s)]
*  like can say this, I don't know this thing, you can put more information, you don't retrain [[00:30:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1843.96s)]
*  the model, you're just adding it to the context here. [[00:30:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1848.52s)]
*  And now you're getting them. [[00:30:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1851.2s)]
*  So these models that are predicting protein structure, like, let's say, right, this is [[00:30:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1852.48s)]
*  the whole basis, and now now other molecules at alpha fold three, can they can? [[00:30:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1856.84s)]
*  Yeah, I mean, is it basically a world where the best generalized model goes in and gets [[00:31:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1863.72s)]
*  that training data and then figures out on its own? [[00:31:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1869.32s)]
*  And maybe you could maybe you could use an example for us. [[00:31:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1872.96s)]
*  Can you tell us about Sora, your video model that generates amazing moving images, moving [[00:31:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1875.06s)]
*  video? [[00:31:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1880.48s)]
*  And what's different about the architecture there, whatever you're willing to share on [[00:31:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1881.48s)]
*  how how that is different? [[00:31:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1886.06s)]
*  Yeah, so my on the general thing first, my [[00:31:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1888.12s)]
*  You clearly will need specialized simulators, connectors, pieces of data, whatever. [[00:31:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1895.5s)]
*  But my intuition, and again, I don't have this like backed up with science, my intuition [[00:31:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1903.78s)]
*  would be if we can figure out the core of generalized reasoning, connecting that to [[00:31:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1909.1s)]
*  new problem domains, in the same way that humans are generalized reasoners, would I [[00:31:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1913.96s)]
*  think be be doable. [[00:32:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1920.34s)]
*  It's like a faster unlock faster unlock than I think I think so. [[00:32:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1922.3799999999999s)]
*  But yeah, you Sora like does not start with a language model. [[00:32:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1931.1799999999998s)]
*  It's that that's a model that is like customized to do video. [[00:32:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1935.02s)]
*  And so like, we're clearly not at that world yet. [[00:32:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1941.26s)]
*  Right. [[00:32:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1943.98s)]
*  So you guys, so just as an example, for you guys to build a good video model, you built [[00:32:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1944.98s)]
*  it from scratch using I'm assuming some different architecture and different data. [[00:32:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1949.7s)]
*  But in the future, the generalized reasoning system, the AGI, whatever system theoretically [[00:32:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1956.02s)]
*  could render that by figuring out how to do it. [[00:32:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1962.8600000000001s)]
*  Yeah, I mean, one example of this is like, okay, you know, as far as I know, all the [[00:32:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1965.82s)]
*  best text models in the world are still a lot of regressive models, and the best image [[00:32:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1970.74s)]
*  and video models are diffusion models. [[00:32:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1974.66s)]
*  That's like, sort of strange in some sense. [[00:32:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1976.46s)]
*  Yeah. [[00:32:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1979.78s)]
*  Yeah. [[00:33:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1980.46s)]
*  So there's a big debate about training data. [[00:33:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1980.62s)]
*  You guys have been I think, the most thoughtful of any company, you've got licensing [[00:33:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1985.3s)]
*  deals now, FT, etc. [[00:33:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1989.3799999999999s)]
*  And we got to just be gentle here, because you're involved in a New York Times lawsuit, [[00:33:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1991.82s)]
*  you weren't able to settle, I guess, an arrangement with them for training data. [[00:33:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=1996.54s)]
*  How do you think about fairness in fair use? [[00:33:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2001.06s)]
*  We've had big debates here on the pod. [[00:33:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2005.94s)]
*  Obviously, your actions are, you know, speak volumes that you're trying to be fair by [[00:33:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2008.54s)]
*  doing licensing deals. [[00:33:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2013.3s)]
*  So what's your personal position on the rights of artists who create beautiful music, [[00:33:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2014.3s)]
*  lyrics, books, and you taking that and then making a derivative product out of it and [[00:33:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2021.54s)]
*  then monetizing it? [[00:33:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2027.94s)]
*  And what's fair here? [[00:33:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2028.98s)]
*  And how do we get to a world where, you know, artists can make content in the world and [[00:33:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2030.7s)]
*  then decide what they want other people to do with it? [[00:33:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2036.78s)]
*  Yeah. [[00:33:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2039.78s)]
*  And I'm just curious of your personal belief, because I know you to be a thoughtful person [[00:34:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2040.14s)]
*  on this. [[00:34:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2043.5800000000002s)]
*  And I know a lot of other people in our industry are not very thoughtful about how they [[00:34:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2044.0600000000002s)]
*  think about content creators. [[00:34:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2048.82s)]
*  So I think it's very different for different kinds of... [[00:34:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2051.06s)]
*  I mean, look, on unfair use, I think we have a very reasonable position under the current [[00:34:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2053.78s)]
*  law, but I think AI is so different. [[00:34:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2058.86s)]
*  For things like art, we'll need to think about them in different ways. [[00:34:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2062.1s)]
*  But let's say if you go read a bunch of math on the Internet and learn how to do math, [[00:34:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2064.42s)]
*  that I think seems unobjectionable to most people. [[00:34:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2073.86s)]
*  And then there's like, you know, another set of people who might have a different opinion. [[00:34:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2078.3s)]
*  Well, what if you like... [[00:34:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2081.2200000000003s)]
*  Actually, let me not get into that just initially not making this answer too long. [[00:34:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2082.1s)]
*  So I think there's like one category of people are like, OK, there's like generalized human [[00:34:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2089.38s)]
*  knowledge. You can kind of like go if you learn that, like that's like open domain or [[00:34:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2093.9s)]
*  something. If you kind of go learn about the Pythagorean theorem. [[00:34:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2099.74s)]
*  That's one end of the spectrum. [[00:35:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2104.42s)]
*  And I think the other extreme end of the spectrum is art. [[00:35:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2105.54s)]
*  And maybe even like more than more specifically, I would say it's like doing... [[00:35:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2111.06s)]
*  It's a system generating art in the style or the likeness of another artist would be [[00:35:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2117.42s)]
*  kind of the furthest end of that. [[00:35:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2123.58s)]
*  And then there's many, many cases on the spectrum in between. [[00:35:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2126.58s)]
*  I think the conversation has been historically very caught up on training data, but it will [[00:35:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2130.42s)]
*  increasingly become more about what happens at inference time as training data becomes [[00:35:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2136.1800000000003s)]
*  less valuable and what the system does accessing information in context in real time or [[00:35:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2143.2200000000003s)]
*  taking like something like that. [[00:35:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2154.5s)]
*  What happens at inference time will become more debated and how the what the new economic [[00:35:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2155.94s)]
*  model is there. So if you say like, if you say like, create me a song in the style of [[00:36:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2160.86s)]
*  Taylor Swift, even if the model were never trained on any Taylor Swift songs at all, you [[00:36:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2168.14s)]
*  can still have a problem, which is that may have read about Taylor Swift, it may know [[00:36:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2175.46s)]
*  about her themes, Taylor Swift means something, but it's not like she's going to be able to [[00:36:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2178.94s)]
*  And then the question is like, should that model, even if it were never trained on any [[00:36:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2182.46s)]
*  Taylor Swift song whatsoever, be allowed to do that? [[00:36:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2189.7000000000003s)]
*  And if so, how should Taylor get paid? [[00:36:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2193.2200000000003s)]
*  Right. So I think there's an opt in, opt out in that case, first of all, and then there's [[00:36:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2197.54s)]
*  an economic model. [[00:36:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2201.62s)]
*  Staying on the music example, there is something interesting to look at from the historical [[00:36:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2203.26s)]
*  perspective here, which is sampling and how the economics around that work. [[00:36:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2209.26s)]
*  This is not quite the same thing, but it's like an interesting place to start looking. [[00:36:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2213.1000000000004s)]
*  Sam, let me just challenge that. [[00:36:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2216.34s)]
*  What's the difference in the example you're giving of the model learning about things [[00:36:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2217.82s)]
*  like song structure, tempo, melody, harmony relationships, all the discovering all the [[00:37:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2222.46s)]
*  underlying structure that makes music successful, and then building new music using training [[00:37:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2229.6600000000003s)]
*  data and what a human does that listens to lots of music, learns about and their brain [[00:37:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2235.06s)]
*  is processing and building all those same sort of predictive models or those same sort [[00:37:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2241.34s)]
*  of discoveries or understandings. [[00:37:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2245.74s)]
*  What's the difference here and why? [[00:37:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2248.54s)]
*  Why are you making the case that perhaps artists should be uniquely paid? [[00:37:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2250.74s)]
*  This is not a sampling situation. [[00:37:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2256.1s)]
*  You're not the AI is not outputting and it's not storing in the model the actual original [[00:37:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2257.58s)]
*  song. Yeah, I was learning structure, right? [[00:37:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2261.82s)]
*  I wasn't trying to make that that point because I agree, like in the same way that humans [[00:37:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2264.38s)]
*  are inspired by other humans. [[00:37:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2268.42s)]
*  I was saying if you if you say generate me a song in the style of Taylor Swift. [[00:37:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2270.1400000000003s)]
*  I see. Right. OK. [[00:37:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2274.1400000000003s)]
*  I think that's like where the prompt leverages some artists. [[00:37:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2275.1400000000003s)]
*  I think personally, that's a different case. [[00:37:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2278.46s)]
*  Would you be comfortable asking or would you be comfortable letting the model train itself [[00:38:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2280.98s)]
*  with a music model being trained on the whole corpus of music that humans have created [[00:38:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2286.38s)]
*  without royalties being paid to the artists that that music is being fed in? [[00:38:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2291.14s)]
*  And then you're not allowed to ask artists specific prompts. [[00:38:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2296.9s)]
*  You could just say, hey, pay me a plenty of a really cool pop song that's fairly modern [[00:38:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2299.74s)]
*  about heartbreak, you know, with a female voice. [[00:38:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2303.7799999999997s)]
*  You know, we have currently made the decision not to do music and partly because exactly [[00:38:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2306.5s)]
*  these questions of where you draw the lines and you know what, like even. [[00:38:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2312.58s)]
*  I was meeting with several musicians I really admire recently. [[00:38:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2316.42s)]
*  I was just trying to talk about some of these edge cases, but even the world in which. [[00:38:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2319.5s)]
*  If we. [[00:38:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2326.5s)]
*  Went and. [[00:38:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2328.26s)]
*  Let's say we paid 10,000 musicians to create a bunch of music just to make a great [[00:38:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2330.34s)]
*  training set where the music model could learn everything about strong song structure [[00:38:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2334.62s)]
*  and. [[00:39:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2340.3s)]
*  What makes a good catchy beat and. [[00:39:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2342.5s)]
*  And only trained on that. [[00:39:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2345.46s)]
*  Let's say we could still make a great music model, which maybe we could. [[00:39:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2346.66s)]
*  You know, I was kind of like posing that as a thought experiment to musicians and they're [[00:39:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2350.58s)]
*  like, well, I can't object to that on any principle basis at that point. [[00:39:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2353.4199999999996s)]
*  And yet there's still something I don't like about it. [[00:39:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2357.7799999999997s)]
*  Now, that's not a reason not to do it. [[00:39:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2359.54s)]
*  Necessarily, but it is. [[00:39:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2362.46s)]
*  Did you see that ad that Apple put out maybe it was yesterday or something of like squishing [[00:39:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2366.18s)]
*  all of human creativity down into one really cool thing? [[00:39:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2370.3799999999997s)]
*  What was your take on it? [[00:39:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2373.1s)]
*  People got really emotional about it. [[00:39:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2376.38s)]
*  Yeah. [[00:39:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2378.02s)]
*  Stronger reaction than you would think. [[00:39:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2379.02s)]
*  There's something about. [[00:39:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2381.18s)]
*  I'm obviously hugely positive on AI, but there is something that I think is beautiful [[00:39:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2386.22s)]
*  about human creativity and human artistic expression. [[00:39:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2391.22s)]
*  And, you know, for an AI that just does better science, like great, bring that on. [[00:39:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2394.54s)]
*  But an AI that is going to do this like, you know, you know, you know, you know, you know, [[00:39:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2398.38s)]
*  do this like deeply beautiful human creative expression. [[00:40:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2402.26s)]
*  I think we should like figure out it's going to happen. [[00:40:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2405.82s)]
*  It's going to be a tool that will lead us to greater creative heights. [[00:40:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2410.1000000000004s)]
*  But I think we should figure out how to do it in a way that like preserves the spirit [[00:40:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2412.86s)]
*  of what we all care about here. [[00:40:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2416.38s)]
*  And I think your actions speak loudly. [[00:40:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2418.38s)]
*  We were trying to do Star Wars characters in Dolly. [[00:40:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2421.78s)]
*  And if you ask for Darth Vader, it says, hey, we can't do that. [[00:40:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2427.7000000000003s)]
*  So you've I guess, red teamed or whatever you call it internally. [[00:40:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2430.9s)]
*  We try. [[00:40:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2434.3s)]
*  Yeah. You're not allowing people to use other people's IP. [[00:40:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2435.3s)]
*  So you've taken that decision. [[00:40:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2438.5s)]
*  Now, if you asked it to make a Jedi bulldog or a Sith Lord bulldog, which I did, it made [[00:40:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2439.7400000000002s)]
*  my bulldogs as if bulldogs. [[00:40:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2444.98s)]
*  So there's an interesting question about your spectrum. [[00:40:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2446.7400000000002s)]
*  Right. Yeah. You know, we put out this thing yesterday called the spec where we're trying [[00:40:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2449.3s)]
*  to say here are here's here's how our models supposed to behave. [[00:40:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2453.54s)]
*  And it's very hard. [[00:40:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2458.9s)]
*  It's a long document. [[00:41:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2460.58s)]
*  It's very hard to like specify exactly in each case where the limits should be. [[00:41:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2461.54s)]
*  And I view this as like a discussion that's going to need a lot more input. [[00:41:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2465.58s)]
*  But but these sorts of questions about. [[00:41:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2470.98s)]
*  OK, maybe it shouldn't generate Darth Vader, but the idea of a Sith Lord or a Sith style [[00:41:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2474.94s)]
*  thing or Jedi at this point is like part of the culture. [[00:41:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2479.42s)]
*  Like, like these are these are all hard decisions. [[00:41:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2482.26s)]
*  Yeah. And I think you're right. [[00:41:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2485.7000000000003s)]
*  The music industry is going to consider this opportunity to make Tellers of songs [[00:41:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2487.14s)]
*  their opportunity. It's part of the four part fair use test is, you know, these who gets [[00:41:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2492.06s)]
*  to capitalize on new innovations for existing art. [[00:41:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2496.9s)]
*  And Disney has an argument that, hey, you know, if you're going to make Sora versions [[00:41:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2500.18s)]
*  of Ashoka or whatever, Obi-Wan Kenobi, that's Disney's opportunity. [[00:41:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2505.8599999999997s)]
*  And that's a great partnership for you. [[00:41:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2510.42s)]
*  You know, to pursue. [[00:41:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2513.7799999999997s)]
*  So we're I think this section I would label as a and the law. [[00:41:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2515.86s)]
*  So let me ask maybe a higher level question. [[00:41:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2519.1s)]
*  What does it mean when people say regulate a family, Sam? [[00:42:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2522.82s)]
*  What does it what does that even mean? [[00:42:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2527.58s)]
*  And comment on California's new proposed regulations as well, if you if you're up for [[00:42:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2529.18s)]
*  I'm concerned. I mean, there's so many proposed regulations, but most of the ones I've [[00:42:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2535.46s)]
*  seen on the California state things I'm concerned about. [[00:42:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2539.18s)]
*  I also have a general fear of the states all doing this them themselves. [[00:42:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2541.38s)]
*  When people say regulate, I don't think they mean one thing. [[00:42:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2546.5s)]
*  I think there's like some people are like beyond the whole thing. [[00:42:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2550.42s)]
*  Some people like don't allow it to be open source required to be open source. [[00:42:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2553.2200000000003s)]
*  The thing that I am personally most interested in is I think there will come. [[00:42:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2557.26s)]
*  Look, I may be wrong about this. [[00:42:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2564.1400000000003s)]
*  I will acknowledge that this is a forward looking statement and those are always [[00:42:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2565.42s)]
*  dangerous to make. But I think there will come a time. [[00:42:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2568.38s)]
*  In the not super distant future, like, you know, we're not talking like decades and [[00:42:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2571.3s)]
*  decades from now where A.I. [[00:42:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2574.5800000000004s)]
*  the frontier A.I. [[00:42:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2577.1800000000003s)]
*  systems are capable of causing significant. [[00:42:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2578.02s)]
*  Global harm and for those kinds of systems in the same way we have like global [[00:43:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2582.98s)]
*  oversight of nuclear weapons or synthetic bio or things that can really like have a [[00:43:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2589.46s)]
*  very negative impact way beyond the realm of one country. [[00:43:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2594.6600000000003s)]
*  I would like to see some sort of international agency that is looking at the most [[00:43:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2598.3s)]
*  powerful systems and ensuring like reasonable safety testing. [[00:43:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2602.74s)]
*  You know, these things are not going to escape and recursively self-improve or [[00:43:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2606.78s)]
*  whatever. [[00:43:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2610.38s)]
*  The criticism of this is that you're you have the resources to cozy up to lobby to [[00:43:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2611.46s)]
*  be involved. And you've been very involved with politicians and then startups, which [[00:43:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2619.62s)]
*  you're also passionate about and invest in, are not going to have the ability to [[00:43:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2623.2599999999998s)]
*  resource and deal with this and that this regulatory capture as per our friend, you [[00:43:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2627.5s)]
*  know, Bill Gurley did a great talk last year about it. [[00:43:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2632.8999999999996s)]
*  So maybe you could address that head on. [[00:43:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2634.8199999999997s)]
*  Do you feel like if the line where we're only going to look at models that are [[00:43:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2636.7s)]
*  trained on computers that cost more than 10 billion or more than 100 billion or [[00:44:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2640.4599999999996s)]
*  whatever dollars, I'd be fine with that. [[00:44:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2644.3799999999997s)]
*  There'd be some line that define. [[00:44:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2647.18s)]
*  And I don't think that puts any regulatory burden on startups. [[00:44:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2648.8999999999996s)]
*  So if you have like the nuclear raw material to make a nuclear bomb, like there's [[00:44:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2652.42s)]
*  a small subset of people who have that. [[00:44:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2657.3s)]
*  Therefore, you use the analogy of like a nuclear inspector in a situation. [[00:44:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2659.1s)]
*  Yeah, that's interesting. [[00:44:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2663.3s)]
*  Sax, you have a question? [[00:44:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2665.38s)]
*  Tomas, go ahead. You had a follow up. [[00:44:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2666.9s)]
*  Can I say one more thing about that? [[00:44:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2669.82s)]
*  Of course, I'd be super nervous about regulatory overreach here. [[00:44:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2671.1800000000003s)]
*  I think we can get this wrong by doing way too much or even a little too much. [[00:44:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2674.78s)]
*  I think we can get this wrong by doing not enough. [[00:44:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2678.46s)]
*  But. But I do think part of and. [[00:44:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2680.62s)]
*  And now, I mean, you know, we have seen regulatory overstepping or capture just [[00:44:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2686.5s)]
*  get super bad in other areas and. [[00:44:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2692.14s)]
*  You know, that also maybe nothing will happen, but but I think it is part of our [[00:44:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2696.3399999999997s)]
*  duty and our mission to like talk about what we believe is likely to happen and [[00:45:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2700.62s)]
*  what it takes to get that right. [[00:45:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2706.38s)]
*  The challenge, Sam, is that we have statute that is meant to protect people, [[00:45:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2708.06s)]
*  protect society at large. [[00:45:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2713.14s)]
*  What we're creating, however, is statute that gives the government rights to go [[00:45:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2715.62s)]
*  in and audit code to audit business trade secrets. [[00:45:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2721.42s)]
*  We've never seen that to this degree before. [[00:45:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2728.66s)]
*  Basically, the California legislation that's proposed and some of the federal [[00:45:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2731.46s)]
*  legislation that's been proposed basically requires the federal government to [[00:45:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2734.46s)]
*  audit a model, to audit software, to audit and review the parameters and the [[00:45:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2739.82s)]
*  weightings of the model. [[00:45:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2744.1800000000003s)]
*  And then you need their checkmark in order to deploy it for commercial or [[00:45:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2745.86s)]
*  public use. And for me, it just feels like we're trying to rein in the [[00:45:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2751.42s)]
*  government agencies for fear. [[00:45:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2759.38s)]
*  And because folks have a hard time understanding this and are scared about [[00:46:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2760.86s)]
*  the implications of it, they want to control it. [[00:46:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2765.1400000000003s)]
*  And because they want and the only way to control it is to say, give me a right to [[00:46:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2768.02s)]
*  audit before you can release it. [[00:46:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2770.7000000000003s)]
*  Yeah, and they're clueless. [[00:46:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2771.9s)]
*  I mean, the way that the stuff is written, you read it, you're like going to pull [[00:46:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2774.2200000000003s)]
*  your hair out because as you know better than anyone in 12 months, none of this [[00:46:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2776.94s)]
*  stuff's going to make sense anyway. [[00:46:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2780.34s)]
*  Totally. [[00:46:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2781.5s)]
*  Right. [[00:46:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2782.2200000000003s)]
*  Look, the reason I have pushed for an agency based approach for kind of like the [[00:46:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2782.6200000000003s)]
*  big picture stuff and not a like write it in laws. [[00:46:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2787.74s)]
*  I don't in 12 months, it will all be written wrong. [[00:46:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2790.7s)]
*  And I don't think even if these people were like true world experts, I don't [[00:46:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2793.62s)]
*  think they could get it right. [[00:46:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2798.8199999999997s)]
*  Looking out 12 or 24 months. [[00:46:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2799.8999999999996s)]
*  And I don't these policies, which is like, we're going to look at, you know, [[00:46:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2803.02s)]
*  we're going to audit all of your source code and like look at all of your weights [[00:46:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2807.22s)]
*  one by one. [[00:46:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2810.54s)]
*  Like, yeah, I think there's a lot of crazy proposals out there. [[00:46:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2811.14s)]
*  By the way, especially if the models are always being retrained all the time, if [[00:46:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2815.38s)]
*  they become more dynamic. [[00:46:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2817.98s)]
*  Again, this is why I think it's yeah. [[00:46:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2819.06s)]
*  But but like when before an airplane gets certified, there's like a set of safety [[00:47:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2820.82s)]
*  tests, we put the airplane through it. [[00:47:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2825.86s)]
*  And totally, it's different than reading all of your code. [[00:47:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2828.38s)]
*  That's reviewing the output of the model, not reviewing the insides of the model. [[00:47:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2831.94s)]
*  And so what I was going to say is that is the kind of thing that I think as safety [[00:47:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2836.86s)]
*  testing makes sense. [[00:47:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2842.58s)]
*  How are we going to get that to happen, Sam? [[00:47:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2844.54s)]
*  And I'm not just speaking for open eyes speak for the industry for for humanity, [[00:47:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2846.34s)]
*  because I am concerned that we draw ourselves into almost like a dark ages type [[00:47:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2849.94s)]
*  of era by restricting the growth of these incredible technologies that can prosper, [[00:47:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2855.1s)]
*  human that humanity can prosper from so significantly. [[00:47:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2860.14s)]
*  How do we change the sentiment and get that to happen? [[00:47:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2863.1s)]
*  Because this is all moving so quickly at the government levels and folks seem to [[00:47:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2865.7400000000002s)]
*  be getting it wrong. [[00:47:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2869.2200000000003s)]
*  And I'm just really concerned. [[00:47:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2869.94s)]
*  Just to build on that, Sam, the architectural decision, for example, that Lama [[00:47:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2871.42s)]
*  took is pretty interesting in that it's like, we're going to let Lama grow and be [[00:47:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2875.7000000000003s)]
*  as unfettered as possible. [[00:48:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2881.66s)]
*  And we have this other kind of thing that we call Lama Guard that's meant to be [[00:48:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2882.82s)]
*  these protective guardrails. [[00:48:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2886.54s)]
*  Is that how you see the problem being solved correctly? [[00:48:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2888.14s)]
*  Or do you see that at the current at the current strength of models, definitely some [[00:48:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2891.14s)]
*  things are going to go wrong. [[00:48:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2896.2200000000003s)]
*  And I don't want to like make light of those or not take those seriously. [[00:48:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2897.34s)]
*  But I'm not like I don't have any like catastrophic risk worries with a GPT-4 level [[00:48:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2901.1s)]
*  model. And I think there's many safe ways to choose to deploy this. [[00:48:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2906.9s)]
*  Maybe we'd find more common ground if we said that like, you know, the specific [[00:48:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2915.9s)]
*  example of models that are capable, that are technically capable, not even if [[00:48:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2921.94s)]
*  they're not going to be used this way of recursive self-improvement or of, you know, [[00:48:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2927.42s)]
*  autonomously designing and deploying a bio weapon or something like that. [[00:48:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2935.7000000000003s)]
*  Or a new model. [[00:49:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2941.54s)]
*  That was the recursive self-improvement point. [[00:49:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2943.1s)]
*  You know, we should have safety testing on the outputs at an international level for [[00:49:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2946.1800000000003s)]
*  models that, you know, have a reasonable chance of of posing a threat there. [[00:49:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2951.3s)]
*  I don't think like GPT-4, of course, does not pose any sort of, I don't want to say any sort [[00:49:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2957.46s)]
*  because we don't. [[00:49:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2966.7000000000003s)]
*  I don't think the GPT-4 poses a material threat on those kinds of things. [[00:49:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2969.1800000000003s)]
*  And I think there's many safe ways to release a model like this. [[00:49:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2972.82s)]
*  But, you know, when like significant loss of human life is a serious possibility, like [[00:49:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2976.1800000000003s)]
*  airplanes or any number of other examples where I think we're happy to have some sort [[00:49:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2984.02s)]
*  of testing framework, like I don't think about an airplane when I get on it, I just [[00:49:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2989.7400000000002s)]
*  assume it's going to be safe. [[00:49:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2992.66s)]
*  Right. [[00:49:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2994.02s)]
*  There's a lot of hand wringing right now, Sam, about jobs. [[00:49:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2995.38s)]
*  And you had a lot of, I think you did like some sort of a test when you were at YC about [[00:49:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=2998.98s)]
*  UBI and you've been... [[00:50:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3002.9s)]
*  Our results in that come out very soon. [[00:50:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3004.5s)]
*  Just it was a five year study that wrapped up or started five years ago. [[00:50:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3006.5s)]
*  Well, there was like a beta study first and there was like a long one that ran. [[00:50:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3011.86s)]
*  But... [[00:50:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3014.98s)]
*  Paul Park, what did you learn about that? [[00:50:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3015.26s)]
*  Yeah, why did you start it? [[00:50:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3016.7400000000002s)]
*  Maybe just explain UBI and why you started it. [[00:50:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3018.1s)]
*  So we started thinking about this in 2016, kind of about the same time, started thinking [[00:50:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3021.7000000000003s)]
*  AI really seriously. [[00:50:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3027.1s)]
*  And the theory was that the magnitude of the change that may come to society and jobs in [[00:50:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3028.62s)]
*  the economy and sort of in some deeper sense than that, like what the social contract looks [[00:50:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3038.5s)]
*  like, meant that we should have many studies to study many ideas about new ways to [[00:50:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3042.94s)]
*  arrange that. [[00:50:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3051.5s)]
*  I also think that, you know, I'm not like a super fan of how the government has handled [[00:50:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3053.82s)]
*  most policies designed to help poor people. [[00:51:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3060.1800000000003s)]
*  And I kind of believe that if you could just give people money, they would make good [[00:51:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3063.0600000000004s)]
*  decisions. The market would do its thing. [[00:51:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3068.02s)]
*  And, you know, I'm very much in favor of lifting up the floor and reducing, eliminating [[00:51:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3069.78s)]
*  poverty. But I'm interested in better ways to do that than what we have tried for the [[00:51:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3076.62s)]
*  existing social safety net and kind of the way things have been handled. [[00:51:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3082.1s)]
*  And I think giving people money is not going to go solve all problems. [[00:51:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3086.06s)]
*  It's certainly not going to make people happy. [[00:51:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3089.78s)]
*  But it might solve some problems and it might give people a better horizon with which [[00:51:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3091.74s)]
*  to help themselves. [[00:51:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3100.74s)]
*  And I'm interested in that. [[00:51:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3101.74s)]
*  I think that now that we see some of the ways, so 2016 was a very long time ago, you [[00:51:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3103.3399999999997s)]
*  know, now that we see some of the ways that AI is developing, I wonder if there's better [[00:51:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3111.02s)]
*  things to do than the traditional conceptualization of UBI. [[00:51:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3115.22s)]
*  Like, I wonder, I wonder if the future looks something like more like universal basic [[00:52:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3122.3799999999997s)]
*  compute than universal basic income. [[00:52:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3127.58s)]
*  And everybody gets like a slice of GPT-7s compute and they can use it. [[00:52:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3129.8199999999997s)]
*  They can resell it. They can donate it to somebody to use for cancer research. [[00:52:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3134.02s)]
*  But what you get is not dollars, but this like slice. [[00:52:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3137.74s)]
*  Yeah, you own like part of the productivity. [[00:52:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3142.1s)]
*  Right. I would like to shift to the gossip part of this. [[00:52:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3144.2599999999998s)]
*  Gossip. [[00:52:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3147.86s)]
*  Gossip. [[00:52:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3148.86s)]
*  Sam, let's go back to November. [[00:52:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3149.86s)]
*  What the flying f*** happened? [[00:52:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3151.62s)]
*  You know, I if you have specific questions, I'm happy to maybe I said maybe I won't. [[00:52:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3158.22s)]
*  You said you were going to talk about it at some point. [[00:52:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3162.9s)]
*  So here's the point. [[00:52:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3164.54s)]
*  What the hell happened? [[00:52:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3165.86s)]
*  You were fired. You came back. [[00:52:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3167.38s)]
*  It was palace intrigue. [[00:52:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3169.02s)]
*  Did somebody stab you in the back? [[00:52:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3171.2599999999998s)]
*  Did you find AGI? [[00:52:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3172.66s)]
*  What's going on? [[00:52:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3173.9s)]
*  Tell us. This is a safe space. [[00:52:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3174.9s)]
*  Yeah. [[00:52:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3176.78s)]
*  I was fired. [[00:52:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3179.6600000000003s)]
*  I was. [[00:53:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3181.02s)]
*  I talked about coming back. [[00:53:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3183.7400000000002s)]
*  I kind of was a little bit unsure at the moment about what I wanted to do because I was very upset. [[00:53:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3185.06s)]
*  And I realized that I really loved OpenAI and the people and that I would come back. [[00:53:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3190.78s)]
*  And I kind of I knew it was going to be hard. [[00:53:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3198.6600000000003s)]
*  It was even harder than I thought. [[00:53:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3201.82s)]
*  But I kind of was like, all right, fine. [[00:53:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3204.02s)]
*  I agreed to come back. [[00:53:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3207.1400000000003s)]
*  The board took a while to figure things out. [[00:53:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3209.42s)]
*  And then, you know, we were kind of like trying to keep the team together and keep doing things for our customers and, you know, sort of started making other plans. [[00:53:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3212.1800000000003s)]
*  Then the board decided to hire a different interim CEO. [[00:53:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3221.1400000000003s)]
*  And then everybody. [[00:53:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3224.54s)]
*  There are many people. [[00:53:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3227.7000000000003s)]
*  Oh, my gosh. What was that guy's name? [[00:53:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3228.7000000000003s)]
*  He was there for like a Scaramucci, right? [[00:53:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3230.5s)]
*  Like, and it's great. [[00:53:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3232.14s)]
*  And I have nothing but good things to say about it. [[00:53:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3235.14s)]
*  I was here for Scaramucci. [[00:53:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3237.18s)]
*  And then where were you when you found the news that you'd been fired? [[00:54:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3241.02s)]
*  Like, well, take me to that moment. [[00:54:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3245.38s)]
*  I was in a hotel room in Vegas for F1 weekend. [[00:54:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3246.66s)]
*  I think that's how I'm there. [[00:54:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3249.42s)]
*  And what's your text? [[00:54:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3250.42s)]
*  And they're like, what did you say? [[00:54:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3251.38s)]
*  Fire pick up. [[00:54:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3252.66s)]
*  I think that's happened to you before, J. [[00:54:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3253.66s)]
*  Kel. I'm trying to think if I ever got fired. [[00:54:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3255.02s)]
*  I don't think I've gotten fired. [[00:54:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3257.7s)]
*  Yeah, I got it. [[00:54:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3259.62s)]
*  No, it's just a weird thing. [[00:54:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3260.46s)]
*  Like, it's a tax from who? [[00:54:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3261.2999999999997s)]
*  Actually, no, I got a text the night before. [[00:54:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3262.7799999999997s)]
*  And then I got on a phone call with the board. [[00:54:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3264.9s)]
*  And then that was that. [[00:54:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3267.66s)]
*  And then I kind of like, I mean, then everything went crazy. [[00:54:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3268.38s)]
*  I was like. [[00:54:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3271.94s)]
*  It was like. [[00:54:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3274.1s)]
*  I mean, I have my phone was like unusable. [[00:54:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3276.22s)]
*  It's just a nonstop vibrating thing of like text messages. [[00:54:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3278.46s)]
*  Call basically, you got fired by tweet. [[00:54:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3281.14s)]
*  That happened a few times during the Trump administration, a few [[00:54:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3284.18s)]
*  Cabin. [[00:54:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3287.94s)]
*  They didn't call me first before tweeting. [[00:54:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3288.46s)]
*  Nice of them. [[00:54:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3290.54s)]
*  And then, like, you know, I kind of did like a few hours of just [[00:54:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3292.14s)]
*  this like absolute fugue state in the hotel room. [[00:54:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3295.22s)]
*  Trying to like I was just confused beyond belief, trying to figure out what to do. [[00:55:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3300.7s)]
*  And so weird. [[00:55:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3304.34s)]
*  And then like. [[00:55:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3306.46s)]
*  Flew home, it may be like. [[00:55:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3308.78s)]
*  Got on a plane like, I don't know, three p.m. [[00:55:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3311.86s)]
*  or something like that. [[00:55:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3313.54s)]
*  Still just like, you know, crazy nonstop phone blowing up. [[00:55:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3315.62s)]
*  Met up with some people in person. [[00:55:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3319.2599999999998s)]
*  By that evening, I was like, OK, you know, I'll just like go do a research. [[00:55:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3321.3s)]
*  And I was feeling pretty happy about the future. [[00:55:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3327.3s)]
*  Yeah, you have options. [[00:55:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3330.98s)]
*  And then and then the next morning had this call with a couple of board members [[00:55:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3332.3s)]
*  about coming back, and that led to a few more days of craziness. [[00:55:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3336.82s)]
*  And then. [[00:55:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3342.06s)]
*  And then it kind of I think it got resolved. [[00:55:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3345.54s)]
*  Well, it was like a lot of insanity in between. [[00:55:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3348.98s)]
*  What percent of it was because of these nonprofit board members? [[00:55:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3352.14s)]
*  Well, we only have a nonprofit board, so it was all the nonprofit board members. [[00:55:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3358.42s)]
*  They're the board had gotten down to six people. [[00:56:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3362.38s)]
*  They. [[00:56:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3365.62s)]
*  And then they removed Greg from the board and then fired me. [[00:56:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3368.22s)]
*  So but it was like, you know. [[00:56:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3373.66s)]
*  But I mean, like, was there a culture clash between the people on the board [[00:56:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3375.26s)]
*  who had only nonprofit experience versus the people who had started experience? [[00:56:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3378.0200000000004s)]
*  And maybe you can share a little bit about if you're willing to, [[00:56:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3382.7400000000002s)]
*  the motivation behind the action, anything you can. [[00:56:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3385.34s)]
*  I think there's always been culture clashes at. [[00:56:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3389.42s)]
*  Look, obviously. [[00:56:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3395.0200000000004s)]
*  Not all of those board members are my favorite people in the world, but I have. [[00:56:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3397.7000000000003s)]
*  Serious respect for. [[00:56:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3402.22s)]
*  The gravity with which they treat AGI and the importance of getting a safety right. [[00:56:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3405.3399999999997s)]
*  And even if I stringently disagree with their decision making and actions, which I do, [[00:56:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3411.7799999999997s)]
*  I have never once doubted their. [[00:57:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3420.74s)]
*  Integrity or commitment to. [[00:57:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3424.06s)]
*  Sort of shared mission. [[00:57:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3428.3799999999997s)]
*  The sort of shared mission of safe and beneficial AGI. [[00:57:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3429.9s)]
*  You know, do I think they like made good decisions in the process of that or kind [[00:57:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3435.6600000000003s)]
*  of know how to balance all the things opening has to get right? [[00:57:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3440.46s)]
*  No. But but I think that like the intent, the intent of the magnitude of. [[00:57:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3443.42s)]
*  Yeah. AGI. [[00:57:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3451.3s)]
*  And getting that right. [[00:57:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3454.0600000000004s)]
*  I actually let me ask you about that. [[00:57:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3456.02s)]
*  So the mission of open AI is explicitly to create AGI, which I think is really interesting. [[00:57:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3457.7s)]
*  A lot of people would say that if we create AGI, that would be like an unintended [[00:57:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3464.3399999999997s)]
*  consequence of something gone horribly wrong. [[00:57:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3470.8599999999997s)]
*  And they're very afraid of that outcome. [[00:57:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3473.54s)]
*  But open AI makes that the actual mission. [[00:57:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3475.8199999999997s)]
*  Yeah. Does that create like more fear about what you're doing? [[00:57:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3477.98s)]
*  I mean, I understand it can create motivation, too. [[00:58:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3483.14s)]
*  But how do you reconcile that? [[00:58:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3484.9399999999996s)]
*  I guess why is that? [[00:58:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3486.9s)]
*  I think a lot of I think a lot of the well. [[00:58:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3487.6600000000003s)]
*  I mean, first, I'll say that I'll answer the first question in the second one. [[00:58:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3491.34s)]
*  I think it does create a great deal of fear. [[00:58:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3494.06s)]
*  I think a lot of the world is understandably very afraid of AGI or very afraid of even [[00:58:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3496.5800000000004s)]
*  current AI and very excited about it and even more afraid and even more excited about [[00:58:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3502.1000000000004s)]
*  where it's going. And we. [[00:58:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3508.1000000000004s)]
*  We wrestle with that, but like, I think it is unavoidable that this is going to happen. [[00:58:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3511.62s)]
*  I also think it's going to be tremendously beneficial, but we do have to navigate how [[00:58:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3517.46s)]
*  to get there in a reasonable way. [[00:58:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3521.14s)]
*  And like a lot of stuff is going to change and changes, you know, pretty, pretty [[00:58:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3523.18s)]
*  uncomfortable for people. [[00:58:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3528.02s)]
*  So there's a lot of pieces. [[00:58:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3529.2599999999998s)]
*  That we got to get right. [[00:58:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3532.62s)]
*  And ask a. [[00:58:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3534.2599999999998s)]
*  Can ask a different question. [[00:58:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3536.06s)]
*  You have created. [[00:58:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3537.3s)]
*  I mean. [[00:58:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3539.6600000000003s)]
*  It's the hottest company and you are literally at the center of the center of the center. [[00:59:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3541.46s)]
*  But then. [[00:59:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3547.0600000000004s)]
*  It's so unique in the sense that all of this value you eschewed economically. [[00:59:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3548.6600000000003s)]
*  Can you just like walk us through like, yeah, I wish I had taken. [[00:59:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3555.0600000000004s)]
*  I wish I had taken equity, so I never had to answer this question. [[00:59:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3557.78s)]
*  If I could go back and. [[00:59:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3560.98s)]
*  I want to give you a grand now. [[00:59:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3561.98s)]
*  Why don't they give you a grand now? [[00:59:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3563.7s)]
*  Why doesn't the board just give you a big option grant like you deserve? [[00:59:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3565.62s)]
*  Yeah, give you five points. [[00:59:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3568.58s)]
*  What was the decision back then? [[00:59:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3569.7799999999997s)]
*  Like, why was that so important? [[00:59:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3571.18s)]
*  The decision back then, the original reason was just like the structure of our [[00:59:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3572.86s)]
*  nonprofit. It was like there was something about. [[00:59:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3577.06s)]
*  Yeah, OK, this is like nice from a motivations perspective, but mostly it was that our [[00:59:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3581.74s)]
*  board needed to be a majority of disinterested directors. [[00:59:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3586.94s)]
*  And I was like, that's fine. [[00:59:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3590.46s)]
*  I don't need equity right now. [[00:59:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3591.46s)]
*  I kind of. But like. [[00:59:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3593.06s)]
*  But in a weird way, now that you're running a company, yeah, it creates these weird [[00:59:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3596.82s)]
*  questions of like, well, what's your real motivation for us? [[01:00:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3600.38s)]
*  It's told that it is so deeply. [[01:00:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3602.78s)]
*  One thing I have noticed, it is so deeply unimaginable to people to say, I don't [[01:00:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3607.38s)]
*  really need more money. [[01:00:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3612.7000000000003s)]
*  Like, and I think I think people think it's a little bit of an ulterior motive. [[01:00:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3614.66s)]
*  I think. Well, yeah, yeah, yeah. [[01:00:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3619.42s)]
*  No, it's so it assumes it's like what else is he doing on the side to make money? [[01:00:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3620.62s)]
*  Yeah, if I were if I were just trying to say like, I'm going to try to make a [[01:00:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3624.22s)]
*  trillion dollars with open, I think everybody would have an easier time and it [[01:00:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3627.14s)]
*  would save me. [[01:00:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3630.78s)]
*  Well, I have a lot of conspiracy theories. [[01:00:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3631.58s)]
*  Sam, this is the back channel. [[01:00:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3633.46s)]
*  You are a great dealmaker. [[01:00:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3636.62s)]
*  I've watched your whole career. [[01:00:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3638.5s)]
*  I mean, you're just great at it. [[01:00:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3640.42s)]
*  You got all these connections. [[01:00:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3641.86s)]
*  You're really good at raising money. [[01:00:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3643.58s)]
*  You're fantastic at it. [[01:00:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3646.98s)]
*  And you got this Johnny Ive thing going. [[01:00:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3648.2599999999998s)]
*  You're in humane. [[01:00:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3650.62s)]
*  You're investing in companies. [[01:00:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3651.7799999999997s)]
*  You got the orb, raising seven trillion dollars to build fabs, all this stuff. [[01:00:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3653.06s)]
*  News to me. [[01:00:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3658.38s)]
*  All of that put together. [[01:00:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3658.86s)]
*  J.K.L. loves fake news. [[01:01:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3660.9s)]
*  But I'm kind of being a little facetious here. [[01:01:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3662.18s)]
*  You know, obviously, you're not raising seven trillion dollars, but maybe that's [[01:01:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3664.14s)]
*  the market cap of something. [[01:01:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3666.9s)]
*  Putting all that aside, the tea was you're doing all these deals. [[01:01:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3667.9s)]
*  They don't trust you because what's your motivation? [[01:01:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3672.74s)]
*  You're end running and what opportunities belong inside of OpenAI? [[01:01:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3676.06s)]
*  What opportunities should be Sam's and this group of nonprofit people didn't trust you? [[01:01:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3680.26s)]
*  Is that what happens? [[01:01:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3685.06s)]
*  So the things like, you know, device companies, or if we were doing some chip fab [[01:01:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3686.38s)]
*  company, it's like those are not Sam project. [[01:01:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3690.7400000000002s)]
*  Those would be like, OpenAI would get that equity. [[01:01:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3692.86s)]
*  They would. [[01:01:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3696.1400000000003s)]
*  Okay. [[01:01:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3696.82s)]
*  That's not the public's perception. [[01:01:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3697.06s)]
*  Well, that's not like kind of the people like you who have to like commentate on [[01:01:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3699.1400000000003s)]
*  this stuff all day's perception, which is fair. [[01:01:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3702.86s)]
*  Because we haven't announced that stuff because it's not done. [[01:01:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3704.5s)]
*  I don't think most people in the world like are thinking about this, but I agree it [[01:01:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3706.74s)]
*  spins up a lot of conspiracies, conspiracy theories in like tech commentators. [[01:01:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3711.7s)]
*  Yeah. [[01:01:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3717.78s)]
*  And if I could go back, yeah, I would just say like, let me take equity and make that [[01:01:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3717.9s)]
*  super clear. [[01:02:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3722.54s)]
*  And then it would be like, all right. [[01:02:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3723.58s)]
*  Like I'd still be doing it because I really care about AGI and think this is like the [[01:02:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3725.34s)]
*  most interesting work in the world, but it would at least type check to everybody. [[01:02:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3728.54s)]
*  What's the chip project? [[01:02:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3732.22s)]
*  That's a seven trillion dollar. [[01:02:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3734.3s)]
*  And where does seven trillion number come from? [[01:02:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3735.5s)]
*  It makes no sense. [[01:02:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3736.98s)]
*  I don't know where that came from, actually. [[01:02:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3737.3s)]
*  I genuinely don't. [[01:02:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3738.94s)]
*  I think, I think the world needs a lot more AI infrastructure, a lot more than it's [[01:02:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3740.54s)]
*  currently planning to build and with a different cost structure. [[01:02:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3746.1800000000003s)]
*  The exact way for us to play there is we're still trying to figure that out. [[01:02:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3750.54s)]
*  Got it. [[01:02:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3756.3s)]
*  What's your preferred model of organizing OpenAI? [[01:02:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3756.5s)]
*  Is it sort of like the move fast, break things, highly distributed small teams, or is [[01:02:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3759.3799999999997s)]
*  it more of this organized effort where you need to plan because you want to prevent [[01:02:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3766.74s)]
*  some of these edge cases? [[01:02:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3770.3399999999997s)]
*  Oh, I have to go in a minute. [[01:02:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3773.2999999999997s)]
*  It's not because. [[01:02:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3774.54s)]
*  It's not to prevent the edge case that we need to be more organized, but it is that [[01:02:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3778.74s)]
*  these systems are so complicated and concentrating bets are so important. [[01:03:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3782.02s)]
*  Like one, you know, at the time before it was like obvious to do this, you have like [[01:03:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3787.3s)]
*  DeepMind or whatever has all these different teams doing all these different things and [[01:03:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3793.82s)]
*  they're spreading their bets out. [[01:03:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3797.46s)]
*  And you had OpenAI say we're going to like basically put the whole company and work [[01:03:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3798.5s)]
*  together to make GPT-4. [[01:03:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3801.7400000000002s)]
*  And that was like unimaginable for how to run an AI research lab. [[01:03:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3803.78s)]
*  But it is, I think what works at a minimum, it's what works for us. [[01:03:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3807.98s)]
*  So not because we're trying to prevent edge cases, but because we want to concentrate [[01:03:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3812.34s)]
*  resources and do these big, hard, complicated things. [[01:03:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3815.46s)]
*  We do have a lot of coordination on what we work on. [[01:03:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3819.2200000000003s)]
*  All right, Sam, I know you got to go. [[01:03:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3822.14s)]
*  You've been great on the hour. [[01:03:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3823.7400000000002s)]
*  Come back any time. [[01:03:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3825.14s)]
*  Great talking to you guys. [[01:03:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3826.42s)]
*  Yeah, that's fun. [[01:03:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3827.7000000000003s)]
*  Thanks for coming. [[01:03:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3828.3s)]
*  Thanks for being so open about it. [[01:03:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3828.82s)]
*  We've been talking about it for like a year plus. [[01:03:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3830.2200000000003s)]
*  I'm really happy it finally happened. [[01:03:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3832.58s)]
*  Yeah, it's awesome. [[01:03:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3834.02s)]
*  I really appreciate it. [[01:03:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3834.54s)]
*  I would love to come back on after our next major launch and I'll be able to talk more [[01:03:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3835.1800000000003s)]
*  directly about it. [[01:03:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3839.02s)]
*  Definitely. [[01:03:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3839.82s)]
*  You've got the Zoom link. [[01:04:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3840.3s)]
*  Same Zoom link every week. [[01:04:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3841.26s)]
*  Just same time. [[01:04:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3842.38s)]
*  Same Zoom link. [[01:04:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3843.2200000000003s)]
*  It's open anytime. [[01:04:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3843.86s)]
*  Please drop it. [[01:04:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3844.8999999999996s)]
*  Just drop it. [[01:04:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3845.5s)]
*  Just put it on your calendar. [[01:04:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3845.7799999999997s)]
*  Come back to the game. [[01:04:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3847.14s)]
*  Come back to the game. [[01:04:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3848.2599999999998s)]
*  Yeah, come back to the game. [[01:04:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3848.98s)]
*  We haven't seen you in a while. [[01:04:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3849.98s)]
*  I, you know, I would love to play poker. [[01:04:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3850.98s)]
*  It has been forever. [[01:04:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3853.06s)]
*  That would be a lot of fun. [[01:04:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3854.18s)]
*  Coming out. [[01:04:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3855.18s)]
*  That famous hand where Chamath, when you and I were heads up and you, you had- [[01:04:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3855.66s)]
*  I don't want to rely on me? [[01:04:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3859.62s)]
*  You and I were heads up and you went all in. [[01:04:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3861.2999999999997s)]
*  I had a set, but there was a straight and a flush on the board. [[01:04:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3864.54s)]
*  And I'm in the tank trying to figure out if I want to lose. [[01:04:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3867.5s)]
*  This back when we played small stakes, it might've been like 5K pot or something. [[01:04:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3870.94s)]
*  And then Chamath can't stay out of the pod. [[01:04:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3875.14s)]
*  And he starts taunting the two of us. [[01:04:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3877.54s)]
*  You should call. [[01:04:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3879.54s)]
*  You shouldn't call. [[01:04:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3880.02s)]
*  He's bluffing. [[01:04:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3880.66s)]
*  And I'm like, Chamath, I'm going, I'm trying to figure out if I make the call here. [[01:04:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3882.06s)]
*  I make the call. [[01:04:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3885.58s)]
*  And it was like, you had a really good hand and I just happened to have a set. [[01:04:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3886.94s)]
*  I think you had like top hair, top kicker or something, but you made a great move [[01:04:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3891.2999999999997s)]
*  because the board was so textured, almost like old bottom set. [[01:04:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3894.9s)]
*  Sam has a great style of playing, which I would call random jam. [[01:04:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3897.2599999999998s)]
*  Totally. [[01:05:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3900.38s)]
*  You got to just get out of the way. [[01:05:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3901.22s)]
*  I don't really know if you, I don't know if you could say about anybody else. [[01:05:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3902.22s)]
*  I don't, I don't, I'm not going to. [[01:05:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3906.1s)]
*  You haven't seen him out play in the last 18 months. [[01:05:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3907.54s)]
*  It's a lot different. [[01:05:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3909.3799999999997s)]
*  I've come back to the game. [[01:05:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3910.22s)]
*  I'm much more snug. [[01:05:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3911.7799999999997s)]
*  So much fun now. [[01:05:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3912.3799999999997s)]
*  Have you played bomb pots before? [[01:05:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3914.3399999999997s)]
*  Have you played bomb pots? [[01:05:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3916.8599999999997s)]
*  I don't know what that is. [[01:05:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3917.8199999999997s)]
*  Okay. [[01:05:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3918.98s)]
*  You'll love it. [[01:05:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3919.2599999999998s)]
*  We'll see you next time. [[01:05:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3920.02s)]
*  Yeah. [[01:05:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3920.8999999999996s)]
*  He's nuts. [[01:05:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3921.22s)]
*  It's PLO. [[01:05:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3921.8999999999996s)]
*  Sam, thank you. [[01:05:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3923.1s)]
*  Two boards. [[01:05:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3923.8199999999997s)]
*  And congrats on everything. [[01:05:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3924.8599999999997s)]
*  Honestly. [[01:05:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3926.2999999999997s)]
*  Thank you, Chamath. [[01:05:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3926.66s)]
*  Thanks for coming on and see you guys. [[01:05:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3927.1s)]
*  Love to have you back when the next F the big launch. [[01:05:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3928.62s)]
*  Sounds good. [[01:05:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3930.62s)]
*  Please do. [[01:05:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3931.02s)]
*  Cool. [[01:05:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3931.74s)]
*  Bye. [[01:05:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3932.22s)]
*  Gentlemen, some breaking news here. [[01:05:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3933.3799999999997s)]
*  All those projects, he said, are part of opening. [[01:05:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3936.4599999999996s)]
*  I, that's something people didn't know before this and a lot of confusion there. [[01:05:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3938.98s)]
*  Chamath, what was your major takeaway from our hour with Sam? [[01:05:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3943.62s)]
*  I think that these guys are going to be one of the four major companies that [[01:05:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3947.02s)]
*  matter in this whole space. [[01:05:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3952.98s)]
*  I think that that's clear. [[01:05:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3954.4199999999996s)]
*  I think what's still unclear is where is the economics going to be? [[01:05:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3956.7s)]
*  He said something very discreet, but I thought was important, which is, I think [[01:06:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3960.02s)]
*  he basically, my interpretation is these models will roughly all be the same, but [[01:06:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3963.94s)]
*  there's going to be a lot of scaffolding around these models that actually allow [[01:06:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3969.14s)]
*  you to build these apps. [[01:06:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3974.42s)]
*  So in many ways that is like the open source movement. [[01:06:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3975.3s)]
*  So even if the model itself is never open source, it doesn't much matter [[01:06:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3977.86s)]
*  because you have to pay for the infrastructure, right? [[01:06:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3982.54s)]
*  There's a lot of open source software that runs on Amazon. [[01:06:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3985.06s)]
*  You still pay AWS something. [[01:06:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3987.66s)]
*  So I think the right way to think about this now is the models will basically be [[01:06:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3989.18s)]
*  all really good. [[01:06:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3996.7s)]
*  And then it's all this other stuff that you'll have to pay for interface. [[01:06:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=3998.02s)]
*  Whoever builds all this other stuff is going to be in a position to build a [[01:06:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4001.54s)]
*  really good business. [[01:06:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4007.02s)]
*  Freeburg, he talked a lot about reasoning. [[01:06:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4008.14s)]
*  It seemed like that he kept going to reasoning and away from the language model. [[01:06:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4010.1s)]
*  Did you know, did you note that and anything else that you noted in our arrow? [[01:06:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4013.3399999999997s)]
*  Yeah, I mean, that's a longer conversation because there is a lot of talk about [[01:06:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4017.06s)]
*  language models eventually evolving to be so generalizable that they can resolve [[01:07:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4020.2999999999997s)]
*  pretty much like all intelligent function. [[01:07:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4028.02s)]
*  And so the language model is the foundational model that that yield AGI, but that's [[01:07:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4030.2999999999997s)]
*  all I think there's a lot of people that have different schools of thought on this. [[01:07:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4036.06s)]
*  And how much, my other takeaway, I think, is that the, I think what he also seemed [[01:07:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4039.02s)]
*  to indicate is there's like so many, like we're also enraptured by LLMs, but [[01:07:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4046.34s)]
*  there's so many things other than LLMs that are being baked and rolled by him and [[01:07:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4052.38s)]
*  by other groups. [[01:07:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4056.94s)]
*  And I think we have to pay some amount of attention to all of those because that's [[01:07:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4057.7400000000002s)]
*  probably where, and I think Freeburg, you tried to go there in your question. [[01:07:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4061.6200000000003s)]
*  That's where reasoning will really come from is this mixture of experts approach. [[01:07:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4064.42s)]
*  And so you're going to have to think multidimensionally to reason, right? [[01:07:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4068.6200000000003s)]
*  We do that, right? [[01:07:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4071.94s)]
*  Do I cross the street or not? [[01:07:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4072.86s)]
*  In this point in time, you reason based on all these multi inputs. [[01:07:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4074.5s)]
*  And so there's, there's all these little systems that go into making that decision [[01:07:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4078.42s)]
*  in your brain. [[01:08:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4081.6200000000003s)]
*  And if you, if you use that as a simple example, there's all this stuff that has [[01:08:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4082.38s)]
*  to go into making some experience, being able to reason intelligently. [[01:08:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4086.34s)]
*  Sax, you went right there with the corporate structure, the board. [[01:08:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4091.3s)]
*  And he gave us a lot more information here. [[01:08:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4096.74s)]
*  What are your thoughts on the, hey, you know, the chip stuff and the other stuff [[01:08:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4100.26s)]
*  I'm working on, that's all part of open AI. [[01:08:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4105.26s)]
*  People just don't realize it. [[01:08:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4107.26s)]
*  And in that moment, and then, you know, your questions to him about equity, your [[01:08:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4108.58s)]
*  thoughts on. [[01:08:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4112.62s)]
*  I'm not sure I was like the main guy who asked that question, Jake, but, um, [[01:08:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4114.06s)]
*  well, no, you do talk about the nonprofit, the difference between the [[01:08:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4119.06s)]
*  nonprofit question about the, clearly was some sort of culture clash on the board [[01:08:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4122.22s)]
*  between the people who originated from the nonprofit world and the people who [[01:08:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4127.22s)]
*  came from the startup world. [[01:08:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4131.26s)]
*  We don't really know more than that, but there clearly was some sort of culture [[01:08:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4132.58s)]
*  clash. [[01:08:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4135.3s)]
*  I thought one of the, a couple of the other areas that he drew attention to that [[01:08:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4136.26s)]
*  were kind of interesting is he clearly thinks there's a big opportunity on [[01:09:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4140.02s)]
*  mobile that goes beyond just like having, you know, a chat, GBT app on your phone, [[01:09:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4142.9800000000005s)]
*  or maybe even having like a Siri on your phone. [[01:09:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4148.9800000000005s)]
*  There's clearly something bigger there. [[01:09:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4151.900000000001s)]
*  He doesn't know exactly what it is, but it's going to require more inputs. [[01:09:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4153.46s)]
*  It's that, you know, personal assistant that's seen everything around you and [[01:09:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4157.62s)]
*  helping you. [[01:09:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4161.58s)]
*  I think that's a great insight, David, because he was talking about, Hey, I'm [[01:09:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4162.38s)]
*  looking for a senior team member who can push back on me and understands all [[01:09:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4166.46s)]
*  contexts. [[01:09:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4170.22s)]
*  I thought that was like a very interesting to think about an [[01:09:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4170.7s)]
*  executive assistant or an assistant that's has executive function as opposed [[01:09:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4173.78s)]
*  to being like just an alter ego for you or what he called a sycophant. [[01:09:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4179.14s)]
*  That's kind of interesting. [[01:09:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4183.34s)]
*  That was interesting. [[01:09:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4184.42s)]
*  Yeah. [[01:09:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4185.34s)]
*  Yeah. [[01:09:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4185.78s)]
*  And clearly he thinks there's a big opportunity in biology and scientific [[01:09:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4186.02s)]
*  discovery after the break. [[01:09:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4189.46s)]
*  I think we should talk about AlphaFold 3. [[01:09:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4190.860000000001s)]
*  It was just announced. [[01:09:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4192.66s)]
*  Yeah, let's do that. [[01:09:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4193.26s)]
*  And we can talk about the, the applet in depth. [[01:09:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4193.82s)]
*  I just want to also make sure people understand when people come on the pod, [[01:09:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4196.26s)]
*  we don't show them questions. [[01:09:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4199.22s)]
*  They don't edit the transcript. [[01:10:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4200.38s)]
*  Nothing is out of bounds. [[01:10:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4202.58s)]
*  If you were wondering why I didn't ask, or we didn't ask about the Elon lawsuit, [[01:10:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4204.02s)]
*  he's just not going to be able to comment on that. [[01:10:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4208.1s)]
*  So it'd be no comment. [[01:10:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4209.62s)]
*  So, you know, [[01:10:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4210.5s)]
*  and we're not going to go to time was limited and there's a lot of questions [[01:10:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4211.7s)]
*  that we could ask him that would have just been a waste of time. [[01:10:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4215.14s)]
*  And they would have already been asked. [[01:10:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4217.3s)]
*  So I just want to make sure people understand. [[01:10:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4218.9800000000005s)]
*  Of course, he's going to no comment on any lawsuit. [[01:10:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4220.38s)]
*  And he's already been asked about that 500 times. [[01:10:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4222.1s)]
*  Yes. [[01:10:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4225.18s)]
*  Should we take a quick break before the next before we come back? [[01:10:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4225.38s)]
*  Yeah, take a bio break and then we'll come back with some news for you and [[01:10:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4227.62s)]
*  some more banter with your favorite besties on the number one podcast in the [[01:10:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4230.3s)]
*  world, the Olin podcast. [[01:10:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4237.06s)]
*  All right. [[01:10:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4238.54s)]
*  Welcome back everybody. [[01:10:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4238.74s)]
*  Second half of the show, great guests and Altman, they come in on the pod. [[01:10:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4239.9s)]
*  We've got a bunch of news on the docket. [[01:10:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4243.58s)]
*  So let's get started. [[01:10:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4245.66s)]
*  Freyberg, you told me I could give some names of the guests that we've [[01:10:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4247.62s)]
*  booked for the Olin summit. [[01:10:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4252.099999999999s)]
*  I did not. [[01:10:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4253.78s)]
*  You did. [[01:10:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4254.74s)]
*  You've said each week, every week that I get to say some names. [[01:10:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4255.339999999999s)]
*  I did not. [[01:10:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4259.139999999999s)]
*  I appreciate your interest in the Olin summits lineup, but we do not yet have [[01:10:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4259.58s)]
*  enough critical mass to feel like we should go out there. [[01:11:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4265.98s)]
*  Well, I am a loose cannon, so I will, it's my two guests and I created the [[01:11:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4269.54s)]
*  summit and you took it from me. [[01:11:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4275.58s)]
*  So I've done a great job. [[01:11:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4277.54s)]
*  I will announce my guests. [[01:11:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4279.14s)]
*  I don't care what your opinion is. [[01:11:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4280.22s)]
*  I have booked two guests for the summit and it's going to be sold out. [[01:11:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4281.9400000000005s)]
*  Look at these two guests. [[01:11:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4286.02s)]
*  I booked for the third time coming back to the summit. [[01:11:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4286.9400000000005s)]
*  Our guy, Elon Musk will be there hopefully in person, if not, you know, [[01:11:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4289.9400000000005s)]
*  from 40,000 feet on starling connection, wherever he is in the world. [[01:11:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4293.82s)]
*  And for the first time, our friend Mark Cuban will be coming. [[01:11:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4296.86s)]
*  And so two great guests for you to look forward to, but Friedberg's got like [[01:11:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4302.54s)]
*  a thousand guests coming. [[01:11:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4306.0599999999995s)]
*  He'll tell you when it's like 48 hours before the conference, but you have two [[01:11:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4307.179999999999s)]
*  great guests coming. [[01:11:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4311.219999999999s)]
*  Speaking of billionaires who are coming, isn't he coming too? [[01:11:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4311.82s)]
*  Yes, he's coming. [[01:11:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4314.62s)]
*  Yes, he's booked. [[01:11:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4315.5s)]
*  So we have three billionaires. [[01:11:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4316.3s)]
*  Three billionaires. [[01:11:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4318.179999999999s)]
*  Yes. [[01:11:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4319.179999999999s)]
*  Okay. [[01:11:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4319.46s)]
*  Hasn't fully confirmed. [[01:12:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4320.0599999999995s)]
*  So don't. [[01:12:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4320.9s)]
*  Okay. [[01:12:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4321.3s)]
*  Well, we're going to say it anyway. [[01:12:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4321.54s)]
*  Has penciled in. [[01:12:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4323.179999999999s)]
*  Don't say it. [[01:12:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4324.139999999999s)]
*  Don't back up. [[01:12:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4324.78s)]
*  Say penciled. [[01:12:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4325.3s)]
*  Yeah. [[01:12:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4326.62s)]
*  Don't back out. [[01:12:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4326.74s)]
*  This is going to be catnip for all these protests organizers. [[01:12:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4327.42s)]
*  Like if you have to pick one place. [[01:12:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4330.34s)]
*  The bear. [[01:12:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4332.74s)]
*  Well, by the way, speaking of updates, what'd you guys think of the [[01:12:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4333.5s)]
*  bottle for the all in tequila? [[01:12:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4337.1s)]
*  Oh, beautiful. [[01:12:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4339.34s)]
*  Honestly, honestly, I will just say, I think you are doing a marvelous job. [[01:12:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4340.62s)]
*  That I was shocked at the design shock, meaning it is so unique and high quality. [[01:12:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4344.62s)]
*  I think it's amazing. [[01:12:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4353.22s)]
*  It would make me drink tequila. [[01:12:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4355.219999999999s)]
*  You're going to, you're going to want to. [[01:12:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4358.219999999999s)]
*  It is a stunning, just congratulations. [[01:12:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4359.94s)]
*  And yeah, it was just, when we went through the deck at the, at the monthly [[01:12:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4363.099999999999s)]
*  meeting, it was like, oh, that's nice. [[01:12:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4368.219999999999s)]
*  Oh, that's nice. [[01:12:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4370.5s)]
*  We're going to the concept bottles. [[01:12:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4371.219999999999s)]
*  And then that bottle came up and everybody went like crazy. [[01:12:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4372.42s)]
*  It was like somebody hitting like a Steph Curry hitting a half court shot. [[01:12:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4376.099999999999s)]
*  It was like, Oh my God. [[01:12:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4378.78s)]
*  It was just so clear that you've made an iconic bottle that if we can produce it. [[01:13:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4380.54s)]
*  Oh Lord, it is going to be like, we can't make it. [[01:13:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4386.66s)]
*  Yeah. [[01:13:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4392.02s)]
*  It can be amazing. [[01:13:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4392.66s)]
*  I'm excited. [[01:13:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4393.26s)]
*  I'm excited for it. [[01:13:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4393.740000000001s)]
*  You know, it's like the ball of science so complicated that we had to do a [[01:13:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4394.66s)]
*  feasibility analysis on whether it was actually manufacturable, but it is so, or [[01:13:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4396.9800000000005s)]
*  at least the early reports are good. [[01:13:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4401.860000000001s)]
*  So we're going to, hopefully we'll have some made for the, in time for the [[01:13:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4403.38s)]
*  all in summit. [[01:13:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4406.9800000000005s)]
*  I mean, why not? [[01:13:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4408.22s)]
*  So it's great when we get barricaded in by all these protesters, we can drink [[01:13:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4409.98s)]
*  the tequila. [[01:13:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4413.62s)]
*  Peter Teal got barricaded by these ding dongs at Cambridge. [[01:13:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4417.74s)]
*  Listen, people have the right to protest. [[01:13:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4421.46s)]
*  I think it's great. [[01:13:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4422.94s)]
*  People are protesting, but surrounding people and threatening them is a little [[01:13:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4423.46s)]
*  bit over the top and I think you're exaggerating what happened. [[01:13:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4427.219999999999s)]
*  Well, I don't know exactly what happens. [[01:13:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4431.219999999999s)]
*  All we see is these videos. [[01:13:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4432.5s)]
*  Look, they're not threatening anybody. [[01:13:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4434.0599999999995s)]
*  And I don't even think they tried to barricade him in. [[01:13:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4435.86s)]
*  They were just outside the building and because they were blocking the [[01:13:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4438.18s)]
*  driveway, his car couldn't leave, but he wasn't physically like locked in [[01:14:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4441.860000000001s)]
*  the building or something. [[01:14:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4448.18s)]
*  Yeah. [[01:14:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4449.5s)]
*  That's what the headlines say, but that could be fake news, fake social. [[01:14:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4450.46s)]
*  This was not on my bingo card. [[01:14:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4453.660000000001s)]
*  This pro-protest support by sacks was not on the bingo card. [[01:14:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4455.3s)]
*  I got to say. [[01:14:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4460.02s)]
*  The constitution, the constitution of the United States in the first amendment [[01:14:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4462.780000000001s)]
*  provides for the right of assembly, which includes protests and sit-ins. [[01:14:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4465.66s)]
*  So as long as they're, as long as they're peaceable. [[01:14:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4469.66s)]
*  Now, obviously if they go too far and they vandalize or break into buildings [[01:14:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4472.78s)]
*  or use violence, then that's not peaceable. [[01:14:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4477.34s)]
*  However, expressing sentiments with which you disagree does not make it violent. [[01:14:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4479.26s)]
*  And there's all these people out there now making the argument that if you hear [[01:14:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4485.82s)]
*  something from a protester that you don't like and you subjectively experience [[01:14:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4489.9s)]
*  that as a, as a threat to your safety, then that's somehow it should be treated [[01:14:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4495.78s)]
*  as valid, like that's basically violent. [[01:15:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4501.0599999999995s)]
*  Well, that's, that's not what the constitution says. [[01:15:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4503.299999999999s)]
*  And these people understood well, just a few months ago that that was basically [[01:15:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4507.099999999999s)]
*  snowflakery that, you know, just because somebody, you know, the rise of the woke [[01:15:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4511.54s)]
*  right now where they're buying into the woke, right. [[01:15:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4518.82s)]
*  They're buying into this idea of safetyism, which is being exposed to ideas [[01:15:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4521.9s)]
*  you don't like to protest. [[01:15:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4525.5s)]
*  You don't like is a threat to your safety. [[01:15:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4526.66s)]
*  No, it's not. [[01:15:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4528.3s)]
*  So now we have snowflakes on both sides. [[01:15:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4529.42s)]
*  Everybody's doing snowflakery on both sides. [[01:15:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4531.58s)]
*  Now it's ridiculous. [[01:15:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4534.219999999999s)]
*  The only thing I will say that I've seen and is this, this, this surrounding [[01:15:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4535.74s)]
*  individuals who you don't want there and locking them in a circle and then moving [[01:15:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4541.86s)]
*  them out of the processor. [[01:15:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4546.179999999999s)]
*  That's not cool. [[01:15:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4547.94s)]
*  Yeah, obviously you can't do that, but look, I think that most of the protests [[01:15:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4548.66s)]
*  on most of the campuses have not crossed the line. [[01:15:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4552.74s)]
*  They've just occupied the lawns of these campuses. [[01:15:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4554.7s)]
*  And look, I've seen some troublemakers try to barge through the encampments and [[01:15:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4557.42s)]
*  claim that because they can't go through there, that somehow they're being [[01:16:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4564.58s)]
*  prevented from going to class. [[01:16:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4568.22s)]
*  Look, you just walk around the lawn and you can get to class. [[01:16:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4569.58s)]
*  Okay. [[01:16:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4573.46s)]
*  And, you know, some of these videos are showing that these are effectively [[01:16:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4574.299999999999s)]
*  right-wing provocateurs who are engaging in left-wing tactics. [[01:16:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4579.139999999999s)]
*  And I don't support it either way. [[01:16:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4583.139999999999s)]
*  Some of these camps are some of the funniest things you've ever seen. [[01:16:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4586.94s)]
*  It's like, there are like at one tent that's dedicated to like a reading room [[01:16:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4589.5s)]
*  and you go in there and there's like these like center. [[01:16:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4594.78s)]
*  Oh my God. [[01:16:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4597.78s)]
*  It's unbelievably hilarious. [[01:16:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4598.299999999999s)]
*  Look, there's no question that because the protests are originating on the [[01:16:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4600.34s)]
*  left, that there's some goofy views. [[01:16:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4603.54s)]
*  Like, you know, you're dealing with like a left-wing idea complex, right? [[01:16:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4606.18s)]
*  But, and you know, it's easy to make fun of them doing different things. [[01:16:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4609.9800000000005s)]
*  But the fact of the matter is that most of the protests in most of these [[01:16:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4614.02s)]
*  campuses are, even though they can be annoying because they're occupying [[01:16:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4617.7s)]
*  part of the lawn, they're not violent. [[01:17:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4621.58s)]
*  Yeah. [[01:17:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4624.02s)]
*  And, you know, the way they're being cracked down on, they're sending the [[01:17:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4624.7s)]
*  police in at 5 a.m. [[01:17:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4627.06s)]
*  to crack down on these encampments with batons and riot gear. [[01:17:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4628.9800000000005s)]
*  And I find that part to be completely excessive. [[01:17:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4633.26s)]
*  Well, it's also dangerous because, you know, things can escalate when you have [[01:17:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4636.700000000001s)]
*  mobs of people and large groups of people. [[01:17:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4640.3s)]
*  So I just want to make sure people understand that large group of people, [[01:17:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4642.14s)]
*  you have a diffusion of responsibility that occurs when there's large groups of [[01:17:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4645.700000000001s)]
*  people who are passionate about things and people can get hurt. [[01:17:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4649.3s)]
*  People have gotten killed at these things. [[01:17:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4652.46s)]
*  So just, you know, keep it calm, everybody. [[01:17:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4654.02s)]
*  I agree with you. Like, what's the harm of these folks protesting on a lawn? [[01:17:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4656.58s)]
*  It's not a big deal when they break into buildings, of course. [[01:17:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4660.26s)]
*  Yeah, that crosses the line, obviously. [[01:17:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4662.42s)]
*  Yeah. But I mean, let them sit out there and then they'll run out their food [[01:17:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4663.9s)]
*  cars, their campus food cart and they run out of waffles. [[01:17:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4666.78s)]
*  Did you guys see the clip? [[01:17:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4671.0199999999995s)]
*  I think it was on the University of Washington campus where one kid [[01:17:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4671.94s)]
*  challenged this Antifa guy to a pushup contest. [[01:17:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4676.18s)]
*  Oh, fantastic. [[01:17:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4679.62s)]
*  I mean, it's some of the funniest stuff. [[01:18:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4681.82s)]
*  Some of the some content is coming out. [[01:18:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4683.94s)]
*  That's just my favorite was the woman who came out and said that [[01:18:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4685.46s)]
*  the Columbia students needed humanitarian aid. [[01:18:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4689.26s)]
*  Oh, my God. They overdubs on her were hilarious. [[01:18:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4692.54s)]
*  I was like, humanitarian aid from the coast. [[01:18:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4695.14s)]
*  I was like, we need our door dash right now. [[01:18:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4698.38s)]
*  We need to be double dashed some boba and we can't get it through the police. [[01:18:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4700.74s)]
*  We need our boba. [[01:18:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4704.7s)]
*  Low sugar boba with the popping boba bubbles wasn't getting in. [[01:18:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4706.18s)]
*  But, you know, people have the right to protest and peaceable, by the way. [[01:18:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4710.9s)]
*  There's a word I've never heard. [[01:18:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4714.46s)]
*  Very good sex. Peaceable, inclined to avoid argument or violent conflict. [[01:18:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4715.66s)]
*  Very nice. Well, it's in the Constitution. [[01:18:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4720.26s)]
*  It's in the First Amendment. [[01:18:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4722.3s)]
*  Is it really? I haven't heard the word peaceable before. [[01:18:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4723.78s)]
*  I mean, you and I are some pot of coal on this. [[01:18:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4726.14s)]
*  Like I don't. [[01:18:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4727.78s)]
*  We used to have the ACLU like [[01:18:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4729.58s)]
*  backing up the KKK going down Main Street and really fighting for. [[01:18:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4733.26s)]
*  Yeah, they were really fighting for. [[01:18:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4737.82s)]
*  And I have to say the Overton window is opened back up. [[01:18:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4739.74s)]
*  And I think it's great. All right. [[01:19:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4743.46s)]
*  We got some things on the docket here. [[01:19:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4744.900000000001s)]
*  I don't know if you guys saw the Apple new iPad ad. [[01:19:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4746.46s)]
*  It's getting a bunch of criticism. [[01:19:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4749.06s)]
*  They use like some giant. [[01:19:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4750.9400000000005s)]
*  Hydraulic press to crush a bunch of creative tools. [[01:19:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4753.46s)]
*  E.J. turntable trumpet piano. [[01:19:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4757.06s)]
*  People really care about Apple's ads and what they represent. [[01:19:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4759.9800000000005s)]
*  We talked about that mother earth little vignette they created here. [[01:19:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4764.02s)]
*  What do you think, Freyberg? You see the ad? [[01:19:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4768.820000000001s)]
*  What was your reaction to it? [[01:19:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4770.22s)]
*  Made me sad. It did not make me want to buy an iPad. [[01:19:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4771.82s)]
*  So, huh? [[01:19:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4774.34s)]
*  Did not seem like it made you sad. [[01:19:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4775.7s)]
*  Really? It actually elicited an emotion, meaning like commercials. [[01:19:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4777.9s)]
*  It's very rare that commercials can actually do that. [[01:19:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4781.74s)]
*  Most people just zone out. [[01:19:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4783.5s)]
*  Yeah, they took all this beautiful stuff and heard it. [[01:19:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4784.78s)]
*  Then it didn't feel good. [[01:19:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4786.78s)]
*  I don't know. Just didn't seem like a good ad. [[01:19:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4788.54s)]
*  I don't know why they did that. [[01:19:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4790.179999999999s)]
*  I don't get it. I don't know. [[01:19:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4791.5s)]
*  I think I think maybe what they're trying to do is the selling point [[01:19:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4793.259999999999s)]
*  of this new iPad is that it's the thinnest one. [[01:19:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4796.58s)]
*  I mean, there's no innovation left. [[01:19:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4799.5s)]
*  So they're just making the devices thinner. [[01:20:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4800.74s)]
*  Yeah. [[01:20:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4803.66s)]
*  So I think the idea was that they're going to take this hydraulic press [[01:20:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4804.7s)]
*  to represent how ridiculously thin the new iPad is. [[01:20:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4807.98s)]
*  Now, I don't know if the point there was to smush all of that good stuff [[01:20:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4812.099999999999s)]
*  into the iPad. [[01:20:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4816.62s)]
*  I don't know if that's what they were trying to convey. [[01:20:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4817.66s)]
*  But yeah, I think that by destroying all those creative tools [[01:20:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4820.099999999999s)]
*  that Apple is supposed to represent, it definitely seemed very off brand for them. [[01:20:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4825.3s)]
*  And I think people were reacting to the fact that it was so different [[01:20:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4829.7s)]
*  than what they would have done in the past. [[01:20:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4834.74s)]
*  And of course, everyone was saying, well, Steve would never have done this. [[01:20:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4836.139999999999s)]
*  I do think it did land wrong. [[01:20:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4839.98s)]
*  I mean, I I didn't care that much. [[01:20:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4841.5s)]
*  But but I was kind of asking the question, like, why are they destroying [[01:20:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4843.9s)]
*  all these creator tools that they're renowned for creating [[01:20:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4847.54s)]
*  or for turning into the digital version? [[01:20:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4853.0599999999995s)]
*  Yeah, it just didn't land. [[01:20:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4856.179999999999s)]
*  I mean, Chamath, how are you doing emotionally after seeing that? [[01:20:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4858.1s)]
*  Are you OK, buddy? Yeah. [[01:21:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4863.14s)]
*  I think this is you guys see that in the Berkshire [[01:21:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4866.14s)]
*  annual meeting last weekend. [[01:21:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4871.14s)]
*  Tim Cook was in the audience and Buffett. [[01:21:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4873.900000000001s)]
*  Was very laudatory. [[01:21:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4878.1s)]
*  This is an incredible company. [[01:21:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4879.26s)]
*  But he's so clever with words, he's like, you know, this is an incredible business [[01:21:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4881.38s)]
*  that we will hold forever, most likely. [[01:21:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4886.26s)]
*  And it turns out that he sold 20 billion dollars with Apple shares. [[01:21:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4890.46s)]
*  We're going to hold it forever. [[01:21:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4895.78s)]
*  Which, by the way, so if you guys remember, we put that little chart up, [[01:21:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4897.22s)]
*  which shows when he doesn't mention it in the in the annual letter, [[01:21:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4901.0599999999995s)]
*  picked up basically like it's foreshadowing the fact that he is just pounding the cell. [[01:21:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4904.46s)]
*  And he sold 20 billion dollars. [[01:21:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4909.18s)]
*  Well, also holding it forever could mean one share. [[01:21:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4911.74s)]
*  Yeah, exactly. We kind of need to know, like, how much are we talking about? [[01:21:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4914.86s)]
*  I mean, it's an incredible business that has so much money with nothing to do. [[01:21:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4919.179999999999s)]
*  They're probably just going to buy back the stock. [[01:22:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4923.179999999999s)]
*  Just a total waste. [[01:22:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4925.54s)]
*  There were floating this rumor of buying Rivian after they shut down [[01:22:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4926.58s)]
*  Titan Project, the internal project to make a car. [[01:22:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4929.78s)]
*  It seems like a car is the only thing people can think of that would [[01:22:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4932.0599999999995s)]
*  move the needle in terms of earnings. [[01:22:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4935.7s)]
*  I think the problem is, Jay, like you kind of become afraid of your own shadow, [[01:22:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4937.42s)]
*  meaning the folks that are really good at M&A, [[01:22:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4940.78s)]
*  like you look at Benioff, the thing with Benioff's M&A strategy [[01:22:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4943.54s)]
*  is that he's been doing it for 20 years. [[01:22:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4947.54s)]
*  And so he's cut his teeth on small acquisitions [[01:22:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4950.9s)]
*  and the market learns to give him trust [[01:22:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4954.42s)]
*  so that when he proposes like the twenty seven billion dollar [[01:22:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4957.34s)]
*  slack acquisition, he's allowed to do that. [[01:22:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4959.82s)]
*  Another guy, you know, Nikesh Arora at Pan W these last five years. [[01:22:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4962.94s)]
*  People were very skeptical that he could actually roll up security [[01:22:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4966.74s)]
*  because it was a super fragmented market. [[01:22:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4969.7s)]
*  He's gotten permission. [[01:22:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4971.86s)]
*  Then there are companies like Danahard that buy hundreds of companies. [[01:22:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4973.299999999999s)]
*  So all of these folks are examples of you start small and you [[01:22:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4976.259999999999s)]
*  you earn the right to do more. [[01:22:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4979.5s)]
*  Apple hasn't bought anything more than 50 or 100 million dollars. [[01:23:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4981.54s)]
*  And so the idea that all of a sudden they come out of the blue and buy a [[01:23:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4984.7s)]
*  10, 20 billion dollar company, I think, is just totally doesn't stand logic. [[01:23:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4988.34s)]
*  It's just not possible for them because they'll be so afraid of their own shadow. [[01:23:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4993.42s)]
*  That's the big problem. It's themselves. [[01:23:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4996.5s)]
*  Well, if you're running out of in-house innovation and you can't do M&A, [[01:23:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=4999.1s)]
*  then your options are kind of limited. [[01:23:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5003.820000000001s)]
*  I mean, I do think that the fact that the big news out of Apple is the iPads [[01:23:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5005.54s)]
*  getting thinner does represent kind of the end of the road in terms of innovation. [[01:23:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5009.620000000001s)]
*  It's kind of like when they added the third camera to the iPhone. [[01:23:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5014.34s)]
*  Yeah, it reminds me of those. [[01:23:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5018.860000000001s)]
*  Remember, like when the Gillette? [[01:23:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5020.54s)]
*  Yeah, they did the three came out and then they did. [[01:23:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5022.1s)]
*  It was the best onion thing was like, we're doing five effort. [[01:23:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5023.860000000001s)]
*  But then I actually came out with the Mach five. [[01:23:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5027.5s)]
*  So, yeah, like the parody became the reality. [[01:23:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5029.94s)]
*  What are they going to do add two more cameras to the iPhone? [[01:23:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5032.14s)]
*  You have five cameras on it. [[01:23:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5034.58s)]
*  Right. Makes no sense. [[01:23:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5036.34s)]
*  And then I don't know anybody wants to remember the Apple [[01:23:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5037.38s)]
*  vision was like going to plus why are they changing the fat iPads? [[01:24:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5040.46s)]
*  That's fair point. Fair point. [[01:24:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5046.22s)]
*  Actually, you know what? It's actually this didn't come out yet. [[01:24:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5047.78s)]
*  But it turns out the iPad is on no Zempik. [[01:24:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5051.26s)]
*  It's actually dropped. [[01:24:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5053.54s)]
*  That would have been a funnier ad. [[01:24:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5055.42s)]
*  Yeah, exactly. Oh, oh, oh. [[01:24:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5057.02s)]
*  We could just workshop that right here. [[01:24:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5061.82s)]
*  But there's another funny one which was making the iPhone smaller [[01:24:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5063.34s)]
*  and smaller and smaller and the iPod smaller and smaller and smaller. [[01:24:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5066.02s)]
*  To the point it was like, you know, like a thumb size iPhone, [[01:24:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5068.3s)]
*  like the Ben Stiller phone in Zulander. [[01:24:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5071.26s)]
*  Correct. [[01:24:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5074.7s)]
*  That was a great scene. [[01:24:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5078.34s)]
*  Is there a category that you can think of that you would love an Apple product for? [[01:24:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5079.62s)]
*  There's a product in your life that you would love to have Apple's version of it. [[01:24:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5085.26s)]
*  They killed it. [[01:24:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5090.7s)]
*  I think a lot of people would be very open minded to an Apple car. [[01:24:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5092.54s)]
*  OK, they just would. [[01:24:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5096.42s)]
*  It's a connected Internet device, increasingly so. [[01:24:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5097.780000000001s)]
*  Yeah. And they managed to flub it. [[01:25:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5101.54s)]
*  They had a chance to buy Tesla. [[01:25:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5104.9800000000005s)]
*  They managed to flub it. Yeah. [[01:25:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5106.66s)]
*  Right. There are just too many examples here where these guys have so much money [[01:25:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5108.860000000001s)]
*  and not enough ideas. That's a shame. [[01:25:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5112.38s)]
*  It's a bummer. Yeah. [[01:25:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5115.5s)]
*  The one I always wanted to see them do, Zach, was TV. [[01:25:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5116.780000000001s)]
*  The one I always wanted to see them do was the TV. [[01:25:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5119.9800000000005s)]
*  And they were supposedly working on it like the actual TV, [[01:25:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5121.66s)]
*  not the little Apple TV box in the back. [[01:25:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5124.66s)]
*  And that would have been extraordinary to actually have a gorgeous, [[01:25:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5126.62s)]
*  you know, big television. What about a gaming console? [[01:25:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5130.9800000000005s)]
*  They could have done that. [[01:25:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5133.34s)]
*  You know, there's just all these things that they could have done. [[01:25:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5134.38s)]
*  It's not a lack of imagination because these aren't exactly, [[01:25:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5138.66s)]
*  incredibly world beating ideas. [[01:25:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5141.9800000000005s)]
*  They're sitting right in front of your face. [[01:25:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5144.34s)]
*  It's just the will to do it. [[01:25:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5145.9400000000005s)]
*  Yeah. Yeah. [[01:25:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5148.26s)]
*  They all in one TV would have been good. [[01:25:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5149.38s)]
*  If you think back on. [[01:25:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5151.900000000001s)]
*  Apple's product lineup over the years, [[01:25:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5153.9800000000005s)]
*  where they've really created value is on how unique the products are. [[01:25:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5156.34s)]
*  They almost create new categories. [[01:26:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5160.26s)]
*  Sure, there may have been a quote tablet computer prior to the iPad, [[01:26:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5162.02s)]
*  but the iPad really defined the tablet computer era. [[01:26:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5165.46s)]
*  Sure, there was a smartphone or two before the iPhone came along, [[01:26:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5168.38s)]
*  but it really defined the smartphone. [[01:26:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5171.26s)]
*  And sure, there was a computer before the Apple too. [[01:26:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5173.46s)]
*  And then it came along and it defined the personal computer. [[01:26:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5175.62s)]
*  In all these cases, I think Apple strives to define the category. [[01:26:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5178.1s)]
*  So it's very hard to define a television if you think about it or a gaming console [[01:26:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5182.38s)]
*  in a way that you take a step up and you say, this is the new thing. [[01:26:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5185.86s)]
*  This is the new platform. [[01:26:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5189.46s)]
*  So I don't know. That's the lens I would look at if I'm Apple in terms of like, [[01:26:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5191.54s)]
*  can I redefine a car? [[01:26:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5194.74s)]
*  Can I make, you know, we're all trying to fit them into an existing product bucket. [[01:26:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5196.26s)]
*  But I think what they've always been so good at is identifying consumer needs. [[01:26:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5199.94s)]
*  And then creating an entirely new way of addressing that need in a real step [[01:26:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5203.46s)]
*  change function from the like the the iPod. [[01:26:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5206.66s)]
*  It was so different from any MP3 player ever. [[01:26:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5210.42s)]
*  I think the reason why the car could have been completely reimagined by Apple [[01:26:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5212.86s)]
*  is that they have a level of credibility and trust [[01:26:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5216.5s)]
*  that I think probably no other company has and absolutely no other tech company has. [[01:26:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5219.7s)]
*  And we talked about this, but I think this was the third [[01:27:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5225.14s)]
*  Steve Jobs story that I left out. [[01:27:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5229.7s)]
*  But in 2000 and I don't know, was it one? [[01:27:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5231.86s)]
*  I launched a 99 cent download store. [[01:27:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5238.5s)]
*  I think I've told you this story in Winamp and. [[01:27:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5241.54s)]
*  Steve Jobs just ran total circles around us, but the reason he was able to is he had all [[01:27:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5246.179999999999s)]
*  the credibility to go to the labels and get deals done for licensing music [[01:27:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5250.339999999999s)]
*  that nobody could get done before. [[01:27:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5254.82s)]
*  I think that's an example of what Apple is able to do, which is to use their political [[01:27:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5256.74s)]
*  capital to change the rules. [[01:27:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5260.14s)]
*  So if the thing that we would all want is safer roads and autonomous vehicles, there [[01:27:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5262.22s)]
*  are regions in every town and city that could be completely converted to level five [[01:27:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5267.9400000000005s)]
*  autonomous zones. [[01:27:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5273.26s)]
*  If I had to pick one company that had the credibility to go and change those rules, [[01:27:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5275.3s)]
*  it's them because they could demonstrate that there was a methodical, safe approach [[01:27:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5278.860000000001s)]
*  to doing something. And so the point is that even in these categories that could be [[01:28:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5284.34s)]
*  totally reimagined, it's not for a lack of imagination. [[01:28:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5288.74s)]
*  Again, it just goes back to a complete lack of will. [[01:28:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5291.34s)]
*  And I understand because if they had if you if you had two hundred billion dollars of [[01:28:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5293.42s)]
*  capital on your balance sheet, I think it's probably pretty easy to get fat and lazy. [[01:28:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5298.86s)]
*  Yeah, it is. And and they want to have everything built there. [[01:28:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5303.98s)]
*  People don't remember, but they actually built one of the first digital cameras. [[01:28:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5307.099999999999s)]
*  You must have owned this right, Friedberg? [[01:28:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5310.38s)]
*  You're like, I remember this. Yeah, totally. [[01:28:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5311.74s)]
*  It's beautiful. What did they call it? [[01:28:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5313.98s)]
*  Was it the eye camera or something? [[01:28:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5315.62s)]
*  Quick, quick, quick, quick, quick. [[01:28:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5317.06s)]
*  Take. Yeah. And the thing I would like to see Apple build, and I'm surprised they didn't, [[01:28:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5319.1s)]
*  was a smart home system, the way Apple has Nest, a drop cam, a door lock, you know, AV [[01:28:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5323.580000000001s)]
*  system, go after Questron or whatever, and just have your whole home automated thermostat [[01:28:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5332.14s)]
*  nest. All of that would be brilliant by Apple. [[01:28:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5338.02s)]
*  And right now, I'm an Apple family that has our all of our home automation through Google. [[01:29:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5340.18s)]
*  So it's just kind of sucks. [[01:29:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5346.42s)]
*  I would like that all to be pretty amazing. [[01:29:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5348.22s)]
*  Like if they did a Crestron or Savant, because then when you just go to your Apple TV, all [[01:29:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5350.5s)]
*  your cameras just work. [[01:29:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5354.62s)]
*  You don't need to. Yes. [[01:29:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5355.78s)]
*  That's that. I mean, and everybody has a home and everybody automates their home. [[01:29:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5358.18s)]
*  So everyone has Apple TV at this point. [[01:29:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5362.14s)]
*  So you just make Apple TV the brain for the home system. [[01:29:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5364.46s)]
*  Right. That would be your home. [[01:29:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5368.5s)]
*  And you can connect your phone to it. [[01:29:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5370.3s)]
*  And then, yes, that would be very nice. [[01:29:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5372.18s)]
*  Yeah. Like, can you imagine like the ring cameras, all that stuff being integrated? [[01:29:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5374.42s)]
*  I don't know why they didn't go after that. [[01:29:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5378.9400000000005s)]
*  That seems like the easy layup. [[01:29:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5380.18s)]
*  Hey, you know, everybody's been talking, Friedberg, about this alpha fold, this folding proteins. [[01:29:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5381.820000000001s)]
*  And there's some new version out from Google and also Google reportedly, we talked about [[01:29:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5390.900000000001s)]
*  this before, is also advancing talks to acquire HubSpot. [[01:29:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5396.5s)]
*  So that rumor for the 30 billion dollar market cap HubSpot is out there as well. [[01:29:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5399.06s)]
*  Friedberg, you're as our resident science sultan, our resident sultan of science and [[01:30:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5403.46s)]
*  as a Google alumni. [[01:30:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5410.82s)]
*  Pick either story and let's go for it. [[01:30:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5413.06s)]
*  Yeah, I mean, I'm not sure there's much more to add on the HubSpot acquisition rumors. [[01:30:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5414.66s)]
*  They are still just rumors. [[01:30:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5417.62s)]
*  And I think we covered the topic a couple of weeks ago. [[01:30:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5418.82s)]
*  But I will say that Alpha Fold 3 that was just announced today and demonstrated by Google [[01:30:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5421.22s)]
*  is a real, I would say breathtaking moment for biology, for bioengineering, for human [[01:30:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5426.42s)]
*  health, for medicine. [[01:30:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5434.42s)]
*  And maybe I'll just take 30 seconds to kind of explain it. [[01:30:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5436.099999999999s)]
*  You remember when they introduced Alpha Fold, Alpha Fold 2, we talked about DNA codes for [[01:30:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5439.54s)]
*  proteins. [[01:30:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5445.14s)]
*  So every three letters of DNA codes for an amino acid. [[01:30:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5445.78s)]
*  So a string of DNA codes for a string of amino acids. [[01:30:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5449.46s)]
*  And that's called a gene that produces a protein. [[01:30:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5453.7s)]
*  And that protein is basically a long like think about beads. [[01:30:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5456.66s)]
*  There's 20 different types of beads, 20 different amino acids that can be strung together. [[01:30:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5459.94s)]
*  And what happens is that necklace, that bead necklace basically collapses on itself and [[01:31:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5464.58s)]
*  all those little beads stick together with each other in some complicated way that we [[01:31:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5469.0599999999995s)]
*  can't deterministically model. [[01:31:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5472.58s)]
*  And that creates a three dimensional structure, which is called a protein, that molecule. [[01:31:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5474.5s)]
*  And that molecule does something interesting. [[01:31:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5478.82s)]
*  It can break apart other molecules. [[01:31:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5480.9s)]
*  It can bind molecules. [[01:31:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5482.82s)]
*  It can move molecules around. [[01:31:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5483.94s)]
*  So it's basically the machinery of chemistry, of biochemistry. [[01:31:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5484.98s)]
*  And so proteins are what is encoded in our DNA. [[01:31:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5489.0599999999995s)]
*  And then the proteins do all the work of making living organisms. [[01:31:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5492.58s)]
*  So Google's Alpha Fold project took three dimensional images of proteins and the DNA [[01:31:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5496.259999999999s)]
*  sequence that codes for those proteins. [[01:31:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5500.98s)]
*  And then they built a predictive model that predicted the three dimensional structure [[01:31:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5502.82s)]
*  of a protein from the DNA that codes for it. [[01:31:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5506.74s)]
*  And that was a huge breakthrough years ago. [[01:31:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5509.62s)]
*  What they just announced with Alpha Fold 3 today is that they're now including all [[01:31:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5511.62s)]
*  small molecules. [[01:31:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5516.5s)]
*  So all the other little molecules that go into chemistry and biology that drive the [[01:31:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5517.38s)]
*  function of everything we see around us and the way that all those molecules actually [[01:32:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5521.94s)]
*  bind and fit together is part of the predictive model. [[01:32:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5526.9s)]
*  Why is that important? [[01:32:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5530.74s)]
*  Well, let's say that you're designing a new drug and it's a protein based drug, which [[01:32:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5531.7s)]
*  biologic drugs, which most drugs are today. [[01:32:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5535.14s)]
*  You could find a biologic drug that binds to a cancer cell. [[01:32:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5536.98s)]
*  And then you'll spend 10 years going to clinical trials and billions of dollars later, you [[01:32:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5540.34s)]
*  find out that that protein accidentally binds to other stuff and hurts other stuff in the [[01:32:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5544.66s)]
*  body. [[01:32:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5549.0599999999995s)]
*  And that's an off target effect or a side effect. [[01:32:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5549.38s)]
*  And that drug is pulled from the clinical trials and it never goes to market. [[01:32:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5551.62s)]
*  Most drugs go through that process. [[01:32:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5555.0599999999995s)]
*  They are actually tested in animals and then in humans. [[01:32:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5557.3s)]
*  And we find all these side effects that arise from those drugs because we don't know how [[01:32:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5560.98s)]
*  those drugs are going to bind or interact with other things in our biochemistry. [[01:32:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5565.14s)]
*  And we only discovered after we put it in. [[01:32:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5569.780000000001s)]
*  But now we can actually model that with software. [[01:32:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5571.860000000001s)]
*  We can take that drug, we can create a three dimensional representation of it using the [[01:32:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5574.58s)]
*  software and we can model how that drug might interact with all the other cells, all the [[01:32:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5578.1s)]
*  other proteins, all the other small molecules in the body to find all the off target effects [[01:33:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5582.900000000001s)]
*  that may arise and decide whether or not that presents a good drug candidate. [[01:33:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5587.700000000001s)]
*  That is one example of how this capability can be used. [[01:33:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5592.18s)]
*  And there are many, many others, including creating new proteins that could be used to [[01:33:16](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5596.18s)]
*  bind molecules or stick molecules together or new proteins that could be designed to [[01:33:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5600.9s)]
*  rip molecules apart. [[01:33:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5604.82s)]
*  We can now predict the function of three dimensional molecules using this capability, [[01:33:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5606.42s)]
*  which opens up all of the software based design of chemistry, of biology, of drugs. [[01:33:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5612.0199999999995s)]
*  And it really is an incredible breakthrough moment. [[01:33:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5617.86s)]
*  The interesting thing that happened though is Google alphabet has a subsidiary called [[01:33:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5620.58s)]
*  isomorphic labs. [[01:33:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5625.3s)]
*  It is a drug development subsidiary of alphabet and they've basically kept all the IP for [[01:33:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5626.82s)]
*  alpha fold three in isomorphic. [[01:33:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5632.179999999999s)]
*  So Google is going to monetize the heck out of this capability. [[01:33:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5634.42s)]
*  And what they made available was not open source code, but a web based viewer that scientists [[01:33:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5638.179999999999s)]
*  for quote non-commercial purposes can use to do some fundamental research in a web based viewer [[01:34:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5643.139999999999s)]
*  and make some experiments and try stuff out and how interactions might occur. [[01:34:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5647.86s)]
*  But no one can use it for commercial use. [[01:34:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5651.299999999999s)]
*  Only Google's isomorphic labs can. [[01:34:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5653.46s)]
*  So number one, it's an incredible demonstration of what AI outside of LLMs, which we just [[01:34:15](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5655.46s)]
*  talked about with Sam today. [[01:34:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5661.0599999999995s)]
*  And obviously we talked about other models, but LLMs being kind of this consumer text [[01:34:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5663.0599999999995s)]
*  predictive model capability. [[01:34:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5667.46s)]
*  But outside of that, there's this capability in things like chemistry with these new AI [[01:34:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5668.9s)]
*  models that can be trained and built to predict things like three-dimensional chemical interactions [[01:34:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5674.02s)]
*  that is going to open up an entirely new era for human progress. [[01:34:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5680.02s)]
*  And I think that's what's so exciting. [[01:34:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5684.26s)]
*  I think the other side of this is Google is hugely advantaged and they just showed the [[01:34:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5685.540000000001s)]
*  world a little bit about some of these jewels that they have in the treasure chest. [[01:34:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5688.9800000000005s)]
*  And they're like, look at what we got. [[01:34:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5692.02s)]
*  We're going to make all these drugs and they've got partnerships with all these pharma companies [[01:34:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5693.3s)]
*  at isomorphic labs that they've talked about. [[01:34:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5696.660000000001s)]
*  And it's going to usher in a new era of drug development design for human health. [[01:34:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5699.22s)]
*  So all in all, I'd say it's a pretty astounding day. [[01:35:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5703.46s)]
*  A lot of people are going crazy over the capability that they just demonstrated. [[01:35:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5705.78s)]
*  And then it begs all this really interesting question around like, you know, what's Google [[01:35:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5709.14s)]
*  going to do with it and how much value is going to be created here? [[01:35:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5712.82s)]
*  So anyway, I thought it was a great story and I just rambled on for a couple of minutes, [[01:35:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5714.66s)]
*  but I don't know. [[01:35:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5718.26s)]
*  It's pretty cool. [[01:35:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5719.14s)]
*  Super interesting. [[01:35:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5719.46s)]
*  Is this AI capable of making a science corner that David Sachs pays attention to? [[01:35:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5720.5s)]
*  Well, it will predict the cure, I think, for the common cold and for herpes. [[01:35:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5726.18s)]
*  So he should pay attention. [[01:35:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5730.9800000000005s)]
*  Folding cells is the app that casual game Sachs just download is playing. [[01:35:34](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5734.26s)]
*  How many chess moves did you make during that segment? [[01:35:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5738.9s)]
*  Sorry, let me just say one more thing. [[01:35:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5742.1s)]
*  Do you guys remember we talked about Yamanaka factors and how challenging it is to basically [[01:35:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5743.06s)]
*  we can reverse aging if we can get the right proteins into cells to tune the expression [[01:35:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5747.22s)]
*  of certain genes to make those cells youthful. [[01:35:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5752.9800000000005s)]
*  Right now, it's a shotgun approach to trying millions of compounds and combinations of [[01:35:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5755.7s)]
*  compounds to do them. [[01:36:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5760.34s)]
*  There's a lot of companies actually trying to do this right now. [[01:36:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5760.9800000000005s)]
*  To come up with a fountain of youth type product. [[01:36:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5763.46s)]
*  We can now simulate that. [[01:36:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5766.42s)]
*  So with this system, one of the things that this Alpha Fold 3 can do is predict what molecules [[01:36:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5768.1s)]
*  will bind and promote certain sequences of DNA, which is exactly what we try and do with [[01:36:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5773.38s)]
*  the Yamanaka factor based expression systems and find ones that won't trigger off target [[01:36:17](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5777.86s)]
*  expression. [[01:36:22](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5782.58s)]
*  So meaning we can now go through the search space and software of creating a combination [[01:36:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5783.3s)]
*  of molecules that theoretically could unlock this fountain of youth to de-age all the cells [[01:36:28](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5788.26s)]
*  in the body and introduce an extraordinary kind of health benefit. [[01:36:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5793.22s)]
*  And that's just, again, one example of the many things that are possible with this sort [[01:36:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5796.58s)]
*  of platform. [[01:36:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5800.18s)]
*  I got to be honest, I'm really just sort of skimming the surface here of what this can [[01:36:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5801.62s)]
*  do. [[01:36:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5805.14s)]
*  The capabilities and the impact are going to be like, I don't know, I know I say this [[01:36:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5806.42s)]
*  sort of stuff a lot, but it's going to be pretty profound. [[01:36:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5810.5s)]
*  There's on the blog post, they have this incredible video that they show of the [[01:36:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5812.34s)]
*  coronavirus that creates a common cold, I think the 7-PNM protein. [[01:36:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5817.139999999999s)]
*  And not only did they literally like predict it accurately, they also predicted how it [[01:37:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5822.82s)]
*  interacts with an antibody, with a sugar. [[01:37:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5829.22s)]
*  It's nuts. [[01:37:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5832.98s)]
*  So you could see a world where like, I don't know, you just get a vaccine for the cold [[01:37:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5833.62s)]
*  and it's kind of like you never have colds again. [[01:37:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5838.18s)]
*  Amazing. [[01:37:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5840.26s)]
*  I mean, simple stuff, but so powerful. [[01:37:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5841.22s)]
*  And you can filter out stuff that has off-target effects. [[01:37:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5843.38s)]
*  So so much of drug discovery and all the side effects stuff can start to be solved [[01:37:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5845.46s)]
*  for in silico. [[01:37:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5849.46s)]
*  And you could think about running extraordinarily large, use a model like this, run extraordinarily [[01:37:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5850.82s)]
*  large simulations in a search space of chemistry to find stuff that does things in the body [[01:37:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5855.62s)]
*  that can unlock all these benefits, can do all sorts of amazing things to destroy cancer, [[01:37:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5861.78s)]
*  to destroy viruses, to repair cells, to DH cells. [[01:37:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5866.9s)]
*  And this is a hundred billion dollar business, they say. [[01:37:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5870.9800000000005s)]
*  Oh my God. [[01:37:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5873.3s)]
*  I mean, this alone, I feel like this is where I, I've said this before. [[01:37:53](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5873.78s)]
*  I think Google's got this like portfolio of like quiet, you know, the other, yeah. [[01:37:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5877.94s)]
*  Yeah. [[01:38:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5883.7s)]
*  What if they hit? [[01:38:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5883.78s)]
*  And the fact, and I think the fact that they didn't open source everything in this says [[01:38:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5884.82s)]
*  a lot about their intentions. [[01:38:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5888.82s)]
*  Yeah. [[01:38:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5889.94s)]
*  Yeah. [[01:38:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5890.259999999999s)]
*  Open source when you're behind closed source, lock it up when you're ahead. [[01:38:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5890.5s)]
*  But the Yamanaka actually, interestingly, Yamanaka is the Japanese whiskey that Saks [[01:38:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5894.74s)]
*  serves on his plate as well. [[01:38:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5899.54s)]
*  It's delicious. [[01:38:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5900.82s)]
*  I love that Hokkaido. [[01:38:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5901.46s)]
*  Jason, if you didn't find your way to Silicon Valley, you could be like a Vegas lounge comedy [[01:38:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5904.1s)]
*  guy. [[01:38:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5909.38s)]
*  Absolutely. [[01:38:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5909.7s)]
*  Yeah. [[01:38:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5910.5s)]
*  For sure. [[01:38:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5910.58s)]
*  Yeah. [[01:38:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5911.06s)]
*  I was actually, yeah, somebody said I should do like those 1950s talk shows where the guys [[01:38:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5911.3s)]
*  would do like the stage show. [[01:38:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5916.18s)]
*  The shtick. [[01:38:37](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5917.62s)]
*  Yeah. [[01:38:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5918.18s)]
*  Somebody told me I should do like Spaulding Gray, Eric Boghossian style stuff. [[01:38:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5918.42s)]
*  I don't know if you guys remember like the, the monologue is from the eighties in New York. [[01:38:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5922.9s)]
*  It's like, oh, that's interesting. [[01:38:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5926.9s)]
*  Maybe. [[01:38:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5927.7s)]
*  All right, everybody. [[01:38:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5928.02s)]
*  Thanks for tuning in to the world's number one podcast. [[01:38:48](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5928.82s)]
*  Can you believe we did it, Chema? [[01:38:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5932.74s)]
*  The number one podcast in the world and the All In Summit, the Ted Killer. [[01:38:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5934.34s)]
*  If you are going to Ted, congratulations for genuflecting. [[01:39:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5940.259999999999s)]
*  If you want to talk about real issues, come to the All In Summit. [[01:39:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5944.5s)]
*  And if you are protesting at the All In Summit, let us know what mock meet you would like to [[01:39:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5948.0199999999995s)]
*  have. [[01:39:13](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5953.78s)]
*  Freeberg is setting up mock meet stations for all of our protesters and what milk you [[01:39:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5954.18s)]
*  would like. [[01:39:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5958.9800000000005s)]
*  Yeah. [[01:39:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5959.62s)]
*  All vegan. [[01:39:19](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5959.780000000001s)]
*  If you want, if you're oat milk, soy, nut milk, just please, when you come to protest. [[01:39:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5960.26s)]
*  We have five different kinds of xanthan gum you can choose from. [[01:39:24](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5964.34s)]
*  Right, Chema? [[01:39:26](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5966.820000000001s)]
*  All of the nut milks you want and then there'll be mindful yoga with them. [[01:39:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5967.3s)]
*  Can we have some soy like? [[01:39:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5971.38s)]
*  Yes. [[01:39:32](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5972.34s)]
*  On the south lawn, we'll have the goat yoga going on. [[01:39:33](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5973.38s)]
*  So just please note that the goat yoga will be on for all of you. [[01:39:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5976.18s)]
*  It's very thoughtful for you to make sure that our protesters are going to be well-fed, [[01:39:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5980.5s)]
*  well-taken care of. [[01:39:45](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5985.7s)]
*  Yes. [[01:39:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5986.74s)]
*  We're actually, Freeberg is working on the protestor gift bags. [[01:39:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5987.219999999999s)]
*  The protestor gift bags. [[01:39:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5990.66s)]
*  They're made of Yakutama folding proteins. [[01:39:52](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5992.82s)]
*  So you're good. [[01:39:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5995.94s)]
*  A folding proteins. [[01:39:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5997.7s)]
*  I think I saw them open for the Smashing Pumpkins in 2003. [[01:39:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=5998.5s)]
*  On fire. [[01:40:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6003.139999999999s)]
*  Enough. [[01:40:03](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6003.54s)]
*  I'll be here for three more nights. [[01:40:05](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6005.139999999999s)]
*  Love you, boys. [[01:40:06](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6006.74s)]
*  Bye-bye. [[01:40:07](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6007.54s)]
*  Love you, besties. [[01:40:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6008.0199999999995s)]
*  Is this the All-In-Pot or open mic night? [[01:40:08](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6008.82s)]
*  What's going on? [[01:40:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6010.58s)]
*  It's basically it. [[01:40:11](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6011.219999999999s)]
*  I'm just bored. [[01:40:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6012.42s)]
*  I'm playing a dog taking an English in your driveway. [[01:40:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6038.82s)]
*  Sex. [[01:40:40](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6040.74s)]
*  Oh, man. [[01:40:41](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6041.219999999999s)]
*  Oh, man. [[01:40:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6043.62s)]
*  My habitat sure will meet me at once. [[01:40:44](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6044.42s)]
*  We should all just get a room and just have one big huge orgy because they're all just [[01:40:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6046.42s)]
*  useless. [[01:40:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6049.7s)]
*  It's like this sexual tension that they just need to release somehow. [[01:40:50](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6050.259999999999s)]
*  What? [[01:40:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6054.42s)]
*  You're the bee. [[01:40:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6054.82s)]
*  What? [[01:40:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6056.259999999999s)]
*  You're the bee. [[01:40:56](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6056.66s)]
*  You're the bee. [[01:40:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6057.299999999999s)]
*  Bee. [[01:40:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6057.86s)]
*  What? [[01:40:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6058.5s)]
*  That's gonna be good. [[01:40:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6058.9s)]
*  We need to get merch. [[01:40:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6059.7s)]
*  Besties are back. [[01:41:00](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6060.5s)]
*  I'm doing all this. [[01:41:01](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6061.219999999999s)]
*  All right. [[01:41:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6069.78s)]
*  That's episode 178. [[01:41:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6070.66s)]
*  And now the plugs. [[01:41:12](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6072.259999999999s)]
*  The All-In Summit is taking place in Los Angeles on September 8th through the 10th. [[01:41:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6074.0199999999995s)]
*  You can apply for a ticket at summit.allinpodcast.co. [[01:41:18](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6078.5s)]
*  Scholarships will be coming soon. [[01:41:23](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6083.38s)]
*  If you want to see the four of us interview Sam Altman, you can actually see the video [[01:41:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6085.54s)]
*  of this podcast on YouTube, youtube.com slash at all in or just search All-In Podcast and [[01:41:30](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6090.0199999999995s)]
*  hit the alert bell and you'll get updates when we post. [[01:41:36](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6096.66s)]
*  We're doing a Q&A episode live when the YouTube channel hits 500,000 and we're going to do [[01:41:39](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6099.78s)]
*  a party in Vegas. [[01:41:46](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6106.099999999999s)]
*  My understanding when we hit a million subscribers and look for that as well. [[01:41:47](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6107.94s)]
*  You can follow us on X, X.com slash the all in pod. [[01:41:51](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6111.7s)]
*  Tick tock is all underscore in underscore talk, Instagram, the all in pod and on LinkedIn [[01:41:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6115.86s)]
*  just search for the all in podcast. [[01:42:02](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6122.0199999999995s)]
*  You can follow Chamath at X.com slash Chamath and you can sign up for a sub stack at Chamath [[01:42:04](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6124.02s)]
*  dot sub stack dot com. [[01:42:09](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6129.3s)]
*  I do free, but it can be followed at X.com slash free Berg and Ohalo is hiring. [[01:42:10](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6130.42s)]
*  Click on the careers page at Ohalo genetics.com and you can follow sacks at X.com slash David [[01:42:14](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6134.660000000001s)]
*  sacks. [[01:42:20](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6140.9800000000005s)]
*  Sacks recently spoke at the American moment conference and people are going crazy for [[01:42:21](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6141.540000000001s)]
*  it. [[01:42:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6145.860000000001s)]
*  It's into his tweet on his ex profile. [[01:42:25](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6145.9400000000005s)]
*  I'm Jason Calacanis. [[01:42:27](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6147.9400000000005s)]
*  I am X.com slash Jason. [[01:42:29](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6149.780000000001s)]
*  And if you want to see pictures of my Bulldogs and the food I'm eating, go to Instagram dot [[01:42:31](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6151.78s)]
*  com slash Jason in the first name club. [[01:42:35](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6155.86s)]
*  You can listen to my other podcasts this week in startups to search for it on YouTube or [[01:42:38](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6158.58s)]
*  your favorite podcast player. [[01:42:42](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6162.66s)]
*  We are hiring a researcher apply to be a researcher doing primary research and working with me [[01:42:43](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6163.86s)]
*  and producer Nick working in data and science and being able to do great research, finance, [[01:42:49](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6169.54s)]
*  et cetera. [[01:42:54](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6174.34s)]
*  All in podcast dot co slash research. [[01:42:55](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6175.3s)]
*  It's a full time job working with us. [[01:42:57](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6177.54s)]
*  The besties. [[01:42:58](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6178.98s)]
*  We'll see you all next time on the all in podcast. [[01:42:59](https://www.youtube.com/watch?v=nSM0xd8xHUM&t=6179.78s)]
