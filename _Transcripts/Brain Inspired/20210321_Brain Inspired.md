---
Date Generated: April 20, 2024
Transcription Model: whisper medium 20231117
Length: 3867s
Video Keywords: ['Education', 'Science', 'Technology']
Video Views: 5227
Video Rating: None
---

# BI 100.4 What Ideas Are Holding Us Back?
**Brain Inspired:** [March 21, 2021](https://www.youtube.com/watch?v=cSEEthU9qzw)
*  What A and D is? Assumptions or terms? Do you think is holding back? Neuroscience and AI. And why?
*  I looked at the questions, I didn't really rehearse anything. I think it's kind of more interesting to kind of do it on the fly.
*  This is Brain Inspired.
*  Hey, my name is John Day. I just graduated from undergrad at the University of Michigan and I'm currently a research assistant at the IRS Yen in Tokyo under Professor Minbo Kai.
*  One thing that has been a major takeaway from the show is a recurring theme about creativity. It's mentioned in different ways, sometimes implicitly in a number of different episodes, but there are two in particular that stood out for me.
*  The first one was with Sam Gershman in podcast number 28 where he said, quote, I think it's hard for students to follow the path of greater resistance because they look around and they see what's happening, what's successful, what's not successful.
*  And it's natural for people to gravitate towards the things that they perceive as being more successful.
*  But the paradox is that to be really successful, to really make a difference, to change the way people are thinking, to change the course of scientific discovery, you have to swim against the current.
*  You have to pursue ideas that are not popular, that people might not agree with, that might not really make sense at first.
*  You have to befriend the crackpots and read poetry and, you know, take long walks and do all the things that aren't ostensibly the right things to do, the productive things, the good uses of time.
*  So we have to be really attentive to the unappreciated importance of uselessness.
*  End quote. The second was episode number 86 with Kent Stanley in which she was talking about open-endedness and constraints versus objectives.
*  In that episode, Stanley seems to make the case that pursuing something because it is interesting can be a good enough objective itself, especially when it comes to human innovation.
*  I think these ideas are just as applicable as a way to approach research as they are a way to approach life.
*  Interpreting Sam Gershwin's thoughts in a slightly different way, I thought, you know, perhaps not only should I read poetry and take long walks, but in some way or another, perhaps AI and neural networks should be encouraged to do so too.
*  Regardless of how silly that might seem, these moments for the podcast have served for me as a reminder of the importance of doing things a little bit differently and the intuitive or human aspect of research.
*  And these seem to be especially important things to keep in mind in the field of brain-inspired AI in particular, where I've already come across my future people who are adamant about AI and neuroscience not belonging together.
*  These reminders have encouraged me to trust in the validity of pursuing interesting things, both in my personal pursuits as well as in the pursuit of understanding intelligence.
*  Thank you, John. And thank you, Sam, for the memorable crackpot quote.
*  Yeah, let's all be more creative. Stick to our intuitions. I like that.
*  All right. On this fourth installment of the 100th episode, Smorgasbord, I asked previous guests to answer the question posed by my Patreon supporters.
*  What ideas, assumptions or terms do you think is holding back neuroscience and or AI and why?
*  So I'll give my own answer, because why not?
*  I think one assumption holding us back is the assumption we're all wanting to explain the same thing, that we all have the same target of explanation or targets of explanation.
*  The reason it's holding us back is, I believe, it leads to misunderstanding each other, misinterpreting each other because we have different goals and interests.
*  I know I personally have had kind of a nonchalance or carelessness in general about specifying, even to myself, specifying what exactly the target is of my own understanding with respect to various questions.
*  And I've really been led to this realization or opinion. It's actually just an opinion, of course, through many of the conversations I've had with guests that have been on this podcast, both on air and off air.
*  And of course, through my own learning and exploration.
*  As an example, something John Krakauer brings up a lot, and you'll hear him in at least one of these episodes bring this up, is the need for pluralism and explanation.
*  The kind advocated by philosophers and historians of science like Hasek Chang, Hank Direct, whom John turned me on to, and plenty others like Karl Kraver, who's been at this for a long time now.
*  I think that's right, and I'm all on board with that.
*  But I also think there's likely some interaction between pluralistic explanations and just the varieties of targets of explanation.
*  So in some cases, the pluralism actually might refer to different things to explain, although we often conflate those different things as if we're talking about the same thing to explain.
*  An example that comes to mind to me is this recent brandishing, I'd say, of behavior as having an explanatory primacy over neurobiology for understanding brains and minds.
*  To me, this all depends on the scope and the targets of what you want to explain and which of the 17 different approaches to levels of analysis you're using, of which Mars levels are the most famous recently.
*  There's a rich philosophical literature on these issues with plenty of disagreement, and it's something I'm enjoying continuing to ponder.
*  And it's made me more careful, at least, although maybe not much more skilled yet, in thinking about what some offered explanation is really pointing at, is really trying to explain.
*  Anyway, that's one thing I think is holding us back.
*  But you're about to hear around 25 more better answers to this question.
*  I list and link to all these folks in the show notes at brandinspired.co.
*  Slash podcast slash 100 dash four.
*  And as usual, I randomized their order of appearance during the episode.
*  All right, let's do this.
*  Let's see what's holding us back so that we can solve all this stuff way before we're dead and maybe even upload our minds so we can exist until the great heat death of the universe.
*  Peter Wolfsoma, one thing that neuroscientists think need to appreciate more, and I learned that also in the last years, is the importance of everything outside the cortex.
*  So I'm working always in the cerebral cortex, and I think it's a really important structure.
*  But I think I increasingly realize that it's not all that's there.
*  You have all these important subcortical structures, thinking of the thalamus, the basal ganglia, the cerebellum.
*  And so there are many supporting things that these structures do, but also, I think, important processing steps that these subcortical structures enable.
*  And that is a really important question.
*  I think we now have the technology also to address these questions by having tools that allows the record from those subcortical structures and also looking at the interactions between those subcortical structures and the cortex.
*  So I think that is where a lot of progress will be made in the next couple of years.
*  Grace Lindsay, Gatsby Computational Neuroscience Unit at University College London.
*  Okay, what ideas, assumptions, terms do you think is holding back neuroscience slash AI and why?
*  I don't know if this is, uh, I guess it's an assumption.
*  I think that neuroscientists might actually think that the brain is simpler than it is in a certain way.
*  And, I mean, we don't have good intuitions about how complex hierarchical nonlinear systems behave.
*  It's very hard to think about how those systems behave and to predict what they'll do.
*  And that's, I mean, that's definitely what the brain is.
*  It's a nonlinear hierarchical complex machine.
*  And our simple intuitions for how to understand it, I think, might be leading us down a wrong path where we think we can, you know, study these very simple tasks and find neurons that encode the simple variables of the simple tasks.
*  And, oh, it's just this clear chain of, you know, this neuron fires and leads to that neuron.
*  And obviously, on some level it is. It's just neurons connected.
*  But really actually understanding how information is encoded in a brain area and is passed on and transformed in other brain areas, it's just so complex.
*  And I feel like I only really appreciated how nonintuitive interacting neurons can be when I started trying to understand artificial neural networks, trained artificial neural networks and playing around with those and just looking at how you can modulate activity at one level of the network.
*  And it can create these changes at another level that are just not exactly what you'd expect at all.
*  And so I do think that perhaps we think linearly.
*  We think as systems neuroscientists, we kind of think, well, if the neurons that represent this thing in this area, if they go up, then they'll drive the neurons that represent that same thing at the other area.
*  And, you know, it's all a clear story from there.
*  And really, no, none of it's none of it's clear.
*  I don't think I think we need to really embrace the immense complexity that just having a bunch of neural networks.
*  Having a bunch of little nonlinear units interacting can create.
*  Okay, so my name is Marcel van Gerven.
*  I'm chair of the AI department at the Donders Institute.
*  I think what might be holding us back in neuroscience is a lack of realization that AI and particularly machine learning is very critical to understanding the brain.
*  And of course, on your show, I think we talked to people who do realize this, right, that there's this this big synergy between AI and neuroscience.
*  But I think AI is definitely playing a very critical role, which might not be seen by all neuroscientists.
*  And the reason I say this is that AI is ultimately about understanding the fundamental principles of intelligence, right?
*  So as McCarthy, I guess, defined it at some point as AI is the science and engineering of making intelligent machines.
*  And I think that's that's a very important definition.
*  And I also see AI as a science, namely, really trying to understand what the mechanisms are that allow adaptive systems to survive in complex environments.
*  And if this is not about the brain, then what is right?
*  So to me, that's a very important point.
*  Andrew Sachs here. What ideas, assumptions or terms do you think is holding back neuroscience and why?
*  One is biological plausibility.
*  It seems very innocuous.
*  You know, you'd like your theories to sort of fit broadly with our background intuitions.
*  But I think the way it's deployed is, in fact, not helpful.
*  So it's often used to say that a certain set of details about the brain are extremely important to be included in any model.
*  So things like certain aspects of the electrophysiology of neurons, if you don't have them in there, it can't be biologically plausible.
*  But I think the truth is the brain is so complex, it's very hard to tell what type of algorithm the brain could be running.
*  And ultimately, I think science is supposed to be based not on intuition, but on hard evidence.
*  And so rather than satisfying ourselves with intuitive judgment to plausibility, maybe a better approach would be to say, how can I falsify theories that come out?
*  If you think a theory is not biologically plausible, then it should be easy to do an experiment which would really torpedo it.
*  So, for instance, maybe you think the brain doesn't do second order like Newton's method optimization.
*  But really, how do we know? I mean, the brain is so...
*  If people will wax poetic about how complicated a single neuron is, who knows?
*  Who knows what could emerge out of the complex circuits of the brain?
*  So it seems to me worth reserving judgment a bit and trying to understand how we can empirically test these principles rather than relying on intuition.
*  And on the AI side, I think an assumption which maybe is worth questioning is that non-linearity is fundamentally mysterious.
*  So I think a lot of people have just given up hope that we will understand deep network systems.
*  And I don't think there's reason to give up yet.
*  Making progress on these questions, they're difficult questions.
*  So it will take a long time.
*  But I think we still can understand nonlinear systems to some extent, and we have in many other fields.
*  And so I don't see a reason to stop trying in AI.
*  Hi, my name is Jane Wong. I'm a senior research scientist at DeepMind.
*  What ideas, assumptions, or terms do you think is holding back neuroscience, AI, and why?
*  Well, in neuroscience, I think it's the idea that more neural data or recorded data is necessarily better.
*  As we have better and better methods for recording and collecting brain data,
*  I sometimes assume that we will necessarily gain increased understanding and insight into how the brain works.
*  But I think that without a paralleled increase in our ability to model and create theories for that data,
*  on which to scaffold our understanding of that data, I don't think that it will create much additional insight.
*  So I think we really need to focus on increasing our theories for that neural data.
*  And I think that that typically means that we need to be tying that to behavior and to understanding tasks and what those tasks entail.
*  In AI, I think that the idea holding us back is the idea that the better model performance is necessarily or is equal to better research.
*  I think that working with toy environments and smaller models can often bring much more insight than deploying huge models on benchmarks
*  that yield you a gain in a few fractions of a percent over the current state of the art.
*  The former, I think, is research, so getting better understanding and maybe a new sort of innovative way of modeling something,
*  but that maybe isn't fully optimized. The latter, I think, is engineering.
*  It's very impressive engineering, but I don't always agree with the way that our field has equated that to being the best research.
*  So I think that we need to maybe place a bit less emphasis on just benchmark performance and comparing two numbers and more on analysis and insights.
*  And I think that a large part of that means that when you introduce new models, you should first validate them on much smaller toy environments
*  and to make sure that you really understand what they're doing and that your performance is due to something that gives you insight or that you can truly understand.
*  Thomas Nosolaris, Department of Neuroscience, University of Minnesota.
*  What assumptions are holding back neuroscience and why?
*  I'll just speak about neuroscience. There's an assumption that there's a gold standard measure of brain activity
*  and that to truly understand the brain, all other measures of brain activity have to be converted into this gold standard.
*  I think that that assumption, it constrains the way that we think about phenomena that occur at different scales.
*  I think, in fact, that depending on the question you're asking, measurements at very different scales are sometimes more or less appropriate.
*  So voxels instead of cells, for example. I don't think this is a crisis necessarily, but I think there's a misplaced emphasis on maybe spikes.
*  I know that's heretical to say, but I think it's true and I think we're going to learn a lot when we start looking for and trying to articulate computational principles
*  that make predictions at multiple scales and for multiple different kinds of brain measurements.
*  This is Steve Potter from the Georgia Tech Laboratory for Neuroengineering.
*  What are the ideas, assumptions, terms that you think are holding back neuroscience and AI and why?
*  Well, the belief that we are on the right track with deep learning multilayer artificial neural networks is holding us back.
*  Deep learning networks have provided the ability to understand the brain and the brain's behavior.
*  Deep learning networks have provided lots of advances lately, but to think of them as a way to help understand the brain I think is a big mistake.
*  They are drawing attention further and further away from actual brain physiology.
*  To create AI with deep learning systems, to me, is like putting bigger and more complicated gasoline engines into a car
*  when all along we should have stuck with electric motors and batteries, for example.
*  We should stay focused on brain biology for inspiration for how to make more brain-like AI. That's my opinion.
*  My name is Kendrick Kay. What ideas, assumptions, terms do you think are holding back neuroscience and AI and why?
*  I guess I'm going to change the question slightly. I'd rather answer the question what, not necessarily ideas holding things back,
*  but what is the lack or what's the deficiency or what's the challenge or why is this whole endeavor hard?
*  I guess my answer is pretty predictable. I'd say something like the fact that the issues we're tackling in neuroscience slash AI
*  slash whatever term you want to apply to all this kind of work, trying to understand the brain or trying to understand intelligent complex behavior,
*  is the fact that we lack a coherent, we as a field, I suppose, lack a coherent perspective,
*  meaning that there are many different ways to think about it, conceptualize it. We're not really sure what the right way is to do it.
*  Many different systems of the brain, many different species, many different experimental methods, many different analyses,
*  many different new sort of machine learning concepts and approaches and frameworks.
*  And, you know, of course, there's no magic bullet. We don't know which one is right. So we keep exploring.
*  But I guess what worries me, or not really worry, but what intrigues me is how as a field we can consolidate these things.
*  It's very hard for one individual to do everything or even know just a sliver of what's all out there.
*  And in fact, that reminds me of Paul's podcast. I mean, by now, Paul has heard so many diverse perspectives
*  that in some sense, Paul's the smartest person in the room. So maybe Paul can solve this problem.
*  And I see this all the time in different subfields and people out there, you know, they do their one thing, they know their thing,
*  but they're sort of blinders. They're not really aware of the many other things going on out there and many different other approaches and ideas.
*  But in some sense, we need to consolidate. And in some sense, that's the sort of one of the goals of CCN to bring it back to that idea.
*  And there are many extra science factors here. Like, of course, there is not an incentive structure that really promotes having a very broad,
*  integrative view of a lot of things. And we have limited resources, of course, so we can't really know everything.
*  So maybe the hope is that science is somehow a hive mind that like even though individual scientists don't know much,
*  we will eventually somehow in kind of a swarm sense, move in the right direction. And so I think of people as vectors.
*  So is your vector pointed in sort of the right direction? And like, what's the vector average of all the scientists out there?
*  Is it actually pointed sort of incrementally in the right direction?
*  Or are we just sort of random walking all over the place, which is obviously a pessimistic view?
*  Blake Richards. There are a few assumptions, such terms, such ideas that I think are holding us back.
*  One of the most important ones is the entire question of innateness.
*  I think that a lot of ink and, shall we say, characters get spilled over the question of how much innateness should be in a neural network.
*  And, you know, to what extent is learning the answer, etc.
*  And the funny thing is that I think that that's arguably a distraction from the reality, which we've all known for a long time,
*  which is that you don't want to fully hard code innate things into the system,
*  but you need a lot of good inductive biases to help encourage the system to learn particular types of things.
*  And I think that the problem is that the entire question of innateness sometimes distracts people, myself included,
*  like I'm not susceptible to this, because someone will couch something in terms of innateness or non-innateness,
*  and then the other camp who do or don't like that term will get their backs up,
*  when really it's all just about everyone's reaction to this term innateness.
*  I think that the funny thing is that the actual amount of daylight between the different positions in this area is not that great.
*  And so really the best thing for everyone to do is just to pursue their research strategy
*  and try to find just the right amount of inductive biases or not to give them good results.
*  And we shouldn't presuppose that X amount of innateness is what's necessary or not.
*  I think that distracts us often in these discussions.
*  The other thing that I think has been holding us back in neuroscience AI as a specific kind of subfield is an obsession with heavy implasticity.
*  To explain that a little bit, I see paper after paper after paper as a reviewer and area chair and stuff,
*  where people are trying to do various different things with heavy implasticity.
*  And typically what they mean by heavy implasticity is just that they're just going to take the correlations between presynaptic and postsynaptic activity to do the synaptic weight updates.
*  The reason that I think that this is holding us back is because I don't see what the utility of this constant quest to get heavy implasticity to do everything actually is.
*  What do we buy from all of this research?
*  The reason I don't understand what we buy is, first of all, from an AI perspective,
*  sometimes people like to suggest that these heavy learning algorithms will produce something better for AI.
*  But not only has no one ever demonstrated that, but no one's ever really been able to mount a convincing theoretical argument for why that would be the case either.
*  Really, if you can follow a gradient, why wouldn't you follow a gradient? Stochastic gradient descent works well.
*  The other thing that I think, the reason I think it's a problem is because I think the real reason that people get really into heavy implasticity is they work on the understanding that we know that the brain does heavy implasticity.
*  And this is what I want to challenge everyone in the field on.
*  The data on synaptic plasticity is extremely heterogeneous, and there are some experimental protocols where you get clear heavy implasticity,
*  and other experimental protocols where it's anti-heavy, and other experimental protocols where it's something else altogether.
*  It depends on all sorts of little configurations in your experimental protocol.
*  And furthermore, when it comes to actual plasticity in the in vivo animal, there's growing evidence that there's all kinds of complicated things going on that aren't incorporated into these Hebbian models.
*  So first and foremost, we know that neuromodulator systems are clearly having additional impacts on synaptic plasticity that are not considered in Hebbian models.
*  Another example would be astrocytes. There's growing evidence that, you know, glial cells help to regulate synaptic plasticity in a way that would totally change the game computationally in terms of what you could build into your synaptic plasticity rule.
*  And then one of my personal favorites is we know that a lot of feedback connections can influence plasticity.
*  As we've shown in our paper, if you use that principle in a model with apical dendrites, you can even follow gradients.
*  So I think given all of the computational and biological considerations, it's not clear that the answer has to be pure Hebbian plasticity in the sense of like the learning algorithm can only pay attention to presynaptic and postsynaptic activity.
*  From either an AI or a neuroscience perspective, that's not a desiderata.
*  And I think the field would actually gain a lot by letting go of the obsession with that particular form of learning role.
*  I'm Jay McClellan. I'd like to comment on the AI side more than the neuroscience side.
*  I think in the AI side, there are lots of really smart people who are thinking very hard about encouraging a greater degree of systematicity in our agents, in our learners.
*  And I don't think our agents have sufficiently systematic behavior as it stands.
*  But I don't think the way to get there is the usual approach that I see most people trying to take, which is to figure out, you know, a way of building in some architectural constraint that is going to encourage systematicity.
*  The successes of models like BERT and GPT-3 and many, many other models, in my view, have a lot to do with the fact that they absolutely eschewed building in an explicit systematicity as such.
*  The successful language models have created a situation in which they can use much more context than previously was possible with LSDMs.
*  The attention mechanisms allow finding relevant information over a huge temporal swath of prior context without a gradient loss problem.
*  And I think it's that. It's not the building in of systematicity that's resulted in greater success.
*  What Chris Manning and his group have been able to show is those models actually exhibit a good deal of systematicity in their representations.
*  They're extracting grammatical structure, but not because it was built in, but because they are able to exploit it through the learning process.
*  And what I look for is more progress, figuring out how to innovate with deep learning and other kinds of architectures without explicitly building in the solution you're looking for, but letting it emerge.
*  My name is Jim DiCarlo.
*  There's a notion I noticed in neuroscientists that they don't appreciate yet cross validated prediction of X, where X can be any kind of set of things as an actual goal of science.
*  I've come to realize that scientists are taught that prediction is even what they should be trying to do.
*  It's a strange concept they think they're trying to understand.
*  And this relates to the idea that if you think you have an understanding, you should be able to predict.
*  And if you don't, you don't have an understanding.
*  But I believe that may be because neuroscientists aren't trained and always in that mold of what sounds like machine learning terms.
*  So we need to figure out how to take those terms and make neuroscientists comfortable with them, because I think they agree with them in spirit.
*  But we haven't me among my conclude myself haven't been able to communicate enough that that's actually what we all want to do.
*  It's just how we talk about it differently.
*  Talia Conkle.
*  What ideas, assumptions, terms do you think are holding back neuroscience and AI?
*  Off the cuff, I think biological plausibility might be holding back neuroscience and AI just a little bit.
*  I think like here's a question.
*  Like, is it interesting if you end up with a model system that has some interesting representational properties?
*  You know, maybe they're even brain like or human like, but are clearly not solving it in the way that human minds and human brains are solving it.
*  Is that interesting?
*  Some people might go, well, it's not biologically plausible.
*  So no, why would I care about that solution?
*  But if you take a more cognitive stance, that that representation exists and is probable is interesting.
*  How did that get built?
*  And what what are its properties?
*  And, you know, is it possible to arrive at a similar kind of representation through more biological properties once you understand that representation in the first place?
*  I think is perfectly a reasonable end.
*  But even if it's not biological, I think it's an interesting kind of level of representation and approach anyway.
*  So I think I think maybe their sort of models as model organisms with solutions in and of themselves and representations in and of themselves are interesting from a cognitive science perspective.
*  And I think biological possibility can be sort of approached from the other end.
*  Like we've got we've got it built in a non biologically possible way.
*  And it has a lot of the properties that we want.
*  Can we get there in a biologically plausible way or something that has levels of abstraction that sort of match a little bit more with our systems neuroscience level understanding?
*  So I think that might be, I don't know, a slightly different perspective that might open up some new routes.
*  I think the thing that all that's the most is the idea that the brain has to learn rule that the brain have to understand the solution in order to act.
*  We always think about extrapolation as the part of the brain.
*  The brain of a limited experience from this limited experience, you have to learn the rules and act in completely new setups that you never been in.
*  And I think what's true for the machine learning community and to the human brain actually that the problem they are trying to solve is very different.
*  They try to interpolate and to predict within the parameters they are learning.
*  So then memorization and simple interpolation can be a solution.
*  Usually we despise a scientist because scientists we want to have the essence we want to have understanding for us a model with one fifty billion parameters is not a model.
*  A model has to be like Newton laws. They have to be like five intertabal parameters that we can really understand and manipulate.
*  And it's like over fit with a billion parameters to be enough data points to get interpolation to things that are similar in essence.
*  Doesn't feel smart enough.
*  And I think this is the main reason that stop us from having progress.
*  This idea that the brain really learn the rules.
*  And it's very difficult for people to understand what we are saying because, you know, as scientists we try to understand.
*  We don't try to act and we as a scientist our mistake.
*  We think that the brain is also a scientist that the brain also try to understand and not trying to act.
*  But actually the brain put more premium in action than understanding the brain have to act and survive and do the right outputs.
*  The brain doesn't care about understanding as much as we do as scientists.
*  This is Wolfgang Maas from the Graz University of Technology in Austria.
*  A lot of theoretical work and modeling work also in theory neuroscience is really based on the assumption that you have only one or two types of neurons, say excitatory and inhibitory neurons.
*  But lots of the results tell us now that there is a fairly large set of neurons which has different response properties and also particular have different slow variables.
*  And so therefore I think that we probably need to face this also in our theoretical work.
*  And for example, in the first step, when you look for results from dynamical systems or conceptual inspiration, almost all dynamical systems that I have seen being studied, they consist of homogeneous units.
*  They're all of the same type then.
*  And I don't really see yet theory and conceptual tools to help us to understand dynamical systems which consist of 100 different types of components.
*  Let's take a quick little break and then we'll get back to the responses.
*  I think my favorite thing that I learned from Brain Inspired is probably the work coming out of Jim DiCarlo's lab with convolutional networks in the ventral visual stream because as someone coming from the AI side and getting into the neuroscience side, it's just so gratifying to be able to bring that up every time someone says that deep learning isn't useful.
*  Well, look at this, this work they're doing with the synthesized images and targeting neural populations.
*  So that's just really fun to pull out out of the back pocket.
*  The work that Megan Peters' lab is doing in metacognition, that was really interesting to learn how you could drive apart someone's performance on a task while increasing their confidence on that task, even though they're actually performing worse and worse.
*  One of the most important things I learned might be the fact that it's okay and really necessary to think deeply about philosophy when doing neuroscience, whether you're interested in the difference between explanation and prediction or you're contemplating what we can learn from evolution in terms of thinking about it as a process of information compression or thinking about the resources that we can learn from.
*  The source constraints that it places on the system and what impacts that might have for how the system develops or the usefulness of creativity and mind wandering or the usefulness of mistakes and lapses.
*  You know, these are all things that I feel like don't get enough serious consideration when you're just talking science.
*  But the guests on Brain Inspired, I think, do a really good job of reminding all of us how important it is to take a step back and think about the bigger picture.
*  This is Paul Ciszek from the University of Montreal.
*  Well, in terms of neuroscience, one of the things that holds us back is that in order to solve a problem, we need to define them explicitly.
*  And I think we don't have the right definitions for many of the problems that we're trying to work on.
*  And so one example that I always say is that we define the problem of behavior as an information processing problem, where the goal is to produce the right response, given some input.
*  And that seems like a reasonable way to define the problem, but I think it's incorrect.
*  And I think it's just as incorrect.
*  So I'll make an analogy here.
*  I think it's just as incorrect as describing a car as a device that converts chemical energy into kinetic energy, which, of course, the car does that.
*  Right. But I think it's much more informative to think of a car as a device that moves people from place to place.
*  Right. And then energy conversion is just part of that larger process.
*  So I think processing information is something the brain does, but it's just a means to an end.
*  The real purpose of the brain is to control the organism's state.
*  In other words, it's to generate outputs that result in right input.
*  And so it's all about control.
*  Once we define it that way, now we can talk about the component functions in a more correct way.
*  And so I think in neuroscience and AI, we think so much of sort of you receive an input, you think about it, and you produce an output or you produce a knowledge or something.
*  And I think that's like removing most of what the brain does and trying to treat this one aspect as a separate thing.
*  I don't think that's a good definition of the real problem.
*  It's Patrick Mayo. What ideas assumptions term do you think is holding back neuroscience and AI?
*  Yeah, I don't I don't think there's any specific terms holding neuroscience.
*  I'll speak about neuroscience holding neuroscience back.
*  I think it is very easy to fall into the trap of looking for phenomena that match the terms that we already have in, say, the English language,
*  when the phenomena and the mechanisms for those phenomena in the brain may not nicely align to single words or already defined terms.
*  I hate to say that we might need to define new terms, since that's also a problem is people defining new terms for things that already exist.
*  But I think about something like attention and fractionating attention into subcomponents.
*  And it's possible we're just going to need more specific or better terminology.
*  My name is Conrad. I'm a neuroscientist at the University of Pennsylvania.
*  What ideas do you think are holding back neuroscience and why?
*  Well, personally, I believe the overly strong emphasis on reductionist explanation of thought is one of the main things holding back neuroscience.
*  Why? The more the space of credible mechanistic models of things that are like the human brain is incredibly big.
*  For example, if we say every synapse can have 10 different values, 10 to the 15 some synapses, then it means that we basically have,
*  and let's say each synapse can take one out of 10 values, we have a 10 to the 10 to the 15 dimensional space.
*  Even if we measure a parameter perfectly, it gets us from the 10 to the 10 to the 15 dimensional space to the 10 to the 10 to the 15 minus one dimensional space,
*  which is just as big as the space in which we were before.
*  So measurement only makes allows us to make progress if the set of models that we consider is small.
*  And I think the field is being very imprecise about the dimensionality of the models that we're really looking at.
*  David Popol, and I work at NYU and at the Max Planck Institute.
*  What's holding us back is that we forgot that we have a past.
*  So our methods fetish has let us forget that our colleagues in the 19th century and the first half of the 20th century thought through a lot of these problems in a careful way already.
*  And with some of my friends and colleagues, we've been arguing that the kind of behavioral, cognitive, computational decomposition of problems into their elementary constituents
*  should play in more or should be more in the foreground to make real progress in neuroscience and perhaps AI as well.
*  You ignore Helmholtz, Fechner, Tinbergen, Marr, etc. at your own peril. Read old stuff.
*  This is Brad Love from UCL.
*  What ideas, assumptions do you think are holding back neuroscience AI and why?
*  Well, I guess I could use my previous answer here, though that was more about AI.
*  So let's focus more on neuroscience, where I think there's a problem, a different problem of integration.
*  So there's so much research, there's more scientists than ever, and we all publish more papers than ever.
*  But all these papers are kind of little.
*  And I'm not talking about like they're in little journals.
*  I'm talking about the papers that appear in Nature, Neuroscience, Neuro and eLife.
*  They all involve, you know, narrow paradigm and kind of their own internal world and preferred brain area.
*  And it's hard to see how it's all going to be put together.
*  You know, some, you know, maybe like a tedious social psychologist would be like go on about experiment or degrees of freedom and replication issues.
*  But that's not what I'm talking about.
*  I'm talking more about theoretical integration.
*  So it's a real challenge, I think, how we're going to link all these findings together in some coherent whole, you know, modeling could help if it was a little bit more complex.
*  It's a real challenge, I think, how we're going to link all these findings together in some coherent whole, you know, modeling could help if it was applied beyond one paper, which it not often is, you know.
*  So that's one way is just having, you know, models that could link many findings together.
*  So you just basically understand the model to understand the domain.
*  But, you know, yeah, in general, I'm not really sure if we're on a path to integration that would lead to a human understandable science.
*  I mean, multi-level theories, theories of different granularities or levels of attraction certainly would help make sense of the increasing volume of findings we are creating.
*  But, you know, I just don't think we could keep going on like this, like generating all these one-off hits, these papers that seem exciting, but the whole is less than the sum of its parts.
*  Though the field is progressing for sure.
*  You know, then, of course, I guess there's all these, you know, fads that aren't helpful in red herrings, like subjective aspects of consciousness, which isn't even a scientific question beyond cataloging correlates.
*  So I guess, you know, there's just this problem of integration and then the secondary problem of just, you know, fads and obvious dead ends.
*  So I'm Rodrigo Quienquiroga, neuroscientist at the University of Leicester.
*  Well, I think it's what I said before.
*  I think we need to look more deeply into the function of the human brain and see that there are some principles in the human brain that we're still not even trying to replicate in computers or in artificial intelligence.
*  And maybe when we start to, I mean, to see how to implement these principles, which I have no clue how to do it.
*  I have no clue how to make a computer decide what to forget and what to keep and extract the common sense.
*  But if we start thinking about these processes, which are quite natural in the human brain, well, maybe this will lead to a very major breakthrough in artificial intelligence.
*  I'm Steve Grossberg.
*  Part of the problem is reliance on old computational ideas.
*  For example, deep learning uses the same method that we use in the computer.
*  Steve Grossberg, part of the problem is reliance on old computational ideas.
*  For example, deep learning uses steepest descent learning by gradient descent.
*  But steepest descent learning by gradient descent was discovered by the famous German mathematician Gauss.
*  These are old ideas that have just been juiced up a little bit.
*  And they can't explain autonomous adaptive intelligence, which is really, to my mind, the main challenge of future computational science.
*  Moreover, deep learning is supervised.
*  You need to have a teacher giving you the answer on every learning trial.
*  But much of the learning that we experience is unsupervised and indeed adaptive resonance theory can learn in arbitrary combinations of unsupervised or supervised learning trials, just like we do.
*  I'm Mark Humphries at the University of Nottingham.
*  An idea or concept that's holding back neuroscience that sounds prosaic when I say it is averaging, I think.
*  So when we record, particularly in systems neuroscience, when we record neurons in response to the animal being shown a picture or played a sound, or we record the neurons when the animal is moving or deciding or planning or whatever,
*  then almost the reflex action of everyone having recorded a neuron in response to the same thing happening is to average it in some way.
*  So we end up with a tuning curve which shows for a given neuron its average response to different properties of the stimulus, so different properties of the angle of a bar or the frequency of a tone or something.
*  We end up with the hypercommon sort of PSDHs, these Perry stimulus time histograms, with graphs you draw where you get the neurons firing during a particular period of the task and you align it to say when the sound happens and you average over its responses.
*  And you get this lovely graph showing its kind of average response over time to that stimulus.
*  But the problem with both those things, of course, is that they are really useful to us as the reader and observer to try and make sense of this complicated data.
*  So they appear in papers and they're a historical legacy when you have to draw these graphs by hand.
*  But obviously they're quite misleading about how the brain works.
*  So the brain never gets to see these averages, right?
*  The brain gets to work in the moment with what this barrage of spikes is happening.
*  Each neuron sees a barrage of spikes on a different set of its inputs in each moment.
*  And that's what it gets to work with.
*  And it gets to be part of its own next barrage of spikes when it sends a spike.
*  And the brain just gets to work moment to moment with this huge vector of spikes.
*  But the PSDH in particular is so common that it underlies, and tuning curves underlie, most of our theory development.
*  So most even people who are working on incredibly complicated analysis methods of looking at high dimensional data or finding out what sort of ways neural populations are encoding simultaneously different properties of the task like dot motion and color or
*  or encoding arm movements or encoding whatever aspect of the decision making process they're looking at.
*  They often begin by simply averaging the neurons together to get the PSDHs and then start from there.
*  So they're just looking at the low dimensional high dimensional of the average responses.
*  Whereas the average response, of course, hides a practically infinite array of individual responses on each trial.
*  So the average over say, 100 trials could be hiding 100 different responses that when averaged together have this one beautiful peak.
*  And this really struck me. I've been struck me this about a decade ago.
*  I did a little bit of work where I was looking at a single dopamine neuron and it was responding to a painful stimulus applied to a pore.
*  He drew the PSDH and about 20 milliseconds after the after the poor was touched by little electric shock, you got a big jump in activity followed by a pause.
*  And then it goes back to baseline.
*  That's the average over, you know, it was about 120 trials of this thing.
*  But then when you look at the individual neural responses and you take a clustering algorithm, a cluster of those individual neural responses together,
*  you see there was actually three qualitatively different responses that when the pine pole was stimulated, it reset where the neuron was firing.
*  So you either see pushed it forward in time, back in time or made it spike immediately.
*  Those three different responses averaged together look like this beautiful peak and trough PSDH or which none of the responses individually look like that.
*  So as it sounds, I think averaging is taking us very far away from how the brain is actually working.
*  John Crackauer, I think that the biggest thing holding us back is that we can't imagine that there may be strong pluralism.
*  In other words, true multiple ontologies.
*  In other words, it may well be that there will never be.
*  I think what you would like there to be is a proper alignment of the mapping between the implementational level and the folk psychology of everyday life.
*  In other words, it may well be that words like jealousy and lust and envy and justice are going to have their own causality and meaning.
*  And you're not going to be able to, in some isomorphic way, map them onto neural mechanisms.
*  And I think that there's going to have to be an ability to deal with that forever.
*  Just like maybe I've discussed this with you before, that light can be a particle and it can be a wave.
*  I think that's what's going to hold people back is not being able to cope with the possibility that there is some kind of.
*  I think Carl Kraver's talked about this recently.
*  I think he calls it pluralistic preservationism.
*  And I think that's what we're going to have to accept is that sort of strong version of pluralism.
*  And I think we'd be a lot happier if we accepted that in science.
*  In other words, psychologists and neuroscientists could live together with that pluralistic preservationism.
*  What I think is difficult is how to actually mimic the things we've been discussing by an AI.
*  See that that's a different problem, right?
*  Explaining the brain versus creating a new one and all that pluralism in terms of understanding isn't necessarily going to immediately translate by taking that pluralistic stance into a better.
*  AI. It's not clear to me that getting the understanding right, getting the multiple ontologies right is necessarily going to mean that we're going to create a common sense.
*  I don't it doesn't follow one from the other.
*  Do you see what I'm saying?
*  Hi Paul. This is Yuri.
*  The first one is what ideas, assumptions, terms do you think is holding back neuroscience and so on?
*  And the other one is related in my mind.
*  Do we already have the right vocabulary and concepts to explain how brains and minds are related?
*  My answer.
*  Neuroscientists began to study the brain, buying into a system created by philosophers and psychologists for understanding the soul and the mind without ever asking how those terms,
*  whose brain functions we are trying to understand, such as consciousness, were brought into our thinking in the first place.
*  Neuroscience has inherited this paradigm from such philosophy driven framework, which portrays the brain or more precisely the soul and the mind as a tool to learn about the true nature of the world.
*  Early thinkers used introspection and gave names to mental operations and now millennia later, we search for neural mechanism that might relate to their dreamt up ideas.
*  Of course, an inevitable consequence of this framework, what I call outside in, is the assumption that the brain's fundamental goal is to perceive signals from the outside world, process such information, correctly interpret them.
*  In order to respond to these signals, an additional operation is needed.
*  Wedged between the perceptual inputs and the organism response is the terrain of a hypothetical central processor.
*  This is an entity that chooses what to do with the processed information.
*  This poorly understood but often speculated about terrain has been referred to by various terms, such as free will, homunculus, consciousness, executive functions, intervening variables, black box or more recently, decision maker.
*  Depending on the experiment as philosophical inclination, or whether the hypothetical operation is applied to the human brain, brains of other animals or computer models.
*  Yet, of course, they all refer to the same thing.
*  The key assumption in this perception, decision, action paradigm is that information is processed properly so that something somewhere in the brain can decide to select the correct action.
*  An implicit in practical implication of this outside in framework is that the next frontier for progress in contemporary neuroscience should be to find the central processor somewhere in the brain and systematically elaborate the neural mechanisms of decision making.
*  This is exactly what's going on at full speed in today's neuroscience.
*  Over the past decade, decision making has become a bust term and applied to virtually all research without pausing a bit and asking ourselves, do we know precisely what we are looking for?
*  In my new book, the brain from inside out, I argue that this outside in framework may not be the best strategy to understand the brain.
*  Brain evolution didn't start out to generate a program where the end product should be the human level cognitive faculties.
*  Instead, brains evolved to induce actions and learn to predict the consequences of those actions as afforded by a particular environment.
*  The brain is not interested in the true nature of the world.
*  Instead, its main preoccupation is to help its host to survive and prosper in its niche.
*  I speculate that this action centric approach is more strongly embedded in evolution.
*  The neuro physiological findings are more compatible with it and the problems can be formulated differently, including the problem of the relationship between brains and minds.
*  I suggest that by trying out this inside out strategy, perhaps some of the currently controversial term may become dispensable.
*  The choice of a particular framework is important in our everyday practice because frameworks shape our ideas, both about experimental design and interpretation.
*  I don't exist that my suggested strategy is perfect.
*  Yet I believe that this alternative is perhaps more fruitful than the currently dominant outside in framework that so strongly influences AI.
*  So my name is Stefan Leijnen.
*  So I think one thing that is holding back neuroscience and AI is this idea that we can simulate the brain to the lowest level of detail.
*  And that will answer the questions that we have about the higher levels.
*  Of course, it's a very exciting idea and it's a worthy pursuit to make a scalable answer, to see how big we can get these virtual brains and also to maybe gain insights into what happens to these systems if they exist of not thousands, but millions or billions of parts.
*  Many things can emerge and the behavior can be unexpected.
*  But I think the problem is that in order to explain what happens at the higher level, we need to have an understanding of how we can get from the lower level to the higher level.
*  So and I see this, for instance, in quantum explanations of consciousness.
*  So I'm OK with the idea that consciousness is a quantum process.
*  And someone might be able to convince me that on this quantum process, there's consciousness happening.
*  But it doesn't explain the fact that I'm conscious and my scare is not.
*  Or that I'm more conscious than my cat is.
*  I think the only way we can solve a problem like that is if we look at the higher levels and think about how we can move from one level to the next.
*  And what we need to let go of, and this is, I think, the most difficult part, is the reductionist idea that there's one single level at which everything can be explained.
*  And this is typically the level of physics.
*  Oddly enough, when you think about quantum physics, it's actually the level below physics, but we still tend to call it physics.
*  But moving upwards, we always have this sort of gut feeling that the real science is expressing it in physics.
*  So if you can sort of go down from consciousness to psychology, to biology, to chemistry, we end up with physics.
*  And that's the level at which we explain it.
*  I think if we can let go of this idea and actually give a sense of almost like a validity at these higher levels,
*  and explain how you can move from one level to the next, I think that will open up a whole box of possibilities.
*  And also really, it's pivotal in moving the neuroscience and AI field forward.
*  I'm Nathaniel Dahn at Princeton University, Princeton Neuroscience Institute.
*  I think we're still missing top-down understanding.
*  Again, it's easy to make a bunch of measurements and describe them, a sort of stamp collection perspective on science.
*  But to sort of achieve understanding, and also to achieve AI for that matter,
*  I think it's going to be necessary to sort of impose more kind of top-down theoretical ideas,
*  and not just expect that if we catalog every synapse in the worm that we're going to understand how the worm works.
*  I think that's just a lost cause, and something else has to discipline that.
*  I think that's going to be top-down ideas from computer science.
*  Thank you for your support. See you next time.
*  Thank you.
