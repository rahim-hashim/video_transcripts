---
Date Generated: September 27, 2024
Transcription Model: whisper medium 20231117
Length: 5842s
Video Keywords: []
Video Views: 78
Video Rating: None
Video Description: Show notes: 
https://braininspired.co/podcast/194/

Patreon for full episodes and Discord community: 
https://www.patreon.com/braininspired

Apple podcasts: 
https://itunes.apple.com/us/podcast/brain-inspired/id1428880766?mt=2
Spotify: 
https://open.spotify.com/show/2UZj8c8Ap5oc2gh2rJxLLe

The Transmitter is an online publication that aims to deliver useful information, insights and tools to build bridges across neuroscience and advance research. Visit thetransmitter.org to explore the latest neuroscience news and perspectives, written by journalists and scientists. 

Read more about our partnership: https://www.thetransmitter.org/partners/

Check out these stories:
Dopamine and the need for alternative theories 
Reconstructing dopamine’s link to reward

Sign up for the “Brain Inspired” email alerts to be notified every time a new “Brain Inspired” episode is released: https://www.thetransmitter.org/newsletters/ 

To explore more neuroscience news and perspectives, visit thetransmitter.org.

Vijay Namoodiri runs the Nam Lab at the University of California San Francisco, and Ali Mojebi is an assistant professor at the University of Wisconsin-Madison. Ali as been on the podcast before a few times, and he's interested in how neuromodulators like dopamine affect our cognition. And it was Ali who pointed me to Vijay, because of some recent work Vijay has done reassessing how dopamine might function differently than what has become the classic story of dopamine's function as it pertains to learning. The classic story is that dopamine is related to reward prediction errors. That is, dopamine is modulated when you expect reward and don't get it, and/or when you don't expect reward but do get it. Vijay calls this a "prospective" account of dopamine function, since it requires an animal to look into the future to expect a reward. Vijay has shown, however, that a retrospective account of dopamine might better explain lots of know behavioral data. This retrospective account links dopamine to how we understand causes and effects in our ongoing behavior. So in this episode, Vijay gives us a history lesson about dopamine, his newer story and why it has caused a bit of controversy, and how all of this came to be.
I happened to be looking at the Transmitter the other day, after I recorded this episode, and low and behold, there was an article titles Reconstructing dopamine’s link to reward. Vijay is featured in the article among a handful of other thoughtful researchers who share their work and ideas about this very topic. Vijay wrote his own piece as well: Dopamine and the need for alternative theories. So check out those articles for more views on how the field is reconsidering how dopamine works. 

0:00 - Intro
3:42 - Dopamine: the history of theories
32:54 - Importance of learning and behavior studies
39:12 - Dopamine and causality
1:06:45 - Controversy over Vijay's findings
---

# BI 194 Vijay Namboodiri & Ali Mohebi Dopamine Keeps Getting More Interesting
**Brain Inspired:** [September 26, 2024](https://www.youtube.com/watch?v=lbKEOdbeqHo)
*  The type of things that you want in an algorithmic way of thinking is to look for invariants in
*  the data, right?
*  What are the things that don't change?
*  And this is a very powerful invariant, right?
*  And so if that's true, then I mean, this should be the thing that everyone should be modeling,
*  because this is the core thing that any algorithm should capture.
*  We might not know what dopamine does and what dopamine is, but we almost definitely know
*  that dopamine is not pleasure.
*  So one takeaway, dopamine is not the pleasure signal and maximizing your dopamine is not
*  necessarily a good thing.
*  There are like specific predictions that predictions where TDRM fails to make predictions about
*  what happens.
*  Why would that be controversy as opposed to progress?
*  It's a very good question.
*  So why is that controversial?
*  This is Brain Inspired, powered by the transmitter.
*  Hello good people.
*  I'm Paul.
*  Vijay Nambudri runs the NAMM lab at the University of California, San Francisco, and Ali Mohebi
*  is an assistant professor at the University of Wisconsin-Madison.
*  Ali has been on the podcast before a few times, and he is interested in how neuromodulators
*  like dopamine affect our cognition.
*  And it was Ali who pointed me to Vijay because of some recent work that Vijay has done reassessing
*  how dopamine might function differently than what has become the classic story of dopamine's
*  function as it pertains to learning.
*  The classic story is roughly that dopamine is related to reward prediction errors.
*  That is, dopamine is modulated when you expect a reward and don't get it, and or when you
*  don't expect a reward but do get it.
*  Vijay calls this a prospective account of dopamine function, since it requires an animal
*  to look into the future to expect a reward.
*  Vijay has shown, however, that a retrospective account of dopamine might better explain lots
*  of known behavioral data.
*  This retrospective account links dopamine to how we understand causes and effects in
*  our ongoing behaviors.
*  So in this episode, Vijay gives us a deep history lesson about dopamine, also his newer
*  story and why it has caused a bit of controversy and how all of this came to be.
*  Coincidentally, I happened to be looking at the transmitter the other day after I recorded
*  this episode, actually, and lo and behold, there was an article titled Reconstructing
*  Dopamine's Link to Reward.
*  Vijay is featured in the article, among a handful of other thoughtful researchers who
*  share their work and ideas about this very topic.
*  So go check that article out for more views on how the field is reconsidering how dopamine
*  works.
*  So I link to that and all of Vijay and Ali's information as well in the show notes at braininspired.co
*  slash podcast slash 194.
*  Okay, hope you enjoy our conversation.
*  So Vijay and Ali, thanks for coming on.
*  So I've had Ali on before and Ali pointed me, Vijay, to your work and said that you'd
*  be an interesting person to talk to and he pointed me to a specific paper.
*  And so I thought it'd be fun to have Ali on as well because he's a dopamine expert.
*  You're a dopamine expert.
*  I don't know anything about dopamine.
*  So I thought it'd be fun to have kind of a conversation here.
*  Having said that, so welcome and thanks for being on the podcast, first of all.
*  Of course, thank you.
*  Having said that, I put the word out to my Patreon supporters in my Discord community
*  last night, so it was kind of late.
*  There wasn't much time.
*  And all I said was I was going to have a couple dopamine experts on and to send me any dopamine
*  questions you might have.
*  And the question I got reads like this.
*  I'd really like to hear a view on how the causal association story contrasts with the
*  reinforcement learning story.
*  Wow.
*  I understand it's been quite controversial in the literature and so it would be great
*  to hear from experts.
*  And then he sent me a link to your paper.
*  I see.
*  Wow.
*  There you go.
*  There you go, Vijay.
*  So that is a very well-informed Patreon community.
*  They're on top of it.
*  Yeah.
*  They're on top of it.
*  So I do want to talk about that, but I also want to talk about dopamine, the dopamine
*  story writ large and just historically and how it's evolved.
*  So let me just, I'll start off by reading.
*  I did a little chat GPT research here.
*  I'm going to read 10 functions, 10 theories about the function of dopamine, and I'm not
*  going to describe them.
*  Reward prediction error.
*  Incentive salience.
*  Motivational drive.
*  Learning and habit formation.
*  Attention and cognitive flexibility.
*  Motor control theory.
*  Aversive learning and punishment prediction.
*  Effort-based decision-making.
*  Dopamine as a generalized neuromodulator.
*  And finally, social and emotional regulation.
*  That's too many functions, isn't it?
*  Yeah, absolutely.
*  But it's telling that the first one listed was maybe the first real, oh, I don't know.
*  Well, maybe you can correct me on this.
*  Real success in the dopamine literature was that temporal difference learning, reward
*  prediction error.
*  And that's kind of the classic story of dopamine, right?
*  Yeah, absolutely.
*  What is that story?
*  I would say that, may it interject?
*  I would say that that's not where it started, right?
*  That is correct.
*  I mean, what happens if you, so I think, I mean, we always learn function from malfunction,
*  right?
*  With dopamine, what happens if you deplete a person, a patient from dopamine?
*  That's Parkinsonian case, right?
*  So I think the motor control aspect is where it all started, right?
*  So I should back up and say, so the big success story in my little area, the thing that this
*  podcast focuses on in how neuroscience and AI can coexist and inform each other was the
*  temporal difference learning.
*  Right.
*  And then, like, I think early in the 80s, when people started doing electrophysiology,
*  sending down electrode in monkey brains and looking at dopamine cells, they were expecting
*  to see like motor related signals.
*  So early Schultz studies were all trying to look for motor related signals.
*  And then they would see if a monkey is sitting there reaching out to grab a piece of apple,
*  there's some dopamine activity when they get there.
*  But surprisingly, it was always after the movements, right?
*  So if a signal is related to movement, was it happening after that?
*  Right.
*  And then, like, in time, these stories of TDRL were developed that I'll let Vijay talk
*  more about the history of that.
*  But I just wanted to say that that is not where the dopamine story started.
*  Okay.
*  Yeah, no, I appreciate that.
*  And Vijay, like, you don't have to go into like technical detail or anything, but just
*  kind of the broad strokes.
*  Yeah.
*  I think actually one more thing to add to Alis point about the history is that like
*  even pre dopamine becoming TDRL, in addition to the movement stuff, there is also the idea
*  that dopamine is related to pleasure, right?
*  Like it's the rewarding molecule.
*  And so that was actually maybe even one of the OG theories before.
*  I mean, actually, in popular press, you know, like lots of people that I talk to, it is
*  still that theory, right?
*  And so, and I mean, the great thing about dopamine, I think, as a case study in terms
*  of how science progresses, is that...
*  Yeah, I want to talk about this.
*  Yeah, there's been sort of multiple ideas of what dopamine does right from the get-go,
*  right?
*  Right from when it started, there's always been multiple views.
*  And I think that's a sign of a healthy scientific discussion, right?
*  That there are different things going on.
*  I do agree that I think for right now, I think definitely the most well-established view
*  is the temporal difference reinforcement learning view.
*  And it's the, I mean, you know, the discovery by Schulz was like seminal, right?
*  I mean, that's sort of kick-started in some sense, this whole field of at least what dopamine
*  function is as it relates to like very rapid sort of dopamine activity, right?
*  There's sort of this parallel literature as that was developing, where people were looking
*  at long time scale disruptions of dopamine signaling, a lot of Parkinson's disease is
*  like the longest time scale one, but then also pharmacological inactivation experiments,
*  where that story is slightly different from the standard story of the learning story,
*  right?
*  But can they coexist?
*  So you started off by saying, you know, something to the effect of there are many different
*  views and then you just started talking about the time scale.
*  And I mean, maybe they're all correct, right?
*  Like what does dopamine need to have?
*  Oh, no, here's what you started off by saying is like it's a sign of healthy science, right?
*  But and yet, I'm not sure how you feel about your dopamine theory, but then people who
*  have their own dopamine theories, they need to be staunch supporters of their own theory
*  and everyone else is wrong, right?
*  Yeah.
*  Yeah.
*  I don't think that that's the right way to look at things.
*  I mean, no, of course.
*  I mean, there's one line that I think in one of the reviews that, you know, it's a beautiful
*  thing, even for dopamine, it is too much to be doing all these things, right?
*  So that's, I think, a great line, right?
*  I mean, I think in many ways, some of these theories are complementary, but in many ways,
*  they're also not complementary, right?
*  So it can't be that they're all simultaneously correct.
*  It can't be that like the single molecule does everything.
*  And even if it's involved in everything, maybe the way that it's involved in everything
*  is probably not exactly the same set of algorithmic function and stuff.
*  But at broad strokes level, I think it is a sign of a healthy debate that there are
*  different views.
*  But the next step that we want to is to like really figure out exactly what the things
*  are that are not possible to coexist in these different views.
*  And what are the things that are possible to coexist?
*  Just to contrast it with like a brain area.
*  So I think it's great that it has a neutral name, dopamine.
*  It doesn't say pleasure amine or surprise amine, right?
*  Whereas in like the cortex, for example, there's a brain area named motor cortex.
*  And that means it does motor activity, right?
*  And that's not all it does.
*  But because we've named it that, it's really hard to think of it in any other way.
*  Yeah, absolutely.
*  So dopamine is neutral.
*  Yeah, yeah.
*  No, I mean, I think that is good.
*  There's a funny one along those lines.
*  We've named our field neuroscience, right?
*  It's named after one cell type in the brain.
*  Right.
*  Do we need to switch it to brain science?
*  Brain science, right?
*  That's probably the more welcoming view of all the different cell types, right?
*  Well, maybe we can do brain science and the rest of the world can do neuroscience.
*  Brain and behavior.
*  Brain is nothing without behavior.
*  Oh, my gosh.
*  All right.
*  And then we have to go into society.
*  Yeah, we're going right to all the key controversies.
*  All right, well, let's focus.
*  But yeah, we'll go back to the temporal difference reinforcement learning stuff.
*  So yeah, to back up in the history.
*  So TD was really like, I mean, I do believe that it was really like one of the big successes
*  of this field, right?
*  Because it is one where like we had, at least in neuroscience up until then, I mean, there
*  were aspects of this going on in other fields.
*  But like, TD was like one of the poster tiles for where like we have these sort of computational
*  theories that were developed in a different fields that have clear algorithmic function
*  and can be very effective.
*  And then you find that something like that actually exists in the brain.
*  I mean, that's exactly what every theorist's dream is, right?
*  I mean, that's so like it is.
*  I mean, it is essentially what kickstarted this whole way of thinking, of thinking about
*  things in an algorithmic way.
*  So like it is easily the most successful theory in that sense.
*  And so, yeah, so before we get to like what the current state of the field is and whatnot,
*  just to explain what TD is, the core idea actually predates TD, right?
*  The core idea comes from psychology, is a question of how do animals learn?
*  What do they actually learn about?
*  Is there some sort of algorithmic principle whereby they learn associations in their world?
*  And so associations are probably too many things.
*  You could learn that one cue, one word is associated with something else, so on and so forth.
*  So it's a very general set of things.
*  But then people, just because it's in animal models, it's actually very difficult to assay
*  things without any sort of behavioral output.
*  You typically only study associations related to rewards or punishments, right?
*  Because you can actually get these very defined motor outputs that you can like clearly tell
*  that the animal actually is responding and sensing something, first of all,
*  and then responding to something else based on the fact that they predict something else, right?
*  So then the set of associations that we study in general is limited in that sense,
*  just because of the way that we assay them in animal.
*  And so within that field, the biggest success is the Roscarlo Wagner theory in 1972.
*  The idea being that up until then, the core idea was that the way that animals learn
*  and people learn associations is when two things happen relatively close together in time,
*  then you learn that those two things are associated, right?
*  And then people, pre-Roscarlo Wagner found some really interesting examples where that's not true,
*  where you can have stimuli that are very close together in time with a reward or a punishment,
*  but you still don't learn that that is actually associated with it.
*  And then the core insight there from Roscarlo Wagner was that actually a simple way to describe
*  all these different phenomena is that you learn based on error.
*  And that was like the first big revolution in this field, right?
*  In the question of how animals learn.
*  And so the core idea of that is very simple.
*  You just have some prediction of what will happen.
*  You have some prediction of rewards, whether they will happen or punishments,
*  but whether they will happen, and then you actually see what happens.
*  And then you look for the difference.
*  And then if there is a difference, that means that your prediction was wrong.
*  You could use that difference to actually improve your prediction the next time you see the same thing.
*  The way that you're saying it, it sounds like you're aware of all this,
*  but it might be more accurate to say that circuits in your brain take care of this for you.
*  And you don't have to be aware of predicting it.
*  It's just happening in the brain as an algorithm.
*  Yes, that is 100 percent true.
*  I mean, how much of this is actually conscious versus not clear?
*  Most of it is probably unconscious.
*  I think the way that we all talk about our favorite theories, we talk about intuitions.
*  And the best way to tap into intuition is to think about conscious things that we at least have reason to put.
*  But yes, I mean, all of this could be completely subconscious.
*  And so the idea was that error is prediction error specifically and specifically reward prediction error,
*  as far as rewards go, is the key quantity that allows you to learn better predictions of whether things will actually be followed by rewards.
*  And so that was all done in a trial based way, if you will.
*  The idea was that every time you have a trial, you get, let's say, one cue predicting associated with the reward.
*  And then you don't think about time in any other sense.
*  It's just basically on this trial, you had a cue and you had a reward or you didn't have a reward.
*  And then collectively across a bunch of trials, whether do you learn to associate the things or not.
*  But obviously, that is not how animals live.
*  Like, I mean, animals don't magically know that right now this is a trial, right now this is a trial kind of thing.
*  And so that was obviously a limiting factor.
*  And temporal difference reinforcement learning actually started to solve that for the first time,
*  where it actually started incorporating time within a trial.
*  And so that was one of the key advances in TDRL when it comes to the neuroscience aspect of things.
*  I mean, there's a whole bunch of TDRL stuff related to computer science that I'm not going to get into.
*  But I was going to say that it also drove reinforcement learning algorithms in AI.
*  Absolutely. Yes, absolutely.
*  Still is.
*  Still does.
*  Still does, yeah.
*  And so the core thing, as far as neuroscience and animal learning goes, is that TDRL allows you to have some way that you can define the progress of time within a trial.
*  So the idea is you could have, for instance, on a single trial, you have one cue followed by another cue, followed by reward.
*  And now this allows you to actually keep track of the sequence of things within a single trial,
*  which is actually kind of hard to do with the Westphalovagner type of theory,
*  because that was all just at the same moment there are a bunch of things happening versus not happening.
*  So it clumped the entire trial into a single thing, where the sort of sequential effects was sort of hard to actually model within the Westphalovagner framework.
*  And so the key advance in TDRL is that this allowed you to actually form these actually fairly good time resolution, like predictions with good time resolution.
*  Right.
*  And it's, I mean, correct me if I'm wrong, but it's attractive because it's a fairly simple idea.
*  Exactly.
*  And so that's another reason, you know, Occam's razor sort of approach, it's appealing in its simplicity.
*  Exactly. Yeah. I mean, it's super elegant, right?
*  I mean, the core idea of TDRL is extremely elegant, the basics of it.
*  Right. And so, I mean, as far as, you know, like a didactic version of TDRL goes, you know, the one that I teach students, it's a beautiful theory.
*  Because like it has like it just has the core elements that you need to actually explain the phenomena that you want to explain.
*  Right. I didn't I didn't realize that you were teaching students the history of this stuff, too,
*  because then you have like a real left turn probably midway through the semester with your own work.
*  Well, actually, I don't teach students yet my own work, because I actually do just one lecture on this stuff for the for the core systems neuroscience class here for the grad students.
*  And so, like, in that on that resolution, I feel like they need to know TD and West Carly Wagner way more than they need to know my work.
*  I see.
*  Because the context of that is way more important.
*  Right. I mean, to get the because that is the way of thinking.
*  And I'm I'm trying to get for students, I'm trying to get people who have not thought about anything related to this, like from all backgrounds in neuroscience to actually start at least thinking about the problems in neuroscience in this sort of way.
*  In sort of an algorithmic way, in an algorithmic way.
*  And so for that, I think our sort of work comes later.
*  OK, that's not quite fair. Yeah, it comes on the brain.
*  I will be teaching VJs work next semester.
*  So I'll do that. You don't have to teach.
*  So yeah, I would be teaching a full course then.
*  Yes, it's a full course on neurobiology of learning and decision making.
*  So causal association and dopamine.
*  And where where will VJs work come in?
*  What what have you got the syllabus ready yet?
*  Yeah, I mean, I have like 85 percent ready.
*  I'll send it to you. Yeah.
*  I realize we're burying the lead, but that's OK.
*  We're building attention.
*  Well, because so so you just very nicely told, you know, like that historical
*  temporal difference learning story and and why it's attractive
*  and how it worked and how, you know, ushered in this kind of algorithmic thinking.
*  But then so then why are there so many other theories?
*  Why are there so many various theories that all have
*  some supporting evidence? Yeah.
*  Et cetera. It's a good question.
*  So actually, before I get into like our own stuff, I want to sort of still stick
*  with a historical perspective.
*  So while while the TDRL stuff was becoming very successful, right?
*  Like where Schultz's seminal work, like you start recording dopamine neurons
*  are putative dopamine neurons in Joseph's case.
*  You find that the activity of these neurons in a very fast temporal
*  resolution actually seems to abide very nicely with this prediction or sort of idea.
*  And that's very powerful.
*  So that stuff was going in parallel.
*  You also had all this data on like dopamine pharmacological
*  inactivation type things where you actually like an active dopamine
*  on longer time scales.
*  And then you look for whether dopamine is important for learning,
*  where it's a dopamine is important for other things. Right.
*  And in general, the story there was much more complicated
*  like the story from the the TDRL side was very simple.
*  Now dopamine is really about learning. Right.
*  I mean, here's this we found the magical signal that is useful for learning.
*  It's prediction error.
*  So that story was become it's elegant and and very nice and narrow.
*  And that's what you want.
*  Like powerful theories to be right, like simple and and broad in scope
*  in terms of explanatory power.
*  But at the same time, you do an experiment. Yeah.
*  So like one of the things, one of the mysteries in the field
*  that is still not resolved and that's not even and not related
*  and not solved, I guess, including in our own work,
*  is the idea that like there are forms of learning that that people find
*  that dopamine is not important for as best as we can tell. Right.
*  For with like pharmacological studies.
*  So the classic example of this one is sign tracking
*  versus goal tracking. Right.
*  And you can show it. Are you aware of this?
*  No, no, no. What's sign tracking?
*  Yeah. So so if you this is all work from like rodent studies. Right.
*  And so what you find is basically that if you do just like a simple
*  Pavlovian conditioning. So the idea is you have a rat.
*  Let's say is there's a lever. Right.
*  It doesn't have to press the lever.
*  And then there's, let's say, a port where you got a signal.
*  Or where you get the reward.
*  And let's say the lever is the queue.
*  So you use the lever as the queue.
*  So the lever just comes out before the reward is available.
*  You don't have to press the lever at all.
*  And then reward comes later. Right.
*  And so the the idea.
*  So when you take normal rats and make them do this task,
*  you find that broadly speaking, there are two classes of animals.
*  Both class of animals learn the association between the lever and the reward.
*  But one class of animals, actually, once they learn that the lever
*  predicts the reward, the moment the lever comes out, they'll go out
*  and hang out by the reward port.
*  So those are called goal tracking animals because they know that the rewards
*  are about to come and they're right there by the reward to actually collect it.
*  Sign. So by sign, you kind of mean Q.
*  It's a synonym for Q. Yes, exactly.
*  And so the alternative is the sign trackers.
*  Those animals actually, when the lever comes out, they go to the lever.
*  And they start messing with the lever, press the lever, bite the lever
*  to the lever, all sorts of things.
*  They don't go by the award.
*  And then when the reward actually gets delivered, they'll walk over
*  and collect the reward. Right.
*  So it's this weird dichotomy.
*  And it but both of them are learning.
*  But they're both learning about Pavlovian associations in that
*  the queue predicts the reward.
*  They both know that.
*  Except the the the sign tracking animals are actually
*  thinking they're learning some sort of operant conditioning, I suppose,
*  because they don't have they're not passive.
*  Yeah, they're not passive.
*  But it's not clear that they think that they need to press the lever.
*  Sure. Right. Yeah.
*  But but yeah, either way.
*  And then some of the behaviors that they produce is not just like, you know,
*  typically in operant lever pressing status, they just go and press the lever.
*  Right. And they don't do anything else.
*  But here the sign trackers are different because they actually go into the lever.
*  They do even like conservatory type things
*  as if the lever itself is a food or like the lever itself is a reward.
*  Right. It's it's it's a weird thing.
*  And and what you find is that dopamine is actually not needed
*  for for that type of behavior, for the sign tracking.
*  That means actually so you cannot get gold tracking,
*  but sign tracking seems to be independent.
*  So you so wait.
*  So you deplete dopamine and then all of a sudden all you have are side trackers.
*  You do not deplete dopamine and then you have some side trackers and some gold trackers.
*  Yes, exactly. Yeah.
*  And so so that's weird. Right.
*  And so then the so the idea was basically that
*  you have this whole type of behavior,
*  behavior learning where animals actually don't seem to need dopamine.
*  And so so that was sort of a parallel literature,
*  essentially, in that, like, what exactly is dopamine useful for wasn't actually clear.
*  But they're learning something.
*  They're learning something. Exactly.
*  You know, all of this was a sort of fodder for the other theory
*  that dopamine is actually important not for learning per se,
*  but for ascribing value to cues.
*  Right. Like incentive motivation.
*  And incentive motivation.
*  Yeah, that goes hand in hand with I was going to say with motivation.
*  Motivation. Exactly. Goals.
*  Yeah. Yeah.
*  Which makes me think that it's the other way.
*  So now it's not black.
*  It's a sign tracking, perhaps. Right.
*  So I would say that it's a sign tracking. You're right. Yeah. Yes.
*  I mean, if we are to recreate the theory,
*  I mean, that makes total sense for dopamine to be needed for sign tracking.
*  Yes. With incentive value idea that you are
*  attributing value to this cue. Right. Absolutely.
*  As Kent Barridge would say that it's like a magnet.
*  That sort of like approach behavior is an important thing in motivation
*  research, at least in rodents or animal work that like when you're motivated,
*  you are more likely to approach a reward or like a rewarding cue or something.
*  And that becomes important in like drug addiction research as well.
*  So it's not the reward itself that you're attributing values to.
*  It's the sign that is the cue that you get attracted to.
*  And that's why when you're re-experiencing
*  an environment where you had a high at you, some drug related incident happened.
*  You will recreate all these.
*  So you chew on the crack pipe instead of loading.
*  You may do that because there's value in that cue itself, not the high.
*  And in transit, dopamine is not related.
*  And it makes sense.
*  What we mentioned earlier, that dopamine is not the pleasure signal.
*  People still, if you go ask in the street, people say dopamine is a pleasure signal.
*  But if you ask me like a popular neuroscience communicator, they would say that, you know.
*  Yeah, I mean, they would say that cold shower also has some effects on dopamine.
*  But you mentioned you mentioned Barrett there is Barrett the like the liking.
*  Liking wanting, right?
*  So, yeah, that is exactly the idea.
*  So liking would be the pleasure, right?
*  I mean, when you like something, it is the pleasure part.
*  But then wanting is the approach is the motivation part.
*  That's more related to dopamine.
*  Yes. OK. I did mix that up.
*  I've said everything correct, except for the fact that like dopamine
*  is not important for sign tracking.
*  It's actually crucial for sign tracking, not for goal tracking.
*  Oh, OK. OK.
*  Yeah, which is exactly what I was leading up to, that it is important for incentive salience.
*  So I just mixed them up.
*  So what is the incentive salience?
*  What were you going to say about that?
*  So the idea about incentive salience is basically so it's a second hit on your chat dbt, right?
*  And I think so. Yeah.
*  So the idea for incentive salience, exactly what Ali was saying, that
*  that basically dopamine is not important for learning per se.
*  It's actually important for ascribing sort of motivational properties to cues.
*  Oh, this is related to your work that we'll come to, perhaps.
*  Perhaps. Yeah, that's right.
*  They're all related. They all they all have like related components, right?
*  And so so the idea was basically that
*  like the animals that actually go and chew on the lever and stuff.
*  That seems to be dopamine dependent.
*  But the animals that just go and hang out by the reward port
*  where they know exactly what's going on,
*  that seems to be dopamine independent.
*  OK. And so so this is a controversy in the fields at the time
*  where you had all the shul stuff developing,
*  where it was like more and more evidence that rapid
*  time scale dopamine activity is all related to learning, crucial for learning.
*  But then here is one type of learning that dopamine,
*  in fact, you would argue in like a simpler form to understand,
*  doesn't require dopamine.
*  Right. And it's actually and actually you're learning.
*  If you're learning, you're learning the wrong thing.
*  Yes. Yeah. Yeah.
*  So now what is right versus wrong is unclear because like it's not
*  offering, right, because the animals are free to do whatever they want.
*  OK, you're learning the less.
*  The thing that is going to
*  decrease your survival and therefore decrease your evolutionary lineage.
*  So one could say if evolution is normative, you could say that's the wrong thing.
*  Yeah, I know. Yeah.
*  That could be an interesting discussion.
*  But but actually, so let's say we skip that discussion because there are.
*  I just mean, OK, how about a simpler thing is you're going to get satiated less quickly.
*  If you if you are basically which say if you're hanging out,
*  if you have the dopamine, you're going to chew on the lever.
*  Yes. That means you're going to get the reward a second later.
*  Five seconds later. Yes.
*  The the argument, I think, is that now I need to go back to the data.
*  I think that as best as I remember, these animals were not slow at picking out the reward.
*  I mean, they were slightly slower, but not that much slower.
*  The complex thing that I was going to say is that the argument is that like
*  if you were to go by the lever and chew on the lever and all that,
*  maybe that is exactly what increases your survival and that like,
*  you know exactly what the cues are that are important.
*  But it's more important just to see the lever and make a beeline to the.
*  All right. So anyway, I'm not picking.
*  Yeah, I agree.
*  But I'm just saying that there are different views that people have heard people say.
*  Yeah. So, OK, cool.
*  So that's the argument.
*  Yeah. So, OK, cool.
*  So that was a controversy that was happening, you know, while the school stuff was developing.
*  Right. And that's a lesser known story.
*  Yeah. Lesser known story in the, I guess, like the recording systems,
*  neuroscience community, but in the dopamine community.
*  Yeah. Well, no, I'm not.
*  That's the thing that's specifically in the drug addiction.
*  Right. Because big part of NIDA is National Institute of Dopamine.
*  Right. So a lot of funding.
*  So like addiction is like dopamine research has been an important part of addiction research.
*  And I would say that that incentive value has been very dominant and influential in that field,
*  but maybe not as much in the learning and reinforcement learning at AI fields.
*  Yeah. And I was going to also say, I mean, Vijay, one of the things that you point to is work
*  by people like Randy Gallistel.
*  And one of the things that you are deeply familiar with, as is apparent in your work,
*  is all of this literature on learning and different modes of learning, which I think,
*  as Randy Gallistel has pointed out to me, is like super important and super rich history to draw from,
*  to test these different things, that there's so many different ways you can test learning
*  and different facets of learning that will key into whether your story is more or less correct.
*  So that's an important thing that I have missed out on.
*  And so I need to revisit all that literature, but I don't know where to begin because it's so vast.
*  Yeah, I know. I agree. So I mean, Randy's stuff, I would say genuinely is lesser known
*  when it shouldn't be. Right. I mean, like everyone should be knowing this.
*  Like everyone interested in learning should know all the literature that Randy has talked about.
*  I mean, and collaborators of Randy have worked on experimentally.
*  So that literature is like absolutely crucial. I mean, it forms some of the bedrock in our own
*  thinking of how our own work evolved a lot from Randy's core ideas and the listing of the problem.
*  OK. Yeah. I mean, one of the nice things about that, I keep pointing to Randy,
*  but that's just because I had him on the podcast, but about that kind of research is
*  the tenor of it is you say, well, look, like we can show that animals behave in this way,
*  and it just doesn't work with your algorithm. It doesn't work with the account that you're giving
*  of a circuit level mechanism. So therefore, and so you have to account for those sorts of behavioral
*  findings. Yeah, absolutely. And so that has been a tricky one. Right. I mean, so explaining.
*  So just to back up, I mean, in case like listeners aren't familiar. So Randy has,
*  and this sort of can lead into sort of our own work. Randy has shown experimentally, not Randy,
*  but like Peter Balsam, others, John Gibbon, etc. And then Randy has sort of summarized them in
*  a very influential paper. And then he's sort of talked about this for a while now and has published
*  a lot of papers. And so they showed that, you know, like, you know, like, oh, on the one hand,
*  there's this like competing sort of idea, right? Like, there's the temporal contiguity idea that,
*  like, if Q and reward are separated from each other, you learn less about them. Right. And,
*  and then there's a there's a different literature that shows that if you space things out more,
*  then you learn more from each experience, right, just at a qualitative level. And so like, how
*  these two things interact was not clear at all. Some of the old literature actually suggests some
*  like extremely interesting sort of relationships between both of those, both of those aspects of
*  learning. So what they found was that, and this, so actually to back up, so in TDRL, contiguity,
*  like temporal contiguity, even though, like we say that it moved beyond temporal contiguity in
*  by saying that error prediction and prediction errors are a thing that drive learning, right?
*  Temporal contiguity is still a factor in the learning in that, like, if you separate Q and
*  reward more, there's more temporal discounting. And so you actually end up learning less, it's
*  harder to learn. Right. And so temporal contiguity is still a factor. But the the interesting thing
*  in the in the other literature, vast literature, as you're saying, shows that if you actually
*  increase the Q reward delay, and I'm saying Q reward, but a lot of the stuff was actually
*  done with punishments and stuff, but like, I'm just going to stick with Q reward just for
*  simplicity. So if you increase the Q reward delay, it's not clear, it's not actually apparent in the
*  data that it always increases the time to learn or the number of trials to learn. Right. You can have
*  the Q reward delay be long. But if you increase the intertrial interval, also in a proportional
*  manner, you find that the number of trials that actually takes to learn is actually conserved,
*  it's invariant. Yeah, very counterintuitive, and very profound, right? I mean, if you like the type
*  of things that you want in an algorithmic way of thinking is to look for invariants in the data,
*  right? What are the things that don't change? And this is a very powerful invariant, right? And so
*  if that's true, then I mean, this should be the thing that everyone should be modeling, because
*  this is the core thing that any algorithm should capture. Right. But it turns out that I mean,
*  like a lot of the original work that summarized this was a meta analysis in the early 80s, but
*  from work in the 70s. And then Randy's paper popularized it in early 2000s. But it's largely
*  been ignored in the neuroscience community. Right. I mean, very few neuroscientists.
*  It's terribly inconvenient.
*  Yes, exactly. It is terribly inconvenient. And especially to the dominant model, like the TDRL
*  model, it is inconvenient. And so I think that's sort of the reason why maybe it's been ignored.
*  And that I think is a fatal flaw. And I mean, that's for sure, I can say pretty confidently
*  that we should rectify that as a field, right? And so the difficulty is that in a standard TDRL
*  sort of view of how learning works, these sort of invariants don't naturally come about.
*  Okay.
*  And so you can maybe bake that into the system into the algorithm, but that's not like it's a
*  natural thing. It's not a natural thing, right? You're just adding in that thing as a constraint.
*  And so that's not really satisfactory. And people have taken cracks at it and found that,
*  you know, there are ways to sort of take the error based idea and try to capture this,
*  but it's not in an easy way that captures all the other things that don't mean.
*  So this is, I think, a somewhat unsolved problem on the TDRL side of things, right?
*  I say somewhat unsolved in that, like, there's been some attempts, but it's largely not
*  being shown that, like, you can actually capture the standard don't mean things
*  while also capturing this in the same framework.
*  So you're telling the story as if this is like the history of your thinking about it almost,
*  right? And then this led you to the conclusion that you needed something new, something else,
*  something to account for. Is that how what I want to know is where your idea, which is a
*  fairly simple inversion. Yes.
*  Yeah. TDR like how did how you came up with that?
*  What led you maybe you could just really briefly say what the idea is and then we can unpack it
*  more because I want to know how you came up with the idea because it's so simple, man.
*  Yeah, it is a very simple idea, right? The core essence of it is extremely simple.
*  So to explain what the idea is. So the core idea of TDRL is that the way that you learn predictions
*  is by learning through prediction errors, right? So you actually have you make a prediction and then
*  you look at what actually happened and there's an error and then use that error to actually update
*  the prediction. So there's a bell and then usually you get a cookie after the bell and then sometimes
*  there's a bell and you don't get a cookie and you learn, you know, then you have a different signal.
*  The first time you get a cookie after the bell, you weren't expecting it and the dopamine says,
*  whoa, there's I was not predicting that and there's a big error because you get the reward.
*  Exactly. And so these signals
*  are modulated based on your predictions and the error that is generated.
*  Exactly. And so the critical thing here is that everything that you need in this algorithm is
*  forward looking, if you will, in that like what you're always looking for future relationships,
*  right? Exactly what predictions are. And you call that prospective.
*  Prospective, right? Exactly. And the alternative view, just to state it simply and then we can go
*  to the history and how this came about. The alternative view is that actually the way that
*  there's a different way that you can learn associations and it's simply by looking backwards.
*  Right? So imagine that you got the ice cream or something like the output of the cookie,
*  then you look back to see what might have been the thing that caused this.
*  And so if you consistently find that something precedes it, then you know that those two things
*  are associated. And it turns out that you can show mathematically simple base rule type things
*  showing that like if you know the backward associations, you know the forward associations
*  too. You can compute it very easily. So here's my guess. Here's my guess is that
*  you were just thinking about Bayes rule and then you thought of the prospective story as part of
*  some conditional probability. And then you're like, I could just reverse this with Bayes rule.
*  Am I right? That is definitely was one of the steps.
*  I'll take it. Yes. So it's very close. I think so now going to like how this evolved.
*  Actually, maybe before going that, like just to say that like, you know, why is this so intuitive
*  in some sense, right? Like the version of the story that I give in talks to get people intuition.
*  It's also one that I tap into consciousness for just because that's the easiest way for people to
*  actually tap into the intuition. But all of this may be subconscious, right? But the core intuition
*  is this. Let's imagine that you just one day you just feel sick, you feel nauseated. And you just,
*  you know, like stomach pain, etc. You feel like you've been food poisoned or something,
*  you ate something that you just didn't like. And what do you do? You, I will say, like pretty
*  much everyone looks back in their memory to think about what is the thing that they might have eaten
*  that would have caused this. And let's say that you happen to eat at a new restaurant that day.
*  I probably bet that you probably not go back to that restaurant again for a little while,
*  because you think that that you will associate the eating at that restaurant with with getting sick.
*  Now, from that example, there are two things that are clear. One thing is the way that you
*  associated eating at that restaurant with getting sick is a backward is fundamentally backwards,
*  right? When you were eating at the restaurant, you weren't thinking, are my going to get sick?
*  Am I going to get sick? It's just that when you got sick, you were actively looking back in memory
*  to think about where you ate. And then you realize that there's a possibility possible explanation,
*  I just, I just, you just made me realize I believe my wife is slowly poisoning me. Go ahead.
*  I couldn't resist. I'm sorry.
*  Yeah. And so the other thing that you realize from there is that once you've associated eating at
*  that, you know, eating at a restaurant with illness, then you can invert that retrodiction
*  to a prediction, right? You intuitively do that. Like, the thing that makes you decide not to go
*  there is not that it's not the retrodiction. It's a prediction that if you see the restaurant,
*  you will probably imagine getting sick. And that's why you don't want to go there, right? So you've
*  intuitively converted a retrodiction to a prediction. So intuitively, it seems like this
*  process is happening, right? That you learn a retrodiction, and then you convert that implicitly
*  to a prediction. Question is, how does that happen? And base rule is the answer.
*  Why has this not been thought of years and years ago, or proposed or has it in some form?
*  Yeah. So this was the thing that like I so once you know, like this sort of worked out,
*  get into that a bit more. But once I worked out the full theory on the retrodiction to
*  prediction conversion, I was just puzzled by exactly this thing. I mean, it seems such a
*  simple idea. Why is it this not being thought about? And so the answer is after after working
*  this out, like, and I'll tell you, there are some key differences in the way that we worked it out
*  from the prior attempts at it. I did realize that people have thought of similar things. So in fact,
*  in one of the original things that kickstarted the whole field is commons blocking experiments
*  that preceded Rascarlo Wagner. And commons explanation for blocking is something similar
*  to this, that you basically look back, he called it backward scanning. And the idea is that you look
*  back to find associations, right? And what is blocking? Let's just say what blocking is. Sorry,
*  I know, because it's kind of a it's just a technical term. Yeah, absolutely. And I mean,
*  this is something that I sort of alluded to at the start. So this is actually one of the most
*  sort of influential results in the in the early days in psychology, which actually moved the
*  field from temporal contiguity as the as the key thing to learning from errors. So the the idea is
*  this. So imagine that you first teach an animal that one cube predicts a reward. A lot of this was
*  done with punishment. But again, like, I'll stick with rewards. You predict reward, you've already
*  learned this. And then in the second phase, what you do is you while you present this queue, you
*  also present another queue, and then give the same reward. Right? Now, this queue is obviously
*  temporarily contiguous with this reward. So if it's just temporal contiguity that you're learning,
*  that is the thing that allows you to learn, you should learn the new cube reward relationship as
*  well. But it turns out that in general animals don't, at least they don't show behavioral evidence
*  that they do. Right. And so then why is that? And that's called blocking in that like the first
*  queue actually blocked the ability of the second queue to learn to learn a new queue to learn a new
*  associate association, right? And and so the idea there was that Raskar Lavagner showed that like,
*  well, if this is all driven by error prediction error, then this works. Because like, if you if
*  this queue already predicts reward, then the second time this queue is associated with reward, even
*  though there's something else, this has already been predicted, there's no prediction error here.
*  So you don't learn the second year. Right. But, but the original explanation that the person who
*  discovered this actually gave was not that was actually that you're looking back for cost is
*  typing. Right. So it is a retrospective backwards scanning. And then that's similar to the to your
*  idea that well, so that so that first queue is predictive. If you're looking back, that first
*  queue is predictive 100% of the time. Exactly. And then the second queue is just predictive less
*  percentage of the time because it was introduced later. It was introduced later. Exactly. Yeah.
*  And you don't need to attribute causal significance to that thing. Right. Like there's always
*  distractors that happen in your world. So you don't need to assign cost causality to everything that
*  precedes something, right? You want to know that it's consistently preceded. Yeah. Yeah. And so,
*  so this idea, so that was maybe the original form of the backward retrospective view. Now,
*  the critical thing that was missing there was this inversion from the retrospective to
*  perspective. Right. In that view, there, this was proposed as a way of learning associations, but
*  it was not mathematically formalized as a way that that you could actually take this
*  retro addiction and then convert that to a prediction, which is finally the thing that
*  you want to learn. Using Bayes. Using Bayes or any other form. Right. Sure. So, so that,
*  so that was missing in that explanation. Now, the other time that this retrospective thing was
*  actually in the literature was actually Randy's work. So Randy, as we touched upon briefly,
*  had this idea that has this idea that all of this is just based on a temporal map or a cognitive map,
*  if you will, of time. And so if you just store the exact time of when everything happens in memory,
*  and your memory is perfect, let's say, then you can arbitrarily go back and think back about
*  whether things proceed something or things are followed by something. And so then the idea is
*  that retrospective makes as much sense as perspective in that sort of a computation.
*  I see. And so then you can have, and they could in fact be not directly, and in the way that he
*  writes it, it's not just a direct base inversion, because it's a slightly different framework of
*  the perspective versus retrospective. But either way, so that was one other place where the
*  retrospective had made its appearance before we published it. But did you discover this
*  after you had your ideas? Yes. And which is funny because Randy's work is sort of been like
*  one of the core sort of drivers for me in terms of my, at least thinking about the philosophical
*  problems of learning. And Randy's just published so many papers.
*  How could you be expected to? I mean, it takes a career just to follow someone else's career.
*  Exactly. Yeah. And so it was just like, yeah, I just didn't know about this. I mean,
*  there's a very specific paper where Randy talks about the retrospective thing, and I just didn't
*  know about it. Oli, how many, has that happened to you frequently where you have an idea, you start
*  working on it, and then you realize it's been done seven times before in different ways?
*  You know, my ideas are so original. Of course. Yeah. So here too, then, with the retrospective
*  thing, there was not this perspective, conversion of a retrodiction to prediction didn't exist in
*  this way either. So that part is new, I think. So the way that we proposed it, I think that was the
*  first time that that came about. Right. Because you need that because you do need to behave
*  moving forward. Exactly. Moving forward. Exactly. And also, just given that we are on the historical
*  sensitive, this idea, so like, you know, there are two threats to how I realized this. One thread
*  is that like, when I first learned reinforcement learning in TDRL, I had this vague feeling that
*  something was missing. And that was just the time component of it. It took me like about 13 years
*  to verbalize what I thought was missing, right? It took a long time. I knew that it was something
*  related to time, but exactly why it was hard, and we can get into that later. And so that was one
*  thread where I felt I had some dissatisfaction with the standard view of TDRL. But the retrospective
*  versus perspective thing was sort of almost independent of that thread in that that came
*  about because like I'd actually collected some data in my postdoc where I found some weird
*  patterns of responses in orbital frontal cortex neurons that projected VTA. And that didn't make
*  any sense to me. And the only way that I could sort of, you know, like post talk rational,
*  retrospectively rationalize the results was to think about this retrospective framework at the
*  time. So then, so the first time that in my published work that I mentioned retrospective
*  is actually in the discussion of that paper. And because that paper was, it's not like designed
*  to test this idea in any way. So it's only makes an appearance in the discussion, right? Because
*  it was like a loosely formed idea at the end of that paper, to just sort of roughly qualitatively
*  explain the patterns of findings that I found. And then so like I had that idea that like maybe
*  this retrospective thing could work. The problem with the, you know, like the problem the way that
*  I was just describing it is that all of this is the Raskar-Lavagner type equivalent. It's sort of
*  a trial based view in that like you're just thinking about does the Q procedure reward,
*  right? We're just looking at whether on a given trial Q procedure reward versus not.
*  But the core advanced or TD is to go from that view to a time based view, right? Like where you
*  have time differences. And that actually required additional work to show that the same retrospection
*  or retro addiction to prediction work would actually hold in this in the standard way that
*  people do TDR out. Because we have like the Bayes equation has nothing to do with time.
*  With time, exactly. It's just a conditional probability thing, right? And conditional probability
*  alone does not get you to these long run sums of like value type things that you define in TDR,
*  which we can get into as well. It turns out time is fairly important in life.
*  Yes, exactly. And it's tricky to think about how time plays a role in this sort of stuff,
*  like in learning. And so now, like now that we've maybe sort of built up the intuition for these
*  things, I can start to now poke holes in the standard way that people assume things work in
*  TDR. And that's sort of it's when I realized these things that I was able to verbalize what I thought
*  were the problems. And the question is like, how can you then try to figure out a solution for this?
*  And that's where the I sort of made the connection between the retrospective
*  thing and then the problem with the time stuff. Oh, nice. And so the the views this right, like
*  standard TDR, all the ideas, I mean, set in the in the core didactic way, it's a very simple idea.
*  You simply you just have, you know, like a queue and a reward here. And you make a prediction here,
*  and that you make a prediction that value zero or something, at the next moment, let's say,
*  and then you break up time into these small components, time bins, and then this time bin,
*  you're predicting that nothing will happen. But then you actually get reward. Right. And then when
*  you get reward, you actually are surprised. So you get a prediction error, you ascribe that prediction
*  error to something that came before it. And then the standard didactic view, you actually ascribe
*  it to the thing that immediately proceeded it. And that's, let's say a time step. And so you assign
*  value to that time step. And then the next time the state before that gets value for that, from the
*  next state, and so on and so forth. And then eventually you go assign value to the queue.
*  That reward prediction error is a dopamine hit. Exactly. And since just to bring it back to
*  dopamine. Exactly. So that's the standard TDR view. So now if you think about how this works in reality,
*  well, let's so the thing that you're assuming happens here is that you keep track of time
*  from the queue, right? Now, in the simplest way, that's obviously a simplification that like you
*  keep track of time perfectly, bin by bin. And that obviously everyone knows is a simplification. So
*  no surprises there. But still the critical assumption is that you are keeping track of time
*  from the queue. Now, this raises a problem. Right. So when think of how many queues there are in our
*  environment, there's so many infinite, right? I mean, there's no way that that's not infinite.
*  And so are you keeping track of time from every single view that you experience?
*  And are you doing this in parallel? So like, you know how much time it's been since that queue,
*  and then this much time since that queue, and all of those things are happening in parallel.
*  Right. So every queue, you need to have a separate clock for how long it's been since that queue has
*  happened. You should look up Mark Howard's work. He's been so he might say that you can do that
*  using a Laplacian transform, but no need to revisit that. Right. Yeah, very similar with
*  Mark's work. Yeah. And Mark's stuff has relationships to the learning as well, which
*  is sort of a different angle to this whole thing. But I don't think that he solves this particular
*  problem in that like in the standard TD way, standard TD way, right? We're still talking
*  standard TD. That like each queue, you're assuming that you keep track of time from there. Right.
*  And then when you ascribe value for that reward or prediction error, you are ascribing value to that
*  queue in terms of how long it's been since that queue has happened. Right. So for each view,
*  you assign it differently. And the other critical assumption is that, you know, like,
*  you know, like all of TDR all depends on states as the as the input to this to this algorithm.
*  Right. So basically, the idea is that TDR is an algorithm that operates on some inputs, and gives
*  you some output. The output is the value. The input is the state state of the world. Right.
*  And this in standard computer science stuff, this all makes very simple sense and states are,
*  you know, something that you know, a priori in an engineering sort of setting, you know,
*  exactly where you're trying to learn. So it's very easy to define all this. Now, when you're
*  trying to take that architecture and ascribing it to animal behavior, you have to make a bunch of
*  assumptions. So, so basically, the state inputs that you give to this algorithmic
*  black box, if you will, I mean, it's not a black box, obviously, we know every component of it.
*  But the inputs that go into it are are what exactly right, like, what are the state inputs?
*  So essentially, what you need is a state input that tells you that for every time moment or
*  every time step, you you something tells you that this is the current state of the world.
*  Right. Because TDRL fundamentally is about temporal differences.
*  And then what that means is there's one time step and another time step. And you're calculating the
*  difference in predictions between these time steps. And to do that, you need to know what state your
*  what is the thing that allows you to even make a prediction at this moment, that is the predefined
*  states, essentially, exactly. And now when you think about that sort of thing with this problem,
*  standard sort of Q happened, let's say trace conditioning. So nothing is happening. There's
*  a lot of delay. So Q happened. There's a lot of time delay where nothing else is happening.
*  No external input is being given to the animal and then a reward comes.
*  Then you need to have a state defined for every every moment in time. Right. And how do you define
*  a state for every moment in time, the state that you define for every moment in time is basically
*  that you define the time since the queue. Right. That's the only thing that defines the state of
*  that one, even though that moment is a it's just like a delay period. Right. Any other delay period.
*  There's nothing else externally that tells you what this moment of step time is. Yeah. Right.
*  And so now you have to keep sort of like you have to arbitrarily keep track of nothingness,
*  keep track of nothingness from every possible previous thing. Right. And going back to that
*  for just a second, like the I sort of thought about this just right after I said, you know,
*  it's infinite. It is infinite in principle. However, there are salience differences in
*  queues. Right. Absolutely. Yes. Like not all objects are as shiny as others. Absolutely. Anyway. Yeah.
*  Absolutely. So so you can narrow it. It's if you add the attention component to it or bottom up,
*  let's say, yes, and or top down. That really narrows the search space, the search space down. Yes.
*  There are still problems with that view. But I mean, yeah, I mean, each of these can be
*  recent to and then there are some problems that come in some solutions and then like you keep on
*  going. But like this could be a whole a whole day. So may just add that the salience also has to be
*  learned. Right. Yes. I mean, yeah. Unless it's like a hawk coming down if you're a mouse. Yeah.
*  Something exactly. Yeah. Yeah. Like loud thought of a door. Like you don't necessarily have to
*  learn it. But even so. Yeah. I mean, I won't get down that argument just yet. Yeah. Sorry. Yeah.
*  Really, it's absolutely the very good point. Right. I mean, these are some of the good points
*  that you need to start to consider as you go down this pathway is that each of them will have its
*  own set of issues. So just sticking to the main thread. So essentially, you are trying to keep
*  track of time from everything. Now, the important thing is for you to ascribe value for this reward
*  to the state that preceded it, the state that preceded it should be a repeatable, identifiable
*  thing. Right. At least in the standard view. So if it's a repeatable identifiable thing,
*  then that means that whatever the neural state is in the brain at the time that just proceeded
*  that reward should on the next time that the cube got presented at the same delay, you should get
*  the same state brain state in the brain. And that seems just hard, right? Like to get like,
*  exactly the same type of things repeated reliably trial after trial, and keeping track of time that
*  precisely. That seems hard. Now, you know, like people will say that like you can get TD type
*  things to actually work without it. And that's the whole other, you know, set of discussions too. But
*  like I'm talking about the standard didactic view. Yeah. And even like in a natural in a natural
*  living condition, even if you have like the exact same cue, the exact same stimulus, the context is
*  never ever going to be the same. Exactly. There would be never be perfectly repeatable. Exactly.
*  And so and remember, you also have to keep track of time since every cube, right? Because you're
*  trying to like, obviously, the time since every cube is not going to be repeatably the same.
*  Hard. It's hard. It's hard. It's a very hard problem. And so if if you're trying to learn this
*  this way, then it is difficult. And, and it turns out that if you are trying to do this in
*  a prospective way, that like, when you get any cue, you're trying to learn what it's following,
*  what is following that cue, you kind of necessarily have to do this sort of thing,
*  right? Like of keeping track of time step and the passage of time, because time step by time step,
*  because what you need to do is, whenever that thing is happening, you need to give more weight
*  to the queue, and that you've ascribed more predictive power to the gear. But when it's
*  not happening, you need to downweight the prediction to the queue. So you kind of because
*  you just don't know when future things are going to happen. You sort of necessarily have to do this
*  time step by time step for everything, just going for everything going to infinity, essentially.
*  And your insight is that if is if something happens that is awesome, or that you want to remember,
*  or that you want to repeat, then it makes more sense to start looking back in time to see what
*  was paired with that with high correlation.
*  Exactly.
*  However, then but don't then you then you need to look at an infinite number of things in the past.
*  Exactly.
*  This one's zero, zero, zero, zero, one.
*  Yes. So the problem doesn't entirely go away. So, so, you know, there are different aspects.
*  So sort of like the there are, in some sense, the core insight of our work is that there's maybe two
*  steps to learning. The first step is to know that something is possibly related to something else.
*  Okay.
*  Right.
*  Like a threshold.
*  Yeah, exactly. Crosses some threshold. You're just simply trying to make connections.
*  Does this connection exist? If you believe that that connection exists, then you can go in and
*  then try to understand the properties of that connection. Right. Like what is a temporal delay
*  there? What is the associated probability of reward, that kind of stuff. The critical thing
*  to first know is just that is whether there's a connection. And if you're trying to do that,
*  then you do have to keep track of the different cues in memory. Now, here's where the retrospective
*  view actually allows you to solve the salience problem in some sense. You do need to keep all
*  these cues in memory. But you could, you know, like when you're doing these backward sweeps,
*  this is not part of our algorithm, but like you could in principle add this. When you're doing
*  these backward sweeps, you could sort of do backward sweeps with varying levels of thresholds
*  for the salience if you wanted. You only look at the most salient things as a thing that you could
*  aspire to. And if you don't find anything that's related that way, then you bring down the salience
*  a little bit and then you look back again and you look for search for things that might be preceding,
*  but in a salience way. And it's still sounds hard, but the point is that I mean, I think something
*  of that sort must happen, right? I mean, not necessarily the retrospective, but something
*  off the way of filtering the salience type things and storing things in memory must happen.
*  And the core thing that we're sort of going, the core advantage of going this way
*  is that one, we're using the fact that we have memory and we're storing things in memory,
*  right? And then that allows you to actually look back for associations. And the key critical thing
*  that this gets you in the base framework is that now all the associative components or the
*  associative learning where you're learning associations, it's all only triggered when
*  events come, right? Rather than updating your associations time step by time step,
*  every time step, which is a hard thing for every possible thing and every possible outcome too,
*  right? It's not just, we know that we simplify and talk about just reward learning. We know that
*  animals have different predictions or different types of rewards. We already also know that
*  animals also learn Q2Q relationships. So in reality, this is much more complex, right?
*  And so for all of those associations, you should do time step by time step updates going in the
*  forward direction. I don't think even Ali can do that. I'm not sure, but.
*  Ali might be the only person. He's talented. I know that. Yes. The only animal on earth.
*  He is an animal. I'll give him that.
*  And so the advantage going in the backward direction is that you can do this in an event
*  based way, right? And so now you don't need to do this every time step by time step. You just need
*  to do it when something very meaningful happens. You just update backwards and take timestamps of
*  are things proceeding this in a reliable way? That's at least a few orders of magnitude easier.
*  Exactly. Exactly. And so that was sort of the core insights. And this doesn't solve the core
*  problem of how do you learn the time delays and stuff, because I've sort of ignored that part
*  here. So here the core aspect is you're just still looking for things, whether they proceed
*  meaningful things, right? And you, of course, account for the time delays in the memory
*  of what you store. But knowing exactly this algorithm doesn't tell you, well, this is a
*  number in terms of what is the delay between the Q reward association. So that we sort of pushed
*  aside and said, that's a second step to learning. Like you actually have to learn that, but it's
*  much easier to learn that if you already know that this particular Q is very important and this
*  particular reward is very important. And that association is actually where you're trying to
*  measure the time to live between. And this is where something like replay might come in very
*  handy, right? If you're just replaying offline over and over and over, that's sort of an auto
*  learning system, because then you can match it. I mean, if you're literally replaying from the
*  event, it's a little more feasible. Exactly. And replay might be a way that you could get this
*  retrospective type thing to work. Right. Well, that's what I meant is like reverse replay,
*  I guess is what you would. Yeah. Exactly. Yeah. So anyways, so the core idea is basically just
*  this. That like the core advantage is that like, if you do this in an event triggered way, when you
*  know something meaningful has happened, you look back, then the number of computations that you
*  have to do might be fewer, at least in terms of the associated component, computation, right?
*  You still have to keep track of some other things. And that you need to do time step by time step.
*  But those things are not associative. Those things specifically are the overall rates of
*  different events. So you still need to keep track of overall rates. How often do cues happen in my
*  life? This particular how often this reward happened in my life, this particular reward.
*  And then you also have to do the inverse. So exactly do the perspective. Exactly. Yeah. And
*  that perspective now, the advantage is you don't need to compute the perspective every time step.
*  You can just compute the perspective when you need it. Right. When it when you find the thing
*  that you think is most causally related to the event. Exactly. Exactly. Please correct me because
*  I I'm sure I'm saying lots of things incorrectly. No, no, it's all correct. Yeah. So that's sort of
*  the core insight. So that's how this sort of view came in. Right. Now, in none of this is dopamine
*  apart, right? I mean, so this is all independent of dopamine, if you will. So this is algorithms.
*  This is all and this is even at the algorithmic level, we're not talking about
*  you know, like how which things you should learn from etc. This is just simply if you wanted to
*  learn all pairwise relationships between every possible thing in the world, which is probably
*  hard, then you could do it this way. Right. Like you could you could have not only could you do it
*  this way, this is a better way to do it. Exactly. The prospectus is the better way to do it just
*  because it keeps track of them. And now, again, like to give credit to the other side, that's not
*  to say that there's no possible way potentially whereby you could do it in a prospective way.
*  I think it but it's just that if you do it in the prospective way, you will necessarily have to do
*  this time bending stuff and like, compute that. So let's pause here then. And maybe I'll ask Ali
*  first if you if you are aware of or versed in the quote unquote controversy of this, like, did
*  did this in your view make like a big? Was there a big fuss with VJs papers, you know, the recent
*  the first time I heard it, I just know purchase from you, but from like the field, right? I mean,
*  you can you pointed me to it as you know, cheerleading it. So I know there was no not
*  a controversy from you. But but are you aware of this controversy in the in the literature or
*  have you not? Yeah, no. Yeah. I mean, there's all I mean, the dopamine. There's never anything with
*  dopamine that there's not controversy in the literature. Right. But but this in particular
*  is a very specifically what it's saying is, hey, you've you've all had it backwards.
*  So maybe I want to go down to one sentence before we get to that. So one thing that I
*  didn't describe in the algorithmic thing is just what exactly dopamine I think is doing in that.
*  We're going to come to it. But let's let's go ahead and do that. Just very quickly, just before
*  the controversy, because then, oh, that that is the controversy. Yeah, exactly. Because this
*  is that like, in addition to this retrospective thing, like, you also need to do one more thing,
*  which is this retrospective thing. At this level, I just described something where you learn every
*  possible association, right? And, and maybe you don't want to do every possible thing, just
*  intuitively, we just described, and you only want to do this for like meaningful outcomes, right.
*  And so the core idea with the dopamine was that there's this additional step to this retrospective
*  thing that describe that filters out and tells you what are the meaningful things
*  that you need to the dopamine tells you what's meaningful. Exactly. Okay. And so that's that's
*  the additional step. And I, the controversy is more related to that part, and less so related
*  to the retrospective perspective. Okay, but but a reward prediction, so an error is inherently
*  meaningful. So why is what is the controversy? Why is it controversial?
*  Ah, so there's a lot of I mean, you know, like, there's a lot of similarities between this sort
*  of idea of like, what is meaningful and what is our and I'll just interject and also say that we're
*  using human language for these terms. Definitions are slippery and exactly gets ridiculous. Exactly.
*  And we're also simplifying it to a to a big extent, right. And so,
*  so yes, so the before addressing the contours, the quick thing to say is that it just turns out
*  that I mean, just exactly as you went to it, this meaningfulness thing and prediction are thing just
*  sound somewhat similar. I mean, they they have like a lot. And that's exactly the core, the reason
*  why, you know, like we decided to look into this, which is that like, it turns out that when you
*  mathematically formalize this, there's a lot of similarities between this meaningfulness that
*  thing and, and RP. And so the idea was, well, if if you've not done experiments that try to look at
*  the where the differences are coming, then you wouldn't have known which one is actually and
*  then our argument was that maybe all of this evidence for RP might also be consistent with
*  this other thing of the meaningfulness. And you know, you should you should do experiments that
*  test that perhaps. Maybe right.
*  Yeah. So that's if I may ask a not a softball question. Yeah. So what defines the cue in your
*  interpretation? Because like, look, we are under barrage of sensory information. I'm an animal
*  walking around. I'm getting like continuous visual inputs, auditory, right. So how do you keep track
*  of those events? I'm continuously seeing things, a tree. Yes. I don't know. Right. So
*  absolutely. In an experimental setup, it's easy to define that to have like four set of cues and
*  then try to see which one is meaningful in the real world. How would you? This is a hard problem,
*  right. And this is a hard problem that I think basically all these theories sort of just show
*  away and then say some other smart reason takes care of it. And so my version of that is to say
*  that a cue is something that like, you know, the higher order sensory regions that define what the
*  objects are sensory objects. So let's say it it it it it it it it it it it it it it it it it it it it it
*  area it for visual stuff defines what cues are like visual cues are those things that are given
*  like that are identified by those neurons, right, like objects, sensory objects, if you will.
*  And there's learning associated with that. And there's like filtering associated with that. And
*  there's like complex operations, like you take sensory input and then, you know, Alid turning
*  this way versus Ali turning that way is still Ali. And that's a hard problem to solve. But
*  that problem is typically studied by sensory neuroscientists and they have come up with
*  reasonable solutions for that. And have shown that like there are neurons in the brain that can do
*  that. Yeah. The correct answer is that the most important part of the brain, which is the downstream
*  structure or upstream structure. Upstream. Upstream here. Yeah. Upstream. It's the sensor.
*  We'll take it. Yeah. Sensory structures will take care of it. I mean, if you want the full
*  solution to this whole problem, that's basically saying, well, how does the brain work? I mean,
*  it's essentially impossible to describe it. Like I think the way that we start to formalize this,
*  we at least start to now get into all the things that we're assuming. Right. And yeah.
*  Yeah. I'm asking because I'm very interested in that because I think that would relate to
*  attention as well. 100%. And how like the dopamine system is now involved in attention.
*  Gating of incoming information. Exactly. Which I think, I mean, we will not get into that,
*  Paul, but one of the most important things about dopamine is its role in attention.
*  You actually missed that in the 10 command. Yeah. Yeah. You, I mean, you're responsible.
*  You have to be responsible to our GBC user. Well, I also didn't intentionally didn't include
*  prospective or sorry, retrospective. Yeah. So, but it'll be on the list next year.
*  This is, I'm going on a tangent, but I think, I mean, this is another important thing about like,
*  yeah. Yeah. So let me just interject and say one of the things that I have appreciated about your
*  work also is that the way that you've approached it, at least, well, the way that you approach it
*  in the literature, which is super helpful, I think in terms of thinking about how to tackle
*  a problem in general is that you list out some of the assumptions of TD learning and then point
*  out how they're wrong. So that's a powerful way to build your own argument and say like,
*  here are the holes and here's how we can fill those holes. Yeah. I mean, you know,
*  I wouldn't use the word wrong or say, I would just say hard, implausible. It's just sort of
*  our incomplete or incomplete or yeah, exactly. Yeah. And yes, good save there. Yeah. Yeah. So,
*  so we get back to the controversy stuff. Ali, you want to take the controversy bit?
*  But okay. So I'll just ask in the beginning, is the controversy due to the fact that so many
*  people's careers or their reputations are at stake and they're just feeling not hurt by it, but
*  defensive perhaps, because a lot of controversy begins by feeling by powerful people feeling
*  defensive. I mean, you know, like I know some of these powerful people and I will say that like,
*  I don't think that that's it alone. I mean, maybe there's a component of it, right? But I,
*  I wouldn't say that that's really the main driver. I think the main driver is that, you know,
*  extraordinary claims require extraordinary evidence. And the idea that-
*  And papers and science. Yeah. And papers and science. Yeah, exactly. And so, so the,
*  you know, the dopamine story and the TD story is, you know, like, was 25 years old, right? At the
*  time that like we had our paper published, 1997 to 2022. And so like something that has lived on for
*  25 years with thousands and thousands of papers supporting the idea- Is bound to be wrong.
*  No, that is not going to be over-turned with one paper. Okay. Yeah. Yes. Right. But I think what
*  was beautiful about this work that's like, it's not my work, so I can just say nice things about it,
*  was that it looked at, like you mentioned, Paul, looked at predictions of TDRL and some of them
*  were not explained by actual data, right? That we will get into it. And then came up with an idea,
*  this retrospective learning, that could explain both TDRL predictions and places where it would
*  fail. Right? So I think that was a beauty of it. And that is, I think, the main controversy, maybe.
*  But how is that controversial? There are like specific predictions that Vijay may go through
*  some of them. I mean, there's like, I don't know, 13 in that paper, 14, huge number, right? But
*  that predictions where TDRL fails to make predictions about what happens.
*  Why would that be controversy as opposed to progress?
*  It's a very good question. So why is that controversial? So here we get into the heart of
*  TDRL and what TDRL is. Right? TDRL, the way that I described it, I said there's a box,
*  and that's where the box in which the algorithm lives. And there are some inputs that go into it,
*  and there's some output. Right? Now, which of these is TDRL? Right? Like TDRL is really the box
*  where the computations are happening. But the things that go into it are not TDRL per se.
*  Oh, okay. Right? So now you're doing TDRL.
*  Exactly. Okay. I mean, the argument is, if you're doing TDRL, that's not a single thing. Because
*  the input that goes into the algorithm can be many different things. That's your argument. No, no,
*  it's the controversy argument. It's the alternative argument. So it's the argument from the TDRL folks
*  as to why this should not make us talk about TDRL. Because it doesn't matter what's going into the
*  box. It's what the box is doing. Yes. And so because of that, the problem that comes down to
*  it. So if I'm supporting TDRL, right, this is the counter argument that I have. It's like, you have
*  convincingly demonstrated that this TDRL box with a specific set of inputs is wrong, and those
*  predictions are wrong. But how do you know that this TDRL box with a different set of inputs is
*  wrong? Okay, right. And this is the problem. And that's where this is, it becomes almost philosophical,
*  and it's a good thing to get into. So the argument is this basically, that now when it comes to these
*  things that are the inputs that are states, right, I mean, it's a formal term that I use. And the way
*  that you define states is inherently ambiguous when it comes to time, time delays, because there's no
*  objective thing that in the world that tells you exactly what state you're in. So you can define
*  that in many different ways. And so now people have defined them concretely in many different ways.
*  And we did look at the concrete things that people did define it as and show that none of those
*  concrete things actually fit with the data. Right. Now, the so I think that from our perspective,
*  we argue, well, the at least the published concrete things we've looked at, and those don't
*  fit the data. Now, the counter argument is, well, but that's not to say that, well, you've not done
*  a good job of looking for things that are that are possibly the different state inputs, right,
*  you could have had different state inputs where you could have looked for this. And then you could
*  have also assumed different parameters of the TDR algorithm and how sensitive they are to the TDR.
*  And so that obviously is another aspect. Those are the defined free parameters, right within the
*  box, there are free parameters. Those are, you know, there are many different free parameters
*  within the box. And there's the undefined things that are outside the box, which are the inputs,
*  which technically those are infinite dimensional, right, because they could be anything basically.
*  I mean, yeah, but my guess is you would say that these are valid arguments.
*  Yeah, I mean, I think that's a valid argument, right to say that to actually argue against
*  TDR as a family, you would need to show that any possible inputs with this algorithm would not fit.
*  And my question to that is, well, how do you ever show that? Right, right. And well, you need to
*  keep track of all prior possible possible inputs as you move forward in time without knowing what
*  the reward is. But every every possible input queue, sorry, I'm trying to bring it back to
*  Yeah, exactly. And there may be different ways to keep it, you know, like you can store things
*  in memory in many different ways. So like which way of storing things in memory should you should
*  you be considering, right? And so our argument, so this is where I think that the TDRL thing
*  fields has a there's a little bit of a philosophical problem. And that relates to falsifiability
*  as a concept in science and to the extent to which that is a core concept that we still stick
*  by, right? And for scientific progress for theories. In my view, TDR is a framework.
*  It's not a theory. It's not a theory. It doesn't I mean, if you were to ask folks that really defend
*  it, I mean, I have done it to give me a list of predictions. Here are exactly the predictions
*  where if you were to find these, I am telling Well, that's it, right? I mean, there's no way
*  that TDR can rescue it. There's no clear answer to that question. Right? So like, how can you prove
*  this theory wrong? Give me a way whereby you're 100% certain that if this was true, this theory is
*  wrong. And I don't know the answer yet. And so until you know that, like you can't like fully
*  falsify a framework, right? And that's a valid point. But to me, then the flip side to that is,
*  well, if you cannot falsify a theory, how much does that aid in our understanding anyways?
*  Right? Like, don't you want to have a theory that is very concrete where you can falsify it? Right?
*  And so if you can't do that, then that's a framework. And it's useful. I mean, I'm not
*  saying that it's useless, right? I mean, it's extremely useful to ask a way to think through
*  things once you get the results and to kind of come up with explanations for it. But it's all
*  post hoc explanations. And it's not the way that science should be done as Popper envisioned it.
*  It's definitely not the Popperian view. Right? And so this is where we get into the philosophy
*  of science debate. Right? And so that's where I think and a lot of the controversy in this field
*  actually comes down to this particular problem. That's a lot of it. What's actually pretty
*  beautiful about that is that you arrived at that through your algorithmic approach.
*  Yeah. I mean, it came to it just from looking at the assumptions, right? Like, looking at the
*  assumptions and which ones can be defended worse than not. And that's when you realize that, well,
*  some of the assumptions actually are so flexible that you have so many degrees of freedom there.
*  So then how exactly do you test? See, I think, however, that this same argument,
*  look at the assumptions can be applied to almost any topic in at least neuroscience.
*  Biology, like earlier, when you said that you just had an intuition that something was missing about
*  TDLR. And I was thinking, and then it took you 13 years to be able to vocalize what that intuition
*  is. Well, I've had an intuition that there's something very wrong with almost all of neuroscience.
*  And that means it's going to take me like 100 years to vocalizing it, you know? Yeah.
*  That's the tough part. Yours is a more tractable intuition. And I so I'm envious of that.
*  Yeah. I mean, you know, like, this also goes back to some of the Randy stuff, maybe like with
*  memory. And, you know, like, Randy's obviously been arguing that, like, there's a lot that's
*  wrong with neuroscience and all of how we think about memory is wrong. But to actually be fair to
*  folks on the opposing side, where, you know, like on the TDLR side, there's one more aspect of this
*  that is controversial. The second aspect of this that is controversial is that though the
*  retrospective to prospective, you know, conversion that is fully within the view of the standard
*  sort of TDRL perspective view, like, and that like, it's all long run sums of things, events,
*  and discounted sums. So it's all the same view. So that part is less controversial. I think the
*  controversial bit is that the way that we handle the definition of what is meaningful
*  is you can't write it out as a solution to a problem that you define. And then this is a
*  problem that you're trying to solve. And this is exactly the solution, right? It's more of an
*  intuitive type approach to say, well, there's a set of core intuitions that whatever meaningfulness
*  is should abide by. And here's a way to mathematically formalize that. Right.
*  Well, you have to operationally define meaning then and then then we're, you know, because if
*  not, then we are back into the unfalsifiability problem.
*  Exactly. And so we have taken a very concrete way of defining it. The problem with that is that
*  that is not, it's, I mean, there are elements of this that have a lot of intuitive and there's an
*  intuitive derivation for it. But there's not a problem statement where we can say, well, this,
*  if you want to maximize this objective criterion, this is exactly the way that you would define it.
*  I see.
*  Right. And that's a valid concern.
*  And there's not a there's not a formal theoretical formal solution.
*  Normative solution. Normative solution. That's a fully valid concern. Right. And so I so in that
*  sense, this is I think of this as the first step towards towards going in that direction. Right.
*  And so and we're working on this and, you know, who knows if we will end up coming up with something
*  that is a fully normative sort of thing that and, you know, the my suspicion is that if we work
*  through that, the eventual solution that we come up with is not going to be exactly like the anchor
*  prediction. It would have a lot of the intuitive features of it. And so that's why a lot of the
*  things that we test are those intuitive aspects of things, not exactly like how much up or down.
*  Like it's it's not the, you know, we're not looking for dopamine is 20% up from or like 10% up from
*  this than than what it should be. We're looking at while dopamine should be higher versus lower,
*  it should be positive versus negative. And so so that's the level of predictions that we're
*  testing at. And, you know, like, and there's a completely different story from this retrospective
*  view that is beyond the Jiang et al paper, we're finding other things that seem to be coming
*  directly from the retrospective view, where those happen to be true. Right. I mean, and those are
*  experiments that we did explicitly to falsify our model, right. Like, and it turns out that it wasn't
*  falsified. And so, so those things exist. And so at least so far in our attempts, we have looked at
*  some of the some really wacky predictions of this retrospective view. And those wacky predictions
*  seem to generally hold up so far have not been falsified. So when you tweet about something like
*  that, do you often use the like that raised hands emoji? Like, well, but we're still we're still not
*  wrong. Yeah, yeah, no, it's it's a tricky one, right? It's a tricky one, because this is not a
*  common approach in neuroscience. And so what's not like trying to falsify your own trying to falsify
*  your own? Well, it's supposed to be it's supposed to be but unfortunately, it's not. And so, so it's
*  a tricky one, because when we say that, like, we end up finding results that were consistent with
*  our prediction, wacky, maybe. Yeah. The way that at least some members of the audience of the
*  neuroscientific audience take it as, well, you just looked to prove your theory. Right. And you're
*  claiming this as proof for your theory. And there's no, there's no way to there's no way to convince
*  them otherwise, right? Yeah, like, how do you convince them otherwise? I mean, I can't give you
*  like the full thread of the thinking that I had. Yeah, you need to you need to prospectively write
*  out all your future thoughts exactly. Yes. And you can. Yeah, exactly. And so, I mean,
*  like, you know, what I'll say is, we haven't discussed that. But like, some of the things that
*  we have found are very inspired by Randy stuff. But some of the things, but not exactly Randy stuff,
*  there are key differences from Randy stuff, too, but like very inspired by them. But some of the
*  things that we find if if my job is to try to find evidence supporting this theory, let me just
*  say that that will not be the thing that I'll go and try to test. Because my core intuitions are
*  that there's no way that those could be some of those predictions could be true, right? Like,
*  starting out. And so like, if I wanted to just find some evidence consistent with the framework,
*  it's way easier to look at things that you know, this thing could fit and also many other things
*  could fit. So that at least I could say, well, that thing is consistent with our stuff.
*  It's not uniquely consistent with our stuff, but at least it's consistent with our stuff. If I wanted
*  to just build out evidence, that would be the approach that I would do right, like in a purely
*  statistic way. If you if you want to, if you want to like, you know, like if you want to test these
*  things, it's, I feel like it's, I would be hard pressed to try to come up with a way to look at
*  the experiments that we've done with not an intention of at least an expectation that that
*  would falsify our results. Right. There's it. Yeah.
*  May I say that? Oh, sorry. The fact that we're just having this conversation, I mean, I'm biased,
*  but would speak to the maturity of the dopamine learning field, because the majority of neuroscience,
*  we don't even have these frameworks, right? Here, I think we have a framework. And now we can start
*  questioning some of the assumptions. So I think we've come a long way. And so,
*  yeah, just 100%. But it's a it's in general, a pretty friendly community, right? The dopamine,
*  not without controversy. I mean, there's definitely controversy, right? But I think people
*  people talk, people talk respectfully to each other. That's all you can hope for. Right. Well,
*  thank you so much for spending so much time with me. And, and for the careful elucidation of the
*  history, I didn't really know that we're going to get such a history lesson. And I learned something,
*  I'm not sure I know what dopamine does. But I think it's this is like, super valuable for people
*  who do think they know what dopamine does, that it's not, it's not going to change popular science
*  outlets who will still say, Oh, I got a dopamine hit with that piece of cheesecake, because it
*  may be happy. The happiness. Yeah. But, but at least hopefully, it'll it'll reach some people and,
*  and they'll realize that man stories, it's hard. But, and there are lots of different ways of
*  approaching it. And I really appreciate the simplicity and the elegance of the solution
*  that you have come up with VJ. So thank you. Appreciate it. Yeah, there's one thing that maybe
*  I add, if you have time still. So just like, very quickly. So one thing that I wanted to say is,
*  there's a both a positive message and a negative message, I guess, like in the industry trajectory,
*  in that, like, all of the debate about dopamine, that we've been having, right, big controversy in
*  the field are not about the, the all the details that Ali was talking about all the variability
*  across different regions, the axonal regulation of dopamine release, etc. Right? I mean, it's,
*  it's about the simple thing, like this one dimensional signal, what does it represent,
*  right? And we're throwing a bunch of stuff. And even that simple one dimensional stuff,
*  we haven't yet settled as the field, right? So to me, the negative side of that is that that
*  shows you that neuroscience as a field still has a lot of maturing to do. It's very young. And the
*  fact that maybe one of the most investigated questions with like, where we're talking about
*  just literally a one dimensional signal is still not settled, actually says that while other things
*  probably will also need to have these moments. Now, the the positive side of this is that this,
*  I think, is the sign of a field that is actually starting to get to the direction where you're
*  starting to have these debates. And so I think that that was sort of, you know, like, it's a thing
*  that, like, in general, we need across all of neuroscience, across all the different topics
*  in neuroscience, that sense, the domain field is leading in that sense. So so VJ is going to end
*  with celebrating our incompetence. I like it. I mean, I think it's a very, the main point that
*  I'm making is that it's a very exciting time to be a neuroscientist in that, like, even though we've,
*  you know, collected a lot of data, I think as a conceptual sort of field, there's still so much
*  to be done. I agree. Yeah. I mean, my last line would be that, like, we might not know what dopamine
*  does and what dopamine is, but we almost definitely know that dopamine is not pleasure. So one take
*  away, dopamine is not the pleasure signal and maximizing your dopamine would not is not necessarily
*  a good thing. So yeah, I do second that. All right. So thanks guys for coming on again,
*  and good luck to you both. Yeah, thank you so much for all. That was great.
*  Thanks for having me. Yeah, it was lovely. Thank you.
*  Brain Inspired is powered by The Transmitter, an online publication that aims to deliver
*  useful information, insights and tools to build bridges across neuroscience and advanced research.
*  Visit the transmitter.org to explore the latest neuroscience news and perspectives
*  written by journalists and scientists. If you value Brain Inspired, support it through Patreon
*  to access full length episodes, join our discord community, and even influence why invite to the
*  podcast. Go to braininspired.co to learn more. The music you're hearing is Little Wing performed
*  by Kyle Dunovan. Thank you for your support. See you next time.
