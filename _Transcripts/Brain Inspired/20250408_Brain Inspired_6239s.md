---
Date Generated: April 09, 2025
Transcription Model: whisper medium 20231117
Length: 6239s
Video Keywords: []
Video Views: 87
Video Rating: None
Video Description: Aran Nayebi on reverse-engineering brains to build autonomous agents, and an update to the Turing test for NeuroAI.

Show notes:  https://braininspired.co/podcast/209/

Patreon (full episodes and Discord community):  https://www.patreon.com/braininspired

Apple podcasts:  https://itunes.apple.com/us/podcast/brain-inspired/id1428880766?mt=2
Spotify:  https://open.spotify.com/show/2UZj8c8Ap5oc2gh2rJxLLe

The Transmitter is an online publication that aims to deliver useful information, insights and tools to build bridges across neuroscience and advance research. Visit thetransmitter.org to explore the latest neuroscience news and perspectives, written by journalists and scientists. 

Read more about our partnership: https://www.thetransmitter.org/partners/

Sign up for the “Brain Inspired” email alerts to be notified every time a new “Brain Inspired” episode is released: https://www.thetransmitter.org/newsletters/

To explore more neuroscience news and perspectives, visit thetransmitter.org.

Aran Nayebi is an Assistant Professor at Carnegie Mellon University in the Machine Learning Department. He was there in the early days of using convolutional neural networks to explain how our brains perform object recognition, and since then he's a had a whirlwind trajectory through different AI architectures and algorithms and how they relate to biological architectures and algorithms, so we touch on some of what he has studied in that regard. But he also recently started his own lab, at CMU, and he has plans to integrate much of what he has learned to eventually develop autonomous agents that perform the tasks we want them to perform in similar at least ways that our brains perform them. So we discuss his ongoing plans to reverse-engineer our intelligence to build useful cognitive architectures of that sort.
We also discuss Aran's suggestion that, at least in the NeuroAI world, the Turing test needs to be updated to include some measure of similarity of the internal representations used to achieve the various tasks the models perform. By internal representations, as we discuss, he means the population-level activity in the neural networks, not the mental representations philosophy of mind often refers to, or other philosophical notions of the term representation.

0:00 - Intro
5:24 - Background
20:46 - Building embodied agents
33:00 - Adaptability
49:25 - Marr's levels
54:12 - Sensorimotor loop and intrinsic goals
1:00:05 - NeuroAI Turing Test
1:18:18 - Representations
1:28:18 - How to know what to measure
1:32:56 - AI safety
---

# BI 209 Aran Nayebi: The NeuroAI Turing Test
**Brain Inspired:** [April 08, 2025](https://www.youtube.com/watch?v=_RMVdo3TN_k)
*  The kind of grand challenge right now in AI, even if you don't necessarily care about the [[00:00:00](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=0.0s)]
*  brain in particular, is generalized embodied intelligence and the ability to... [[00:00:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=8.74s)]
*  Is it... [[00:00:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=14.18s)]
*  Ideally... [[00:00:15](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=15.18s)]
*  Is embodied part of that? [[00:00:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=16.18s)]
*  It is the ultimate endpoint. [[00:00:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=17.18s)]
*  If you took two brains, there's just going to be variability between them within, you [[00:00:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=20.98s)]
*  know, individuals in a given species. [[00:00:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=25.66s)]
*  Fix the same brain area, fix the same stimulus, there's just going to be variability in how [[00:00:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=28.5s)]
*  they process that. [[00:00:32](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=32.3s)]
*  So when a model is imperfect as a match to the brain, we need to be able to disentangle [[00:00:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=33.38s)]
*  whether that's because it's actually a poor match to the brain, truly, or because there [[00:00:40](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=40.44s)]
*  is inherent evolutionary variability between brains that we need to account for. [[00:00:45](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=45.86s)]
*  The fundamental difference between AI, this technology that we're building, and prior [[00:00:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=53.08s)]
*  technology is that now you're starting to build a technology that takes in inputs and [[00:00:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=57.879999999999995s)]
*  intentionally produces actions. [[00:01:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=62.879999999999995s)]
*  It's an agent. [[00:01:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=64.67999999999999s)]
*  This is Brain Inspired, powered by the transmitter. [[00:01:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=72.84s)]
*  Hello, everybody. [[00:01:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=76.24s)]
*  It is Paul. [[00:01:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=77.68s)]
*  This is Brain Inspired. [[00:01:19](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=79.2s)]
*  Oh, we just had our second Complexity Group discussion... meeting... discussion. [[00:01:21](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=81.84s)]
*  It was so fun. [[00:01:30](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=90.32000000000001s)]
*  I'm worn out from it, but it was a lot of fun. [[00:01:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=91.32000000000001s)]
*  If you are interested in complexity, a large group of us, over 300 and... I think there [[00:01:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=94.12s)]
*  are 325 people now on this email list, are going through these foundation papers that [[00:01:40](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=100.0s)]
*  I spoke with David Krakauer about a few episodes ago, and we just had our second meeting. [[00:01:47](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=107.08s)]
*  I'm learning a lot, so it's awesome. [[00:01:54](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=114.08s)]
*  I hope if you're part of it, you're enjoying it. [[00:01:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=116.32s)]
*  Okay, welcome to this episode. [[00:01:59](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=119.32s)]
*  Arun Nayyabi is an assistant professor at my own Carnegie Mellon University. [[00:02:01](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=121.96000000000001s)]
*  He's in the machine learning department. [[00:02:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=128.24s)]
*  Arun was around in the early days of using convolutional neural networks to explain how [[00:02:11](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=131.64s)]
*  our brains perform object recognition. [[00:02:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=137.27999999999997s)]
*  You'll hear me allude to Dan Yamans, who was on one of the first episodes of this podcast. [[00:02:21](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=141.48s)]
*  Arun, although he's been through many labs with big names that you've probably heard [[00:02:27](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=147.48s)]
*  of, Dan Yamans was one of them, but it was of note to me because it was where I began [[00:02:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=153.56s)]
*  this podcast. [[00:02:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=157.72s)]
*  He was beginning to actually do the science. [[00:02:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=158.84s)]
*  Anyway, since then, he has had a whirlwind trajectory through different AI architectures [[00:02:41](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=161.64000000000001s)]
*  and algorithms and how they relate to biological architectures and algorithms. [[00:02:47](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=167.08s)]
*  We touch on some of what he has studied in that regard, but he also recently started [[00:02:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=173.52s)]
*  his own lab at CMU, as I mentioned, and he has plans to integrate much of what he's learned [[00:02:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=178.16s)]
*  to eventually develop autonomous agents that perform the tasks that we want them to perform [[00:03:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=184.2s)]
*  in ways that are at least similar to the ways that our brains perform them. [[00:03:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=189.64s)]
*  So we discuss his ongoing plans to quote unquote reverse engineer our intelligence to build [[00:03:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=194.6s)]
*  useful cognitive architectures of that sort. [[00:03:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=200.29999999999998s)]
*  We also discuss Arun's suggestion that at least in the neuro AI world, the Turing test [[00:03:23](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=203.76s)]
*  needs to be updated. [[00:03:30](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=210.44s)]
*  So the Turing test is this famous benchmark test proposed by Alan Turing, someone as a [[00:03:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=211.92s)]
*  thought experiment joke apparently, that if you can trick a human to that rather that [[00:03:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=217.6s)]
*  if a computer can trick a human into thinking that the computer is a human, it passes the [[00:03:43](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=223.79999999999998s)]
*  Turing test, which means that the computer is thinking. [[00:03:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=228.44s)]
*  It's been debated whether this is a good test over many years, but it is the thing that [[00:03:52](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=232.88s)]
*  keep coming back to when trying to assess whether an artificial system is a thinking [[00:03:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=237.84s)]
*  system or at least a good artificial system. [[00:04:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=244.84s)]
*  And Arun thinks that we need to update this. [[00:04:07](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=247.94s)]
*  So the original test was just about the behavior, whether a computer could trick a human. [[00:04:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=249.72s)]
*  And Arun's point is in the neuro AI world where we're trying to build models that mimic [[00:04:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=254.48000000000002s)]
*  human behavioral output, human function or biological function, not necessarily human. [[00:04:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=260.28000000000003s)]
*  It's important to also compare the internal representations between the systems that we [[00:04:27](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=267.44s)]
*  build, but also between the species that we're testing them against and between individuals [[00:04:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=273.12s)]
*  within the populations of those species. [[00:04:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=278.94s)]
*  And I think I just said internal representations, but that's what he wants to compare what he [[00:04:41](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=281.04s)]
*  calls internal representations. [[00:04:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=286.68s)]
*  And by that he means simply the population level activity in the neural networks or whatever [[00:04:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=288.2s)]
*  system you're using to build it. [[00:04:54](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=294.84s)]
*  So not representation in the sense of mental representations and philosophy of mind or [[00:04:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=297.12s)]
*  other philosophical notions of the term representation, simply the activity of the populations of [[00:05:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=304.32s)]
*  units, for example. [[00:05:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=309.98s)]
*  Thank you to my Patreon supporters. [[00:05:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=312.36s)]
*  Thank you for the ongoing support from the transmitter. [[00:05:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=314.44s)]
*  Hope you guys are well. [[00:05:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=317.84000000000003s)]
*  Hope you enjoy my conversation with Arun. [[00:05:19](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=319.52s)]
*  Arun, I was just I literally was just looking up. [[00:05:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=322.92s)]
*  Dan Yehmans was episode number seven of this podcast, which was like, you know, 100 years [[00:05:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=329.44s)]
*  ago or something. [[00:05:36](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=336.8s)]
*  But and now here you are and you matriculated partially through Dan Yehmans lab. [[00:05:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=338.12s)]
*  You went fast, it seems. [[00:05:45](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=345.04s)]
*  Yeah, it felt like time flew. [[00:05:47](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=347.0s)]
*  So I started in 2016, my PhD with Dan and graduated in 2022. [[00:05:49](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=349.46s)]
*  So a couple of years ago with him and Surya. [[00:05:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=357.78s)]
*  Yeah, yeah. [[00:06:00](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=360.29999999999995s)]
*  Surya Ganguly. [[00:06:01](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=361.29999999999995s)]
*  This is not good because I think I got my PhD. [[00:06:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=362.38s)]
*  I think I earned my PhD in 2016. [[00:06:05](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=365.78s)]
*  And look, you're way, way ahead of me. [[00:06:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=368.17999999999995s)]
*  Well, you know, it's it's been definitely a ride. [[00:06:11](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=371.74s)]
*  That's for sure. [[00:06:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=376.58s)]
*  And things have things have changed so quickly, right? [[00:06:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=377.74s)]
*  And at least in AI in those years, I remember like when we started, it almost feels like [[00:06:21](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=381.26s)]
*  like talking about the old times, but it really wasn't that long ago. [[00:06:27](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=387.94s)]
*  So like when I started my PhD, TensorFlow was not yet out. [[00:06:30](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=390.53999999999996s)]
*  And we were using Theano. [[00:06:36](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=396.09999999999997s)]
*  So and Keras had just come out. [[00:06:39](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=399.06s)]
*  So as a master student, I was contributing to that a little bit, which is fun. [[00:06:41](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=401.29999999999995s)]
*  Did you have a background in math and computer science? [[00:06:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=406.3s)]
*  Yeah, that's right. [[00:06:49](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=409.22s)]
*  Yeah. So I did I did my undergrad in math and symbolic systems, which at Stanford was [[00:06:50](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=410.02000000000004s)]
*  like basically the cognitive science major. [[00:06:55](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=415.38s)]
*  So I was always interested in the brain. [[00:06:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=417.5s)]
*  I just didn't know what to do with that interest until later on, basically. [[00:06:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=418.86s)]
*  And then I did a master's in CS and in AI to kind of transition to like a slightly more [[00:07:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=423.90000000000003s)]
*  empirical field. [[00:07:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=430.06s)]
*  And by then, I kind of had the I knew like so I took actually I had a meeting with very [[00:07:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=432.3s)]
*  graciously actually by Bill Newsom. [[00:07:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=440.5s)]
*  And he had mentioned that, oh, we need more people with like math backgrounds, basically [[00:07:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=442.98s)]
*  back then back then. [[00:07:30](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=450.02s)]
*  Right. To like contribute to neuroscience. [[00:07:32](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=452.21999999999997s)]
*  And he actually referenced the Dian and Abbott, like the theoretical neuroscience book. [[00:07:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=457.34s)]
*  And so I was really excited. [[00:07:45](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=465.9s)]
*  I was like, wow, like there's a whole thing like information theory. [[00:07:47](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=467.9s)]
*  And there was like some machine learning, a little bit of machine learning there. [[00:07:50](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=470.14s)]
*  And so I was like, well, OK, I should I should get myself familiar with that a lot more [[00:07:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=473.5s)]
*  than like number theory and logic and like theoretical computer science, which is more [[00:07:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=478.05999999999995s)]
*  of my background. [[00:08:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=483.38s)]
*  And that's that's what led me to do like a master's in AI actually to prepare to do [[00:08:05](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=485.82s)]
*  basically theoretical neuroscience at the time. [[00:08:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=490.18s)]
*  I mean, we could go many different ways here. [[00:08:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=494.06s)]
*  But so I mentioned Dan Yeamans because those early convolutional neural network models [[00:08:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=497.5s)]
*  that accounted for brain activity were one of the things that got me into an interest [[00:08:24](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=504.42s)]
*  in using AI models to study brains. [[00:08:32](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=512.5799999999999s)]
*  And it was one of the early successes. [[00:08:36](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=516.66s)]
*  And you were right in the thick of it in the early days. [[00:08:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=517.9799999999999s)]
*  And coming from your background, that mathematical computer science background. [[00:08:40](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=520.9399999999999s)]
*  It's such a it's such a computer. [[00:08:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=528.8599999999999s)]
*  It's so non-biological, right. [[00:08:50](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=530.3399999999999s)]
*  It's like going towards biology. [[00:08:52](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=532.4599999999999s)]
*  And now you but you've what I want to ask you is. [[00:08:55](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=535.02s)]
*  How you've come to you from perception to embodied agents and come to appreciate what [[00:08:59](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=539.54s)]
*  the brain how you think of what the brain does and studying the brain. [[00:09:07](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=547.78s)]
*  Totally, totally. [[00:09:15](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=555.38s)]
*  Yeah. So like I like I mentioned, right, I was super gung ho about like basically applying [[00:09:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=557.02s)]
*  math, like like very theoretical thinking to. [[00:09:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=562.74s)]
*  Questions, whatever they may be to the nervous system. [[00:09:26](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=566.86s)]
*  So that was me as like a senior in college, junior, senior in college. [[00:09:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=569.14s)]
*  And I didn't know how to do that. But but when I saw these these books, there was like [[00:09:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=573.46s)]
*  at least some hope. And what became kind of quickly clear. [[00:09:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=577.34s)]
*  So there's this this book that I started to read called Spike. [[00:09:43](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=583.22s)]
*  So it's around that time, basically, as a senior, as I was kind of transitioning to [[00:09:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=588.78s)]
*  do a master's, I was like, well, I should I should actually like work in a neuroscience [[00:09:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=593.18s)]
*  lab to really start to like speak the language a bit more of biology, right? [[00:09:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=596.18s)]
*  Because I've done nothing but biological at this point. [[00:10:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=603.42s)]
*  And so I was very lucky to have the opportunities to work with Steve Backus at Stanford. [[00:10:07](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=607.38s)]
*  So he's he's a very famous like retinal neurophysiologist, but also a computational [[00:10:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=617.4599999999999s)]
*  neuroscientist in his in his own right. And as he always encouraged, like thinking [[00:10:24](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=624.78s)]
*  mechanistically, don't don't be led by just the numbers and the fanciness of the [[00:10:32](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=632.06s)]
*  technical model. So that was really helpful in my own development as like a very like [[00:10:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=638.38s)]
*  math and CS focused student to engage more directly with with biology. [[00:10:44](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=644.9s)]
*  And around that time. So you mentioned like, comnets. [[00:10:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=651.42s)]
*  So I actually I actually didn't hear about Dan's work until like we did a lab meeting [[00:10:54](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=654.78s)]
*  in the back his lab, basically about about that paper, which by then was already like [[00:10:59](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=659.38s)]
*  a year, year and a half later. And what does it mean to think mechanistically? [[00:11:05](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=665.22s)]
*  What did he mean by that? He meant like, like literally like, like what what part of your [[00:11:13](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=673.06s)]
*  model is is corresponding to biology? And like, what is the site? What's the question? [[00:11:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=677.98s)]
*  What's the scientific or biological really question that you're trying to answer than [[00:11:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=682.54s)]
*  just like, you know, making a more technically fancier model that might predict? [[00:11:26](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=686.38s)]
*  Oh, like what's it? What does it do like Mars computational level kind of question? [[00:11:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=693.1800000000001s)]
*  Even even more so like like, so like, you know, in the retina, you have multiple cell [[00:11:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=698.3000000000001s)]
*  types. Right. And so how do these cell types, for example, talk to each other and [[00:11:43](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=703.46s)]
*  collectively yield a particular type of response to a particular type of stimulus? [[00:11:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=711.22s)]
*  So like, you know, there's different types of bipolar cells, there's amicrin cells, [[00:11:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=716.02s)]
*  and then there's there's ganglion cells, there's horizontal cells at the beginning there, [[00:12:01](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=721.86s)]
*  which are more linear in their response profile. So it's like it's a very, at least in the retina, [[00:12:05](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=725.5400000000001s)]
*  is this very clean and clear mapping to a particular computational model and [[00:12:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=730.1s)]
*  and each of those components. And what would the opposite of I'm sorry, [[00:12:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=736.66s)]
*  but I but I want to know like what where he was coming from, or and what you took from it, [[00:12:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=742.4200000000001s)]
*  like, what would be the opposite of that, let's say in the retina, right, non mechanistic thinking, [[00:12:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=745.94s)]
*  or totally. So it's harder to do this in the retina, I think, because there's just so much [[00:12:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=749.94s)]
*  ground truth, but the non mechanistic approach, but like, you could imagine, like, fitting, [[00:12:35](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=755.62s)]
*  I guess, just to put in the language of today, right, like, you could imagine fitting maybe a [[00:12:41](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=761.46s)]
*  transformer to that data, to just like retinal data. And then being like, well, okay, you did [[00:12:45](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=765.38s)]
*  a direct data fit, and you like on held out response patterns, you do well. And that's it, [[00:12:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=771.86s)]
*  like, you know, like, like massively predictive, basically, massively predictive, and then there's [[00:12:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=776.82s)]
*  no clear, like, you don't check if the internals at all, like develop anything that maps on to [[00:13:01](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=781.38s)]
*  like the inter neurons that are there in, in the in the retina, that sort of thing. [[00:13:07](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=787.78s)]
*  Yeah. Okay. Okay. So I'm sorry, I interrupted you, you mentioned spikes also. And I feel like spikes [[00:13:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=792.98s)]
*  is one of those books that is sort of like girdle Escher Bach, or used to be or something. So this, [[00:13:21](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=801.22s)]
*  there's this massive tome by Douglas Hofstadter called girdle Escher Bach, [[00:13:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=808.1s)]
*  g, he's gonna grab it from his shelf. There it is. I have it. Have you read the whole thing? [[00:13:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=813.0600000000001s)]
*  No, I read parts of it. It was very popular in college. Nobody has read the whole thing. [[00:13:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=818.1800000000001s)]
*  I can't say I have. Yeah. Okay. I have it still. All right. So that's another influence on you. [[00:13:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=822.9s)]
*  Anyway, I was gonna say spikes is one of those books that is has been a major influence. And [[00:13:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=826.98s)]
*  there's the spikes book. Yeah. But that was like 1992. 1999. I think 1990. 1998. My copy [[00:13:52](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=832.18s)]
*  says 1996, actually. But MIT Press paperback edition 1999. So I got I got it right there. [[00:14:01](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=841.38s)]
*  But yeah, but but so non brain like that book, right? Because it's all information theory, [[00:14:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=849.3s)]
*  etc. Right, right. So so it's it was so okay, so we were talking about comments. So before we like, [[00:14:15](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=855.7s)]
*  so around that time, it was it was, you know, image net was like the image of benchmark was a [[00:14:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=862.98s)]
*  thing, right? And com nets were were the main way to do at least neural network based vision were [[00:14:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=868.58s)]
*  very promising. Right. So that's the context at this at this time. And we were thinking in the [[00:14:36](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=876.4200000000001s)]
*  lab, so lane and lane McIntosh and Miram Maheshwaranathan were working on they were taking the first [[00:14:44](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=884.18s)]
*  deep learning class basically on on com nets. At this time, those taught by by Andre Carpathy, [[00:14:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=891.2199999999999s)]
*  and Feifei and right, and Justin Johnson, I believe. And so as their class project, they were like, [[00:14:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=898.9s)]
*  well, let's build a convolutional model of the of the retina, you know, it's it's a it's a shallow [[00:15:05](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=905.8599999999999s)]
*  neural network at the end of the day. Let's let's build that. And by the time I joined, [[00:15:11](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=911.78s)]
*  so that was that would have been in the like during the semester. And then I joined in the [[00:15:18](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=918.26s)]
*  summer as a as a as a master student, as part of that as part of that project. And one of the [[00:15:21](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=921.6999999999999s)]
*  reasons why it felt very motivating to work on that was actually because of spikes. So I, [[00:15:26](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=926.74s)]
*  I was drawn to spikes because it was very mathematical. And it was related to the system [[00:15:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=931.22s)]
*  that I was going to be working with the retina. Right. And I was like, wow, there's all this [[00:15:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=937.94s)]
*  beautiful, like, information theory. And like, this is gonna be great. Mathematically tractable. [[00:15:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=942.6600000000001s)]
*  That's something you can. Yeah. Exactly. It's something you can do. And like very much spoke [[00:15:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=948.82s)]
*  to like my own what I was familiar with. So it was a great way to bridge my interest, actually. [[00:15:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=953.46s)]
*  But one thing that really stuck with me in spikes was a sort of like a passage there that was like [[00:15:59](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=959.5400000000001s)]
*  saying, you know, the the natural scenes have many, many parameters. So we've we've basically, [[00:16:05](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=965.06s)]
*  you know, you can prove optimal filter guarantees with things like white noise, [[00:16:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=972.02s)]
*  what's known as Buscang's theorem. But with with the optimal linear filter, but you know, [[00:16:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=976.5s)]
*  with natural scenes, not only do you not have that guarantee, but like, unlike a lot of the stimuli [[00:16:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=982.8199999999999s)]
*  that we tend to probe in the retina, which are controlled by one parameter, so like the intensity [[00:16:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=988.5799999999999s)]
*  of the light, and it's a one D stimulus that varies in time, so like high intensity, [[00:16:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=994.02s)]
*  then step down to low intensity. You know, we don't have there's just like an infinity of [[00:16:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=998.74s)]
*  parameters that you could imagine could control natural natural scenes. And so that hampers our [[00:16:44](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1004.9s)]
*  ability to understand the circuit as a result under that under that condition of natural scenes. [[00:16:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1011.78s)]
*  Naturally. Yeah, yeah. All of a sudden, you're in the real world and you run into a hammer. [[00:16:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1016.9s)]
*  That's exactly right. And so, yeah, it's like, you know, it's very surprising. Actually, I think I [[00:17:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1022.02s)]
*  still I have it highlighted here. Oh, come on. Now you're just showing off that you've engaged with [[00:17:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1028.74s)]
*  it. Yeah. Well, I as a student, it was just like I had no other reference. And this was like the [[00:17:13](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1033.78s)]
*  only way I could really start to make a connection with what I already knew. And so it's really [[00:17:18](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1038.74s)]
*  powerful. And then I just didn't I just didn't read the rest of the book at that point. Oh, [[00:17:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1042.34s)]
*  really? Yeah, yeah. Because I was like, well, it's not it's not doing it's not, you know, [[00:17:26](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1046.18s)]
*  it's very faithful and saying that that these aren't that may be the right set of tools to [[00:17:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1051.7s)]
*  engage with natural scenes. But comments were were that tool. Okay. Oh, okay. And it's weird, [[00:17:36](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1056.9s)]
*  though, that spikes spikes is kind of the polar opposite of girdle Escher Bach, because I think [[00:17:45](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1065.3s)]
*  of spikes is dry and clean and girdle Escher Bach as meaty and wet, somehow, although it's very [[00:17:50](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1070.26s)]
*  like, a lot of people say they got into computational neuroscience because of girdle Escher Bach. So it's, [[00:17:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1078.58s)]
*  I guess in that way that they are alike. And then I don't mean for us to go down the road of [[00:18:05](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1085.54s)]
*  comparing and contrasting the two books and styles. But yeah, yeah, no, absolutely. I mean, I think [[00:18:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1089.3799999999999s)]
*  so so early on, I would say, like, when I was in college, like, it was it was all about cognitive [[00:18:15](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1095.86s)]
*  science and philosophy of mind. And that's and those are just incredibly deep and interesting [[00:18:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1100.8999999999999s)]
*  topics. So I actually I actually took a philosophy of mind course with the late Ken Taylor, who was a [[00:18:26](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1106.82s)]
*  was a philosopher at Stanford. The only actual like African American philosopher on the faculty [[00:18:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1114.02s)]
*  there is this really remarkable man and and a very clear orator of like the problems and issues [[00:18:40](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1120.74s)]
*  about about the mind. And one of the things that always stuck with me in his class was like, you [[00:18:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1128.02s)]
*  know, you and I all have minds. But like, we don't have access to how they work. Like you don't, [[00:18:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1133.14s)]
*  it's not like you're in your head and you're like, Oh, my visual cortex is, is relaying information, [[00:18:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1138.74s)]
*  you know, to this other, you know, to my prefrontal areas or something that there's this [[00:19:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1144.5s)]
*  the lamo cortical loop that's now active, you know, you're not doing any of that, you're not [[00:19:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1148.58s)]
*  even aware of it. And yet we all have minds. And because we're not aware, weirdly, we don't have [[00:19:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1152.1s)]
*  access to their internal workings. We don't really know, like, there's all these mysteries as a result, [[00:19:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1156.58s)]
*  despite all of us having minds. And so I know, I also say, right, like, I was like, what pulled me [[00:19:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1160.8999999999999s)]
*  in initially was that, like, obviously, studying the brain is the deepest philosophical object. [[00:19:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1168.58s)]
*  You know, you could study and it's like about the nature of our condition, yet it was so mysterious. [[00:19:32](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1172.8999999999999s)]
*  And the only way to really engage actually, so I as I mentioned a little bit earlier, I was in the [[00:19:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1178.02s)]
*  more the logic and philosophy community initially. And one of the things that was very interesting [[00:19:43](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1183.3799999999999s)]
*  was that there was like a an annual like logic conference that would happen at the Center for [[00:19:47](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1187.78s)]
*  Language Information, CSLI, it's called at Stanford. And I would go as like an undergrad student. So [[00:19:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1193.3799999999999s)]
*  I'd go to the logic, the graduate logic seminar, and then I'd go to this, this event. And one of [[00:19:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1198.5s)]
*  the really think the things that stuck with me around that time when I was starting to transition [[00:20:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1203.86s)]
*  in neuroscience was the logicians are saying, well, look, like, we have all these theories of [[00:20:07](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1207.38s)]
*  in the philosophy of mind, about how the mind and brain should work. But like, who, like, we can't, [[00:20:11](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1211.8600000000001s)]
*  unless you do an experiment, like, no, there's no way you're going to answer. [[00:20:18](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1218.18s)]
*  The philosophers were saying that? [[00:20:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1222.0200000000002s)]
*  Actually, a logician, his name was Peter Kohlner. He's at Harvard now, is a set theorist, actually, [[00:20:24](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1224.0200000000002s)]
*  works on on those things, but but has some interest in philosophy of mind and was saying this, [[00:20:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1229.3000000000002s)]
*  which really stuck with me. And so that's kind of why then kind of went away from the Girdle-Lesher [[00:20:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1234.02s)]
*  Bach stuff to spikes and more, you know, more like drier science. Yeah. [[00:20:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1238.18s)]
*  Okay. So you okay, by way of story, perhaps again, so you were there in the early days of [[00:20:44](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1244.58s)]
*  the convolutional neural networks. And fast forward to today, and you want to put, let me see if let [[00:20:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1253.54s)]
*  me see if I can state this and then you can correct me. I'm going to state it incorrectly [[00:21:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1262.18s)]
*  on purpose. You want to build a cognitive architecture of four or five different types of [[00:21:05](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1265.3s)]
*  deep learning models, ask things, put them together, have them talk to each other and build [[00:21:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1274.5800000000002s)]
*  working agents that are behaving. There's a there's so many ways I could have said that. [[00:21:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1282.02s)]
*  And that was terrible. And I'm sorry, but correct me. That's no, that's that's right in the in the [[00:21:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1289.06s)]
*  in the primary essential. So right, like, but what's the motivation, I guess, to begin with, [[00:21:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1294.8999999999999s)]
*  which is that the kind of grand challenge right now in in AI, even if you don't necessarily care [[00:21:40](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1300.8999999999999s)]
*  about the brain in particular is generalized embodied intelligence and the ability to is it [[00:21:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1306.6599999999999s)]
*  is embodied part of that? It is. So it is the ultimate end point. But it doesn't I think a [[00:21:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1313.3000000000002s)]
*  lot of the major conceptual issues are actually non embodied. In other words, they're like just [[00:22:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1322.42s)]
*  building even digital agents that can, you know, for example, not go into self loops and and can [[00:22:07](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1327.46s)]
*  plan and reason and adapt to new situations. And, you know, I'm saying this is in ways that are [[00:22:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1334.26s)]
*  clearly things that animals and humans do very well is, is the lifelong learning agents. And [[00:22:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1340.26s)]
*  that's that's really what we want. So I think a lot of the core like software issues or the cognitive [[00:22:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1345.86s)]
*  architecture issues will have to be even addressed in these non embodied contexts. But embodiment is [[00:22:30](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1350.26s)]
*  the ultimate goal. But I think I mean, obviously, there's details there about, you know, robot [[00:22:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1357.86s)]
*  hardware and things like that, that that maybe are not necessarily the core focus. So I agree [[00:22:41](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1361.7s)]
*  with you that it's really more about this kind of cognitive architecture and making sure it works [[00:22:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1366.34s)]
*  in open ended settings to, you know, have these lifelong learning agents. But also that that like, [[00:22:52](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1372.1s)]
*  we can use these as providing insight, both about whole brain data that we're able to collect now, [[00:22:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1378.4199999999998s)]
*  and are emerging, but also get like leverage that cognitive inspiration to build these more [[00:23:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1384.8999999999999s)]
*  general purpose agents, ultimately. Yeah. Let's get into reverse engineering, right? So tell me, [[00:23:11](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1391.86s)]
*  maybe you can I bastardized a summary of what you're up to these days. And we'll talk about [[00:23:18](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1398.26s)]
*  neuro AI Turing tests later. But so so what is your cognitive? Do you consider it a cognitive [[00:23:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1405.4599999999998s)]
*  architecture? I, I consider it a cognitive architecture for, I mean, for two reasons, maybe, [[00:23:32](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1412.58s)]
*  and it gets back to your point about asking about like, what reverse engineering means, [[00:23:40](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1420.66s)]
*  right? Can I think there's many different definitions, and I can kind of tell you what [[00:23:45](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1425.6200000000001s)]
*  my working one is, you're the yours is going to be the Jim DeCarlo one, right? That's my guess. [[00:23:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1428.98s)]
*  It's the Yeah, perhaps. I think it I think it's closest to that. Absolutely. And I think it's [[00:23:54](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1434.66s)]
*  really about understanding the relevant aspects of biological intelligence, those details that are [[00:24:01](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1441.5400000000002s)]
*  useful for intelligent behavior. So it's not about necessarily full brain emulation or emulating [[00:24:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1448.9s)]
*  every biological detail, like say the blue brain project, for example, it's more about like, [[00:24:15](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1455.7s)]
*  just isolating the abstractions from biology that are that are basically hardware agnostic [[00:24:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1462.5s)]
*  algorithms that you can then that are implemented brains, but also can be run in hardware and [[00:24:27](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1467.94s)]
*  abstracted into machines. So what does that mean? What does that really mean in like more concrete [[00:24:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1473.3s)]
*  terms? Well, it usually involves, like matching the population level representations of a model [[00:24:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1477.78s)]
*  that's activations to a neural population activity. And we found, you know, it could have been any [[00:24:44](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1484.34s)]
*  biological observable, right? The brain is a complex object, it could have been the dendrites, [[00:24:52](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1492.4199999999998s)]
*  and it could or it could have been the neurotransmitters. And maybe maybe for certain [[00:24:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1496.5s)]
*  questions that is the relevant biological abstraction. But I think for like a lot of [[00:24:59](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1499.62s)]
*  intelligent behaviors, empirically, at least what we found with no there's no [[00:25:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1504.8999999999999s)]
*  necessarily theory here, it's just empirical observations in different brain areas and [[00:25:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1509.06s)]
*  different species, is that like matching at the level of population activity is constrained by [[00:25:13](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1513.6999999999998s)]
*  doing an intelligent behavior. So there's a relationship between that biological observable [[00:25:19](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1519.1399999999999s)]
*  and intelligent behavior. And to be honest, like if you wanted a one word summary or one sentence [[00:25:23](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1523.3s)]
*  summary of my entire PhD was just showing that this kind of paradigm of a task and architecture, [[00:25:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1528.98s)]
*  and also a learning rule, was a useful way to interrogate those kinds of questions across [[00:25:36](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1536.5s)]
*  brain areas and species that it wasn't like restricted to macaque ventral stream, for example, [[00:25:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1542.42s)]
*  or human behavior. But actually, you know, all a lot of these other brains and these brain areas [[00:25:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1546.98s)]
*  in rodents and in hippocampus or higher cognitive areas, they can be understood to [[00:25:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1553.14s)]
*  this lens of nonconvex optimization. Basically, the devil was in the details of what those loss [[00:25:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1558.74s)]
*  functions are, and what those architectures are. And that's the language. Okay, all right. So [[00:26:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1563.38s)]
*  maybe it's worth mentioning here, I've mentioned it a lot on this podcast. But [[00:26:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1570.26s)]
*  the reason why convolutional neural networks became so popular is because they are a multi layered [[00:26:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1574.58s)]
*  deep learning network. And when you train them in a task on in the early days, visual object recognition, [[00:26:21](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1581.7800000000002s)]
*  and you look across their layers, you can actually match the what was then like the population level, [[00:26:32](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1592.3400000000001s)]
*  quote unquote representations in different layers, and match them with different layers [[00:26:40](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1600.26s)]
*  of what we think of as the hierarchy in our ventral visual stream that we think of as [[00:26:45](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1605.86s)]
*  being important for object recognition. And you, there's been lots of work since then, [[00:26:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1611.1399999999999s)]
*  you have done this work yourself, adding recurrence, etc. And so that's one of the [[00:26:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1616.58s)]
*  one of the modules in your cognitive architecture. Yeah. Yeah. And then what are the other ones? And [[00:27:01](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1621.4599999999998s)]
*  why? Yeah, so, so I should say that I'm not married to this particular set of modules, [[00:27:07](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1627.62s)]
*  it's meant to be free. In fact, the core question is ultimately like by doing this types of [[00:27:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1634.02s)]
*  comparisons of now agents, agent architectures, cognitive architectures, to whole brain data [[00:27:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1640.18s)]
*  across species as well, that we can start to understand if there's conserved modules, [[00:27:26](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1646.74s)]
*  if there's a kind of like general purpose architecture that emerges through lots of [[00:27:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1651.78s)]
*  empirical comparisons. Like across species, general across, across species. Yeah, exactly. [[00:27:35](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1655.46s)]
*  Basically, across this kind of conserved species conserved sensory motor loop. So converting inputs [[00:27:41](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1661.46s)]
*  to two actions. So that's why it's an agent, right? You know, rather than one module is like [[00:27:47](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1667.46s)]
*  one large scale set of brain areas, which is kind of what how we've traditionally done comparisons [[00:27:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1673.5400000000002s)]
*  in neuro AI. But now what we want to do here is really engage with whole brain data that's [[00:27:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1677.7s)]
*  coming online and start to understand how these brain areas interact to give rise to complex [[00:28:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1684.1000000000001s)]
*  behavior. And so I think that's why the agent naturally fits in here. And the idea would be [[00:28:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1688.4199999999998s)]
*  that like, you know, some some natural starting points for modules are a sensory module, right? [[00:28:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1694.6599999999999s)]
*  So this not only vision, but also multimodal. And there's some evidence which we can talk about at [[00:28:21](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1701.1399999999999s)]
*  some point about the how maybe they have similar actually loss functions across like self supervised [[00:28:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1705.9399999999998s)]
*  loss functions across these different sensory modalities. So there's a kind of unification [[00:28:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1711.1399999999999s)]
*  there. But maybe they have different inputs. But that's that's something that we could we could [[00:28:35](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1715.38s)]
*  talk about. So that's why I kind of group it as a sensory module. But then there is a kind of like [[00:28:39](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1719.46s)]
*  future inference or or world model, which I think is is really the hardest. I think competition, [[00:28:45](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1725.46s)]
*  the hardest part of this. Like, I think we've made a lot of progress in sense in sensory systems. [[00:28:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1731.38s)]
*  Obviously, there's there's work to be done still there. But especially the type of work that still [[00:28:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1736.66s)]
*  remains to be done in the sensory system connects with this world model. So basically, being able [[00:29:01](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1741.22s)]
*  to like, have a model of the dynamics of the world of your environment and not like a model [[00:29:05](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1745.22s)]
*  of real physics, like actual physics, like you and I, you know, have the maybe intuition that like, [[00:29:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1750.82s)]
*  two objects will will fall if one faster if one is heavier than the other. And even though we know [[00:29:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1756.42s)]
*  they should both fall in a vacuum at the same rate. So I don't mean actual physics, I mean, [[00:29:21](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1761.38s)]
*  just intuitive physics, like what's intuitive to us and helps predict things. How does that differ [[00:29:26](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1766.02s)]
*  from like the tenon bomb physics engine approach? So it is certainly the basically is the physics [[00:29:30](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1770.26s)]
*  engine. It's just that the difference is that we aren't assuming symbols as the input, we aren't [[00:29:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1778.02s)]
*  assuming like a program or anything like that. We're actually assuming unstructured visual inputs [[00:29:45](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1785.06s)]
*  coming in being processed by a sensory system, the output representation of that sensory system [[00:29:50](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1790.1s)]
*  is then fed into the to the to the world model. So it's visually grounded, basically, or sensorily [[00:29:55](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1795.78s)]
*  grounded rather than we kind of assume that there's a particular type of output format that vision [[00:30:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1802.42s)]
*  gives us. And then we proceed with that more symbolically. Okay, gotcha. And this is the why. [[00:30:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1808.82s)]
*  So why maybe is that distinction actually important for function? It's well, because [[00:30:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1814.66s)]
*  we operate humans also and animals operate in unstructured environment, in a wide range of [[00:30:18](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1818.5s)]
*  environments. Whereas when you handcraft those inputs, you know, that might be useful for studying [[00:30:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1822.9s)]
*  particular environments, but it's very hard to then generalize to the open ended unstructured [[00:30:27](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1827.7800000000002s)]
*  environments that we all naturally engage with. And so that's the key here is open ended, being [[00:30:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1831.8600000000001s)]
*  able to deal with open ended environments. So that's, that's at a high level, that's module [[00:30:36](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1836.74s)]
*  number two is the world model. So sensory world model. And then there's like planning, which I [[00:30:41](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1841.38s)]
*  don't know if it should be part of necessarily distinct, but just to distinguish the fact that [[00:30:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1846.02s)]
*  like, you know, if you have a good world model, you can also planning is easier, especially long [[00:30:50](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1850.18s)]
*  range planning. And maybe there's a hierarchy there of time scales. So you might plan at high level [[00:30:55](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1855.46s)]
*  and then fill in the details. So at a high level, you might have abstractions that allow you to do [[00:31:01](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1861.46s)]
*  more longer range planning. So for example, like, when, when you do decide to get out the door, [[00:31:07](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1867.3s)]
*  and then go to the Mellon Institute, right, in that in that case, you don't plan every step, [[00:31:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1872.98s)]
*  right, you plan at the level of landmarks, right, or large scale kind of things, right. And so [[00:31:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1877.8600000000001s)]
*  that's, that's what I mean. But then, but then you're the rest of your body fills in those other [[00:31:23](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1883.3s)]
*  details, the fine grained motor commands, etc. And that brings me to the last point, which is [[00:31:27](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1887.22s)]
*  the motor module, which then executes these high level commands, and maybe in a hierarchical way. [[00:31:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1891.78s)]
*  And then, and then I guess maybe there's like a final module, if it's helpful to think in these [[00:31:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1898.26s)]
*  ways. Again, I expect these to all be approximating the actual brain, that's just [[00:31:43](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1903.54s)]
*  more for like conceptual clarity, to frame things is the intrinsic goals. So in other words, [[00:31:47](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1907.54s)]
*  how do we guide what plans we care about or select, and then therefore the actions we execute? Well, [[00:31:54](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1914.1s)]
*  we can leverage the world model by like, planning through a specific types of actions, but those are [[00:31:59](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1919.46s)]
*  guided by intrinsic drive. So, you know, unlike reinforcement learning in games, like Go or chess, [[00:32:05](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1925.78s)]
*  where you have a very well defined reward function. And that's, that's where RL thrives. [[00:32:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1932.98s)]
*  In real environments, there is there isn't one. So animals do rely on things like different [[00:32:19](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1939.46s)]
*  behavioral states, hunger, pain, etc. that are that are both built in. But ultimately, so that, [[00:32:24](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1944.82s)]
*  you know, you can think of there's like built in intrinsic drives, but also more learned ones that [[00:32:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1951.6200000000001s)]
*  might support more open ended learning to like, seek out more information beyond the pre training [[00:32:35](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1955.54s)]
*  data. So I just put that in the language of AI. Basically, if we if you want an LM agent even to [[00:32:41](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1961.22s)]
*  go beyond its pre training data, you want to specify the right autonomous signals, which is [[00:32:45](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1965.94s)]
*  still an open question to do that. And to adapt online. Yeah, so so so those are the five modules, [[00:32:50](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1970.1000000000001s)]
*  right? So sensory modeling for now you say yeah, for now, for now. So sensory world modeling, [[00:32:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1977.8600000000001s)]
*  planning, motor and intrinsic goals, how they're combined will matter. So I just kind of assumed [[00:33:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1983.06s)]
*  a feed forward one for now. Of course, they they expect there to be back connections. But, [[00:33:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1990.26s)]
*  but the other aspect of it is that not only these modules aren't fixed, right? So the standard [[00:33:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=1997.86s)]
*  paradigm in AI is to like fix things and then do it at test time evaluate it. What does that? Oh, [[00:33:21](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2001.54s)]
*  oh, what do you mean fix like freeze the parameters, freeze the parameters and then and then evaluate [[00:33:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2009.14s)]
*  a test time of what we want to do. And ultimately, obviously, there's plasticity right in the brain [[00:33:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2014.66s)]
*  is for these animals to or these agents to adapt online. We need to specify update rules as well [[00:33:41](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2021.14s)]
*  for those modules. So not only how they interact, but also how they update online to new challenges. [[00:33:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2028.74s)]
*  And so that kind of speaks to the learning rules aspect of neuro AI, which I think is less touched [[00:33:54](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2034.42s)]
*  on and we can touch on it about it too. And there's things to say there. But but that's the high [[00:33:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2038.98s)]
*  level approach. So you can think of like, for example, test time reasoning that people have now, [[00:34:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2043.38s)]
*  where you're trying to get the LM to reason online and either via chain of thought or [[00:34:07](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2047.8600000000001s)]
*  trying to do this in a more an online setting rather than just scale up the pre training data, [[00:34:15](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2055.3s)]
*  like just get to reason at test time as like a special case of this broader goal of [[00:34:19](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2059.38s)]
*  getting these modules to be to be adaptive. Yeah. [[00:34:24](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2064.02s)]
*  I mean, let's talk about plasticity. You just said that it's not so much the focus of neuro AI, but [[00:34:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2068.2599999999998s)]
*  is that yeah, but is it has it gone out of favor? Is that a thing? Because for a while, [[00:34:36](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2076.18s)]
*  it was like all about, oh, back propagation. It's not right brain like, and we need to figure out. [[00:34:40](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2080.18s)]
*  But but there are lots of people like, working on synaptic learning rules, etc. And I, [[00:34:44](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2084.66s)]
*  I'll just jump the ship here and say, like, and you can correct me again, these modules that you [[00:34:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2091.2999999999997s)]
*  say that are sort of up in the air, but they don't have to have the same learning algorithms, [[00:34:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2096.98s)]
*  necessarily in different parts of the brain, have different learning algorithms. That's something [[00:35:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2103.3s)]
*  that you want to figure out. But people aren't focusing on learning anymore. I would say not [[00:35:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2108.26s)]
*  not anymore. I would say it just wasn't. Oftentimes in neuro AI, we're not modeling the, [[00:35:13](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2113.78s)]
*  the developmental process. Never. Yeah. Right. And we often often you might hear phrases like, [[00:35:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2120.18s)]
*  oh, this is this back prop is basically a proxy for evolution, or as a proxy for evolution and [[00:35:26](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2126.98s)]
*  development. And we don't cleanly separate those things. Okay. One way to say this is like, [[00:35:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2131.06s)]
*  we have something that works. So we're going to use it and worry about it later. Is that [[00:35:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2137.62s)]
*  it's like that. It's, it's, it's also that like, we're not explicitly modeling it. And you can't [[00:35:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2142.34s)]
*  in the standard framework where you just train something of back prop and get to the adult state [[00:35:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2148.02s)]
*  of that particular brain area. I think of pre training is more like, you can imagine you [[00:35:52](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2152.1s)]
*  pre train these modules to get to a desired state. But you unless you have an agent, [[00:35:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2156.58s)]
*  you can't actually go and test an update rule online with not through batches, but through [[00:36:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2164.02s)]
*  online interaction. Can you like more faithfully disentangle the evolution part where the pre [[00:36:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2169.54s)]
*  training stops? And where the module updating begins the kind of more developmental aspects. [[00:36:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2174.9s)]
*  So if we wanted a more formal computational grounding on development, and we want to neuro [[00:36:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2180.1s)]
*  AI to engage with that, I think that's why the agent based approach would more explicitly [[00:36:24](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2184.1s)]
*  speak to that by disentangling those two things, those assumptions that we're currently making, [[00:36:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2188.9s)]
*  and we're kind of lumping into back prop. And, and furthermore, like, you know, you mentioned [[00:36:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2193.94s)]
*  that there was like a lot of work actually on biologically plausible learning rules, and there [[00:36:40](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2200.98s)]
*  has been right. And I myself have worked on it with with folks, where the big challenge, say up [[00:36:45](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2205.46s)]
*  till 2019 or 2020 was from 2016 to 2020. Basically, there was a flurry of activity to [[00:36:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2213.3s)]
*  understand back prop in the brain, right? But it's almost to like, show that back prop happens in the [[00:37:01](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2221.0600000000004s)]
*  brain. Right? Yeah, that's right. Because we all use it for, for trainees networks is very [[00:37:06](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2226.5800000000004s)]
*  successful, right? And, and so the natural kind of like inclination was like, well, some version of [[00:37:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2232.1000000000004s)]
*  it might be in the brain, we should go look for it. What would that what would that be? But the [[00:37:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2237.94s)]
*  trouble was that, like, we couldn't scale any. So, okay, what's, what's the biggest there's a [[00:37:21](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2241.94s)]
*  there's a few gripes about, which are very well motivated gripes about back prop as a non [[00:37:27](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2247.38s)]
*  biologically plausible learning rule, I would say the main one just to keep things succinct is [[00:37:32](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2252.7400000000002s)]
*  that the forward and backward weights are always tied. So in other words, an update, an error [[00:37:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2258.34s)]
*  update, in a feed forward network, always involves the transpose of the forward weights of each layer. [[00:37:44](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2264.1s)]
*  And so oftentimes, when we think of implementing back prop in the brain, or as an update rule for [[00:37:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2271.2999999999997s)]
*  a module in this kind of more embodied agent framework, we assume that that that would be a [[00:37:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2276.58s)]
*  separate network that is basically computing the errors. And so if something like what's called [[00:38:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2282.5s)]
*  weight transport is necessary, then then that circuit would actually have to have an exact copy [[00:38:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2288.58s)]
*  of the forward weights. And that's weird at every time step, you need that. And that just seems very [[00:38:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2296.26s)]
*  like inconsistent with like the fact that by biology, very noisy and messy and non robust to [[00:38:21](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2301.0600000000004s)]
*  Yeah, I mean, you know what I mean? So right, it's like a very, but it's very, [[00:38:26](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2306.26s)]
*  oh, but people like Tim Lillicrap has shown have shown that you don't need to do it that way. And [[00:38:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2309.38s)]
*  you can approximate back propagation. I'm sorry, this is a total tangent, but you can approximate [[00:38:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2314.6600000000003s)]
*  it almost randomly with some feedback. So that's the thing. So I actually ended up being a was a [[00:38:39](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2319.7s)]
*  control that they ran, showing that it shouldn't work. That's what Tim told me actually. [[00:38:49](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2329.06s)]
*  Showing it shouldn't work. And then it did on MNIST. And they were like, well, we should [[00:38:55](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2335.7s)]
*  investigate this. It was very, very interesting story. Yeah. And, but one thing that Tim, so one [[00:39:00](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2340.02s)]
*  thing that motivated me to work on it was actually Tim gave a talk at like cosine 2016, that like, [[00:39:07](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2347.46s)]
*  they tried to take their feedback alignment algorithm, which is this, this thing of replacing [[00:39:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2352.82s)]
*  the backward weights with random weights, and scale it up to deeper architectures and on harder [[00:39:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2357.06s)]
*  tasks in MNIST. So like CIFAR 10, CIFAR 100, ImageNet, and especially ImageNet, because that [[00:39:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2362.1s)]
*  was the main vision data set that if trained with backprop gave you a neurally plausible [[00:39:26](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2366.82s)]
*  representations, right, that actually predicted brain data, not MNIST, for example. So they wanted [[00:39:32](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2372.34s)]
*  to scale up to that. And he was saying, look, guys, like, the moment we do this, like, it just fails, [[00:39:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2377.78s)]
*  like, at these harder tasks, there's this bigger and bigger gap that grows with the performance [[00:39:44](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2384.6600000000003s)]
*  backprop. And so, you know, that, at the time, I didn't know what to do with with that. It was very [[00:39:50](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2390.9s)]
*  interesting. And I knew I wanted to work on it. It wasn't until like, 2019 or so that maybe there [[00:39:59](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2399.54s)]
*  was some evidence that like, updating the backward weights, though, with what rule that's the key [[00:40:06](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2406.58s)]
*  question, could start to patch that up, but not completely. And so, with Dan Koonin and Javier [[00:40:11](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2411.54s)]
*  Sagasthayi-Brena, we like did work, and Surya and Dan, we like developed a broader language of like, [[00:40:23](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2423.3s)]
*  you know, basically, like, very looking through space of update rules and on the backward weights. [[00:40:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2431.6200000000003s)]
*  So rather than keep them random, to update them. And once we had a kind of like, library of [[00:40:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2438.6600000000003s)]
*  primitives, based on things like energy efficiency, improving the communication clarity [[00:40:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2446.5s)]
*  between the forward and backward updates, that sort of thing. Then we could kind of do a bit of a [[00:40:54](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2454.98s)]
*  search there. And we started on smaller scale experiments, finding just like, what was working [[00:41:00](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2460.82s)]
*  and starting to scale with ImageNet, that we could then find that ultimately through a larger scale [[00:41:05](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2465.54s)]
*  search that something like an OHA style update, though not exactly, but just to kind of like, [[00:41:11](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2471.78s)]
*  summarize it. An OHA style update was like, good with a few. So anyway, so once you have the [[00:41:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2476.98s)]
*  language for it and update rules, you could finally close this gap. And the main other issue was that [[00:41:26](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2486.02s)]
*  even if you close the gap on one architecture, like ResNet 18, as you went to deeper models, [[00:41:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2491.78s)]
*  the same hyperparameters of your local learning rule didn't actually transfer. And this is actually [[00:41:36](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2496.1800000000003s)]
*  unlike SGD. So in SGD, in Backrop, you actually do that transfer does occur. And so that would be [[00:41:40](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2500.26s)]
*  really unfortunate from an evolution point of view, every time you create a new organism, [[00:41:47](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2507.0600000000004s)]
*  you have to like, do another search of the hyperparameters. So we found actually robust [[00:41:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2511.86s)]
*  primitives that were robust, hyperparameter robust, and then you could transfer to very [[00:41:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2518.5800000000004s)]
*  deep architectures as well on ImageNet. So once we had that, then there was like an N of one [[00:42:05](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2525.14s)]
*  example of like, here's a learning rule that's vector error based, but doesn't require the [[00:42:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2530.5s)]
*  weight symmetry that Backrop needed. And so then maybe that starts to become a plausible candidate [[00:42:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2536.1s)]
*  to look for in brain data. Okay. All right. But it's not the hot topic these days, I suppose. [[00:42:21](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2541.7799999999997s)]
*  Not anymore. Yeah, I wouldn't say it is. And I mean, there was a kind of question of like, [[00:42:27](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2547.3799999999997s)]
*  what brain data should you measure? And so we had some work on that using artificial neural [[00:42:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2553.22s)]
*  networks, those follow up work and showing that the activations are actually enough. So related [[00:42:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2557.2999999999997s)]
*  to my earlier point about how model activations correlate with intelligent behavior, tracking [[00:42:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2562.1s)]
*  those changes across time also correlates with better identifying the learning rule in artificial [[00:42:47](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2567.54s)]
*  neural networks where you have ground truth. And you can weaken that with noise and limited [[00:42:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2573.06s)]
*  observations to start to mimic what we actually get in the brain. And it's still robust to that, [[00:42:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2576.74s)]
*  unlike synaptic weight changes, which are the more natural thing to look at. And activations [[00:43:00](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2580.34s)]
*  are much easier experiment to do. You just do EFIS, right? As opposed to like tracking the [[00:43:05](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2585.46s)]
*  dendric spines, which is a much harder experiment. So there is an open question of like, can we go [[00:43:11](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2591.94s)]
*  and validate this in data? And we have some evidence that it's a much easier experiment [[00:43:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2596.58s)]
*  than previously thought. But yeah, it's, as you say, it hasn't been the main focus of the field, [[00:43:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2600.6600000000003s)]
*  nor of at least of my own interest at the moment. But I think it's important. I think once we get, [[00:43:27](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2607.54s)]
*  because basically I want to get to an agent architecture that does work. But then we can [[00:43:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2613.38s)]
*  then start to study those questions of the module update rules, development, and those things. So [[00:43:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2618.5s)]
*  those are interesting down the line. They're just not the immediate interest because there's just [[00:43:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2622.5s)]
*  kind of like upfront challenges to begin with to get to the modules to some initial state that's [[00:43:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2626.34s)]
*  actually good and they combine well to begin with. So you want to make an agent in a robot, [[00:43:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2631.06s)]
*  right? Or is that a more longer term? The robot is a longer term. Right now it's all in sim with [[00:43:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2637.62s)]
*  like biomechanically realistic bodies. So biomechanically realistic bodies also have [[00:44:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2644.02s)]
*  like a much larger number of degrees of freedom than current robot bodies. So they're about like, [[00:44:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2649.2999999999997s)]
*  so let's say spot is about, I think 10 degrees of freedom or something like that. But of course, [[00:44:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2656.58s)]
*  they have lots of low level control. There's a beauty in the hardware stack, but we wanted [[00:44:21](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2661.22s)]
*  to kind of more focus on the actual like biomechanical control aspect of it, the high [[00:44:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2665.62s)]
*  degree of freedom biomechanical control aspect of it. So that's why we're doing it in sim where [[00:44:30](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2670.98s)]
*  you don't need hardware to approximate it. You can actually be more exact about it. [[00:44:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2674.9s)]
*  And because a lot of neuroscientists like do VR experiments, there's no gap in the eval. Like you [[00:44:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2678.74s)]
*  can literally take the same stimulus in sim and just put it in a new simulated world that [[00:44:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2686.34s)]
*  matches what the experimentalists did. And experimentalists like it because like sim stuff, [[00:44:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2691.54s)]
*  because it's very controllable and repeatable. And so you can now close the gap with the [[00:44:55](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2695.7000000000003s)]
*  evaluations as well. Right. Yeah. And it's not real. I mean, you're very well aware of Moravec's [[00:45:00](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2700.82s)]
*  paradox that the hard things are actually kind of easy so we can play chess in computers really well [[00:45:07](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2707.3s)]
*  and the easy the things that we think are easy like ping pong are hard, right? Like physically [[00:45:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2714.82s)]
*  doing ping pong because you can simulate it with however many degrees of freedom that you [[00:45:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2722.7400000000002s)]
*  want in a an agent and you can control it. But then once you get in the real world, [[00:45:26](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2726.26s)]
*  it's all of a sudden hard. The reason why I'm bringing that up is because one of the things [[00:45:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2731.3s)]
*  I wanted to ask you, which I think is related is, you know, there have been a lot of since the early [[00:45:35](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2735.54s)]
*  go fi AI days, there have been a lot of cognitive architectures, right? And it's kind of transition [[00:45:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2742.34s)]
*  from symbolic. Then you have like hybrids like Chris Elias Smith's spawn and you have [[00:45:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2748.42s)]
*  Randy O'Reilly that are making like really more connectionist type cognitive architecture type [[00:45:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2756.6600000000003s)]
*  things. But without fail, I believe maybe this is not the case for Randy O'Reilly, but everyone has [[00:46:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2763.46s)]
*  not everyone. A lot of the people who have worked on these things have suggested that it's [[00:46:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2776.1s)]
*  it's not so difficult to actually get one module to perform well. It's actually the [[00:46:21](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2781.38s)]
*  crosstalk between modules, the control between the modules. That's the difficult thing. And it seems [[00:46:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2788.26s)]
*  to take way more effort than actually getting the modules themselves to do what you want them to do. [[00:46:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2794.42s)]
*  So is that on your? So I actually agree. But but I would say that it comes with a nuance though, [[00:46:39](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2799.78s)]
*  that it depends on your goal. So if your goal is like more open ended, unstructured environments, [[00:46:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2806.98s)]
*  it is also a challenge in itself to build the modules. So like the world model or it took a [[00:46:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2811.86s)]
*  while to even get like a good sensory encoder. So if you go beyond like particular tasks to like [[00:46:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2816.82s)]
*  more open ended tasks, it's already a challenge in itself to get to that pre training, right? Like [[00:47:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2822.34s)]
*  we didn't have good SSL things till like a few years ago, loss function. So and advances in that [[00:47:07](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2827.2200000000003s)]
*  in better SSL algorithms also led to better brain models in, for example, mouse visual cortex, [[00:47:13](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2833.2200000000003s)]
*  where it's not categorization optimized. So all to say, like, definitely connecting the [[00:47:19](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2839.3s)]
*  modules is not trivial. But even constructing the modules, especially I think nowadays with the [[00:47:26](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2846.7400000000002s)]
*  world model, and figuring out what those representations even should be is actually, [[00:47:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2853.3s)]
*  a is actually, I think, still a core challenge. So to give an even more concrete example, like [[00:47:39](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2859.38s)]
*  for today, right, a very common approach right now is VLM. So vision language modules. And [[00:47:44](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2864.98s)]
*  you know, we can go and collect tons of robot data in the world, like we can drive Teslas, [[00:47:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2873.3s)]
*  like people drive Teslas, and they have like a century's worth of data. It's not hard to get [[00:47:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2878.82s)]
*  lots of data. Actually, it's not the bottleneck. It's that the scaling laws have not been as [[00:48:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2884.42s)]
*  favorable on those types of data with the existing VLM architectures. So even if we wanted to go back [[00:48:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2890.74s)]
*  to the sensory module, right, like, you know, the skill has haven't been as favorable as they [[00:48:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2896.3399999999997s)]
*  haven't been in language. And I think part of this is, is the architecture itself. And in particular, [[00:48:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2902.02s)]
*  like, the way we tokenize, in other words, the way we like process the inputs to give to these [[00:48:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2908.1s)]
*  specific VLM architectures are random patches. And so they end up basically learning something [[00:48:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2913.22s)]
*  like a convolution, which doesn't end up being ultimately a step change or an advance over what [[00:48:39](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2919.3s)]
*  we had with CNNs. They're actually converged, so it's sort of like the platonic representation [[00:48:43](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2923.86s)]
*  hypothesis. They're like converging on very similar representations. And also, as a result, [[00:48:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2928.42s)]
*  like, you know, vision transformers are also very similar matched to the brain as CNNs are, [[00:48:52](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2932.5s)]
*  because they're effectively approximating convolution. So and I think that in language, [[00:48:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2937.6200000000003s)]
*  though, the notion of a token, individual constituent words is very much semantically related to what [[00:49:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2942.5s)]
*  that modality to that modality is trying to do, right? Combining words gives rise to the higher [[00:49:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2948.18s)]
*  meaning. But the patches themselves are like, we don't have a good, in other words, prompting [[00:49:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2952.82s)]
*  language for vision yet. And I think there's still like an advance there to be made even in that [[00:49:18](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2958.18s)]
*  domain. So is, are these the sorts of issues that made you start focusing more on what would what [[00:49:23](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2963.62s)]
*  you have written as Mars algorithmic level rather than so Mars three levels, right? And everyone [[00:49:32](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2972.58s)]
*  for the past 10 years has been have been focused on the computational level. And that's what AI [[00:49:40](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2980.5s)]
*  focuses on, for example, you have written and many people have that, oh, the reason why these [[00:49:45](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2985.7s)]
*  models work so well is because we give them a goal, give them a task, right? And that's the [[00:49:50](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2990.42s)]
*  computational level, something that they need to accomplish. And then the algorithmic level, how [[00:49:55](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=2995.46s)]
*  they accomplish that algorithmically is somewhat less important, but they can learn them. And [[00:50:01](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3001.7s)]
*  that's why these models are so great. And then the implementation level, who cares? Just you have to [[00:50:06](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3006.58s)]
*  put something in there and it and eventually it'll give rise to it. But so are these the sorts of [[00:50:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3012.8999999999996s)]
*  issues that have made you focus more on that algorithmic level? Yeah, so I think I think just [[00:50:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3017.8599999999997s)]
*  to even translate what you just said to to like neuro AI, right, like the, the competition level [[00:50:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3025.7s)]
*  is the task and the algorithmic level is related to the architecture, but also the interactions [[00:50:30](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3030.58s)]
*  between the task and the architecture, too. And so in some cases, I think, like, you know, it's not [[00:50:35](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3035.54s)]
*  just about specifying the right goals. It's not just about figuring out the right self supervised [[00:50:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3042.1s)]
*  objective, like next token prediction or contrastive learning, that sort of thing, which is an advance [[00:50:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3046.8199999999997s)]
*  and so it's also figuring out the architecture that meshes well with that modality. I think there's a [[00:50:52](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3052.34s)]
*  lot of promise in using transformers or like the token based kind of paradigm, because it is a [[00:50:59](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3059.46s)]
*  little bit modality independent. In other words, you know, it's kind of general purpose, you just [[00:51:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3064.26s)]
*  swap in different data. But I think with that generality, I think not all sensory systems are [[00:51:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3068.34s)]
*  necessarily equal in that way. I think there's nice, maybe there's a lot of shared explained [[00:51:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3074.34s)]
*  variance between them. And we have some evidence for that. But I think going forward, if we really [[00:51:18](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3078.9s)]
*  want to get things that are better at intuitive physics, where a lot of the models lack in terms [[00:51:24](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3084.58s)]
*  of human capabilities, we will likely have to be start to become more specific about how we, [[00:51:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3089.06s)]
*  at least to put in the language of today, tokenize or process inputs that are more vision and, [[00:51:35](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3095.62s)]
*  and embodiment based. And they may just be different than language. And this is probably [[00:51:40](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3100.82s)]
*  consistent with how maybe things are in the human brain where the language areas were evolved later, [[00:51:45](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3105.62s)]
*  and are a little bit like topographically distinct from what visual cortex looks like. [[00:51:50](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3110.98s)]
*  You don't have language in your in your modules yet, right? [[00:51:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3117.46s)]
*  So the way that's right, you're concerned about also comparing across species. And given that [[00:52:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3122.34s)]
*  humans are the only species that use language, I know that's arguable, but let's just say it, [[00:52:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3128.82s)]
*  it might not be what you're after. That's right. And I think that's maybe why I think that there's a [[00:52:13](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3133.94s)]
*  there's a core underlying desire of us wanting algorithmic desire of us wanting these agents to [[00:52:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3140.7400000000002s)]
*  better understand the world. Animals certainly build models of the world without language. And [[00:52:27](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3147.78s)]
*  that's already hard. So this relates to the the like coming up with a prompting language for [[00:52:32](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3152.5800000000004s)]
*  vision for VLM is better than this kind of like random image patch token thing that I think would [[00:52:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3157.46s)]
*  better speak to the kind of visual intelligence that animals have that that could that needs [[00:52:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3162.02s)]
*  improvement in existing architectures today. But I think I think where language can play a role, [[00:52:47](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3167.3s)]
*  even if you're modeling at that level, is at least like, you know, you could argue that you're [[00:52:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3173.0600000000004s)]
*  using language when you train a model on supervised categorization, right? Like you're, [[00:52:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3177.86s)]
*  you're providing labels for the for the images. And so, you know, you could make that argument [[00:53:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3182.9s)]
*  that that language is maybe a useful guide to learn representations, even if you're they're [[00:53:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3189.78s)]
*  not in a linguistic context. So I think, for example, like, if we're stuck on a certain [[00:53:15](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3195.22s)]
*  question, we can't come up with the right self supervised objective that we think an animal [[00:53:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3202.02s)]
*  could implement could be implementing plausibly, or, you know, related to that, like a better [[00:53:27](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3207.7799999999997s)]
*  prompting language for vision that isn't language based, then I think it's fine as a proxy, [[00:53:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3213.62s)]
*  at the moment to use the kind of the less good but still gets you somewhere [[00:53:39](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3219.7799999999997s)]
*  a supervised language conditioned version of that loss function on before you kind of figure out the [[00:53:43](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3223.7799999999997s)]
*  self supervised thing. And I think that's fine, I might actually get you quite far. And I think we [[00:53:50](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3230.9s)]
*  shouldn't throw that away either. It's just not used in a specifically linguistic context. It's [[00:53:55](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3235.22s)]
*  really more used for guiding representations, which is, you know, what we've been really doing [[00:54:00](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3240.8199999999997s)]
*  language is a remarkable ability to like teach machines how to like reason like us are like for [[00:54:05](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3245.2999999999997s)]
*  us to communicate that basically. Yeah. But one of the things that I that I enjoy about your approach [[00:54:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3250.1s)]
*  is that you appreciate the sensor motor loop, right, and the agentic aspect of intelligence [[00:54:15](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3255.46s)]
*  and existence. But then like earlier, you're the way you're talking about it, it sounds very much [[00:54:24](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3264.1s)]
*  like a input output. Brain is a computer metaphor kind of thing. And you started in vision, you [[00:54:30](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3270.2599999999998s)]
*  started in sensation, and you've come to appreciate the motor aspect of it. Some people would turn [[00:54:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3278.82s)]
*  that around, like active inference people and say that actually you're you're behaving to adjust your [[00:54:44](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3284.02s)]
*  sensory input. How do you where you land on that? Oh, totally. Yeah. So so this this this relates to [[00:54:50](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3290.1000000000004s)]
*  the to the online to why having an agent is actually the way I think to studying questions [[00:54:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3297.06s)]
*  about development or online learning, rather than our existing way, which is just to like lump it [[00:55:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3303.6200000000003s)]
*  all into backprop single backdrop update is that is that yeah, like, this is also related to the [[00:55:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3308.66s)]
*  goals, like whatever goals it's using to guide its exploration in the environment, the intrinsic goals [[00:55:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3314.42s)]
*  that's, which is where those come from. That's another thing I wanted to put a pin in that. [[00:55:19](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3319.54s)]
*  Yeah, no, that's I think that's that's a interesting question on its own. And and but but like, [[00:55:24](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3324.58s)]
*  whatever those goals are, they're gonna they're gonna they're gonna alter the training data. And [[00:55:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3329.62s)]
*  one of the reasons you want the online update rules is to adapt to distribution shift online. [[00:55:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3333.46s)]
*  So if you're going to go beyond your pre training data, and you're going to explore your environment, [[00:55:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3338.74s)]
*  you have to also adapt to the fact that you're going to encounter out of distribution things [[00:55:45](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3345.2999999999997s)]
*  simply by exploring the world a bit. And so in order to handle that, in a robust and reliable way, [[00:55:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3348.8199999999997s)]
*  you'll also need an update rule and where and the exploration strategy is guided by these intrinsic [[00:55:55](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3355.3799999999997s)]
*  goals. So where do those goals come from? So that I think is is is unclear. [[00:56:00](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3360.8199999999997s)]
*  Fully like I don't think there's a definite answer yet. But we have Yeah, I mean, it's [[00:56:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3368.66s)]
*  the computer scientist way, right is to then program in the goals. Is that achievable? Can [[00:56:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3374.5s)]
*  you program in the goals? Whatever, because it seems to be this mysterious central core of our [[00:56:21](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3381.06s)]
*  existence biologically, right is that we have these intrinsic goals, no one knows where they [[00:56:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3388.58s)]
*  come from. It's an internal reference signal that we have to follow. We have to where we want to be [[00:56:32](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3392.18s)]
*  at homeostasis. But it's kind of a mystery. And the computer programmer wants to like just all right, [[00:56:36](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3396.42s)]
*  program in the goal, right? Is that achievable? So I think it's less like it's actually this is [[00:56:43](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3403.06s)]
*  related to, you know, things like reward hacking that people talk about these days, [[00:56:49](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3409.7s)]
*  in in modern agents, which is that like, we might think we're programming it in [[00:56:54](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3414.26s)]
*  by specifying it, but actually, because these things are optimized, [[00:56:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3418.26s)]
*  just specifying a particular goal might lead to unexpected behavior, emergent kind of [[00:57:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3422.1800000000003s)]
*  behavior. Exactly emergent, either desired or undesired behavior. That's harder to, [[00:57:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3428.42s)]
*  you know, if we were dealing with like computer programs, like quite literally, [[00:57:15](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3435.3s)]
*  then yeah, you know, it should, I mean, even then computer programs can lead to unexpected behavior [[00:57:19](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3439.2200000000003s)]
*  too, right? You just didn't fully anticipate as the as the creator of the program, all the [[00:57:23](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3443.54s)]
*  possible things that that could lead as an outcome, right? So like, right, like AI safety, people [[00:57:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3448.98s)]
*  like have used the paperclip maximizer as the unexpected version of this, right, that it [[00:57:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3454.18s)]
*  maximizes paper clips. And then it's like, well, like, I should just take everything take over, [[00:57:39](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3459.3s)]
*  right? And that's an unexpected outcome. So even when you knew, like, it was very myopic and very [[00:57:43](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3463.7s)]
*  specific, it can lead to that. So but like, to your broader question of like, where where are [[00:57:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3468.42s)]
*  these goals, I think, I think it likely they could be distributed. So I mean, the obvious candidate [[00:57:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3473.94s)]
*  for everything that someone doesn't know is to say it's prefrontal cortex. But you could also [[00:57:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3478.66s)]
*  imagine that, but that, but a lot of organisms don't have prefrontal cortexes and that's right. [[00:58:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3482.98s)]
*  That's right, actually. So related to other organisms, you know, there's beautiful work by [[00:58:11](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3491.86s)]
*  Misha Arens group at Janelia that shows that zebrafish have a kind of futility induced [[00:58:18](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3498.7400000000002s)]
*  passivity that's actually computed in non neuronal cells in astrocytes. Oh, really? That's cool. [[00:58:26](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3506.82s)]
*  And, and so we're actually looking, working with them studying this related to this intrinsic [[00:58:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3514.66s)]
*  goals question, trying to figure out what these intrinsic goals are, but and we should have [[00:58:41](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3521.46s)]
*  something else soon. But the main thing is that that like, you know, I think I think these goals [[00:58:45](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3525.94s)]
*  can be computed in a lot of different ways and a lot of different parts of the brain that isn't [[00:58:50](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3530.82s)]
*  just PFC or something neuronal, but even other animals in non neuronal cells, potentially. [[00:58:54](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3534.7400000000002s)]
*  And even even if it is fully neuronal in other higher species, it could be that that's done in, [[00:59:01](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3541.86s)]
*  say, like, large scale, like, say, flammable cortical loops, etc. I don't think it's localized, [[00:59:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3548.1s)]
*  in other words, like, in fact, the evidence that we're seeing is maybe that it's not so fully [[00:59:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3554.1s)]
*  localized, but that that animal behavior is still very stereotyped. In other words, like from the [[00:59:18](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3558.02s)]
*  kind of decade of neuroethology with machine learning that's applied to naturalistic videos, [[00:59:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3565.62s)]
*  going back to our naturalistic discussion in the beginning, people have found that that these ML [[00:59:30](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3570.02s)]
*  tools are kind of auto discovering behavioral primitives that seem to be reliably switched [[00:59:35](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3575.14s)]
*  between more or less. And so it does kind of, that's kind of what motivates this kind of like, [[00:59:40](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3580.34s)]
*  hardwired intrinsic goals thing that I'm mentioning here is that there are unlike [[00:59:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3586.5s)]
*  end to end RL, where it's like one objective, there's probably multiple things that switches [[00:59:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3591.06s)]
*  between and dynamically state switches between and, and that might not necessarily have to be [[00:59:55](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3595.38s)]
*  localized, those individual goals might be represented in different parts of the brain. [[00:59:59](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3599.94s)]
*  Hmm. Let's talk to ring test. [[01:00:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3603.86s)]
*  And that's what is it? I mean, is there, we're all over the place. And I apologize, this is that's [[01:00:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3608.5800000000004s)]
*  my fault as the host, but it's been it's fun for me. So fun for me, maybe before we move on, is [[01:00:13](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3613.46s)]
*  there is there something that you want to add on the cognitive architecture slash? I keep calling [[01:00:18](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3618.98s)]
*  it cognitive architecture. Sorry. So the agent, embodied agent that you're working on, you know, [[01:00:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3625.3s)]
*  I call it a cognitive work teacher, too. But I think maybe the main difference is that that what [[01:00:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3629.6200000000003s)]
*  we really care about our open ended, unstructured environments that like humans and animals are in, [[01:00:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3633.54s)]
*  and actual like not just compare. So open ended tasks, and also like comparison to not just [[01:00:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3638.74s)]
*  behavior, but but internal representations as well. So comparing the individual modules to the [[01:00:45](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3645.46s)]
*  individual brain areas, and then interactions between modules and online updates there in to [[01:00:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3651.22s)]
*  developmental signals down the line. But, but, but this actually nicely segues into the Turing [[01:00:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3656.18s)]
*  test, because right at the ultimate root of this is quantitative comparison, right with [[01:01:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3663.14s)]
*  either before you start talking about it. So you've put out there's a manuscript, [[01:01:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3669.8599999999997s)]
*  it'll be linked in the show notes. Is it called the neuro AI Turing test? Yeah, okay. Which [[01:01:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3674.58s)]
*  modernizes Alan Turing's Turing test, which he didn't call it the Turing test, but it came to [[01:01:21](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3681.7s)]
*  be known as the Turing test where can you fool a human, if you're a computer, can you fool a human [[01:01:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3688.34s)]
*  to think that you are a human. And so that was very focused on behavior. And you're saying, [[01:01:35](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3695.54s)]
*  yes, that's great. But in the neuro AI world, we actually need to also compare the internal [[01:01:40](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3700.6600000000003s)]
*  quote unquote representations, which we'll talk about what that means. But and so you have the [[01:01:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3708.7400000000002s)]
*  behavioral comparison, but also the internal representations comparison, and that should be [[01:01:52](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3712.98s)]
*  a benchmark of sorts. That's that's exactly right. And the key principle is that for any any measure [[01:01:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3717.2999999999997s)]
*  of comparison, you want, you want your models to be as good as brains are to each other. [[01:02:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3723.86s)]
*  In the context of internal and behavioral representation. So what does that mean? [[01:02:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3729.94s)]
*  Explain what that means? Yeah. Yeah. So basically, right, like we expect that so okay, there's two, [[01:02:13](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3733.54s)]
*  there's two issues here when doing these model brain comparisons, at least, which is one is that [[01:02:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3740.9s)]
*  brains are stochastic. So unlike our models, which are deterministic, they they respond [[01:02:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3745.94s)]
*  variably to the same stimulus. And that's been well quantified up till now as the internal [[01:02:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3751.14s)]
*  consistency of the neurons. So those statistical noise ceiling that we call in the paper. And [[01:02:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3757.7000000000003s)]
*  that's often been either either 100% is used as a ceiling or implicitly or that more quantified [[01:02:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3762.98s)]
*  metric of the internal consistency of the stochasticity of how neurons are consistent [[01:02:50](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3770.5s)]
*  to each other across trials has been used. But the other aspect of it is that that like, [[01:02:55](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3775.78s)]
*  it doesn't that doesn't like as cleanly semantically map on to like how we actually [[01:03:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3784.58s)]
*  compare models to brains. Like right, we're like mapping one representation to another. And this [[01:03:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3789.14s)]
*  statistical noise ceiling isn't really capturing that it's just capturing the stochasticity of [[01:03:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3794.18s)]
*  neurons, which which is good, we want to capture that. But it's not the only thing we want to like [[01:03:18](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3798.5s)]
*  measure a ceiling. So if anything actually becomes a correction term later on. But the main thing we [[01:03:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3802.74s)]
*  want to capture is the fact that like, even if a brain was deterministic, right, even if the [[01:03:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3809.62s)]
*  stochasticity didn't matter, if you took two brains, there's just going to be variability between them. [[01:03:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3814.02s)]
*  And there's just like within, you know, individuals in a given species, fix the same brain area, [[01:03:40](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3820.74s)]
*  fix the same stimulus, there's just going to be variability in how they process that. So [[01:03:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3826.66s)]
*  when a model is imperfect as a match to the brain, we need to be able to disentangle whether that's [[01:03:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3831.7s)]
*  because it's actually a poor match to the brain truly, or because there is inherent [[01:03:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3838.42s)]
*  evolutionary variability between brains that we need to account for. And so that's why we [[01:04:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3844.5s)]
*  emphasize so whatever metric you use of comparison, we can talk about that is, is under that metric, [[01:04:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3849.7799999999997s)]
*  you should also do a comparison of the of the brains to each other as though the brain was [[01:04:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3856.26s)]
*  another model. And then the idea that you have one model against lots of different brains, [[01:04:19](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3859.78s)]
*  multiple models against lots of different brains within a species, for example. [[01:04:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3865.0600000000004s)]
*  Yeah, so so it's still like using the context of this integrated benchmarking of like having [[01:04:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3871.1400000000003s)]
*  multiple models to at least, you know, one set of brain data was the one brain. You can, of course, [[01:04:36](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3876.34s)]
*  go it's good to like, then maybe do cross species comparisons down the line. But this is at a minimum, [[01:04:45](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3885.2200000000003s)]
*  like the the the base thing you want to first strain out is the model to brain to single brain [[01:04:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3891.7000000000003s)]
*  comparison question. And then to generalize it is just applying that same procedure over and over. [[01:04:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3897.3s)]
*  So once you've established that procedure, what that ceiling should be, it's going to be the same [[01:05:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3903.3s)]
*  one that applies to other brain areas, other species as well. So this is where your mathematical [[01:05:06](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3906.9s)]
*  and logic background really comes through, because you really specify how these comparisons [[01:05:13](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3913.7s)]
*  are going to be made theoretically, right. And you were just talking about the metric. So let's talk [[01:05:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3920.98s)]
*  about how to compare these like, you leave it open and say you can compare any metric of whatever [[01:05:26](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3926.1s)]
*  you use for the representation. I just you did air quotes. Yeah. Yeah, no, I think that's that's very [[01:05:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3933.9399999999996s)]
*  totally appropriate for this. Because, right. You know, the question of what metric to use is [[01:05:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3942.1800000000003s)]
*  certainly an important one. I think oftentimes as a field, and certainly I have as well, there has [[01:05:47](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3947.62s)]
*  been an implicit assumption that there's like, one platonically good metric that we ought to strive [[01:05:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3953.78s)]
*  for of like model to brain goodness. And, you know, this is also even reflected in pre neuro AI days, [[01:06:00](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3960.1800000000003s)]
*  where we wanted like brains, like we had different notions of brain likeness in words, [[01:06:06](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3966.18s)]
*  like sparsity, energy efficiency, things like this, that we wanted our models to have. And so [[01:06:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3972.5s)]
*  like, when we when we assume that even in this more quantitative setting, that there's a [[01:06:19](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3979.7s)]
*  platonically good notion of a metric, then that is then in some sense, it's like we're saying [[01:06:23](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3983.06s)]
*  upfront, well, we know what, what what it means to match the brain well. And so you might as well [[01:06:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3989.2999999999997s)]
*  just bake that in, and then you're done, right? Like, I mean, the problem is that that's not the [[01:06:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=3994.98s)]
*  case, right? That we don't know upfront, what a priori what a good brain model should be, [[01:06:40](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4000.02s)]
*  we have data, and, and we want to match the data as good as brains are to each other under that, [[01:06:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4006.58s)]
*  the assumptions of that data collection process. That's the reality, the empirical reality of it. [[01:06:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4013.94s)]
*  And I would also argue that given that the brain is a complex object, there's different things that [[01:06:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4018.66s)]
*  people focus on, like we were talking about how like, people have different definitions with [[01:07:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4022.66s)]
*  neuro AI. It's like, you know, some people really do care about, you know, topography, for example, [[01:07:06](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4026.1s)]
*  right, which a CNN is not a 3d spatial map, right, it's 2d. So under a topographic metric, [[01:07:11](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4031.7799999999997s)]
*  it would be zero effectively, right? Under that. So different people have different things for [[01:07:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4040.3399999999997s)]
*  their question. So I don't actually even think in in, not only in platonic realities, they're not [[01:07:24](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4044.8199999999997s)]
*  a platonic we good one, but it's just highly question dependent anyway. [[01:07:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4049.78s)]
*  What you keep saying platonic here, and I know that you've thought about this, there's what is [[01:07:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4053.3s)]
*  it the platonic hypothesis that's been floating around? What is that? And why do we like or dislike [[01:07:36](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4056.82s)]
*  it? No, so I, so yeah, so I just meant platonic and like, kind of like an overall notion of good [[01:07:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4062.9s)]
*  that we should all strive for. Yeah, okay, then let's not go into the platonic hypothesis then [[01:07:49](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4069.6200000000003s)]
*  it's too far on the side. But you mean like an ideal that is an ideal that we should all be [[01:07:54](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4074.98s)]
*  striving for. And I think that basically there is is no such thing as an ideal, it's just question [[01:07:59](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4079.94s)]
*  dependent, and the brain is very complex. So you might focus on different aspects of it. And [[01:08:06](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4086.26s)]
*  all we're trying to do here is just say, there's, we want to standardize that operationally and say, [[01:08:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4090.82s)]
*  well, whatever metric you choose for your question, make sure that you assess model goodness up to how [[01:08:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4096.42s)]
*  brains vary under that measure. Okay, so it's, in some sense, it's extremely pluralistic, [[01:08:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4102.26s)]
*  because it allows the user, here's what I want to ask is like, how do I pass your neuro AI [[01:08:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4108.5s)]
*  Turing test? And how do I fail it? Right? So I can come with my own question, as long as my question, [[01:08:35](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4115.22s)]
*  as long as I adhere to the scientific rigor of the neuro AI test, I can come in with any question or [[01:08:41](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4121.780000000001s)]
*  any assumptions as long as I state them. So I can, it almost sounds like I can pass it if I want to. [[01:08:49](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4129.38s)]
*  Right. So, so of course, you could you could define a, maybe a trivial metric or something where [[01:08:55](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4135.7s)]
*  it's zero, or something on that. And then and then the models are right. And so, so certainly, [[01:09:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4142.74s)]
*  you could do that. But then but then you could argue, well, that was what was sufficient for [[01:09:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4149.06s)]
*  your question. So, I mean, you know, I can't I can't tell, I can't tell scientists, like, [[01:09:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4152.1s)]
*  what is a good question or not, that's their judgment, of course. But but instead, yes, so [[01:09:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4156.98s)]
*  the, the idea is that you define a metric that you want to score model goodness on, [[01:09:23](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4163.219999999999s)]
*  let's say, let's say that metric is, well, you mentioned efficient coding or sparsity, [[01:09:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4169.78s)]
*  right? You could just do it on sparsity. You could do it on individual spikes. In a population, [[01:09:35](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4175.219999999999s)]
*  you could do it on, I don't know, astrocyte calcium signaling, yeah, whatever you want to do. [[01:09:41](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4181.7s)]
*  It's meant to be extensible, right? It's meant to encounter the broad range of applicability [[01:09:47](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4187.139999999999s)]
*  and diversity of questions that we have in the brain sciences naturally, because the brain is [[01:09:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4191.7s)]
*  complex. I can tell you, though, in practice, what I, I do and why, in terms of the metrics [[01:09:55](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4195.379999999999s)]
*  that I choose, I think for a lot of settings, so I want to be clear, like envision, like, I think [[01:10:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4202.179999999999s)]
*  we have had the luxury of such advances to get us to models that were really good. And so, you know, [[01:10:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4208.099999999999s)]
*  the most common benchmark was, for example, HVM, which is the human, which is the, the game is at [[01:10:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4214.1s)]
*  all one that people kind of push on that there was the initial one that brain score used, [[01:10:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4220.34s)]
*  brain score now uses a lot more other vision benchmarks too, as part of it. But like, you know, [[01:10:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4225.780000000001s)]
*  for a while, like when we were first working with HVM, which is like 2016, 2017, one of the impetus [[01:10:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4231.38s)]
*  for coming up with this animal to animal measure was that, like, according to statistical noise [[01:10:35](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4235.9400000000005s)]
*  ceiling, like the models were explaining 50% or 60% of that. And we're like, Whoa, like, clearly, [[01:10:41](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4241.14s)]
*  there must be like advances in vision needed to beat this like very simple visual behavior of like [[01:10:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4246.9800000000005s)]
*  an animal staring at a stimulus and doing nothing else with it, like, you know, the first, you know, [[01:10:54](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4254.02s)]
*  150 milliseconds of visual processing. And it turned out that actually, when you looked at the [[01:10:59](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4259.54s)]
*  animals to each other, and we, this was actually ended up being a supplement in the Conver and [[01:11:06](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4266.42s)]
*  in paper, and then we like made it one of the main figures in the neuro AI thing, which is because [[01:11:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4272.34s)]
*  like, even the authors of the neuro AI turing tests, like have just relegated this thing to [[01:11:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4276.02s)]
*  supplement for some reason, I mean, not intentionally, because we there's other focuses, it was just, [[01:11:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4280.82s)]
*  yeah. And so on HVM, it wasn't 60%, it was actually 90%. So in other words, it's just to say that like, [[01:11:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4285.62s)]
*  now that's not to say we've solved object recognition, we're saying on this particular [[01:11:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4294.58s)]
*  data set, this benchmark that people have been trying, I don't know if you would really want to [[01:11:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4297.54s)]
*  go and invest major money to do advances in vision to push on HVM in particular, you might instead be [[01:11:43](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4303.0599999999995s)]
*  motivated and say, okay, one of two things, I'm done with the question. So because this benchmark [[01:11:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4311.62s)]
*  was the thing that mattered to me most, and I think the vision models are good enough for my [[01:11:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4317.54s)]
*  purposes, or two, because of this saturation, you'd be more motivated to say, I'm going to go [[01:12:00](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4320.98s)]
*  collect more data, higher variation data, or to really push on object recognition, and again, [[01:12:06](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4326.419999999999s)]
*  set my ceiling to the animal to animal consistency there. Like one, like for example, we did a bunch [[01:12:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4332.5s)]
*  of extrapolations of the neuro AI turing tests on the HVM data. And we found that actually having [[01:12:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4337.0599999999995s)]
*  more conditions and having more neurons did start to improve that ceiling. I mean, it was starting [[01:12:23](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4343.62s)]
*  to saturate at some point, but it's within the realm of collecting, it motivates that you should [[01:12:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4348.58s)]
*  actually go and collect it, do something more concrete and collecting a new experiment there [[01:12:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4354.18s)]
*  too, as another possible viable route. But it does rule out as a viable route, I think, [[01:12:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4358.42s)]
*  to continue to push on, for example, HVM, where you may have thought the gap was much bigger, [[01:12:44](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4364.74s)]
*  but it's actually much smaller than you imagined. What is a representation? When you use the term [[01:12:49](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4369.14s)]
*  representation, what do you mean? I mostly, actually 100% of the time mean, like the population [[01:12:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4376.58s)]
*  activity. Yeah, it's weird because that's all I mean. Yeah, I think that that's the common usage [[01:13:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4384.5s)]
*  in neuroscience, but that of course, there's the philosophy of mind way of using it, cognitive [[01:13:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4389.3s)]
*  sciences. So it's kind of a slippery term. And it's kind of deflated in that sense, in that it [[01:13:15](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4395.78s)]
*  just means the activity of whatever you're studying. Yeah, so I've had a hard time understanding, [[01:13:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4400.74s)]
*  and maybe you could tell me about this, but understanding the arguments in cognitive science [[01:13:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4408.98s)]
*  that aren't population activity and why it's a more nuanced term. But I've honestly literally [[01:13:36](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4416.58s)]
*  meant it as a vector of population responses, of firing rate, binned firing rate. So very precisely, [[01:13:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4422.34s)]
*  it's a very precise notion. I mean, I guess, from my understanding into the cognitive sciences, [[01:13:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4428.98s)]
*  does that vector necessarily contain all the semantically meaningful stuff that we're assigning [[01:13:55](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4435.7s)]
*  to a behavior? I think that's what... Well, I don't like to do the etymology thing or [[01:14:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4442.82s)]
*  tear apart words, but re-present, represent is re-present. And in the, I think, it is the idea of [[01:14:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4450.82s)]
*  like, okay, well, this is presented in my mind, and it is attached to the thing in the world. [[01:14:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4460.66s)]
*  And it's like somehow a copy of that. It's re-presenting in my mind, which is very different [[01:14:26](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4466.74s)]
*  than just a measurement of activity in some brain region. Yeah, yeah. Oh, and I mean, I think we can, [[01:14:32](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4472.0199999999995s)]
*  I mean, we can see that, like, you know, if you go to visual cortex, there are some efference copies [[01:14:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4478.98s)]
*  from other areas representing other things that you can decode. That's the thing is you can decode [[01:14:43](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4483.86s)]
*  a lot from a lot of different brain areas. But is that meaningful? Is it causal? Is it, [[01:14:49](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4489.219999999999s)]
*  is it correlational? Is it a representation? So there's a lot of talk these days about [[01:14:55](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4495.219999999999s)]
*  being more careful with the term representation, but the way that you use it, [[01:14:59](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4499.86s)]
*  you don't have to be careful with it. Maybe we just need a different term, because it just [[01:15:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4503.94s)]
*  means the activity. Yeah, I literally just mean the activity. So for me, that's always there. [[01:15:07](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4507.62s)]
*  The question of like, the semantic representation or something like that, if you want to call it a [[01:15:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4512.74s)]
*  slightly different term, or maybe a completely different term, meaning, meaning to it and like, [[01:15:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4517.14s)]
*  oh, this, this part of the, it kind of like emerges from this, like, maybe pre neuroscience [[01:15:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4522.82s)]
*  view of like pre modern neuroscience view of like, there's this like one function that you can [[01:15:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4528.0199999999995s)]
*  assign to this one region, like cluster of cells in the brain. And like, that's where it is. And [[01:15:32](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4532.5s)]
*  like, I think that's, you know, by and large, probably a very toy view of brain because it's, [[01:15:39](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4539.06s)]
*  you know, it's quite distributed in ways that we don't expect. And I think the only way to really [[01:15:44](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4544.1s)]
*  get at those kinds of questions is not to go in assuming that there is a very localized function. [[01:15:49](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4549.06s)]
*  I mean, in some cases there are, but not in a lot of cases, and then do those kinds of quantitative [[01:15:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4553.7s)]
*  comparisons between these, like, that's why to engage with whole brain data, you want to go in [[01:15:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4558.1s)]
*  this embodied agent direction, because ultimately these things do interact and do influence each [[01:16:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4562.5s)]
*  other. And you want to test whether that hypothesis of the interaction between the modules is a good [[01:16:06](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4566.660000000001s)]
*  one. And the only way you can do that is to have the modules, to have them interact, and then do [[01:16:11](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4571.700000000001s)]
*  this kind of neuro AI Turing tests on top, just to kind of combine all these ideas together at the [[01:16:15](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4575.14s)]
*  level of population activity to begin with, to really assess does this vector, this population [[01:16:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4580.1s)]
*  activity contain that semantic content? And I think you can answer that in a quantifiable way. [[01:16:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4585.78s)]
*  Yes, no, or point six, five, right? But it's not one, one point or zero, basically. [[01:16:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4591.0599999999995s)]
*  It's interesting, though, that you, okay, so you just said something that I very much [[01:16:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4597.139999999999s)]
*  jive with and appreciate that back in the phrenology days, right, we said brain area x does [[01:16:41](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4601.139999999999s)]
*  function y, right? And you said that that's not necessarily the case, it's probably not the case, [[01:16:47](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4607.0599999999995s)]
*  it's very distributed. However, the thing that you're wanting to build is made up of modules [[01:16:52](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4612.18s)]
*  that do functions and then have to talk to each other. How do you how do we reconcile those things? [[01:16:59](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4619.54s)]
*  That's an excellent, excellent question. So when I say function, right, I'm not saying like, [[01:17:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4624.26s)]
*  like, you know, do like, do the Jennifer Aniston recognition, right? Or, or anything like that. [[01:17:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4629.14s)]
*  It's like a more general purpose, like, it's such a general loss function anyway, that is like [[01:17:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4634.9800000000005s)]
*  actually highly nonspecific. Yes, there is some localization. But it's like, you know, for example, [[01:17:19](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4639.86s)]
*  for self supervised learning, like next token prediction, right, or a world model, right? [[01:17:24](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4644.5s)]
*  It's it's trying to like, figure out the dynamics of like, from the current state to the next state. [[01:17:30](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4650.259999999999s)]
*  That's a very general thing. It's not as specific as maybe in the phrenology days, where it's like, [[01:17:35](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4655.0599999999995s)]
*  oh, it's this specific thing. Like, you know, recognizing a particular person or, or face or [[01:17:41](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4661.7s)]
*  places alone or things like that. I'm not saying that the brain doesn't have it. [[01:17:49](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4669.3s)]
*  I think that maybe through this optimization of a general purpose, high level goal, [[01:17:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4673.06s)]
*  you can learn internal representations that do have more specific content to them. So I think [[01:17:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4677.06s)]
*  that's entirely possible. And we know that right, like face patches emerge, that sort of thing. So [[01:18:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4682.34s)]
*  it's just that with the it's just that the the kind of top down guides to build those [[01:18:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4688.34s)]
*  high level modules, those large scale modules are a lot more general purpose than specific. [[01:18:13](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4693.22s)]
*  Okay, fair enough. Ron, what what's holding you back these days? [[01:18:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4697.38s)]
*  Two questions. What are you excited about? And then what's in your way? [[01:18:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4702.74s)]
*  That's a good question. So I mean, what I'm excited about is ultimately, like even beyond, [[01:18:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4708.18s)]
*  you know, specific scientific questions we can ask, like, we are entering an era where we're [[01:18:36](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4716.26s)]
*  just building more capable systems. And, you know, even modern agents that we're building today with [[01:18:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4722.1s)]
*  LMS, they start to have like cognitive components to them, like people are starting to realize, [[01:18:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4726.98s)]
*  oh, you need a memory, things like that, you know, and compositionality modularity. [[01:18:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4731.219999999999s)]
*  And so, so that's good. And, you know, it's consistent with how the breakthroughs needed [[01:18:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4736.179999999999s)]
*  in AI to get to the next generation have also led to better better overlaps with the brain, [[01:19:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4743.62s)]
*  partly because there's been fewer solutions to get there. And so like, since the brain has already [[01:19:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4750.259999999999s)]
*  reached that, it's there's a high probability of that overlap. So that's the contravariance principle [[01:19:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4754.58s)]
*  or in AI, the platonic representation hypothesis that we've referenced a couple times now. And [[01:19:18](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4758.66s)]
*  so what I'm excited about is that we're entering an era of more and more capable systems. And I [[01:19:23](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4763.86s)]
*  do feel that like, it's, it's, I what used to maybe be a sci fi dream, or even a dream. It was still [[01:19:27](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4767.86s)]
*  it felt a little bit within reach, but still a far away dream of 50 years back when I started [[01:19:36](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4776.9s)]
*  neural networks. [[01:19:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4782.82s)]
*  Are you about to say it's all going to happen within five years? [[01:19:43](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4783.38s)]
*  No, no, I'm not about to say give give a timeline. But I do think that it's it's a lot sooner than [[01:19:47](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4787.78s)]
*  maybe even a capable system. So like, I don't mean general intelligence, I just mean, even like a [[01:19:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4793.38s)]
*  weekly capable AI system that can do tasks for autonomously for 24 hours, that would already [[01:19:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4798.58s)]
*  have like a huge economic impact. And already LLMs are have changed, you know, education, right? Like, [[01:20:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4804.66s)]
*  you know, we make our tests like in class, and like on paper, because like, so that, you know, [[01:20:11](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4811.94s)]
*  students are really tested on what they actually know. [[01:20:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4817.46s)]
*  I was questioning myself just yesterday, walking in the windy sunshine in Pittsburgh here. And I [[01:20:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4820.5s)]
*  was thinking like, Am I am I learning faster now that I'm using chat GPT to find things out? Or is [[01:20:27](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4827.379999999999s)]
*  it slowing it down? Or like, how is it affecting me? Anyway, it's different. [[01:20:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4834.58s)]
*  I was I was thinking that too. And like, you know, in some ways, am I like engaging as critically [[01:20:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4838.26s)]
*  as I used to, but at the same time, does it outweigh the amount by which I can quickly learn a new [[01:20:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4842.820000000001s)]
*  topic? Right? It seems it seems quite efficient to me. I think overall, for myself, it's been a [[01:20:47](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4847.700000000001s)]
*  real benefit. That's right. And like, yeah, like, yeah, I know there are problems and hallucination, [[01:20:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4853.54s)]
*  etc. But it's gotten much better. One and two, like even at the stages that it's like hugely [[01:20:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4858.740000000001s)]
*  impacted are like, I haven't met a person that doesn't fully use it or use it to some extent, [[01:21:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4864.26s)]
*  even when they were skeptical initially, like, right, it started to become like Google search, [[01:21:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4869.46s)]
*  right? It basically has taken that place. Yeah, has taken that place. Right. And so that's so I'm [[01:21:13](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4873.7s)]
*  just saying like, even like a not AGI at all, AI system, that's just useful and capable, which I [[01:21:18](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4878.42s)]
*  think is the target of a lot of AI, AI companies today is going to impact things in ways that [[01:21:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4885.22s)]
*  maybe we can't always foresee. So that's what I'm excited about. It's like at least entering an [[01:21:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4891.86s)]
*  era where that sort of starts to feel like I find a little bit like, you know, having a little [[01:21:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4897.219999999999s)]
*  assistant, but essentially, you can hold a reasonably productive conversation with is cool. [[01:21:41](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4901.7s)]
*  So that's that's what I'm excited about what I guess held back by is, is, I think, it continues [[01:21:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4908.5s)]
*  to be like, as always in science ideas. In other words, like, I don't think it's so much compute, [[01:21:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4916.1s)]
*  actually, like, I think, as an academic, you try to be more. In fact, the lack of compute, though [[01:22:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4922.02s)]
*  they were, it's quite fine, actually, for our purposes, is, is drives you to be more creative. [[01:22:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4929.780000000001s)]
*  And I think that's the fun part. But it's also that, like, you know, making sure that, you know, [[01:22:15](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4935.14s)]
*  what if, you know, we're in the wrong paradigm, or something in some way, right. And I think I'm [[01:22:19](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4939.46s)]
*  always open to that. What percentage do you put on on the likelihood that we're in the wrong paradigm? [[01:22:23](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4943.78s)]
*  Well, probably, probably 10%. Oh, you think we're in the right paradigm. And this is a weird thing [[01:22:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4953.46s)]
*  to say, like, there is a correct paradigm, because there's I don't think there is, but we're in one [[01:22:41](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4961.54s)]
*  right now. We're in one that I think is very productive and empirically so and more so than [[01:22:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4966.66s)]
*  other prior approaches. So I think that speaks volumes. I don't I think there's limitations to [[01:22:50](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4970.82s)]
*  it too. So I you know, maybe the ultimate paradigm that people use might be very different. And I [[01:22:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4976.66s)]
*  think that's totally normal within scientific progress, we just, it makes sense to push as hard [[01:23:01](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4981.94s)]
*  as you can on the existing thing while it's bearing fruit, and see how far you can take it. And then [[01:23:06](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4986.9s)]
*  when it stops doing that, you have like very, like reasonable next steps of what to take rather than [[01:23:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4992.0199999999995s)]
*  kind of completely jumping ship immediately. And that's sort of my I guess, my my style is push hard [[01:23:18](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=4998.26s)]
*  on the things, you know, really steelman it. And because it's failed, you know, you were the advocate [[01:23:24](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5004.5s)]
*  of the thing. Now you're seeing it's no longer empirically giving you gains, move on to the next [[01:23:30](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5010.1s)]
*  thing. And at least you know, where where where no longer what problems it's falling short on [[01:23:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5014.1s)]
*  to generate new hypotheses. But and I also do think that like, you know, if the ultimate goal is like, [[01:23:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5018.34s)]
*  is is the brain, at least at a very detailed, like molecular level to that's helpful for disease, [[01:23:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5026.0199999999995s)]
*  not just at this kind of algorithmic level that we're talking about, that's more hardware agnostic, [[01:23:53](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5033.62s)]
*  that's often talked about in Neuro AI. I think that can take a lot longer. I think that'll take [[01:23:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5038.18s)]
*  that will probably happen before we have these very competent algorithms that occur, [[01:24:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5043.7s)]
*  wait, especially before wait, what? So sorry, we will have better disease treatment before we have [[01:24:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5050.1s)]
*  No, no. Oh, okay. After after and I think I think I think that's where AI for science can be helpful. [[01:24:18](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5058.1s)]
*  I think actually that like, by having these better agents that can also accelerate the types of [[01:24:26](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5066.02s)]
*  because like biology is enormously complex. And the brain as a part of that is itself enormously [[01:24:32](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5072.18s)]
*  complex, especially as you go down to beyond the algorithmic level to like the synaptic level and [[01:24:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5077.7s)]
*  the neurotransmitter level. And I think that that like, what will ultimately I aid that discovery [[01:24:44](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5084.0199999999995s)]
*  is our systems that can really process lots of data and not be tied to like particular simple [[01:24:49](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5089.94s)]
*  stories, which is what biology, you know, has suffered from to some extent, and really like, [[01:24:54](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5094.74s)]
*  help maximize and find new information to generate those hypotheses. So I think I think in other words, [[01:25:01](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5101.0599999999995s)]
*  like, that's why I particularly focus on the algorithmic level is because I think that like, [[01:25:05](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5105.22s)]
*  there's a lot of room to improve there. And I think we can get there much more quickly. [[01:25:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5109.860000000001s)]
*  But then that in turn has benefit for studying the brain at a more detailed level for disease, [[01:25:13](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5113.3s)]
*  that sort of thing. And biology more broadly, science more broadly down the down the line. [[01:25:19](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5119.22s)]
*  Jim DeCarlo, you know, we mentioned him earlier in his reverse engineering approach [[01:25:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5125.14s)]
*  thinks that when you can predict when you engineer something, you build it and you can make [[01:25:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5131.14s)]
*  predictions that basically is understanding. And what you were just saying about that we need, [[01:25:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5134.98s)]
*  you know, these agents to handle lots of data. There is a we suffer, we're simple, we need simple, [[01:25:41](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5141.219999999999s)]
*  short, low complexity sentences, symbolic things to hang on to, to say that we understand something. [[01:25:52](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5152.259999999999s)]
*  So are we losing understanding there? Or do you agree with Jim that it's that's what understanding [[01:25:59](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5159.78s)]
*  is? Yeah, so, you know, I think that ultimately, we'll always need our AI systems to [[01:26:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5164.9s)]
*  communicate simple things to us. It's just that just like, you know, [[01:26:13](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5173.54s)]
*  just like how in our AI, we talk about three things, the task architecture and learning rule, [[01:26:19](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5179.62s)]
*  like we're not you and I don't transmit the weights of the neural network to each other, [[01:26:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5185.0599999999995s)]
*  right? Like in terms of our understanding, we say, look, this pattern of these three things [[01:26:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5189.0599999999995s)]
*  explains this brain data, or predicts this neural activity of like 1000s of neurons to [[01:26:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5193.22s)]
*  100s of conditions. So that so there's always going to be a higher order language by which [[01:26:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5197.780000000001s)]
*  humans talk to each other and form models of the world. And those be informed by AI systems that [[01:26:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5202.1s)]
*  don't make those simplifications, but still communicate to us. In that context, I don't [[01:26:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5208.1s)]
*  think there's any way around, like us as a species using these things, it'll have to be in that in [[01:26:52](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5212.18s)]
*  that kind of language. But I would argue that neuro AI already does that because it summarizes [[01:26:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5217.3s)]
*  those three things in the particular context neuro AI. And one of the reasons why I so I do agree [[01:27:01](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5221.3s)]
*  typically with Jim Jim about the prediction thing. So related to the to the Turing test thing, [[01:27:05](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5225.54s)]
*  the types of metrics I usually use, especially in settings where we have less knowledge of [[01:27:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5230.42s)]
*  brain areas, like where we're making progress, it's much better to have a predictive model, [[01:27:15](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5235.38s)]
*  like we basically start with zero predictive models of the system to get to like one predict better [[01:27:19](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5239.860000000001s)]
*  predictive model already under linear prediction is already a huge advance and leads to control [[01:27:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5245.54s)]
*  optogenetic control, that sort of thing that we were talking that Jim Jim is known for. [[01:27:30](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5250.74s)]
*  And then down the line, and this is kind of where the AI agent thing comes in, [[01:27:35](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5255.78s)]
*  we want to make more finer grade distinctions for particular questions, like we're no longer [[01:27:39](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5259.3s)]
*  in the dark ages of that brain area, where, where, you know, we want to push on linear [[01:27:43](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5263.3s)]
*  predictivity, we then want to like ask very specific questions about maybe neurotransmitters [[01:27:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5268.82s)]
*  or chemicals there to like aid in a BMI, for example. So if you're going to have a brain [[01:27:54](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5274.099999999999s)]
*  machine interface, then you're going to have to care for about the particular physiology of that [[01:27:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5278.5s)]
*  individual, not just the average. So that's where again, like when we come to disease and other [[01:28:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5283.38s)]
*  things, you're going to want to ask these more finer grained questions, but you're going to build [[01:28:08](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5288.98s)]
*  on the most linear really predictive model to start with, and then iterate it for that particular [[01:28:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5292.66s)]
*  question. But you do still have to coming back to the neuro AI Turing test again, because I wanted [[01:28:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5297.38s)]
*  to ask you this earlier, because we were talking about how you can measure any metric that you want, [[01:28:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5302.66s)]
*  any any representation that you want, as long as you adhere to the theoretical premise of the test. [[01:28:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5309.139999999999s)]
*  But it seems that there's a lot built into what you decide to measure, what you what you decide [[01:28:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5314.98s)]
*  is the right metric. And so there's there's judgment, I think, that could be had on on that, [[01:28:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5322.34s)]
*  right? I could dismiss a model that measures only oscillations, for example, or that that would be [[01:28:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5328.34s)]
*  one that a lot of people would agree with me with, and a lot of people would disagree oscillations, [[01:28:54](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5334.02s)]
*  right? I just want to throw something out there that some people think is epiphenomenal, [[01:28:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5338.58s)]
*  and doesn't matter. And something people think that it's causal. I think it's both, that doesn't matter. [[01:29:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5342.74s)]
*  But so if I decided to measure beta synchrony, and that's my measure, then someone would say [[01:29:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5349.3s)]
*  could say, well, that's not even worth paying attention to, even though it passes the neuro AI [[01:29:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5357.46s)]
*  Turing test. So how do I know what metric I should measure? No, that's an excellent question. And [[01:29:21](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5361.46s)]
*  and there's two things I think I think if your goal is that particular question, [[01:29:30](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5370.1s)]
*  then then it makes sense to discard the other stuff and like focus on that for now. [[01:29:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5374.9800000000005s)]
*  Then I could say, then a lot of people would say, well, that's a worthless goal. [[01:29:39](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5379.86s)]
*  Totally, totally. But that's what scientists, that's a scientist debate all the time about, [[01:29:43](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5383.38s)]
*  I mean, it's no different than anything we've already been doing. The main thing, though, [[01:29:47](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5387.78s)]
*  is like if our ultimate goal, though, as a field is to have a consistent complete theory of brain [[01:29:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5391.86s)]
*  function across scales. And we don't have that today, but maybe in the future, then I think we [[01:29:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5397.22s)]
*  want it to agree on as many metrics that we all agree on as a field are valuable. And anything [[01:30:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5403.46s)]
*  that the brain has, like if that's your goal is ultimately a complete, we want it to be a [[01:30:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5409.22s)]
*  consistent, we want it to be passing neuro AI, multiple neuro AI Turing tests, rather than just [[01:30:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5412.900000000001s)]
*  one across all of those benchmarks. Okay. The other related question that I wanted to ask is, [[01:30:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5417.06s)]
*  you know, given multiple realizability and degeneracy in the way that populations can [[01:30:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5422.820000000001s)]
*  transform signals to then enact some action, if my, if the representations don't align, [[01:30:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5429.62s)]
*  is that really a problem? Like, can't I get to the same location by taking a different route? [[01:30:36](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5436.0199999999995s)]
*  And as long as I'm getting to that location, it doesn't really matter what my internal [[01:30:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5442.5s)]
*  representations are doing, if I'm achieving the task. [[01:30:46](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5446.66s)]
*  Yeah, that's right. And, and you could make that argument with one thing that's just [[01:30:50](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5450.74s)]
*  interesting is that you end up like, do the contravariance basically, the platonic [[01:30:54](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5454.660000000001s)]
*  represent you just end up like, even if you were like, Hey, I'm just purely an AI person, like my, [[01:31:00](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5460.02s)]
*  I don't really care about matching the brain. It just turns out that like your advances lead [[01:31:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5464.9800000000005s)]
*  to better brain models too. And like this was the case in vision, even with SSL objectives, like not [[01:31:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5472.02s)]
*  only were we better models of, of, of, of, of, of, of, of, of, of, of, of, of, of, of, of, of, of, [[01:31:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5477.9400000000005s)]
*  of, or with vision, it was like just high variation task was better models of primary visual cortex [[01:31:24](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5484.9800000000005s)]
*  that is better SSL. It is astonishing and so fucking cool. Yeah. And, and it's like, it didn't [[01:31:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5491.14s)]
*  have to, I mean, the converse principles is kind of explaining maybe why that might be, but it's, [[01:31:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5497.700000000001s)]
*  it's interesting that same with language and transformers, right? Like the, like the GPT [[01:31:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5502.820000000001s)]
*  based models are the best models, predictive models so far of human language areas. And, and [[01:31:47](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5507.22s)]
*  uh, when, when SSL objectives came out, uh, that were better, uh, like, and, and we were part of [[01:31:54](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5514.5s)]
*  that, like it was, you also got much better models of mouse visual cortex because it didn't, it gave [[01:32:01](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5521.0599999999995s)]
*  you more general purpose thing for the constraints of the smaller cortex and lower visual acuity it [[01:32:06](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5526.74s)]
*  needed. So in other words, it was like all of these advances in AI is fundamental advances that [[01:32:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5530.66s)]
*  we need to get to this ultimate goal of an open-ended autonomous agent, basically, right? I mean, [[01:32:15](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5535.139999999999s)]
*  that is what AGI is really. And, and all of those to get there have led to much better theories of [[01:32:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5540.34s)]
*  the internals than prior, prior theories that came before it. So I would say that like, if there is [[01:32:27](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5547.62s)]
*  a sign, what's lurking on, what does that tell you? Well, that tells you that there's a kind of [[01:32:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5553.62s)]
*  lurking underneath it, a science of intelligence that unifies like neurosciences goals, cognitive [[01:32:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5557.14s)]
*  science and AI, where it is about really building a kind of hardware agnostic theory of intelligence, [[01:32:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5562.74s)]
*  that it's about optimization under different constraints. And that's how these different [[01:32:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5568.66s)]
*  brains, different brain areas relate to one another across this type of spectrum. [[01:32:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5571.62s)]
*  All right, Aron, thank you for letting me take you on lots of divergent wandering paths here. [[01:32:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5577.22s)]
*  Is there anything else that we missed that you wanted to discuss or highlight or that you're [[01:33:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5582.5s)]
*  excited about or fearful of? Oh, I can, I can mention maybe, maybe for like a couple of minutes, [[01:33:07](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5587.3s)]
*  a little bit of the AI safety stuff. Sure. All right. We've had Steve Burns on the podcast of [[01:33:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5594.26s)]
*  the big AI safety person. That's awesome. Yeah. So, so I mentioned, you know, the main goal is [[01:33:19](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5599.38s)]
*  building, building better autonomous lifelong learning agents and using that to try to engage [[01:33:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5605.46s)]
*  with whole brain data. But the other, the other aspect of it is, is, you know, what happens once [[01:33:31](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5611.86s)]
*  we get there? And, you know, I think that's also another place where being an academic actually [[01:33:35](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5615.86s)]
*  makes a lot of sense because we don't really have a good science right now of alignment of, [[01:33:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5622.74s)]
*  of getting AI, making sure that these AI systems are aligned with human values and preferences. [[01:33:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5628.26s)]
*  And, you know, you mentioned, you know, programming and goals and how that least unexpected [[01:33:54](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5634.5s)]
*  behavior, the reward hacking, and that's a very common thing. And we want, we want to, you know, [[01:34:00](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5640.34s)]
*  try to avoid features of that as we build more and more capable systems, because even a weekly [[01:34:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5644.9s)]
*  capable system will have consequences, both good and bad. And we want to try to mitigate those, [[01:34:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5649.22s)]
*  those things as much as possible. Now, to be clear, I'm not a, I'm not a doomer or anything [[01:34:15](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5655.06s)]
*  like that. Like I don't, I do think that genuinely that I think humans have a higher risk of harming [[01:34:18](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5658.820000000001s)]
*  one another than any AI system, especially we have today. But that doesn't mean that, [[01:34:23](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5663.9400000000005s)]
*  that they can cause some harm. And at the end of the day, it's a technology we're building. So we, [[01:34:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5668.58s)]
*  you know, we have to, yeah, there's a nice quote by Dylan Hadfield Millel, who's at MIT, [[01:34:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5673.860000000001s)]
*  he works on AI safety, saying that, look, like when you're a bridge builder, like a civil engineer, [[01:34:40](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5680.18s)]
*  you also care about bridge safety. Right. So, so it's like that, I think it's like a part of, [[01:34:44](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5684.34s)]
*  it should be a natural part of if you're building these systems to think about that. And I think [[01:34:49](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5689.3s)]
*  that's one thing where academics can help because like right now, a lot of the alignment stuff is [[01:34:52](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5692.900000000001s)]
*  very high level frameworks, like policies, they're not really policies, they're just like [[01:34:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5697.14s)]
*  discussions. They're not precise. And we also don't have any guarantees of like, okay, say [[01:35:01](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5701.06s)]
*  hypothetically, we do achieve agents that are autonomous and capable, what then? Like, what are [[01:35:07](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5707.06s)]
*  the guarantees there? What's hard? And what's, and so I think that's where actually like my math [[01:35:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5712.18s)]
*  background maybe from earlier comes in is like, you can start to prove theorems about rational, [[01:35:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5716.660000000001s)]
*  capable, fully capable agents that don't misalign for trivial reasons by failing, but literally [[01:35:23](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5723.54s)]
*  they're ideal. They're computationally unbounded even. If there's things that are hard for them, [[01:35:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5729.3s)]
*  then it's going to be something we should avoid in practice. So one work that we had recently, [[01:35:37](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5737.38s)]
*  it's called like barriers and pathways to alignment that's under review is showing that like [[01:35:44](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5744.1s)]
*  some of the first complexity theoretic barriers to alignment, to the alignment problem. So if you have [[01:35:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5748.820000000001s)]
*  basically in a nutshell is showing that if you have too many distinct tasks or too many agents, [[01:35:54](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5754.34s)]
*  there's always going to be problems where the number of bits they have to exchange to prove to [[01:35:59](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5759.3s)]
*  provably reach alignment is going to be too large, basically. And so in other words, like, [[01:36:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5764.1s)]
*  what you ideally want to align are you want to choose your tasks and agents wisely. In other [[01:36:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5770.58s)]
*  words, you don't want to miss, it's not a question of like, if they'll misalign, it's when there's [[01:36:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5774.34s)]
*  always going to be tasks that, that even if they were incentivized to align, they will misalign. [[01:36:20](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5780.900000000001s)]
*  So we have to really be careful about the ones that we and agents that we want, [[01:36:26](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5786.66s)]
*  the tasks and agents that we want this alignment for. A corollary of this theoretical result is [[01:36:30](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5790.82s)]
*  saying that, you know, some people talk about brain computer interfaces as solving the alignment [[01:36:36](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5796.82s)]
*  problem. So in other words, like Elon Musk, for example, in Neuralink, right, like that's the only [[01:36:40](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5800.339999999999s)]
*  way we'll like merge with the AI is to, is this, yeah. And one obvious one, obviously in practice, [[01:36:44](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5804.099999999999s)]
*  the issue is with that is that our brains are constrained. And so, I don't think that that's [[01:36:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5811.46s)]
*  naturally going to be a bandaid to solve the alignment problem. But the other one is that [[01:36:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5817.38s)]
*  because these theoretical results is that even if our brains were unconstrained, we were these [[01:37:00](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5820.66s)]
*  perfectly rational, capable agents that were computationally unbounded, there would be these, [[01:37:04](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5824.82s)]
*  if we have too many distinct tasks and agents, like imagine all our BMIs or BCIs are connected [[01:37:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5830.42s)]
*  by Bluetooth, that you wouldn't, the number of bits you have to exchange would just be too large [[01:37:14](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5834.9s)]
*  anyway that you couldn't guarantee it. So still this, so one, like BCIs won't solve the alignment [[01:37:19](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5839.46s)]
*  problem. And two, choose your tasks and agents wisely. You know, the task of making a sandwich [[01:37:25](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5845.06s)]
*  and getting your agent aligned with you is far less, whether when it causes misalignment is far [[01:37:30](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5850.42s)]
*  far less harmful than running a nuclear power plant. [[01:37:34](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5854.5s)]
*  All right, let's go back to the, if you have another minute or two. Yeah, I do. All right. So [[01:37:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5858.66s)]
*  I'm a child. I'm going to try to illustrate this through a stupid story. All right. So I'm a child [[01:37:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5862.74s)]
*  and I build my first bridge by putting a piece of tree trunk over the creek and I try to walk [[01:37:47](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5867.7s)]
*  over it and it breaks. And in that case, I didn't, I didn't plan everything and say, all right, [[01:37:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5878.179999999999s)]
*  I'm going to build this bridge, but I have to care about the safety. I just made it and I'm still [[01:38:06](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5886.26s)]
*  around. And then I made a better bridge the next time. And then even a better bridge. And then [[01:38:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5890.9s)]
*  eventually started thinking about safety. And the point of this stupid bull story, I apologize, is [[01:38:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5896.18s)]
*  I think throughout human history, human history is not replete with examples of we're going to [[01:38:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5902.42s)]
*  plan for the safety. Human history is we're just going to move forward and make it. And then the [[01:38:30](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5910.5s)]
*  safety comes later. So why is this an exception? This is an excellent question. So one of the [[01:38:35](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5915.860000000001s)]
*  reasons why I think the theoretical study in this case is warranted because it's actually [[01:38:41](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5921.86s)]
*  faster to prove a theorem than to run it. We don't have AGI yet and we probably shouldn't [[01:38:47](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5927.219999999999s)]
*  anyway, if we were. The reason for this is that the fundamental difference between [[01:38:51](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5931.299999999999s)]
*  AI, this technology that we're building and prior technology is that now you're starting to build [[01:38:57](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5937.0599999999995s)]
*  technology that takes in inputs and intentionally produces actions. It's an agent. And so as a result, [[01:39:02](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5942.9s)]
*  we're not talking about the type of people. It's not passive. It's not like, for example, [[01:39:12](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5952.179999999999s)]
*  I was in an airport and the elevator broke and like, that's an inconvenience. And what do people [[01:39:18](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5958.58s)]
*  do when they have disability? That causes issues. But I'm not talking about machine failures here. [[01:39:24](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5964.179999999999s)]
*  I'm talking about things that are unique to AI. And so people in the AI safety study that right [[01:39:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5969.62s)]
*  now. They study hallucinations and current LLM systems and so forth, white box attacks, etc. And [[01:39:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5973.7s)]
*  that's great and really useful for today. I guess what I'm talking about here is like, suppose we [[01:39:40](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5980.18s)]
*  fix the functional problems. I'm not talking about misalignment with like failure modes there, [[01:39:45](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5985.62s)]
*  but we want to avoid the situation where you've now built a capable agent that is out there. It [[01:39:52](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5992.18s)]
*  might in the short term seem like it's agreeing with you and like helping you out, but it's like [[01:39:58](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=5998.900000000001s)]
*  either spreading lots of misinformation either for its own end or otherwise. And that ultimately [[01:40:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6003.86s)]
*  that leads to catastrophe in one way or another. And I'm not actually like, I don't think that's [[01:40:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6010.179999999999s)]
*  tomorrow or anything, but I think that as we're building more and more capable AI systems, [[01:40:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6017.38s)]
*  this question starts to become more relevant. And I think we can go far beyond, you know, sketches [[01:40:21](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6021.86s)]
*  of discussions about that are not as precise to really prove guarantees of like, okay, look, [[01:40:28](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6028.58s)]
*  if this is actually hard for a very capable system to align with it, then we have to avoid it in [[01:40:36](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6036.5s)]
*  practice. And furthermore, and this is something I'm working on now is how can we build better [[01:40:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6042.5s)]
*  incentives? So beyond RLHF, reinforcement learning with human feedback, which is the way we right [[01:40:48](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6048.02s)]
*  now align these LLMs with human values. Can we go beyond that and like design incentives with [[01:40:52](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6052.26s)]
*  theoretical guarantees that prevent the scenario that I'm talking about? And so that's an, and then [[01:40:56](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6056.98s)]
*  really implementing current systems today. So it does speak to systems today, but has guarantees [[01:41:03](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6063.54s)]
*  in the systems of tomorrow. All right, cool. Last thing before I say goodbye, I happened upon, [[01:41:07](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6067.86s)]
*  we were trying to figure out what movie to watch with my son the other day. And one of the ones I [[01:41:17](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6077.459999999999s)]
*  thought was like Matrix would be a good one. And then anyway, I happened upon like a little, [[01:41:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6082.1s)]
*  the little clip where they're talking about what the Matrix is, right? And the what happened with [[01:41:27](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6087.3s)]
*  humanity and stuff. And I remember I'm old enough to remember when this movie came out, and it was [[01:41:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6093.780000000001s)]
*  super cool. And everyone loves the same. I remember going to the theater to watch it. Oh, yeah. Yeah, [[01:41:38](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6098.1s)]
*  but I watched this thing. I thought this is so dumb. It's, it looks so cartoonish, not looks, [[01:41:42](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6102.900000000001s)]
*  but the premise is so cartoonish and ridiculous. It made me feel better that I've learned something [[01:41:49](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6109.3s)]
*  and now it looks, it's not like, oh, that's Matrix. That's awesome. So I don't know. How do you, how [[01:41:54](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6114.42s)]
*  do you feel about the Matrix in retrospect? In retrospect, I think it's, it's obviously very [[01:42:00](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6120.34s)]
*  exaggerated. I don't think anyone's going to get there or anything, but human bioelectric, [[01:42:05](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6125.46s)]
*  human body. Yeah, very creative. I think it's, I mean, the first, the first Matrix hands down, [[01:42:10](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6130.58s)]
*  one of the best movies. Everything else after was, you know, especially did you see the fourth one? [[01:42:16](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6136.42s)]
*  No. But good. Okay. We can leave it at that then. Yeah, good. All right. Anyway. All right. All [[01:42:22](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6142.26s)]
*  right. I've got to actually go to work in a few minutes and I suppose you do too. Thank you so [[01:42:29](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6149.06s)]
*  much for your time and I'll see you around campus, I hope. Yeah. Yeah. Thank you so much. This is [[01:42:33](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6153.54s)]
*  wonderful and a great pleasure and honor to be here. [[01:42:39](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6159.22s)]
*  Thank you for your support. See you next time. [[01:43:09](https://www.youtube.com/watch?v=_RMVdo3TN_k&t=6189.22s)]
