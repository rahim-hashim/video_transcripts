---
Date Generated: April 20, 2024
Transcription Model: whisper medium 20231117
Length: 4124s
Video Keywords: ['Education', 'Science', 'Technology']
Video Views: 1995
Video Rating: None
---

# BI 100.3 Can We Scale Up to AGI with Current Tech?
**Brain Inspired:** [March 17, 2021](https://www.youtube.com/watch?v=oj5kozyUYeM)
*  Given the continual surprising progress in AI powered by scaling up parameters and using
*  more compute while using fairly generic architectures, GPT-3, do you think the current trend of scaling
*  compute can lead to human level AGI?
*  And if not, what is the critical missing component?
*  Or do we need to do something fundamentally different?
*  Hi, this is Jeff Hawkins.
*  I'd like to congratulate Paul for the 100th episode of the Brain Inspired Podcast.
*  This is Brain Inspired.
*  So in fact, you're the role model of your podcast made me start a small podcast on my
*  own that's called Stimulating Brains because I really wanted to listen to a podcast about
*  deep brain stimulation, which is my field.
*  And there was none, at least I know none.
*  And I thought that's a niche.
*  So even now, on the website, it says that it's heavily inspired by your podcast and
*  the intro is similar because I just, you know, I try to learn from the best.
*  Thank you so much for doing that.
*  Oh, man, that is absurd and ridiculous.
*  But thank you, Andy.
*  It's also very kind.
*  I'm glad you've been inspired to start your own podcast.
*  So everyone go check out Andy's Stimulating Brains podcast.
*  It's at StimulatingBrains.org where they discuss shocking topics.
*  Oh, it's sad really.
*  I just I'm sorry I couldn't help it there.
*  All right, welcome to the third part of the 100th Brain Inspired episode, Bonanza.
*  I'm Paul, as always.
*  Here is today's question that many of my previous guests answered.
*  Given the continual surprising progress in AI powered by scaling up parameters and using
*  more compute while using fairly generic architectures like GPT-3, do you think the current trend
*  of scaling compute can lead to human level AGI?
*  If not, what's missing?
*  All right, so there's the question for this episode.
*  My terse response here is no, we cannot scale using what we have right now.
*  Largely, that's because I, like a few other people who respond in this episode, reject
*  the premise of the question.
*  AGI, what is that?
*  Human level AGI, what is that?
*  I reject the premise that that's a valid question.
*  So I would have probably not responded to this question, but I appreciate the thoughtfulness
*  with which many of my guests did here.
*  So I ran all the responses through a recurrent convolutional neural network to determine
*  the order this time.
*  Not really.
*  I randomly sorted the order in a spreadsheet.
*  Old school lazy style.
*  Like usual in the show notes, I list each person and link to their information.
*  So if you hear something you like, you can visit the show notes.
*  braininspired.co.
*  slash podcast slash 100 dash one, two, three, 100 dash three.
*  So we have a few more of these collections to go.
*  And I want again to just thank all of the previous guests who took the time and made
*  the effort to share their thoughts for this occasion.
*  Please do consider supporting Brain Inspired on Patreon if you're getting value out of
*  the show, which I hope you are.
*  It's really quite cheap to do and it's the sole source of income for the podcast.
*  I don't do advertising.
*  I hate advertising on podcasts.
*  I understand it, but I hate it so I don't do it.
*  All right, away we go.
*  This is Wolfgang Maas from the Graz University of Technology in Austria.
*  If scaling up is not sufficient, what's the critical missing component now towards human
*  level AGI?
*  A lot of thought and research at the moment starts to work on generalisation, but this
*  much more drastic form of generalisation, where you give test examples which doesn't
*  come from the same distribution at the training example.
*  And so they usually know one refers to this as OOD generalisation.
*  And I think we know very little about the way how the brain achieves this OOD generalisation.
*  It's kind of an anecdotal insight there, but we don't really know how the prior knowledge
*  is really encoded in neural networks of the brain and how this prior knowledge is used
*  to support abstract reasoning then.
*  And this problem is related to the previous problem that I have sketched, namely, taking
*  into account that the brain has been shaped by evolution.
*  So therefore, I don't expect that we find there really a clean theory how the brain
*  has managed to use prior knowledge to achieve OOD generalisation.
*  And it may be a bunch of hacks which altogether work quite well then.
*  This is Paul Humphries at the University of Virginia.
*  I want to address the question of whether scaling up contemporary computational devices
*  will enable us to reach artificial general intelligence.
*  But I want to address this question from a slightly different perspective, in part by
*  not looking at purely cognitive abilities, but emotional intelligence, and in particular
*  the faculty of empathy, which is one of the ways in which human beings can bond with one
*  another.
*  Imagine that you are in a situation where your mother has just died, or your favourite
*  pet has just died, and somebody, or an artificial device, is attempting to convey their sympathies
*  to you.
*  There is a major difference between a person who is pretending to be empathetic and someone
*  who is genuinely feeling the suffering that you are undergoing.
*  Now there is little doubt that artificial devices will be, and in fact in some areas
*  already have been, developed in order to behaviourally mimic the ways in which human
*  beings interact with one another in these kinds of circumstances.
*  But here the question is whether this would lead to a devaluation of human expectations
*  about how to behave in those kinds of circumstances.
*  We're all familiar with the ways in which people's modes of argumentation over social
*  media have changed in the last couple of decades, and it has spilled over from the artificial
*  domain into our interactions in real life.
*  So suppose that we became habituated to artificial nurses in a hospital trying to console us
*  after the death of our mother.
*  We would not have the kind of expectations that we now do for real nurses, who in general
*  are very highly skilled at conveying their sympathy with us in similar situations, perhaps
*  because their own mother has died.
*  So we become inured to a behavioural approach, which is very different from what our current
*  expectations are.
*  And then we would transfer this, perhaps, to our expectations from other human beings,
*  as well as from robots.
*  Now this may or may not be a good thing, but it will be different, because we currently
*  draw a sharp distinction between ordinary human beings and con artists, whether the
*  con artists are swindling old people out of their life savings by pretending to be sympathetic
*  with the elderly person's situation, or in an extreme case, of serial killers.
*  If there's no distinction between pretending to be empathetic with other people and actually
*  having the genuine internal emotions, we will have moved into a different kind of moral
*  territory than we now inhabit.
*  And I'll leave it up to you to assess for yourselves whether you think that this is
*  a good or a bad situation.
*  Chris Eliasmith.
*  I'm grouping these two together because I think they share an answer.
*  Recall that the questions are, what is the most important disagreement in the field and
*  what is the right direction?
*  Do you think scaling with more parameters and generic architectures is going to lead
*  to human-level intelligence?
*  One important disagreement in the field is whether we need to worry about cognitive-level
*  processing and representation or not.
*  I think Gary Marcus and Jan Lecuna had something of a Twitter war over this one, actually.
*  I think models like GPT-3 assume we don't.
*  If we have enough data, we can learn everything we need to from that data, including what
*  we might have thought of as conceptual organization, cognitive manipulation of concepts, how to
*  usefully model the world and all the relations in the world, and all this kind of stuff.
*  So all that higher-level cognitive stuff.
*  At a more abstract level, you can actually think of this as a version of the question
*  of whether we should allow ourselves to impose structure on the networks and representations
*  in our models.
*  This is a version of the same question because including concept-like representations is
*  exactly an example of imposing structure on the network and representations.
*  I'm actually a big believer in the importance of this approach for several reasons.
*  One, it's resulted in huge successes in the past.
*  Just look at convolutional neural networks.
*  This is the workhorse of deep AI.
*  Those networks adopt the structure we find in the biological visual system.
*  Another reason is that nature has had billions of years to determine the right network structures
*  and representations that we start with when we're born.
*  So the amount of computation that that evolution represents is kind of unfathomable.
*  So we're going to have to jump the queue, if you will, in some way.
*  And I'm guessing that the insights we get from neuroscience and psychology and good
*  old ingenuity are the kinds of things that are going to let us not pay that huge computational
*  cost.
*  And finally, I come back again to the spawn model.
*  There's really no way we could have started with a blank slate or a really deep generic
*  network and just train that network in order to give us a model that does what spawn does.
*  Of course, because spawn is a neural network, we can actually back prop through the final
*  version after we built it and we can optimize.
*  But to get it to that original functionality using just back prop and a lot of data just
*  isn't a viable option for several reasons.
*  One is that models like GPT-3 cost about $12 million to train their 175 million parameters.
*  If we assume that we can just scale by parameter count, spawn would still cost $1.4 million
*  to train and frankly, my lab doesn't have that kind of budget.
*  Second, this approach assumes that there is a data set that covers the 12 different tasks
*  that spawn can do.
*  Unlike the terabytes of data on natural language, getting that kind of data for intelligence
*  tests and copy drawing and so on just isn't possible.
*  And third, we haven't seen much evidence that we can train a single model that does a lot
*  of different tasks, tasks like vision, motor control, reinforcement learning, intelligence
*  tests and so on with this one big data set.
*  This kind of heterogeneity of task is really foreign to most neural network models.
*  So ultimately, I think that my arguments speak to a need for integrating methods, not that
*  one's better than the other.
*  I love deep learning.
*  My lab uses it all the time.
*  But we also use concept like representations and we impose the structure of the mammalian
*  brain on lots of our models.
*  In some ways, the approach I'm suggesting here is what I argued for in my book, How
*  to Build a Brain.
*  It's an integrated hybrid approach to building systems that can achieve biological cognition.
*  Andrew Sacks here.
*  Given the continual surprising progress in AI powered by scaling up parameters and using
*  more compute while using fairly generic architectures, do you think the current trend of scaling
*  can lead to human level AGI?
*  I think the answer depends on how precisely you read the question.
*  So that sometimes these questions can be can lead to apparent disagreement because people
*  interpret them slightly differently.
*  So if you take the question very literally, i.e. whether scaling compute can in principle
*  lead to AGI, then I think you'd have to answer yes, because if you are allowed to train different
*  subparts of the network in different ways and craft a very particular data set, then
*  almost no matter what the end answer looks like, what sort of principles we end up embodying
*  in a system that has AGI, you could probably train some network with some carefully set
*  up tasks such that it would do a reasonable job approximating that.
*  But I don't think that's what most people mean.
*  So the other interpretation is whether a standard deep network with today's architectures
*  and learning rules and data sets that's just a billion times bigger is going to become
*  conscious and clever.
*  And there I think the answer is no.
*  Today's systems are taking really fantastic steps toward that goal.
*  But they're only steps and I think we have new concepts waiting to be found and discovered.
*  What's the missing critical component?
*  And how could we get these components fastest?
*  Well for starters, if I knew I would be writing a paper right now, so I don't.
*  But I think one important aspect is the richness of environmental feedback.
*  So GPT-3 is trained on all of Wikipedia, but still it doesn't get the targeted instruction
*  that humans get.
*  And I think that dialogue with an intelligent agent who wants you, who's in a teacher-student
*  relationship where the teacher is self-emboweled with incredible intelligence, all of our social
*  learning abilities, these things give us I think a much better way of learning rather
*  than the current approaches that rely on Wikipedia.
*  And I also think part of the trouble is that we don't know what we don't know.
*  So in terms of practical ways, how do you discover what you don't know most quickly?
*  I think the answer is to strengthen our prospects for scientific discovery.
*  And when it comes to neuroscience, we've developed a huge range of new methods that are exceptionally
*  exciting, from neural pixel probes recording thousands of neurons to calcium imaging and
*  optogenetics.
*  And my personal view is the epoch of methods development has already occurred to the extent
*  that now we can go after the real scientific questions we're interested in.
*  We can start to extract general principles, we can design experiments where animals are
*  doing relatively complicated tasks.
*  And the data coming back from those experiments, I'm sure will surprise us and lead us towards
*  what we don't know.
*  Yeah, the scaling up issue, it's like a betting issue, like where would you predict things
*  will go in the future?
*  My bet is that you can't scale up to get human level AGI.
*  I think expert systems, including the really powerful neural network architectures that
*  you have today, like GPT-3, are really, really good and impressive at doing the things that
*  they've been trained to do.
*  And I think a lot of their power and effectiveness comes about because they're honing in on just
*  particular functional or behavioral similarities that hold between the artificial system and
*  the brain.
*  But to get to human level AGI, there's going to have to be a whole lot more going on that
*  is just not captured by a system which just relates to or corresponds to a biological
*  brain in just some narrow respect.
*  So that's my reason for thinking that just making bigger and bigger technologies of the
*  same sort that we have today will not lead to AGI.
*  So the critical missing components would be like all the biology?
*  All the biology, all the biology, but I think some of the biology, I mean, the context that
*  the brain has as one organ in a body plus that it's made of metabolizing tissue, I think
*  that's going to be part of the story about how biological intelligence is general.
*  And those are just not part of the technology that is there today.
*  This is Steve Potter from the Georgia Tech Laboratory for Neuroengineering.
*  Okay, so we're talking about components for general AI and human level AI.
*  I would say that AI experts are obsessed with using some technology that's completely
*  incongruent, which is digital computation, to emulate a completely analog thing, the
*  brain.
*  Yes, perhaps by using a large warehouse full of servers and an entire hydroelectric power
*  plant we may eventually see human level AGI on a digital platform.
*  But compared to a living brain that uses less than 100 watts will be an embarrassing
*  accomplishment.
*  We ought to be trying to model and emulate the brain using a substrate that is more like
*  it.
*  In other words, analog, not digital.
*  Blake Richards.
*  No, scaling up by itself is probably not going to provide the entirety of the answer.
*  However, I will add the following caveat.
*  We've seen a remarkable amount of progress simply by scaling up.
*  Furthermore, there are two ways in which there's still a lot of space to scale up to what brains
*  actually do.
*  The first is just the size of the networks.
*  Everyone knows that the networks are getting bigger and bigger, but they're still not at
*  the same scale as real brains.
*  So we still have a ways to go before we can say that scaling up to the size of real brains
*  wouldn't necessarily give us additional advantages.
*  The other, and in my opinion even more important question of scaling up, is the question of
*  scaling up the datasets.
*  And by this I don't mean the amount of labeled data or something like that.
*  What I mean is literally the richness of the data.
*  If we consider something like GPT-3, it's just been raised on a corpus of text.
*  That is the entirety of its experience of the world.
*  In contrast, human beings are raised in this incredible multimodal world where they can
*  act and they get things from five different senses and there's a coherence to everything
*  and rules of physics that apply to all of the interactions, etc.
*  And then they learn a language on top of this experience.
*  So my personal guess is that we can also achieve a lot simply by giving our AI more naturalistic
*  experiences of real rich multimodal worlds where they can act in them.
*  And this is actually a really hard problem for scaling up because honestly, to properly
*  simulate the kind of experiences that say a human baby has would be an immense amount
*  of computational resources.
*  So there's a lot of scaling up we can still do.
*  Now as I said at the beginning though, do I think that'll take us all of the way there?
*  No, likely not.
*  And I think the reason is that we have seen time and time again in AI that picking the
*  right inductive biases is incredibly helpful for solving a problem.
*  And my guess is that the inductive biases that exist in, as you put it, fairly generic
*  architectures like GPT-3 are not suitable for inducing the sort of understandings of
*  the world and representations that the human brain likely has.
*  Basically, I suspect we have additional inductive biases that are missing from these systems.
*  And I suspect those inductive biases will be key to getting the scaling up to give us
*  something more like human intelligence.
*  But I think the scaling up is also probably critical and will buy us a surprising amount.
*  This is Paul Cizek from the University of Montreal.
*  So I think no.
*  I absolutely don't think that scaling up just by adding more computational power is going
*  to get us to human level AI.
*  And I think for the most part, really, we're still just solving the same kinds of problems
*  that we've already been solving for a long time, essentially mapping problems between,
*  again, input and output, an image and a labeled model.
*  And I think the kinds of problems where we've succeeded are ones where success is defined
*  as meeting the criteria of some external supervisor, like a program.
*  And I don't think scaling that up is actually going to go beyond.
*  If we scale up our performance on other mapping problems, that's great, but I don't think
*  it's going to lead to human-like AI because that would actually have to be its own supervisor.
*  And I'm not talking about unsupervised learning.
*  Of course, there's many systems for doing unsupervised learning.
*  What I mean is we have to build systems that discover and define their own criteria for
*  what success in the task is.
*  And I think that is not being done, and that won't be suddenly achieved if you just scale
*  up.
*  Although I do think that research on reinforcement learning is quite promising because I think
*  that is probably the path towards systems that do discover and define what success is.
*  But I think the missing component, when you ask about the missing component, I think it's
*  the same one that's always been missing, and that's the question of meaning.
*  So good old fashioned AI systems failed to achieve human-like AI because they were really
*  just about syntactic rules for manipulating symbols, et cetera, without any regard for
*  the semantics that those symbols are supposed to capture.
*  And then neural networks replaced symbols with vectors and if-then rules with matrices.
*  But the tasks that they solve are still essentially syntactic and not semantic.
*  So they don't capture meaning.
*  And many people have been saying this for decades.
*  John Searle said this in the 80s, Stephen Harnad in the 90s.
*  Now Melanie Mitchell and many others are saying the same thing.
*  And as far as I can tell, everyone who has actually thought about this issue pretty much
*  agrees that this is a central issue that we need to address and that current approaches,
*  scaling up current approaches doesn't really address it.
*  But I think the problem is a lot of people are just not thinking about these issues because
*  they're kind of philosophical issues and maybe they feel like their time is better spent
*  to just try to build the next system that impresses us with its superhuman performance
*  and some new mapping task.
*  I think recent advances have made a lot of people think that we're making such great
*  progress that the big insights are just around the corner and we're all in a race to get
*  there as fast as we can.
*  But I don't think that's the case.
*  I think we're smashing ourselves into the mountain as fast as we can instead of trying
*  to hike up around it, sort of find the path around it that's more slow.
*  I think a lot of people by ignoring the deep philosophical gaps are, which I think most
*  people acknowledge that consider them and including many of the people that work in
*  AI, they acknowledge them.
*  But I think apart from that, there's just so much excitement that people rush into the
*  field with that they think, well, you know, I'm going to make the next bigger system and
*  that will be intelligent.
*  I don't think that's the case.
*  Of course, I could be wrong.
*  This is Brad Love from UCL.
*  All right, next question.
*  Given the continual surprising progress in AI powered by scaling up parameters and using
*  more compute while using fairly generic architectures, e.g. GPT-3, do you think the current trend
*  of scaling compute can lead to human level AGI?
*  I don't think just more of the same is going to give you AGI, for example.
*  When you train more and more on the same type of data or just make the same kind of architecture
*  bigger and bigger, I mean, diminishing returns are going to kick in.
*  And if you take GPT-3 as an example, I mean, it's amazing.
*  What it's really doing is just passively observing a lot of text.
*  It's building really interesting embedding spaces.
*  And I think it's showing systematic understandings of domains.
*  I mean, I'd like to test it more.
*  But you know, it's not a reasoning system.
*  It's not even really about language understanding.
*  It's a language model.
*  And it's a quite good one, at least from my perspective, from an information theory perspective,
*  because you know what I would want my language model to do is reduce my uncertainty and
*  you know, which word is going to come next.
*  And GPT-3 is great at that.
*  So I guess, you know, what are we going to need to actually get to the human level AGI?
*  I mean, I think one problem with things like GPT-3 is it's really just learning in one modality, right?
*  It's just observing text.
*  And of course, first, there's this issue, too, of, you know, you need to be actively
*  engaged with your environment.
*  So you think of a child just playing and, you know, bouncing a ball around or banging
*  on their head.
*  Not only are they actively engaging in a complex environment, but they're getting multimodal
*  information.
*  You know, so just not a bunch of words coming, right?
*  They have visual feedback.
*  They have, you know, feedback from their arms or muscles, proprio sensory feedback.
*  And so I think when we have all these different kinds of embedding spaces we could build for
*  the same event and we could link them together, then we can get a richer understanding of
*  a domain.
*  So like Brett Rhodes, I have a paper in Nature Machine Intelligence on this and I think this
*  is really true.
*  So we can't really just make these systems that just do one thing, one modality, just
*  words or just images.
*  But I have to link these embedding spaces and, you know, people do that, but I think
*  that's a really promising direction.
*  I'm Jay McClellan.
*  I think that, of course, that general artificial intelligence is going to continue to benefit
*  from scaling up and that in some ways we have just a huge further distance to go in terms
*  of reaching the kind of scale that, you know, the parallel distributed processing systems
*  we have in our brain offers us.
*  At the same time, I don't really think that the kinds of agent models that are currently
*  exhibiting, you know, surprising successes are going to eventually solve the problem.
*  And, you know, if you read the GPT-3 paper, the authors seem to share that view.
*  They appreciate many of the limitations that their model has and they expressed considerable
*  hope and expectation that they continue to be able to make improvements, but not just
*  by scaling it up.
*  There were several interesting ideas in their paper about directions to go.
*  Looking for myself, you know, the way I've thought about this is what would it take to
*  create an artificial PhD student?
*  You know, somebody who could attend classes, take notes, think about the ideas that other
*  people have offered, start developing their own ideas about what they wanted to pursue
*  for their research and, you know, draw on the ideas of others and build a program and
*  direction for themselves and end up being somebody like, you know, Jeff Hinton or Dave
*  Rumelhart or even Einstein and Isaac Newton, right?
*  You know, these are all people who came into a field, saw where things were, saw things
*  that weren't kind of problems that weren't being solved, spent a lot of time thinking
*  hard about how to solve them and developing new ideas.
*  And I think that really requires, you know, developing more agency in our agents, right?
*  The agents we have now are reacting to environments.
*  Maybe they're engaged in certain kind of rollout like behaviors that start to start to look
*  like planning and thought in ways that are certainly interesting, but they don't have
*  any sense, as far as I can tell, of a long term ability to formulate a goal and to work
*  towards a goal.
*  And, you know, I think that as a general matter, AI isn't really quite as goal directed as
*  it should be.
*  Even, you know, reward maximization isn't the same as, you know, wanting to get gas
*  in your car.
*  I mean, wanting to get gas in your car is a very specific thing.
*  Which gas station is open, right?
*  If there's gas shortages, where are their lines and stuff like that?
*  So you're planning very much in a very specific problem space about a very specific goal.
*  And we need more of that.
*  And we need more of agents that can have these goals over very extended time periods.
*  And, you know, the way we do when we're planning a research agenda or something like that.
*  Let's take a quick little break and then we'll get back to the responses.
*  My name is Asiyem Oprad and I'm finishing my PhD, hopefully soon, in Norway, Oslo
*  University.
*  During my PhD, I was searching for how this computational models used for psychology.
*  And then I discovered James McClellan.
*  I mean, I haven't thought that he's a well known person.
*  So I was searching for him to get familiar with the talks.
*  And then I listened to your very inspired interview with him.
*  But the way he answered your questions and he kind of positive and he was really instructive.
*  The kind of question you had with him at the end was kind of made me courage to email him
*  and see if I can visit him.
*  I mean, I'm not sure if I email other guests of you.
*  Maybe they will respond me this way.
*  But he was really nice.
*  You know, some kind of discussions general like academia versus industry, these kind
*  of questions was also useful for me because I rethink about should I stay in academia
*  or should I go for industry?
*  And at some point, because I see there are many people who are doing really great jobs
*  and think, okay, you don't need to stay in academia.
*  You can go and do something else because there are many great scientists who are really
*  doing better jobs than you.
*  So.
*  Megan Peters, University of California, Irvine.
*  Given the continual surprising progress in AI, powered by scaling up parameters and using
*  more compute while using fairly generic architectures like GPT-3, do you think the current trend
*  of scaling compute can lead to human level AGI?
*  And if not, what is the critical missing component?
*  So my answer is no.
*  I don't think that more and more compute is going to solve the problem.
*  I don't think that it's going to magically create a breakthrough to just crank up how
*  much horsepower we throw at a particular problem.
*  I feel pretty strongly actually that there are some components to the system that we
*  don't really understand yet and that we don't understand how to build yet.
*  Gary Marcus actually talks about the need for what he calls a substrate to support the
*  capacities that are necessary for AGI, artificial general intelligence, to be able to flourish.
*  And not about just building an AI with certain capacities in a modular kind of way,
*  but building this substrate to support those capacities in their development.
*  So we can't just take like the compositionality module and the symbolic manipulation module
*  and the meta-learning module and like plug them in to make a super AI.
*  So I agree with him that that approach isn't going to help us build an AI that can actually
*  be truly intelligent.
*  Could actually learn how to maximally or optimally use these types of capacities that
*  we might give it in this modular sense.
*  And certainly that kind of modular approach is not going to allow an artificial intelligence
*  agent to work on developing these certain capacities beyond their original programming,
*  which I kind of think is one of the things that drives our own intelligence, the capacity
*  to kind of evolve to fluidly switch between these modules to decide when each module is
*  appropriate and how to best use it in that instance.
*  So I think I'm going to copy Gary here, at least in a small way, and at least in so far
*  as saying that we need something like what he calls this substrate.
*  We need to develop an infrastructure that can support an agent in selecting these modules
*  like we would, or even in developing new modules.
*  And I think that that's something that we probably can't get from just throwing more
*  and more and more compute at the problem with our existing toolkits.
*  Hi Paul, this is Dean Buonamano.
*  Given the continual surprising progress in AI powered by scaling up parameters and using
*  more compute while using fairly generic architectures such as GPT, do I think the current trend
*  of scaling compute can lead to human level AGI?
*  So GPT-3 is indeed spookily impressive, and in many ways it seems to have essentially
*  passed the Turing test.
*  But ultimately I think it's really an example of the amazing power, the astonishing power
*  of statistically guided mixing and matching of words and sentences.
*  So in some ways I find AlphaZero chess and AlphaZero Go more impressive because they've
*  clearly exceeded human performance, where GPT-3 is basically using human templates that it
*  was exposed to to mix and match words and sentences.
*  What I was particularly surprised about with GPT-3, at least as I understand it, is that
*  there's no recurrency, that is, its neural net is entirely feedforward.
*  This is of course in sharp contrast with the absurdly recurrent nature of the brain circuits,
*  which is the brain is littered with positive and negative feedback.
*  And I expect recurrency and feedback is critical for AGI or artificial general intelligence.
*  For example, it's hard for me to imagine how the GPT architecture would support coming
*  up with two hypotheses and mulling them over and then reporting which one it thinks is
*  best.
*  I don't think the GPT architecture supports that, so I don't think that type of architecture
*  will scale up to AGI.
*  Additionally, it should be said that your question is a bit of a trick question because
*  nobody really agrees to any fixed criteria or fixed definition of what AGI is.
*  Perhaps we'll know it when we see it or perhaps enough knowledge and statistics will pass as AGI.
*  I don't know.
*  But if we go back to the 50s and 60s, many people thought that beating a human at chess
*  should be taken as proof of human level reasoning and intelligence.
*  But we changed that bar pretty quickly in the 90s for obvious reasons.
*  So similarly, I think AGI will be a moving bar for a while.
*  Talia Konkel.
*  Given the continual surprising progress in AI powered by scaling up parameters and using
*  more compute while using fairly generic architectures, for example, GV3, do you think
*  the current trend of scaling compute can lead to human level AGI?
*  I guess I reject the premise.
*  I don't know what human level AGI is.
*  I think I may be a bit contrarian on this one.
*  I think we're good at a lot of different tasks, but also because we practice a lot of those tasks.
*  And I don't quite know what human level AGI is, so I can't answer that question.
*  I'm Steve Grossberg.
*  Given the continual surprising progress in AI powered by scaling up parameters and using more
*  compute, do you think the current trend of scaling compute can lead to human level AGI?
*  My previous reply leads me to say no if you're going to use deep learning, but yes,
*  if you use neural architectures such as the predictive art architecture that I published in
*  2018, and you can download from my webpage, site, site.edu.steveg.
*  This predictive art or part architecture includes multiple parts of the brain, each
*  carrying out functions that are enabled by their interactions with other brain regions.
*  Moreover, new and really revolutionary computational paradigms underlie our brain's
*  astonishing abilities to adapt to changing environmental challenges.
*  Two of the paradigms that I was lucky enough to introduce are called complementary computing,
*  which clarifies what is the nature of brain specialization. Why are there so many
*  brain regions doing parallel processing with multiple processing stages? And laminate computing,
*  which asks why are all neocortical circuits organized into characteristic layers?
*  I'm Nathaniel Dahn at Princeton University. I think it won't by itself. I think it's,
*  you know, I've been around long enough to see the pendulum swing a few times,
*  and I think it's obvious that it's going to swing back towards more understanding, more theory,
*  and not just sort of this engineering approach of like dumping more and more data in larger and
*  larger models and sort of tweaking things and seeing if you can get it to work. Like that's
*  worked surprisingly well, and that's great. But, you know, the last phase of this ended with
*  a lot of stuff that was more motivated by theory and kind of higher level understanding,
*  and that works too. And I think the most exciting stuff in AI already right now is stuff that's a
*  little more nuanced and involves a sort of mixture of sort of classic algebraic methods that do the
*  parts that are statistical methods that do the sort of parts that are amenable to that,
*  augmented by deep networks for other parts that are amenable to the things that are good for us.
*  So things like this sort of line of alpha go algorithms and networks, they sort of integrate
*  regular old research with sort of deep networks and sort of clever ways,
*  and I think that's where the future is.
*  Okay, so my name is Marcel van Gerven. I'm chair of the AI department at the Donders Institute.
*  Yes, but there are some problems we need to fix first. So scaling up, yes, that's very nice,
*  but we also live on a planet with finite resources. So if we do the scaling up,
*  we definitely need to make things more efficient, right? So scaling up is a very important
*  thing. And I think that's a very important thing. And I think that's a very important thing.
*  So we need to make things more efficient, right? So I think that will be a big undertaking.
*  And that's also what you see happening. People moving away from using large GPU clusters to
*  doing computing on the edge or embedded systems, working on tiny ML solutions. So really trying
*  to think about, okay, how do we do this in a much more efficient manner? And then working on tiny
*  ML systems. That's one way to do it. Working on neuromorphic computing solutions, which really
*  embrace event-driven computation. That's another solution. But I think in the end, it's also
*  about bringing computation closer to physics. So there's also quite a bit of work in material
*  science that is trying to make materials ready for computation. So really trying to do things
*  at a more atomic scale. And that will definitely help us improve the efficiency of current AI systems.
*  Kanaka Rajan, before I get into the answer, right? These advances, GPT, for instance,
*  are astounding at what they're trained to do. So let me state that upfront. Just the claim that
*  they're going to do all of the things magically bugs me because it sounds almost like a religious
*  stance. So that said, do I think that the current trend of scaling compute, without changing
*  anything about fundamentally the architectures or anything can lead to human-level AGI? Plainly
*  speaking, no. Scaling compute is basically the same as algorithmically as filling a giant lookup
*  table brute force. So someone recently quipped that it's like trying to go to the moon by building
*  a tall ladder. So I may have somewhat heretic views on this compared to the traditional ML bro
*  you talk to. But first, yes, humans, we do a lot of things, some at the same time even, but none of
*  them perfectly well. So computers, even calculators have been outperforming us for decades now. So
*  there's that. But the key is that they're do so in specific things. The second thing I take issue
*  with is the exceptionalism implied by such a goal statement. That is that human brains are somewhat
*  this mystical magical pinnacle of generalized intelligence. Yes, we're smart. Yes, we have Bach
*  and math and music, but other nervous systems, including not even strictly embodied ones like
*  cephalopods do a number of pretty amazing things behaviorally. And all of the biology that got
*  there got there through evolution and through very different wetware. So let's at least pretend to
*  care about these aspects of biology. I mean, you know, it is brain inspired. So what is the critical
*  component, right? So that's the second part of this question. And what are practical ways to obtain
*  such components? I think understanding that's one. So does GPT truly understand what it's generating
*  responses to compositionality is another aspect that comes to mind. So even, you know, I hesitate
*  to even use the word proto languages, but something like whale song, right, contains elements of
*  compositionality. So I think that the fundamentally noteworthy theoretical advances towards
*  understanding cognition and then eventually emulating that in, you know, human level AGI systems
*  will come from novel network models that incorporate more biological architectures like,
*  you know, feedback, recurrent connections, multi-region interactions, complex neuron like units,
*  time varying patterns of activity dynamics, and then partly or fully unsupervised learning
*  algorithms. So practically, I say talk to neuroscientists, or generally scientists whose
*  models are geared towards being able to understand how biological wetware implements the functionality
*  one is interested in.
*  John Crackhour This is John Crackhour. No. I think, you know, it's interesting. Take, for example,
*  you mentioned GPT-3, right? So there are people who will say that if you look at the paper,
*  unusual things begin to happen at some scale, like suddenly it can add numbers when in fact
*  it was never given, you know, addition to learn overtly. So there are emergent properties
*  that seem to come along just by scaling up as you were. But no, I mean,
*  it does sentence completion. That's what GPT-3 does. It doesn't understand things.
*  And the idea that suddenly just doing more of the same understanding will suddenly just pop out,
*  I feel is a very, very odd stance to take. I don't think it's clear how to proceed. There
*  are some people in AI who believe that you have to come up with some more circumscribed definition
*  of thinking. And one idea might be that thinking is the ability to fill in the incomplete knowledge
*  that is right in front of you. In other words, the world in front of you with knowledge that
*  you bring from your past. And somehow you can use that knowledge to dictate your behavior
*  at that moment. And thinking is this ability to bring knowledge to bear on your current context.
*  Now, whether there's going to be an ability to formalize that and to define it in such a way
*  that you can get traction on it is unclear to me. I think there are people in AI who know this and
*  realize it's a problem. How do you bring declarative overt knowledge and facts to bear on decision
*  making and do it in a way that you're not just going back through the back door to GoFi and
*  programming it all in? But people like Gary Marcus, I think, will say,
*  just accept that you're going to have to do both. Find a way to program in knowledge and then have
*  that used in some way by deep neural nets in combination. But it's a complete crapshoot right
*  now. So I'm Rodrigo Quenquiroga, neuroscientist at the University of Leicester. I think we're still
*  not looking deep enough to how the human brain actually works. And this is my research. I record
*  the human brain and I see some principles which are kind of like on the atympos of what we see in
*  AI. And if I keep it at a very simple level, in principle, a computer will just replicate
*  information. But the computer does not have understanding. And I think a basic principle
*  of how the human brain works, and particularly for memory, which is what I study, is that we
*  forget a lot. I mean, there's a lot of information that we don't even process. So we're very good at
*  extracting very little information that we consider to be meaningful, and extract some sense from this
*  information and then working with this sense that we extract from that. So this process of abstraction,
*  of going to the higher level, thinking, if you like, involves a lot of forgetting because in
*  order to abstract something, you have to forget details. You have to leave details aside.
*  And of course, this leads to a lot of errors because the moment you forget details, there are
*  things that you will miss. And I think the way we have been developing computers and general machines
*  is not to have these errors and therefore not to forget things. But I think this is the opposite
*  to what you want to try to see if you can somehow replicate human level intelligence,
*  which is based on forgetting a lot of information and developing common sense in order to do the
*  things that I'd say before, general intelligence, the ability of abstraction, making inferences, and so on.
*  Grace Lindsay, I am a postdoc at the Gatsby Computational Neuroscience Unit at University
*  College London. Do you think the current trend of scaling compute can lead to human level AGI?
*  I'm going to include scaling data in this as well because GPT-3 uses a lot of data as well.
*  Probably not. No, I don't think so. Especially if you want to mimic human learning. Maybe you
*  can get an artificial agent that's very good at a task by giving it 8,000 human years
*  worth of time to learn and all of the written language in the world or something like that.
*  That's not what humans have. That's not what they use to get to human level intelligence.
*  So certainly if you want to replicate the learning of humans, I don't think that
*  just more compute and more data is going to be the answer. Also, I guess there's probably something to
*  be the versatility of humans and the fact that it's a single brain doing so many vastly different
*  tasks from motor control to sensory processing to planning and all those kinds of things.
*  I don't even know what more data would mean in that circumstance. You'd have to have all data
*  about everything. So yeah, I don't think just pure scaling is going to be the ticket to human level
*  AGI. I think it'll get you far on certain specific tasks. Like a single agent doing a single task,
*  you can probably get pretty far just by scaling with compute and data. But an integrated agent
*  that learns at the rate of humans probably needs something else in terms of advances in learning
*  algorithms or architectural modules. Honestly, I have no idea because if I knew, I would publish it.
*  My name is Conrad Kalning. I'm a neuroscientist at the University of Pennsylvania.
*  Given the continual surprising progress in AI powered by scaling up parameters and using more
*  compute while using fairly generic architectures, e.g. GPT-3, do you think the current trend of
*  scaling compute can lead to human level AGI? I absolutely don't think that. So if we look at GPT-3,
*  it's in a way a really big lookup table. Not like it's been trained with everything that humans have
*  ever said or at least a significant subsection of that. So in that sense, the answer to most
*  question is already in that data set. But a lookup table isn't intelligence. The thing that makes us
*  be so impressive in a way at solving difficult tasks is that we can deal with new situations
*  based on very, very limited amount of data. GPT-3 cannot do that. And it's not that there's something
*  wrong about GPT-3. It is just that GPT-3 completely lacks what makes human intelligence so interesting.
*  It cannot argue internally. It cannot tell stories. And in fact, the thing that makes
*  texts written by GPT-3 so sad is that it completely gets story wrong. So if the question is what are
*  the critical missing components, I think if you ask me, the critical missing component is the ability
*  to tell stories. Let me highlight why I think this is so important. The world around us, the world that
*  we construct with our friends and families, is very much a world of stories where we talk about
*  high-level phenomena and we, over very short periods of time, maybe a few minutes, go through
*  something that might otherwise take a really long period of time. And I think one of the things that
*  we're critically missing is machine learning systems that really tell themselves stories.
*  This is Jeff Hawkins. Can we get to human-level AI by scaling our current systems? Or do we need
*  to do something fundamentally different? As long as I've been interested in the field of intelligence,
*  I have believed that we first have to study the brain. You have to study the brain to not only
*  learn what mechanisms are used in the brain to create intelligence, but to learn what intelligence
*  itself is. For I don't believe we have a good working definition of intelligence. Today's AI
*  is focused primarily on solving particular tasks using particular techniques that seem to work,
*  but without a fundamental understanding what intelligence is. Here's what we've learned
*  by studying the brain. We've learned that intelligence is based on the ability of our brain
*  to learn a model of the world. We have a tremendous amount of knowledge about the world in our heads
*  and it's stored in a model. The model tells us what things look like, what they feel like,
*  what they sound like. The model tells us where things are relative to each other and the model
*  tells us how things change as we manipulate them and as we move about the world. It is the model
*  that is the basis of intelligence. Not any specific tasks that we might accomplish,
*  but a model allows us to accomplish almost any task. We can ask, given our model of the world,
*  how we might achieve a certain result or what will happen if we take certain actions.
*  We've learned a lot about how the brain learns a model of the world. I'm going to just talk about
*  three high-level components. The first is we learn through movement. Do we learn a model of the world
*  we don't just sit statically and look at images. We move our bodies, we move our eyes constantly,
*  we move our fingers, we pick things up, we poke things, we hear, we touch something and see what
*  it sounds like, see what it feels like. We push things, we try things out. We have to move to the
*  world, we have to move our senses to the world, we have to interact with the world to learn.
*  This is almost completely lacking from today's AI. The second component of how the brain learns
*  a model of the world is how it stores information. How does it store knowledge? What we've learned is
*  that the brain stores knowledge using reference frames. You can think of a reference frame like
*  a Cartesian coordinates x, y, and z. It's a structure that is a metric structure, meaning
*  the brain assigns knowledge to it. So when it wants to learn something, it assigns
*  the sensory inputs to locations in a reference frame. In the brain, these are implemented by
*  types of cells called grid cells and place cells. And we've figured out that grid cells and place
*  cells exist throughout the entire neocortex, which is the organ of intelligence. It's essential to have
*  a way of representing knowledge. And reference frames are the way that brains do this. Today's
*  AI do not have anything equivalent to this. And finally, the third big topic I want to talk about,
*  what we've learned about brains, is knowledge in the brain is distributed in a certain way.
*  If I ask you, where is knowledge about a cell phone in your brain? It is not in one location.
*  It is not in two locations. It turns out that in your brain, specifically the neocortex,
*  you have thousands of models of your cell phone. They are complementary models. There's models of
*  what your cell phone looks like. There are models of what your cell phone feels like. There are
*  models of how it, the sounds it makes. And so knowledge is distributed across many, many models
*  in the human neocortex. There's about 150,000 cortical columns. Each one is a modeling system.
*  So it's a highly distributed system. And I believe that you have to build intelligent
*  systems this way too. It allows the flexibility of integrating different modalities, such as hearing,
*  touch and vision. It allows the system to deal with ambiguity, because some models will be
*  understanding some parts of the world, others will be understanding other parts of the world.
*  And it allows us to build intelligent machines in different embodiments that can have all types
*  of different shapes. They don't have to look like humans. So again, the three things I'm arguing
*  that are missing in today's AI, three huge things are learning through movement, storing knowledge
*  in reference frames, and a highly distributed modeling system. We've written a series of
*  neuroscience papers about this, and we call the overall theory the thousand brains theory.
*  And we've just started taking these ideas and implementing them in machine learning and AI.
*  We think that is the future. I've also written a book that's coming out in March called
*  A Thousand Brains, which will cover this topic, both neuroscience and an AI point of view.
*  Thank you for listening.
*  Uri Hassan from Pritzson University. So I think that the scaling in AI is miniscule relative to the brain.
*  You know, GPT-3 has 150 billion parameters. That sounds like crazy. If I'm taking two boxes in the
*  brain, six millimeter of cortex, I have more synapses than GPT-3. So the scaling in the brain
*  is way larger than what we see in machine learning. So I've never like scaled about this scaling
*  argument. Saying that, I think that the foundations of the brain and the foundation of machine
*  learning are very similar. The brain is also using a direct fit and blind supervision
*  in a closed loop dynamical system to fit to the world without understanding.
*  So I see deep connections between the brain and this machine learning.
*  But this is only the starting point. It will be a mistake to say that machine learning have any
*  intelligence, but human do. So while I think that the foundations are very similar between human
*  and machine, something is really missing in machine, but we have in humans, right?
*  We can understand physics. We can understand rules. We can have calculus. We can go to the moon.
*  We can think. These machines do not think. So I think that while the foundation is similar,
*  the next stage in AI and in cognitive neuroscience is to understand our form. It's like blind fit and
*  other parameterization and memorization and interpolation and fitting. You can go to the
*  next stage of extracted information about the world by taking the shape of the world. So that's
*  something that we will need to think about. And it's really missing in machine learning.
*  My name is Jessica Hamrick. Do I think the current trend of scaling compute can lead to
*  human level AGI? No. I think that while of course the recent advances are very impressive
*  in terms of showing what we can do as we do continue to scale up compute and data, I think
*  having a generally intelligent system requires both lots of experience and being able to choose
*  the experience that it performs on the world. So you need systems that are able to perform
*  interventions on the world and learn from those interventions. I think just treating the problem
*  as a supervised learning or prediction problem isn't going to get us there. But then when it
*  comes to actually scaling up the compute and data for actually performing interventions, I think that
*  this is really hard and not something that is easy to do. We can train, for example, agents
*  in simulation, which of course can learn to interact with the world. But these simulations
*  that we can develop are really impoverished compared to the real world, especially if you
*  compare to the millions of years of evolution and the vast complexity of the natural world.
*  There's so many different species, so much complexity in the real world that we've evolved
*  to learn to interact with. And that's not something that we are currently able to replicate in
*  simulation at all. And of course, we also don't want agents just running wild in the real world.
*  And so there's a real question of how is it going to be possible at all to really scale up
*  the data and compute to train these types of agents. And I think the answer is probably no.
*  And for that reason, I think that we need to adopt a hybrid approach where we look for what
*  are the most promising types of inductive biases or structures that we can build into agents
*  that we know evolution settled on as good solutions to the problem, figure out how to combine those
*  with powerful learning systems so that we don't have to replicate the entirety of evolution.
*  Because I think that that's basically what we were saying is what we would want to do is like
*  with enough data and compute, we can replicate evolution. But I just don't think that is really
*  a possibility. Thomas Nosolaris, Department of Neuroscience, University of Minnesota.
*  Do you think the current trend of scaling compute can lead to human level AGI? No, I don't.
*  Why? And what's missing? It's something like creativity, which I think requires very efficient
*  world modeling that would lend itself to testing hypotheses about causality and the consequences
*  of ones and other agents actions. I would guess that emotion is also an important part of AGI.
*  I'm just guessing. I know very little about that. But it's certainly salient and it certainly guides
*  a lot of the way that we think. And I'm not at all convinced that it's something that we can be smart
*  without. Emotion, that is. What are the practical ways to obtain such components fastest?
*  More funding for basic neuroscience.
*  Brain Inspired is a production of me and you. I don't do advertisements. You can support the
*  show through Patreon for a trifling amount and get access to the full versions of all the episodes
*  plus bonus episodes that focus more on the cultural side but still have science.
*  Go to braininspired.co and find the red Patreon button there. To get in touch with me,
*  email paul at braininspired.co. The music you hear is by The New Year. Find them at the newyear.net.
*  Thank you for your support. See you next time.
*  Let me into the snow
*  The covers of the past
*  They take me where I go
