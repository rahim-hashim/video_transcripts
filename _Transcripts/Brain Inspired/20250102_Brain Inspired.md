---
Date Generated: January 03, 2025
Transcription Model: whisper medium 20231117
Length: 5891s
Video Keywords: []
Video Views: 11
Video Rating: None
Video Description: Eli Sennesh on a biologically plausible predictive coding mechanism.

Show notes:  https://braininspired.co/podcast/202/

Patreon (full episodes and Discord community:  https://www.patreon.com/braininspired

Apple podcasts:  https://itunes.apple.com/us/podcast/brain-inspired/id1428880766?mt=2
Spotify:  https://open.spotify.com/show/2UZj8c8Ap5oc2gh2rJxLLe

The Transmitter is an online publication that aims to deliver useful information, insights and tools to build bridges across neuroscience and advance research. Visit thetransmitter.org to explore the latest neuroscience news and perspectives, written by journalists and scientists. 

Read more about our partnership: https://www.thetransmitter.org/partners/

Sign up for the “Brain Inspired” email alerts to be notified every time a new “Brain Inspired” episode is released: https://www.thetransmitter.org/newsletters/

To explore more neuroscience news and perspectives, visit thetransmitter.org.


Eli Sennesh is a postdoc at Vanderbilt University, one of my old stomping grounds, currently in the lab of Andre Bastos. Andre’s lab focuses on understanding brain dynamics within cortical circuits, particularly how communication between brain areas is coordinated in perception, cognition, and behavior. So Eli is busy doing work along those lines, as you'll hear more about. But the original impetus for having him on his recently published proposal for how predictive coding might be implemented in brains. So in that sense, this episode builds on the last episode with Rajesh Rao, where we discussed Raj's "active predictive coding" account of predictive coding.  As a super brief refresher, predictive coding is the proposal that the brain is constantly predicting what's about the happen, then stuff happens, and the brain uses the mismatch between its predictions and the actual stuff that's happening, to learn how to make better predictions moving forward. I refer you to the previous episode for more details. So Eli's account, along with his co-authors of course, which he calls "divide-and-conquer" predictive coding, uses a probabilistic approach in an attempt to account for how brains might implement predictive coding, and you'll learn more about that in our discussion. But we also talk quite a bit about the difference between practicing theoretical and experimental neuroscience, and Eli's experience moving into the experimental side from the theoretical side.

0:00 - Intro
3:59 - Eli's worldview
17:56 - NeuroAI is hard
24:38 - Prediction errors vs surprise
55:16 - Divide and conquer
1:13:24 - Challenges
1:18:44 - How to build AI
1:25:56 - Affect
1:31:55 - Abolish the value function
---

# BI 202 Eli Sennesh Divide-and-Conquer to Predict
**Brain Inspired:** [January 02, 2025](https://www.youtube.com/watch?v=Ex35cwt0Qos)
*  Why do things feel like stuff? [[00:00:00](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=0.0s)]
*  Why do we engage in the behaviors we behave in? [[00:00:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=7.0s)]
*  You know, not why in the normally scientific, you know, reductionist sense, what are the [[00:00:11](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=11.08s)]
*  mechanisms once we hold the behavior fixed, but instead, if we don't hold the behavior [[00:00:17](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=17.04s)]
*  fixed, what are you or any other organism going to choose and why that choice instead [[00:00:23](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=23.16s)]
*  of something else? [[00:00:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=29.28s)]
*  There's sort of this problem where in neuro, we are often doing paradigms or tasks that [[00:00:33](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=33.120000000000005s)]
*  from a pure AI point of view might be considered almost trivial, but from a biological plausibility [[00:00:40](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=40.24s)]
*  point of view, that often makes them hard again. [[00:00:47](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=47.52s)]
*  I had actually been prepared for the concept that you might walk arrogantly into experimentation [[00:00:53](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=53.2s)]
*  with some grand theory and think this is going to totally be right and do your first experiment [[00:01:00](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=60.120000000000005s)]
*  and it's totally wrong. [[00:01:06](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=66.72s)]
*  And in fact, that happened. [[00:01:08](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=68.56s)]
*  This is Brain Inspired, powered by the transmitter. [[00:01:18](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=78.0s)]
*  Good day to you. [[00:01:21](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=81.92s)]
*  I am Paul. [[00:01:23](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=83.84s)]
*  This is Brain Inspired podcast, as you just heard. [[00:01:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=84.84s)]
*  Eli Sinesh is a postdoc at Vanderbilt University, one of my old stomping grounds. [[00:01:28](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=88.04s)]
*  Eli is currently in the lab of Andre Bastos. [[00:01:34](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=94.4s)]
*  Andre's lab focuses on understanding brain dynamics within cortical circuits, particularly [[00:01:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=97.36s)]
*  how communication between brain areas is coordinated in things like perception and cognition and [[00:01:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=103.28s)]
*  behavior. [[00:01:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=110.32s)]
*  So Eli is busy doing work along those lines these days, as you'll hear more about in [[00:01:52](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=112.08s)]
*  a moment. [[00:01:57](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=117.16s)]
*  But the original impetus for having him on this podcast is his recently published proposal [[00:01:58](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=118.52s)]
*  for how predictive coding might be implemented in brains. [[00:02:03](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=123.83999999999999s)]
*  So in that sense, this episode builds on the last episode with Rajesh Rao, where we discussed [[00:02:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=127.83999999999999s)]
*  Raj's active predictive coding account of predictive coding. [[00:02:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=133.28s)]
*  I've said predictive coding multiple times now. [[00:02:17](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=137.48000000000002s)]
*  So as a super brief refresher, predictive coding is the proposal that the brain is constantly [[00:02:19](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=139.8s)]
*  predicting what's about to happen, then stuff happens, and the brain uses the mismatch between [[00:02:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=146.56s)]
*  its predictions and the actual stuff that's happening to then learn how to make better [[00:02:32](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=152.96s)]
*  predictions moving forward. [[00:02:38](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=158.88s)]
*  So I refer you to the previous episode for more gruesome details about that process. [[00:02:41](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=161.4s)]
*  Eli's account of predictive coding and how it might be implemented in brains, along with [[00:02:47](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=167.0s)]
*  his co-authors, of course, they call it quote unquote divide and conquer predictive coding, [[00:02:52](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=172.76s)]
*  and you'll hear why in our discussion. [[00:02:58](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=178.24s)]
*  The divide and conquer approach, among other things, uses a probabilistic approach to account [[00:03:00](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=180.72s)]
*  for how predictive coding might be implemented in brains. [[00:03:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=187.08s)]
*  We also talk quite a bit about the difference between practicing theoretical and experimental [[00:03:10](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=190.32s)]
*  neuroscience and Eli's experience moving into the experimental side from the theoretical [[00:03:19](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=199.28s)]
*  side, which, well, you'll hear. [[00:03:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=206.48s)]
*  It turns out everything has its own challenges, let's say. [[00:03:31](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=211.04s)]
*  All right, show notes for this episode, ourbraininspired.co. [[00:03:35](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=215.36s)]
*  slash podcast slash two hundred and two. [[00:03:38](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=218.12s)]
*  As always, thank you for being here. [[00:03:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=223.48s)]
*  Thank you for listening. [[00:03:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=224.92000000000002s)]
*  Thank you to the transmitter for your support of this podcast. [[00:03:46](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=226.26s)]
*  And thank you to the patrons who also reach out in support. [[00:03:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=230.4s)]
*  Here's Eli. [[00:03:54](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=234.84s)]
*  So Eli, are you ready? [[00:03:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=235.84s)]
*  Yes. [[00:04:00](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=240.04s)]
*  We were just chatting about how you are just a few floors up from where I did my postdoc. [[00:04:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=242.04s)]
*  And this is your first postdoc, right? [[00:04:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=247.32s)]
*  Yes. [[00:04:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=249.76s)]
*  Yeah. [[00:04:10](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=250.76s)]
*  And in Nashville, Tennessee. [[00:04:11](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=251.76s)]
*  And I'm curious. [[00:04:14](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=254.56s)]
*  So this is kind of a, in some sense, a follow up episode because I just had Rajesh Rao on [[00:04:16](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=256.36s)]
*  to talk about his active predictive coding work from which updates the original predictive [[00:04:22](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=262.2s)]
*  coding framework from 1999 that focused all on sensory. [[00:04:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=269.06s)]
*  And so what he did here was like basically bring in an action part of the story into [[00:04:36](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=276.09999999999997s)]
*  the predictive coding. [[00:04:41](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=281.62s)]
*  Yeah, it's very lucky timing. [[00:04:42](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=282.62s)]
*  We actually just read his APC paper in Journal Club. [[00:04:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=284.29999999999995s)]
*  Oh, really? [[00:04:47](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=287.78s)]
*  Two days ago. [[00:04:48](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=288.78s)]
*  Oh, we did it a few weeks ago. [[00:04:49](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=289.78s)]
*  Yeah, it was helpful. [[00:04:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=290.78s)]
*  And so we'll get to your related work, compare, contrast, etc. [[00:04:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=295.74s)]
*  But first, I kind of I know you have a computer science background and I kind of I'm trying [[00:05:00](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=300.68s)]
*  to understand your worldview. [[00:05:06](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=306.38s)]
*  People ask me what my worldview and I can't describe it because it assumes I have a worldview. [[00:05:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=309.1s)]
*  But like how you approach the world because I know you you have that background in computer [[00:05:15](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=315.96000000000004s)]
*  science. [[00:05:19](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=319.92s)]
*  And so that's there's kind of a computational I don't mean dry in a bad way, but a very [[00:05:20](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=320.92s)]
*  computational kind of algorithm centric approach that I thought, well, maybe that's kind of [[00:05:27](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=327.64000000000004s)]
*  where he's coming from. [[00:05:32](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=332.84000000000003s)]
*  But then I know you did some work with Lisa Feldman Barrett. [[00:05:33](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=333.84000000000003s)]
*  It's all about feelings and how that drives so much of what how we interact with the world. [[00:05:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=337.08000000000004s)]
*  So I'm just curious what your kind of worldview in the neurosciences. [[00:05:42](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=342.82s)]
*  Oh, man, he's getting comfortable. [[00:05:46](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=346.36s)]
*  Yes. [[00:05:49](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=349.8s)]
*  OK, so I mean, there is actually. [[00:05:51](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=351.76s)]
*  I don't know if I have a worldview, but I sort of have a direction and a vibe. [[00:05:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=355.6s)]
*  OK, oh, I like that. That's a good way to phrase it. [[00:05:59](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=359.44s)]
*  Like, I feel things out for what I think could be a workable scientific approach to try and [[00:06:01](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=361.52000000000004s)]
*  address the questions I'm sort of interested in. [[00:06:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=367.68s)]
*  And overall, I feel like the question I'm interested in is like. [[00:06:11](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=371.32s)]
*  This is going to sound even sillier than saying consciousness, I'm sorry to say, but why do [[00:06:17](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=377.92s)]
*  things feel like stuff? [[00:06:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=384.6s)]
*  Oh, why do we engage in the behaviors we behave in? [[00:06:27](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=387.0s)]
*  But why in the. [[00:06:32](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=392.16s)]
*  You know, not why in the normally scientific, you know, reductionist sense, what are the [[00:06:35](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=395.12s)]
*  mechanisms once we hold the behavior fixed? [[00:06:41](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=401.28000000000003s)]
*  But instead, if we don't hold the behavior fixed, what are you going, you know, what are [[00:06:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=404.96s)]
*  you or any other organism going to choose and why that choice instead of something else? [[00:06:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=410.03999999999996s)]
*  What does that mean? Hold the hold the behavior fixed. [[00:06:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=415.71999999999997s)]
*  Oh, OK, so now I just get to channel Lisa straight up. [[00:06:58](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=418.64s)]
*  You know, so often in neuroscience experiments and I'm thinking particularly actually, you [[00:07:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=422.79999999999995s)]
*  know, some of the animal experiments we do here at Vandy. [[00:07:08](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=428.36s)]
*  We basically, you know, head fix your animal, you know, like first you chair the animal [[00:07:12](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=432.24s)]
*  head fix, then you train them to fixate on a dot on screen. [[00:07:17](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=437.8s)]
*  Monkeys are much smarter than mice, so they can do this in exchange for juice. [[00:07:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=444.36s)]
*  Right. Yeah, I just described my entire academic career there. [[00:07:30](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=450.28000000000003s)]
*  Yeah, an increasing portion of mine. [[00:07:35](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=455.44s)]
*  And, you know, then you basically have them move their eyes as the only motor output of [[00:07:38](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=458.48s)]
*  whatever you're having them do. [[00:07:46](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=466.48s)]
*  So the most highly constrained lab experimental setup as you can so that when you're asking [[00:07:49](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=469.32s)]
*  the question, does, for example, frontal eye field do encode some sort of decision process [[00:07:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=475.36s)]
*  related to behavior? [[00:08:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=482.16s)]
*  You don't have to worry about all of the other conflating factors that are involved in the [[00:08:03](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=483.32000000000005s)]
*  other behaviors. [[00:08:08](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=488.68s)]
*  Yes. And so, you know, I'm very I'm a strong believer that experiment is theory laden. [[00:08:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=489.72s)]
*  And this means that if you're doing one of these highly constrained experiments and you [[00:08:17](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=497.64000000000004s)]
*  have a theory about what frontal eye field is doing great, you've controlled everything [[00:08:21](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=501.08000000000004s)]
*  else that you can test your theory about frontal eye field. [[00:08:25](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=505.88s)]
*  Now, back to channeling Lisa, you know, if what you're trying to investigate is not [[00:08:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=509.08s)]
*  something that you can leave, you know, something that you've left unconstrained in your [[00:08:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=517.88s)]
*  setup, then you can't actually test theories about it. [[00:08:42](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=522.36s)]
*  So I would say, for instance, you know, like, can you use a head fixed monkey in a chair to strongly [[00:08:47](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=527.3199999999999s)]
*  test, you know, these physiological theories about allostasis, interoception, these other nice [[00:08:54](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=534.76s)]
*  keywords that I wrote about with like Lisa and Karen? [[00:09:00](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=540.2s)]
*  Questionable. [[00:09:04](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=544.12s)]
*  That is really how I ended up. [[00:09:06](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=546.12s)]
*  Gosh, there's a whole story actually of how I ended up working with Lisa and Karen. [[00:09:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=549.08s)]
*  Yeah, let's hear it. If you're willing to divulge. [[00:09:14](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=554.28s)]
*  Yeah. [[00:09:16](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=556.52s)]
*  So the short answer is, you know, I think it's a good idea to do a little bit of research [[00:09:18](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=558.92s)]
*  on this. The short version is the stars aligned in a way that they never have before since. [[00:09:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=564.28s)]
*  Obviously, doesn't every academic have that story? [[00:09:30](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=570.92s)]
*  That's right. [[00:09:33](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=573.32s)]
*  But, you know, really, it's that I had this computer science background. [[00:09:34](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=574.12s)]
*  And then late in my master's, actually, I started getting interested in like cognitive science, [[00:09:41](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=581.32s)]
*  sort of very like Brendan Lake, Josh Tannenbaum type of stuff. [[00:09:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=590.52s)]
*  And so I had to like spend a couple years studying things to sort of go back and try [[00:09:56](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=596.52s)]
*  and give myself the background to engage with any of this, try to change direction. [[00:10:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=602.92s)]
*  And I discovered that like what I was really interested in was sort of the feeling and the [[00:10:08](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=608.68s)]
*  why. And so I started trying to figure out, well, okay, who has an approach that is kind [[00:10:12](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=612.5999999999999s)]
*  of like this to these subjects where the kind of like this is sort of, you know, very probabilistic. [[00:10:18](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=618.68s)]
*  You know, they were using like probabilistic programs to model concept learning. [[00:10:28](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=628.8399999999999s)]
*  And this was all working very nicely for them. You know, they had that science paper in 2015. [[00:10:33](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=633.4799999999999s)]
*  I was so impressed. You know, I started reading neuroscience. [[00:10:39](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=639.3199999999999s)]
*  And you decided to continue. [[00:10:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=644.3599999999999s)]
*  Oh, no, it just gets so much worse. I'm kind of embarrassing myself here. [[00:10:49](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=649.4799999999999s)]
*  You're going to you could probably guess now if someone's looking for probabilistic approaches [[00:10:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=655.4799999999999s)]
*  in neuroscience mid-20, mid to late 2010s, who are they running into? [[00:10:59](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=659.9599999999999s)]
*  Well, not many people, but you've already named some of them. But that I was about to ask you, [[00:11:04](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=664.8399999999999s)]
*  why probabilistic? Maybe we could start there. [[00:11:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=669.2399999999999s)]
*  Because I was just, I mean, at the time, I was an amateur and I was just sort of vibing and trying to [[00:11:12](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=672.28s)]
*  go, you know, trying to go from one thing that I felt like I could kind of understand to another [[00:11:17](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=677.4s)]
*  thing I felt like I could kind of understand. And, you know, at least partly at the beginning, [[00:11:22](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=682.36s)]
*  you know, the interesting part about the Tenenbaum and Lake kind of work to me was, hey, [[00:11:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=689.56s)]
*  unlike that old field of AI that I took a course in in undergrad, and hated it. [[00:11:35](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=695.8s)]
*  This is symbolic style, old field or connectionism old field? [[00:11:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=703.0s)]
*  I mean, I went to undergrad at UMass Amherst. So the AI class was symbolic, search, heuristics, [[00:11:47](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=707.4s)]
*  all of that logic, then the machine learning classes that I didn't take at the time, [[00:11:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=715.4s)]
*  were like random forests, SVMs. You know, I think there was some neural networks, [[00:12:01](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=721.4s)]
*  but they've hired a lot more people doing neural networks since then. [[00:12:06](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=726.28s)]
*  Hmm. [[00:12:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=729.08s)]
*  You know, and like to be, oh, RL, RL was absolutely huge at UMass Amherst, [[00:12:11](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=731.8000000000001s)]
*  to the point that they hosted the RL conference, you know, in Amherst this past year. And like, [[00:12:17](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=737.32s)]
*  I eventually realized like, oh, those big guys Sutton and Bart, wait, Bardo, like Andy Bardo, [[00:12:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=744.5200000000001s)]
*  who would just walk through the hall. Yeah, that Andy Bardo. [[00:12:30](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=750.44s)]
*  That's an isn't that a super interesting thing about academia when you meet, I don't know, you [[00:12:33](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=753.32s)]
*  know, maybe hero was the wrong word, but these sort of godheads of classic things, and then, [[00:12:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=757.64s)]
*  oh, they're just regular folks. I mean, that's why I say this is so embarrassing is like, [[00:12:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=763.8s)]
*  I actually went to school in a department full of such great people. [[00:12:49](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=769.08s)]
*  When you don't know, you don't know. Right. And I just, like, I was honestly kind of dismissive [[00:12:54](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=774.1999999999999s)]
*  about it because I was like, okay, this is all just, you know, heuristic search. Like, [[00:12:59](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=779.96s)]
*  it's heuristic search, you throw a lot of, you know, processing power at it. And maybe sometimes [[00:13:05](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=785.88s)]
*  it kind of works. But like, this is really like, this is not actually how a brain or a mind would [[00:13:11](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=791.96s)]
*  work. Like, you know, this isn't the real thing. And then when I started reading those like Ten [[00:13:19](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=799.96s)]
*  and Balmond Blake things, you know, they were saying, well, we fit to behavior. You know, [[00:13:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=806.2s)]
*  we've done an actual experiment and checked. So we're not just defining some toy task that we can [[00:13:30](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=810.84s)]
*  then solve computationally with reasonable ease and then go back and forth between approximations [[00:13:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=817.72s)]
*  and heuristics, you know, for the rest of our careers until an AI winter hits and wipes us out. [[00:13:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=824.36s)]
*  Gosh, maybe I did take something from you, Mass Amherst, actually. Like, maybe I took some [[00:13:52](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=832.9200000000001s)]
*  residual post-trauma from the AI winter. But yeah, they were fitting behavior and actually [[00:13:57](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=837.32s)]
*  fitting a wide variety or a reasonable variety of experimental tasks with human participants. [[00:14:05](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=845.4000000000001s)]
*  And I said, okay, now there's something here. Like, now there's a real world to compare against. [[00:14:12](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=852.5200000000001s)]
*  So then how did that take you to like, Lisa? [[00:14:21](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=861.4000000000001s)]
*  Oh, sorry. Yes. So I was trying to prompt you for the name Friston. [[00:14:25](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=865.24s)]
*  Yeah, sure. Carl Friston just to be. [[00:14:32](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=872.36s)]
*  Yes. Carl. Yeah. So actually via, let's see, I was working with [[00:14:36](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=876.44s)]
*  this embedded electronics company. I still have the hoodie over there. [[00:14:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=883.8s)]
*  And like they had an MIT postdoc. He mentioned some of the Carl Friston stuff around the same [[00:14:48](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=888.76s)]
*  time that Andy Clark's book came out. So that's surfing uncertainty? Is that the? [[00:14:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=895.8s)]
*  Yeah, that one. And that one had a lot of citations to people that I already, [[00:15:01](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=901.4799999999999s)]
*  you know, names I already recognized. So I read it. I went absolutely wild for it. [[00:15:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=907.3199999999999s)]
*  And he was sort of mentioning in the book, like, you know, there's some people who are [[00:15:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=913.4799999999999s)]
*  actually applying this approach to emotion. I see. [[00:15:17](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=917.08s)]
*  And even better, the people who were applying this approach to emotion, [[00:15:21](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=921.72s)]
*  you know, Lisa and Karen, or at least Lisa and Karen locally to me, right? I was in Boston at [[00:15:28](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=928.76s)]
*  the time, you know, had a collaborator in this big interdisciplinary group that they had tried to [[00:15:34](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=934.2s)]
*  form and maintain with varying success. It really shone for a while. And I think the pandemic might [[00:15:42](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=942.6800000000001s)]
*  have done it in a little bit, but, you know, they had a collaborator Jan Willem van de Meent [[00:15:49](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=949.24s)]
*  who actually did the computational side, probabilistic programming. [[00:15:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=955.8000000000001s)]
*  So of all damn things, I wrote a cold email knowing no better way to go about this. [[00:16:01](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=961.8000000000001s)]
*  Yeah. And they actually answered. [[00:16:10](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=970.1999999999999s)]
*  They were probably fairly thirsty for someone, you know, interested in it, [[00:16:14](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=974.5999999999999s)]
*  because it's not still not that widespread, right? [[00:16:19](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=979.7199999999999s)]
*  Yeah, no, I mean, as far as I know, like none of this is widespread. [[00:16:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=984.1199999999999s)]
*  If you take the first and stuff too seriously, people say you're in a cult. [[00:16:28](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=988.5999999999999s)]
*  Oh, I actually didn't join the cult until later. [[00:16:32](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=992.68s)]
*  When I met Maxwell Ramstad, he's like, eventually convinced me of a lot of free energy stuff. [[00:16:36](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=996.84s)]
*  Wait, so, okay. So yeah, so Carl is famous for the free energy principle. And he considers it [[00:16:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1003.4s)]
*  a framework, not a theory, by the way. I mean, people, when pressed at least a couple years ago, [[00:16:49](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1009.48s)]
*  he considers it a framework for thinking about the globe overall function of the brain instead of [[00:16:54](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1014.92s)]
*  a theory for what it's worth. And it has a lot of detractors and a lot of cheerleaders. [[00:17:01](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1021.64s)]
*  And so you drank the Kool-Aid, eventually? [[00:17:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1029.72s)]
*  I sipped the Kool-Aid, didn't go all in. Because by the time I was being given it to sip, [[00:17:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1033.8s)]
*  I had sort of been around Lisa and Karen enough that I had really absorbed like, nope, [[00:17:20](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1040.28s)]
*  you got to have your Ivo Devo, your neuroanatomy, your mapping onto actual biology. It's the [[00:17:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1046.2s)]
*  biology that really, really, really counts. So I drunk my advisor's Kool-Aid instead of [[00:17:33](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1053.48s)]
*  the cult's Kool-Aid. [[00:17:41](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1061.4s)]
*  Good for them, good for your advisors. But some people with your background would then, [[00:17:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1064.6000000000001s)]
*  instead of embracing the biological neural plausibility, would go the other direction, [[00:17:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1070.04s)]
*  back to where it feels safer. I'll just say, right before we spoke for two minutes before [[00:17:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1075.72s)]
*  I pressed record, and you were talking about how neuro AI is hard. Is it the neuro part that's hard? [[00:18:01](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1081.08s)]
*  I don't want to make a public announcement, but it's not like, okay, you know what? No one cares. [[00:18:14](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1094.3600000000001s)]
*  No one is ever thinking about you, right? You're on camera. You're in front of an audience, [[00:18:21](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1101.24s)]
*  but no one's ever thinking about you. Sure. Okay. I think I might well end up heading [[00:18:25](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1105.4s)]
*  in sort of the neuro AI direction. I don't want to say as just like a career, like just for the [[00:18:34](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1114.2s)]
*  money or for a career prospects thing. But what I have noticed is that a lot of computational [[00:18:41](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1121.4s)]
*  neuroscientists are sort of renaming their work that now. Oh, yeah. It's a big, I just got back [[00:18:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1130.36s)]
*  from a brain initiative workshop called neuro AI. I just got back from a Norway workshop called [[00:18:58](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1138.76s)]
*  neuro AI. And that term is really being embraced. Because it sounds cool, I think, mostly. Yeah. And [[00:19:05](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1145.16s)]
*  I've sort of got this impression that like the real difference between one thing and another [[00:19:12](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1152.92s)]
*  is basically what was your training and what department are you looking for a job in? [[00:19:17](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1157.88s)]
*  And the number of departments, I think the number of departments that would do my completely ideal [[00:19:23](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1163.4s)]
*  thing is null. You know, and I'm sure most people end up saying that well before they go on the job [[00:19:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1169.32s)]
*  market, but and I'm not going on the job market right now. So luckily lucky me, but. [[00:19:35](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1175.32s)]
*  What would that be? Can you describe what that would be? [[00:19:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1183.4s)]
*  Ideally, like really, really question based or question driven science, something close to [[00:19:47](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1187.0s)]
*  cognitive science. In my PhD, I used to make up a fantasy field computational affective science. [[00:19:53](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1193.88s)]
*  Okay. By analogy to computational cognitive science. Now, computational cognitive science [[00:20:00](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1200.68s)]
*  is already a fairly small subfield that often overlaps into the computer science departments, [[00:20:08](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1208.52s)]
*  because that's who will give some of them jobs. And the number of cognitive science departments [[00:20:15](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1215.64s)]
*  at universities that do like the full six discipline, hexagonal multi handshake thing [[00:20:21](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1221.0s)]
*  is a handful. Or less. Yeah. Or less. Like, there's psychology departments who want you to do [[00:20:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1229.96s)]
*  psychology experiments. There's neuroscience departments who often want you to do neuroscience, [[00:20:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1237.96s)]
*  either theory or experiment, but they're defining the discipline, often quite narrowly. [[00:20:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1243.96s)]
*  Like I had a culture shock when I came to Vanderbilt and found out that what they mean by [[00:20:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1250.76s)]
*  computational modeling or theory is basically like biophysical or bust. Well, that depends on who [[00:20:54](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1254.76s)]
*  you're talking with, right? Because you have people like Gordon Logan there also who that's, [[00:21:03](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1263.56s)]
*  that's I'm not sure if you run around, run past him much if he how active he even is still. [[00:21:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1267.72s)]
*  I don't run into him. But yeah, you know, at least I'm talking, let's say about my lab and [[00:21:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1273.96s)]
*  a couple other labs that I interact with. Yeah. Yeah. Like there's a real emphasis on, [[00:21:20](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1280.44s)]
*  you know, be biophysical or don't do anything at all. Or be biophysical or give up theory [[00:21:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1286.6000000000001s)]
*  and become an experimenter. So how Okay, so then where do you sit in relation to that push, right? [[00:21:33](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1293.48s)]
*  I'm trying to suss out your like, your level of abstraction and what you think is important. [[00:21:40](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1300.76s)]
*  So my level of abstraction is that when I reached the end of my PhD, I said, Okay, [[00:21:47](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1307.0s)]
*  I formally did my PhD in a computer science department. If I'm ever going to really [[00:21:53](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1313.0s)]
*  investigate questions, I need to go get experimental training. Yeah, so you told me [[00:21:58](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1318.52s)]
*  this a while back. Yeah. Yeah. Yeah. So, you know, I basically said, all right, I'm going to go get [[00:22:03](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1323.6399999999999s)]
*  as hardcore a postdoc as I can. And that was the biggest mistake you've made. No, just kidding. [[00:22:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1329.64s)]
*  But is that what is that why you're saying like, that the difficulty of neuro AI is the joining of [[00:22:15](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1335.72s)]
*  the two kind of like that experimental and computational approach? Yeah, like, it's not [[00:22:22](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1342.3600000000001s)]
*  a mistake to go and get experimental experience, but is a culture shock. It took me about six [[00:22:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1349.16s)]
*  months to really be able to make progress on absolutely anything on the experimental side. [[00:22:34](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1354.36s)]
*  Why is that? Why is that? So I just I mean, I know these things and people who do experimental work, [[00:22:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1364.12s)]
*  we all kind of cry together, you know, and talk about how hard everything is and you know, but it [[00:22:49](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1369.6399999999999s)]
*  but it you know, in my case, there, you know, I'd rather not talk about it. It's sort of private to [[00:22:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1375.4799999999998s)]
*  lab stuff. Sure. Okay. You know, I don't want to suffice it to say that you run into way more [[00:23:01](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1381.16s)]
*  problems than you would imagine you might with that. Yes. Yes. Way, way, way more. And [[00:23:08](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1388.28s)]
*  the thing is, I had been prepared for the I had actually been prepared for the concept [[00:23:20](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1400.44s)]
*  that you might walk arrogantly into experimentation with some grand theory and think this is going to [[00:23:25](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1405.56s)]
*  totally be right. And you do your first experiment and it's totally wrong. Complete no result. [[00:23:32](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1412.52s)]
*  And in fact, that happened. But I was prepared for that. The part that I was much less prepared for [[00:23:40](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1420.84s)]
*  is how do I even connect a theory to an experiment? So the part that I wasn't, you know, [[00:23:49](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1429.56s)]
*  no results were sort of a thing that I like, steeled myself, you know, work it out on, [[00:23:57](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1437.56s)]
*  you know, work it out exercising, basically, just try to sweat until you can't be frustrated [[00:24:06](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1446.84s)]
*  anymore that your theory is wrong. Oh, well, the theory is wrong, even while you're submitting a [[00:24:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1453.8799999999999s)]
*  theory paper about it. But see, that is in the Poparian sense, that is the best kind of progress, [[00:24:19](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1459.0s)]
*  right? Because it's an answer. It's an answer, though, I hate to say it. But now that I'm looking [[00:24:25](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1465.96s)]
*  at another way of analyzing the data, it might get more complicated again. Sure. Yeah. So let me tell [[00:24:33](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1473.56s)]
*  you about the actual experiment that we have in both mice and macaques. You know, we have this [[00:24:41](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1481.16s)]
*  thing called the glow paradigm, global local oddball. So, you know, first you give three [[00:24:47](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1487.88s)]
*  identical stimuli per trial, AAA. This used to be done in auditory. Now we're doing it in, you know, [[00:24:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1495.16s)]
*  we've been doing it in visual. And then, you know, the local oddball is that fourth stimulus is B. [[00:25:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1502.1200000000001s)]
*  It's something different. Well, okay, what the heck is a global oddball? You know, in our manuscripts, [[00:25:11](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1511.48s)]
*  we describe it as more complex oddballs. Well, a global oddball is where we set up the expectation [[00:25:19](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1519.0s)]
*  for the animal, right? We try to intervene on the internal model and, you know, make it think there's [[00:25:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1526.04s)]
*  a B coming, but then we give it an A. So let's say what we end up doing is testing. [[00:25:34](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1534.2s)]
*  These are intermixed for the animal about 80 20. So 80% local 20% global. So you're really setting [[00:25:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1543.8s)]
*  up the expectation. Yes, actually, there's, you know, 50, there's like days and days of habituation, [[00:25:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1550.2s)]
*  followed by 50 trials of pure local oddball at the beginning of recording. So that were basically, [[00:25:57](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1557.32s)]
*  you know, habituating and queuing the expectations as powerfully as we can. [[00:26:03](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1563.4s)]
*  And so what we're trying to do is disentangle, you know, what happens if you have a predictable [[00:26:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1569.16s)]
*  change versus an unpredictable repetition. And the idea is from a neurophysiologist's point of view [[00:26:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1573.4s)]
*  is that, you know, then at the end, you're going to have a bunch of controls. [[00:26:25](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1585.48s)]
*  Like those come after the main block. So after the main block, we record a series of essentially [[00:26:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1589.4s)]
*  control sequences that are going to allow us to do statistical contrasts. And the idea is to then [[00:26:35](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1595.96s)]
*  eventually say, all right, well, if you can figure out if you can control for every other mechanism, [[00:26:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1603.0800000000002s)]
*  you can think of. So adaptation of the sensory neurons in V1, you know, like, [[00:26:48](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1608.2800000000002s)]
*  this is where it starts to get really messy and hard also. [[00:26:57](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1617.64s)]
*  Well, not just messy and hard, but like, if you can control for everything you think of, [[00:27:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1622.44s)]
*  and there's still some difference between global oddball, A, A, A, unexpected A, [[00:27:06](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1626.8400000000001s)]
*  and just pure repetition or adaptation, A, A, A, A, then ah, now you've found a signature of surprise [[00:27:14](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1634.1200000000001s)]
*  processing. And for a long time, I have just been staring at this experimental setup going, [[00:27:22](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1642.52s)]
*  how is that surprise processing? Or like, what theory have we articulated about predictive coding [[00:27:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1649.64s)]
*  in the Rowan-Ballard sense that says this is surprise processing, you know, rather than, [[00:27:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1657.16s)]
*  I mean, you know, who says the brain is tuned to look at angled gradings, [[00:27:45](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1665.16s)]
*  moving angled gradings on a screen that flash on and off? [[00:27:49](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1669.96s)]
*  Well, you can, okay, so in other words, you can't control for everything. [[00:27:53](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1673.32s)]
*  Or it's not just that you can't control for everything. It's that, as I said, I believe, [[00:27:59](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1679.56s)]
*  you know, experiments is theory laden. And if your theory is about the brain, [[00:28:05](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1685.32s)]
*  you know, predicting the continuous stream of sensory input, then flashing a series of [[00:28:11](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1691.0s)]
*  angled gradings that are optimized essentially to drive, you know, V1 to a maximum degree. [[00:28:16](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1696.76s)]
*  Well, under predictive coding theory, that's saying you're trying to drive, [[00:28:27](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1707.24s)]
*  you're trying to optimize prediction error. So how do we expect to simultaneously optimize [[00:28:31](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1711.48s)]
*  prediction error while also provoking another kind of prediction error? [[00:28:40](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1720.04s)]
*  That being surprise? [[00:28:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1724.6s)]
*  Yeah. Or like, well, that's the thing, our setup, you know, conflates like prediction error, surprise, [[00:28:47](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1727.7199999999998s)]
*  you know, visual change. [[00:28:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1735.32s)]
*  Yeah, right. Because you're using that oddball. There's a visual difference in oddball that you're [[00:28:58](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1738.76s)]
*  using. Oh, and I didn't get, I should have said actually, this is pretty much the standard paradigm, [[00:29:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1742.84s)]
*  as it turns out, for studying predictive coding and goes back to about 09. [[00:29:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1747.7199999999998s)]
*  Can you you said that surprise and prediction errors are often conflated. So what is the [[00:29:12](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1752.76s)]
*  difference then between surprise and prediction error? Theoretically, perhaps, maybe if [[00:29:18](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1758.36s)]
*  Yes, I would, I would say you need to commit yourself to a theory in order for there to be [[00:29:25](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1765.08s)]
*  a difference. But then the problem is, if you're trying to test a particular theory, [[00:29:30](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1770.2s)]
*  you should use the definitions from within that theory. Okay. [[00:29:36](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1776.12s)]
*  So prediction errors within predictive coding theory, you know, they're the residual [[00:29:39](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1779.5600000000002s)]
*  when you subtract the prediction from the data. [[00:29:46](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1786.1200000000001s)]
*  Yeah, what the organism expects top down signals, then it gets some observational data bottom up [[00:29:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1790.92s)]
*  signals. And then there's a difference in the mismatch between the prediction and the actual [[00:29:59](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1799.4s)]
*  observed data. And that's what gets passed forward. [[00:30:05](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1805.24s)]
*  Exactly. And [[00:30:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1809.6399999999999s)]
*  I guess I would, well, okay, how to relate that to surprise, you know, I would reach for my [[00:30:16](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1816.36s)]
*  information theoretic definition, because I'm a quant person, okay, and say, okay, well, [[00:30:21](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1821.48s)]
*  surprise is the negative log probability of the stimulus. You know, and essentially, those would [[00:30:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1826.76s)]
*  be two different quantities. You know, when I eventually wrote my own, like computational [[00:30:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1837.0s)]
*  modeling paper, prediction error was the gradient of surprise. So they're related, but distinct, [[00:30:42](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1842.84s)]
*  and you sort of have to use math to talk about how. But, you know, I'm, I guess I'm trying to [[00:30:51](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1851.8s)]
*  just describe the culture shock of going from, you know, sort of this environment that was, [[00:30:59](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1859.8s)]
*  it wasn't oil and water, you know, we mixed, but like, there was a very quantitative side that I [[00:31:10](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1870.68s)]
*  worked on, and a very biological side. And then, you know, I come to this like glow paradigm, [[00:31:15](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1875.32s)]
*  this experiment, and I find that, oh, the quantitative side is just removed out from [[00:31:23](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1883.3999999999999s)]
*  under me, I have to reconstruct it entirely myself. So that's what you were getting at when you were [[00:31:28](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1888.84s)]
*  talking about how what we'll call it neuro AI is hard. Yeah, like actually taking [[00:31:35](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1895.8s)]
*  there's sort of this problem where in neuro, we are often doing paradigms or tasks that [[00:31:45](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1905.48s)]
*  from a pure AI point of view might be considered almost trivial. Yeah. But [[00:31:53](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1913.48s)]
*  from a biological plausibility point of view, that often makes them hard again. And then [[00:32:00](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1920.4399999999998s)]
*  if you're actually trying to explain neuronal data, or worse, trying to map some real theory [[00:32:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1927.48s)]
*  of the brain onto neuronal data, rather than just, you know, suggest that there could exist [[00:32:14](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1934.84s)]
*  some mechanism explaining this behavior. Because, you know, like, there's been multiple [[00:32:23](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1943.8799999999999s)]
*  computational models of same, you know, of the same behaviors. I'm sort of thinking of like the [[00:32:30](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1950.52s)]
*  famous drift diffusion models of, you know, decision making. Yeah, like, how do you know if [[00:32:36](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1956.4399999999998s)]
*  the brain is doing a drift diffusion, you know, accumulate evidence to a threshold and then decide [[00:32:42](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1962.28s)]
*  algorithm for decision making or, you know, resource constrained reinforcement learning [[00:32:49](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1969.6399999999999s)]
*  algorithm for decision making. There are experiments that have been fit with both these [[00:32:57](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1977.24s)]
*  kinds of models. Yeah, that's right. How do you know? Massive shock for me that there's just like, [[00:33:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1982.04s)]
*  oh, wait, is everyone just pretending? What do you mean pretending? Pretending that what [[00:33:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1989.72s)]
*  they're doing is valid and what everyone else is doing is not or what? Well, pretending that, [[00:33:15](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=1995.16s)]
*  like just taking data and, you know, fitting it such that you can claim to use your theory to [[00:33:20](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2000.2s)]
*  explain behavior, but you haven't actually tested it against substantive alternative theories, [[00:33:27](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2007.96s)]
*  rather than some kind of null hypothesis. Like, what the heck is our null hypothesis [[00:33:35](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2015.72s)]
*  regarding behavior in the brain? Or alternative hypotheses, it doesn't even have to be null, [[00:33:41](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2021.08s)]
*  just a clear alternative. Yeah. Yeah. There's something that actually that Jeff Shaw, I'll just [[00:33:47](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2027.16s)]
*  elevate him in this regard, like every year when I was a postdoc, there's a fundamental set of [[00:33:56](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2036.2s)]
*  papers, one of which is like the method of alternative hypotheses, where we tried to base, [[00:34:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2042.92s)]
*  but I think because of these things, because it's hard, like you mentioned drift diffusion, [[00:34:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2049.48s)]
*  and I was doing drift diffusion work, essentially, stochastic accumulator work, which is exactly [[00:34:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2053.7200000000003s)]
*  what you're saying, does the neuron like ramp up to some threshold? And then that actuates the [[00:34:19](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2059.88s)]
*  behavior. And that's one of the things that Jeff Shaw is famous for. And so, you know, his idea is [[00:34:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2064.76s)]
*  to look in the brain and test it and ask it, right? Through recordings. And of course, it's [[00:34:31](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2071.56s)]
*  not super clean, because we're dealing with different kinds of stimuli in this very controlled [[00:34:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2077.24s)]
*  environment, the frontal eye field. As we know now, any given brain area doesn't just have a single [[00:34:42](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2082.44s)]
*  function. Right? So there's, you know, mixed selectivity in brain areas where they're doing [[00:34:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2090.6s)]
*  overlapping populations of neurons or doing overlapping functions, things. So if so, but [[00:34:59](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2099.56s)]
*  anyway, oh, yeah, I mean, any talk of like, frankly, any talk of selectivity, slightly makes me want to [[00:35:06](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2106.36s)]
*  scream and I've just been re-eculturating myself to an environment where like, the word degeneracy, [[00:35:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2113.0s)]
*  and, you know, to an environment where these things are not the assumptions anymore. [[00:35:20](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2120.04s)]
*  Wait, where degeneracy is not an assumption? [[00:35:25](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2125.88s)]
*  Where degeneracy isn't the assumption, you know, top-down influences often aren't the assumption. [[00:35:27](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2127.96s)]
*  Like, it's a very, and I'm not saying this as a negative thing. In a certain way, I like it, [[00:35:34](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2134.44s)]
*  even though I don't think I can make a career out of it. Like, very Andy Clark quoting Quine, [[00:35:40](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2140.7599999999998s)]
*  you know, had this thing about desert landscapes. Like, a neurophysiologist's point of view is a [[00:35:48](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2148.12s)]
*  very desert landscape point of view. There's the things I can measure, nothing else. Nothing else [[00:35:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2155.0s)]
*  exists. I'll talk about selectivity because I think I can measure it. And if you tell me that [[00:36:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2162.2s)]
*  that's actually caused by what I do, rather than an observation of a causally independent system, [[00:36:08](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2168.28s)]
*  then I will get in an argument with you because I think I'm measuring something real. [[00:36:17](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2177.16s)]
*  I see. So what you're describing, it's interesting that you find yourself in that world now because [[00:36:22](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2182.68s)]
*  in some sense, that's kind of the old school world, which is still very much alive and thriving. [[00:36:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2189.48s)]
*  Whereas there's been this recent push into a much more, you know, more naturalistic [[00:36:36](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2196.7599999999998s)]
*  types of tasks and removing the constraints from the lab, you know, the lab-based experimental stuff. [[00:36:41](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2201.7999999999997s)]
*  And that's hard in a very different way. [[00:36:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2210.3599999999997s)]
*  So let me, you know, make some applause or give some applause to Andre here, right? [[00:36:53](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2213.24s)]
*  I think he doesn't do that kind of experiment yet because he's actually pushing something that's [[00:36:59](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2219.64s)]
*  already very risky and innovative. He calls it madeleine multi-area high density laminar [[00:37:06](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2226.2s)]
*  electrophysiology, which basically amounts to saying, you know, let's have like not just one [[00:37:12](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2232.68s)]
*  neuro pixels probe in one area. Let's just cover the brain in neuro pixels probes. [[00:37:21](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2241.16s)]
*  Yeah. So neuro pixels probes are like these really high density, multi-electrode probes [[00:37:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2246.9199999999996s)]
*  so that when you put them in any given area of the brain, you're getting recordings of [[00:37:31](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2251.8799999999997s)]
*  hundreds to sometimes thousands of neurons. [[00:37:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2257.56s)]
*  Exactly. Yeah. You know, and all of our work includes the LFP, the local field potential, [[00:37:39](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2259.96s)]
*  as well as, you know, the individual spiking signals, you know, and then we analyze both [[00:37:47](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2267.32s)]
*  together, which, you know, I won't say who, but like someone I really, really respect a lot. [[00:37:54](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2274.68s)]
*  I went and visited their lab, actually one of my scientific heroes. You know, [[00:38:01](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2281.64s)]
*  I went and visited their lab at one point. Can't say who you can't say who. [[00:38:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2287.16s)]
*  Yeah. I'm realizing I can't even specify this little. Well, the point being at one point, [[00:38:10](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2290.8399999999997s)]
*  I asked, you know, do you analyze the LFPs? And they said, no, we just look at the spiking. [[00:38:14](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2294.92s)]
*  You know, and I think, you know, respect to Andre, like he's, [[00:38:20](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2300.92s)]
*  I didn't talk about it before because like, it's not as native a part of my worldview. It's what [[00:38:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2306.68s)]
*  I'm learning. But, you know, this is actually a very ambitious thing. You know, even for a [[00:38:32](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2312.12s)]
*  simple experiment, we'll have like two full neuro pixels probes taking, you know, multi-unit activity, [[00:38:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2317.72s)]
*  individual spikes that we sort with Kilosort, you know, then LFP. [[00:38:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2324.52s)]
*  You know, and sort of LFP is what people talk about as measuring when people use the term [[00:38:51](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2331.48s)]
*  oscillations. Sorry. Yeah. No, I was saying population level signal. Oh, there's that too. [[00:38:58](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2338.6s)]
*  Yeah. But it's a different, it's a kind of a complimentary signal. The other thing is [[00:39:04](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2344.7599999999998s)]
*  spikes are definitely the outputs of neurons, whereas LFP is thought to more closely track [[00:39:08](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2348.52s)]
*  that population level input. Yeah. So then we also, you know, and then we like analyze both. [[00:39:15](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2355.08s)]
*  So we're often doing, you know, cross correlation or coherence measures of like LFP to spike. [[00:39:22](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2362.6s)]
*  And this actually tells you quite a lot. And it's, you know, it's difficult. It's ambitious. And [[00:39:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2369.1600000000003s)]
*  my understanding is that it's also not easy to get grants in. [[00:39:39](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2379.48s)]
*  Like I think Andre won his NSF career just this year. And that was the first grant that the lab [[00:39:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2383.6400000000003s)]
*  had gotten in, I think, possibly three years of operation. For joint, specifically for joint spike [[00:39:49](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2389.4s)]
*  LFP analysis. For a meddling as a whole. Yeah. Okay. For like this research program of, [[00:39:56](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2396.76s)]
*  you know, let's measure in multiple areas. Let's measure the LFPs and the spikes. [[00:40:05](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2405.0800000000004s)]
*  Let's try to capture as much as we can, so to speak, as many times as we can. [[00:40:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2409.6400000000003s)]
*  Let's really try to push the limits on how dense the sampling can be in electrophysiology [[00:40:16](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2416.92s)]
*  because of, you know, essentially the resolution issues with imaging or EEG that you would not [[00:40:21](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2421.96s)]
*  want to use those. You would want to use electrophysiology. Yeah. Okay. So backing up [[00:40:28](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2428.44s)]
*  here. So I was just at this brain initiative workshop and it was brought up multiple times, [[00:40:35](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2435.0s)]
*  you know, so the idea was to think in terms of like, well, what would we need in 10 years? [[00:40:40](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2440.04s)]
*  What's an ambitious goal for 10 years in neuro AI? And two people, one person said, [[00:40:45](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2445.2400000000002s)]
*  suggested this and then it was echoed by another person that what we need is to be able to record [[00:40:51](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2451.88s)]
*  synaptic strengths. So, you know, for example, neural networks, the strength between the units [[00:40:58](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2458.04s)]
*  is where all the parameters are. And that's those billions and billions of parameters that [[00:41:04](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2464.36s)]
*  in these large language models, et cetera, those are what get changed that that strength [[00:41:08](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2468.76s)]
*  between in the connections. And if there was just a way for us to measure that in the brain, [[00:41:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2473.2400000000002s)]
*  then that's an ambitious goal and it's a worthwhile goal. But my immediate thought was, [[00:41:18](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2478.28s)]
*  you know, there's that age old question, like, what would you do if you could measure all of the [[00:41:25](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2485.5600000000004s)]
*  spiking from all of the neurons? Would you even know what to do with it? And [[00:41:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2489.6400000000003s)]
*  no, the questions, no, we don't because because you because it goes back to the theory ladenness. [[00:41:34](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2494.0400000000004s)]
*  Like you have to have, you have to come from some sort of framework or theory to then ask questions [[00:41:39](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2499.0s)]
*  of that data. So just collecting the data is not going to get you there. Right. [[00:41:46](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2506.36s)]
*  Yes. And I think that's where that's where I'm just going to put my cards on the table and say, [[00:41:51](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2511.32s)]
*  I think that's an open challenge for the field. And I'm happy to be working on it. [[00:41:57](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2517.96s)]
*  What is the open challenge? Sorry, to figure out how the heck you analyze your data in a properly, [[00:42:03](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2523.08s)]
*  you know, theory driven or question driven way, rather than just [[00:42:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2529.48s)]
*  I don't want to say this like it's too bad of a thing. Right. But rather than just running [[00:42:16](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2536.6s)]
*  statistics and then saying I found an effect. Well, that's that's interesting. Interesting, [[00:42:21](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2541.56s)]
*  because that's kind of what the AI side does in neuro AI. It's like throwing a bunch of statistics [[00:42:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2546.84s)]
*  at the data. And even Terry Sonowski brought this up at the workshop. Like what what principles [[00:42:33](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2553.8s)]
*  have we learned? What what principles are there to gain from this approach? Right. [[00:42:40](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2560.36s)]
*  Yes. Here's where I would sort of reach back into my training with Jan Willem as a probabilistic [[00:42:45](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2565.4s)]
*  programmer and say, for God sakes, we need to be writing down generative models, fitting them to [[00:42:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2570.68s)]
*  data and then doing model comparison. You know, we need to actually have some measure of how well [[00:42:56](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2576.92s)]
*  does something fit the data? What theory motivates it? And then, you know, compare them in a principled [[00:43:03](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2583.96s)]
*  way. And I think that, you know, machine learning can actually help with that. And I've seen a lot [[00:43:12](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2592.12s)]
*  of very, very productive, you know, and like a flurry of new work, essentially, in just analyzing [[00:43:17](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2597.96s)]
*  neural data. But then you also have to convince, here's the hard part. Those things can get [[00:43:25](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2605.4s)]
*  published in machine learning conferences. And then you have to both teach the experimenters to [[00:43:31](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2611.88s)]
*  use them and convince them to use them and teach it to them in such a way that [[00:43:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2617.64s)]
*  they don't need you as a statistician or machine learner to actually, you know, stand over their [[00:43:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2624.44s)]
*  shoulder telling them how to encode every little hypothesis because you want them to use it a dozen [[00:43:51](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2631.16s)]
*  different times. And they can't just keep you around forever as some kind of consulting machine [[00:43:56](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2636.2s)]
*  learner. Right. Well, you know, actually, so I'm going to, it's not name dropping, [[00:44:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2642.2799999999997s)]
*  because I wasn't like talking with him. But I remember Jeff Hawkins years ago at a, [[00:44:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2647.72s)]
*  giving a keynote, I think at the annual Society for Neuroscience lecture, and I'm sure he's made [[00:44:12](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2652.8399999999997s)]
*  this point over and over again. You know, the traditional physics approach is you have your [[00:44:18](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2658.04s)]
*  theorists and you have your experimentalists and they're sort of happy to play together. And that's [[00:44:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2664.2799999999997s)]
*  not the case necessarily in neuroscience and that we need to get to a point where [[00:44:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2669.48s)]
*  the experimentalists are happy gathering the data to feed to the theorists who then can analyze it. [[00:44:34](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2674.52s)]
*  But that sounds awful to me too. Right. I mean, I, so I will actually say I would much rather that [[00:44:39](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2679.88s)]
*  experimentalists be capable and happy of analyzing their own, with analyzing their own data. [[00:44:49](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2689.48s)]
*  And the reason is that, you know, if I say I'm going to be a theorist or a computationalist, [[00:44:56](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2696.44s)]
*  then, you know, data analysis is something that pays the bills, perhaps. It's something that can [[00:45:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2702.28s)]
*  help get a routine number of papers out the door, you know, for like a machine learning person. [[00:45:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2709.88s)]
*  I'm, I am actually thinking of someone, Scott Linderman over at Stanford. Like, you know, [[00:45:18](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2718.12s)]
*  you'll notice that a lot of his papers are basically just machine learning based data [[00:45:25](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2725.0s)]
*  analyses for neural data. And that's great. That's the thing. Like that can build a career. [[00:45:28](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2728.76s)]
*  Now, personally, is that what I would want to think about as a theorist? How do we analyze data? [[00:45:35](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2735.4s)]
*  No, no. Like, you know, that is not the thing that I have, you know, a secret manuscript that [[00:45:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2743.32s)]
*  I've been trying to finish for a year. You know, the thing where I have a secret manuscript that [[00:45:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2750.2s)]
*  I've been trying to finish for a year is, you know, how do we explain emotion in a quantitative [[00:45:56](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2756.7599999999998s)]
*  way or affect core affect valence and arousal in a quantitative way by going all the way back [[00:46:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2762.3599999999997s)]
*  to, you know, the urbilitarian and then picking C. elegans as a model organism. [[00:46:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2769.24s)]
*  Yeah, good luck with that. [[00:46:14](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2774.44s)]
*  Exactly. Yes. See, exactly. Like, good luck with that. [[00:46:18](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2778.76s)]
*  But people like you mentioned Scott Linderman. And so he develops a lot of tools that are being [[00:46:23](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2783.32s)]
*  used in these naturalistic kinds of tasks, right? And that skill set is seem seems to be what is [[00:46:28](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2788.04s)]
*  really valuable in the academic marketplace, at least these days. Do you think I have that right? [[00:46:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2797.4s)]
*  Um, yes. Yeah. Like when I was so I'm going to use myself as an example instead of him because, [[00:46:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2804.84s)]
*  you know, I know myself better, right? And I don't think I could speak for the narrative arc of his [[00:46:51](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2811.48s)]
*  career. But I know that when I started my PhD, the starter project that I got put on was here's a new [[00:46:56](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2816.2000000000003s)]
*  way of analyzing fMRI data in a little bit more theory driven way. And it worked. [[00:47:04](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2824.04s)]
*  Sorry, but what was the Oh, you just needed to employ that method. [[00:47:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2833.7200000000003s)]
*  No, I mean, it wasn't just Oh, there was some method and we employed it, we were building [[00:47:18](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2838.44s)]
*  something new. Because, you know, our collaborator on the psychology side had some data and he wanted [[00:47:22](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2842.44s)]
*  to analyze it and the standard ways of analyzing it were inadequate to the theoretical question he [[00:47:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2849.96s)]
*  wanted to ask. Yeah. So he wanted us to build something new, we built it, we published it, [[00:47:36](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2856.1200000000003s)]
*  you know, that gets citations, there was a follow up. You know, I think there's now follow ups to [[00:47:42](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2862.2799999999997s)]
*  the follow up, like by other groups. Right. You know, that, like that stuff is [[00:47:48](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2868.12s)]
*  this is going to sound horrible, but I don't mean it in a bad way. That stuff is good commodity [[00:48:00](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2880.12s)]
*  science. But it's also necessary. I can I can make it sound even better. Yeah, it's like the, [[00:48:04](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2884.36s)]
*  it's the Toyota of science, right? Like I drive a Toyota, I only bought a car this past year, [[00:48:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2889.96s)]
*  but I drive a Toyota because you know what, it's practical. Yeah. Yeah. You know, that is very [[00:48:15](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2895.7200000000003s)]
*  practical science that you can reliably like never run out of new reasons to do more of it, [[00:48:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2904.6s)]
*  and therefore never run out of publications. Well, that's right. That's right. But this goes back to [[00:48:31](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2911.88s)]
*  the to the idea of so does that contribute to progress in theory progress in understanding [[00:48:36](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2916.44s)]
*  principles? Or is it just a very practical way to harness and say something about the data that's [[00:48:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2924.68s)]
*  being generated? I think a lot of I think it has the potential to do both. But by default, [[00:48:53](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2933.96s)]
*  it mostly does the second one. And that's not a criticism. That's to say I think the field has, [[00:49:01](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2941.96s)]
*  you know, the ingredients for a really great synthesis, sort of laying around in different [[00:49:08](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2948.2s)]
*  people's labs. And what we need is essentially like a small conference or workshops worth of [[00:49:14](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2954.2s)]
*  cross pollination, where you can get the people with the appropriate skills all in the same room, [[00:49:21](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2961.48s)]
*  give them the incentives to work together. And I think it's actually the incentives that are the [[00:49:27](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2967.64s)]
*  hard part. This idea of getting the proper the people with the proper skill sets in the same room [[00:49:34](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2974.3599999999997s)]
*  for a couple days. It's awesome. The proper skill set is a shifting landscape itself. Right now we [[00:49:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2983.3199999999997s)]
*  have a very specific one like people like you and Scott, whom you mentioned and stuff like where [[00:49:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2990.8399999999997s)]
*  these commodities, these tools are extremely valuable, widely used. But, you know, going back [[00:49:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=2995.24s)]
*  to Hubel and Weasel, right there on transparencies, they're like putting like just little shapes [[00:50:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3002.8399999999997s)]
*  and trying to listen for the sound of neurons. Even like Jeff Shaw, whom I mentioned earlier, [[00:50:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3009.08s)]
*  would tell us stories about, you know, you're in lab, you'd make like a little hole out of like a [[00:50:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3013.9599999999996s)]
*  wooden cutout, and you'd like put a light up in there. And it is the neuron active or not. It's [[00:50:19](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3019.3199999999997s)]
*  a very different world back then, very different skill set. And so I don't know how we track that. [[00:50:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3024.04s)]
*  And that's a meta problem. Yeah. I mean, that's why I say like, if you're going to have a division [[00:50:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3029.32s)]
*  of people's jobs or departments into theorist and experimenter, then I would want the experimenters [[00:50:36](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3036.2799999999997s)]
*  to be able to analyze their own data. Because, you know, then they can do that even if it's a [[00:50:42](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3042.2799999999997s)]
*  bit quantitative. And even if that's something of a moving frontier sometimes. And then the theorists [[00:50:48](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3048.44s)]
*  they can focus on asking questions like, well, how does the brain actually work now that we've [[00:50:54](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3054.6s)]
*  measured it? You know, now that we're able to interpret the measurement. Let's get back to [[00:51:00](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3060.2799999999997s)]
*  predictive coding, though. I mean, are you, so are you, no, you don't want to pin yourself into a very [[00:51:06](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3066.44s)]
*  narrow corner. But I mean, where are you in terms of, so the idea of predictive coding, [[00:51:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3073.4s)]
*  predictive processing is that we are constantly predicting what is coming into our senses. And so [[00:51:19](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3079.8s)]
*  we have to have sort of a model, to use the term loosely, of what we infer to be causes of things [[00:51:25](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3085.48s)]
*  coming into our senses, infer to be a cause in the world. So we're making these predictions from our [[00:51:33](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3093.8s)]
*  world model. Bayesian brain hypothesis is one way to say it. Free energy principle is another sort [[00:51:39](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3099.48s)]
*  of framework implementation. So are you on board with like this being the function of the brain, [[00:51:46](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3106.68s)]
*  a major function of the brain? Where does this sit? Let's say major function of sensory cortex. [[00:51:53](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3113.24s)]
*  Major function of sensory cortex. Yes. Why sensory? So there's, okay, going to lapse into [[00:51:59](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3119.16s)]
*  neurophysiology vocabulary for a little bit. You know, sensory cortex is usually well laminated. [[00:52:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3127.16s)]
*  Like there's laminar sensory cortex down in these low areas. And then as you move [[00:52:14](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3134.04s)]
*  both up the hierarchy towards cognitive areas, what we think of as cognitive areas, [[00:52:19](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3139.32s)]
*  and also sideways over to motor, you get different patterns of lamination. [[00:52:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3144.76s)]
*  So the cortex is a laminar structure, meaning it has a fairly repeated, well, very repeated motif. [[00:52:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3149.16s)]
*  Six layers. Now, the one that raw sensory stimulus comes into is layer four. And the thing is that [[00:52:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3157.88s)]
*  when we talk about different lamination patterns, we're talking about, I believe they're called [[00:52:48](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3168.6s)]
*  agranular and disgranular. And those have either much less layer four or they're entirely missing it. [[00:52:56](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3176.12s)]
*  I think that that's right. I think agranular has no layer four and disgranular maybe has a weaker. [[00:53:04](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3184.6s)]
*  Yeah, like a weaker layer four. But now if you were asking yourself, [[00:53:11](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3191.48s)]
*  okay, so if I'm doing Bayesian computation, then my observed random variable, [[00:53:16](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3196.2s)]
*  which is the stimulus, it has to come in somewhere. And if I'm using this hypothesis about the laminar [[00:53:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3204.2799999999997s)]
*  microcircuit doing predictive coding, then where's that coming in? It's coming in in layer four. [[00:53:32](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3212.36s)]
*  So what is the circuit doing if it doesn't have layer four? [[00:53:38](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3218.1200000000003s)]
*  That's where the generative network is, right? [[00:53:41](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3221.96s)]
*  Maybe. Logically, it can't be doing variable by variable Bayesian inference. [[00:53:45](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3225.7200000000003s)]
*  It could just store priors. But then why does it have a layer two, three? Because that's the one [[00:53:53](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3233.88s)]
*  that computes errors and thereby updates the predictions now. So actually really, since we're [[00:54:01](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3241.48s)]
*  following on Rajesh Rao's episode, I actually really like his hypothesis that oh, two, three is [[00:54:08](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3248.6s)]
*  the one that handles sensory data. Five, six is actually handling chiefly motor data. And when [[00:54:15](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3255.96s)]
*  you compute an updated sensory prediction, you might route it through there on its way somewhere [[00:54:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3264.28s)]
*  else. But then fundamentally, he would be saying, okay, now, you know, oh, and he also [[00:54:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3269.0s)]
*  notes that there's thalamic projections into a cortical column that don't have to go through [[00:54:36](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3276.36s)]
*  layer four. Right. So the desire is to bypass layer four, bypass layer four being a necessary [[00:54:42](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3282.52s)]
*  part of predictive coding. Is that one way to... Well, or ways of reformulating the predictive [[00:54:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3290.68s)]
*  coding hypothesis so that you can still have sensory data coming in even when there isn't a [[00:54:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3295.8s)]
*  layer four. And then you just have physiological and evolutionary questions about why are these [[00:55:01](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3301.0800000000004s)]
*  areas agranular, disgranular, laminar? What are the differences between them and the similarities? [[00:55:08](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3308.44s)]
*  But you haven't totally abandoned your framework. Whereas if you're committed to layer four being [[00:55:16](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3316.52s)]
*  where sensory observations come in, then logically the Bayesian computation can only be done in [[00:55:21](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3321.64s)]
*  laminar sensory cortex. Okay, I see. So when I say I think I'm committed to this being, you know, [[00:55:27](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3327.64s)]
*  an explanation of laminar sensory cortex, I'm being kind of minimalist. [[00:55:34](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3334.7599999999998s)]
*  Sure. Okay. Okay. But so you're on board with Raj's story about like the incoming two, three, [[00:55:40](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3340.2799999999997s)]
*  layer two, three, outgoing layer five and how that's one way that it biologically, [[00:55:47](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3347.16s)]
*  plausibly could be implemented. But your divide and conquer predictive coding also strives to be [[00:55:53](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3353.0s)]
*  biologically plausible. Maybe we can start with like what is divided and what is conquered in [[00:56:00](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3360.52s)]
*  divide and conquer predictive coding and then maybe talk about it. Sure. Yeah. Okay. So [[00:56:06](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3366.44s)]
*  if you go look at some of the free energy papers, I think there's even one called the graphical [[00:56:14](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3374.2000000000003s)]
*  book, like the graphical brain. You know, they tell a story about how a probabilistic graphical [[00:56:21](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3381.64s)]
*  model has these different nodes representing different, you know, unobserved random variables [[00:56:27](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3387.48s)]
*  and these get mapped onto cortical areas. And then the communication between areas is, you know, [[00:56:33](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3393.88s)]
*  a series of messages in a belief propagation algorithm that eventually gets down to primary [[00:56:40](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3400.12s)]
*  sensory areas where the random variable is observed. Now, this kind of algorithm makes [[00:56:45](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3405.4s)]
*  a very specific assumption that they call the mean field assumption about essentially saying, [[00:56:54](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3414.8399999999997s)]
*  essentially saying, we're going to approximate the posterior distribution [[00:57:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3422.44s)]
*  with a product of independent representations. So we'll have one representation for the visual, [[00:57:08](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3428.28s)]
*  one for the audio, you know, one that represents the integration of visual and audio, [[00:57:20](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3440.28s)]
*  but they're actually all going to sort of be statistically independent in, you know, [[00:57:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3446.28s)]
*  the approximate posterior scare quoting as implemented in the brain. And by the way, [[00:57:33](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3453.8s)]
*  on the machine learning side, we know that this is quite a bad representation of a posterior [[00:57:41](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3461.2400000000002s)]
*  distribution. Why is that? You know, essentially it can't represent correlated posteriors. [[00:57:47](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3467.4s)]
*  Because of the independence assumption. [[00:57:53](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3473.32s)]
*  Yeah, like it's making a very strong independence assumption [[00:57:57](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3477.08s)]
*  that was necessary to simplify the math in like 2003. Literally, the first time variational [[00:58:00](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3480.92s)]
*  inference was published was in a PhD thesis from 2003 or so. Like, you know, all my respect to [[00:58:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3487.88s)]
*  people who are developing new things and make simplifying assumptions, right? But of course, [[00:58:14](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3494.92s)]
*  the point of science is that we always want to try and relax our simplifying assumptions and ask, [[00:58:19](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3499.72s)]
*  can we come up with a way to essentially, can we assume that the real world is really complex [[00:58:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3506.6s)]
*  and complexify our models over time so as to accommodate the real world? [[00:58:33](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3513.8799999999997s)]
*  Well, but then you're also dealing with Occam's razor, you're dealing with trying to figure out, [[00:58:39](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3519.56s)]
*  well, what can we abstract away? What are the important things that we can abstract? And so, [[00:58:45](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3525.48s)]
*  when you make assumptions like that mean field assumption, you are making tradeoffs. It's just [[00:58:51](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3531.56s)]
*  whether they're the right tradeoffs given what you're trying to answer, right? [[00:58:58](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3538.6s)]
*  Yeah. And, you know, sort of what I have learned through my PhD on the machine learning side [[00:59:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3542.52s)]
*  was that if you have a complex structured graphical model, as might be used in some [[00:59:10](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3550.2s)]
*  cognitive science task, then mean field variational inference doesn't work very well. [[00:59:16](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3556.04s)]
*  And I thought, well, you know, if I take a theory, or sorry, if I take a hypothesized model [[00:59:22](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3562.92s)]
*  from neuroscience and I apply it in AI, it just doesn't work very well. Is that what the brain [[00:59:31](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3571.16s)]
*  does? No, I don't think the brain, you know, fails at things that are doable with current AI methods. [[00:59:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3577.4s)]
*  Or rather, I don't think the brain fails at doing things that we've observed it to be able [[00:59:45](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3585.88s)]
*  to do in actual behavior. You know, I think that's a case where the algorithmic model is [[00:59:51](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3591.0s)]
*  just inadequate. So I said, okay, let's make a better one. Instead of mean field, you know, [[00:59:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3595.8s)]
*  independent, independence assumptions, let's instead try to break down the random variables [[01:00:03](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3603.32s)]
*  from one another so that you maintain their correlations when you update them. [[01:00:11](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3611.4s)]
*  Is this the dividing part? [[01:00:15](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3615.48s)]
*  Yeah. Yes. [[01:00:17](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3617.08s)]
*  Just say it again. So what are you, you're dividing, go ahead. [[01:00:20](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3620.04s)]
*  You take this, you know, a probabilistic graphical model. So it's a mathematical [[01:00:22](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3622.2s)]
*  analogy to the brain's internal model of the world. And you say this consists of a bunch of [[01:00:28](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3628.84s)]
*  different variables that are connected to each other in various ways. You know, kind of like [[01:00:34](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3634.2s)]
*  cortical columns, we can imagine. So this is actually how I imagined it was, you know, [[01:00:40](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3640.2s)]
*  one cortical column, one random variable. And then when they communicate with each other, [[01:00:46](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3646.12s)]
*  you know, those are conditional dependencies and all that. And then I said, okay, let's try to [[01:00:52](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3652.04s)]
*  divide this so that we can update each random variable in a way that takes into account [[01:00:57](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3657.64s)]
*  the correlations with the other random variables that it's connected to. [[01:01:05](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3665.3199999999997s)]
*  Is that the conquer part? [[01:01:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3669.56s)]
*  That's the divide part. [[01:01:11](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3671.56s)]
*  Well, yeah. [[01:01:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3673.56s)]
*  Or yeah. Then the update is like the first step of conquer. Then the real conqueror is that we have [[01:01:14](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3674.3599999999997s)]
*  all of these importance weights from the world of like Monte Carlo methods for Bayesian statistics [[01:01:23](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3683.16s)]
*  that then let us eventually write out, you know, here's how good a fit to the joint model we have, [[01:01:30](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3690.36s)]
*  to the whole probabilistic graphical model. So we're saying we want to do local updates that [[01:01:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3697.88s)]
*  maintain some kind of global coherence. And it gets called divide and conquer because, [[01:01:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3703.4s)]
*  well, frankly, NeurIPS is ultimately a computing conference. [[01:01:52](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3712.28s)]
*  Computing conference? [[01:01:57](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3717.24s)]
*  No, yeah. [[01:01:59](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3719.48s)]
*  Yeah. And like all computing people have taken an algorithms class [[01:02:00](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3720.3599999999997s)]
*  where they talk about divide and conquer methods. [[01:02:05](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3725.0s)]
*  I see. I didn't realize that. So this is a well-known phrase in the algorithmic world? [[01:02:08](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3728.12s)]
*  Yeah. Like if you talk to algorithmists and say divide and conquer, they'll say, oh, okay, [[01:02:14](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3734.3599999999997s)]
*  so you're taking some kind of huge data structure and recursively performing the same computation [[01:02:19](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3739.16s)]
*  on each component before going on to the connected bits. [[01:02:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3746.12s)]
*  Okay. That makes a lot of sense. [[01:02:30](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3750.8399999999997s)]
*  And it just sort of happened that like you needed a lot of Monte Carlo tricks to make this work, [[01:02:33](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3753.08s)]
*  but when you do, it's very sort of intuitive why you would want to do it that way if you [[01:02:39](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3759.24s)]
*  were then going to map your probability model onto a physical circuit structure [[01:02:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3764.92s)]
*  where the different random variables are spatially separated and have to signal to each other. [[01:02:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3770.7599999999998s)]
*  So what is in the divide and conquer model? What is required for it to be biologically plausible? [[01:02:56](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3776.52s)]
*  So the claim of biological plausibility we made is to say the computations are purely local. [[01:03:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3787.48s)]
*  Right. Instead of the local updates, instead of globally. [[01:03:16](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3796.2s)]
*  Right. So people have talked about, could backpropagation be implemented in the brain? [[01:03:19](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3799.48s)]
*  Tomaso, our senior author, has this paper on using Gaussian predictive coding to actually [[01:03:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3806.12s)]
*  implement backpropagation as a substrate for backpropagation. And in this paper, [[01:03:34](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3814.12s)]
*  we're sort of saying, well, let's assume you can't do backpropagation. You don't have any kind of [[01:03:40](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3820.68s)]
*  global computation graph or computation automatic differentiation tape in the brain. [[01:03:47](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3827.4s)]
*  But let's assume that one cortical column can signal to another and that if you're [[01:03:56](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3836.6s)]
*  representing one random variable locally, then you can do really three things with it. [[01:04:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3842.3599999999997s)]
*  Sample from its distribution. Measure the log density of its distribution. [[01:04:11](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3851.08s)]
*  So the log probability density, where density just means that you're talking about continuous [[01:04:19](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3859.0s)]
*  random variables and not discrete ones. And three, take the gradient of the log probability [[01:04:25](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3865.64s)]
*  density. And if you can do those three things locally, then you have the primitives necessary [[01:04:31](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3871.48s)]
*  for our algorithm. And you can thereby obtain global coherence out of local computations. [[01:04:38](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3878.52s)]
*  And you don't need any backprop. [[01:04:47](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3887.4s)]
*  Since we were talking about that experiment versus theory meta science topic earlier, [[01:04:49](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3889.48s)]
*  I mean, does this make clear predictions about what kinds of signals that you would expect to see? [[01:04:57](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3897.4s)]
*  Now, here's where it gets biologically implausible. These were still rate-coded neurons, [[01:05:03](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3903.96s)]
*  right? So they can still cross between positive and negative. [[01:05:10](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3910.36s)]
*  Right. So brains use spikes among other signals like LFPs, but essentially all of [[01:05:14](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3914.7599999999998s)]
*  modern machine learning or AI models use rate codes. And there are a lot of people working [[01:05:21](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3921.64s)]
*  on spiking neural networks also, but I assume that if you're going to implement it in a spiking [[01:05:27](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3927.3199999999997s)]
*  network, then you have to go. I mean, it's plausible with the sampling approach, right? [[01:05:32](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3932.2799999999997s)]
*  Because that's what spikes are all about. [[01:05:38](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3938.6s)]
*  Spikes are all about sampling? [[01:05:42](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3942.2s)]
*  Well, you can. So going back to the old debate on like how probability is implemented in brains, [[01:05:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3943.88s)]
*  there's the sampling approach versus the approach where the spike counts map on to [[01:05:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3950.52s)]
*  some probability distribution types, etc. [[01:05:57](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3957.2400000000002s)]
*  Oh, yeah. But with a twist. So with a twist, yeah. So I know there's a lot of sampling approaches [[01:06:00](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3960.12s)]
*  where you essentially say a neuron has a preferred stimulus and implements a likelihood function. [[01:06:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3969.16s)]
*  And the priors are actually represented in the developmental program of the genome, [[01:06:16](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3976.8399999999997s)]
*  not in the neurons themselves. And then those eventually make the prediction that [[01:06:22](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3982.92s)]
*  they make the opposite prediction to predictive coding. They say when the posterior probability [[01:06:30](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3990.92s)]
*  of what the neuron prefers is higher, the neuron will fire more. And predictive coding actually, [[01:06:35](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=3995.16s)]
*  and the free energy principle and all of those approaches are much more information theoretic. [[01:06:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4003.96s)]
*  They say that when the stimulus is thoroughly expected, you should see much less neuromal firing. [[01:06:49](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4009.08s)]
*  And so we're in that family of theories, though we do use random sampling. [[01:06:56](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4016.7599999999998s)]
*  My dispute with spikes being about sampling is that of course, if you patch clamp a neuron [[01:07:04](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4024.12s)]
*  like in vitro, then what is it like 96% of the variance and its spiking is explicable [[01:07:10](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4030.3599999999997s)]
*  deterministically. There's stochasticity in the real brain, but we don't know that the single [[01:07:18](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4038.6s)]
*  neuron is intrinsically stochastic that way. Right, that way. We do know it's stochastic. [[01:07:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4044.92s)]
*  But yeah, but okay. So but then going back to, you started by saying that the major, [[01:07:30](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4050.84s)]
*  this is where it gets into non-biologically plausible mechanisms is that it doesn't use spiking. [[01:07:36](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4056.84s)]
*  Yeah. And actually, I think Blake Richards group has recently written to our rescue [[01:07:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4064.2799999999997s)]
*  with their paper on what is it? Brain-like learning with exponentiated gradients. [[01:07:52](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4072.84s)]
*  Hmm. Brain-like because it uses spiking? [[01:08:00](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4080.2s)]
*  So in their case, brain-like because it obeys Dale's law. [[01:08:04](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4084.2s)]
*  Oh. [[01:08:08](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4088.12s)]
*  So they'll have inhibitory neurons, which are negative and excitatory neurons, which are positive [[01:08:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4089.08s)]
*  and the signs will never flip. And they show- [[01:08:15](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4095.16s)]
*  So they still use rate, but- [[01:08:19](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4099.24s)]
*  Yeah, they're still using rates there. [[01:08:21](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4101.0s)]
*  Yeah. [[01:08:23](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4103.32s)]
*  And how realistic do I think that is? I don't really know. I mean, there's areas that could [[01:08:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4104.62s)]
*  use rate codes, but there's also too many experimental findings showing that precise timing [[01:08:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4117.34s)]
*  matters. So what it could be, and this is not an original thought to me. This is [[01:08:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4123.26s)]
*  coming from a computational brain and behavior paper. I can send you the name. It's from 2020. [[01:08:54](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4134.14s)]
*  It could be a prefix-free code. [[01:09:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4142.38s)]
*  Oh, what? Sorry. Say that again. [[01:09:05](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4145.1s)]
*  A prefix-free code. [[01:09:06](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4146.46s)]
*  Prefix-free. What does that mean? [[01:09:08](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4148.38s)]
*  So that means that once you send a certain pattern of spikes, and by certain pattern, [[01:09:10](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4150.46s)]
*  I mean the precise timing determines which code word it is. But once you've sent a certain pattern [[01:09:16](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4156.3s)]
*  of them, then that code word is over. So prefix-free means that no code word is a prefix of another [[01:09:23](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4163.5s)]
*  code word. So if I say ABAB, then that's either a full code word that now tells you something, [[01:09:32](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4172.22s)]
*  or there's no full code word that starts ABAB except the one I'm already sending. [[01:09:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4183.02s)]
*  So what would that mean? Is that just because of rate code? [[01:09:51](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4191.02s)]
*  Go ahead. Sorry. [[01:09:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4195.42s)]
*  So a rate code would say you listen over a certain period of time. [[01:09:56](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4196.3s)]
*  Right. Whereas the timing-oriented code... [[01:10:00](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4200.78s)]
*  You get N spikes. You divide by time t. [[01:10:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4202.0599999999995s)]
*  Yeah. Whereas a timing-oriented code is like you get a spike at time t. Now you think, well, [[01:10:06](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4206.3s)]
*  what comes next? Spike at delta t. Delta t prime. Delta t prime prime. Right? And you look for [[01:10:14](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4214.46s)]
*  very specific timing, like with musical notes. [[01:10:23](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4223.259999999999s)]
*  In your lookup table, you figure out, oh, I just received this particular message. [[01:10:25](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4225.82s)]
*  Yeah. I just received ta-ta-ta-ta. Gosh, has someone actually tried using third grade music [[01:10:30](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4230.22s)]
*  class on timing codes? But yeah, a prefix-free code would then be a timing-based code where you [[01:10:39](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4239.34s)]
*  say, once I've received a full code word, that's it. I know that I've received a full code word. [[01:10:48](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4248.3s)]
*  I can interpret the whole message. [[01:10:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4255.42s)]
*  Clean your cache and move on. [[01:10:58](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4258.22s)]
*  Yeah. Clean my cache and move on, exactly. But really, I mean, gosh. [[01:11:00](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4260.54s)]
*  On the other hand, that doesn't... See, this is the thing that bugs me is there's also evidence [[01:11:08](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4268.54s)]
*  that dendrites are sort of accumulating this precise spike timings into something more like [[01:11:15](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4275.02s)]
*  a continuous signal that gets fed up to the cell soma. So how can it be that there's precise spike [[01:11:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4284.94s)]
*  timing and there's dendrites that convert from spike timing to spike rate, more or less? [[01:11:31](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4291.099999999999s)]
*  I don't know that those are necessarily problems, right? I mean, so when you were going to say, [[01:11:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4297.66s)]
*  you know, is it a spike timing code or a rate code? Because we know that some things require [[01:11:42](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4302.299999999999s)]
*  precise spike timing, like the interaural differences, right? The underlying how owls [[01:11:48](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4308.38s)]
*  hear, locate sound, for example. Timing is very important, but maybe timing is not as important in, [[01:11:54](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4314.86s)]
*  I don't know, frontal cortex or prefrontal cortex or something. And it could be both. [[01:12:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4322.3s)]
*  Yeah. [[01:12:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4327.34s)]
*  Depending on what you're needing to accomplish, an organ like the brain is fairly complicated, [[01:12:08](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4328.14s)]
*  it turns out. And it might be implementing lots of that degeneracy you were talking about. That [[01:12:17](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4337.66s)]
*  could be the case in terms of how it computes. It's not maybe one or the other, but just depends on [[01:12:22](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4342.94s)]
*  what's needed. [[01:12:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4349.5s)]
*  Yeah, like, that's very, very possible that essentially... So actually, not only is that [[01:12:31](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4351.5s)]
*  possible, like that would go very well with, you know, some of our recent preprints that basically [[01:12:38](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4358.7s)]
*  say predictive coding is a much more cognitive computation that can take place in frontal areas. [[01:12:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4363.5s)]
*  You know, back to our glow paradigm, those global oddballs seem to get detected in frontal areas, [[01:12:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4370.7s)]
*  but not in lower sensory cortex. [[01:12:57](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4377.18s)]
*  Interesting. Okay. [[01:12:59](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4379.34s)]
*  So maybe the laminar cortical column, you know, is something like a big stack of universal [[01:13:00](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4380.78s)]
*  computational primitives that don't tell us much from just reading off the anatomy about what it is [[01:13:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4387.34s)]
*  doing. Oh, God. You know, if we broadcast this, the modular mind people are going to [[01:13:14](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4394.38s)]
*  crawl out from under the rocks. We spend so much time banishing them. [[01:13:20](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4400.9400000000005s)]
*  That's all right. That's all right. There's room for everybody. [[01:13:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4409.34s)]
*  One of the things I wanted to ask you about is... So you're mindful of, you know, what is and what [[01:13:32](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4412.06s)]
*  isn't biologically plausible in this. You think it's important if you're going to understand... [[01:13:38](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4418.62s)]
*  This sounds silly to say. If you're going to understand the brain that you need to [[01:13:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4423.66s)]
*  implement through a model, you need to implement something that is biologically [[01:13:46](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4426.78s)]
*  plausible, but you're willing to forego the spikes. [[01:13:51](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4431.02s)]
*  But so inevitably, any project is going to have hurdles. What hung you guys up the most [[01:13:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4435.0199999999995s)]
*  in getting this thing to work and or getting it ferried out properly? [[01:14:06](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4446.78s)]
*  So two big things. You know, the first time was when I tried to write out all those waiting rules, [[01:14:15](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4455.18s)]
*  essentially saying, like, how do you accumulate, you know, the weights from doing a dozen [[01:14:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4464.06s)]
*  successive updates to a random variable over a dozen passes? And I got something that looked [[01:14:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4469.660000000001s)]
*  really complicated and eventually just exceeded the numerical precision of floating point numbers [[01:14:36](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4476.46s)]
*  in a computer. And what I eventually did was just like have a meeting with how and talk out some [[01:14:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4484.06s)]
*  options. And he pointed out that one of them was essentially just cheating, forgetting the old [[01:14:51](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4491.26s)]
*  importance weights and just saying, you know, I start with some particles, that is, I start [[01:14:58](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4498.780000000001s)]
*  with some samples, I do a computation step on them. Now I have new samples, I'm going to do the same [[01:15:04](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4504.860000000001s)]
*  thing next time. I don't save any weights. And we ended up going with that because it turns out, [[01:15:10](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4510.7s)]
*  you know, once we like, both proved to ourselves that this was legal to do within all the rules [[01:15:16](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4516.94s)]
*  of the game, this just turned out to be the simpler thing that was able to work. [[01:15:23](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4523.34s)]
*  And so you're okay. I mean, so storing the weights over time maybe is not even as biologically [[01:15:31](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4531.099999999999s)]
*  plausible as throwing the curry. Yeah. Well, you said so there were two things that you said. [[01:15:35](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4535.82s)]
*  So the other one is that between the first preprint draft and the second one that represents our [[01:15:45](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4545.26s)]
*  camera ready, we added like this preconditioner that helps the optimization go in the right [[01:15:51](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4551.820000000001s)]
*  directions and respect the geometry of like the latent space and, you know, this very mathematical, [[01:16:01](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4561.26s)]
*  like technical itchy thing. And the thing is without that stuff doesn't work. And you just [[01:16:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4569.5s)]
*  don't perform very well on your test tasks. Now we did manage to rig this up in such a way that it [[01:16:17](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4577.98s)]
*  could be biologically plausible. You know, it's effectively like calculating a certain function [[01:16:25](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4585.099999999999s)]
*  of the prediction errors. So if the prediction errors are locally available, then this thing is [[01:16:31](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4591.9s)]
*  locally available. And you could even, you know, nod to the free energy principle and say, ah, [[01:16:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4597.0199999999995s)]
*  there's that precision of the prediction errors that these free energy guys are always on about. [[01:16:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4603.5s)]
*  But really it was just motivated by getting the damn thing to work. [[01:16:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4610.46s)]
*  In the end, you have to have a product, a working product, [[01:16:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4615.179999999999s)]
*  in the end, you have to have a product, a working product. [[01:16:58](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4618.78s)]
*  Yeah. And, you know, this is where [[01:17:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4622.62s)]
*  I forget which famous person said that, no, two famous people have said this. Okay. [[01:17:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4627.98s)]
*  Richard Feynman and Daniel Dennett have both, you know, said, if you want to understand it, [[01:17:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4633.0199999999995s)]
*  you've got to be able to build it. Did Dennett say that also? I mean, Feynman, [[01:17:18](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4638.46s)]
*  he said a version of that. Did he? Okay. Yeah. [[01:17:22](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4642.46s)]
*  Oh, no. [[01:17:25](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4645.9s)]
*  Feynman's is, I do not understand what I cannot. [[01:17:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4646.94s)]
*  Build. [[01:17:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4649.179999999999s)]
*  Build. [[01:17:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4649.82s)]
*  No. And then Daniel Dennett's is completely different. He actually said at one point, [[01:17:30](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4650.46s)]
*  AI keeps philosophy honest. That's what I was remembering. [[01:17:33](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4653.98s)]
*  Oh, that's interesting. [[01:17:39](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4659.419999999999s)]
*  Which is a whole other can of worms. So my mistake. But, you know, what I would say is, [[01:17:41](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4661.179999999999s)]
*  if you want to say that predictive coding is a thing that happens in the brain, [[01:17:48](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4668.94s)]
*  based on your experimental observations, then it should hypothetically be possible to build an [[01:17:52](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4672.86s)]
*  algorithm that does predictive coding and actually works for, you know, some of the toy tasks that we [[01:17:57](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4677.9s)]
*  use in AI, which are still vastly more simplified than the tasks we use in neuroscience, or rather, [[01:18:04](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4684.86s)]
*  the task of the brain. You know, an AI image generating network does not have saccades. [[01:18:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4693.339999999999s)]
*  Well, unless it's one of Rouse, in which case it does have saccades now, but, you know, that's very [[01:18:20](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4700.94s)]
*  new for AI and completely trivial for neuroscience. And so I think you have to be able to build up AI [[01:18:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4706.78s)]
*  to the point that it's able to do things that are trivial for neuroscience before [[01:18:33](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4713.58s)]
*  you can really say, oh, a computational theory is viable now. No, it has to do the things that are [[01:18:38](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4718.22s)]
*  most trivial for the brain. [[01:18:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4724.54s)]
*  Alright, so then I have two kind of broader questions for you before we [[01:18:45](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4725.9s)]
*  end our conversation today. And one, just going off of what you just said, and I've sort of been [[01:18:54](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4734.7s)]
*  building up to this. Do you need to understand the brain or brain processes or implement [[01:19:00](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4740.139999999999s)]
*  things in a similar manner to how the brain does things to build [[01:19:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4749.820000000001s)]
*  the best artificial intelligence? Do we need to mimic the brain? And at what level, if so? [[01:19:16](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4756.3s)]
*  So I think that depends on how you define. I'm sorry to be philosophical about this, [[01:19:25](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4765.02s)]
*  but it really does depend on how you, it depends on how you define artificial intelligence, right? [[01:19:31](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4771.900000000001s)]
*  Geez. Yeah. [[01:19:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4777.66s)]
*  And I don't like to commit to a definition of that at all, because what I personally want to do is [[01:19:38](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4778.94s)]
*  understand the brain. That is the motivation for me. I want to understand the thing that actually [[01:19:46](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4786.78s)]
*  exists, try to draw, so to speak, laws and principles from it. And then maybe I could [[01:19:52](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4792.62s)]
*  engineer something with those in the same way that you can engineer a steam engine with Newton's laws [[01:20:01](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4801.1s)]
*  and thermodynamics, right? But you do have to do, in my view, the interesting part is to do the [[01:20:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4807.18s)]
*  fundamental science before the engineering. Now, if you are engineering first, then an intelligent [[01:20:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4813.42s)]
*  task is whatever the heck you have a benchmark for. And sort of there's this alternation between [[01:20:23](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4823.1s)]
*  making a harder benchmark and beating the current benchmark. And in that case, [[01:20:32](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4832.46s)]
*  do you really need the brain? Well, no, you need to understand your benchmark task. [[01:20:38](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4838.14s)]
*  Like there's a lot of tasks where if you have a very deep understanding of the task itself, [[01:20:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4843.42s)]
*  you don't necessarily have to understand how the brain would solve that task. [[01:20:48](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4848.06s)]
*  But there's all the talk of AGI, right? In the AI world, we're going to get the AGI by next [[01:20:53](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4853.74s)]
*  Tuesday, or it's going to be the Tuesday after that. No, and then it's like five years. No, [[01:20:59](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4859.34s)]
*  it's just 20 years. I mean, I personally, I feel like going away from definitions again, [[01:21:03](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4863.66s)]
*  I don't know what AGI is. But I think that the humans are the wrong benchmark. It's like just, [[01:21:10](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4870.54s)]
*  what's the right analogy? All we're doing is like staring at ourselves in the mirror. And [[01:21:17](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4877.34s)]
*  yeah, that's real intelligence. It's only because it's us. We think we're great, I guess. [[01:21:20](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4880.7s)]
*  Oh, there I totally agree. Because what is it we got? Optimal chess playing at superhuman level, [[01:21:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4886.62s)]
*  maybe was that a decade before we got neural networks that could pass image net [[01:21:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4897.58s)]
*  classification at a human level? Yeah, half a decade maybe. I think it was 2007 or so. [[01:21:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4904.94s)]
*  At the time, chess was sort of the king task where we thought if we understand how to play chess, [[01:21:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4910.7s)]
*  we understand cognition computationally, or we've built intelligence. And then, [[01:21:59](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4919.5s)]
*  well, I don't even have to say, and then there's a cliche for it, Moravec's paradox. [[01:22:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4927.74s)]
*  Yeah. [[01:22:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4933.58s)]
*  I am very much a Moravec's paradox person where I say, understand embodiment first, [[01:22:16](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4936.46s)]
*  sensory motor stuff first, feeling first. And then maybe later in retrospect, you'll turn around and [[01:22:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4944.7s)]
*  say, here's all these normative principles we derived from our empirical study. And we understand [[01:22:34](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4954.78s)]
*  now how those tell us what intelligence is and how to build it. But the term AGI almost feels like [[01:22:40](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4960.86s)]
*  I admire the people, I admire the sheer ambition of the people who are trying to do that and going [[01:22:56](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4976.38s)]
*  to conferences like the AGI conference. And the other angle on it is, unfortunately, [[01:23:04](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4984.14s)]
*  that I do think in the era of large language models, there's been a tendency to fool ourselves [[01:23:12](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4992.46s)]
*  and define AGI down. So that instead of being a name for something we don't understand and have to [[01:23:18](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=4998.54s)]
*  come to understand only through working at it over time, it's become a name for something that we say [[01:23:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5006.14s)]
*  has happened. Like the latest model from wherever is AGI. [[01:23:35](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5015.42s)]
*  Is it AGI? It's got all of the... Yeah. [[01:23:42](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5022.06s)]
*  Right. And it's like, okay, but that's because it talks. That's because it talks. And we know [[01:23:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5024.620000000001s)]
*  the Eliza effect. We know that if you talk and talk and talk, people will project personhood onto [[01:23:49](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5029.74s)]
*  the words. And to be fair to people, prior to the invention of LLMs, 100% of all linguistic stimulus [[01:23:56](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5036.86s)]
*  we ever received came from other people. Well, except maybe for like bad Markov chains and Eliza [[01:24:04](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5044.94s)]
*  and that sort of thing. The overwhelming super majority. So for an optimal probabilistic [[01:24:12](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5052.38s)]
*  reasoner, if you heard language, then the rational conclusion was that there's a person. [[01:24:18](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5058.54s)]
*  Well, we also know that some of us aren't that bright. For example, I've said, I think only ever [[01:24:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5064.86s)]
*  more of VEC. And you say more of VETCH. Which is it? [[01:24:32](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5072.46s)]
*  I have no idea. Oh, really? [[01:24:36](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5076.06s)]
*  I'm so embarrassed now. Oh, I'm sure I'm wrong. [[01:24:38](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5078.22s)]
*  This is great. I'm sure I'm wrong. Anyway, that's the paradox that [[01:24:41](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5081.74s)]
*  it turns out that it's easy to build. Is this the moment for my thesis defense [[01:24:45](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5085.82s)]
*  that I blanked from my memory where someone like Mitch corrected me on this? [[01:24:49](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5089.0199999999995s)]
*  I don't know. But anyway, that paradox is that the things that we think are hard to do, like chess, [[01:24:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5095.5s)]
*  turn out to be easy. And the things that we think are easy to do, like [[01:25:01](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5101.98s)]
*  walking on two legs. [[01:25:06](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5106.7s)]
*  Like a weight, like B, yeah, B or like a waiter balancing a tray, [[01:25:08](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5108.7s)]
*  moving, walking through a restaurant. Oh man, don't list that as easy. Talk to [[01:25:12](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5112.86s)]
*  a waiter before you call that easy. That's hard. [[01:25:17](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5117.26s)]
*  Well, what I mean are the sensorimotor everyday things, the continuous sorts of behavior. [[01:25:19](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5119.9800000000005s)]
*  Yeah, but some things are hard even for embodied human beings. And that's one of them. [[01:25:25](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5125.5s)]
*  Yeah. Go get a friend who works in the food service and ask them. [[01:25:29](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5129.82s)]
*  I've been a server. I've been a waiter. Okay, that was the poor example. See, again, [[01:25:33](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5133.74s)]
*  I say more of that. I give bad examples. What do you do? Maybe not of us. [[01:25:39](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5139.099999999999s)]
*  I'm sorry. I'm not I'm not supposed to be shaming you on your own show. [[01:25:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5144.139999999999s)]
*  Yeah. What are you shaming me on? [[01:25:47](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5147.58s)]
*  Sorry. Okay, you've been a waiter. Yeah, it's easy for you because you practiced. [[01:25:50](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5150.86s)]
*  I also have ungodly balancing talent. No, that's not true. All right. But I do have another [[01:25:56](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5156.46s)]
*  question because you are interested in how did you phrase it earlier? Not consciousness, but [[01:26:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5162.86s)]
*  feeling. Why anything feels the way it does, right? Another way of [[01:26:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5167.98s)]
*  to say it is like just subjective experience in general or affect, I guess. [[01:26:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5173.42s)]
*  Affect. The affective component of it. Why do some things feel the classical dimensions of [[01:26:18](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5178.139999999999s)]
*  core affect are valence and arousal? So why do things feel pleasant versus unpleasant? [[01:26:24](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5184.62s)]
*  Why do things feel exciting versus relaxing? Okay, you could say or arousing versus sedated. [[01:26:31](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5191.42s)]
*  So, okay, so my question then is, and I was thinking about a Neil Seth, who ties predictive [[01:26:40](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5200.7s)]
*  coding into consciousness and that that's going to solve consciousness, essentially. [[01:26:46](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5206.0599999999995s)]
*  What do you think about sort of maybe that, but also, it's real predictive coatings relation, [[01:26:51](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5211.66s)]
*  possible relation to affect the way you just described it. [[01:26:58](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5218.14s)]
*  So I have to say, I think the second one, the relation to affect through interoception, [[01:27:03](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5223.02s)]
*  homeostasis, allostasis, this stuff is a lot easier to establish than anything about consciousness. [[01:27:10](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5230.46s)]
*  And that's why I've sort of said like, well, I'm not going to touch consciousness with a [[01:27:19](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5239.34s)]
*  10-foot pole. It's much too hard. Like I'm not, everyone's a little bit of a philosopher, [[01:27:23](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5243.9s)]
*  but I'm not very much of a philosopher, so I'm just not going there. As to the connection [[01:27:31](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5251.82s)]
*  between predictive coding and consciousness, I mean, here's one of the reasons I think [[01:27:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5257.339999999999s)]
*  consciousness is so hard to think about is that, oh, what is there? This like classic thought [[01:27:41](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5261.74s)]
*  experiment about consciousness. Like couldn't you imagine a philosophical zombie who has the same [[01:27:47](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5267.98s)]
*  input-output mapping and the same observable behavior, possibly even the same electrophysiological [[01:27:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5275.099999999999s)]
*  readouts as a real person, but isn't conscious. But what about the affect aspect? Then they [[01:28:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5282.459999999999s)]
*  wouldn't have affect either, right? Right. I mean, if they don't have consciousness, it [[01:28:09](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5289.099999999999s)]
*  possibly makes sense to say, right. Like, well, that's what I would ask is, does a philosophical [[01:28:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5293.9s)]
*  zombie have a predictive internal model? Do they have interoception? And I asked myself, [[01:28:18](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5298.94s)]
*  can I imagine someone who has the same internal states and control systems at a physical level, [[01:28:28](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5308.78s)]
*  but doesn't experience them at all? The answer is no. [[01:28:37](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5317.58s)]
*  Yeah, the answer is just no, because I'm like, but there's a latent variable there. There's like, [[01:28:45](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5325.18s)]
*  representations and computations going on. There's internal states maintained over time and [[01:28:51](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5331.82s)]
*  internal dynamics. I can't imagine how there could be no one home. And that's like I said, [[01:28:57](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5337.9s)]
*  I don't study consciousness because I recognize that this is very likely a limitation of my [[01:29:06](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5346.54s)]
*  imagination rather than some kind of answer. It's just the way my intuitions work. And so, [[01:29:11](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5351.74s)]
*  I prefer to be at least on the engineering end where I can bang an intuition against an experiment [[01:29:20](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5360.62s)]
*  that doesn't work and bruise it until it's softer and can be remolded into another intuition. [[01:29:28](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5368.06s)]
*  Now that you're doing experimental work, how do you think about the role of intuition? Sorry, [[01:29:33](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5373.9s)]
*  I know this is another question and I've got to, I actually have to go in a minute, but [[01:29:40](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5380.379999999999s)]
*  do you feel that your intuition has served you better from the computational world, [[01:29:44](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5384.0599999999995s)]
*  theoretical world or the experimental world? [[01:29:48](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5388.78s)]
*  Because it all comes down to that. To make any progress, you have to make a guess. [[01:29:54](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5394.299999999999s)]
*  And that's from intuition. [[01:29:58](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5398.700000000001s)]
*  Actually, I would say I don't know a good way to put those two together right now. I'm sorry, [[01:30:01](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5401.5s)]
*  I just don't. The intuitions from both ends because maybe if I was doing experiments with [[01:30:06](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5406.9400000000005s)]
*  naturalistic behaviors, I would develop more of an intuition for how to let the experimental end drive. [[01:30:15](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5415.02s)]
*  But with the highly constrained experiments, I get an intuition for the task and the setup [[01:30:23](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5423.34s)]
*  and the way that a particular data center animal might behave, but not one for how does, [[01:30:32](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5432.78s)]
*  not one for how do I pass from these spike trains to psychology. [[01:30:41](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5441.900000000001s)]
*  So like the mentalizing I could do about the animal. I have no bridging intuition there [[01:30:49](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5449.34s)]
*  whatsoever. Well, see, now that I do quote unquote naturalistic experiments, meaning we just, [[01:30:54](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5454.62s)]
*  there's just a mouse running around in a box and we measure, measure, measure, measure. And [[01:31:02](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5462.06s)]
*  now we're trying to relate neural activity to that ongoing behavior, which is continuous. [[01:31:06](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5466.7s)]
*  They groom slightly differently. They move their paws slightly differently. Are we going to call [[01:31:12](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5472.3s)]
*  that the same groom as the other one? How do we define that? My intuitions about experimental [[01:31:16](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5476.86s)]
*  neuroscience, which were forged in that controlled constrained environment, I think are not serving [[01:31:22](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5482.78s)]
*  me well. So I'm trying to build new intuitions. Yeah, like, you know, if there's one thing I've [[01:31:28](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5488.7s)]
*  learned in my life, it's really the limits of raw intuition and how you just kind of have to [[01:31:35](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5495.179999999999s)]
*  bang up against experience long enough to start developing, you know, what you, let's end on a [[01:31:39](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5499.9s)]
*  pun, you know, call it posterior intuition rather than prior intuition. Right. Exactly. You have to, [[01:31:45](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5505.98s)]
*  you have to take action and get, get, update your posterior in the way that you phrased it. Yeah. [[01:31:51](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5511.58s)]
*  Yeah. Eli, did we miss anything? What we went half-hazard, we went quite technical there. [[01:31:59](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5519.0199999999995s)]
*  We remained out in the forest some, is there anything crucial that we missed that you want to end on? [[01:32:06](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5526.379999999999s)]
*  Oh, actually, yes. So there's this thing I always keep in my Twitter bio, [[01:32:10](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5530.78s)]
*  abolish the value function. If I'm doing a podcast, I should tell people what that means. [[01:32:18](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5538.46s)]
*  Yeah. What does that mean? That's a great way to find out. [[01:32:23](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5543.66s)]
*  Okay. So that means that at one point in grad school, Jordan Terrio, who will probably listen [[01:32:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5546.78s)]
*  to this, hi, Jordan, you know, recommended this book to me entitled More Heat Than Light by Philip [[01:32:34](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5554.0599999999995s)]
*  Muravsky. Okay. And Philip Muravsky is like a very philosophical leaning, [[01:32:40](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5560.22s)]
*  part economist, part historian. And he wrote this whole book about the analogy between [[01:32:48](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5568.3s)]
*  energy and the conservation of energy and economic behavior. So all of this notion of like, [[01:32:55](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5575.5s)]
*  there's an economic agent who maximizes utility or minimizes cost. That's the value function. [[01:33:03](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5583.26s)]
*  Yeah. All that stuff is the value function. And what he pointed out is that essentially, [[01:33:10](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5590.860000000001s)]
*  if you think like a rigorous physicist, the analogy is bunk. Like it's not, you know, [[01:33:16](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5596.46s)]
*  economic value is not a conserved substance. You know, people produce things that are valuable [[01:33:23](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5603.66s)]
*  and then consume them. The amount of value is not a fixed constant number that stays the same all [[01:33:31](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5611.5s)]
*  through all of this. Well, thinking like a rigorous physicist, would it be called an emergent [[01:33:38](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5618.62s)]
*  property of production then? I mean, like, I'm not sure what Muravsky would say there. But his [[01:33:43](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5623.74s)]
*  point was that in order to get all the math that was imported into economics, and then by the way, [[01:33:52](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5632.46s)]
*  into cognitive psychology, into reinforcement learning, into optimal control, into all these [[01:33:59](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5639.9s)]
*  things that we use in psychology and neuroscience, imported from economics. And to get that from [[01:34:06](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5646.7s)]
*  physics to economics in the first place, you have to assume a conserved substance. So a conserved [[01:34:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5653.099999999999s)]
*  quantity which represents a physical substance on which you can then have a gradient flow, [[01:34:21](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5661.26s)]
*  a certain kind of dynamical system. So absent that, where do we go? What is the result of [[01:34:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5666.3s)]
*  abolishing the value function? Right. And so if that's just the wrong metaphor, then I think we [[01:34:31](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5671.82s)]
*  need to go into a much more control theoretic theme of mind where some signals represent references [[01:34:38](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5678.219999999999s)]
*  and they can be directly compared to input signals from the bottom up by a comparator. [[01:34:45](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5685.0199999999995s)]
*  Yeah. And then when I shift from- [[01:34:52](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5692.219999999999s)]
*  Folks, yeah, I've come around to the physical theory things as well. [[01:34:54](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5694.299999999999s)]
*  So then when I shifted my point of view from all of these decision-making tasks, they're about [[01:34:58](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5698.700000000001s)]
*  grabbing more value, imaginary gold coins, like in Super Mario. [[01:35:07](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5707.02s)]
*  Neuroeconomics. [[01:35:12](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5712.22s)]
*  Neuroeconomics. [[01:35:13](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5713.18s)]
*  Yes. Versus they're about measuring the distance between a desired outcome and a actual outcome, [[01:35:14](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5714.700000000001s)]
*  much more perceptual control theory. [[01:35:23](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5723.660000000001s)]
*  Where the reference signal is internally generated by which you compare, which is [[01:35:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5726.0599999999995s)]
*  amenable to predictive coding. [[01:35:30](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5730.78s)]
*  Yeah. And then I thought, okay, well, now we've gone from substance to distance. [[01:35:32](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5732.38s)]
*  These are completely different metaphors. Distance is the superior one because as soon as you set up [[01:35:36](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5736.78s)]
*  a mathematical model, you can measure the distance in the parameter space. [[01:35:42](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5742.78s)]
*  And by the way, that's actually the difference between reinforcement learning and active [[01:35:48](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5748.62s)]
*  inference in all of that free energy literature is that the active inference people are saying, [[01:35:52](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5752.78s)]
*  let's specify desired outcomes as target probability distributions, then measure the [[01:35:59](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5759.259999999999s)]
*  relative entropy distance from one to the other, and then just try to get closer to the desired [[01:36:05](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5765.9s)]
*  outcome distribution. That's abolishing the value function from substance to distance. [[01:36:14](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5774.7s)]
*  All right, Eli. I appreciate your time. Look forward to more work coming out and good luck [[01:36:20](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5780.3s)]
*  with the experiment. I'm sure we'll be in touch, but good luck with the work. We're learning more [[01:36:26](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5786.78s)]
*  experimental research. [[01:36:31](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5791.1s)]
*  The latest analysis seems to draw a very different conclusion than the ones we pre-printed. [[01:36:34](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5794.06s)]
*  Shocking. [[01:36:39](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5799.1s)]
*  We're going to have to reconcile those. [[01:36:39](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5799.820000000001s)]
*  All right. I know you have an office mate there also, need to get back in the office. [[01:36:42](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5802.3s)]
*  Tell them thank you for letting me take up some of your time. Thanks for coming on. [[01:36:47](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5807.34s)]
*  Yep. Thank you. [[01:36:52](https://www.youtube.com/watch?v=Ex35cwt0Qos&t=5812.06s)]
