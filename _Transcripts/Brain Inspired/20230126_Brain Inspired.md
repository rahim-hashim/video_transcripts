---
Date Generated: April 20, 2024
Transcription Model: whisper medium 20231117
Length: 5334s
Video Keywords: []
Video Views: 2953
Video Rating: None
---

# BI 159 Chris Summerfield: Natural General Intelligence
**Brain Inspired:** [January 26, 2023](https://www.youtube.com/watch?v=yxNqgQQ0PLE)
*  Discussions around how we should build AI that don't account for what we're trying to build are essentially pointless.
*  There is no right way to build AI.
*  An anomaly arises because what we have is we're still in the mindset where the goal is to build to recreate a human.
*  But suddenly we're like in the natural world and then it's like, okay, so we want to recreate a human in the natural world, right?
*  And then this suddenly starts to be a bit weird.
*  People in machine learning and AI research, particularly people who've entered the field more recently, say things like, it's not clear what we have ever learned from the brain, back from the study of them.
*  So this is, you know, kind of in my view, this is...
*  This is Brain Inspired.
*  Good day to you. I am Paul. My guest today is Christopher Summerfield.
*  Chris runs the Human Information Processing Lab at University of Oxford, and he's also a research scientist at DeepMind.
*  You may remember him from episode 95 with Sam Gershman when we discussed ideas around the usefulness of neuroscience and psychology for artificial intelligence.
*  Since then, he has released his book, Natural General Intelligence, How Understanding the Brain Can Help Us Build AI.
*  And in the book, Chris makes the case that inspiration and communication between the cognitive sciences and AI is hindered by the different languages each field speaks.
*  But in reality, there's always been and still is a lot of overlap and convergence about ideas of computation and intelligence.
*  And he illustrates this using tons of historical and modern examples.
*  So I was happy to invite him back on to talk about a handful of the topics in the book, although the book itself contains way more than we discuss.
*  You can find a link to the book in the show notes at braininspired.co slash podcast slash 159.
*  Thanks for listening. Here's Chris.
*  I was just looking it up and it was actually almost a year ago today that we spoke last.
*  You were on with Sam Gershman and it was mid-January.
*  So I was just looking it up because I was thinking, what episode was that? When was that?
*  And since then, you have published this bright, shiny new book, Natural General Intelligence.
*  And I was in the preface to the book.
*  You write that it took you 10 months to write the book.
*  And this is there's so much in in the book that that's a blistering speed, it feels like, to write a book.
*  But you also mentioned, hey, AI is, you know, increasing, advancing at a super rapid pace.
*  And so, one, how did you write the book that fast? And two, how much worry did you have that it would be immediately outdated, essentially, the AI facts in it?
*  Right. So neuroscience, not a neurosciences, right?
*  Psychologically not advancing so fast, AI advancing very fast.
*  So you didn't have to worry so much that you'd be outdated in your neuroscience.
*  Yeah. Thanks for those questions. Yeah. Well, I didn't feel very fast when I was writing it.
*  I can tell you, maybe, maybe I don't I don't know how long it takes people to write.
*  But yeah, I think one of the reasons I found it relatively easy to to write, at least once I got going, was because some of the material is part of a course which I teach.
*  So the sort of structure of the book and the kind of the conversations and the arguments and the things that I felt I needed to explain was sort of present, you know, kind of in my mind.
*  So yeah, that's that's that's one reason.
*  And, you know, kind of also, I've completely incidentally, you know, I just found that actually I really love writing.
*  I think, you know, kind of probably much better at writing than I am actually doing other aspects of science.
*  I really enjoyed it. And, you know, I the synthetic process of, you know, kind of getting those ideas down and trying to make sense of them was personally like hugely enriching.
*  I'm sure many other people have that experience, too.
*  In terms of the book being out of date. Yeah. So obviously, I would love people to read it.
*  It's not out of date, by the way.
*  I mean, you know, kind of I would love people to read it. But the neuroscience, of course, is not out of date and the neuroscience, you know, kind of I still feel like neuroscientists are to some extent, you know, this this ideas come out in machine learning and AI research.
*  And it takes a while for them to percolate through to the first typically to the computational community and then out into the sort of wider communities.
*  And that process, you know, kind of some of the models and ideas which are described in the book will be, you know, kind of probably news to a lot of people working in mainstream neuroscience and certainly to my kind of, you know, the audience that I had in mind while I was writing, which is sort of like undergraduates who are interested in cognition and computation in the brain.
*  So that's definitely true. But in terms of AI research and like, you know, kind of the sort of what's new, you know, sorry, guys, but, you know, it was out of dates.
*  It was out of date three months after I submitted it. Dramatic research.
*  I actually, you know, kind of OUP, you know, OUP are actually a little bit faster to turn around books than some publishers.
*  But even so, you know, kind of by the time it was due to go to production, which was I think in July 22, like three or four months after I submitted it, the they said, well, hey, do you want to update it?
*  Because has anything changed?
*  That's what I was going to ask.
*  I need to rewrite the whole thing. Sorry.
*  No, but did you? Were you were you in your, at least in your mind?
*  Were you thinking, do I need to go and see if a new model has been released and updated? Or did you just say, screw it, I'm going to, this is what this is what it is.
*  Well, I mean, you know, kind of the major thing.
*  So obviously GPT-3 was out while I was writing the book and I and as was the initial DeepMind's initial model.
*  So Gopher, their language model was released while I was still writing the book.
*  But both were well, the language models, you know, between the time when I submitted and the time when it went to production, we went from basically having like, you know, a couple of language models to having a whole explosion of startups and so on.
*  The other thing that changed dramatically was text to image generation.
*  So when I was writing the book, there were a bunch of models.
*  So there was the first version of Dali and then there was Glide, which was this sort of, I can't remember.
*  I can't keep up. Yeah, I don't. Yeah.
*  I think I can't remember if that was brain. No, I can't. I probably should know this, but I can't. Anyway, these early models, which, you know, everyone was like, wow, that's incredible.
*  But then compared to what came later to what, you know, Dali 2 in particular, and then the fusion, and now we have, you know, they were, you know, they were a bit rubbish by comparison.
*  So, you know, kind of I had to go in and update all the figures in the book because I'd had these figures of like, wow, you can do with text to image generation.
*  And, you know, if they now what we can do is much better.
*  It's almost like we need to change the format and books need to be digital where you can just plug in the latest creation, right?
*  Figure 10.2 and it just refreshes every month or something.
*  Yeah, like in Harry Potter, the use is animated. Yeah, I guess that would be great.
*  Yeah, so, okay, so I didn't read the subtitle of the book. The title is natural general intelligence and the subtitle is how understanding the brain can help us build a eye.
*  And throughout the book, you talk about the all of the principles in a eye that can be kind of mapped onto some of the things that we have learned throughout the years in neuroscience.
*  But of course, in like the industry world in a lot of people are fond of bragging that we don't need to pay attention to any natural intelligence.
*  And look, we did this on our own and you're actually learning from us. So sometimes it was the wording in the book was, you know, I think you use the word inspired a few times and sort of mapped them together.
*  But it wasn't clear always like whether that the artificial intelligence advances were, you know, how much attention they were actually paying to the natural intelligence research, right?
*  So I don't know if you just want to comment on that, but because there are so many different like when you put them all together, there are so many different ways in which they kind of can map onto each other.
*  But is it I had the sense that AI world really was proud about not having paid attention to in my world natural intelligence world.
*  Do you think that's the case? Or do you think that I know in your world in like the deep mind world and in your neuro AI kind of world, that is a celebrated aspect.
*  But in AI world writ large, do you think that people feel the same?
*  Yeah, there's so much to say here. Let me let me see if I can order my thoughts on trying to answer the question by part.
*  Obviously, mine aren't.
*  So, so the first the first thing to say is that let me answer a question which you didn't ask, which is about the converse direction. Right. So has AI been useful neuroscience unequivocally? Yes. Right.
*  So, you know, kind of our field over the past few years has been dramatically invigorated by this kind of know what people call neuro AI or I don't know they have different different terms for it.
*  But yeah, the advent of, you know, connectionist models in the form of, you know, deep learning architectures recurrent architectures, increasingly deep RL, you know, kind of like all of these ideas percolating into neuroscience has been great for the field.
*  And it's, you know, it's sparked some interesting debates around, you know, kind of what are we trying to do in computation?
*  You know, do we care about how much do we care about interpretability versus predictive validity for our models?
*  You know, kind of what is an you know, what is an explanation of a neural system if it has 100 million parameters, those kinds of questions, which have been very rich, I think good for the field to have those.
*  And, you know, kind of just computational ideas flowing into neuroscience, and that's, that's great. And we can talk more about that. But let me try and answer the question that you did ask.
*  So, you asked about what people in machine learning and AI research think about the role of neuroscience or cognitive science and its relevance for their research.
*  So there are many, many answers to this question, because there are many diverse viewpoints. Let me let me try and answer the first tackle one of them.
*  And this is something that I hear quite often. So sometimes people in machine learning and AI research, particularly people who've entered the field more recently, say things like, it's not clear what we have ever learned from the brain, from the study of.
*  So this is, you know, kind of in in my view, this is a view. This is an opinion which does not adequately reflect the influence that neuroscience has had on AI historically.
*  So historically, you know, from the very first symbolic AI models, right, which maybe today are somewhat out of fashion, but, you know, kind of the fields of of psychology and the native cognitive science and neuroscience and AI research have been like, you know, kind of intricately intertwined in their goals.
*  You know, to sort of understand thinking and build a thinking machine. And then as we move into the era of connectionist models and then deep learning, you know, obviously the structure of these models is inspired by neuroscience in the sense that, you know, kind of network, they involve information processing in networks of neurons,
*  be it simplified networks of neurons, which obey many of the principles that we've learned about from neuroscience and the people who built the first successful models were either trained in psychology on neuroscience or explicitly, you know, kind of have articulated how those ideas inspired them.
*  And that continues to some extent up to the present day. So, you know, if you think about the latest architectures, you know, kind of LSTM models, recurrent neural networks and LSTM models, you know, obviously have a very clear biological correlate or biological.
*  I mean, I don't know if the if you're going to have a must would say that sorry.
*  Schmidt, you have a must as a philosopher. He didn't build any and your own networks as president.
*  But you better get it right with Schmooch Huber too, because he'll let you know. Right. If you don't, the comparison to have a must is flattering.
*  Anyway, OK, I don't know if Schmooch Huber would say that he was inspired by neuroscience, but it's clear that those, you know, kind of there is a there is an analogous current of thought in psychology neuroscience about how working memory happens, which is really, you know, really important.
*  And then if you think about, you know, even up to the current day, right, think about meta learning, meta learning, a really powerful approach, you know, kind of directly inspired by ideas about prefrontal cortex and basal ganglia in a paper whose first and senior author were both neuroscientists.
*  You know, I think that that historical trend is there and it continues to this day.
*  Now that doesn't mean that people in machine learning and AI research always appreciate that.
*  There are so many different things. I mean, like I wanted to jump in when you said LSTM because you tell the story about the mapping of LSTMs with the basal ganglia and the gating function account of basal ganglia.
*  And actually, I don't know that I had even appreciated that connection.
*  I mean, I know that LSTMs, of course, anytime you have a new model in AI, the neuroscience world kind of grabs it and says, maybe this is a model for how the cortical column works, which has been done with LSTM also.
*  But so I have like, you know, tons of notes from your book and we could talk and we will talk about tons of topics.
*  But to get us in to that, I'm kind of wondering, you know, along the process of you writing it, even though you wrote it at such breakneck speed, did you change your mind about anything or did you come to appreciate anything, you know, that stands out in your mind that you maybe you thought this way?
*  And then, well, that's what changing your mind is. You thought this way about something.
*  And then as you wrote about it and clarified your thoughts and learned more information, you learned that maybe you should think about it a different way, et cetera.
*  Yeah, I think probably not globally. I think, you know, kind of really.
*  But I think, you know, the book does not, I didn't set out to kind of really espouse one viewpoint or opinion, right?
*  A lot of science books, including the ones, you know, many that I know and love, you know, what they do is they use a particular viewpoint as a way in to expose some problem.
*  And, you know, they make the book compelling through, you know, how compelling their argument is in favor of this view, right?
*  You know, I could give you many examples. I didn't set out to do that, right?
*  I wanted to give something which was a bit more balanced and I wanted to give voice, you know, in debates around like, how should we build AI, right?
*  Like these perennial questions of like, you know, what are the constraints that we need?
*  Do we need really explicit constraints like some people have argued, you know, for symbolic style or variable binding style, variable binding style constraints?
*  I tried to give voice to like both sides of the argument and often to try and point out where I thought maybe they were sort of missing each other, missing the point that each other was making.
*  And, you know, kind of I often feel that my viewpoint, maybe, you know, kind of this is this is just me, but I often feel that my viewpoint isn't sort of like, oh, you know, kind of where there's two kind of different camps, it's like, oh, I'm strongly aligned to A or I'm strongly aligned to B.
*  My viewpoint usually tends to be, you know, when someone from A is evaluating ideas from camp B, this is where they miss the point and vice versa, right?
*  And, you know, my view is often about where to opposing positions converge or diverge rather than, you know, necessarily strongly espousing one or the other.
*  Is part of that talking past each other?
*  How much of it is talking past each other and how much of it is a true staunch divide?
*  Yeah, I think there's a lot of talking past each other and there's a lot of semantics involved, right?
*  I mean, in the debate, you know, in the later chapters of the book, I make reference to a very, very long standing debate, which relates to, you know, the question of, as I mentioned, the question of whether we need constraints on our models, which are quite explicit.
*  So, you know, kind of cognitive scientists over throughout the past 20 years, I mean, of course, before as well, but especially throughout the past 20 years have, you know, taken issue with the undifferentiated nature of the information processing in connectionist or deep learning models.
*  And they've said we need things like explicit constraints.
*  Those might be, you know, kind of, they might be mechanisms which, you know, kind of deliberately discretize information in ways that make amenable to, to like composition or, you know, kind of the sorts of discrete or rule based processing that we have traditionally thought of as part of what humans do.
*  And, you know, kind of those people have, have very often kind of advocated for those types of constraints without asking the question of whether they implicitly emerge out of the types of architectures that we build anyway, right?
*  So, you know, kind of it's clear that neural networks can learn to perform, you know, various classes of rule based inference.
*  And actually they can do so in, in highly structured rate ways under the right constraints and with the right inductive biases.
*  And, you know, kind of very often people from the symbolic camp have sort of said, you know, rather than saying, oh, well, actually that validates my point that, you know, kind of this form of computation is important, right?
*  Instead, they've tried to focus on, you know, identifying corner cases where neural networks fail and saying, well, you know, if you haven't done it right.
*  And, you know, I think that's made the debate a little sterile.
*  On the other side of the argument, you know, kind of there's a, there's a sort of unpleasant kind of bradagio in amongst machine learning researchers that is like, you know, kind of a sort of muscular tendency to assert superiority because their models are, you know, kind of,
*  they're, they're more effective in an engineering sense, right? Because they actually do stuff.
*  And, you know, to be really dismissive of ideas from, from cognitive science.
*  And, you know, I think that's also equally unhelpful because, you know, as, as Jan LeCun, who's, you know, someone who's advocated for, of course, the deep learning approach, you know, he's been not only one of the founders of that movement, but also kind of a spokesperson for, for that opinion.
*  You know, as he says, what does he say? Oh, no, I've lost my train of thought.
*  Oh, it's terrible. He says more than a few things.
*  He says more than a few things.
*  Is this pertaining to the nativist empiricist divide, the symbolic versus?
*  Yeah, sorry. Yeah.
*  Yeah, as he says, as he says, like, you know, kind of machine learning researchers are in the very business of building inductive biases into their model, right?
*  You know, kind of those cognitive scientists who've argued for a kind of a nativist, what they've, what's been billed as like a nativist approach, right? That we need to build in these constraints.
*  You know, what LeCun has said is, well, you know, kind of machine learning advances through innovation on those, those very inductive biases.
*  And, you know, that's, that's literally, if you're a machine learning researcher, that's literally your day job is to find those inductive biases. And I just think that's true.
*  Right.
*  So the conversation.
*  Yeah, the conversation has become, I think, you know, kind of circular and a little boring sometimes.
*  So you see, you point to inductive biases, biases and talk about them at some length as, you know, this middle ground, right? That the symbolic AI folks should acknowledge that there is built in structure.
*  The nativists, right? That there is built in structure.
*  But it's not just one big undifferentiated neural network.
*  And on the other hand, I suppose the connectionist, which, you know, as you say, Yon LeCun, who was, you know, one of the first people off of Fukushima's Neocognitron to start building that structure into what into the earliest convolutional neural networks, that there should be an acknowledgement that yeah, we, we, and maybe there is already an acknowledgement, as you were saying, that the inductive biases are necessary to advance.
*  Yeah, exactly. And I think often the, you know, and this is a point that I really tried to make in the book, you know, kind of discussions around how we should build AI that don't account for what we're trying to build are essentially pointless, right?
*  There is no right way to build AI.
*  But what there is, is there are solutions to particular problems. And, you know, very often when people disagree, they're not actually disagreeing about the pathway to building the technology. What they disagreeing about is what the problem is in the first place.
*  And, you know, when we talk about inductive biases, you know, clearly debates about the extent to which we should handcraft in particular constraints or inductive biases.
*  Those are theories, not just of like what will work, right? They are theories of what will work in the context of a particular environment that you have in mind.
*  Of course, you need to tailor the degree of constraint in your model to the open-endedness or diversity of the data that you want it to be able to process, right?
*  So, you know, if you only ever want the network to do one thing, then of course you should tailor it to do that one thing.
*  It would be silly not to, right? If you want it to do lots and lots of things, then it needs to be very general. And the question is how general is the natural world?
*  You know, if you imagine that what we want is to build something that, you know, kind of behaves in the natural world, how general is the natural world?
*  Well, of course the natural world is, you know, infinitely rich and complex, but that doesn't mean that it doesn't have constraints too.
*  And when we look at the inductive biases that have been successful, they tend to reflect properties of the natural world.
*  So, you know, in the natural world, I don't know, like time goes forward.
*  So it's pretty useful to have a memory system because that means that your behavior can be conditioned not only on the now, but also on the past.
*  And the relevance of information in the past may not be like uniform across time, right?
*  Things that happen recently tend to be more relevant than things that happened a long, long time ago.
*  And there is a function that describes that relevance and it's not necessarily, you know, kind of completely linear as well.
*  And that means that, you know, we need not just one memory system, but multiple memory systems, right?
*  We have memory systems which deal with the immediate past and memory systems that deal with the longer term.
*  And that's reflected in the types of it's reflected in our sort of psychology 101, like memory is modular.
*  But it's also reflected in the structure of the inductive biases that we build into our models.
*  And an STM is a great example, right?
*  You know, what it does is it says, well, you know, actually, we need to maintain information over two timescales in memory, one which is sort of ongoing and active and one which can be placed into a kind of pending state and then released when needed.
*  And that turned out to be a really good solution, probably because it matches a property of the world, right?
*  That some information, you know, you need to place in a pending state and then be able to use it when it comes up, which is very useful if you want to not lose your train of thought.
*  Well, speaking of losing one's train of thought, I mean, I'm going to not I was going to ask you about this later, but I'll just bring it up now because you were talking about the properties of the natural world.
*  And this comes toward the end of your book when you're talking about artificial general intelligence.
*  And you a moment ago were mentioning about what you build depends on the problem that you want to solve.
*  And I mean, you talk you talk about umvelts in the book, which is the term given to, you know, the the relationship essentially between an organism or an agent, I suppose, an artificial agent.
*  And it's the environment that it's dealing with, its its capabilities within that environment and its perceptual world is built around its scale, its needs, its desires.
*  You know, an ant has a very different umvelt than we do.
*  And in, you know, not the same breath, but you also talk about affordances, which which is that relationship based on like what we can do based on the properties of the natural world.
*  You know, we, you know, can reach for an apple based on our abilities and the accessibility of the apple and our abilities to eat it and consume it and so on.
*  And, you know, I've just been pondering the usefulness of creating artificial intelligence in the guise of humans.
*  And whether an AGI or any AI could have could ever have be exposed to the same things that we're exposed to.
*  Right. Because you also talk about homeostatic needs.
*  Why would we want an AI to ever have homeostatic needs and homeostatic needs shape our the problem?
*  Okay, this is why I was coming back to the problem that we're solving, which is staying alive, reproducing, right?
*  The problems that we're solving, why would we want to build anything to solve those same problems?
*  To essentially have the same structure and relationship with the natural world.
*  I'm sorry that was a long winded commentary and question or...
*  No, I think it's clear. I mean, yeah, it's a it's a great question.
*  I mean, you know, kind of at the root is the question of like, you know, kind of what do we want to build and why?
*  And the answers to that have changed and they've changed in a way which reflects our different theories of cognition and our different understanding of biology.
*  You know, kind of when people set out to build AI in the first place, you know, kind of the the goals that they set themselves were very, very divorced from like the the everyday structure of the environment.
*  So, you know, kind of if you think of the initial like reasoning, like reasoning systems, you know, kind of whether it was like, you know, kind of systems for proving theorems through pure logic or systems for, you know, kind of playing games like chess or whatever.
*  These are pursuits which are kind of very divorced from, you know, the kind of quotidian embedding in the natural world with, you know, kind of that's characterized by by bodily movements and, you know, interactions with social others and so on.
*  And so naturally, you know, it was it was quite easy to have a paradigm that says, OK, well, humans can do X.
*  So let's try and build something that does X.
*  And that that was the initial paradigm and that paradigm kind of continued on into, you know, kind of the deep learning era in which suddenly we have models which are grounded in much more naturalistic settings.
*  And here an anomaly arises because what we have is we're still in the mindset where like, OK, the goal is to build to like recreate a human.
*  But suddenly we're like in the natural world and then it's like, OK, so we want to recreate a human in the natural world.
*  Right. And then this suddenly starts to be a bit weird, right, because it's like, why would we want to do that?
*  Like already, you know, there are eight billion people on the planet.
*  If you want to create humans, it's actually not that hard to create new humans, although, you know, it does take two of you and a little bit of time.
*  Sometimes it's hard. Yeah. But yeah, I get what you're saying.
*  Yeah. So you're absolutely right to point that out. Yeah. So it's not always simple, straightforward for everyone.
*  But in general, on average, we, you know, as a population, we have been able to, you know, kind of to populate the planet without recourse to to AI.
*  OK, so yeah, I mean, the question is where do we sit now?
*  Right. And how do we reconcile this idea of like that the goal of AI building is somehow agent hood, like the recreation of human agent hood with the kind of much more general problem of like, what would we actually do with the system if we got it?
*  Right. And it becomes relevant because as we realize more and more that our intelligence is human intelligence is grounded in those very facets of our behavior, which are like kind of not really, you know, the the the slightly esoteric, you know, kind of ability to reason about abstract problems or whatever, but also grounded in things like how we interact with each other socially.
*  Or even grounded in like, you know, movements of our body, right? You know, object recognition is part of object recognition is is not just being able to attach a label to something, but being able to pick it up.
*  Right. And when we take that on board, then suddenly we ask ourselves the question, what if we built these systems, would they actually have the same bodies as us? Well, maybe, but maybe not.
*  And then you ask, well, would they have the same like social life as us? And then you're like suddenly into super weird territory.
*  So, you know, almost certainly not right. You you you I mean, unless we get into the weird science fiction territory.
*  Yeah, no, I mean, right. I don't know, you know, self organizing systems, etc. Always come back, you know, to this. Well, let's say in a tractor state or something. But I just can't imagine that trying to emulate something, trying to emulate humans in building an A.I.
*  The affordances, the own belt, the problems to solve the whole collection of problems to solve is going to be so different that it just it has to be a very different kind of intelligence.
*  Right. So that's the argument. Right. So if you cut out those things, if our intelligence depends upon, you know, how we interact with the world physically and and socially.
*  And we we a priori just, you know, kind of don't want to build an agent that would share those properties, then it means that its intelligence will be different from ours.
*  You know, in the same way that, you know, kind of I love there's a book by a primatologist called Franz de Waal, very well known primatologist called Are We Smart Enough to Know How Smart Animals Are?
*  It's a well known, very successful popular science book. I love that work. And, you know, basically, it's a sort of it's it's a story of anecdotes from across his career and, you know, really good experimental research as well, which shows that broadly
*  animals are incredibly smart, but they're just not smart on our terms. Right. And we deem them as being less smart than us because they're not smart in the way that we are smart. Right.
*  And I think when we evaluate the intelligence of AI, it's often exactly the same thing. You know, we sort of say, oh, well, this agent isn't smart because it doesn't understand this thing that we humans understand.
*  And, you know, kind of, well, often there's no reason why it should. Right. Because, you know, it doesn't go to the opera or, you know, didn't go to dinner parties or do whatever humans do, you know, kind of many, many other ways of interacting with each other, of course.
*  But yeah, you know, and so, so we wouldn't expect it to share our intelligence. So the paradigm is changing. It has changed. And people are realizing that now, I think.
*  Maybe before. Well, yeah, sorry. I'm just jumping around, but I'm just taking a taking cues off of what you're talking about to talk about more things that you discuss in the book. And one is the concept of intelligence.
*  And we were just talking about how we have a very particular kind of intelligence and we define all other animals and AI agents based on our notion of intelligence. But our notion of intelligence has changed the focus of what we have considered intelligent behavior and thought.
*  Has changed over the years. And I don't know if you want to just kind of step through a few of the ways that it's changed.
*  But and or just acknowledge that it's an ongoing process.
*  Yeah, I think maybe there's one thing to say. So, you know, kind of I worried a lot about what to call the book.
*  And I wanted to call it natural general intelligence by analogy with artificial general intelligence, obviously.
*  But there was something that really put me off the title, and that was the word intelligence itself, because that word has a really bad.
*  And with reason. Right. And that's because, you know, kind of the whole notion that, you know, intelligence is this sort of, you know, this essence of individuals that we can use to kind of we can quantify and we can use to categorize people as maybe having more of it or less of it and so on.
*  And so on, you know, has has rightly, you know, kind of been viewed as like, you know, kind of elitist and discriminatory and more than just being viewed as that, you know, kind of intelligence testing the psychometric measurement of intelligence.
*  And that has not so much latterly, but certainly throughout the 20th century has been used to basically, you know, kind of reinforce cultural stereotypes and and worse.
*  You know, some of the earliest uses of intelligence testing were related to, you know, eugenic theories about who should and should not be allowed to to reproduce.
*  Those were bad.
*  Yeah, so, you know, rightly, you know, the word intelligence, I think, turns people off.
*  And, you know, kind of I do try to highlight that in my book and I highlight it a little bit sort of by analogy, but with the point that I made about our failure to understand animal intelligence right that from the perspective of the sort of, you know, the Western.
*  Ivory Tower in the developed world, you know, kind of very often it is an utter failure to understand the situatedness of intelligence in other cultures or other modes of behavior that has led to, you know, kind of discrimination and bias in the way that we think about other people.
*  Right. You can see that in the history of intelligence testing, which is a pretty, you know, dismal. It's a pretty dismal history in some ways.
*  I mean, you know, I'm sure that there is we have learned things about, you know, kind of the structure of cognitive ability, and I wouldn't want to take away from those who work in that area.
*  Yeah, but I'm sure everyone who's working in that area today would acknowledge that it has a darker side and I think it is the it is the reflection of exactly the same point that I was making about, you know, kind of our failure to appreciate animals abilities.
*  First of all, maybe maybe there should be a caveat when defining intelligence and you talk about the common leg and hunter definition that they collected 50 or so different definitions of intelligence and kind of whittled it down to a very small adaptive ability to adapt in new environments.
*  I believe something I'm paraphrasing, but maybe maybe maybe the little footnote should be given the particular set of problems of the agent or organism.
*  You know, because it really does depend on the problem state, the set.
*  Yeah, absolutely. Absolutely. And I think our research has, you know, kind of, as I said, I mean, maybe this is an unfair and sweeping generalization, but coming from cognitive science, but spending time with machine learning and AI researchers.
*  I often feel that, you know, kind of the in machine learning and AI research, they're much better at the solution than they are of the problem.
*  So, you know, kind of the people are really good at like, you know, kind of finding inventive solutions to particular problems.
*  But they're much less good at thinking about what the relevance of that particular problem is in the first place.
*  You know, the whole field is structured around benchmark tests, right? But nobody really asks where these benchmark tests come from or what they mean.
*  I mean, people do, but, you know, kind of it's not the sort of day to day topic of conversation.
*  Whereas in cognitive science, you could almost make the opposite, right? People spend so much time worrying about the problem that they don't actually build models that are any good.
*  Which, of course, is another untrue sweeping generalization. But, you know, if you wanted to characterize the continuum, you know, that would be one way of thinking about, you know, what the difference between the two is.
*  We'll get into talking about like the current state of reinforcement learning models later.
*  But what do you, you know, there's a large push these days in reinforcement learning for agents.
*  And you talk about this in the book to use more ecologically valid tasks, right? So there's a lot of game playing in, in like navigational maze type spaces and playing multiplayer games.
*  What do you see as the current state of that? I mean, is that close enough to ecological validity? Are we still super far away?
*  And again, with the caveat that it depends on what we're trying to do, right? But assuming we're trying to build like some human like agents or something.
*  Yeah, there's a there's obviously a lot to say here. So reinforcement learning has been one of the dominant paradigms within machine learning and AI research for 40 years now or something like that.
*  And, you know, it kind of grew out of a, you know, the methods in RL grew out of an approach which was to try and solve control problems, essentially, right?
*  These are problems in which there's a relatively clearly defined objective. And what's difficult is to work out how to get there, right?
*  And what RL does is it uses various methods, you know, some of them based on sort of, you know, kind of learning, you know, kind of stored values for states or actions.
*  And some of them based on, you know, sort of explicit search methods.
*  But it uses a variety of approaches to try to satisfy a well defined objective.
*  So that's why RL lends itself so nicely to things like playing Go, right? Because there's a well defined objective like, you know, well, I mean, we can we can talk about what the objectives might be.
*  There are many objectives, actually, when we play games.
*  But it's easy to make the assumption that the objective is just to win. And if you make that assumption, then RL is wonderful, right?
*  So, you know, kind of in video games, we also have a clearly defined objective, because most of them have a score and you keep on going, you try to maximize your score and then you get on the leaderboard.
*  And if you if you die, then it's game over and you know, you've got to start again. So it's very clear.
*  But but life is just not like that. Right. So there isn't in life.
*  No, nobody says, you know, kind of this was your score for today.
*  Sometimes. But yeah.
*  Do they? I mean, not not not not in not in general setting, not in general. Yeah.
*  Yeah, you know, you might get a score. You get a score for specific things, right? You know, maybe you're very happy, right? You did well on your exam, whatever.
*  But like, you don't get a score for like, you know, how nice you were to your colleagues or like, you know, whether like you enjoyed the sunshine or, you know, kind of what you don't get a score for.
*  You don't get a score for what life is really about. Right. Yeah.
*  Yeah. And so the paradigm is hard to translate. It's really hard to translate.
*  And and yeah, there's no there's no fine final point when you just win life. Right.
*  It would be nice, but there isn't. So so so what actually happens in the real world?
*  And this is a huge paradox, right? Because we have this paradigm and RL of all of the the paradigms that we have in machine learning and research.
*  RL is the one that's had the biggest impact on psychology neuroscience, right? Because it grew up in this intertwined fashion with animal theories of animal learning.
*  And we know that the neurophysiology of, you know, sub cortical to some extent cortical systems is, you know, really good match to what is actually implemented in successful algorithms, which have these optimality guarantees, which are really nice and whatever.
*  So there's this beautiful, beautiful story. But at the same time, like the whole paradigm is just a convenient fiction, right? It's just not true.
*  Like there are in RL, RL is when you build an RL model, what you're doing is you're working within a paradigm called the Markov decision process.
*  And, you know, within that paradigm, you have these things called observations, which are like sensory data and these things called rewards, which are what you're trying to ultimately maximize over, you know, some some theoretical future discounted.
*  And they're different things, right? But in the real world, this is just not true. You know, there is no difference between the taste of the apple and the reward of the apple, right?
*  The reward of the apple is its taste, right? Or maybe it's the feeling of, you know, kind of the satisfaction that you have when you are, you know, you are hungry and you're less hungry anymore. But that itself also is a sensation, right?
*  There are no rewards. There's just observations, right?
*  So, but the RL paradigm is in the real world.
*  But the reinforcement learning world would want to map reward onto, let's say, level relative level of dopamine, right? With unexpected value or something.
*  Because that's the computational push push to map it onto a tangible, ethical value. But you're talking about qualia and the feelings. And so we don't have quantification of our feeling of awe yet. Right.
*  And it's not about it's not about qualia. It's not subjective. It's about the flow of information in RL rewards come from the world, right? The environment is like the research decides that Apple is worth 10 points. And when you eat it, you get 10 reward points.
*  When you have when there's an increase in your dopaminergic signal, that's not like the environment impinging directly on the brain to produce that signal. That signal is produced by the agent. It's the agent that decides what's good and what's not. Right.
*  But but the reason why this is really difficult in the RL paradigm is that it leads to chaos. Because if I can just decide what's good and what's bad, and then I'm just like, well, I'm sorry, but I'm just going to decide that like sitting on the sofa and watching TV is like, you know, is good.
*  And that's fine. And I'll just be happy forever until, you know, kind of I die of hunger and thirst or whatever, right? I don't mind because I'm having a great time sitting on the sofa.
*  So, you know, but then, you know, that that that is the problem is that in so in reinforcement learning, you know, kind of we we often distinguish between like extrinsic and intrinsic rewards, right?
*  And the the salvation of the RL paradigm is that, you know, kind of in we can still think about rewards in in the natural world. But those rewards must be intrinsic, which is that they must be generated by the agent, not by the world.
*  And the question is, computationally, how did we evolve constraints that shape those intrinsic rewards, so that our behavior is still adaptive, even though we are deciding what's valuable and what's not.
*  And so, you know, kind of I'm not saying that RL is wrong. It's just that the simple kind of the simple story about extrinsic rewards and the map onto the video game or the the the chess playing it or go playing example just doesn't work.
*  You need a you need a different paradigm.
*  But you don't think that to have an intrinsic reward that an agent needs to be quote unquote truly autonomous and and then we get into the territory of well, is that even possible if they're living in a simulation that we program through the computer that we built and programmed?
*  Yeah, well, this is this is the difficulty, right, which is that, you know, kind of one of the reasons why it's nice to control the agents behavior with extrinsic rewards is that the researcher has really tight control over the objectives right in theory, right?
*  It's like you can say, you know, kind of this is what I want the agent to do right within limits. I mean, the agents can still learn to do things you weren't expecting.
*  Right. You have these things called specification gaming or whatever.
*  But if you let the agent make up its own reward function, then you're sort of in unknown territory, right?
*  How do you do agent learn to do whatever and then that's where you get into issues of safety and, you know, kind of because because if the agent just, you know, if you if you build an intrinsic function, like you say, oh, I just want my agent to be curious.
*  Right. You know, if you if you if you have it, if you have an extrinsic reward, which is like, you know, kind of please win this game or please eat lots of apples, then you can be pretty sure that the agent is going to do something like that.
*  Right.
*  That doesn't mean there aren't safety concerns there as you know, many people have reminded us but but the safety concerns if the agent is making up its own value function are much more severe or trying to satisfy something like like, let's just have control over the world.
*  Right.
*  Right. And it's like, oh, you know, there's lots of ways to have control over the world. Some of them might not be so good for people.
*  But, okay, so this this kind of harkens and we'll go down the reinforcement learning path a little bit more here and then we'll jump back out.
*  You know, you mentioned the reward is enough hypothesis, which was a paper written a few years ago. I think the title was reward is enough. Basically, you know, as a siren call that all you need all we really need for true.
*  AI, quote unquote, is is reward, but then that gets in the definition of what a reward is. And you were saying there aren't rewards separate from objects. And then, then you can make the concept of reward so deflationary right so if you, if you say, instead of programming and reward, you give it in trend, some intrinsic reward by giving it curiosity, right, but then you can just call curiosity reward, and the, and then the word reward becomes meaningless.
*  So, I don't know what I guess I'm just asking for your comments on, you know, you argue against the idea that reward is enough, but but you were just saying that to give an agent intrinsic reward.
*  You know, we could go into chaos but then I don't know what intrinsic reward is if it's curiosity or if it's control, and or whether, because you're still programming programming it to maximize something. There's still an objective function that you give it so it's not its own objective function right the agent can't come up with its own objective function.
*  Question mark. Yeah.
*  So clearly these issues are the these issues are sometimes a bit hard to wrap your head around. So I think that that paper is actually making a claim, which I, which I don't agree with but I think it's a slightly different claim from what it actually says in the title, right, the title maybe is a little bit of hyperbole.
*  What the claim really is, I think, is that a, of course you can define anything as a reward function right so you can say okay well would you know if it's intrinsic rewards then you know kind of maximizing my knowledge or you know doing really good prediction in my generative model right these are my intrinsic objectives and their reward I can write them down as reward and then I just maximize that and you know kind of everything.
*  So, so the argument that there is a version of that claim which feels unfalsifiable right.
*  But I think what they're actually saying is that that reward function. When you write it down will be simple.
*  That's the nature of the claim in that paper is that it's simple enough that you could, as a researcher as an AI researcher, you could write it down. There's some like magic formula, which is sufficiently compact.
*  Right, that you write down please Mr agent, please or Mrs agent, please will you optimize this this this this and this right and it's sufficiently simple that by just doing that.
*  You get intelligent behavior and you don't have to, this is an example I give in the book, like you don't have to then you know go through the world like the games designer of Atari games go through like assigning rewards to particular states like eating actions or eating apples or whatever.
*  You don't have to go through the natural world as an AI researcher and like you know assign 10 points for seeing a beautiful sunset and you know 30 points for you know I don't know going to the football match or whatever.
*  Because that would just you know because obviously nobody has time to do that right and actually take more time to design a reward function in that way, then it would just you know do it to build a stupid.
*  Yeah, yeah.
*  So it's a claim that paper makes a claim about the simplicity of the reward function which I think you know is a, it is a defensible claim and then you know people write that paper very very very accomplished and you know researchers who've thought very deeply about this problem for many years so it shouldn't be dismissed.
*  I personally do not think that you know kind of I do not think that there is a simple extrinsic or a simple reward function of any form that can be written down the maximization of which is just going to inevitably lead to intelligence.
*  Do you think that we are not algorithmic then in that sense that you know as we develop right our reward functions change and as when we walk in a new room and there's a new problem to solve and there's only a screwdriver to solve it.
*  You know is it an algorithm that determines hey that's metal screwdriver so I can connect these two wires to turn the light on because the function of a screwdriver changes given so I'm harking it back to now Stuart Kaufman's example of the next adjacent to the light.
*  The next adjacent possible I think it's called where essentially the argument is that we're not algorithmic because in a given situation.
*  Everything that we've learned in the world, the static concepts through you know computational steps.
*  You still wouldn't know how do you a screwdriver could still be used for a new purpose in a different setting right and in that sense it's fundamentally not algorithmic and your objective function then has to change essentially to be able to use that screwdriver.
*  I'm kind of mixing words here but anyway you know are you are you suggesting then that we would not be able to write down one equation.
*  However complicated it is that would lead to intelligence via the reward is enough hypothesis.
*  I think I do agree that we can't do that but I would maybe put it slightly differently I mean I'm not quite sure I understand what you mean by algorithmic but maybe that's because I haven't read Stuart Kaufman sounds like I should.
*  But the you know kind of I think there's another way of putting my perspective which is you know kind of in all of machine learning we use an optimization paradigm right so there's some objective which is specified right.
*  And then the agent gradually converges towards that objective or not things go wrong but you know kind of ideally they converge towards that objective and then.
*  You know kind of almost irrespective of which paradigm you're working in there's an assumption that they do so in a general enough way through.
*  The heterogeneity of the training set to diversity of their experience that you can then you know kind of take them and you can plonk them in new and different environments and they will you know kind of they will behave with similar levels of performance right.
*  But you know kind of what what this assumes is that the agent is somehow going to be able to sample you know kind of.
*  You know kind of to take many samples from the training distribution and model the training distribution adequately where the training distribution here corresponds to you know kind of the set of experiences which it would be possible to have right.
*  And I think that that's just not how it works in life right because many because we don't stick around for all that long right.
*  You know you know we only get to have one life and you know for most people that life comprises only a tiny tiny tiny subset of the possible experiences that we could have right.
*  And so that whole sort of you know there's a global optimum for life and we're all just trying to optimize towards it.
*  I think that paradigm is just not really applicable to the natural world I think instead what happens is that we sort of make a lot of stuff up on the fly.
*  So you know I really feel like what we do is we optimize very very locally like very very locally and in a in a really kind of like forgetful way a lot of the time.
*  I do.
*  We get locally good at some stuff and then you know we go off we formulate some views or we learn some policies and we put them into practice but then things change because the world is really open ended right.
*  And yeah.
*  We're not that good at dealing with change right we don't behave like an agent you know.
*  When I visit a new city if I'm adept at behaving in that city it's not because I have such a diverse set of experiences of all the possible things that a new city could throw up that I can just throw a shot.
*  Rather what happens is when I go to a new city I use I have a set of like reasonable but very very blunt priors and I sort of rapidly assemble them to kind of you know just about get by probably misunderstanding a lot of stuff and probably you know being incredibly inefficient.
*  And then you know then I go home and I probably forget most of it and then I solve another problem.
*  So the whole like optimization towards convergence paradigm you know I just don't think it works all that well in the natural world maybe you could think of it working for evolution but for for learning over the course of a single lifetime single individual I just don't think it's all that useful.
*  You also when you go to a new city and you haphazardly get by you end up at a coffee shop and whether it's maybe it's okay and then later you tell yourself oh I did that well when it was perhaps you know we lie to ourselves and you were kind of alluding to I think it's Nick Chater's book that you talk about in your book about how I don't flat flat mind right.
*  We have flat mind.
*  The mind is flat which is a section in your book so I like that example.
*  But but you do. So, a few times in the book you talk about, you say, this is one of the biggest challenges in in AI these days and one of those is finding the right mix and match of reward functions.
*  I don't know if you want to just say a few words about that because given that we were talking about reward reinforcement learning.
*  Yeah, I mean you know I think this is an interesting question maybe not even for AI research but it's an interesting question for neuroscience right.
*  Like to think about, you know kind of RL clearly has given us a useful paradigm for thinking about learning, you know, even if the caveats that I just raised, you know, apply, but I think it's, it's useful to think about what, what should we actually optimize for right, you know kind of what what are the, what are the objectives, you know kind of in
*  the world in which we make up.
*  To win, win, win. That's the objective.
*  Yeah, you know I mean it's, it's interesting isn't it. You know you think of the other thing about RL, let me say something else that I think is relevant which is the other thing about RL is that, you know, that's a bit weird in the way it's typically implemented and used.
*  Is that rewards when you get them, they're not fungible right. So an asset which is fungible means that you can store it and disburse it at a later time right so like money right so I can put it in the bank and then spend it later.
*  But that doesn't happen in, in like if you, if you think of a deep RL agent playing Atari right, it gets reward, but the reward just goes to its score right. The reward like augments its score and it like updates its value function and so on.
*  But you can't like when I, when I encounter assets in the natural world, then you know kind of typically an action which is available to me is to kind of store them in some way so I can use them.
*  This happens even with like primary reinforces right so if you think of what food does right, food isn't like something which is like instantly disbursed pink right what it does is it you know improves your calorie you know it has calories which like and it has nutrients
*  which like you know kind of your body uses over various different time scales to support itself right so it's disbursed in that way. The same with you know kind of economic assets right then they're used gradually over time.
*  So you know kind of this this notion that what actually matters in the world are there are inputs which change our bodily state in various ways.
*  That's what I do when I'm eating an apple I'm changing my bodily state.
*  And you know in RL that's all in the world and you could say okay yeah because maybe that's the state which I can observe and so on and you know clearly everything you can always shoehorn everything into an RL paradigm if you want to but like I think that the point here is that you know kind of we need to think more in neuroscience about what it means to learn those actions which to learn to or sorry to learn to learn rewards for observations.
*  As a function of their consequence for our bodily state right and that then you suddenly get into all sorts of interesting territory like you know kind of you know about mental health you know kind of because of course you know sort of failures to appropriately learn intrinsic rewards for states you can imagine that as being you know into being tied to you know various kind of psych disorders maybe depression and its comorbidities.
*  You know we also need to learn things like intrinsically we need to control we need control right so control it's very useful because it tells us that when we're being efficacious and when we're not this is something really really important.
*  And you know similarly we need to learn policies which which exert the right level of control right and there's a lot of evidence that you know come having that control is something that really ties heavily into a sort of you know kind of healthy mood states and healthy psychology of course if you you know you seek to have too much control or too little control.
*  Then that can be tied to pathological states right so you can have one way of thinking about I'm not an expert in psychiatry computational psychiatry but you can see the connection to things like obsessive-congulsive disorder for example so I think these things are really interesting and they're not very deeply explored.
*  In from from that computational perspective in psychology and neuroscience and I think that's a that's a big opportunity.
*  So that's what you mean by the right balance and mix of.
*  Mix of.
*  The right reward functions.
*  Yeah exactly exactly and you know to try and understand what what people really want right.
*  You know kind of it's it's it's a.
*  It's a silly question but like what do people want.
*  That's the problem is we don't know that you know we can't really define intelligence outside of our own problem space and even individually our problem spaces are different right but.
*  And we can't say what we want and what we want changes what we claim to want anyway changes and is that what we really want yeah.
*  Well that's that's Nick Chaita's point in the mind is that book right which is that we don't really have something similar to my we don't really optimize point and you know.
*  And you know.
*  I mean hugely hugely influenced by.
*  Reading next work and also talking to him about it you know he's such an inspiration for me.
*  But yeah he says we don't really have a value function and we just kind of we sort of make up what we want.
*  Isn't that pessimistic.
*  Isn't that pessimistic.
*  It's not highly true like you know we probably you know if you were water deprived for two days you know you probably really would want a glass of water.
*  And you know people you know really do care about each other.
*  You know and there are things that we really really do do preferences that we really really do have a lot of our preferences are just kind of average.
*  But yeah especially if we're all satiated and you know happy and healthy and you can make whatever preferences you want.
*  Yeah Nick Chaita.
*  But even even in those even in states of need right I mean you know people will go to extraordinary lengths you know kind of people will will go to what they will suffer extraordinary probation in pursuit of some abstract ideal which they've just kind of well I mean you could say it's made up I mean you know.
*  It's made up in a sense.
*  Well we don't know right.
*  That's that's a problem with the ontology of the universe I suppose and our now we're yeah I'm pushing us a little too far.
*  You need to get a philosopher on here to answer this.
*  I'm sure many.
*  Yeah well no one yeah yes and they all have the correct answer.
*  All right so I'm going to jump jump now again toward the end of your book when you know so before we were talking about reinforcement learning we were talking about the nature of intelligence and the nature of the problems that we're solving and I was questioning whether you know an AGI could ever have you know our intelligence because the nature of their problems are just going to be different and why would you create another human number eight eight billion seven million blah blah blah.
*  But you talk about the notion of going you know beyond our brains right building intelligences that are superior to our brains not not human like necessarily not human level necessarily but but moving beyond and whether that's wise and you know there's all sorts of different ways to think about intelligence.
*  But I would I'd just like to ask you like what you know what what for me having read your book and thinking about these things some of the things that you've been talking about and I think that's a good point.
*  But but moving beyond and whether that's why I think there's all sorts of ethical things that you touch on when you write about that as well but I would I'd just like to ask you like what you know what what for me having read your book and thinking about these things.
*  And I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I think that's a good point.
*  I don't see, except for the paperclip destroyer example, why we wouldn't do that and why it
*  wouldn't be possible.
*  Yeah, so, I mean, there are many answers to this question.
*  So the first one is that in narrow domains, of course we have already built things that
*  are better than human.
*  Calculator.
*  In many ways, right?
*  My calculator is better at maths than me, at least at calculation.
*  AlphaGo is better at Go than everyone.
*  And, you know, my bet is that chat GPT is probably more knowledgeable than any other
*  that exists on the planet right now.
*  That doesn't mean it's always right, as people have been, you know, obviously quick to
*  find out, but it probably is on balance, like better at answering general knowledge
*  questions than any of the other 8 billion humans.
*  I don't know.
*  I think that's probably a defensible claim.
*  I might have to think about it a bit more.
*  Another point that you mentioned is that we always compare it to like the super expert
*  in whatever domain we're talking about, right?
*  So we would compare a language model to Hemingway or something like that and not the
*  average person.
*  Yeah, no, for sure, for sure.
*  So the question, you know, kind of we rapidly get entangled in questions of like, what do
*  we want and what's right and so on.
*  So there's many there's many ways to answer this question.
*  I mean, one is to counterpoint two different approaches, which I kind of point in the
*  book. This is a sort of, you know, in the beginning, you asked me this question about,
*  you know, kind of neuroscience and its role in AI.
*  And there's a sort of there's another way in which psychology and neuroscience are
*  relevant for AI research.
*  And it's it's around the the role, not of like theories of psychology, but the role
*  of like human data and human data collection.
*  And this is a counterpoint to, you know, we talked about like, you know, kind of how if
*  you want to build something that plays Go or chess really well, then, you know, kind
*  of you can use RL. It's a very clear problem.
*  And actually, you know, what we learned in the process of solving Go, for example, was
*  that you can do better if you stop using human data.
*  So part of the RL is enough claim is also about that.
*  Right. It's sort of saying like you don't need to copy people.
*  You just need to get the reward function right.
*  And then everything else with enough computation and blah, blah, blah will take care of itself.
*  And so, you know, that's exemplified by, you know, kind of the story of AlphaGo, which,
*  you know, many people will be familiar with, but, you know, kind of basically, you know,
*  AlphaGo originally was built by, you know, there was there was a lot of it was bootstrapped
*  off a lot of supervised training from human play, essentially.
*  Right.
*  And, you know, lots of Go games, straight from the Internet and like, you know, kind of
*  predictions about which moves were likely to lead to which moves did lead to a win
*  conditional on how humans play.
*  And then later the team, you know, kind of got rid of that that human element and just
*  built something that played with itself and there it was able to do even better.
*  And that's the foundation for the story that, you know, kind of what we we don't want
*  humans in the loop at all.
*  Right.
*  But I think that the advent of language models, large generative models and so on have
*  really pushed our thinking back in the opposite direction.
*  So it's back towards the idea that actually what we really do want is something that
*  learns to be what humans want.
*  So in other words, the ultimate reward function is not like you write down something clever,
*  like, oh, be as curious as you can, or, you know, kind of be as powerful as you can or
*  whatever, and you just let it loose and it takes over the world.
*  Actually, what you want is an agent which is exquisitely sensitive to social feedback
*  and lo and behold, just like we are.
*  Right.
*  So, you know, kind of the really hard thing about being human is that, you know, it's
*  difficult to know what other people want and what other people think, because it's not
*  written on their faces, at least not, you know, very clearly.
*  And so very often you say things that offend people or that, you know, kind of the other
*  people don't like, or you miss, you fail to anticipate their needs or their desires or
*  their beliefs and you get stuff wrong.
*  And that's what makes it really hard to be a person.
*  Right.
*  So we learn over the course of development to be really, really sensitive to social
*  feedback from one another.
*  This seems somewhat related to inverse reinforcement learning.
*  Is that Stuart Russell's inverse reinforcement learning, where the idea is to learn the
*  preferences of the human?
*  That's kind of a related idea.
*  Well, it's related.
*  It's related to value alignment in general.
*  Yeah.
*  Right.
*  So, you know, kind of the objective, I think we're increasingly realizing that the, you
*  know, if we want AI that people will actually use and engage with in the way that, you know,
*  kind of people really, you know, kind of engage with and are starting to use these
*  language technologies for a range of different applications.
*  If we want that, then, you know, kind of the signal that we really care about is the
*  social feedback.
*  And, you know, kind of that's not something which was a priori obvious, I think, you
*  know, kind of throughout the history of AI research, deep learning research in which
*  it was always the researcher that set the objective.
*  Right.
*  But, you know, we've been talking about throughout this conversation, we've been talking
*  about how, you know, the natural world doesn't tell you, you know, what's good and what's
*  bad and whether you've won.
*  And that's true to some extent, but social others, right, social others provide you with
*  dense feedback, which is really, really important and which we have learned to interpret in
*  ways which, you know, are obviously hugely consequential for how we behave.
*  And so, yeah, so this is, I mean, this is what is happening, right?
*  So, you know, the new method, which people call RLHF, reinforcement learning from human
*  feedback, is like taking over.
*  It's, you know, learning to do this well is what everyone is, you know, kind of excited
*  about right now, I think, because this is the answer to building models which are going
*  to, like, really be, you know, be preferred by people and be useful to people.
*  Now, that doesn't mean you sidestep all of the thorny problems around value alignment
*  or like fairness, for example, because, you know, as soon as you say, OK, well, we're
*  going to, our agent is going to be trained to satisfy human preferences, it's like, well,
*  whose preference is, right?
*  So, you know, you don't want it to be, is it going to be the preferences only of like
*  educated Western people?
*  You know, how do we know that the preferences of like historically marginalised groups are
*  properly weighted in this?
*  What happens when people want different things, right?
*  What happens when you have like majority minority and dynamics and, you know, you have a
*  dissenting views and, you know, who does the agent satisfy?
*  So, you know, kind of this is, I think, where the action is right now.
*  And so I don't talk about this really very much in the book.
*  But this is actually the research that my group does.
*  OK.
*  And so, you know, for many years, we worked on these problems of like value alignment,
*  group alignment, preference segregation.
*  And, you know, I'm really gratified that now that we have good models, like in the
*  wild or, you know, kind of almost in the wild, that these issues are kind of coming to the
*  fall.
*  Because there, you know, maybe you should think of another podcast, which is like, you
*  know, kind of about the relationship between AI and social science, because, you know,
*  kind of I really feel that that is as exciting a frontier as the one, you know, the
*  membrane between neuro and AI, which you've done so much to, you know, to highlight.
*  Someone should do that podcast.
*  I cannot. I can't fit it, man.
*  You said you didn't really talk about that much in the book.
*  Another thing that you so I'm aware of our time and I have like this long list of
*  questions. So I'm trying to like sort of prioritize what to ask you.
*  And one of the things that I wanted to ask you, we started talking off, we started off
*  talking about, you know, large language models and foundation models, which you allude
*  to in the book a lot. But when it comes to like the Transformers and your book is, you
*  know, largely all about comparing AI and neuro and inspiration back and forth and how
*  we can use neuro for AI. But then you kind of have a one-off line where you say, well,
*  it's really not clear yet how these awesome new models compare to the way to our
*  psychology and our brains.
*  And there have been a few studies comparing like the like fMRI and EEG and that we are
*  we are prediction machines just like the transformer models.
*  And so it's kind of early days in that.
*  But I wonder if you have more thoughts on that since publishing the book or or just,
*  you know, if you if you think Transformers are getting further away from our brains or
*  if they're actually closer to our brains and psychology, then then we are appreciating
*  at the moment.
*  Yeah, great question.
*  Thank you for asking that.
*  So, yeah, you're absolutely right.
*  So people have done really nice work taking transformer models and comparing to the
*  brain sort of somewhat by analogy with the way that people compare convolutional neural
*  networks and the ventral stream system.
*  And lo and behold, there are commonalities and you might say, oh, that's expected because
*  they're both predicting the same thing.
*  So presumably it's the semantic information being predicted that is the root cause of
*  the shared representation.
*  So maybe it's not that surprising, but it's not to detract from the importance of that
*  work. It's been a really nice brick in the wall.
*  But you're asking a more, I think, a deeper question, which is like, why the hell are
*  Transformers so good?
*  So nobody knows why Transformers are so good, but they are so good.
*  They're really, really, really good.
*  And, you know, kind of it's clear that the step change that has happened since 2019 in
*  generative models is not just about scale, it's about the transformer.
*  So here's a theory.
*  And this is, you know, kind of it's sort of my theory, but it's really heavily inspired
*  by other people, in particular, the work of James Whittington and Tim Berens, who have
*  thought about this very carefully and probably in more detail than me.
*  And if you think about what a transformer does, what it does is it takes information,
*  you know, in, for example, in a sentence and, you know, rather than receiving bits of
*  information one by one, you know, what it does is it takes a chunk of information and
*  it kind of buffers it.
*  And what it learns are essentially, it essentially learns kind of a set of it's called
*  self-attention, but it essentially learns a set of like cross mappings between the
*  information which is provided in the sequence and critically the positions in the
*  sequence that that information occupies.
*  Right.
*  So you actually have a transformer, explicit representation of not only the contents, not
*  only what is in the sentence or the image or whatever, but where it is.
*  Right.
*  And this is very different from like an LSTM, because in LSTM time is, time is there, but
*  it's implicit, right?
*  You just get the things bit by bit in time.
*  You never learn to represent time itself.
*  Right.
*  But the transformer actually affords the opportunity to represent position in the
*  sequence, which if you're doing language going forward is time, right?
*  Or space in an image.
*  So what we have suddenly is...
*  A transformer is not recurrent, like technically.
*  Well, yeah, I mean, it depends on how you find recurrent, but it certainly does not
*  have the, it certainly does not have, it does not maintain information through recurrent
*  dynamics in that way.
*  Right.
*  Sorry to interrupt.
*  Yeah.
*  No, no, that's absolutely fine.
*  But anyway, but I think that the key element here, what I think is really important is
*  that you have explicit representations of contents and position, which we might call
*  like structure, like where is everything in relation to everything else?
*  So why is that important?
*  What's important because several people, including Tim and Tim Berens and a number of
*  other people, you could probably credit Christian Dohler with some of these ideas.
*  Yeah.
*  And my group has very explicitly kind of made this claim as well, which is that it is the
*  explicit representation of space in either a navigational sense or a kind of like peripersonal
*  space, like, you know, the space that's around you, that explicit representation of space,
*  which is really important for doing kind of complex forms of reasoning.
*  And the argument goes something like the following, right?
*  It's a little bit dense, but let me see if I can explain it.
*  So I'll explain the theory that we articulate, which is not in terms of allocentric space or
*  navigation, it's in terms of like reaching and grasping.
*  So, you know, imagine that you have, imagine you want to learn a very simple abstraction,
*  like the concept of three, right?
*  So three, what does three mean?
*  Three means that there are three objects in a scene, right?
*  Okay.
*  So we understand the concept of three.
*  And what I mean by understand the concept of three is what I mean is that if you saw a
*  new scene that had three objects that you had never seen before, right, completely new
*  objects, you would still know there are three of them.
*  Right.
*  So that means that your knowledge of three is independent of like the contents of this
*  painting, right?
*  Quite an explicit way, right?
*  And so how do you actually learn that?
*  It's really hard to think about how a neural network would learn that, because what neural
*  networks learn, of course, are just statistical mappings between inputs and outputs, right?
*  So, you know, kind of, it's hard to know if you take three new and completely different
*  objects, they'd be out of distribution.
*  So how would you ever, how would you ever learn that?
*  Well, one argument is that what you do is you use the way in which you interact with a
*  scene, so your action, as an input to the system.
*  And by using that as an input to the system, so by explicitly representing how you can
*  move, what you're doing is you're effectively constructing space, like three objects only
*  exist in three different locations, because you can move, what it means, what that means
*  is that you can move to them and pick them up in different places or move their eyes to
*  them.
*  And those are different locations that you're, that you've moved your eyes to.
*  So the idea is that our knowledge of abstractions is formulated through the actions that we
*  take in space.
*  And it's by taking those actions in space that we learn an explicit representation of
*  space.
*  It means we actually have codes for space, which is, of course, primate, that's what's
*  there in the dorsal stream, right?
*  You know, we have these things called salience maps, which are intricately tied with, you
*  know, kind of like space, the representations of where objects are in space.
*  So that explicit representation of space, we argue, is really important for, like, kind
*  of basically being able to reason about objects and being able to compose scenes from different
*  combinations of objects in different spatial positions.
*  And it's basically the, the primate ring factorises what things are and where they are
*  really, really explicitly.
*  And so the argument is in the transformer, that is the explicit representation of item
*  and position that in the same way gives it the ability to do that amazing composition.
*  So if you ask Dali for, you know, a blue tomato, you know, kind of on top of a pink
*  banana, then, you know, it will generate an image that has that property, right?
*  And, you know, as far as we understand, like non transformer based architectures can't
*  do that. And I believe that it's because the explicit representation of contents and
*  position allows the network to kind of factorise what and where in the same way as the
*  primate ring factorises that information in the dorsal and ventral stream.
*  We're leaving, sorry, we're leaving out when in that account, right?
*  So, you know, I think you were mentioning it when you take action, I thought you were
*  also going to start talking about the timing of those actions.
*  But in a sense, time and space could be considered analogous, right?
*  So like reading, the transformer doesn't read the way that we read, but there is a
*  timing. And so we have that order also.
*  Yeah, sorry, I sort of garbled my examples, right?
*  So, you know, kind of, yeah, the transformer is easiest to illustrate by thinking about a
*  sentence. But like, you know, kind of the examples of composition are like most vivid
*  when thinking about like image generation, right?
*  And in image generation, position space in a sentence position is time in the primate
*  brain, or at least in the human brain, you know, kind of it seems more like when you
*  really need to do like time over longer periods, like actual, you know, kind of clearly we
*  represent the dynamics of action or the, you know, we know that neural signals in the
*  parietal cortex in the dorsal stream, you know, build up in concert with the dynamics of
*  action. We know that parietal cortex is really important for really short, like visual
*  short-term memory or iconic memory, like short representation, you know, kind of in between
*  two saccades, for example.
*  But the sort of representation of time in a sentence in the primate, in the human brain
*  requires a prefrontal cortex, right, which is why, of course, you know, you have
*  disordered action sequences and disordered language if you have lesions of the various
*  parts of the prefrontal cortex.
*  So I think that the time is a similar story, but in humans, you additionally need the
*  prefrontal. But yeah, same principle.
*  Not too, you know, sometimes the way that I frame questions, I'm sort of, it's like I'm
*  advocating some sort of AI versus neuro claim to foundational ideas, right?
*  Because what I was going to ask you is in, let's say, 10 years, if you're going to
*  rewrite artificial natural intelligence, would you be able to include a story about
*  how transformers map onto and or were inspired by neuroscience?
*  And I ask because they have not been.
*  But I don't think they were inspired by neuroscience.
*  So I gave you lots of examples of, you know, kind of where ideas kind of co-emerged.
*  So you might say that maybe there's a co-emergence of these ideas in neuroscience and
*  and AI research. But, you know, kind of the transformer, you know, I do not think the
*  authors of the transformer paper, you know, kind of read a neuroscience textbook and
*  then were like, hey, this is a great idea.
*  It's definitely not how I am.
*  Although, you know, no, sure.
*  But maybe you could say the ideas are there in the ecosystem.
*  But yeah, I mean, I think we will know more in in 10 years time.
*  Hopefully before then, we will know more about that process of factorization and
*  composition. It's already a sort of pretty significant research topic in computational
*  neuroscience. And, you know, I'm fairly sure that, you know, we're going to learn more
*  about that. And that's going to be really fruitful.
*  Maybe the transformer is going to be, you know, yet another tool for AI research that,
*  you know, gets co-opted and used to understand the brain.
*  It wouldn't surprise me in the least.
*  All right. So I've taken us to the limit here.
*  I really appreciate your time. I have one more question for you.
*  And earlier, you mentioned that you didn't want to call the book Natural General
*  Intelligence, and yet the book is called Natural General Intelligence.
*  So why is it called that? And did you have an alternative?
*  No, I just have a limited imagination and eventually I gave up looking for other
*  kinds.
*  OK, great answer.
*  All right. Well, I really appreciate the book and our conversation today.
*  Thanks for coming on again. And I guess I'll see you in a year's time or so, perhaps.
*  Well, I have to write another book then.
*  Yeah, get on it.
*  Thanks so much. It's always a pleasure.
*  I alone produce Brain Inspired.
*  If you value this podcast, consider supporting it through Patreon to access full versions of all the episodes and to join our Discord community.
*  Or if you want to learn more about the intersection of neuroscience and AI, consider signing up for my online course, Neuro AI, The Quest to Explain Intelligence.
*  Go to BrainInspired.co to learn more.
*  To get in touch with me, I'm at BrainInspired.co.
*  You're hearing music by the New Year.
*  Find them at the newyear.net.
*  Thank you. Thank you for your support.
*  See you next time.
