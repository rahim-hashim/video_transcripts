---
Date Generated: April 20, 2024
Transcription Model: whisper medium 20231117
Length: 5744s
Video Keywords: ['Science', 'Technology', 'Education']
Video Views: 12046
Video Rating: None
---

# BI 086 Ken Stanley: Open-Endedness
**Brain Inspired:** [October 12, 2020](https://www.youtube.com/watch?v=PWCCbP2o1-s)
*  Maybe the pinnacle of what makes human intelligence human is its open-endedness.
*  Not problem-solving, but this tendency we have to explore.
*  That's the whole point.
*  There wouldn't be humans if humans were the objective.
*  That's what's so fascinating about it.
*  That's a kind of interesting lesson for AI.
*  Maybe a cautionary tale.
*  The process that produced human intelligence was one that wasn't trying to make it.
*  If you look at the window and you see all the nature around you, and you think about
*  it was a single run of a single process that produced all of living nature, what could
*  be more impressive than that?
*  It's literally biblical, like what has been created here.
*  This is Brain Inspired.
*  Hey everyone, it's Paul.
*  Deep learning is all about using some objective function to train a network.
*  For example, minimizing the error between the correct answer and what the network produces.
*  And often the objective is for the network to perform well on some benchmark task, like
*  the ImageNet or MNIST datasets.
*  Let's make models and perform experiments with the objective of answering specific questions,
*  testing specific hypotheses about some brain function.
*  And on we march, making a hard-won, steady progress, improving deep network performance
*  by tenths of a percentage, creating models of brain processes that inch closer to accounting
*  for some cognitive function.
*  However, what if a better way forward, at least better for making big fundamental progress,
*  is to not chase objectives, but rather to let creativity and intuition drive the work
*  that we do and take us in uncertain and potentially radically new directions.
*  That's what Ken Stanley calls open-endedness, and he thinks it's a powerful and unfortunately
*  neglected framework to apply to really ambitious problems, like developing AI.
*  Ken hasn't always known it's called open-endedness or called it himself open-endedness, but going
*  back to the early algorithms that he develops in neuroevolution, evolving new neural networks
*  as opposed to training a given neural network, he's always been driven by the principles
*  of open-endedness.
*  He and Joel Layman wrote a book about it called Why Greatness Cannot Be Planned, and Ken recently
*  open-ended research team to develop AI at his present company, OpenAI.
*  So that's what we talk about, open-endedness.
*  This is a topic that applies to many facets of life, and I got so excited, I was a little
*  more all over the place than usual, but that's okay.
*  I think it went in many interesting directions.
*  And I enlisted a couple past podcast guests, Stefan Lenin and Melanie Mitchell, to send
*  some questions for Ken, so I play those.
*  So that was fun.
*  This was a really fun conversation that maybe will inspire you to think about how open-endedness
*  might apply to many of your endeavors.
*  As usual, you can find links to the things that we reference in the show notes at braininspired.co
*  slash podcast slash 86.
*  If you value this podcast and you want to support it and hear the full versions of all
*  the episodes and occasional separate bonus episodes, you can do that for next to nothing
*  through Patreon.
*  Go to braininspired.co and click the red Patreon button there.
*  And here is Ken Stanley.
*  Ken, I have three questions for you today, and within those three questions there are
*  an infinite amount of things to discuss.
*  So and I also in addition have two surprise guest questions for you along the way.
*  Great.
*  So these simple questions are, one, how do we achieve or solve, one might say, open-endedness?
*  And of course, to get there, we'll talk about open-endedness.
*  How is that related to achieving AI, whatever that is, and where, if at all, does neuroscience
*  or understanding natural intelligence fit in?
*  So I assume you have pretty straightforward answers to all three of those questions, right?
*  I can address those questions.
*  I don't know if it's straightforward, but I can address them.
*  Okay.
*  So let's start with open-endedness because I like this quote.
*  You've given lots of talks on this, and at least one of the talks you ended with this
*  quote, to achieve our highest goals, we must be willing to abandon them.
*  At least this was your last sentence.
*  So what is the field of open-endedness trying to achieve?
*  And along the way, I guess we need to define what or sort of define what open-endedness
*  is.
*  Right, right.
*  Yes.
*  So open-endedness is really inspired by things that we observe in nature, which are what we
*  call open-ended processes.
*  And there's only a few, but they're really amazing.
*  And the first, maybe the most canonical, is evolution or natural evolution, evolution
*  on Earth.
*  And the particular aspect of it that's so remarkable is just that it went on and on
*  and on for more than a billion years and kept on inventing new stuff that's interesting.
*  And we just don't know artificial processes that do things like that.
*  We can't build them right now.
*  There's a very few machine learning algorithms, which is basically the field that I work in,
*  where you would actually want to run them more than a week or a month, let alone a year
*  or 10 years, let alone a million or a billion years.
*  Even on today's faster, more powerful computer platform?
*  Yeah, that's the funny thing.
*  Yeah, we have this powerful compute, but even so, it wouldn't be worth it to run for a billion
*  years.
*  There's nothing would happen.
*  Because these algorithms eventually converge.
*  So either it's good news or it's bad news, but either way they are done.
*  The good news is it converged to the solution.
*  See, it's good.
*  The bad news is it got stuck.
*  So now you're stuck.
*  It's not good and it's stuck and it's ended.
*  But in open-endedness, there is never the end.
*  That's what's interesting.
*  That's why it's called open-ended.
*  And the point is that it's something that's creative basically forever.
*  And hopefully, even in sort of like the grandest version, it's not just forever creative, but
*  it gets even more interesting the longer it goes.
*  So evolution is a bit like this, with these relatively simple organisms early on, like
*  single-celled things.
*  And then you get to stuff like human-level intelligence, the flight of birds, photosynthesis,
*  like all.
*  And the thing that's amazing about this is all in one run.
*  We're used to things in machine learning where like, okay, I could get a bunch of interesting
*  results, but they're probably separate experiments that just are ran from scratch separately.
*  This is all the same thing.
*  It's all starting from the same root of the same phylogenetic tree.
*  And so it's just a remarkable thing that this can't even exist.
*  But since we know it can, we'd like to be able to reproduce it.
*  And there's more examples than just natural evolution.
*  Like the other big one would be like the history of human civilization and human invention
*  in particular, like human invention, which includes things like art and music and science
*  and everything that we've created.
*  It's also a giant tree of discoveries that isn't heading anywhere in particular and is
*  in effect kind of like one run.
*  And it just kind of seems to keep getting more interesting.
*  You know, we start out with things like wheels and fire.
*  And here we have like computers and space stations now.
*  And so it's just another amazing process.
*  And so the question is, can we create artificial processes that have this property?
*  And that's what I would call open-ended processes.
*  And I guess I would say I'm sort of trying to coin a new term, strong open-endedness.
*  Strong open-endedness is like it never ends.
*  Okay.
*  Well, is that like strong AI?
*  Is that why the?
*  I think I started to think just how to make the distinction because there are experiments
*  that people have run where there's kind of like some open-endedness.
*  Like for a while it does some surprising stuff and that's cool.
*  Like I like, that's fun to watch, but it always just ends, you know, it doesn't go forever.
*  So like strong open-endedness would be like go forever just so that we can have a distinction
*  here and say what we're really interested in achieving.
*  Like the strong open-endedness is just like way out there amazing.
*  Like weak open-endedness, like we can do that right now, but it's not quite as interesting.
*  It's far from as interesting.
*  I mean, you've been working on open-endedness for a long time now and you and Joel Lehman,
*  is that his name?
*  Joel Lehman.
*  Your co-author?
*  Yeah.
*  Yeah.
*  Yeah.
*  Yeah.
*  Joel Lehman in Why Greatness Cannot Be Planned.
*  And that book, you guys describe open-endedness and you end with AI and with I think human
*  innovation.
*  Is that right?
*  As the two, no, is that right?
*  No, evolution.
*  AI and evolution, yeah.
*  Evolution, yeah, as the two case studies, the two big examples in that book.
*  Right, right.
*  And I only see like super positive reviews of that book.
*  It's almost, I wonder what you, first of all, is that true?
*  And then I wonder like, what do you make of just how positive the responses are to the
*  book and just to open-endedness in general?
*  Anytime I see a talk by you, you're always interrupted by someone saying, first of all,
*  This is blowing my mind.
*  And secondly, here's my question, which never happens in a talk.
*  What do you make of it, of the positive response?
*  And what does that say about our current era?
*  Right, right.
*  Yeah.
*  So it's true that I've gotten a lot of positive feedback.
*  Not everything is positive.
*  I mean, just to acknowledge that you can find a few bad reviews here and there.
*  And in fact, like I expected there to be some negativity because the book was clearly polarizing.
*  I mean, it's basically a challenge to the status quo.
*  The status quo that like most things should be objectively driven in our culture, in our
*  society, like the way we run things.
*  And so a lot of people believe in this stuff.
*  A lot of companies are run based on objectives.
*  A lot of academic research is run based on objectives.
*  And so I expected that they could ruffle some feathers to suggest that we shouldn't do that.
*  I also expected some people would love it because it's basically like having a straight
*  jacket and people don't really like it even though we just impose it on ourselves.
*  So it's not surprising to me that there's a lot of people who are also very happy.
*  But it's true that mostly I hear there's a lot of people happy.
*  So I guess my conclusion is that this self-imposed straight jacket of just like objective obsession
*  that is part of our culture is like one of those things that we did to ourselves without
*  realizing we're doing it.
*  Like most individuals don't like it.
*  Even the people involved in those systems.
*  They just thought like it's necessary for some reason.
*  Like I've got to satisfy my manager and let that manager somehow know that they can trust
*  me and so I'm just going to subscribe to this and then I'll force my reports to do that
*  also and it all aligns together.
*  And then suddenly the whole world is working this way.
*  So I think the appeal of the book is the message that like actually some things work better
*  if you don't do it that way and take off the straight jacket.
*  And that's a liberating kind of a message.
*  I mean you must get all sorts of crazy questions and life experience stories like, you know,
*  of course I listen to your talks, I read the book and I think, oh my slacker young self,
*  all I was doing was being open-ended.
*  I wasn't, I must have been doing something right, you know, so you must get all sorts
*  of responses like that as well.
*  Yeah, I do.
*  It's really interesting the life stories that I've gotten from this.
*  It's really gratifying actually because as a computer scientist it's like completely
*  not what you expect in your career to have like almost therapeutic interactions with
*  people.
*  Well, I also thought you must be getting all sorts of invitations from like businesses,
*  but then I thought, no, that can't be true because your message is antithetical to their
*  supposed progress, right?
*  So they don't want you coming in and telling all of their employees to not obtain the objective.
*  But aren't you getting inundated with the self-help industry because this is, this seems
*  very self-help oriented as well.
*  You know, that's an interesting question.
*  It actually sort of, it sort of is the opposite of that.
*  Really businesses have approached me or more like business conferences.
*  Like they've asked me to come talk to you.
*  There's a really unbelievable diversity of different kinds of communities that have asked
*  me to talk.
*  I mean, I even spoke to the retirement investment community.
*  So the people who run our retirement accounts.
*  What does that book have to do with that?
*  What I've learned is that everywhere is looking for some form of disruption.
*  People want to get out of their shell and out of the box and think of something new.
*  And the book is just by its nature kind of about how to do that.
*  And so at the top level, most people are interested in that, I think in almost any industry.
*  And so it's not as resistant as you would think.
*  There's very few kind of like bean counters that really, really believe in the system
*  of just metrics and objectives.
*  There are a few, there's a few.
*  I think they're pretty rare.
*  They're like a caricature.
*  Most people are just sick of it that I've encountered.
*  Even the administrators who are the gatekeepers, they're like people like that.
*  Like when I get to talk to them, they're usually like, I just really don't know how to do something
*  different here.
*  Like, tell me how to reorganize.
*  I mean, I remember I went to one huge lab where they had almost a billion dollars that
*  they were managing.
*  And one of the first questions that the leader was asking me was like, well, how should I
*  reorganize how we allocate all these funds?
*  And I was like, this is a crazy question.
*  Like I'm going to tell you how to like reallocate your funds in this gigantic organization.
*  And so I think it's just like people don't really, hadn't really realized there is an
*  alternative.
*  It's just like, this seems like the only really viable way to go is like, yeah, of course
*  there needs to be accountability.
*  How could we live in an organization without accountability?
*  Objectives provide accountability and metrics tell us how we're doing on those objectives.
*  And so while we don't enjoy it and we don't like it and we wish we could be all freewheeling
*  and interesting, like when it comes down to it, most people are like really uncertain
*  like what that really means that's actually going to work in practice.
*  I mean, have there been particularly challenging criticisms that you've taken to heart and
*  that has challenged your intellect to challenge the framework of open-endedness?
*  Yeah, I think like the biggest question, I don't know if it really called criticism,
*  but it's kind of critical is just how do you control it?
*  To the constraints?
*  It relates to constraints, but it's sort of like the whole theme of this is just like
*  to let you just do what you want.
*  If you think about it from an AI perspective, it's basically saying, let an algorithm just
*  go off and dither around and find interesting stuff.
*  But we're not going to tell it what to do.
*  But like everybody wants to tell things what to do.
*  Like ultimately we want, we do want to tell things what to do.
*  I want this vacuum cleaner to clean my room, not to just explore around and find interesting
*  things to do.
*  That's what it's supposed to do.
*  And so like people at first are like, this is super exciting.
*  We can find new things we can think of.
*  But then they're like, but how do I control it?
*  I want it to do X.
*  And there's a tension there.
*  I mean, even outside of algorithms, like when we talk about the book and just like how you
*  run an innovative organization, it comes up also because people are like, well, I can
*  let my employees kind of explore around in some way.
*  But then how do we get anything done?
*  Like, what is it?
*  Well, but isn't this like Google's 20?
*  Is it 20 percent rule?
*  I forget what it was or is.
*  I forget the exact number, but it is like that.
*  Yeah. Like that kind of thing.
*  It feels dangerous because you're like, how do I then channel that back to like our company
*  goals and back to objectives?
*  Like at some point we have some objective.
*  And so, yeah, there's a bit of uncertainty.
*  What does it mean to control something where like the main property of the thing is that
*  it's not something you control and that's a paradox and it leads to tension.
*  And, you know, I recognize that that is something to be uncomfortable about.
*  But I think the answer is just that that there are there's tradeoffs and you just to
*  decide where you fall along those tradeoffs.
*  Like you're never going to get both.
*  You can't have total control and total creativity at the same time.
*  That just doesn't happen. So you'd have to at least recognizing the tradeoff is useful at
*  the get go. But then, you know, you can you can have compromises.
*  It doesn't have to be all or nothing.
*  Well, you repeatedly emphasize that really what open endedness is good for is for ambitious
*  goals. And, you know, in your vacuum cleaner example, that is not exactly an ambitious
*  goal. If you if the solution is to clean the floor, you don't want your vacuum cleaner
*  like mine did last night getting stuck and then running out of batteries.
*  Yeah. You know, because it's exploring.
*  Right. Yeah. So there's a tradeoff there as well.
*  Correct. That is. Yeah.
*  And that is an important distinction.
*  Yeah. Like a lot of I think people who didn't like the book, that's one of the things that
*  they missed is that we're talking only about really ambitious stuff.
*  Yeah, it's true. Like we never said and I never thought that like we should eliminate all
*  objectives from from the world or that things that aren't super ambitious should somehow
*  just turn into like creative activities where you just wander around aimlessly and hope
*  something happens. Like if you want to make a sandwich to make a sandwich.
*  I have some pretty ambitious sandwich designs.
*  Right. But I mean, it we even acknowledge this in the first chapter.
*  I mean, we were we were aware that people would say things like that and that that's
*  like a straw man that they would attack, like that people would say, this is extreme, like
*  this. They're saying that we should never do these things.
*  Look at all the useful things we've done with objectives.
*  But we know that we're not saying that there's never been anything useful.
*  We're talking about things where we're making really, really, you know, blue sky discoveries
*  here. And those are the kinds of things where the objectives are not going to serve you
*  well. I mean, it is so just conceptually, it's difficult to grasp.
*  I mean, you yourself have said maybe we don't need to define it per se, but it's even
*  somewhat difficult to characterize it.
*  And I came up with an equation here and you can correct me.
*  So or a recipe, we'll say.
*  So it has to be highly ambitious for open endedness to work, has to be highly ambitious.
*  You have to have some intrinsic motivation because you can't just kind of sit around
*  whittling a stick and hope something will happen.
*  Right. You have to have motivation to explore and you have to truly be seeking novelty.
*  And we can talk about your novelty search algorithms and what that has led to and the
*  concept of divergence.
*  But then the last part of the recipe is is valuing interesting findings.
*  And so we need to talk about what interesting means.
*  Yeah, yeah, yeah.
*  Is that is that close?
*  Is that a close recipe?
*  That is those are those are good ingredients.
*  But I think it sort of depends on what you're trying to cook a little bit, you know,
*  because there's different there's different places where you would do something open ended.
*  And it might just be a subset of those that apply.
*  Like, for example, like natural evolution, you know, isn't curious, like it just because
*  it's not an entity.
*  And yet it is open ended.
*  But like when when humans are reinforcement learning agents are engaged in sort of
*  exploratory, playful discovery, then they may be motivated intrinsically by curiosity.
*  And so like there's a certain context dependence to these ingredients.
*  But each one of those ingredients is one of the kind of is one of the things that it's
*  one of the weapons in the arsenal, so to speak, that you could use.
*  But I don't think you always need all of them.
*  Well, but you could say that evolution is highly motivated to explore the search space
*  of of possible lives of possible configurations that have high fitness or whatever
*  within within the constraints of survive and reproduce like you've mentioned.
*  So maybe not maybe that's a maybe the teleology of being highly motivated doesn't fit with
*  the evolution story. But you can make a case.
*  Yeah, I see that. Yeah, that's I mean, that gets a little bit into semantics, I guess.
*  Yeah. Like what do we do?
*  You're motivated. Does it have to be like a person or is it going to be like a thing?
*  But yeah, if you look at it that way, you can interpret it that way.
*  But I think what really matters is ultimately like the more algorithmic questions than like
*  it's that's more like a matter of interpretation.
*  Like, do we do we think of the process as being motivated to discover things that we
*  could or we could not? But the real question is what makes it motivated?
*  Like, what is the what are the actual algorithmic mechanisms that allow something to
*  diverge like this? And that's what gets into the real implementation.
*  Like how you actually get an open ended process going.
*  But yeah, the things you mentioned are kind of the things that we touch on.
*  So it's a good list. Yeah, it's a little hard to digest the whole list at once, probably.
*  But yeah, yeah, yeah.
*  I don't really want to step through them, you know, because we're going to be talking about all of
*  it along the way.
*  Yeah. Something you said already previously, just shortly before you have people in
*  business talking, asking you like, I don't really know how to move forward.
*  Like what? Yeah. How to be creative in the work environment.
*  And it immediately made me think of AI and the current.
*  So so there's a deep learning explosion and a lot of people think that there it's kind of
*  coming up against a wall. I mean, there's still a lot of progress being made.
*  But on the whole, you're still making a modicum of improvement on these benchmark tests,
*  which is antithetical to open endedness.
*  So in a sense, this is a could be a blessing for the AI world as well.
*  Yeah, yeah. I mean, I mean, I come from that world.
*  So it was actually like starting with the observations in AI that eventually led to this
*  like larger theory that applies to more than just AI.
*  And so, you know, the thing is, it's true that in AI, we tend to be very benchmark driven.
*  That's part of the culture.
*  And that's very objective. And it's also underwhelming and not exciting.
*  And even the people in the field know that. But they still try to beat the benchmark anyway,
*  because sometimes it's hard to think of what else to do.
*  But I'll tell you what else to do.
*  To me, like that isn't really progress.
*  Like that is, or at least it's not the kind of progress that interests me.
*  The kind of progress that interests me is the invention of new playgrounds.
*  Like what I think is really, really interesting in AI is when an entirely new playground
*  appears, which is a very rare event in performance doesn't even matter.
*  That's the problem with performance metrics.
*  Like, you know, a new playground when you see one and you can criticize the heck out of it
*  and give all kinds of reasons it doesn't work really well and it doesn't beat this thing
*  on this benchmark. But it still created a whole new world of low hanging fruit of ideas
*  that like nobody would have considered if it hadn't come into fruition.
*  And so I'm really, yeah, I'm really interested in creating new playgrounds, which is basically
*  like stepping stones, which is related to this kind of theory of open-endedness is the
*  discovery of a new stepping stone opens up a whole new frontier of possibilities.
*  And those are things that I would consider to be progress like in the field of AI.
*  What do you consider progress for yourself personally?
*  You mean, is it professionally or just like in like my normal life?
*  Yeah, like at the end of a day, what makes you feel like you've made progress?
*  Is it if you've done something interesting or novel, you know?
*  Well, I think it's yeah, it's the playground thing.
*  Like if I could open up a new playground, then that's like probably the most exciting thing.
*  Like it's like a whole new world has been created of possibilities.
*  Yeah, I would like to do that.
*  It does seem like what you do is fun.
*  Is it fun? Yeah, yeah, for sure.
*  Yeah, I mean, it's definitely enjoyed most of what I've been doing.
*  I mean, yeah, from the research, I have enjoyed it.
*  Yeah, it's fun.
*  Yeah, I mean, trying to do open-ended stuff.
*  Open-endedness is associated with play.
*  So thinking about it a lot is like thinking about playing.
*  I mean, you come from, you know, you've created a bunch of evolutionary algorithms and
*  I mean, you come from a deep background of machine learning and you've developed these
*  novelty search algorithms along the way to to push toward open endedness.
*  I mean, so let's talk about creativity for a second.
*  Yeah. I mean, there's just so many different avenues that we could go down.
*  So I'm not even I don't even know if we've established a good baseline for people to
*  understand what open endedness is yet.
*  Sure enough, we could address it a little more, I guess.
*  Like what is open endedness?
*  Like I would I wanted to acknowledge like there is a community that was interested in
*  open-endedness for like decades, like maybe 30 years, which they call them themselves
*  the open ended evolution community.
*  Well, they don't use the word community, but open ended evolution.
*  And they've been discussing a lot of these issues for a long time and they have
*  their workshops, but it was very tethered to the word evolution and the community
*  stayed small. And so I don't like I want to say that I like sort of I own the idea and
*  sort of own the definition like that is all has been discussed for a long time.
*  But I think what the kind of realization that I had was just that why is this community
*  so small? Like it seemed like this is like the coolest topic ever.
*  It's so interesting. And I just like within the last five years, something just was like,
*  I got to do something about this.
*  Like this there should be like many like 10 times more people investigating this.
*  And so when it comes to definition, I think I'm more concerned with why is this so
*  interesting than what is the particular definition?
*  Because in my experience in that community, there was a lot of argument about definition.
*  There's also an AI. There was a lot of argument.
*  Like what is what is intelligence really?
*  What is it? And this has never been resolved.
*  And what I think is interesting is it didn't need to be resolved.
*  Like there's been plenty of progress without resolving the question of what is intelligence.
*  It helps to have some discussion.
*  I'm not saying you shouldn't talk about it at all, but I just don't think there needs to be some
*  like definitive final answer to like the definition.
*  It's something you know when you see and if you look at the window and you see all the nature
*  around you and you think about it was a single run of a single process that produced all of
*  living nature. I mean, what could be more impressive than that?
*  It's literally biblical.
*  Like what has been created here?
*  I mean, it's creation.
*  Yeah. So like, yeah, we could we could get into the nitty gritty semantics of what is it actually
*  about. But it's just it's something incredible.
*  The whole point. Well, one of the big points is that there's no objective to that creation, to that
*  biblical creation. There was no objective.
*  I mean, I don't know.
*  Humans, right? That's the objective of evolution, right?
*  That's where the top tip of the arrow of evolution.
*  That's one thing I strictly like to say.
*  It's not what it is, because that's that's the whole point.
*  Like there wouldn't be humans if humans were the objective.
*  That's what's so fascinating about it.
*  Like that's that's that's a kind of interesting lesson for AI.
*  It may be a cautionary tale.
*  You know, like the process that produced human intelligence was one that wasn't trying to make
*  it like there's nothing in the in the ingredients of the initial run of evolution.
*  Like at the beginning of time where you could say this is trying to produce a brain, that'd be
*  crazy. You can't extrapolate that far out.
*  It's it's some kind of like orthogonal happenstance like that isn't on isn't really
*  directly related to the constraints of the system from the beginning, which is survive and
*  reproduce. You give a lot of examples of things like that have happened in the past that are
*  examples of open-endedness, like vacuum tubes weren't created to make computers but are
*  necessary for computers.
*  You're I'm not going to make you recapitulate the pick breeder story, but you put this
*  little applet thing on the web where people can go and like click on an image that they
*  like. And then it turns into like that becomes the child and turns into a bunch of different
*  images. And you do this over and over and over and people can share their different journeys and
*  stuff. And you end up with these images that are very impressive, but not but not what you
*  were striving for. So if you're striving for a particular image, you're going to end up with
*  Jupiter with its storm spot on it, for instance.
*  And who knows what that person or team was striving for.
*  You've ended up with some interest, like a car, you ended up with without intentionally doing
*  it. So you end up with so open.
*  This is an open ended process and you end up with interesting things, things you didn't
*  expect that might be useful for something else.
*  But how important is it that the end product is useful, does something is usefulness part of
*  interesting?
*  Well, I guess that's I mean, it is the answer that is partly related to just what do you care
*  about? I mean, if we're going to artificially trigger an open ended process, we might ask why,
*  why are we doing this? If it's just to see things that are interesting, then maybe they don't have
*  to be useful unless your definition of interesting means it's useful.
*  Well, you must usefulness must you get a kick out of it.
*  I guess that's useful.
*  Yeah. Yeah. I mean, if that's what you're hoping, then then you're hoping that everything
*  basically will be useful if that's what it means.
*  And like getting a kick out of it is useful.
*  Then then, yeah, hopefully everything gives you a kick.
*  Or actually, I shouldn't say that.
*  Hopefully a lot of stuff gives you a kick because we're OK with some stepping stones that don't
*  satisfy anything.
*  Like the stepping stones are useful in their own right because they got you to that place where
*  you got a kick out of it.
*  So it's not like every single thing you traverse has to itself be great.
*  But you should get some you should get to places that are really interesting at some point.
*  Well, that's like an activation energy that you have to get over to get to the interesting
*  point. You could look at it that way.
*  You said evolution is an open ended process and apologies.
*  I'm kind of all over the place, but I have, you know, about a billion thoughts on these things,
*  as everyone does, I'm sure.
*  But I've also heard you said that evolution sort of cheats us out of many of the of its
*  potential interesting end products because the thing that it creates does not survive and
*  reproduce. And who knows if that thing that it created that didn't survive and reproduce
*  wasn't a stepping stone to something potentially even better or more interesting.
*  Yeah, yeah, I have talked about that.
*  This is a subtle issue, but it's about the it's about the constraint, you know, and that this
*  is something people often get confused is a different kind of constraint and objective.
*  Like in evolution, it's true that everything has to survive and reproduce.
*  So a lot of people think, well, that must be an objective then.
*  Like the objective is to survive and reproduce.
*  So it's like we're trying to get to that point.
*  But but I think you have to remember that an objective, at least I think from from a machine
*  learning perspective, is a place that you don't start where you're already there.
*  Like it's somewhere you move towards.
*  And the thing is, the very first organism must have survived and reproduced by definition, right?
*  Because it's like here we are.
*  So the problem was solved at step one.
*  So that doesn't really mesh with the usual conventional notion of an objective.
*  And that's because at least in my view, it's not an objective, rather, it's just a constraint.
*  Like we're already there from the beginning.
*  But the constraint says that we have to stay there.
*  Like every single thing ever that's going to perpetuate more search has to also satisfy that
*  constraint. And so so it basically is a pruning mechanism.
*  So it's saying like, what are we not going to look at rather than what we are?
*  I'd rather think of it as a not like we're not going to look at things that don't have
*  basically Xerox machines inside their stomach, like copying machines, like basically walking
*  copying machines. And the only things that you can perpetuate are walking copying machines.
*  And if you're not a walking copying machine, we don't care how interesting you are.
*  You're out of the game.
*  And yes, that means all those things that were interesting but like couldn't reproduce, you
*  know, are gone. Those lineages have never been explored.
*  And this is just a theoretical thing, because I mean, practically, obviously, they can't be because
*  they can't reproduce. Right.
*  But like theoretically, like, you know, you could imagine from a computational perspective,
*  something could let it reproduce anyway.
*  Like we could mutate their their genome and like artificially inseminate something and put it out
*  there. Like that could be that science fiction.
*  But like from a computational perspective, like we're talking about algorithms, it isn't
*  science fiction because like in evolutionary algorithms, you can just make anything reproduce
*  you want. It's completely up to us.
*  So that means that there's this lost opportunity, like all these things that have been pruned out
*  of the search. But the way to think about it is like that the constraint to me is the
*  thing that ensures that things are interesting.
*  Like it is possible to have a constraint that would admit lots of crap that isn't interesting.
*  You know, like like what if the constraint was just you have to get to a minimal mass in
*  order to reproduce? So in other words, like if you get above a certain mass, then like, you
*  know, God will come and make a child for you.
*  You don't need to have reproductive organs or anything like you'll just get a child.
*  Well, I'd say the world would be pretty uninteresting.
*  There'd be all these inert masses literally everywhere.
*  This is still the world of DNA.
*  I mean, it's still going to be organisms on Earth, but it would be junk.
*  And so something about surviving reproduces, ensuring that like everything's interesting.
*  It's true that it prunes out other things that could have been interesting because there's
*  plenty of things that have been alive that were really interesting, but just never managed
*  to reproduce. But at least everything that did reproduce was interesting.
*  And that's why nature is so interesting.
*  At least to me, like everything's interesting.
*  Every there's not an organism on Earth that isn't interesting.
*  And that's because of this constraint.
*  But if we widen it and make it less constrained, we get more stuff.
*  But we also admit some things perhaps that are interesting as we get less constrained.
*  So there's some trade off.
*  Like we could fill the world with inert blobs.
*  I mean, if it is so useful, if open-endedness is so useful, why hasn't evolution done that?
*  Like it's that's a very teleological statement there.
*  Like you mean expanded.
*  Like allowed for uselessness for a few stepping stones in order to achieve usefulness.
*  Well, yeah, that's I mean, because I think it's because evolution doesn't care about any of
*  this. It's not it's not a concern to it.
*  Whether it's not even a thing.
*  Yeah, it's not it's not trying to do anything.
*  So it just is what it is.
*  But that question does come up in artificial evolution then, you know, because then we
*  actually we're deciding what we care about and someone does care.
*  So then we have to confront these kind of questions.
*  Like what do we actually want it to do?
*  One of the things that I appreciate that you appreciate in your work has highlighted this
*  recently is the importance of the environment, environmental constraints.
*  Right. So the question is, how do you achieve true open-endedness that will create enough
*  complexity forever to be able to then generate new things forever and potentially interesting
*  things forever? And you've started at least the latest that I know you're you're making
*  mazes and a so it's a co-evolution algorithm where your your agent is trying to solve the
*  maze in an open-endedness fashion.
*  But then you're also developing new mazes.
*  So there's a co-evolution of the maze itself and the agent trying to solve it.
*  Do I have that right?
*  Yeah. Yeah. That's the minimal criterion co-evolution that we did with Jonathan Brandt.
*  Yeah. And that and there's also something else that I worked on with colleagues at Uber
*  ILOVES, which was called Poet or the paired open ended trailblazer, which also also involves
*  environments. And yet it's getting towards this issue of, you know, I mean, if you look at
*  things like novelty search, they're really about exploring the space of solutions.
*  You could think of it as like different behaviors.
*  But like there needs to be another side to this, which is like, what are the actual things
*  there that are there to do in the world?
*  Like opportunities, in other words, like what are the opportunities to do something new?
*  You can look for new things to do all day.
*  But like if I lock you in a room and there's nothing in the room, there's only so many new
*  things you can do. You have to get out of that room and find something new to do.
*  And so like a lot of the environments that we create in artificial systems are like this kind
*  of locked room. Like there's there's only so much that can happen and then it's kind of over.
*  So we need something that somehow gets environmental diversity also at the same time as
*  we're getting solution diversity.
*  And those algorithms are beginning to explore that question and recognizing just how important
*  that is, that opportunities also have to come out.
*  And, you know, evolution on Earth does this by making the organisms both opportunities
*  and solutions. And so that's kind of cool.
*  It's like a little bit self-referential, you know, because like a tree is a solution, like
*  it's a way of life and a way of surviving.
*  But it's also an opportunity for somebody to eat leaves so you can have a giraffe.
*  And so it's both. It's both a solution and an opportunity.
*  And then the giraffe makes an opportunity for something else, I guess, to eat the giraffe.
*  And so so that's why evolution has been able to keep going for like a billion years, because
*  it's not just generating solutions, it's also generating opportunities.
*  Well, it's highly complex and highly recursive, somewhat somewhat like the brain.
*  This reminds me of Stuart Kaufman's concept of the adjacent possible.
*  Are you familiar?
*  Yeah, yeah, yeah, yeah, I see the connection.
*  I mean, this is the same, you know, he his more recent thought processes or I guess talks
*  that he gives talk about the unboundedness of complexity and and just just in the way that
*  you just spoke about it, that an organism that different opportunities are afforded as
*  things develop one opportunity or one develop one evolutionary development.
*  He uses the the swim bladder, for instance, which then can evolved and then can become
*  a habitat for a new microbe or whatever.
*  And then that is a completely new thing in the universe.
*  And it's generative and creative in that respect.
*  So it really crosses with that.
*  Yeah, yeah, I think Stuart Kaufman and I probably would get along well.
*  We've never actually spoken to each other, but we obviously we have some similar interests.
*  And I kind of think of this adjacent possible.
*  It's a little bit more like the how, whereas I think I'm a little bit more talking about
*  the what it's like.
*  Like it's more like explanatory is how I think of it.
*  He might disagree. Maybe I'm mischaracterizing, but it's sort of like, how can you explain
*  how this could be possible?
*  Like, how could all this stuff have happened?
*  Well, it's because of this notion of adjacent possible.
*  Like there's the search space has this very intriguing property that like there's these
*  really counterintuitive adjacent hops that you can take from certain points in the search
*  space to other points that just aren't what we would expect.
*  And it's almost completely unpredictable.
*  And it sort of explains how it's possible for all this stuff to exist.
*  But it also raises questions philosophically, like why is the search space that way?
*  That's just like an amazing property of the universe.
*  It's like a prior property.
*  Evolution doesn't explain that evolution uses that searches that.
*  And then, like, I think I'm more like when the stuff I'm talking about is more like,
*  well, what are the ingredients you need to actually implement something like this?
*  So it's less philosophical and more kind of like, what's the algorithmic formula here
*  like to really do this?
*  And so I feel like it's sort of complementary, like, which is why I think we'd have a good
*  discussion.
*  OK, so one more question here about open endedness, and I'm going to play a question for
*  you. And this is more about, you know, creativity and the idea of an objective.
*  So just as as an example, vacuum tubes.
*  I don't know what vacuum tubes were innovated or invented for.
*  Do you happen to know?
*  Yeah, I did research this.
*  I might be getting a little fuzzy, but I believe that originally they were just used for
*  experiments with electricity.
*  Like people. So they were invented for a purpose with an objective in mind.
*  So this is a case, though, where, you know, this is an example where the the product
*  that eventually was used in a different framework to build computers actually did come
*  from an objective driven pursuit.
*  And I'm wondering if so open endedness is a natural way to think about creativity and
*  how to be creative, how to generate creativity.
*  But I'm wondering if creativity would occur regardless of whether there's an objective.
*  Right. Yeah, that's an interesting question.
*  I think that objectives, you can have creativity with objectives, but it's much more
*  likely to be.
*  It's much less likely, I should say.
*  It's less likely that you would have creativity with objectives, because like, let's look
*  at there's like an old joke about grants, like when scientists apply for grant funding
*  that like, you know, the best thing to do is propose what the panel wants to hear, which
*  is the objective, and then just do whatever you really want it to do once you get the
*  money. And that just, I think, exposes the fraud of this, like the subjective stuff.
*  It's just it's basically a security blanket to make everybody feel like we're actually
*  on some kind of track.
*  But the truth is, the cool stuff happens off the track.
*  Does that mean that like there's no exceptions?
*  I mean, of course, there's exceptions.
*  There's going to be some exceptions. But there's a general rule like this is actually a
*  better, a stronger principle is when you're getting off the track.
*  Now, if you think about your example and like you're getting you have objectives, but you
*  still lead to something kind of interesting.
*  But you have to recognize that what really happened there is still ultimately not very
*  objective because it's usually it's a serendipity.
*  Like it's like, well, I was working on this thing, but actually it turned out that the
*  thing that was really useful about it wasn't what I was trying to do.
*  It wasn't the objective.
*  Like the computer is not what the vacuum tube people were trying to build.
*  So that step was actually a non objective step, not an objective step.
*  And you and basically the creativity happened when you abandoned your objective, you let
*  it go. So a lot of people have gone down a path that was objective and just had
*  serendipity and realize that something else is possible here.
*  And I think that falls into this kind of non objective interpretation.
*  This goes back to being motivated, intrinsically motivated.
*  And I'll use a quote you gave in the book.
*  This is, you know, pastures quote about being prepared.
*  Fortune favors the prepared mind.
*  And that seems important that you're at least pushing forward.
*  And it's really, I don't know, it just seems to be an important ingredient for me.
*  The work, the doing something seems important.
*  Yeah, like that's also like, you know, in the book, we say serendipity is not an accident.
*  So it's a kind of a similar notion.
*  And it's true that pushing forward is a form of prepared mind.
*  I guess I would agree with that.
*  And but there's also just like, I find it really interesting when I looked like at
*  Wikipedia's serendipity page, because there's like a serendipity, serendipity, I think
*  it has like a bunch of inventions and things and like all kinds of serendipitous
*  discoveries.
*  And it's really fascinating to look at it, like all these things people weren't trying
*  to do that they accidentally did.
*  But like microwaves, for example, microwave ovens, like there is research.
*  Yeah, true.
*  Yeah.
*  So but one interesting thing about it is there's that everybody seems to be really
*  smart and have a good track record, like all these people who had accidents.
*  You know, so how could this be an accident?
*  Like if there are accidents, then it shouldn't have to correlate with how smart you
*  are. Like, like probably the best person for having an accident is some lunatic on
*  the street running into the walls on the side of the road.
*  You know, they're going to have lots of accidents.
*  But it's not that's not what serendipity is really about.
*  I mean, the prepared mind means that you are opportunistic is what I think.
*  So it's basically you are willing to switch on a dime and you can see when new
*  opportunities arise. And that's where real genius is, I think.
*  Like, it's not vision.
*  Like people like visionaries.
*  They think geniuses are visionaries who like saw 15 steps ahead of the horizon.
*  I don't think of it that way.
*  I think it's that the geniuses are the people who actually realized that there is
*  something one stepping stone away before anyone else realized it.
*  It's like we have the thing we need right now.
*  It could change the world, but no one has yet seen the connection.
*  I mean, everyone has had an example.
*  Everyone's career path is defined by this.
*  No one is no one thought when they were seven that they're here's here's my path.
*  And then they followed it. But they might have started following something that was
*  interesting, which led to something else.
*  So everyone has experienced this.
*  And yet we don't we don't follow it in many other aspects of our life.
*  It's true. It's also a personal thing.
*  Yeah, it's not just about like discovery in this kind of like big,
*  grandiose form. It's like about your individual life, too.
*  Yeah. I mean, a lot of it is is non objectively driven.
*  I'm not sure everyone is every is every like maybe some people really just stuck
*  to the plan from day one. But but yeah, I think for most people,
*  there's a process of discovery. And I think life is open ended.
*  People are open ended individuals, not just us as a society,
*  but just your whole your individual life.
*  And because of that, that is an aspect of human intelligence.
*  It also needs to be needs to be understood and thought about and celebrated
*  and celebrated for sure. Yeah.
*  OK, so question number one here from an old old podcast guest.
*  This is Stefan Lenin.
*  He he uses AI to generate creative things.
*  So he studies creativity and wants to harness the power of AI to study creativity.
*  So so here's his question.
*  I can in your work, you talk about open endedness,
*  both in nature and in artificial intelligence.
*  Now, nature, of course, has been an inspiration to many, if not all,
*  AI techniques, obvious examples being genetic algorithms
*  that are based on evolutionary processes and neural networks
*  inspired by the stuff our brains are made of.
*  Yet in these cases, we use a rather simplified understanding of how nature works,
*  typically resorting to mechanical explanations that, as you say,
*  work towards some kind of optimization or a predefined goal.
*  So my question to you is as follows.
*  To make progress in AI towards open endedness,
*  do we need to adopt new models of biological processes
*  that do justice to open endedness in nature?
*  Or is the challenge even more profound?
*  And does the way we build artificial systems fundamentally obstruct open endedness,
*  like the way we store an analog signal that contains potentially infinite
*  amounts of information as a binary digit?
*  All right. You get all that?
*  Yeah. Interesting, interesting question.
*  Thanks for that question.
*  So the first part of the question is about
*  do we need to change something about how our algorithms reflect
*  which happening in nature?
*  And I think so that's it.
*  That's that's the sort of easier part of the question.
*  And I think that was a clear yes.
*  But like we have to realize that I think part of the point here is we don't
*  understand what's happening in nature.
*  Like that's why if we did, we would just make the algorithm work that way.
*  But is are the current algorithms up to date with what we do understand about nature?
*  I mean, of course, it's an open ended process, you know, pushing forward, right?
*  Because, you know, AI doesn't doesn't incorporate brain stuff at all.
*  Yeah, is it? That's that's true.
*  That like you could ask, you know, how up to date are we with
*  with our current understanding?
*  And I would just say that we're continually understanding nature better.
*  So at any moment in time, you know, it's probably we have
*  we have a better understanding than we did a few years before.
*  And the algorithms will accordingly be a little bit better.
*  But we're still not there.
*  We're still not at the point where I think we fully understand nature.
*  Like nature, the problem is that like you have all these textbooks
*  that explain evolution, like you read them in high school or maybe in college.
*  And and that's that's like a message that we understand it.
*  Like, here's the explanation. It's simple. It's like one chapter.
*  Like there, there you go.
*  Like all these profound things have been explained.
*  But the problem is that that's that explanation is not the same as a full understanding,
*  because that's just an explanation.
*  That's not the same as a formula, an algorithm.
*  Like to me, like to really understand would be to have the algorithm.
*  And that just is elusive. The algorithm of nature.
*  Like we can do evolutionary algorithms,
*  but nobody really thinks an evolutionary algorithm is the same thing as evolution in nature.
*  They're inspired by it.
*  They have some reflection of it, but it's just a shadow of it ultimately.
*  And so there's something we're missing.
*  And we are continually gaining more insight into what we're missing.
*  Like, I think now I know more about what's really going on in nature
*  than I did like 15 years ago.
*  But like, I still think there's things that we don't fully grasp or understand,
*  or else we would just do them,
*  because if we really understood it, we would just write it down as an algorithm.
*  And we can't and we haven't and have failed to do that.
*  And so it leads to the next part of the question of,
*  is there something even more profound going on here?
*  Like we're in the world of binary and digital,
*  but we really need to be in the world of analog.
*  That's way more deep.
*  And I think I don't know.
*  I don't know, because I don't know what I don't know.
*  But if I had to guess, I would say no.
*  I think that the at least for open endedness.
*  Now, there may be other parts of nature where we're off kilter,
*  like maybe consciousness or something.
*  Maybe that can't be done through digital computation.
*  But for open endedness, I believe it should be doable with digital computers.
*  I think we have the substrate we need and I think that we can conquer this.
*  Well, I think there wasn't his that was like a specific
*  the digital analog was a specific example he gave.
*  But but but I think he was asking more about our our biases.
*  Right. Just to think about everything as a goal driven.
*  And whether we can even escape that, even if we are aware of it.
*  Right. And, you know, whether we're asking the right questions
*  or if we're limited by that, that by our approach, by our biases and stuff.
*  Yeah, that's a different angle on it.
*  I didn't think of it that way. Interesting.
*  I think no, I mean, I don't think we are
*  like fundamentally limited by those biases.
*  Like we have we do come with these biases, but but I just think
*  we're flexible enough to get around them.
*  I mean, that's I mean, I'm getting around them.
*  So I think it's it's there's nothing like fundamental
*  about the human mind where it can't think in other ways.
*  May may take a little flexibility, but I think we have it.
*  So so I think I think we can do this.
*  Yeah, I'm probably stretching his intended question anyway.
*  But yeah, fair enough.
*  I don't want to describe that to him, but it's still an interesting angle
*  to think about. Yeah.
*  So but do you think that we are incorporating like what we
*  do understand about, you know, nature and evolution and the algorithms
*  that we do understand, are we incorporating them well enough?
*  Well, I guess, yeah, that's a good question, because I guess that's more
*  like a practical question, because it's like, what is the community actually doing?
*  And, you know, I would as I would situate myself
*  as somewhat of a rebel in that respect, that like, I really want to do that.
*  And others are more kind of satisfied to move in a more conventional way.
*  So some of us are are not constrained in that way.
*  But but I guess you could say that in general,
*  AI has been more conventional minded and less oriented towards open endedness.
*  So, yeah, we're not if we say we, I mean, like the whole community,
*  then we're not really we're not really taking this to heart yet that much.
*  But I think we're we're moving in that direction because of the fact
*  that it's becoming more and more popular to discuss.
*  And I see the word coming up more.
*  And like, it's also true that people in AI seem to like it when they hear it.
*  It's like they didn't even think about it before.
*  But when they hear about it, they're like, that actually is really interesting.
*  And so I think it's going to kind of like spread a little bit.
*  And you'll see it growing inside the community.
*  It seems like it's exploding, but I'm biased because I've been learning about it.
*  You know, so but given all the positive responses, you know, some of the comments,
*  why isn't this everywhere already?
*  Things like that, nature must give you
*  you must be optimistic about the progress.
*  Yeah, that's an interesting question.
*  Like it's about yeah, it's kind of like a strategic issue.
*  You're a meta question about the like, how is this going to actually spread around?
*  And it's it's
*  I think it's it is spreading.
*  And so, you know, is it exploding?
*  It's kind of a matter of opinion, what we call an explosion.
*  Definitely increasing in people's interest, but but it's still a small minority.
*  I think of like the whole community, like if you look at all of deep learning
*  or even all machine learning, like you're not most people are not
*  even thinking about open endedness.
*  So fair enough. That's still true, I think.
*  So there's still a lot of room for an explosion.
*  And I do I do think that it deserves more mind
*  share than it's still getting even now.
*  And I think it's you know, why is it not getting it is partly because
*  there's a very practical orientation to the field where
*  it's just like, how do we solve problem X?
*  And that's a lot of the way that we validate which direction to pursue
*  and where to put money, which is ultimately determines which research actually gets done.
*  So open endedness doesn't really serve that very well.
*  Like open endedness is really about like, well, let's just see what happens
*  if we don't know what's going to happen.
*  And that's like super interesting.
*  But I can't guarantee you that, like, you know, your telephone call center
*  is going to be three percent more efficient next year.
*  And so like that's just like, yeah, it doesn't align with this culture
*  of sort of practical problem solving.
*  But it may align with the idea of getting to something like AGI.
*  I mean, then like this, this is a more grandiose thing where like these kinds of more
*  philosophical issues come into play in reality.
*  But like in just like the general culture, the field, it doesn't it doesn't align perfectly with it.
*  So I think that's the limiting factor.
*  Let's see while we're OK.
*  So we let me go ahead and play the next question because it's related to
*  natural processes and applying them.
*  OK, so this is Melanie Mitchell, complexity scientist.
*  You're familiar with her.
*  I'll just play the question.
*  Great. This is Melanie Mitchell.
*  My question is this.
*  You brought a lot of new ideas to the field of evolutionary computation,
*  especially as applied to neural networks.
*  I'm wondering what you think are the most important new ideas
*  for evolutionary computation that come from biological
*  inspirations but haven't been used yet in the field?
*  Thank you. Well, I just want to say hi to Melanie, because I have met her and know her.
*  And it's cool to get a question from her.
*  And I really like her work.
*  So, yes. So coming from the biological side,
*  where are the opportunities in evolutionary computation?
*  I think that what's what's useful
*  is to reinterpret
*  biology outside of the conventional way of explaining it.
*  So I don't know if that exactly answers the way Melanie is thinking of it.
*  What do you think of the conventional way?
*  Yeah, what is the conventional?
*  I think we have a sort of a conventional narrative about what evolution is in nature.
*  So thinking really about biology in nature, just forgetting algorithms for the moment.
*  But like in nature, we think of it as a kind of death match.
*  Like you get this this term survival of the fittest.
*  And it's like there's a competition going on.
*  It's very competitive.
*  Like the narrative is competitive.
*  And we don't really question that.
*  That's a pretty well.
*  I mean, after you go to high school, that's sort of like the idea you get.
*  Like you are the product of like millions of years of brutality.
*  And now we've got this super hyper optimized being that can take on the world.
*  And I think that that narrative is not necessarily helpful algorithmically.
*  Like when you think about importing what we see in nature to to algorithms
*  that are artificial and useful to us and powerful and open ended ways,
*  that narrative isn't really what we need.
*  I think that like trying to look for alternative narratives in nature
*  is super inspiring and interesting from an algorithmic perspective.
*  And I can give an example of this,
*  which is what I would call the Rube Goldberg machine interpretation.
*  So this is a totally alternative way of thinking about it.
*  Like if so, instead of thinking about it as a competition.
*  And by the way, I'm not saying it can't be thought of that way.
*  Like it's possible for there to be more than one valid interpretation of a system.
*  So this is just a different interpretation.
*  It's not saying the other one is wrong because they're just interpretations.
*  But what the interesting thing about interpretations,
*  they lead to different types of ideas.
*  And that's why it's valid. It's useful to have different interpretations.
*  So in this interpretation, what I would I would we think about is this, you know,
*  instead of thinking evolution as a progression where like we went from one point
*  to like better and better and better points, think about it instead.
*  As nothing is changing ever.
*  And we're always doing the same thing.
*  And so in that view, what we have is a situation where there was a single cell,
*  which is like presumably the first cell on Earth,
*  and it reproduced and made another cell.
*  And so the thing that it did, what it accomplished was it got another cell
*  out of the first cell.
*  And if you look at it that way, then like every single organism
*  that's ever existed has only accomplished that at most.
*  I mean, some have accomplished less because they didn't reproduce.
*  But that's it. That's all they've done.
*  Well, I mean, look at what I mean, if you have a child and you're a human,
*  you know, they were a single cell.
*  What's the use of all the rest of it? It's not necessary.
*  Like we can get from a single cell because you were a single cell,
*  you know, when you started out and then all this this huge digression,
*  like multi trillion cell digression, which is human life, just to get another cell.
*  It's ultimately just the same thing that first cell ever did.
*  And so in this interpretation, the way you can think of it is think about it
*  like a Rube Goldberg machine.
*  Like we don't need all that to do what we did.
*  Like we don't need like all of these, like levers and pulleys and ramps
*  and things falling down and fires lighting up to open your newspaper
*  in the morning with some crazy, complicated machine.
*  I actually saw a guy who had this huge giant machine he built
*  to open his newspaper just for fun. Yeah.
*  We don't need any of that. Yeah, he's kind of pretty famous.
*  But we need action comic hero movies, right?
*  We may need that.
*  So I would have to go with that.
*  But like so in the same vein as like that Rube Goldberg machine
*  that opens the newspaper, we don't need any of this stuff to get another cell.
*  But what's happening, what's happening in nature is that we're riffing on this idea,
*  on this theme of making another cell in infinite variations for eternity.
*  And that is why we're getting so much interesting stuff.
*  It's that actually, like the interesting stuff,
*  like intelligence itself is totally orthogonal to like the constraints
*  of the process, which is just saying make one cell from another.
*  But if you riff on that theme forever, like you can get amazing things,
*  just like if you built machines to open newspapers forever.
*  Like eventually they can be as sophisticated as like the most sophisticated space station.
*  Like and so interior to that, there could be inventions
*  that are amazingly powerful, like human intelligence.
*  But it's a completely unnecessary digression from what actually has to get accomplished.
*  Unless it increases the ability to accomplish it.
*  Yeah, but you could say that.
*  But I would question so that that goes back to the metaphor,
*  like the original narrative, which is the competition.
*  Matt Nersy, it's really hard to drop that.
*  We want to go back to that.
*  So then people tend to go back in that direction.
*  And but the thing is, like, you know, you don't.
*  So the idea is better for accomplishing it is like now this progression idea
*  and things are getting better and we're in a competition.
*  And that's why things are changing.
*  But the thing you have to remember is it's not clear what we mean when you say better.
*  Well, higher probability of going from one cell to the next cell, let's say.
*  We don't. I mean, like a bacteria reproduces many more times than a human.
*  So any individual human is far inferior and actually also just in terms of total biomass on Earth.
*  The bacteria have us totally beat, as do the ants.
*  So we're not winning on any objective metric.
*  Biologists hate this kind of thing, by the way.
*  And I say things like this, they're like fitness isn't really meant to
*  like compare like one species to another in this way.
*  But that sort of to me is like being well, because you're kind
*  of like setting the rules of the game. So I can't have this narrative.
*  Like I think we should be open to these kinds of like interspecies comparisons
*  to make the point of the narrative.
*  Like there is no superiority here.
*  Like this is total Rube Goldberg.
*  Then you can I am superior to the apes.
*  OK, I don't imply biologists think we're superior because they would totally object to that.
*  I know that they don't think that. Yeah.
*  But yeah, but it's just this idea that like there's some advantage to it.
*  It's an advantage in some way.
*  It doesn't have to be an advantage.
*  All it is, is that every anything that can be will be.
*  That's the rule we need.
*  And then we'll get to see everything, everything that's possible within the constraints.
*  Like I think about it, like it's almost like milk pouring out on a spilling out on a table.
*  Like eventually they'll cover the whole table.
*  You get to see everything except where there's obstacles in the way.
*  Like it won't pour around walls.
*  And so to me, the walls are a metaphor for the constraints
*  that like you have to be a walking Xerox machine.
*  Like there's all these places the milk could spill,
*  like riffing on people who died and never reproduced, like maybe they had like genius.
*  They could have been really interesting lineages, but they died before they reproduced.
*  That's the that's the walls in the way of the milk.
*  It couldn't spill in that direction.
*  And so it doesn't go everywhere because of those constraints,
*  but everywhere else it will go.
*  And that's why we see all this cool stuff.
*  Wait a billion years and you'll get like amazing stuff
*  because everything that's possible is being revealed.
*  So, well, one way to look at evolution is that it's terribly wasteful and inefficient.
*  For the objective of of keeping cells alive or something,
*  you know, passing cells along, producing cells,
*  because it makes all sorts of terrible things through random mutation that
*  that maybe in a different environment could lead to something interesting.
*  But so the the idea of spaceship Earth, like it's this wonderful place
*  that is just perfectly suited for us.
*  The opposite side of the corner of that is this really harsh environment
*  that only a few. Yes, it's very complex, but only a few things would be
*  truly viable within that very constraining complexity.
*  So then I think open endedness seems even more inefficient than evolution
*  because what you want to do is break down those constraints
*  and really let it really explore.
*  What are your thoughts on that?
*  Well, I don't completely understand, like when open endedness,
*  if you say open endedness is less efficient evolution, because to me,
*  evolution is the canonical open ended system.
*  So it's sort of that is what open endedness is, is evolution.
*  But but you've I've heard you talk about how so evolution doesn't let
*  everything continue, right, because it kills off the things that don't suit
*  its suit your environment. Yeah.
*  And I know we're being very anthropomorphizing evolution, which is not good.
*  But but but open endedness would be much more forgiving.
*  If I understand. I see, I see.
*  Well, that's just that's one dimension of open endedness,
*  which is like the degree to which is open.
*  Like it's true that like the ultimate open ended algorithm
*  would just explore literally everything, like every single organism
*  would get to have children.
*  What's the word for that gradient?
*  The openness? Is there a word for that?
*  I mean, that's like random search and stuff.
*  Well, that's it. Yeah, it's kind of like random search.
*  Like everywhere the search goes, basically, we'll just keep going.
*  Yeah. But it it's a population based random search.
*  It's also I call it gentle earth.
*  Like it's like a metaphor in a way.
*  Like it's an earth, gentle earth, like an earth where nobody
*  where nobody fails to reproduce, where everybody reproduces.
*  I think it's interesting to think about
*  general earth and what it would be like, you know, because it's like
*  all of the branches of the tree of life that were pruned.
*  What if they weren't?
*  What would be on this planet right now?
*  And I think, you know, there'd be a lot of blobs
*  that don't do anything on the planet right now.
*  There'd also be a lot of interesting stuff that we've never got to see.
*  So there'd be both.
*  And so when we talk about inefficiency, it's
*  it's it's a little fuzzy.
*  Like what what is efficiency with respect to?
*  Like inefficient with respect to, you know, producing efficient,
*  viable creatures or maybe efficiency with respect to producing interesting new stuff?
*  Well, I think just even in terms of resources.
*  And I know that brings it very down to earth, but just the resources,
*  you know, like AI gets gets knocked for being
*  for taking up a lot of energy, for instance. Right.
*  But if you're going to run an open ended algorithm, if you're doing it well,
*  I suppose it would take up almost infinitely more.
*  Yeah. Resources. I see.
*  Yeah. OK. Yeah. In terms of resources.
*  Well, no, I think I don't want to grant that because I think
*  I think actually it's a better.
*  Well, let's see.
*  I think it's a better use of resources in some sense.
*  Like it's like let's we have this amazing computational power.
*  Let's say we do like some amazing supercomputer.
*  You can't give me an algorithm that really uses it.
*  Like there's nothing to do with it.
*  Like what the heck is going to actually like exploit this resource?
*  Now, I know that in deep learning, we can do some amazing things
*  with with really big computation.
*  So I'm not even talking about that.
*  I'm talking about things that are even bigger than that.
*  Like I'm talking about things where I could run it for a thousand years
*  and it would like change everything.
*  There's nothing available for that.
*  So like the most efficient use of that would be an open ended algorithm.
*  It would actually exploit it to the maximum extent it could be exploited
*  and show you everything you ever dreamed of would come out of it.
*  In the long term, in the ambitious long term, in the long term. Yeah.
*  But I mean, that's what that's I mean, computation is space and time.
*  So it's taking advantage of space and time.
*  That's what it should be doing.
*  It requires more patience, though.
*  I mean, you know, like like you've heard on this is not in our culture right now.
*  Patience is not part of our culture.
*  Yeah. And it's a practical problem, too, because it's like, I mean, you can't.
*  We're not going to get to see it in our lifetime.
*  It's going to take a thousand years.
*  So it's true that like in practice, like if we're going to explore open ended
*  algorithms, we need to make ones that produce something
*  worth our attention within our lifetime or else like we're not going to have
*  the patience in practice.
*  That's just a practical reality.
*  And so it is this is like a real dilemma, I think, in the field is that you don't
*  know when to stop your run, because like, you know, if I ran it for five days,
*  you know, and it was OK, but maybe if I had run it for 10, it would be amazing.
*  But when do I actually stop and know to say like, yeah, this is the evidence I need?
*  And that's just an aspect of open end is you don't know when to stop it.
*  Can I ask you about learning with respect to open endedness?
*  Because so all these neural networks and I know I'm focused on deep learning,
*  which and you, you know, you have plenty of deep learning experience and and,
*  you know, you even work on neurally plausible back propagation like DOPA.
*  What do you call it?
*  Oh, the yeah, the different end is a differentiable plasticity with Thomas
*  Mcconnie, which is really his ideas.
*  Yeah. Anyway, but then, you know, you heavily have a evolutionary algorithm
*  and, you know, neuro evolution type of background as well.
*  Yeah. But learning.
*  So learning, I want to I want you to help me in my own mind,
*  compare learning and open endedness, because learning has is normative.
*  It has a direction.
*  It actually, by definition, has a goal because you're learning toward
*  you're getting better at something. Right.
*  And and some people think of even evolution as a really slow
*  learning algorithm.
*  But what I think that you don't think of it that way.
*  So so it's more so I almost want to compare learning and evolution
*  because you think of evolution and open endedness in general
*  as sort of a search process that finds interesting things.
*  And evolution might it might find usefulness within the domain of life.
*  Right. Do I need to think of learning more like that search space?
*  Or do I need to think about evolution
*  and that searching more like a learning mechanism?
*  Does that make sense? Yeah.
*  Yeah. It's a really interesting comparison to put evolution
*  next to learning and say, well, how do they relate to each other?
*  Especially open ended evolution and learning.
*  I think one of the things that it points to is just that learning,
*  in my view, at least, as you alluded, can be open ended.
*  I think that, you know, open ended is not just only about evolution.
*  And that's an important kind of social point,
*  because the open ended evolution community,
*  by adopting that word evolution,
*  is sort of implicitly excluding lots of people working things like deep learning.
*  They don't mean to, I'm not saying they mean to, but just the terminology excludes.
*  And so I think it's really important to open up the terminology
*  and acknowledge that open endedness is a property of many kinds of systems
*  that are not necessarily evolutionary systems.
*  And that's what's so amazing about it, including learning systems.
*  And so I think because of that, we have to we have to allow for learning
*  to be open ended.
*  So it's a slightly different than just kind of normative view of learning
*  that you're describing.
*  And I think, though, that it's it raises interesting questions,
*  which is which illustrates why it's important to at least try to think this way
*  and explore this idea.
*  Like, because if you think about it, like as a human being, clearly we can learn.
*  We're learning systems like that's why we're inspired to do machine learning.
*  And and yet it's arguably like our path is very open ended.
*  As humans.
*  I mean, we discussed this a little earlier even in this program,
*  like because you were you're pointing out like how like an insert a career paths
*  like can have a very kind of open ended discovery process.
*  And but I would argue even like early in life, like babies and toddlers,
*  like it's not clear to me that this is an objectively driven process.
*  You know, when I watch my baby,
*  there's not one like learning how to walk or something.
*  Congrats. The memories from way long ago for me.
*  That's awesome. Yeah. Yeah.
*  And he like he's been like it's not just him,
*  but I generally observe that it doesn't seem to me that babies have a goal in mind.
*  They're just trying whatever they fit.
*  And the thing is, they're following stepping stones.
*  It's like once they they're shaking their arm around, they realize it hits something.
*  They just realize you can hit things. That's kind of interesting.
*  So maybe you can hit things and then you realize,
*  actually, I can hit things to places that are useful to me.
*  And then maybe I could hold things.
*  And it's like this isn't because like the baby started out with this big plan.
*  Like, I got to figure out how to hold things.
*  It just sort of banged into things, found stepping stones.
*  But once it did something, it realized it could use that to do something else.
*  And so you might argue that like this whole developmental process,
*  which certainly I think deserves to be called learning, is a completely open ended process.
*  It's just that it's also inevitable.
*  So there's this there's this interesting notion of inevitability and open endedness
*  that like at the beginning of an open ended process,
*  it's not necessarily the case that everything that happens is totally unpredictable.
*  Like there can be open ended processes where the early stages actually are pretty predictable.
*  Like you're going to run into things like holding and walking and stuff like that as a baby,
*  even if it's open ended.
*  But eventually, because our whole life is open and it is not predictable.
*  Like you could not predict when you're a baby that you're going to be running an interview show.
*  Like, that's not clear from the path.
*  Yeah, that was my point from earlier.
*  But even even doing this, like so I know that right now this goes back to the motivation.
*  As long as I do what I'm doing right now and give it my all in 10 years,
*  I'm probably not going to be doing this podcast,
*  but it'll lead to something interesting that I value.
*  Yeah, yeah, indeed.
*  And but I think that's that's just an illustration of learning open endedly.
*  Like in the process of discovery that you're going through,
*  like I think you certainly deserve to be credited with learning.
*  Like, I mean, we could say it's something other than learning.
*  But I think it's a form of learning.
*  It's a learning without a curriculum is what it is.
*  Like no one has laid out before you the steps that you should take along this path.
*  You just are stumbling through them.
*  But but as you stumble through them, you are actually learning a lot
*  and becoming more more wise about like, you know, all these aspects of the world,
*  which are just the ones you encountered, which are different than the ones I encountered.
*  But all of us have a different path like that.
*  And I think that there's a lot of wisdom gained along each of those paths.
*  And that's learning.
*  All right, Ken, let's let's switch gears for the last little bit here.
*  So I know you've had an interest in brains and in the way that brains work,
*  you know, going back to the beginnings of your interests in, you know,
*  before machine learning, I think, I don't know if that's what got you into the whole thing.
*  And you often say, who knows?
*  At the end of a talk, you say, who knows?
*  Open-endedness may just very well get us to AGI without trying.
*  And then but then you also show a picture of a brain and say AGI
*  and you say and get us to a brain.
*  All right. Do you think of AGI as human level intelligence?
*  Or is there some other way that we should think about?
*  And then I want to talk about intelligence in general eventually as well.
*  Cool. Yeah, I think that so AGI is well, these terms are really contentious for some reason.
*  Like there is AI, there's AGI, there's a human level intelligence.
*  It's very controversial.
*  Kind of strong open-endedness.
*  Yeah, well, there's a new one out there.
*  And but so like I'm not as tied to necessarily a specific terminology,
*  but I think with an understanding of what AI is really meaning to to articulate,
*  it's it's it's an idea.
*  Well, I guess that is AGI intelligence.
*  So or is it human intelligence?
*  It's well, AGI refers to generality because the artificial general intelligence is what it stands for.
*  And I guess that in some ways, it may be an oversimplification,
*  I guess, in my view of what we're actually aiming for,
*  because the word general is promoted so much there.
*  Like it's not like only generality is really the issue here.
*  But generality is an issue.
*  It's just there's other issues.
*  And I understand the allure of generality because it's based on like we seem to be able to do so many things
*  that like we're basically general intelligence, you could say.
*  But the thing that it kind of misses is the fact that like we're also like extreme specialists.
*  In other words, we specialize like our lives are about specializing.
*  Like it's very unusual.
*  You find a master in two domains, let alone three.
*  I don't like, you know, like, like one of the top 10 best basketball players in the world
*  and the top 10 physicists in the world at the same time.
*  Like it just doesn't happen.
*  And that's because people specialize over their lifetime when they become great at something.
*  And so to talk about general intelligence kind of papers over or fuzzes that out,
*  that like the extreme specialization is also a characteristic of being human.
*  And we may we may theorize that somehow that won't be in the age yet.
*  Like maybe the age yet we imagine something that actually is a master of everything.
*  So it's not like a human because humans don't seem to do that.
*  But I would argue that that might actually not work.
*  Like maybe maybe there's a reason that like we also need to specialize.
*  We have amazing general capabilities.
*  I'm not denying that, but we also have amazing specialization capability.
*  So it's very complex, like how these things mix together.
*  And so I don't think the terminology necessarily elicits all of that.
*  But I mean, we can use that terminology if we want to have something to talk about.
*  It's fair enough to me to call it AGI.
*  Yeah. I mean, I mean, we can just call it AI.
*  We can call it whatever you want to call it.
*  OK, well, you know, considering like open endedness, should AI take any.
*  Guidance from looking at brains for or just any natural intelligent processes.
*  Yeah, I mean, at least in the sense I mentioned about babies and open
*  endedness throughout lifetime, but that's almost an objective, right?
*  Sorry to interrupt.
*  Hmm. Is it almost an objective?
*  Well, now, what exactly are we saying is an objective there?
*  Well, I don't in some sense, it seems antithetical to to open endedness
*  to say if we want to solve AI or create AI, what we should do is pay attention
*  to brains, for instance, if we wanted to.
*  I see what you're saying. Yeah.
*  Shouldn't we explore beyond the boundaries of what brains can do?
*  Yeah, yeah, yeah. Can do shouldn't should we not?
*  Should we use that as guidance?
*  Well, the confusion that's happening here is because we're actually in an open
*  ended sandwich because there's open endedness on both sides of us.
*  Like the open endedness that leads to us is very different from the open
*  endedness that's inside of us.
*  You know, we are we are open ended.
*  That's the open endedness inside of us.
*  So your lifetime has an open ended aspect to it.
*  And your intelligence is very tied in with being open ended.
*  I think I think human intelligence, maybe the pinnacle of what makes human
*  intelligence human is its open endedness, like not problem solving,
*  but this tendency we have to explore.
*  We're just amazing at that.
*  And that's why there is a history of invention and civilization and so forth.
*  But there's also the open endedness that precedes us.
*  That's a completely different thing.
*  That is the explanation for how we got the brain we have, which is evolution.
*  And so like we're in the middle of that.
*  And so you can you can conflate those two things when you talk about open ended
*  because they're different from each other.
*  But it's really interesting that they both are open ended
*  because like you could say that, well, like to understand the openness
*  that precedes us, it won't help us to create a goal, which is a target,
*  which is the open endedness, which is inside of us.
*  Like that might be an argument you can make because that suddenly does
*  sound like it's a goal.
*  But I think that's conflating two different things.
*  Like when I'm saying that the open endedness that's inside of us is interesting
*  from like a neuroscientific point of view, informing AI, it's about like
*  the question once you when you get to us, what is it actually like?
*  Like what kind of structure do you expect to get if it's like a human?
*  Like it's got to have this open ended property to it.
*  Like the cognitive aspect of it is going to be open ended.
*  And and I'm just saying that I think that that sometimes is
*  under emphasized, like in AI's interpretation of cognition,
*  because it's often viewed sort of as a problem solver
*  or like a or like a classifier or something like that.
*  And and so and so it might be helpful for us
*  in understanding what kind of thing we're aspiring to here
*  to realize that there's this really, really magnificent aspect of our humanity,
*  you know, which is our open endedness,
*  which might be getting a little bit of short shrift.
*  And I would like to understand what actually accounts for that, you know,
*  because like a lot of our a lot of our the metaphors that we're using,
*  like we think about things like back propagation
*  are related to the idea that there's a target that we're moving towards.
*  But what is actually the cognitive apparatus of open endedness
*  from an algorithmic point of view or like a neural propagation point of view?
*  I mean, it's extremely hard question to answer
*  because it's like to reduce very abstract concepts into real
*  like neural network explanations is like, who knows when that's going to happen?
*  But it's still, I think, interesting to think about that.
*  And then on the other end, the evolutionary side is like
*  it's a whole other reason that we need to think about it,
*  because we might need it to get to that point.
*  I mean, do you think that our conception of evolution
*  or just the ontology of how biology works?
*  Are we there? Have we solved it?
*  I mean, I know that there's remaining questions in evolution, but
*  is there going to be a radical new theory idea
*  that is going to frame our understanding of, you know,
*  that that can encompass something like open endedness
*  and our cognitive abilities like that?
*  Well, I think we're not.
*  Yeah, I don't think we're there.
*  Sort of as I kind of hinted before, like, I don't think we have the full theory,
*  whatever that is, that like accounts for open endedness in evolution.
*  I don't think we're there.
*  Because, again, I think that to get there, if we were there,
*  then we could actually just implement it as an algorithm and we'd be done today.
*  And so there's something we still don't fully understand,
*  but I think we're closer.
*  We do understand some important ingredients, I believe, at this point.
*  And so that then leads to this question of the grand theory
*  and basically a shift in theory and evolutionary theory.
*  And that leads to all kinds of controversy, is what I've noticed.
*  Like, it's the evolutionary theory is is is just such a crowning achievement
*  of science that like saying that there's like a fundamental shift,
*  even suggesting it is like just like like an absolute heresy
*  that there even might be one.
*  And so like because because I think what scientists are afraid of
*  is that you're hinting that like some of the main underpinnings
*  of the theory are wrong, the current theory.
*  And that's that that that might that's not necessarily true.
*  Just because we see we have a fundamental shift in understanding
*  doesn't mean that some underpinnings are wrong.
*  You know, like there's still selection going on.
*  I mean, these things are happening.
*  And so somehow you have to like thread that needle of like preserving
*  the parts that and acknowledging that there are parts that are worth
*  preserving here, but still saying that there are still fundamental
*  insights to be had.
*  But broadening, broadening the I'm sorry, broadening,
*  broadening like the overall narrative of what's going on.
*  That's why I mentioned like having new narratives,
*  like new interpretations is helpful.
*  And and but but to really do that, I doubt I'm the person
*  because because I'm not a biologist, so I'm not equipped
*  because like this is like it's so the politics are just so complex.
*  And I probably don't even begin to understand them.
*  But I've seen other people like I've seen people who thought what I
*  some of our stuff was interesting, who were invested more in biology,
*  who tried to kind of push the needle a little there with some some new theories.
*  And I've seen that they run into dramatic resistance.
*  And it's probably appropriate because the theory is is so powerful.
*  Yeah. Yeah.
*  But but I do think like that we're going to have to do some updates
*  because because again, I believe that you can't really claim
*  to understand what's going on if you can't implement it.
*  So my bar is kind of the AI bar.
*  All right. If you're if you got if you biologists really understand it,
*  so well, then just write a program and we should see.
*  We should see nature in all its glory inside the computer.
*  It is it is ironic, though, the new, you know,
*  oh, you dare question Darwin, burn him at the stake.
*  You know, the new heresies.
*  That's fun. Yeah, it's a little ironic.
*  So neuroscience is OK.
*  So this podcast is tries to at least
*  ostensibly is about the interface of neuroscience and AI.
*  And neuroscience gets a lot of criticism these days for
*  sort of being stamp collecting and not having enough theory
*  driving the experiments.
*  And we're collecting more and more data.
*  But then, oh, where's the theory that will, you know, that we can
*  frame the narrative of and then do better experiments and
*  understand what the data is about.
*  Does neuroscience need open endedness, that type of pursuit?
*  Because we write, you know, I'm not in it anymore.
*  They write grants, you know, like you said earlier, to pretend like they're
*  working on their grant question, right? Yeah.
*  And then make progress that way.
*  But but does there need to be a more open ended sort of pursuit?
*  And neuroscience, do you think?
*  I would guess the answer is yes.
*  But I want to admit that I'm not a neuroscientist,
*  so I can't really credibly critique the field.
*  But it's like every field seems like they need more open endedness.
*  I would guess that in neuroscience, what that means is it's about
*  the idea that, you know, there are some neural phenomena
*  that I just want to look at, but I don't really know what they are.
*  Like, I don't know what they mean.
*  I don't know what they explain.
*  I don't have a theory. I don't know.
*  But I just have a gut feeling that like this is interesting.
*  And that's why I want to I want to give me five hundred
*  thousand dollars so I can look at this.
*  And I bet you that's like impossible.
*  Like, you can't say that. Yes.
*  And that is that is a sense in which it could be more open ended.
*  Is that we should let people say that because like some of our discoveries
*  are going to be because something's interesting, not because we even know
*  what the heck it is, especially in a system as complex as this one.
*  And our neuroscientists that have been trained
*  for like 30 years or something before they become a professor,
*  whatever they are, a scientist.
*  They got this depressing.
*  You know, they they they deserve a little bit of acknowledgement
*  of like that effort that we put into them.
*  Like they also put an effort, but society has invested in them for decades.
*  Imagine how much we've spent on this.
*  Like, can't we just acknowledge that after all those decades,
*  maybe they have their intuition is worth following.
*  Like that something's interesting.
*  Like they don't have to have a theory like they're they're mature enough.
*  Now, as scientists, they actually might have beyond to something
*  when they have a gut feeling.
*  I'm not saying they shouldn't have to justify their feelings at all.
*  Like I wouldn't accept a grant that just said, this is cool.
*  Like, let's look at it.
*  But I can explain to you, I can explain why something's interesting to you
*  without knowing where it's going.
*  Like I should be able to do that.
*  We don't challenge ourselves to do that enough, I think.
*  Like just to say, this is why I think this is really cool.
*  And I will not tell you what's going to happen if I investigate it.
*  But it's not like we're not idiots here, like who can't communicate
*  with each other just because we don't have an objective.
*  Like there's a lot of other stuff we could be talking about
*  other than just where we're going, because we just don't know.
*  I mean, that's the nature of exploration, which is what science should be about.
*  So, yeah, I think I'm sure that neuroscience, because it is
*  it is one of these fields where there's so much we don't know.
*  And we're in such a morass of complexity that absolutely it could be better
*  served by allowing some of that kind of exploratory investigation.
*  I'm sure that's true.
*  Not saying it should all be that way.
*  OK, that's the straw man everybody likes to attack.
*  Like we can't get rid of all objective. That's crazy.
*  I'm not saying we should get rid of all objectives.
*  No, of course not.
*  But let's put some resources into this kind of thing and start acknowledging
*  that we've invested enough in these people so that their intuitions actually matter.
*  But this is a different sort of usually when people think of a bottom up approach,
*  they think of a data driven approach, like see what's in the data,
*  collect the data, look for patterns and then use those patterns to map
*  on to whatever cognitive function that you think you might be working,
*  want working on, et cetera.
*  But what you're what open endedness suggest and what you're suggesting,
*  if I have it right, is that it's a different kind of bottom up approach
*  that explores.
*  I don't know if intuitions is the right
*  is the right word for it, but do I have that right?
*  Is it do you see it as a bottom up kind of approach, not just a neuroscience,
*  but to A.I. as well?
*  Yeah, yeah, that's true.
*  I see the bottom up top down point.
*  It is maybe kind of bottom.
*  Is it bottom up? I have to think about it.
*  Does it really mean is it really bottom up?
*  Because it's not going from theories of what?
*  So in neuroscience, there's this kind of tension between creating
*  theories versus doing experiments and collecting data at the,
*  you know, on the implementation level versus creating theories
*  about what's computationally going on.
*  It's true. Yeah.
*  Yeah, it does seem like that that's reasonable to say.
*  So, yeah, it is about
*  it's kind of like, what would happen if I did this?
*  Like, I don't know, but I'd like to know.
*  But it's not just, you know, yeah, it's not just collecting tons of data.
*  I mean, maybe data collection is sort of open ended.
*  I mean, that like if it's just for the sake of getting the data,
*  like, I don't know what I'm going to see, but I'm just looking at it.
*  Then then you're kind of saying, I don't know, I just want to look at this thing.
*  That's what I want to be paid to do.
*  So, like, yeah, there's some degree of open endedness there.
*  So, Ken, you're at the University of Central Florida,
*  and then you went to I think I have this right.
*  You went to Ubre AI Labs.
*  Now you're at Open AI.
*  Congratulations on the.
*  Wow, it's a very new new job, right?
*  Well, a couple of months old.
*  Four months. Yeah. Yeah, thanks.
*  What are you doing?
*  What's going on at Open AI?
*  You're heading a team of open ended.
*  What do you call them? Open ended ers?
*  What do you call your team?
*  Well, this the open ended this team.
*  But I think maybe she calls it open endeders.
*  Is it that's a tough term to say?
*  That's terrible.
*  It won't take you long to come up with something better.
*  Not very catchy.
*  But yeah, I started the open ended this team at Open AI,
*  which is great because that's what we've been talking about here.
*  So, yeah, I'm really trying to push forward
*  the progress that we've just been discussing.
*  And Open AI saw the potential for open endedness
*  to dovetail with the aspirations of AGI, which I agree with.
*  And I think that the amazing talent and resources there
*  with respect to machine learning and deep learning
*  are very compatible and complementary to the goals of open endedness.
*  And open endedness is complementary to their goals as well.
*  So I think it's a really good, good pairing of ideas
*  and makes it a great place to be to be exploring this topic.
*  So it's more of the same.
*  You're just on a different scale and with a different team sort of.
*  In a way, yeah, it's interesting that it's like,
*  I guess it's the first time that I've really led a team
*  that's just is like explicitly called open endedness.
*  Like my entire career before this, I think I've been sort of implicitly
*  trying to pursue open endedness
*  and not because I necessarily always like hiding it or something.
*  But I don't think I really fully crystallized
*  what I'm really interested in until maybe recently.
*  Like I was probably everything I've been doing.
*  Like you could see how it has something to do with open.
*  It's like going back to neat or something like that,
*  which is like evolving, increasing complex neural networks.
*  But I wouldn't have used that term back then.
*  And, you know, I just kind of gradually realized that's what I've been really.
*  It's really been inspiring me for some reason.
*  I don't know why I'm so inspired by this.
*  But so like this is like I finally, yeah, like made it concrete.
*  So it's explicit.
*  Let's pursue open endedness.
*  That must be satisfying.
*  Yeah, it is satisfying.
*  It's it's weird.
*  I mean, I once I wrote something when I was like 16, like a program,
*  because I just read about evolution in the biology textbook.
*  And I was like, this should be I could write this in basic.
*  Basically, it was a programming language that I knew.
*  And it's like it sounded like a program.
*  I was like, this must be.
*  But the thing I really wanted to do when I was 16 was I was hoping
*  like something would evolve that was like weird or interesting.
*  Like I had no goal in mind.
*  I just was like, some cool stuff might happen.
*  And I created the most crazy program ever.
*  Like, I don't it's not scientifically valuable at all.
*  But but it was it's interesting to think back to that,
*  like going back to when I was 16, I was like pretty much pursuing open endedness
*  like right there for decades ago.
*  So I think I just somehow I don't know why.
*  It's just like what I'm interested in.
*  And so it's really great. Yeah, to finally do it.
*  Yeah, it's like just justifying your entire implicit career up to now.
*  Yeah, yeah, exactly.
*  It feels like it's a validation or something like actually doing open
*  endedness for real now.
*  Well, I'm looking forward to seeing where things go, where, you know,
*  where it takes you.
*  If you're right, you will not be studying open endedness in the future sometime.
*  Right.
*  Why? Because it well, because the search space is so vast
*  and you're amenable to searching within that space.
*  Yeah, I agree completely.
*  Like, yeah, who knows?
*  I could see myself deviating.
*  So that's true.
*  There was one thing I wanted to ask you before I let you go about open endedness
*  and its relation.
*  I have like such a long list.
*  You wouldn't I should just send you my notes.
*  I don't want to take you so long to read the funds.
*  I just had so many questions.
*  I'm not going to show you my notes.
*  But one of the you know, because so many things come to mind
*  when you let yourself swim in this space a little bit.
*  And one of those is so there's this idea of focused learning and
*  so called diffuse thinking, focus thinking, diffuse thinking.
*  Anyway, it's the thing where you're working hard on a problem.
*  You're really focused on it.
*  You kind of get stuck and you kind of keep hammering at it.
*  And then you walk away and you make one of those super complex sandwiches
*  that we talked about earlier.
*  And you take a shower and then you, you know, you're unfocused
*  and you're unconscious processes.
*  Is that open endedness at work in the brain?
*  Well, there's that's a really interesting question.
*  I mean, again, another interesting question that I've thought about.
*  And it's it has some elements of when we talk about open ends, like, for example,
*  one of the elements is you're not trying to solve the problem and you solve it.
*  That's clearly like a kind of a non objective process.
*  In that sense, at least like it like I became.
*  What finally made it possible to figure this out was to stop trying to solve it.
*  And that's like, you know, to achieve your highest goals,
*  you must be willing to abandon them.
*  It's like totally compatible with that notion.
*  But then the other aspect of it is that it's kind of mysterious,
*  like because I don't actually know what is going on subconsciously
*  because it's subconscious.
*  So like I went in the shower and it popped out in like was
*  was the thing happening subconsciously actually open ended itself like that?
*  I don't know what's going on. It's possible.
*  Like I think it's plausible.
*  Like your brain is following stepping stones kind of casually.
*  And because of that, it's willing to entertain options
*  that you wouldn't normally consciously consider.
*  And so maybe that that was what freed you and liberated you
*  to actually find a different path in like back to where it actually leads,
*  does lead to where you're hoping to go.
*  And so it doesn't seem crazy to think stuff like that could be true.
*  And I've definitely experienced it, too.
*  Like I actually explicitly, intentionally try not to think about things
*  that are like when I realized that there's like a really big problem
*  that I wish I could solve, I just shut it down for a few months.
*  I'm like, I'm not even going to think about it.
*  And yeah, yeah, because I'm like, I just feel like it's not the time.
*  Like as soon as you as soon as I have a feeling like I'm really trying hard,
*  then I'm like, this is a sign that it's not the right time to do this,
*  because that's very objective.
*  Look, when you're trying to do it like really hard.
*  And it's like it's too hard of a problem, like really hard problems are not like
*  that. You can't just try them.
*  You got to let it kind of settle in in some way that like,
*  if you don't know what's going on, but it might happen.
*  So actually try to do that.
*  That's really great.
*  That means my whole life is a waste because everything seems very hard.
*  You know the story about Edison,
*  the way that he would come up with solve problems, solve things that he was working on.
*  He he would hold two metal two metal balls in his hand
*  and sit down in his chair and like start to doze off.
*  And as he would doze off, he would drop the metal balls and they would fall into
*  this metal pan and make this loud noise and wake him up.
*  And often that would he'd have the solution.
*  That's the maybe that's urban legend.
*  But it sounds like that's a fun one.
*  Yeah, actually didn't I didn't know that.
*  Something I should have known about, like it's a cool legend, at least.
*  Yeah, like that is, I mean, the subconscious and just it's
*  yeah, it's a little more
*  amorphous than like it's not like algorithmic because you can't really say
*  what is what are we proposing here to do?
*  But but like it seems it seems somehow about the same kind of stuff.
*  And I feel it feels like it like viscerally, like when I have an idea
*  that came from like not thinking about something,
*  it doesn't feel like I was trying, you know, it just it feels like it came out of left field.
*  That's why it's like a Eureka type of situation.
*  You know, you're like, where did that come from?
*  I get just like popped in.
*  Yeah, which is beautiful and frustrating.
*  Yeah, it's true.
*  Yeah. Yeah.
*  Partly because you don't know if you can ever do it again.
*  Like there's no formula to that.
*  So I always feel worried if that's how I thought of something
*  because I'm like, I don't know what I just did.
*  Like I said, I can't repeat the process. Right.
*  Well, this has been really fun.
*  I appreciate you taking the time with me and letting me
*  ask my silly questions of you.
*  You must get so many silly questions from people.
*  Interesting questions, let's say.
*  I don't think they're silly.
*  They were really it was a great conversation.
*  Yeah, these are great questions.
*  Well, I appreciate it. I wish you luck.
*  Moving miss you luck moving forward.
*  Although, of course, you don't need it.
*  But but thanks, Ken.
*  Thank you. Thanks for the opportunity.
*  It was really fun.
*  Brain Inspired is a production of me and you.
*  I don't do advertisements.
*  You can support the show through Patreon for a trifling amount
*  and get access to the full versions of all the episodes
*  plus bonus episodes that focus more on the cultural side,
*  but still have science.
*  Go to brain inspired dot co and find the red Patreon button there.
*  To get in touch with me, email Paul at brain inspired dot co.
*  The music you hear is by the New Year.
*  Find them at the New Year dot net.
*  Thank you for your support. See you next time.
