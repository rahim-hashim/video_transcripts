---
Date Generated: April 20, 2024
Transcription Model: whisper medium 20231117
Length: 6161s
Video Keywords: ['Education', 'Science', 'Technology']
Video Views: 5532
Video Rating: None
---

# BI 127 Tom√°s Ryan: Memory, Instinct, and Forgetting
**Brain Inspired:** [February 10, 2022](https://www.youtube.com/watch?v=zoH8G3sG3M0)
*  We were in the privileged position to be able to label and manipulate specific ensembles of cells in the hippocampus of awake behaving mice.
*  And in doing so, we were able to show that these specific ensembles of neurons were somehow containing,
*  or at least are a part of the information that is allowing us to hitchhike on to specific memories.
*  You start to think more about the value of forgetting, and that forgetting may be not so much a bug of the brain, but a feature.
*  As long as they're made of the same stuff.
*  Then you create the opportunity for the evolution and learning processes to be continuous.
*  And in a sense, what you have is this rolling evolution convergently happening by learning evolution, learning evolution, influencing one another.
*  This is Brain Inspired.
*  Welcome everyone, it's Paul.
*  So in the last episode, I spoke with Randy Galistel about his idea that the engram, the physical substrate of our memories,
*  must be stored within neurons in some sort of stable molecular substrate.
*  That goes against the grain of most modern neuroscience, which suggests our memories are somehow stored among the connections and structure of ensembles, or networks of neurons.
*  Today I speak with Tomas Ryan to talk about the modern neuroscience of engrams,
*  and his work using techniques like optogenetics to label and control specific ensembles of neurons that are known to be involved with specific learned memories.
*  Tomas runs his lab in the University of New York, and he's working on a study that's been published in the New York Times.
*  Tomas runs his lab at Trinity College Dublin, and he and his colleagues have been able to take advantage of modern optogenetic techniques to do things like teach an animal some new behavior,
*  for example, teach a mouse to avoid some part of its cage where it has learned it may get a mild foot shock,
*  then Tomas can label neurons that are involved in learning that new behavior, then they can make the animal forget that new behavior,
*  so for example, the mouse would no longer avoid that unpleasant part of its cage, as if it has no memory of the learned behavior,
*  then make the animal again remember that learned behavior, in this example again avoiding part of its cage,
*  and they do that just by stimulating the ensemble of neurons that were originally labeled as being involved in learning the behavior.
*  So based on many experiments like this, Tomas has the view that the engram is stored in the structure of ensembles of neurons.
*  So we begin by comparing this view to Randy Gallistole's intracellular engram story,
*  and then we go on to talk about Tomas's other research and theories about the role of forgetting
*  and how forgetting can be viewed as a form of learning based on the predictability and affordances and demands of the environment.
*  We talk about instinct and Tomas's ideas on how instinct and memories are overlapping concepts
*  and how memories learned during our lifetime may become instincts in future generations, and we talk about plenty more.
*  Show notes are at brandinspired.co.uk slash podcast slash 127.
*  So since I had Randy Gallistole on last time as a slight background and transition from that episode,
*  I'll just play a short segment when Randy actually brings up Tomas, and Randy didn't know that Tomas was going to be on this episode,
*  so I'll play it since we begin this episode with their differing views on the engram. Enjoy.
*  Tomas Ryan, who he'll be on the next episode.
*  Yeah, OK, so you can follow up on this.
*  You ask him what his problem with Randy's story is because he and I have been arguing, of course, but OK,
*  so he and I agree that the information isn't stored in the plastic sentences,
*  and he admits that he does not have a story about how the information is stored.
*  The engram.
*  He's all focused on these cell assemblies. He's focused on this sparse coding.
*  And I say, Tomas, that's all very interesting.
*  What? We both think that the real name of the game is looking for the engram, and those cell assemblies, they aren't the engram.
*  Your own work shows that it must be inside those cells.
*  So Randy said, ask him what his ask him what's his problem with my story about the engram and it being intracellular,
*  sub molecular processes, encoding numbers in order to compute.
*  So, Tomas, I guess I'll start off by asking you how you would respond to what's your problem with Randy's story.
*  We all want to understand how memory is stored in the brain and the change in the brain that is caused by learning,
*  which stores a particular piece of information we call memory engrams.
*  Randy insists that the engram must be molecular, that it must be intracellular, and it's here where computations are happening.
*  The basic problem that I see with any account of memory,
*  which insists that information must be stored at a molecular level within neurons,
*  whether that's some kind of protein structure, whether that's some kind of code in polynucleotide chains,
*  is that the brain would have to know which cells to find that molecule in.
*  So we know that sparsity is a fact. We know that when we have a particular experience,
*  that particular cells are being activated and we know that for a given memory,
*  we have a sub portion of cells distributed across many brain regions, which seem to encode that memory engram.
*  So if you're going to say that the information is in a molecule within the cell,
*  you have to explain how the brain knows how to find that particular cell in the first place,
*  and that necessarily requires a mnemonic mechanism for storing memory which is not occurring at a subcellular level.
*  So he tacked on to his story that you also need an index, essentially, a pointer, right?
*  So you not only need to store a number, you need to store an index.
*  So I guess his take on that would be that's also encoded, those addresses.
*  Galistyle insists on there being a von Neumann type structure in how the brain does computation
*  and that what we need to be finding, therefore, is some kind of Shannon information.
*  And he insists that Shannon information, in his words, is the only game in town.
*  And I completely accept that Shannon information is the only game in town when we're talking about information theory
*  in the context of computer science and engineering. At least it's the only game in town so far.
*  But when we're talking about biology, we have to be open to other kinds of information storage.
*  One of those we already know a great deal about, which is genetic information,
*  which is a kind of information which existed, obviously, for billions of years before we did,
*  which is not encoded in any kind of Shannon form, is not computed in any kind of von Neumann architecture,
*  and which we had to discover. We didn't invent it. We had to discover it with theoretical insights,
*  but primarily with a progress of empirical discoveries and experiments that allowed us to have that kind of information
*  revealed to us through science. And that is a kind of information that we know is just not like Shannon information.
*  And I think what is particularly apt here is that Cloud Shannon did write a very short piece once called The Bandwagon,
*  where he was trying to illustrate that information theory as being conceived was conflating in some respects two different things.
*  On the one hand, it's about communication of information, which is what Shannon information is for.
*  It's for faithful communication of categorical information with total error reduction from a source to a destination.
*  It does not contain any a priori semantic information.
*  Semantic information needs to be written into the Shannon information method of information transmission.
*  When we're talking about the genetic code, we're talking about a type of semantic information
*  that is embedded by some kind of biosemiotic structure into DNA.
*  And when we're thinking about memory, we're going to be dealing with very different substrates.
*  We're going to be dealing with something happening at a neurobiological level, obviously.
*  But that type of information is likely to be something that is currently unknown, currently undiscovered.
*  And there's no a priori rationale for imposing Shannon information structures on the brain in that respect.
*  And there's nothing wrong with looking for them.
*  There's nothing wrong with looking for information in the brain that may be informed by concepts that we have from computer science.
*  I mean, that's useful. And it may or may not be partly true.
*  Where I take issue with Galistal's perspective is that he insists that this is the only way
*  and that therefore the brain must conform to that.
*  We just know from genetics that's just not true. There are other ways of storing information in biology.
*  You know, I have to be honest. On one hand, I like Randy's story because it is intuitively appealing, right?
*  Given what we know about how von Neumann computers work and what we have discovered about and invented about computation.
*  On the other hand, like you say, I mean, brains evolved. We didn't design them.
*  So it's something that we have to discover and learn about instead of.
*  So it's interesting to think about using a product of our own engineering to then look back and say that must be how it's happening in the brain in some sense.
*  So it's intuitively appealing. It's also kind of simple, right? To think, well, it must just be there.
*  The numbers must be there in the neurons.
*  But then. I also I think it's a much more interesting story if it's not done that way, if it's in if memories are encoded somehow, you know, among the connections, even if we don't have a clean story about how that's done yet.
*  So it's just a much more fun story to think about how brains might be doing this in a non von Neumann way.
*  Right. Biology is messy is messy.
*  And the way that Gallistyle thinks about this seems to be very much influenced by fedorian notions of language of thought, which essentially says we have to have computation in the brain, which which we do.
*  And that therefore it must be numbers all the way down.
*  And what Randy is saying when he's challenging conventional neuroscience is that animals, including rats and mice, clearly have concepts of numbers, because they have concepts of time.
*  And therefore we must be able to find those numbers in the brain if we are to really say that we have a full explanation of memory.
*  Now, on that proposition, I agree.
*  I think that if we do have a full explanation of memory, we will have to be able to say how an animal or human is able to have a concept of a particular integer number in its brain.
*  But that does not mean that that information needs to be stored at the basic Ngram level in a numerical digital format.
*  And one way of illustrating this is to think about other ways that numbers are represented in biology.
*  And one very obvious way is the number of bones that we have in in our skeletons or the number of vertebrae that we have in our back.
*  Humans have about 33 vertebrae in our backs. Mice have about 28.
*  I think elephants have about 60. All humans have this number of vertebrae.
*  You will not find that number in our genome, even though it is genetically encoded, even though we are genetically determined to have 33 vertebrae.
*  You will not find 33 anywhere in our genome.
*  What happens is there is an analog to digital or rather an analog to integer transition where you have a body plan with a gradient of morphogens,
*  which works with a fixed number of Hux patterning genes to create somites segments during a segmentation phase of early development.
*  And they form integer things, which are segments, which then become our vertebrae, partly in a very consistent genetically encoded way
*  that is only meaningful at a developmentally expressed level in the context of a developing organism.
*  The same kind of logic can be true of any other way of storing integer quantities or temporal information in the brain,
*  where you start with an continuous gradient that has thresholds imposed on it according to certain constraints,
*  which result in integers which can then represent absolute numbers.
*  And I don't see anything controversial about that. I think that's just a basic feature of developmental biology.
*  And I also think that there's an awful lot to learn about the coding of information from the study of innate information in itself.
*  And by that I mean instinct. So if we are to really get a handle on how information is coded in the brain,
*  whatever your favorite mechanism is, whether you're concerned with it at a system physiological level, a synaptic level or at a molecular level,
*  I think it's really important to consider the overlap of instincts and memories.
*  And it seems to me that by understanding the minimal essential components of memory, engrams and innate, what I would call engrams,
*  we can get a handle on the minimal components, minimal biological components of latent states of long term information in the brain.
*  And so when Randy Gallistyle is proposing very strongly that information must be stored at a molecular level in engram cells,
*  or rather that there are engram molecules, I think he also needs to explain how innate information is coded for at that level, at that kind of molecular level.
*  And I asked him that question once. I said, OK, so we have instincts too. And he said, yes, of course we have instincts.
*  I mean, there are many definitions of instincts. But taking the view of an instinct that it is coding for information about environmental affordances that are relevant to you.
*  So, you know, we have nine or 13 or so different definitions of instinct, depending on who you read.
*  It's not just enough for something to be innate to be an instinct. So essential pattern generator, the fact that I can breathe is innate.
*  It doesn't necessarily mean it is an instinct. I would say an instinct is something that is in your brain that is genetically determined, that is developmentally constructed.
*  There may be some plasticity in how that is constructed, but we know what we mean.
*  It means that we have knowledge about our environment, about the environment we evolved in to a certain extent.
*  I posed to Randy that you need to be able to explain how an instinct is coded for in the same language, in the same biological language as to how the memory is coded for.
*  And Randy said, well, that's very straightforward. It's coded for in the DNA.
*  But that's really not enough of an answer. It's not really coded for in the DNA because DNA in isolation, genes in isolation will never produce a brain structure.
*  It happens because of an unfolding of nested information patterns that are developmentally realized that can be described as Kalmagorov complexity that results in that brain structure being formed.
*  And it seems to me that at a molecular level, it would not really make sense to be storing instincts in that respect.
*  I think we accept sort of tacitly that instincts are stored in the anatomy of the brain.
*  And it's worth pointing out that we don't study instinct yet as much as we've studied memory.
*  I mean, there's far more work being done on the neuroscience of memory than there has been on the neuroscience of instinct.
*  And that's partly because that's a throw over from experimental psychology, which traditionally focused more on learning than on innate behavior for most of the century.
*  But there is a lot of work still on the neurobiology of instinct.
*  And most people would agree that it's a reasonable viewpoint that instincts are informational patterns that allow us to compute how to realize adaptive behavior in the moment through our brain structures, through the way our brain circuitry is structured at both a macro and a micro level.
*  And we're still working out the details, of course, of different instincts.
*  And we don't understand all of our instincts.
*  But it seems to be seems to be realized and encoded at a systems level.
*  I don't think that there is a good framework for thinking about innate information that is innate, instinctual information about how to behave in your environment at an intracellular level.
*  I've never seen any evidence or even a case made for that.
*  So I think that's another problem with the with the view that memory would be intracellular.
*  But one of the major problems that I have is, is the is the kinetics of this or rather not the kinetics, but the temporal dynamics, I suppose I should say.
*  So one of the reasons why Randy has challenged the traditional synaptic plasticity and memory field for so long is because he observed that it did not follow the temporal conditions of associative learning or what we call associative learning.
*  And that the heavy and mechanisms that we were observing for inducing synaptic plasticity, LTP and even spike timing dependent plasticity didn't really parallel what we were seeing at a behavior level.
*  And his view was that the temporal constraints need to be grounded in the facts of experimental psychology in behavior, which makes complete sense.
*  And I agree with that. I've agreed with that for many years.
*  The problem is that when you start going into specific molecular mechanisms that he's particularly focused on, which are intracellular, which involve DNA coding, which involve the shifting of polynucleotide chains in a way of storing information, but also retrieving that information.
*  It isn't biologically plausible, in my view, for that information to be recalled at the time scales that we require it.
*  So we know that memories can last a really long time that you can then summon them in an instant.
*  You can summon them in seconds or less when you're really motivated to or when you just have a random thought.
*  For a molecule to be expressed in a nuclear way that has a manifest effect on the electrophysiological properties of that cell to then affect behavior.
*  I mean, you're talking about minutes to hours for that kind of operation to happen based on even the most generous, most generous understanding of how we can understand intracellular molecular processes.
*  Exactly. So it's about speed. And so I don't consider it realistic.
*  So my own view, which is I don't think a very radical view, is that memory is not stored at an intracellular level.
*  But where I agree with Randy is that it's also probably not stored at the level of synaptic weights.
*  And by that, I mean at the level of the strength of individual synapses or or individual populations of synapses.
*  And that rather it is plasticity of synaptic wiring that creates new sub anatomical micro anatomical circuits that didn't exist before,
*  which allow the brain to store information in a way that is stably embedded in the structure of the brain and that can be rapidly accessed whenever required and and kept and kept there in essentially a state that requires very little energy,
*  very little thermodynamic effort, because you don't need to keep synaptic weights at a certain strength.
*  And it also reduces error reduction in that sense, because what you're concerned with is the wiring of synaptic connections.
*  Do you have a pathway between a particular set of nodes?
*  And if you do, then you have that information which can be agnostic to the actual strengths of the different connections between those nodes.
*  So you can have drift of synaptic weight either as an effect of further experience or because of the synaptic homeostasis or synaptic elasticity,
*  which we know exists, which is when the set point of a particular set of synapses goes down to a previously existing synaptic weight following a previous experience.
*  And if we remove the magnitude of synaptic weight as being a crucial feature of information storage,
*  what we're left with is, I think, a much more stable, reliable, durable way of storing information and importantly,
*  one that is congruent with how the brain is likely storing innate information that is instincts.
*  OK, so I'm going to ask you about optogenetics here.
*  But one more question before that.
*  Scientists are people, right?
*  And we are we cling to our ideas and it's really hard for us to change our minds about anything.
*  Have you can you remember a time when scientifically your mind has been changed where you thought one thing and evidence convinced you that it was another thing?
*  Because I'm not sure that so Randy certainly is never going to change his mind.
*  I don't believe about his thoughts on the way that memory works.
*  And, you know, this is beyond just memory.
*  It's more of a sociological question, I suppose, about science.
*  Do any of us really ever change our minds?
*  I think it's really important to be able to change your mind.
*  But it happens very slowly for things that are a matter of theory or opinion.
*  I mean, the way science works is that it is slightly adversarial, but it's grounded in data.
*  And when somebody eventually does an experiment, we must pay attention to that.
*  And eventually we have to admit that a particular fact or a particular thing is a certain way.
*  I think people are more difficult to persuade that they need to change their minds when we're talking about theory and not and not facts.
*  And, you know, I often tell my students that science is theory based, not fact based.
*  We can't get a full picture of the world.
*  Certainly not a full picture of the brain just by stitching together facts that we discover in the lab.
*  We have to have a coherent theory.
*  The theory has to have the capacity to make predictions and for us to form questions.
*  And we design experiments in order to test a theory.
*  In a sense, the data that we produce in experiments are tools in a sense to test the theory, to refine the theory and to assess the predictive value of a theory.
*  Most of the science that we do does not really challenge or inform theories in a very large sense.
*  We tend to evolve our theories in a much more gradualistic way, working as a community.
*  And I learned this very much from people like reading reading people like Daniel Dennett, where he takes a very, very subtle, slow approach for trying to persuade people that they're the received wisdom on something maybe incorrect.
*  So you asked me when was my mind changed?
*  I suppose that I've been a Cartesian dualist for much of my life and I'm not a religious person.
*  I think I left organized religion when I was about 12 years of age, but I got into neuroscience in my 20s and I think I was a Cartesian dualist right up until I was about 28, 29.
*  And I was resisting any notion that there may be nothing magical about the mind because I was still hanging on to this idea that many of us seem to have that there's some kind of not a homunculus, but some kind of driver in my brain that I was really in control of everything that was going on.
*  And when through reading Daniel Dennett, I really got the slowly was brought towards the perspective that was based on facts that I was already aware of, but had never been arranged in this way that I realized that that was wrong.
*  And that what I was calling consciousness was really some kind of, as Dennett says, a user illusion.
*  And why would I was so impressed by was the lack of ostentatiousness in how he did this, because he didn't take a dramatic polemic combative position with respect to this.
*  You know, he created a picture. He was very good at he's very good at meeting the reader where they are.
*  He communicates crucial insights and creates a picture, and I think that that is a very productive and civilized way of changing people's minds.
*  And I hope I try and bring that into my own teaching with respect to my own science.
*  I got into neuroscience because I was really fascinated with an idea that I think has since been shown to be not entirely accurate.
*  So I was originally a geneticist. I was a genetics undergrad and I was getting quite bored of the way genetics had gone.
*  So I majored in genetics because I really wanted to understand what we knew about genomics and how we evolved as a species.
*  And by the time I got into my sophomore year, I became quite quite bored of what was going on.
*  I mean, it seemed to me that we had answered so many of the questions, at least that I wanted to answer.
*  And I was wondering what I would do with my with my life and what I would I become a scientist.
*  And then I went to one lecture by a Drosophila memory geneticist called Mani Ramaswami, who was explaining to us at the Trinity College Dublin
*  how short term memories were encoded as covalent changes in proteins that supported synaptic plasticity,
*  whereas long term memories were encoded through gene expression cascades that would result in late phase long term potentiation and permanent synaptic weight changes.
*  And I was immediately fascinated by this idea.
*  And that day, I think I became a neuroscientist because I just thought it was the most amazing thing that a genetic switch would decide
*  how you go from a short term memory to to a long term memory.
*  And so I got into neuroscience and I had a PhD experience that was quite was extremely valuable to me at Cambridge.
*  And it was in my postdoc at MIT when I was actually working on Ngram cells where we started to test some of those ideas that were part of the bedrock of the memory field.
*  There was always a problem with the idea that long term memories would require gene expression changes.
*  And some of this was illustrated by Randy Gallistal's work and others because the temporal conditions of this would would require hours to happen.
*  And of course, there was a stopgap, you know, placeholder in that theory where you would have a short term synaptic correlate that would lead to a long term synaptic correlation.
*  It was very much focused on the molecular and cell biology of long term potentiation.
*  And that if we simply understood the idea was really if we simply understood the molecular and cellular biology of long term potentiation,
*  we would be able to explain how synaptic weight plasticity was maintained indefinitely.
*  And therefore, we would understand memory.
*  And through reading more and more behavioral neuroscience and becoming more aware of experimental psychology,
*  I started to see the problems with this actual with some of the premises on which we were basing that one was that we were avoiding the issue of information storage
*  because we didn't have the technology to study particular memory and grams in a functional way at the time.
*  What we were doing was we were studying the cellular biology and the synaptic physiology of the process of learning and the process of memory maintenance,
*  the process of memory retrieval and reconsolidation.
*  The problem was that we were not really understanding how specific memories were coded, how my memory of having coffee with you is very different from my memory of meeting a student today,
*  even though they were going to both types.
*  Both memories are going to be involving the same brain regions, of course, and would involve the same biological processes.
*  The point is that we're also studying separate engram, separate memories using the same biology, just as different genes are still are coded in exactly the same material.
*  All of our genes are coded for more or less in DNA.
*  So the same molecular mechanism, but completely different pieces of information.
*  So engram labeling technology, which came into its own in 2010, 2012 because of work in the Tanagawa lab and before that, to a certain extent, in the Josson lab in Toronto.
*  And of course, the Tanagawa lab, we were building on the tools created by Mark Mayford in San Diego for immediately gene transgenics and also very much on octogenetics.
*  But we were in the privileged position to be able to label and manipulate specific ensembles of cells in the hippocampus of awake behaving mice.
*  And in doing so, we were able to show that these specific ensembles of neurons were somehow containing or at least are a part of the information that is allowing us to hitch icons to specific memories.
*  And using this technology, we can elicit artificial recall of specific naturally formed contextual memories in the mouse.
*  And we can also inhibit them. And since then, this technology has been applied to many different behavioral paradigms and in many different brain regions.
*  And one early experiment that we did was to ask whether memories would survive in an engram if we disrupted late phase long term potentiation or if we disrupted rather memory consolidation in general,
*  which includes a set of gene expression dependent biological processes, which are required for a number of things, including the maintenance of strength in synaptic weight.
*  And when we did this experiment, we were able to see that we were able to abolish as far as we could measure it, changes in synaptic weight that were specific to engram cells,
*  both measured by the strength of excitatory postsynaptic currents, but also the density of postsynaptic dendrites and engram cells and other people have reproduced these findings.
*  But what we found was that when we optogenetically stimulated those cells, that the information seemed to survive in a number of different behavioral conditions.
*  So in other words, the learned information was still being carried in those engram cells, despite the fact that that information was not being accessed by natural retrieval cues.
*  We got a lot of pushback from the community at this time.
*  What was the nature of the pushback?
*  Well, people approached this finding with the degree of skepticism that it deserved.
*  We had a long time in peer review.
*  We had a long time persuading people at conferences, of course, and it's completely appropriate.
*  But what was the what was what was it that they don't like the story of some sparse representation of engram cells and you can just zap them and and the mouse recalls the contextual behavior or they didn't.
*  What part didn't they buy or was it all of it?
*  I think it depends on who you talk to.
*  I mean, I think there were probably three broad schools of neuroscientists that were interested in this kind of result.
*  On the one hand, you had the molecular neuroscientists.
*  On the other extreme, you had the systems neuroscientists.
*  And in between, you had what I would call the synaptic plasticity and memory people.
*  And they all would have approached it with slightly different, I think, attitudes.
*  The molecular neuroscientists and I was previously a molecular neuroscientist, right, are working from a molecules first approach.
*  You know, they're working on NMDA receptor biology or they're working on PKM Zeta or they're working on prions.
*  They're working on some kind of molecular pathway that generally speaking works for a number of different physiological phenotypes.
*  But tends to they tend to focus on synaptic weight changes.
*  They tend to focus on LTP.
*  And their goal is to contribute to the molecular understanding of memory.
*  So there was an immediate immediately response there because they wanted to know, well, if not this molecule, then what is the molecule that's storing memory?
*  On the other extreme, you had the in vivo physiologists, people who are studying how ensembles of cells are functioning in a weight behaving animals,
*  going back to the discovery of place fields and everything since then.
*  And there's a justifiable reaction to N-gram biology from that quarter of neuroscience.
*  And the reason being is that they've spent decades listening in very carefully on how the piano of the brain and the piano,
*  how the N-gram is actually played in a very delicate and specific symphony.
*  And then we came in with a set of hammers and started banging the piano keys, making quite strong statements and then walking off.
*  And that's obviously going to get some people's backs up to a certain extent.
*  We're using optogenetics to turn on and on, turn on and off ensembles of cells that don't spike at the same time in vivo.
*  And we were doing it at unnatural frequencies.
*  And until holography becomes so sophisticated and ubiquitous that everyone can do it in an efficient way,
*  we're going to have to continue using these kind of methods.
*  And we accept those limitations entirely.
*  But we think that doing these kind of experiments at least allows us to ask certain questions that give us reasonably clear answers to a certain extent,
*  particularly when there are loss of function, but even when there are gain of function,
*  because even though we activate a particular N-gram at 20 hertz and all the N-gram cells in that brain region are spiking simultaneously,
*  the downstream circuitry we hope is going to take over to a certain degree of normality.
*  It's going to...
*  Like the dynamics of the...
*  Exactly.
*  The sequence and dynamics of the firing.
*  Yes, exactly.
*  And in other experiments, of course, you don't need to do the behavior in real time.
*  You can stimulate to create an association and then you can look at natural behavior afterwards.
*  And of course, the real, I think, benefit of N-gram technology as we do it is that we don't interfere with learning at all
*  and that the learning is done in a completely natural way.
*  And we allow the brain to show us where the cells are.
*  We don't engineer anything.
*  There's nothing artificial.
*  Just to pause on that for one moment.
*  So you teach a mouse, let's say, or a rodent a task or a fear conditioning paradigm,
*  and you have used transgenics so that the neurons, the N-gram cells,
*  quote unquote, the N-gram cells that are involved in that learning process start to express these early genes that are involved in plasticity.
*  Right.
*  And the transgenics then can basically label those very specific cells.
*  And then that's when you can start manipulating.
*  Right.
*  So we're hijacking immediate early genes.
*  And immediately genes have been studied for many years by people like Mike Greenberg and Mark Mayford engineered a particular transgenic mouse.
*  Now there are other kinds of mice that we can use.
*  But his mouse co-opted the C-phosphor motor, which is one gene whose expression is a function of neural activity.
*  And then we can use this gene to activate and, excuse me, to label the naturally activated cells in a particular learning experience.
*  And we can control the labeling of the cells using exogenous constructs that are dependent on antibiotic administration or on tamoxifen administration.
*  But putting the technical details aside, the point is that we can label cells that were naturally formed or N-gram cells that were naturally formed during learning.
*  And so the methodology is not perfect when it comes to activating these cells,
*  but they do give us a functional analysis of N-gram cells in a way that can be controlled, I think, to the standards of any standard behavioral neuroscience experiment.
*  But with respect to how people responded to it, I think in between there was the synaptic plasticity and memory field,
*  who for a long time were working on the premise that the kind of plasticity that we must be observing
*  in order to fit in with the idea of Hebbian synaptic plasticity as a mechanism for memory storage would be what we call long term potentiation,
*  synaptic potentiation, which can be caused by high frequency trains in vitro or in vivo or also by spike timing dependent plasticity induction procedures,
*  which are, we think, more natural and that these would be ways of storing information in the strength of connections between neurons that were already connected.
*  But if we step back for a second and we look more broadly at the kinds of synaptic plasticity that can happen,
*  there's a very nice review that was written by Karl Svoboda and others, I think in 2004,
*  which very nicely separated out the idea of synaptic wiring versus synaptic weight.
*  And there are two different kinds of plasticity that often occur at the same time.
*  And because they occur at the same time, they can be experimentally difficult to disentangle.
*  So plasticity of synaptic weight refers to the strength of the connections and plasticity of synaptic wiring means you're changing the connectome.
*  You're changing the connectome. You're forming new connections in your lattice that were not there before the learning experience.
*  And what adds to the confusion is that both types of synaptic plasticity can be induced by two different types of biological plasticity.
*  One would be the structural plasticity of dendritic structures and the other being what most people would refer to simply as synaptic plasticity,
*  which means in the simplest case, the addition of AMPA receptors to synaptic, to post synaptic sites.
*  You could also include presynaptic plasticity.
*  And the reason that both types of biological plasticity can contribute to both kinds of information plasticity is that when you consider plasticity of synaptic weight,
*  obviously that can be increased by trafficking AMPA receptors to the synapse.
*  And that's the simplest way we think about it.
*  But it also can be triggered by structural plasticity, because if you have a population of neurons or a subpopulation of neurons that are all connected to a post synaptic set of neurons,
*  if you increase structural plasticity by having more dendrites on the post synaptic side, it's technically a structural plasticity.
*  But really, you're just increasing effective synaptic weight.
*  You're not really changing synaptic wire because they're already connected.
*  Then you move over to the informational plasticity of synaptic wiring.
*  That can be mediated both by structural plasticity, obviously, because if you have slight axon movements or if you have dendritic movement or new dendritic spines or new dendritic structural plasticity,
*  you can form new connections between neurons that were not there before and so you change the connectome.
*  But you can also have plasticity of synaptic wiring due to basic intracellularly driven synaptic plasticity when you consider silent synapses.
*  So silent synapses are NMDA receptor only synaptic connections that are existing, but they're nonfunctional because they have no AMPA receptors.
*  And you can imagine how in certain cases these become silent or excuse me, become unsilent.
*  And we know about this based on different experiences.
*  So in that sense, a very simple biological event of transporting AMPA receptors to the postsynaptic site would not structurally change the connectome,
*  but functionally change the connectome because it could create a new pathway that was there but never really active.
*  But the point is that when we're considering the plasticity of synaptic wiring,
*  this is a kind of plasticity that seems to survive the disruption of late phase synaptic potentiation.
*  And it seems to survive with the memory in various cases of amnesia.
*  And this wasn't just done in the Tanagawa lab.
*  These kinds of experiments have been reproduced at the Denny lab at Columbia and the Franklin lab in Toronto and my own lab at Trinity College Dublin,
*  also in Japan and in Germany.
*  And more and more what we're finding is that in different cases of amnesia,
*  the memory survives despite the fact of amnesia and that the connectivity between N-gram cells survives.
*  And my working hypothesis, I suppose, is that it's the plasticity of synaptic wiring initiated at learning that is allowing us to store information in a long term and stable state,
*  while the plasticity of synaptic weight, which doesn't alter the connectome, but of course occurs even in new synaptic context.
*  So if you form new, if you make new synaptic connections during learning, those synaptics are also going to be potentiated.
*  This is what I mean by they're happening at the same time, which causes some confusion.
*  But that plasticity of synaptic weight, I agree with Gallus Dal is probably not storing information.
*  It's probably quite important for learning because learning happens often over minutes or even hours, depending on the episode.
*  But plasticity of synaptic weight happens really quickly.
*  So surely if you disrupt that, you're going to disrupt learning.
*  It's also possibly involved in recall.
*  One hypothesis is that the reason that we can't recall memories in different cases of amnesia is because of the disrupted synaptic weight strength.
*  But that's a hypothesis that hasn't been proven.
*  That's your current thinking as well, right?
*  That we have all these latent, inaccessible, unrecallable, naturally unrecallable memories from learning.
*  I used to think that, but that's something I've changed my mind on also in the past few years.
*  Because what we know about N-gram cells, even in cases of amnesia, is that the cells are still functional.
*  It's not like the cells are silenced.
*  It's not like they're hyper-inhibited or anything like that.
*  It's just that they're not statistically activated enough during what we consider to be a targeted recall session for memory recall to occur.
*  But they're still functional. They can still code for other information.
*  So why is it that they're not being activated?
*  It seems to me that what's happening when we cause amnesia,
*  whether we cause amnesia by some kind of experimental amnesia,
*  such as anisomycin, which is a drug that blocks memory consolidation by inhibiting protein synthesis,
*  or if we have pathological cases of amnesia, such as traumatic brain injury or early Alzheimer's disease,
*  that we're causing this reversible inaccessibility of memory,
*  but that this type of thing that we're doing may be actually co-opting a natural process,
*  a natural process of forgetting, and that there may be something more general going on here.
*  And I think that we've been limiting ourselves when we think about memory accessibility in a very binary way,
*  like it's either storage or it's either retrieval.
*  Once we start getting into binary perspectives, we start getting very oppositional, very categorical,
*  and sooner or later everyone is wrong and things can get a little bit unfriendly at times.
*  When we start thinking more about it being a continuum of accessibility,
*  we start to open up new ways, I think, of considering the function of what I would call differential memory expression.
*  When we say accessibility, it's kind of a loaded term because we're saying it's more difficult to access than engram,
*  but actually maybe the engram is accessible, but it's just not as inclined to be as expressed under those conditions.
*  And so Paul Franklin and I recently wrote a perspectives piece on this, which we worked on for quite a long time,
*  which was to try and put together all of the literature that we could on the general topic of forgetting,
*  and forgetting in the broad sense, so including amnesia,
*  which would be considered by some a kind of forgetting or a kind of pathological memory loss, depending on how you want to look at it.
*  And when you consider forgetting as the loss of information that's already been learned,
*  we've known about it for over 100 years, it's been characterized by experimental psychologists for over 100 years,
*  we tend to consider forgetting as a nuisance.
*  We tend to accept it as a kind of unavoidable consequence of having imperfect biological brains.
*  And wouldn't it be wonderful if we could not forget anything and if we could all be super-nemonists like what Luria was studying.
*  But then you start to think more about the value of forgetting and that forgetting may be not so much a bug of the brain, but a feature of the brain.
*  And this is particularly pertinent as we get more and more interested in brain inspired AI in recent times.
*  And the idea that memory, that the forgetting of memory may be adaptive,
*  was put forward a few years ago by Blake Richards and Paul Franklin,
*  who argued that forgetting allows us to have a greater amount of behavioral flexibility,
*  because it avoids long-term memories from overfitting to noisy environments.
*  Because if we overfit to noisy environments, it's harder to learn anything new, it's harder to make predictions.
*  And this would be really maladaptive for real life, it would be terrible for things like foraging or for further learning.
*  And that generalization in itself may be considered a kind of a form of partial forgetting.
*  OK, that makes sense, that makes sense at an ethological level and it makes sense for artificial intelligence to a certain extent.
*  But there's a cost there. And the cost is that you're losing that information, which may be valuable to you in the future.
*  But that's where the reversibility of memory loss allows us to circumvent that cost.
*  Because if the memory is recoverable under certain circumstances, then the brain can potentially have it both ways,
*  in that we can forget what we need to forget in a way that is driven by the predictability of the environment.
*  But that does not mean the information will be unrecoverable in future as conditions change.
*  And this was the core idea on which we were trying to build that perspective piece while we were gradually changing our mind as we were writing it.
*  So on that account, and by the way, I like this idea, I like that paper.
*  On that account, as you partially forget, like you were saying, it leads to better generalization.
*  But then your idea is that you have all these latent, eventually recallable memories still within that.
*  Because on the earlier account, you permanently lose those quote unquote memories as you get better at generalization.
*  But you want to say that that's not necessary, that you can have these latent structures that are still there and recallable when needed.
*  Exactly. I mean, I think that all engrams are essentially latent structures because the engram has to retain information
*  for years and potentially decades of that memory not necessarily being activated.
*  When we say that we observe forgetting, or we experience forgetting,
*  it's based on a second order expectation about how we should behave or how a subject should behave under certain conditions.
*  But the subject's brain may have a different idea of what is the most adaptive computation to make.
*  And this is what is already well accepted in the memory extinction literature.
*  So in the experimental psychology of memory and in the neuroscience of memory, we've known about how memory can change,
*  not just through reconsolidation, which is a relatively recently characterized phenomenon of the last 20 years.
*  But going back even further, we've had memory extinction, counter conditioning, interference, the Cayman blocking effect, and so on.
*  And these are all associative changes to memories that have been formed.
*  And then you change the contingencies so that they're expressed under different circumstances.
*  Now, Randy Gallistyle has rightly criticized me for using the term inhibitory memory in that perspectives piece,
*  and has pointed out that extinction is not the inhibition of a previously formed memory.
*  I know that it's just a convention, just as we conventionally describe these memories as associative memories from a pragmatic descriptive sense,
*  even though it doesn't necessarily mean associations are happening in the brain.
*  We know extinction changes the expectancies for Pavlovian association or an operant association, and that this changes how the animal behaves.
*  It doesn't mean the engram is being repressed. It means the engram is being differentially expressed or differentially used.
*  And it seems to me that the process of memory extinction, which is generally considered to be a form of learning
*  and very loosely described as a form of memory updating, might be the same thing that is happening in natural forgetting.
*  And there's a number of reasons to think this. One is based on the biology of natural forgetting.
*  We've known for about 10 years or more now that it's an active process.
*  And if you disrupt certain biological signaling cascades that involve molecules like RAC1,
*  then you don't get natural forgetting across the timescales that are generally studied in animal models.
*  So this was always a puzzle that forgetting required an activity. It wasn't actually a passive process.
*  And then it turned out that some of the processes that were involved in active forgetting were actually initiated by neuromodulators such as dopamine.
*  And that this feedback was at some level perceptually generated.
*  And the dopaminergic activity was triggering natural forgetting of memories in both flies and in mouse models.
*  And then Yisong's lab, who is an expert on natural forgetting, mainly in flies but also in rodents,
*  he then used N-gram technology to show that under certain circumstances,
*  naturally forgotten memories could be retrieved in the brain of the rodent.
*  So more and more it seems to me, and partly because the rate of natural forgetting and the kind of natural forgetting that you have
*  is a function of your environmental experience. It's a function of environmental predictability.
*  I mean, you tend to remember things that are more valuable to you in your environment.
*  So if you're very hungry, you'll tend to remember memories that are associated with where to find food.
*  If you are socially isolated, you tend to forget social memories.
*  These are all things that are documented in rigorous animal-based studies.
*  And it seems to me that what's happening is that forgetting is being modulated by the predictability of your environment
*  that is using the same biological apparatus as prediction error-based learning is in memory extinction.
*  The difference is that when we're studying memory extinction in the associative conditioning world, we as experimentalists,
*  we control the conditions that are important.
*  We control the rules of contingency and contiguity in the associative conditioning experiments.
*  So we know when they're violated.
*  We know when the animal roughly should be having prediction-based learning and be updating its representation
*  by whatever mechanism it is, whether it's extinction, whether it's counter conditioning or whatever.
*  In contrast, when we study natural forgetting, we don't know what the animal is really focusing on
*  because often we're using very different behavioral parallels.
*  We might be using maze learning or we might be using object-based memory or visual recognition
*  or more complex parallels if we're studying it in humans.
*  But we don't really know how the subject is then seeing similar things, shapes and experiences as it's going about its life,
*  either through further experiences in the laboratory or if it's a free living organism or human being,
*  the similar things that it sees out in the world.
*  In a sense, what might be happening in natural forgetting is a range of different prediction violations
*  that is happening all the time and is pruning the structure of generalized memory engrams in our brain.
*  And that in a sense, you could posit that under at least some circumstances that forgetting may be some kind of a form of learning.
*  So, can I tell my wife, so I'm a gifted forgetter, and maybe what you're telling me is that now I can tell my wife
*  that actually me forgetting what she finds important, you know, some event or something,
*  is actually I'm super bright by forgetting well, I'm generalizing well,
*  and that she should commend me for forgetting rather than become upset with me.
*  Exactly. I mean, my wife tends to get annoyed that I remember so many things.
*  So it goes bad either way. And this is the point. People who have hyper-eidetic memories, I don't have perfect memory.
*  No? Okay.
*  But I think the more fixed you are with your memories, the more problematic it also gets.
*  You're not going to be adaptable enough to change.
*  And in terms of social interactions, it's going to also create annoyances either way.
*  The gradient, because we're here again, we're back to an analog gradient of probabilities, is there for a reason.
*  Our environment is always changing. And because our environment is always changing, our memories need to change with them.
*  And forgetting in many respects is a loaded term.
*  I think that it's far better to consider it to be about belief formation,
*  that memories, and to a certain extent instincts, are not so much pieces of information that exist on their own.
*  What they are is a set of latent structures in our brain which allow us to have beliefs that can make predictions of the world
*  with respect to different environmental affordances.
*  And I suppose that's an inactivist perspective in that we're concerned with being a part of the world around us and behaving appropriately.
*  And when we consider forgetting, we're always thinking about there's a right answer.
*  But in nature, there isn't always one right answer.
*  There's many things you can do. It changes. And in any situation, there are many behaviors you can use.
*  Forgetting just simply means not using the one that we would have expected you to use.
*  And I don't think that ethologically speaking, that is the most useful way of thinking about it.
*  But we can look at different types of memory to test this idea in very superficial terms.
*  So we can think about memories that we never forget and then memories that we always forget.
*  So I've often been struck by how elderly people who might be in their 70s or approaching 80s can do things like get on a bicycle
*  and cycle it perfectly well even though they haven't cycled a bicycle in 30 or 40 years.
*  So they have a complex motor memory that they learned at some point in their lives.
*  They haven't used it in possibly decades. And then they can just go and cycle a bike.
*  You don't seem to have natural forgetting for complex motor memories.
*  And it occurred to me that, and there's not a lot of data on this,
*  but that it could be because the physical environment for our bodies as to how they need to move in the world doesn't really change
*  because the laws of motion are based on gravity as it is on Earth.
*  And so the predictability of the environment isn't changing very much.
*  There are perceptual modalities which seem to be more fixed.
*  And those memories, it seems to me, are also more stable.
*  On the other hand, social memories fluctuate all the time because social memories change all the time.
*  And other types of human-specific semantic information also changes so much
*  that we are constantly updating them, partly through a process of forgetting.
*  And then at the extreme end, you have infantile amnesia, which is the type of amnesia which affects 100% of us,
*  which is why don't we remember anything that happened in the first two to three years of life?
*  And this seems to be a genetically encoded form of forgetting, which is specific to altricial mammals
*  that basically wipes the slate clean for episodic and contextual memories, for early life memories.
*  And we still don't really understand why that exists.
*  But like the other kinds of amnesia that we discussed today, this seems to be reversible.
*  And it seems that infant memories are being retained in the brain of animals right into adulthood.
*  Despite all of the things that are happening in late infancy and adolescence and in early adulthood,
*  all of the neurogenesis, all of the myelination effects, all of the synaptic pruning is not causing catastrophic forgetting
*  in an information theory sense of that information.
*  So I think that to answer your question, the way we need to think about memory is new convolutions
*  in the functional, if not structural topology of the brain, that in effect as we have new experiences,
*  we're making the structure more nuanced in a way that alters the structure of the connectome in very subtle ways,
*  which allows for stable information and its accessibility to change with experience.
*  And this in itself would create a framework where new instincts, where new innate structures could also evolve on top of the same structure.
*  So rather than thinking about always starting from the same structure and filling it in with pieces of Shannon information,
*  we need to think about the structure as a part of the informational relationship.
*  And there's some kind of semiotic interaction between the information that is embedded in the brain and the environmental affordance
*  in the world, which is relevant to that particular brain structure.
*  And of course, there are going to be embedded simulations and recursions that are projected in the brain.
*  There are going to be many micro environments within the brain where the brain is feeding back onto itself, of course.
*  But the point is that the evolving structure, either evolving through a process of learning or evolving through a process of actually biological evolution,
*  is going to be one that is always developing in a progressive but completely undirected way.
*  And I think forgetting is, in a sense, a folk psychological term that we've used as a catchall for whenever memory is not retained in a way that we would naively expect it to be retained.
*  Do folk psychological terms like forgetting and like memory, do they impede progress?
*  Progress, because we often redefine terms and instinct as well, right?
*  You said that there are like 13 to 20 definitions of instinct.
*  Do they get in the way? I mean, do they get in the way of your own thinking or can you move beyond the folk psychological concepts?
*  I don't think they get in the way of our thinking very much.
*  I think that it's really important to understand where these ideas came from.
*  So when we define something as an engram or as an instinct or as a representation, or when we use these filler in words in neuroscience like coding,
*  I think we really need to examine where the basis of that idea came from.
*  Does it even have a basis? Is it just something we use to be pragmatic? And that's OK.
*  We have to be pragmatic when doing experiments.
*  In general, I think that we need to be very open minded about how we're approaching the neuroscience of memory or possibly the neuroscience of anything.
*  I think that there's no need to be very rigid in how we're approaching brain information because we just know so little.
*  I mean, the more and more we go forward in neuroscience, the more it becomes clear we don't really know how information is being coded.
*  For me, it is still about the intersection of memory and instinct.
*  And I'm becoming quite interested in the question of the origin of instincts,
*  because the standard Darwinian view is that they evolve randomly by natural selection.
*  And then more recently, well, I suppose this happens every few decades, Lamarckian ideas come back to the fore,
*  where people are interested in transgenerational memory, which isn't tenable in many respects, except in very interesting exceptions.
*  And it seems to me that understanding memory and grand biology is going to give us insights into the future that contribute to understanding the origin of instinct.
*  You have a whole story about how you can develop instincts through memory.
*  Maybe you can elaborate that story.
*  Well, the idea of transgenerational instincts is that if you have a person who learns certain things in their lifetime,
*  then the information somehow goes across from their brain into their sperm or egg producing cells and into their offspring,
*  which then develop and that information manifests in the brain of the offspring.
*  And that would be a kind of Lamarckian evolution.
*  There may be some cases where that kind of thing happens.
*  And I'm thinking about Brian Dias and Kerry Ressler's very interesting experiments on this.
*  And of course, there are other transgenerational epigenetic experiments where response to stress or, for want of a better term, stress or emotional set points do show some kind of epigenetic effect.
*  But that's not what I'm talking about. I'm talking about the transfer of specific pieces of information like an engram.
*  I don't think it's tenable for that to work, even though some very interesting ideas have been put forward by people like Gene Robinson.
*  Because evolution is primarily conservative.
*  The most important thing about biological evolution is once you find the right structure, you don't change it randomly with every generation.
*  You have to have the occasional accident happening and that will create a new thing which might be selected for.
*  But generally speaking, you want to keep everything hyper conservative.
*  But there's an in-between type situation between Lamarckian evolution and very conservative Darwinian evolution, which is based on the Baldwin effect, which was put forward by James Mark Baldwin at the turn of the century.
*  And he wasn't just talking about learning a memory.
*  He was talking about all kinds of biology.
*  And the basic point is that biological plasticity in the lifetime of the organism allows you to test drive different states, different different spaces that are possible for you to exert in your lifetime.
*  That could be a different biological state or it could be the niche that you choose to choose to occupy ecologically.
*  When you find something that works, it then creates a kind of a net whereby a random mutation that results in a developmental change that mimics that type of experiential plasticity will be immediately valuable to the organism.
*  So if you don't have the Baldwin effect, of course, you can still have evolution.
*  And that's just a fact because random mutations happen all the time.
*  Sometimes they're useful.
*  When they're useful, they become fixed.
*  But Piaget observed in the 70s in one of his final books, Behavior and Evolution, that this creates a particular problem for behavior because he just didn't see it as plausible that a new behavior that evolved by purely random mutation would immediately become useful to the organism in its environment.
*  First of all, how could it happen if it hadn't been tested in the lifetime of the animal?
*  There's issues with social acceptance.
*  Would you be accepted by your conspecifics, which creates problems for reproduction?
*  But also, imagine the scenario where we have an ancestral species that isn't afraid of predators.
*  So prey animals are naturally afraid of predators like the mice in my lab are afraid of the smell of fox urine, even though they've never seen a fox in their life.
*  Or a hawk coming from above.
*  Exactly. Exactly.
*  So they have instincts that are relevant for the predators that they evolved with, even though they've never experienced them.
*  Now, at some point, there was an ancestor that didn't have that instinct.
*  Right. Because that's just obvious.
*  That's a truism.
*  Yet those ancestors survived.
*  Otherwise, their children would not be here.
*  Right. So how did they survive in the absence of having those instincts?
*  Well, obviously, they learned.
*  You know, they learned the hard way to adapt to those situations.
*  And they would have formed memory engrams.
*  The ones that survived the encounters with said predators would have evolved them, would have formed memory engrams.
*  And eventually, certain latent states in the brain would become fixed.
*  You learn them by experience or by some kind of social learning.
*  And if you don't learn them, you're not going to survive.
*  You're not going to reproduce.
*  This creates a situation whereby it seems to me that different brain states, different engrams will be formed and discovered by a process of learning that would be more or less similar in all the individuals in the population.
*  And that this allows the population to survive in a particular environmental niche, despite the dangers that are there.
*  And what happens by development, excuse me, what happens by learning can definitely happen by development because development is much more powerful than learning.
*  We can only do a few things in our brain with learning.
*  We can only change so many things in our lifetime.
*  But development is the reason why you're human and not a tiger or a fruit fly.
*  You know, development can decide your size.
*  It can decide whether you have seven or four layers of cortex.
*  If you're very unfortunate, development is what decides whether you have significant diseases or whether you have above average capacity in some in some physical or mental way.
*  So development is has a huge amount of power to it, whether that's influenced by genetic variation.
*  Or or stochastic developmental changes that occur along the way.
*  So the point is that anything that you do by learning, you can definitely do by development.
*  So once you find an engram that works in the population, it's not going to be very difficult for an engram, an instinctual brain circuit to come along by some kind of random mutation or assortment of genotypes.
*  That allows a particular brain circuit to be canalized in a developmental way that mimics a preexisting engram.
*  And what you would have then is a type of learning influencing the origin of an innate structure in the brain that is not transgenerational.
*  There's no magic molecules involved.
*  It's simply a kind of parallel or convergent evolution that is happening not in one generation, but in less generations than you would require by random Darwinian evolution.
*  And this type of evolution was was modeled by by Hinton, by Jeff Hinton in in the 70s.
*  And it's been illustrated to a certain extent in various biological conditions.
*  And the Baldwin effect is not considered to be radical.
*  It's not considered to be, you know, it's not considered to be heretical to anything that we know about Darwinian evolution, but it is considered to be a rather exotic case of Darwinian evolution.
*  But it seems to me that if the biology of structuring an engram is the same as the biology of structuring an engram, memory engram.
*  And I engram. Yeah. And yeah, the different you say engram.
*  It's confusing because one is E and G and one is I and G. Right.
*  So engram with an E being a memory engram, engram with an I being an instinct.
*  If they're made of the same biological stuff, even if if Randy Gallistol is right and they're both molecular or if they're both anatomical or whatever, they as long as they're made of the same stuff,
*  then you create the opportunity for the evolution and learning processes to be continuous.
*  And in a sense, what you have is this rolling evolution of latent structures that form belief states convergently happening by learning evolution, learning evolution, influencing one another.
*  In order to better adapt to the environmental affordances that you have to that you have to deal with.
*  But in the case of, let's say, a mouse learning that a hawk is bad. Right.
*  And then it that that and then they form an engram. So they've learned an engram engram.
*  Well, let me let me ask you your thoughts. Right.
*  I'm trying to connect that with the instinct story that future generations will have that instinct right through the developmental process.
*  Is it because the mice that learned the valuable engram had a predisposition already to you know, through their structural through their previous development?
*  Right. So maybe they were kind of instinctually tuned to learn the good engram and that their then their offspring are also tuned to learn the good engram.
*  But through development, some of them, it actually becomes an instinct because their development happens in a slightly different way than their mom and dad's.
*  Does that make sense? Right. I think it makes complete sense.
*  I think that's the missing ingredient to the mechanistic basis of how it's happening, that there's some kind of eliminative plasticity, not just constructive plasticity,
*  but there's something that is waiting to be pushed off a cliff in how the brain is being formed.
*  And that sort of almost stereotyped kind of plasticity can be initiated by learning or by initiated by changes in development.
*  And it speaks very much to what Peter Robin Hensinger was talking about on this podcast a few episodes ago about how the circuits can self-assert.
*  In slightly different ways, and you can't have an infinite amount of conditions pre-encoded in the brain.
*  OK, that's that's not possible. What you can have is an exponentially increasing range of possibilities,
*  which can be initiated by developmentally stochastic events, which are not low probability.
*  In other words, there can be changes that are about to occur that haven't occurred,
*  which are more likely to occur than any random change because of the way the system is structured.
*  So at the simplest sense, there are two populations of neurons that are easily able to come in contact with each other,
*  that are not in contact with each other, but could be easily used to form a new topological layer that would essentially allow for information encoding.
*  In other words, a state of potentiality, a state of potential plasticity that hasn't formed but can form.
*  And as long as you have lots of growing, many different possible states of potential plasticity that haven't yet happened,
*  the ball can be easily pushed down the hill either by learning or by evolution.
*  And that, I think, is really important because it means that you're not just really waiting forever for a random mutation to completely make a new circuit,
*  which isn't plausible. What you're waiting for is the developmental plasticity that is genetically set off to just find that next step.
*  Which is one of the next potential steps. Which is closer than a completely random... Exactly.
*  And I think that's the point of the thesis of a self-assembled brain, is that it's one step at a time,
*  but that at each step in each section, you're creating a number of different possibilities which allow you to be adaptable,
*  but allow that adaptability to have meaning across individuals in a population that has some kind of conservation and some kind of meaning that is common between us.
*  This actually reminds me of Stuart Kaufman's concept of the adjacent possible.
*  Every new step we take opens up unknowable, but next new steps that, in this story, those next new steps would be learning during the lifetime that then narrow down the future possibilities,
*  but we can't really predict how that's going to happen. Sorry. As an aside, Tomas, we've covered so much ground here.
*  I'm going to ask you maybe what seem like random questions, but are connected to a lot of what we've talked about.
*  One of the things I was wondering when you're talking about instinct is whether, if this makes sense to you, this question,
*  whether instincts can change during the course of a lifetime. I'm thinking about the elderly, right?
*  Can they have through their development along their lifetime, is it wrong to say form new instincts or would the instincts already be there?
*  Because you think of instinct and memory as a continuum, so would it be wrong to say that, I mean, is there a possibility that instincts could change coded through development through the lifetime?
*  I'm fascinated by that question, and I really like to take both an evolutionary and a developmental approach when trying to understand memory.
*  And instincts, I mean, we know they show differences across the lifespan.
*  I mean, those of us who are parents probably know this the most acutely because we have really strong parental instincts,
*  but they don't express, generally speaking, until you're at the point of life where you're reproducing.
*  Similarly, there are other instincts that don't express until adolescence and so on.
*  So we know that instincts show up at age appropriate times, but we also know that instincts change in our lifetime based on relevant experience.
*  So we a large part of education can be argued to be about the nuance, creating nuance on human instincts, how we respond to each other socially.
*  At a very primate level is not adaptive for modern human society, and we spend a great deal of time in trying to get people to understand themselves and to understand how to behave in a socially acceptable way.
*  We do this first by basic Pavlovian and operant mechanisms.
*  And as we get older, we teach people to have more insight about themselves.
*  This is very much, I think, dealing with our innate behaviors, but you can do more simple experiments in rodents, too, where instincts can be modulated by experience in the same way that memories can.
*  And the more I think about this, the more it seems to me that the brain doesn't even know what an instinct or a memory is.
*  In other words, that once you have a structure in the brain that has some information about the world, it's just a part of the brain and it's going to be maintained as a part of the brain.
*  And it can be updated in the lifetime of the organism by a process of learning.
*  It may have emerged by a process of evolution by a mechanism of developmental plasticity.
*  It may have emerged by a process of learning by the mechanism of synaptic plasticity in the broad sense altering the connectome.
*  But the origin doesn't matter. What matters is the product.
*  What the product is information in the brain that is relevant to the world.
*  Why would we need to know what was a memory and what was an instinct?
*  What we need to know is what we think is true.
*  And what we need to be able to do is to change it as our environment changes in order to maximize our chances of survival and reproduction in an adaptive way for ourselves and for our conspecifics.
*  So think, you know, reading your papers and there are always pictures, right, of like these networks of neurons, some of which are a certain color, and those are the engram neurons.
*  Do you think of memories as having definitive boundaries?
*  Like, so let's say you have a network of 2000 neurons that are part of the engram.
*  Is that the boundary of the memory or, you know, does the boundary of them?
*  Well, how do you think about the boundaries of memories?
*  Like, what is an individual memory, for example, right?
*  I'm not sure we'll ever be able to identify an individual memory in the way we can an individual gene.
*  When we look at a population of engram cells, due to technical limitations, some of those are background cells of a home cage environment.
*  Yeah.
*  But we're not getting all the engram cells because we're missing certain immediately genes that may have also been active.
*  We may miss cells that were important for learning, but that didn't express an immediately gene.
*  We're not activating every brain region usually, so we're only looking at a section of the engram.
*  But I think probably most troublingly, there seem to be cells that are not active at the point of learning, but become part of an engram after learning.
*  And this shouldn't be very surprising when you think about it.
*  So there may be cells in the vicinity of the activated cells, and we're only labeling the activated cells as current technology, that were completely nothing to do with the learning experience.
*  But that plasticity that happened during learning then brought those cells into the engram.
*  And this changes everything.
*  And this has been observed in certain studies that people haven't made a big deal out of, but it's known to be the case.
*  So when does an engram stop and start?
*  We have no way of knowing with current methods.
*  But even if methods were better, I'm not sure we'd be able to answer that question, because I think that an engram is a very, very important part of the engram.
*  A subset of the information in the brain, which we're tying to a particular experience as investigators, but that in reality is not going to have a one-to-one relationship with anything in particular.
*  So a gene has a relationship to a protein. It's one-to-one. You go from one concrete thing to another concrete thing.
*  I don't think we're going to have a one engram, one meme relationship, or one engram, one bit of information relationship, because they're all going to be tied up with each other.
*  Of course there is separation. So this is one of the original premises that I brought up in this discussion.
*  Of course there is separation of very similar things, separation of your face versus other faces that I'm aware of.
*  But there's also generality between those things.
*  If there wasn't generality, I wouldn't have the concept of faces.
*  And part of that generality is based on the idea that we have an innate structure in the fusiform gyrus for being able to see human faces, which is why we can have so many different memories of them.
*  And we separate those, but they're not going to be completely in isolation.
*  So I'm not sure we'll ever be able to identify the crucial boundary of a particular memory.
*  But maybe I'm wrong. Maybe it will be more straightforward in 10 or 15 years.
*  Or maybe Randy Gallistal is right and we'll just find the gene that encodes for my memory of your face in a particular cell in my fusiform gyrus.
*  Right. OK. Nicely done there.
*  The reason why I have thought about that, thinking about different engrams, different memories, what separates them, is because when I think of memory colloquially, I think of my own subjective experience of my memories.
*  Do you think that it's...
*  Imagine what you're going to say is that there's not an isomorphism between the engram cells and your subjective experience of memory.
*  But do you think it's important to, and is it on your to-do list, to try to connect the mechanisms of engram cell memories to our sense, our subjective sense of our memories?
*  I get asked that question quite a bit when it comes to developmental amnesia, because people ask me whether infants are really having the same subjective experiences as adults, which is a broader question than manipulating engram cells themselves.
*  And all I can say is we rely on the behavioral facts that we measure and we have the same standard for behavioral measurement in infants as we do in adulthood.
*  It's not the same when you do human studies because you don't have the same verbal subjective reporting.
*  I think this is a challenge for translational studies, and certainly we want to be able to translate engram biology into the study of human memory.
*  But I think from a broader philosophical perspective, I don't like to think about the subjective experience of recall as being relevant for how we understand memory recall at a scientific level.
*  Except when shining insight into basic facts of memory, which may be relevant for finding holes or points of confusion in different theories that you're forced to accept as a student.
*  I mean, one of the reasons that I got interested in amnesia was as I got older, I became more aware of different things that had meaning when I was younger that I wasn't really paying attention to.
*  And you start to reevaluate different scenarios that you found yourself in as a younger person, as a teenager and so on.
*  And you start to remember things that are quite innocuous.
*  Like, how did I even remember that particular episode?
*  It was so meaningless.
*  I'd never reflected on that before, except now I kind of now I know what that person meant.
*  Now I know what they were saying.
*  That kind of subjective experience did trigger me to question how we understand forgetting, because if very boring, salient memories can survive for so long, then it's it seemed to me that very important memories should also be surviving.
*  Even cases of amnesia.
*  So I think there are clues from the everyday intuition that we get from just the realities of our own memory.
*  But I think that's a different thing from being too concerned with subjective experiences of the aboutness or the qualia of memory recall.
*  I want to switch gears and just in the last moment or two, ask you kind of career type question or questions.
*  Thinking about moving forward and just going off of what you were just talking about and using different tools.
*  I know that you are an advocate of interdisciplinary research and collaboration, and it seems to be one of the cornerstones of your approach in your career.
*  And when I was in grad school, this was always touted.
*  All these programs are interdisciplinary.
*  And now every neuroscience program says that.
*  And it sounds great.
*  And sometimes it's true.
*  Sometimes it's not so true.
*  But it's really hard to quantify.
*  I think the actual benefit of doing this cross disciplinary research and incorporating different types of experimental approaches and different types of people.
*  So in a large sense, to me, it's really vague.
*  The benefits of it.
*  And I wish I could quantify it.
*  Right.
*  Do you think that's a good idea?
*  Quantify it right.
*  Do you have a sense of how it's really helped your own research program, et cetera?
*  I've always just been driven by the question.
*  I want to know something.
*  I'm interested in interesting questions and I don't care what the tools or the theories that are needed to answer them.
*  I teach my students that my undergraduate students that neuroscience is not a discipline.
*  So biochemistry is a discipline.
*  Genetics is a discipline.
*  Physiology is a discipline.
*  And psychology are disciplines.
*  But neuroscience, by definition, is not a discipline.
*  I mean, pretty much anything that has science in its name is probably not a discipline.
*  It's some kind of conglomeration of disciplines.
*  But I also think that we've gone through a period in history where science was just super conservative.
*  And I was trained as a molecular biologist originally in my PhD.
*  And the philosophy and it's still the philosophy in many places is that everything has to be hyper conservative.
*  You have to stay in your lane.
*  You have to get really, really good at the particular technique you're trained on in a sort of craftsmanship kind of way.
*  You have to fall into many holes.
*  You have to climb out of those holes because that's how we were trained in the 60s and 70s.
*  And that's the way everyone should be trained going forward.
*  You're forced to abandon that when you get interdisciplinary.
*  When you're getting interdisciplinary, you're forced to kind of have many things going on at the same time.
*  And you're forced to learn on the fly.
*  Part of that is collaborations.
*  But I think a larger part of that is that our students are a lot more impressive than we give them credit for when we give them the opportunity to be.
*  And one of the reasons that my lab is able to work on a number of broad questions is that, first of all, it has the common thread of memory and gram cells, which unites everything.
*  But also because I continue to be incredibly impressed by the performance of my graduate students and also to a large extent to my undergraduate students.
*  Once you start to move away from very conservative, this is the way we've always done things.
*  So this is the way you should do things.
*  When students are pushed and encouraged, I don't mean in a way that's full of full of anxiety or unrealistic expectations, but encouraged to find their own ways of putting different scientific topics together, of mixing one field with another, one part of which might be very conservative stuff.
*  The other might be more sort of more frontier, more recent technology in one area that mixing one thing with another, one thing being basic and one thing being very, very frontier often kind of produces the best results, provided that they're focused on the question.
*  And above all things that they are rigorous.
*  And when you when you teach students to be rigorous about their own data, they will always have a greater degree of assiduousness with it than you will have as a PI.
*  And I think what's allowed us to do this partly is how everything has changed in the last 15 years.
*  We've had the Internet for longer than that.
*  But in the last 10 to 15 years is where everything has become so unified that students are going to Neuromatch Academy, that they can watch lectures with anybody online, that people are zooming into lab meetings to tell them what they think about the Journal Club paper that we're covering,
*  that students are forming social networks, that all the information that you want is immediately online, that we have practical classes online, that students build their own things now.
*  We're doing things in a way that is much more liberated than it was 15, 20 years ago and certainly more than 30, 40 years ago.
*  And I think as long as it stays rigorous and as long as we take feedback from each other and as long as it's OK to make mistakes as we go along, then I think that we start to see interdisciplinary work not as a challenge or a problem, but as an everyday way of doing science.
*  And I hope that other areas of biology start doing that also.
*  Very good. Final question.
*  And this is really just for me.
*  I was reflecting.
*  So I went snowboarding yesterday with my snowboarding buddy.
*  And, you know, he's not a scientist, but sometimes I tell him about what I do.
*  And he's bright guy so we can keep up.
*  And I was reflecting on how many friendships I've kept in and out of my science life.
*  The question is, what percentage of your friends understand what you do?
*  Have a, you know, at least a cursory or slightly above cursory understanding of what you do.
*  Are all your friends scientists or do you have friends outside of science that you keep up with as well?
*  That's an interesting question.
*  I don't know.
*  I've never quantified that I'm engaged in a fair amount of public communication of science.
*  And for that reason, even my non-academic friends who aren't academics at all are reasonably aware of what it is, what it is that I do.
*  But I do think that us as scientists that we need to be spending much more time in engaging with the public.
*  And I do worry that as we become so hyper competitive and so hyper focused on research that it sort of removes us from the everyday of life.
*  And that's been observed by many academics in the past that the mechanism that which through which scientists are kept out of public conversations are not very direct.
*  It's not by any kind of any kind of direct carrot or stick mechanisms.
*  It's mostly because everybody is just so harried by the everyday stresses of doing their job.
*  And that includes research scientists.
*  But I think what we've seen in the past five to ten years with certain political movements and with the ominous, rapidly accelerating problem of climate change,
*  we don't really have a choice except to accept to properly engage in a serious way with not just our friends and family,
*  but with colleagues and other citizens who aren't scientists.
*  And I would say that the most important thing that I have learned when interacting with people I know who aren't scientists about scientists is that we have to completely reorientate how we're approaching it.
*  We cannot just go and tell them the way that they need to interpret the science.
*  I mean, it's there's no difference between me telling a lay person how they need to interpret neuroscience and me going to an in vivo physiology conference and telling them how they should interpret the memory and that doesn't work.
*  The public fund science and they only should hear about science as a collaboration.
*  We are doing science with them when we're asking the public about science or telling them about science.
*  We need to be asking them what they think is interesting, what they think is important.
*  And we need to be in a non-confrontational way, meeting with them where they are, offering them new insights that our work has produced and then discussing different ways forward where the whole process is considered a partnership.
*  And not a one way communication of information.
*  Communication is bilateral.
*  And I think we as scientists, we need to listen to the concerns of regular people.
*  And that involves being as generous as possible to views that may not be very well articulated all of the time.
*  Thank you for being generous with your time Tomas.
*  This has been fun. I appreciate it.
*  Thank you for having me, Paul.
*  Brain Inspired is a production of me and you.
*  I don't do advertisements.
*  You can support the show through Patreon for a trifling amount and get access to the full versions of all the episodes plus bonus episodes that focus more on the cultural side but still have science.
*  Go to braininspired.co and find the red Patreon button there.
*  To get in touch with me, email paul at braininspired.co.
*  The music you hear is by The New Year.
*  Find them at thenewyear.net.
*  Thank you for your support. See you next time.
*  Be where I go.
