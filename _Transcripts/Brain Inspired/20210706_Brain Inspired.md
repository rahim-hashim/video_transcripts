---
Date Generated: April 20, 2024
Transcription Model: whisper medium 20231117
Length: 5103s
Video Keywords: ['Education', 'Science', 'Technology']
Video Views: 14654
Video Rating: None
---

# BI 110 Catherine Stinson and Jessica Thompson: Neuro-AI Explanation
**Brain Inspired:** [July 06, 2021](https://www.youtube.com/watch?v=JbaUMV7-Rwc)
*  You know, we sometimes talk about, you know, how do you explain the brain or how do you
*  explain cognition as if there's going to be, you know, one answer to that question.
*  The original project of AI was along the lines of, like, we don't have any idea how intelligence
*  works or how thinking works, but if we could make something that thinks, that would tell
*  us something about how it works.
*  And that's kind of playing on, like, an older idea of what an explanation is.
*  If we could have a better understanding of kind of the differences between explanation
*  in these different fields, that that might inspire new ways of explaining in a kind of
*  unified science of intelligence.
*  It's very easy before you've actually tried doing it to criticize the approaches that
*  people are taking in the lab, but then actually trying it yourself and nothing working for,
*  you know, years and then realizing that the way that it's actually done in the lab is
*  so much harder than it looks like.
*  Maybe all philosophers of science, to get their degree, must spend some time in a lab
*  to crush their spirit.
*  That's what needs to happen.
*  Hey, everyone, it's Paul.
*  Today is a mostly philosophical discussion about explanation and understanding.
*  Understanding brains, understanding minds, understanding the deep learning models that
*  we use these days often to model brains and minds.
*  So I have Catherine Stinson and Jessica Thompson on today.
*  Catherine is an assistant professor at Queen's University in Kingston, Ontario, Canada.
*  These days, a lot of her research is on ethical concerns in philosophy, but she's written
*  and thought a lot about how to connect the philosophies of both neuroscience and AI.
*  And Jessica recently began a postdoc, really recently, in Chris Summerfield's Human Information
*  Processing Lab at Oxford.
*  You may remember Chris from a recent episode with Sam Gershman.
*  I want to say that was episode 94, but anyway, it was recent.
*  So I brought Catherine and Jess together today because both of them have written papers recently
*  that are about bringing together the notion of explanation in neuroscience and in AI and
*  the problem or the challenge of using modern tools like deep learning to explain and understand
*  how brains produce cognition and minds.
*  My original goal was to bring their ideas together and explore what I found to be a
*  common element between those main ideas in their recent papers.
*  But we end up touching on a lot of related topics without quite getting to that original
*  goal of mine.
*  There's a lot to ponder among what we do talk about, but I just want to touch briefly
*  on what I found to be some common ground between their two papers.
*  And although we don't fully explore this in our conversation, it may be good to have
*  in the background as you listen.
*  Okay, so Jess's paper is titled, Forms of Explanation and Understanding for Neuroscience
*  and Artificial Intelligence.
*  And part of her paper is focused on how explanations work and differ in neuroscience and in AI
*  and how we might unify them.
*  So Jess proposes to focus our explanations on classes of phenomena that are common between
*  the functions that AI and humans perform, defining the proper scope to which explanations
*  apply.
*  Okay, now Catherine's paper is titled, From Implausible Artificial Neurons to Idealized
*  Cognitive Models, Rebooting Philosophy of Artificial Intelligence.
*  Because Jess's idea focuses more on how explanations are related to phenomena, part of Catherine's
*  piece focuses more on how models are related to the target system that they're modeling.
*  And basically Catherine suggests that it's not a direct relationship between the model
*  and the system, but instead is mediated by a specific kind or aspect of a phenomenon.
*  So both the model and the target system instantiate this specific aspect, and that's how they're
*  related.
*  That's how the model and system are related.
*  And so when we use models to explain some system, we need to keep in mind that they're
*  doing it through some simpler or more abstract aspect or kind of both the model and the system.
*  So these are two subtly different ideas, but what I find in common between them is their
*  concern with the reach or the scope of an explanation or of a model.
*  Anyway, that little introduction doesn't suffice to fully articulate the issues that they are
*  dealing with in both of their papers, which they expound upon more at length.
*  So I encourage you to read their papers, and I encourage you to do it in tandem actually,
*  because they're also filled with history and philosophy surrounding everything that we
*  and that more fully build the cases for the arguments they make.
*  And we do discuss a lot of different topics along the way, including the difference between
*  explanation and understanding as per Hank Direct, whose book on scientific understanding
*  I link to in the show notes.
*  So the show notes are at braininspired.co.uk.
*  Happy explaining and understanding to you out there.
*  Thanks for listening.
*  I've never started an episode talking about knitting, but that's what we're going to do here.
*  Is there some secret underground neuro AI knitting society that I need to know about?
*  Oh my God, if such an underground society exists, I definitely need to join.
*  And if it doesn't, Catherine, we should really start it.
*  I haven't got my invitation yet either.
*  What is the connection?
*  So Catherine, you had one, you were looking at like Morse code related to patterns for knitting.
*  Is there a connection between the patterns in neuroscience and creating these knitting patterns?
*  And I'm sorry if these are naive questions, but it's because they're coming from a naive person.
*  I mean, we could spend probably two hours on this, but since you asked.
*  Yeah, I mean, I had those tabs open mainly because we'd been talking about this and my
*  preparation for today mostly consisted of like looking at knitting patterns and blog posts about knitting and coding.
*  I don't always have four knitting tabs open, although I did have to share my screen once teaching recently
*  and I had a whole bunch of macrame tabs open and I was just sort of hoping that nobody noticed.
*  Yeah, there's a few ways that they're connected.
*  So one of the kind of stories is that codes were passed through knitting in a bunch of cases during wars.
*  And it's that there's sort of two basic stitches that look different.
*  And so you can encode things by alternating those stitches.
*  And then I think that, well, there's more than two stitches.
*  There's a bunch of different stitches.
*  And so you can make patterns and sort of whatever base you want using those,
*  whatever number of stitches that you want to have in your project.
*  But there's two basic ones that are the usual ones along the neck here.
*  There's like alternating knit and purl for the two basic ones.
*  Which makes a rib. Yeah, a rib.
*  It's kind of maybe hard to talk because there's also all three colors.
*  Did you make that? Yes.
*  And there's another connection. This is great for an audio podcast.
*  I'm demonstrating. So there's, as your podcast viewers cannot see, but you can see there's this sort of stripey pattern.
*  You have black and white.
*  And this is a self-striping yarn that's usually used for socks.
*  And this is one that's supposed to look like zebra stripes.
*  And I bought this as a result of seeing a talk at the Turing Centennial in Cambridge in, I think, 2012.
*  And there was a talk about how Turing had figured out a bunch of the math of animal markings
*  and how there's sort of like these repeating patterns that have like kind of like a basic,
*  I don't know, biological code at the base of them.
*  And then I thought during this talk, like, oh, you must be able to use this to make like self patterning yarn.
*  And so I decided that I was going to try to figure out how to do this.
*  But then first I searched, like, does this already exist?
*  And of course, it already did exist.
*  And there was a series of yarns that made various animal patterns.
*  So there's this zebra one that I ended up buying and then a cheetah one.
*  And then there's a whole bunch of others that are sort of less impressive.
*  But you can make like if you know the width approximately of the thing that you're going to be knitting,
*  then you can make something that will make a particular pattern by just having the yarn change color in regular intervals.
*  Jessica, do you I don't know who knits more.
*  But and we don't need to break this out into a huge rivalry.
*  But is there something about knitting that, for instance, do you get ideas from knitting?
*  Is it a generative process or is it really something in the background that you do,
*  like where ideas come because you're doing something that's pleasant and fairly mindless?
*  Or is it really a generative sort of process?
*  For me, it's probably more the latter.
*  It's more like I think knitting can be almost meditative if the pattern is of the right complexity,
*  where you do have to pay attention and maybe you have to count.
*  But a lot of times it's just you're just responding to what's already there.
*  And so you just have to have some basic attention on it and then you can mind wander.
*  And I find that one of the most pleasant states is just getting into like a knitting flow.
*  I think when you're working on research on a PhD or something, it's so hard to make progress.
*  But in knitting, every movement gets you one measurable step closer to finishing some project.
*  And that very constant feeling of slowly moving towards a goal is I think also satisfying.
*  I think my master's was one of my sort of very obsessive knitting phases,
*  one I should have been done already but wasn't.
*  I mean, I think that's a really good point to transition into like what I want to ask about next,
*  because I mean, I was going to say my entire, I was going to say career, but journey,
*  let's say, in trying to understand minds and brains really can be probably best summarized
*  as discovering how little I understand step by step.
*  And so wondering, am I making progress?
*  And then going down a road and realizing, oh, that's not the right road to go down or,
*  oh, now I really don't understand what I thought I was pursuing in the first place.
*  At the same time, it's impossible for me to trace out sort of the evolution of my own thinking,
*  because it's almost like I came from a blank slate with like a few assumptions that were wrong, right?
*  But I didn't like come into neuroscience and AI with some preconceived, steadfast,
*  staunch notions about what I thought consciousness was and what I thought brains do and minds and,
*  you know, AI, for instance.
*  So this is an unfair question is what I'm saying.
*  And so maybe Jess, we'll start with you.
*  How would you characterize the trajectory of your own thinking about brains and minds,
*  but also about explaining and understanding and what these things are?
*  I would say that my ideas have changed a lot.
*  I think I had a similar experience to you in that, you know, you gradually realize how
*  little you know and maybe how little everyone else knows too.
*  And that you don't know how what you're working on fits into something bigger,
*  like presumably science is a collaborative project and we have some goals we're working towards.
*  And at some point, I think I felt like I really didn't have good answers to why is what I'm
*  doing meaningful in any way or how does it contribute.
*  But you started off with that sort of assumption that what you would be doing would contribute
*  to like the larger meaningful picture and then realized over time that maybe it wasn't.
*  Is that how it went?
*  I mean, I think I had probably pretty I wouldn't say I was like I didn't start as an especially
*  good scientist.
*  I think that I you know, you learn how to do sciency things like how to collect brain
*  activity and how to run some software.
*  And that's a lot different than like being able to think about, you know, theories and
*  how to build theories and how to actually make progress towards something.
*  I don't think we make progress in science just by doing sciency things.
*  But yeah, I think I had no idea what I was doing.
*  And now now you have a crystal clear idea.
*  No, not at all.
*  But I do think that reading more philosophy of science was really helpful for me to at
*  least figure out how to pose some of the questions that I was struggling with and realize that
*  there are, you know, some well formed answers to some of them.
*  Maybe they're not, you know, even if they're not the perfect answer, just being able to
*  look at a selection of answers, I think gives context.
*  Yeah.
*  So how have my ideas changed?
*  I think I've become increasingly pluralistic in the sense that recognizing that not only
*  are there many different questions that we're going to want to ask, but there are probably
*  many different ways to answer those questions and that I'm not looking for the right approach.
*  I'm looking for some set of justifiable approaches.
*  And yeah, that'd be like one main takeaway.
*  Is that a freeing feeling, realizing that pluralism is okay?
*  Or is it more of a letdown that there isn't one answer?
*  I think that there is the risk of pluralism devolving into kind of anything goes, which
*  can be problematic.
*  Like we don't want the message to be like, it doesn't matter what you do.
*  So I think that there's still some, it's not easy just because we embrace pluralism.
*  They're still kind of difficult.
*  It's still difficult to find, you know, what do we want to do?
*  So but you kind of started off as a, entered as a scientist and have moved a little bit
*  more toward philosophy as part of that.
*  Is that no?
*  Well, I definitely cannot claim to be a philosopher.
*  I have no formal training in philosophy.
*  My primary research is as a scientist.
*  But I mean, in your interests, like the evolution of your interests and what you feel might
*  be needed to make progress and to fit within the meaning of everything.
*  Right, certainly.
*  So I definitely came into it as a scientist being like, how do I build computational models
*  of perception or cognition?
*  And ended up reading about, you know, what is a model?
*  Yeah.
*  Yeah.
*  Catherine, you came, is it fair to say that you came in more from the philosophical side?
*  Not really.
*  So I, I hardly took any philosophy at all as an undergrad.
*  Like I took a bunch of logic and that was, that was about it.
*  Yeah.
*  Yeah.
*  Yeah.
*  Like who gets to call themselves a philosopher?
*  And like, at what point did that happen to me?
*  Like that's, that's sort of an odd question.
*  I started being-
*  Are you a philosopher?
*  Like now I think I, I have to say that I am.
*  It's still a little bit uncomfortable because the kind of things that I do are not really
*  traditional mainstream philosophy.
*  So there are probably some people who would call what I do not philosophy.
*  And yeah, I mean, the desk rejection I got for a paper recently probably was on that
*  basis that I didn't actually cite any philosophers in the citations.
*  But yeah, no, I started out thinking that I was going to be a math major or a computer
*  science major, but I sort of constructed a kind of like a liberal arts program for doing
*  that despite the absence of that option being available where I was studying it.
*  So I ended up doing cognitive science, which just sort of like as a something that would
*  fit together the things that I wanted to take under something that they were going
*  to call a degree.
*  And then when I was applying to PhD programs, I was undecided between neuroscience and philosophy.
*  So I was applying to both.
*  And it was mainly that, you know, someone at the Montreal Neurological Institute sent
*  me an annoying email that I decided not to go with that option and to go become a philosopher
*  instead.
*  But that wasn't even really my intention either.
*  I found it really odd that people started calling me a philosopher as soon as I arrived
*  there having been a computer scientist until that point.
*  People asking me to justify like, why is philosophy an important thing to do?
*  I'm like, why are you asking me this?
*  I'm not one of them.
*  Anyway, yeah, I mean, my trajectory looks a little chaotic probably on paper.
*  But I think there's like a few pretty common patterns that are all sort of overlapping
*  each other.
*  So one is from like really believing in the project of AI and like the sort of like existential
*  quest that I want to understand the mind and the brain as like a way of, you know, knowing
*  how I work and then becoming like a little less convinced that that's something that
*  possible to do or like a possible to do in our lifetimes or that a little bit less sure
*  that the ways that we're going about it are actually moving towards that kind of a goal.
*  And then also the going from being kind of like a reductionist to being critical of that
*  kind of approach.
*  Yeah.
*  And then also the like, can I do science?
*  Am I not any good at science?
*  That kind of quest to like where it's very easy before you've actually tried doing it
*  to criticize the approaches that people are taking in the lab.
*  And you can always find some problem with an experiment that someone's done, but then
*  actually trying it yourself and nothing working for years and then realizing that the way
*  that it's actually done in the lab is so much harder than it looks like on paper and a completely
*  different kind of thing that it seems like before you've tried it.
*  And then kind of going back and taking on this role of critic again, but from like a
*  different perspective where I'm trying to find in the science like what is working and
*  how you actually find truth in all of these imperfect experiments and also the way that
*  all of these imperfect experiments like don't fit together cleanly at all.
*  And like what do you do with that?
*  And you can't have this sort of clean monolithic picture of how it all flows together.
*  Oh my God.
*  You're just describing the depressing PhD experience of most of us.
*  Like oh my God, it doesn't work and none of it makes sense.
*  And how do I make, yeah, from armchair philosopher to practicing, attempting practicing scientists.
*  This goes to like for me anyway, so I've been reading a lot of philosophy lately, but it's
*  important that the philosophy is embedded in things I'm interested in and scientific
*  questions that I'm interested in because that's what keeps me driving and reading more and
*  learning more because without that and without my own whatever expertise I have, without
*  my own experience doing the science, I think it's easy to get lost in like you're saying
*  and untethered and just start thinking about things and not end up anywhere.
*  And so I can go pretty far until I feel like I'm just kind of spinning my wheels, right?
*  And learning about models as fictions and then where it's really not connected at all
*  to what I'm interested in the doing aspect, the pragmatic aspect of doing the science.
*  So there's always this question of how much philosophy is the right amount for me to be
*  spending my time on and I'm not even in a lab now so I can spend my time on whatever
*  I want to spend on.
*  So Jessica, how much, how do you break down your time and effort in the philosophical
*  side of things versus the practice of science?
*  Yes, so during my PhD I probably by some people's opinion shouldn't have read any philosophy
*  at all, you know, like it wasn't required for what I needed to do to finish my dissertation.
*  But at some point you just, it just felt like the most important questions were there that
*  I couldn't stop.
*  I couldn't, like I just felt like I couldn't continue the science until I had like at least
*  some basic grounding in like why am I even doing this, you know?
*  Like I feel like everyone goes through a phase in the PhD where it's like how do we even
*  know anything about anything?
*  One phase?
*  One phase?
*  A phase?
*  Well you're lucky if it's a phase I guess.
*  Like you're lucky if you come to terms with that, right?
*  So yeah, that phase for me was like okay I need to like figure out how to even talk about this.
*  Moving forward I'm not sure, like I, you know, I just had this paper out where I, you know,
*  I had some things that I wanted to say.
*  I'm not really sure where it's going to go from there in the sense that, you know, I'm
*  not trying to develop a career as a philosopher but I would love if my future involved a
*  philosophical component.
*  I don't, yeah, I don't always see how that would work.
*  Like I would love, you know, scientists we collaborate with people, you know, so I would
*  love to have collaborations with philosophers moving forward.
*  But it seems like philosophers write a lot of like single author papers and I don't know
*  much room there is in that kind of like in between philosophy and kind of meta science.
*  And yeah, so I'm still learning and figuring out how that can be a component of my research
*  career moving forward.
*  So is this like empirical philosopher camp?
*  Empirically informed philosophy.
*  Empirically informed where the history and the practice of doing science is directly
*  enmeshed within the philosophical issues being discussed.
*  Yeah, some philosophers also talk about sort of the naturalistic approach thinking of philosophy
*  as somehow subsumed within some scientific practice.
*  I'm a little bit unclear on exactly the history of the evolution of these ideas.
*  But yeah, I think a lot of scientists who maybe don't know very much about philosophy
*  might have certain ideas about what philosophy is and the methods that are used.
*  Like I've had some people tell me like, oh, that's just some people's opinions and they're
*  not even scientists.
*  Why would you listen to them?
*  And so in the paper, what I was trying to do is try to give people a sense of like the
*  methodologies have evolved and how like contemporary philosophy uses an array of methodologies from
*  anthropology, sociology, psychology, and case studies and field studies.
*  To try to give people an impression that philosophy is not just looking at science from a distance,
*  that there are a variety of methodologies employed.
*  That it's not just, Catherine, do you get the rebuttal, hey, that's just your opinion?
*  I don't think I've ever heard that one bit.
*  Well, not in so many words.
*  Yeah, I mean, I think that that would be a fair criticism of a lot of philosophers,
*  but those don't tend to be the people who would want to be embedded in the lab in the
*  first place.
*  So the, yeah, maybe you should ignore the people who have no interest in what's going
*  on in the lab if you're a scientist.
*  But there are within philosophy of science, just about everyone either has some lab experience
*  or at least reads scientific papers.
*  So has some idea what they're talking about.
*  Maybe all philosophers of science to get their degree must spend some time in a lab to crush
*  their spirit.
*  That's what needs to happen.
*  OK, speaking of crushing spirits, maybe we can start off with just the difference, Jessica,
*  between explanation and understanding.
*  So I mean, I guess I'll just leave it there because you write about this at length in
*  your paper.
*  Are explanation and understanding different?
*  Are they the same thing?
*  I'll let you just answer and then we'll dig down into it.
*  Yeah, so how different they are kind of depends on what, you know, which specific notions
*  you want to discuss.
*  But the, I think if there's one thing I want a listener to take away from this would be
*  the distinction between the subjective sense of understanding.
*  So an individual's experience of having understood something as being distinct from scientific
*  explanation.
*  And that when we're thinking about goals in science, what do we want to achieve?
*  That we don't just want to achieve this subjective sense of understanding, that that's not the
*  extent of our scientific goals.
*  Because if it was, we, you know, we might, we'd be happy with sort of delusions that
*  provide us with some sense of understanding, even if they have no relationship to reality
*  or the things that we actually want to do with the products of science.
*  Was it Hempel that conceived of understanding as this subjective feeling, personal, oh,
*  I get it sort of experience and, and then later through, so in the paper you write about
*  a lot about Hank Direct, I don't know if I'm pronouncing his name correctly, but his sense
*  of understanding, I think his book's called Understanding Scientific Understanding.
*  And he makes a distinction between understanding and explanation in that you have to have explanation
*  and it has to be intelligible in order to derive understanding.
*  And then like you were saying, there's this whole pragmatic notion of understanding as
*  in using understanding to then move forward and do things with that understanding.
*  Yes.
*  So we could say that kind of maybe more traditional philosophers of science focused on building
*  explanation.
*  And these theories were intended to be objective in the sense that they weren't kind of specific
*  to a community of scientists or a social historical context.
*  It was just an objective relationship between the phenomenon to be explained and its explanation
*  codified in some description probably.
*  This goes back also to what we were just talking about in the need for expertise in some domain
*  to claim an understanding, right?
*  And we don't want to be able to say, well, you don't understand because I'm an expert.
*  However, it's the experts that have the domain knowledge that can look at an explanation
*  and decide whether it's intelligible, whether it makes sense, and then counts essentially
*  as an understanding.
*  So it almost takes that expertise to make a judgment on understanding.
*  Do I have that right?
*  Yes.
*  So direct kind of offers this.
*  So maybe just to back up a second, I mentioned that maybe more classical philosophers of
*  science imagined explanation as this objective thing and kind of like dismissed the role
*  of maybe these pragmatic factors in kind of serious theories about explanation.
*  There are alternatives to that view.
*  There are more pragmatic theories of explanation.
*  But direct is really saying we need...
*  Wait, first I wanted to say that someone like Hempel would basically equate scientific understanding
*  and scientific explanation, that you achieve the cognitive state of understanding when
*  you are in possession of a satisfactory explanation.
*  So he would say genuine scientific understanding is what happens when you possess this satisfactory
*  explanation of whatever phenomenon it is you want to explain.
*  So basically, once you have the explanation, you're done.
*  You also have the understanding.
*  It's the state of being once you have that.
*  Distinct from kind of this subjective feeling of understanding that doesn't really have
*  to do with having an explanation or not.
*  It's just you feel like you understand.
*  And then direct says, actually, we need a theory of understanding that's independent
*  of the explanatory strategies that are being employed.
*  That you aren't guaranteed to experience understanding just by virtue of having this explanation.
*  That in order to have that explanation, you actually need to have these intelligible theories
*  first.
*  So he distinguishes between what he calls understanding a phenomenon is sort of like
*  this explanatory understanding.
*  And then kind of understanding a theory is basically what he calls intelligibility.
*  So you have intelligibility is the values that scientists attribute to theories that
*  permit their use.
*  And so it's not just a property of the theory itself, but of the community of scientists
*  that are developing and trying to use those theories.
*  So it depends on the skills and background assumptions of those scientists.
*  And with that understanding of understanding, does understanding remain your goal personally?
*  So direct says that understanding is the central goal of science in general.
*  His work isn't focused on deep learning or AI and neuroscience at all.
*  His is a very broad, just scientific understanding account.
*  Maybe he needs to get to a lab and have his spirits crushed.
*  But I think he does probably because he is more in line with this practicing scientist
*  and practicing philosophy joiner.
*  Back to my question.
*  So with that newer sense of understanding, do you agree that this is like the main goal
*  of science and or you personally?
*  Yeah.
*  So I find it difficult to argue against direct that what he calls understanding phenomena
*  is the main goal or like a primary epistemic goal of science or at least a primary goal,
*  whether it's epistemic or pragmatic, it's a whole other thing.
*  But in order to do that, you need these explanations.
*  So I feel like it still comes back to the importance of explanation in science.
*  And for me personally, sometimes I feel like the conversation in neuroscience and especially
*  in this neuro AI space, I find that it does tend to focus on this intelligibility part
*  where people are concerned about how intelligible our deep neural networks as models.
*  And I feel like that ignores kind of this explanation side where it's like, you know,
*  we don't just want intelligible models.
*  We also want to explain things.
*  And specifically because intelligibility is this changing thing, this context dependent,
*  real dependent thing, I'm sort of willing to be loose with that a little bit.
*  And especially in the case of artificial intelligence, I can see our intelligibility changing.
*  You know, we're making a lot of progress in how we understand deep learning and artificial
*  intelligence more generally.
*  And so I guess I'm a bit optimistic that, you know, intelligibility is something that's
*  currently in flux.
*  And so I don't find that like a great argument for why we shouldn't explore the use of deep
*  neural networks as models in neuroscience and cognition.
*  Yeah, I guess just to restate the one of the central issues in comparing deep learning
*  with brains is that deep learning nets are very complicated as our brains.
*  And so, you know, we turn from instead of trying to use them to explain brains, well,
*  now we need to explain them and make them intelligible before we can make that step
*  of comparison to explaining brains.
*  Is that does that sound right to you?
*  Yeah, I think there's a bunch of things going on here.
*  So the original project of AI was along the lines of like, we don't have any idea how
*  intelligence works or how thinking works.
*  But if we could make something that thinks, then that would tell us something about how
*  it works. That would be like a theory of thinking or a theory of of intelligence.
*  And that's kind of playing on like an older idea of what an explanation is, or what a
*  theory is, where a theory or an explanation is something more like a proof.
*  And like early AI people were like pretty explicitly saying like, yeah, the program
*  that shows this behavior that is intelligent, that is literally like the proof of what
*  we're trying to explain.
*  So like just as you're writing down a bunch of steps sort of showing that one thing
*  follows from another, the program kind of also does that.
*  It's a bunch of steps showing how, you know, the intelligent behavior falls out from
*  whatever you you start with.
*  But that's that's not really the way that that we think about explanation anymore.
*  Like that was sort of shown to not be a good way of understanding science.
*  Partly that just having this kind of like knowing the syntax and having it flow in
*  sort of the right formal ways doesn't guarantee that you get a meaningful result.
*  And also just that that's not the kind of thing that we're we're looking for in
*  science. The meaning of the symbols matters, too.
*  And I think this is all getting kind of tied together in ways that no one really fully
*  understands right now and in this space.
*  And and I think Jess's paper is helpful for pulling out this like this question of like,
*  why are people in deep learning doing the kinds of experiments that they're doing?
*  Like, what is what is the scientific goal behind this?
*  Yeah, so like maybe we don't just need to make like the deep learning network that
*  performs the behavior and just say, OK, we've got the explanation now.
*  It's the program. But if we if we if we can't have any insight into what the program's
*  doing or why it's doing it or how it's doing it, like one question is like, is it actually a good
*  theory of the behavior or is it sort of accidentally doing it in a dumb way just because
*  it has lots of data? But even putting aside that question, even if we think that like,
*  yeah, this thing that plays chess or whatever, it really understands something or, you know,
*  really is generalizing or it really is doing whatever sort of intelligent thing you want it
*  to be doing. If you don't like just having the the syntax, just having the program, that's not
*  enough for it to be an explanation under sort of more contemporary views of scientific explanation,
*  because you need to be abstracting at the right level where you're picking out the relevant things
*  and that sort of thing. I guess the difference might be something like you could say that you
*  have a theory of some kind of physical system if you have like the locations and the momentums of
*  all the particles in the box. There's more to know in a way. But if you don't understand like why
*  they're moving in the kinds of currents that they're moving in, then in some other way,
*  you don't understand it. You don't understand like, you know, why there's a vortex or why it's
*  heating up or why, you know, whatever other kind of macro phenomenon that's going on is happening,
*  you just have a list of numbers. So there's that concern too, I think with even if you believe that
*  a deep learning network really is sort of doing vision or doing object recognition rather than
*  doing like a parlor trick, then there's still this, well, like, that's not all that there is to an
*  explanation. And maybe this understanding piece that it being something that could be understood
*  is part of it. That's maybe one way of putting it. I'll jump in here because I just want to jump to
*  the to what I see as the common thread between both of your recent papers here. And that's
*  basically and you guys will correct me. But also this is steeped in my own coming to terms with
*  how I view explanation and in terms of what deep learning is doing and what brains might may or
*  may not be doing and what minds are is the importance of the target of the explanation.
*  Because even in your example there, like the why is the why of the the balls are moving in
*  different directions. That is a different question. It's a different target of explanation almost than
*  being able to explain it in terms of some sort of coarse graining. Like it's just it's heat,
*  thermodynamics is heat, right? So I wonder if it's probably if it's useful. And we could either go
*  down the mechanistic sort of explanation road, which maybe we should save I'm thinking or if it
*  might be useful to just talk and put on display just your claim that we should be focusing on
*  phenomena specific things to explain and relate it maybe to Catherine your claim that really what
*  models should be doing models are an instantiation of a quote-unquote kind and we can talk about what
*  a kind is and that kind or aspect is one kind or aspect of the target system that you're modeling.
*  And that's a mouthful and probably needs to be unpacked. But would you guys agree? Do I have
*  that wrong that there's that common thread that it's really the target of explanation that seems
*  to be a key to moving forward in explanation? There's a section I think of Jess's paper about
*  the different grains or granularities of why questions. Yeah, I mean, I think that's getting
*  at the same kind of thing. You know, what is the thing that you're explaining? What kind of
*  granularity of why question you're asking? Well, for instance, you just said if if the model I'm
*  not sure I don't remember your exact wording, but if such a system was quote unquote, doing vision,
*  and this is one of those things where you realize, oh, I thought I knew what vision meant. Now I
*  don't know what vision means or the pluralism of what doing vision is depends on depends on
*  what you mean by doing vision, right? And I feel like and I haven't you know, I need to study this
*  more. But I feel like different approaches to explanation are in one of the major differences
*  is that they actually have different targets of what they're trying to explain. And you can almost
*  say that the pluralism has more to do with the target of explanation than the phenomenon almost
*  well, the phenomenon being the target of explanation. Now I'm off the rails. Jess, please.
*  Yes, I would say that how one conceives of what it is they're trying to explain might, you know,
*  bias them or might it might lend them to a particular notion of explanation. So if you really think
*  about cognition as you know, the result of some dynamical system, you know, the brain is a dynamical
*  system, and we get our actions and control through perturbations of those dynamical systems, then
*  you're probably going to want to construct dynamical explanations. And if you're thinking
*  about the brain as an information processing system, and you're thinking about inputs and outputs,
*  that might lend you to think about like more functional explanations where you're decomposing
*  some computation into some sub operations or something. Yeah, so what I say in the paper is
*  that, you know, we sometimes talk about, you know, how do you explain the brain or how do you explain
*  cognition as if there's going to be, you know, one answer to that question. But the brain is involved
*  in a lot of different things. You know, some of the phenomena that we might want to explain in
*  neuroscience look more like biophysics or, you know, biochemistry. And some are a lot closer to
*  the things we do in psychology, or, you know, computer science. Yeah, so I try to say that,
*  you know, is there a way of thinking about the phenomena that we want to explain that like what
*  really ties the explanations together is the class of phenomena. And can we imagine that our
*  artificial agents and, you know, natural intelligence might both be instances of similar phenomena,
*  in which case they might be explained similarly. It doesn't mean that the explanations themselves
*  will be the same or similar, but that the form of the explanation, so like what constitutes a
*  satisfactory explanation, or what can constitute a satisfactory explanation should be shared. So
*  like if you think that this functional explanation really works for explaining some perceptual
*  behavior in monkeys, and then you, you know, train a network to do the same thing, does the same kind
*  of explanation hold in the artificial system as well? Is it worth tying this to, let's, you know,
*  the, what everyone talks about is the hierarchical deep learning networks for vision, for, that are
*  supposed to match up with the ventral stream vision. Is it worth using that as an example,
*  or is it too esoteric, you think? I ask because my experience, so, so there's this dangerous sort of
*  road that I go down, right, where now we have these models that can categorize images extremely
*  well, and the activity among the units through some transformations has a lot of similarity to
*  the neural recordings that happen in monkeys, for instance. And it's awesome, right? It's like really
*  great that the units look like that. And then I worry because one of my reactions is, oh, well,
*  that means I'm not actually interested in how to categorize an object. I'm interested in something
*  more. And I worry that every time an advance is made in AI, I'm gonna, there's a name for this,
*  for this phenomenon where a person says, yeah, but it can't do X yet. And then until it can do X,
*  and then you can just go on ad infinitum until like it explains the entire brain, and then where you
*  left saying, oh, yeah, but it can't do X. So I worry about that. But at the same time, I realize
*  that maybe I'm not interested in object categorization, categorization per se, in this example,
*  I'm interested in how that functions in the larger system to do X and, you know, how it fits within
*  a larger behaving system. And then I realize, maybe I'm not interested in intelligence, maybe
*  I'm interested in life, what am I interested in? And then, and then I have to finally go to sleep
*  at some point. That seems like progress, doesn't it? Doesn't it? Don't you feel like you are
*  reformulating the questions that you want to ask in a productive way? Yes, but I don't know if it's
*  productive, because it just kicks it down the road. And then I'm still left with, well, what am I
*  actually interested in? And it feels like progress. I'm just not sure that it actually is progress.
*  My take on this is that early on, people couldn't imagine any way that that something like vision
*  could be implemented. We had no idea how it could be done. And so the approach of like,
*  let's try to build something that can do the same thing, would at least give us like a candidate
*  hypothesis for how it could be done. Yeah.
*  This is back to Newell and Simon's work. Like, yeah, with the original thing that you're talking
*  about. Yeah. And like at that point, actually being able to do something that looks intelligent
*  was an amazing result that no one had done before. And so even though like now it seems like, yeah,
*  of course you can program something that adds or whatever. Like it doesn't seem impressive anymore.
*  But at the time, like it was, we don't have any hypotheses for how this is done. And so if we can
*  like build something that does it, like at least we have a hypothesis and that's like a major
*  advance. And I feel like we're doing like a bunch more of those moves that we can't imagine how we
*  could implement, you know, this sort of more detailed kind of versions of the task and then,
*  or I could being able to do a bunch of different tasks or being able to do this task, but in a way
*  where you generalize or, you know, whatever kind of additions you want to add to it, that it's sort
*  of equally amazing that we've managed to do something that matches the results pretty well.
*  And I see this in terms of inference to the best explanation that if you've only got one candidate,
*  then that's the best explanation. And that's sort of the state that we're in. And it's not like that
*  isn't a bad way of explaining things. The deep learning models you mean, for instance.
*  Yeah. So if you had like a few different, you know, kinds of AI that all were capable of doing
*  the same thing and they all had different structures, then you'd have a problem.
*  But we don't have that. So like maybe there is just this coincidence where it can only be done
*  in one way, but maybe not. I mean, we do have variation within these types of things, right?
*  Like we have different architectures and people do those kinds of experiments where they plot,
*  you know, how does my, you know, how do these different architectures fair? Or like if I add
*  this element of biological realism, if I add recurrence or if I add this kind of weight sharing,
*  how does that change the correspondence? So I feel like maybe it's still in the,
*  could still be analyzed in the context of inference to the best explanation in a kind of
*  like model comparison framework. But I think it's still going to be limited if all we're doing is
*  just trying to explain brain activity, like statistically capture variance in brain activity
*  as much as possible. That's not what I, like I want to explain the, I want to, the target of my
*  desired explanations are not just, you know, some variance that's associated with the stimulus.
*  And I think, I mean, people are appropriately critical of the sort of approaches where you
*  just like throw a lot more data at it in order to get the details more correct,
*  without being concerned with the architecture making sense. So I think like people are going
*  about this in a couple of different ways, both trying to get the phenomena to match better and
*  also being concerned about whether the architecture matches and like whether the architecture needs to
*  match exactly or not. That's one of the things that's sort of up for grabs, I guess.
*  So you're both steeped in the modern mechanistic mode of explanation. That's kind of the most
*  popular, I'd say also receives the most criticism, which kind of cements it as the most popular or
*  most well-known maybe mode of explanation these days. And I think, Jess, in your paper, you make
*  the distinction between explanation in neuroscience and explanation in AI, at least coming up, you
*  know, that they have different threads, essentially, that neuroscience, and you can correct me if I'm
*  wrong here, that neuroscience generally has tried to explain constitutively the constitutive
*  mechanisms, which means how the parts are doing, like what they're doing. So if you talk about the
*  neurons and their activities and how that relates to the phenomenon to be explained.
*  Whereas in AI, you, I believe, talk about ideological or causal mechanistic explanations,
*  which focus on how it came to be that way. And from, well, a causal story about how, you know,
*  the architecture sets it up so that when you run it through that architecture, it triggers the
*  response through the system. And I didn't explain that well. But this, but what I want to tie it
*  into is this modern notion, which you also talk about in the paper of, you know, how do we
*  understand deep learning and maybe the way some people like Blake Richards and the thousand
*  authors on that one paper, and Conrad Kurding and Tim Lilikrap talk about under trying to understand
*  maybe what we need to do instead of focusing on the activities of the neurons. Maybe we need
*  to focus on the architecture and changes in the architecture and how that changes the activity
*  of the output of the network also changes in the objective functions, which how you're training
*  the network to compare to some objective. And thirdly, focusing on the learning algorithms that
*  are you're implementing in the network and changing all of these. That is a causal story about how the
*  network is changing. Is that a just an awful summary of that modern approach? There's just so
*  many caveats and it's so hard to select the point about neuroscience and be caring more about
*  constitutive mechanisms. I know that that's like a very broad stroke. And like a lot of people will
*  disagree with me with that, that there's I'm sure that there's lots of neuroscientists who
*  write ecological stories as well. I can name one. I think maybe well, I'll just interrupt you real
*  quick because I was wondering what your thoughts were on someone like Paul Cheesak's phylogenetic
*  refinement approach that really looks at evolution in a step by step process and the evolution of
*  function and how evolution changes. That seems more like an ideological causal kind of story.
*  Yeah, these are both in the context of causal mechanical explanation. And I try to
*  I mean, yeah, philosophers, maybe I shouldn't do this, but I try to talk about the mechanisms
*  independent of this whole like without commitment to this causal mechanist framework. Because
*  probably people would say that the deep learning approach to neuroscience,
*  maybe we can't call it rightly causal in that. Yeah, it's abstract. It's often a lot of mathematical
*  relations. So just to say that there's a lot of caveats, that it's maybe complicated. But the
*  main point that I was trying to make was just that the the invitation to a deep learning approach
*  to neuroscience, I was suggesting that it could be analyzed as this invitation to consider
*  these etiological stories about what mechanisms produce four phenomenon that maybe we'd previously
*  been searching for constitutive mechanisms. And maybe that's part of why that those kinds of
*  explanations or those kinds of stories feel unsatisfying to certain neuroscientists, that
*  they're looking for a story that is going to be about how neurons and or like parts of brains
*  in some way like neuro could be neurochemicals or part of the brain, or part of the brain,
*  could be neurochemicals or parts of cells, how they work together to realize whatever it is that
*  you want to explain. The learning story, which is really more about some structural mechanism
*  that sets up the conditions such that a triggering mechanism like presenting
*  an image to an animal triggers the phenomenon that you want to explain, like the recognition
*  of the object in that image. Is it my training as a neuroscientist that makes me feel personally
*  like there's something missing or unfortunate if that's what all I had to go on were the
*  the you know, the learning objectives, the learning algorithm, the objective function,
*  and the architecture, there's something definitely satisfying about it in one sense,
*  but it also isn't the way I sort of naively originally thought of think of, well, brains
*  manifest consciousness and that I can't really tie the architecture to that or something, you know,
*  like the the mind aspect, what I really want is a story about how all of this activity is realizing
*  in your term realizing mind. I don't think you have to give up your desire for constitutive
*  mechanisms. I think you can acknowledge the value of both. But part of the story is that, at least
*  in these modern the ideological approach that we're talking about here is that it's just too
*  hard to to grasp to understand the constitutive mechanisms. Let's move on to what we can understand
*  right now and push that aside for later. I mean, is that a valid? Do you find that disappointing?
*  Or are you satisfied with that? Or, you know, is it even a valid way to proceed?
*  Yeah, so I think that that that point is like we can put it into directs language that they're
*  talking about some notion of intelligibility. That is what is intelligible right now. Yeah.
*  And so I think that that's very justifiable to say this is what I'm going to focus on right now
*  because that's what seems to have the most promise. And I think that, you know, we want
*  a diverse group of scientists making those decisions and coming up with different answers.
*  I don't think we want everyone to be doing the same thing. So, you know, I'm happy for there to
*  be different answers to that question. And yeah, I'm a little bit optimistic on the intelligibility
*  side that I feel like we are developing new conceptual tools that might enable us to have
*  something like a constitutive story or, you know, different types of etiological stories.
*  But I resist a little bit the like, we'll never do this. So don't even bother. But at the same time,
*  you know, I'm going to we should all be doing what we think is most kind of promising. And if we have
*  a bunch of people doing a lot of justified science, then we'll make progress faster
*  than if we all do the same thing. Yeah, I'm wondering whether the this is a bigger picture
*  where you might want to understand not just one person's cognition or, you know, all people's
*  cognition, but all primate cognition or all, you know, cognition, whether artificial or
*  natural or whatever the other option is, then then maybe the etiological picture starts to make more
*  sense. Because maybe the thing that they all have in common is something more like an architecture
*  that allows this thing to develop rather than the details of the constitutive picture.
*  And maybe there's even sort of a gradation between the two. So an architecture is,
*  it could be seen as constitutive just at sort of a higher level. Whereas what you'd maybe be
*  more satisfying to a neuroscientist as a constitutive explanation is more just like a more detailed
*  etiological picture. So it's like filling in the details of the etiological picture for that
*  specific organism in that specific situation, and that very particular brain region at that
*  particular time of day, etc, etc. Yeah. And, and the sort of older philosophical questions about
*  AI are all focused on this, like, what is intelligence in general? What is it, not just in us,
*  but you know, what would it mean in anything? And, and that, that was why people were, I don't know,
*  30 years ago or whatever, drawn to functional explanations rather than more constitutive
*  explanations. Catherine, can we bring in, can we elaborate a little bit, since we haven't really
*  described your account of modeling and how models relate to target phenomenon through kinds, would
*  you mind just kind of explaining broadly that framework and so we can bring it in? Because
*  this is, I think, the common thread. And I want to know why I'm wrong thinking that these are
*  very similar sort of conclusions about what's needed. Yeah, I guess, I mean,
*  Baderman thinks of what he's doing as saying that the kind of explanations that he's talking about
*  with minimal models are not mechanistic. But my move, I guess, is to is more of a unifying
*  move where I think that to be mechanistic, something doesn't have to be constitutive
*  in the sense of like being about the gory details that it can be still be constitutive while being
*  about like abstract kinds of things that are interacting. And they're not abstract in the
*  sense of being sort of like in, you know, the realm of ideas there. So I instead of saying abstract,
*  I try to say generic to distinguish that. So generic is something that's still physical,
*  it's still instantiated, but what you're pointing out about it is something that is more general than
*  detailed. So like an example would be like, I'm sort of a very particular individual, but I'm also
*  an animal. And there's like an animal sitting here. Like that's something that's true, but
*  that's more generic than saying like this particular human with this particular history and
*  makeup is here. And so if you allow mechanistic explanations to deal in that broader kind of
*  thing, which I don't see any reason not to, then you can kind of unify these where
*  generic kinds of things are interacting with each other in the same sort of way that
*  constitutive explanations work. And you can have these explanations work at various levels. And so
*  you can have it be about just a bee, or you can have it be about insects, or you can have it be
*  about animals in general, or you can have it be about intelligence in general. And I don't see
*  really strict divisions between those different kinds of explanations. I think we're still sort
*  of doing the same thing. So you said when you're talking about abstractions or generic,
*  you said they're abstractions of something physical and instantiated. So what about like,
*  like in machine learning, we have a lot of, you know, machine learning theory, we're working with
*  really mathematical objects, something like learning rate. So like the learning rate in a
*  learning algorithm, it's going to be like a critical feature in many explanations for
*  how this system works. Is that an abstraction of something physical? Yeah, I mean, the learning
*  rate would be instantiated in some sort of set of numbers in memory in your computer and or some
*  you know, ways that the program works. But then you can talk about that in a more general way
*  when you're talking about more than one program that all have this similar kind of feature.
*  I don't know if that answers your question. In this example, are we talking about learning rate as the
*  thing to be explained as the kind or the learning rate within the model?
*  I was thinking about it as a component of a mechanism. And whether or not it can count as
*  that if it is like never instantiated in the computer. So like some machine learning theory,
*  like deep linear neural networks, in cases where you don't have to actually run the experiments
*  in the computer, you can just write down the math and you know, what's going to happen? Can we still
*  talk about mechanistic explanations in machine learning theory where you're not
*  putting anything on the computer is an open question that I have. Yeah, I kind of struggle
*  with that one too. And I'm tempted to say that if you so even if you're not writing anything down,
*  you're not actually putting in a computer where it's sort of like, you know, physically compelled
*  to go through the steps of your program. If you're assuming that you're still sort of following
*  some kind of mathematical rules, even in you and you're thinking about it, then that you could see
*  that as something just as mechanistic. Just like you're sort of simulating the program in your
*  wetware.
*  Pete Slauson Are you saying that you can't essentially can't
*  avoid somehow physically instantiating it?
*  Katie McGriff Certainly, you can avoid it, you can just not do it. But
*  if you're doing it just as a theoretical exercise, you're still going through the steps of
*  assuming a certain kind of mathematical sort of rule that you're going to follow. And I mean,
*  you are free to not follow the rules, you're free to, you know, make mistakes in the way that you're
*  doing it. And that's sort of different than something that's instantiated in either a computer
*  or in like an organism. But if you assume that you're going to be following the rules, then
*  in a way you are also instantiating something that's in the same class of thing as what you're
*  thinking about, I guess. But that I think is like pretty controversial. And I'm not
*  not sure I'd want to hang my head on that.
*  Katie McGriff Yeah, maybe we're getting into kind of the leads of this. Like, I feel like when I
*  talk to like machine learning researchers about how they think about, you know, what explanations
*  they might be providing, it does seem like it's in the it's in the math, it's in some ability to
*  to write it down. And not in not in every case, but as in some cases, and I struggle to figure out
*  how to to fit that into what I know about explanation in neuroscience, where I feel like that's
*  maybe less the feeling.
*  Ben Frick Go ahead.
*  Katie McGriff Well, just that like that feels like an opportunity to me, that if we could
*  have a better understanding of kind of the differences between explanation in these different
*  fields, that that might inspire new ways of explaining in a kind of unified science of
*  intelligence.
*  Ben Frick Just backing up one thing that I actually want to know if you guys have
*  thoughts on this is how off I am in thinking that your arguments are have this common thread. And
*  maybe it's just my central interest in trying to understand what it is I'm trying I'm interested.
*  So here's the thing, my little journey, I've realized has been one, among other things about
*  trying to figure out what it is I'm actually interested in knowing. And that's an that's a
*  humbling kind of experience to think, well, what I'm interested in knowing has nothing to do with
*  what I originally thought I was interested in knowing. And so my own target of explanation
*  in that sense has changed over time. Am I wrong in thinking that the crux of arguments that you
*  both make in your paper, the ones I'm focused on at this very moment, Catherine arguing that,
*  that it's the kind the aspect the aspect of something, which is an instantiation of something
*  from the target system that you want to that you want to explain, and that you instantiate
*  that aspect in a model, that thing that kind as the target of explanation, is that how similar
*  and how different is that from just your conception of the need to focus on specific phenomena that
*  then can be addressed through different aspects of artificial systems, natural systems in certain
*  conditions, etc. I see them as compatible. Yeah, you should just listen to Catherine.
*  And she's probably saying what I should have said.
*  And we're done. No. All right, well, maybe what we can do then is and so I, you know, I'll point
*  people obviously to your papers, because this stuff is somewhat endlessly fascinating to me,
*  which is a problem, because you just end up doing philosophy all day. And that's not a problem for
*  you, Catherine, because you're a philosopher, obviously. And whereas Jess can always just say,
*  I dabble in philosophy. Don't take me seriously. I just dabble. She's nodding. That's right. So
*  given your backgrounds and sort of how you got to where you are now, what I wonder if you guys have
*  advice for anyone kind of coming into the field, I mean, in one sense, it's the most exciting time
*  ever, right? Because we have these tools in deep learning to help us what we really think we have
*  a purchase on explanation moving forward, or at least, Jess, you express your optimism, right,
*  that we're getting a grasp on something intelligible that then we can actually, you
*  know, sink our teeth into and move into better explanations. On the other hand, there's a lot
*  up in the air about what we think we do and don't understand. And then, you know, obviously trying
*  to understand the system that we're using to understand the target, the AI system we're trying
*  to understand so that we can even use that to understand brains and minds. I'm wondering if
*  you guys have any advice on anyone kind of coming into this field, wherever they're coming from,
*  something that maybe would have benefited you had you known? Catherine, let's start with you.
*  Yeah, I don't know. I mean, I think I studied machine learning 20 years too early.
*  I just had really bad timing. So don't do that.
*  You had mentioned because just the other day, we chatted really quickly. And you had mentioned that
*  you were not only machine learning what you said now that you were studying too early, but you were
*  studying the bias and fill and what's the word I'm looking for? So policy issues in AI issues that
*  are at the forefront of conversations today, the dangers, the ethics, that's the word I was looking
*  for the ethics of AI. And that's a lot of what you do now as well. But back then, you were
*  interested in this and no one was talking about it. You could have just stuck with it. Right?
*  Instead of Well, I'm giving you I'm planting the advice that I think you should give to people,
*  right? Your advice is not to do things too early. Is that it? Was that the summary?
*  No, I mean, yeah, it's more like I don't know that you can really distill much very good advice out of
*  my experience. It's more like a set of misadventures that through a series of accidents
*  ended up working out. One thing that I think that Jess mentioned earlier was that reading
*  philosophy was something that she was sort of told that she shouldn't be doing. And that's
*  maybe a common thread that the thing that sort of led me to having a job now was doing something
*  that I had been told that I was supposed to not do. So, you know, writing things that aren't sort
*  of like real publications. When I finally decided to quit academia, that was when I allowed myself
*  to do that. And then that one piece of popular writing was like the single most important thing
*  for my career development, like by a wide margin. Can you point to what you're referring to? So,
*  also, this is the first time you mentioned that you quit academia, and now you're back in academia.
*  Maybe you could elaborate a little bit. Yeah, I quit and then now I'm back.
*  Yeah, like this was still back a couple years ago when like philosophy plus AI was not really a
*  viable career plan before this sort of strange shift happened where it's sort of the only viable
*  career plan in philosophy. Yeah, and so I wrote like an op-ed for The Globe and Mail, the national
*  newspaper in Canada about AI ethics basically. And yeah, I mean, that was like ridiculously sort of a
*  door opening thing, like academics and other and loonies and like all kinds of people were
*  emailing me with like opportunities, both desirable and like wildly undesirable
*  as a result of that. And yeah, I mean, just it was really kind of shocking that like the thing that
*  I did only because I was quitting was turned out to be like exactly the right thing to be doing.
*  And that, you know, a few steps later led to me ending up back in academia.
*  What lesson is to be learned from that though?
*  I don't know that there's like a real lesson there. Things don't always turn out the way you expect
*  or like, I mean, you could come up with a lesson like, you know, if you're doing something in
*  grad school that everybody thinks is weird, that doesn't necessarily mean that it's not
*  valuable and isn't going to become like the thing that everyone cares about five years later,
*  10 years later. But I don't think that that's like generalizable advice. Like,
*  sometimes that's not gonna work out.
*  I can think of specific examples and I won't share them where I was at a talk with a fellow
*  graduate student and I thought, why are they doing that? That's no, that's not gonna lead to anything.
*  And then, and now looking back on anything, oh, that's why they were doing that. I don't
*  know if it actually led to anything, but I look back and think, oh, okay, I get it now from this
*  different perspective where I am now, where I was there, it didn't look like a promising route,
*  but where I am now and oh, okay, I understand why they're doing it. I'm trying to think of like,
*  what, you know, a listener would want to, wouldn't want to hear, well, I should quit
*  and do something different and see what happens, you know?
*  Yeah, I mean, it was really just like a series of accidents. So I worked in policy, but then there
*  was a change of government and so the funding left and so then I ended up back in like academia
*  because jobs had suddenly started to exist in AI ethics and that was what I'd sort of
*  decided to do instead when I switched into policy.
*  Well, so Jess, this is your, you're now starting, is this your first postdoc?
*  Yep.
*  So I'll just reveal you, so you're doing a postdoc in Chris Summerfield's lab, who's been on the show
*  been on the podcast in the past. First of all, what do you think of Catherine's non-advice?
*  And then do you have advice for people as well?
*  Yeah, I mean, I was trying to think of like a generic lesson, but they, it's almost a cliche,
*  you know, like, don't be afraid to take risks and follow your passions.
*  You're special.
*  But I think there is something to be said about allowing yourself to do what you're compelled to
*  do and being kind of aware of, you know, where does your motivation for different things come
*  from, how much of it is external and how much of it is internal and, you know, letting that inform
*  your decisions about what to do and just because something's completely internally motivated
*  doesn't mean that it's not worth doing.
*  Is there something you wish someone had told you going into academia, going into studying
*  the various things that you studied that would have been helpful?
*  Yeah, I mean, there's things that I did hear that I've kind of taken to heart.
*  Like in working in interdisciplinary fields, there's sometimes maybe people distinguished
*  between sort of breadth versus depth. There's some people maybe who go really deep on one topic,
*  and other people who their specialty is that they have broad knowledge in several fields.
*  And I think I really identify as somebody in the latter category that my primary creative
*  work in academia is in cross-disciplinary synthesis and being able to kind of find
*  relationships between ideas in different fields.
*  And of course, I still hope to have some deep knowledge about things that I'm never going to
*  do. I hope to be as deep as somebody who like just focused on, you know, statistical physics,
*  and that's what they do or whatever, you know? So kind of figuring out what are you trying to do
*  in that sense, I think can be helpful.
*  Maybe I can distill something out of that because I mean, I'm probably sort of paper thin in terms
*  of my breadth versus depth balance. And I think a lot of advice that people get about academic
*  careers is to go the depth route, to find a specialization and go deep on it, and to hope
*  that you were lucky in choosing one that is a good one. But I think that there's room for both,
*  that you can be a breadth person or a depth person, and that trying to be a depth person
*  when you're more naturally a breadth person is not going to work very well because you'll,
*  you know, you'll burn out or you're, you just won't be interested anymore. But yeah, I mean,
*  looking at just sort of other people's careers, it's, there are lots of breadth people too,
*  who end up being successful.
*  I think it's easier to succeed in terms of finding a position and keeping a job.
*  Do you think it's easier as a depth or breadth person? And you have to choose one.
*  It's a random crap shoot, the way you look at it. Have a famous advisor, like that's the only
*  advice you can really give. Be lucky.
*  And that it's so random. Yeah. But part of luck, so I would say serendipity, right?
*  Because- Do good work and be lucky.
*  I mean, I feel like the obvious answer would be depth though, right? Because there's these,
*  there's these silos in academic institutions that have names on them. And if your degree
*  doesn't have that name on it, then it's, it can be, my impression as someone who hasn't really
*  applied for those jobs is that it can be pretty hard to get into those departments if, you know,
*  you're not a deep person in that field. So, you know, I'm not going to get hired.
*  Well, I will say anything in case someone's listening.
*  But yeah, I feel like there are challenges there if you really want to be an interdisciplinary
*  person in figuring out like, which silo would I actually want to be in or, you know, finding
*  research institutes or centers where you can have joint affiliations. And that seems like a challenge.
*  Yeah, I think it's, it's harder to be a breadth person. Because the way that all these things are
*  set up is designed for everyone being a depth person. And yeah, so it's, it's harder to get
*  most of the jobs that are out there. If you're not really specialized in the right things.
*  And it's harder to get grants probably. And it's harder in a lot of ways. Yeah, but it is
*  the least possible to do the other option. Maybe I'll just bring it back to the last moment here to
*  the notion like directs notion of understanding and intelligibility that it takes sort of specialists
*  in a field right to consider something intelligible. So there's another cliche is like, well, you want
*  both right, you want breadth and depth. Jess, you used the word synthesis and I and that's breadth
*  and that synthesis I think is much harder than people probably appreciate. Because to do good
*  synthesis, you have to have some depth in those in the various fields that you're synthesizing.
*  And that's so it's synthesis sort of demands breadth and depth. I guess what I'm saying is
*  it's hard. This is all hard. But wonderful and great also, I suppose. We'll end on a high note.
*  Do you guys have anything else to add? Or should I just wrap up then?
*  Um, yeah, I don't know. That at least I feel like I benefited a lot from working with a lot of
*  different people who thought differently and that that helped me to be critical. I mean,
*  I think that's pretty, it's pretty common these days that people seek kind of, you know,
*  coach revision and people get internships and there are opportunities to work with different people.
*  That that have people who think radically different, though challenging can maybe be
*  good in a long time to try to try to work through those conflicts.
*  Well, guys, thank you for being here. I'm going to let you get back to your knitting now. Be honest,
*  who's going to be? Are you guys going to be knitting like five minutes after we talk here?
*  I have a little bit of sewing that one of my kids is demanding that I do. But I'm going to eat lunch
*  first. I'll definitely be knitting tonight. It's kind of basically a daily occurrence. I will knit.
*  Well, thanks for taking the time and talking with me guys. Appreciate it.
*  Thank you. Thanks for the invitation.
*  Brain Inspired is a production of me and you. I don't do advertisements. You can support the
*  show through Patreon for a trifling amount and get access to the full versions of all the episodes
*  plus bonus episodes that focus more on the cultural side but still have science.
*  Go to braininspired.co and find the red Patreon button there.
*  To get in touch with me, email paul at braininspired.co.
*  The music you hear is by The New Year. Find them at thenewyear.net.
*  Thank you for your support. See you next time.
*  The covers of the past
*  They take me where I go
