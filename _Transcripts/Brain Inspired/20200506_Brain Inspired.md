---
Date Generated: April 21, 2024
Transcription Model: whisper medium 20231117
Length: 5196s
Video Keywords: ['Science', 'Technology', 'Education']
Video Views: 241
Video Rating: None
---

# BI 069 David Ferrucci: Machines To Understand Stories
**Brain Inspired:** [May 06, 2020](https://www.youtube.com/watch?v=E0IXeCBmH6I)
*  What does it really mean to understand the text? What does it really mean to understand?
*  If you were looking at your child and saying, how am I going to test their understanding
*  for any arbitrary story? What would it mean to have understood it well? We exist in a
*  community of other humans. We are designing and defining ourselves and our future. Machines
*  can never do that for us. It's not about can you solve the problem. It's more can
*  you choose what you want to be. Computer can't choose for us. We have to choose for ourselves.
*  This is Brain Inspired.
*  Yeah, I'll take Brain Inspired for 2 million, please. This AI expert started a company called
*  Elemental Cognition to build AI thought partners that learn to understand stories and help
*  us make better decisions, be more creative, and think more clearly. Who is David Ferrucci?
*  Correct. Hi everyone, this is Paul. And now that I think of it, I cannot be the first
*  person to make that joke. Thankfully, I did not make it when David and I were talking.
*  Today I talked with David Ferrucci, who a few years ago now led the team that built
*  IBM's Watson system that won at the game show Jeopardy. Since then, David has started
*  his own company, Elemental Cognition. He and his team aren't satisfied with the state of
*  language understanding in AI, so they're working to build machines that understand stories
*  or some text, with the longer term goal to build what David calls a thought partner that
*  will be able to teach itself whatever a human asks it to learn so that the thought partner
*  can then help the human with his or her own goals. It's ambitious, and he's optimistic
*  about their progress, and we had a fun conversation about many things, including their latest
*  work in which they developed some new criteria to assess a machine's understanding of narrative
*  stories. And they show that existing systems aren't very close to the level of understanding
*  for them to be useful. You can visit the show notes at brandinspired.co to learn more.
*  All right, enjoy the conversation with David.
*  Dave, I've heard you tell stories about growing up and falling in love with programming on
*  a computer. Is that true? And I want to ask, how did your parents make a living?
*  Well, it is true. I did get introduced into computers fairly early on, I guess. Well,
*  in these days, maybe it might not be considered that early, it was in high school. But in
*  those days, considering that computers were not as anywhere near as ubiquitous as they
*  are today, it was actually quite early. It was, I mean, my dad worked in construction.
*  He had a small construction company. My mom was a stay-at-home mom, and they were both
*  very insistent that I get a really good education and become a medical doctor. And my father
*  would just get uneasy if I wasn't spending every minute of the day studying. And I remember
*  one summer, his school's out, he used to play Alice Cooper's school, it was out forever.
*  Wait, what year is this? When did you graduate high school? Is that too personal? I'll get
*  the Alice Cooper release date here and we'll figure it out.
*  Well, it hadn't been released for some time, but it was still a favorite to play when you
*  got out of school. And so I just want to hang out with my friends. And my dad's like, what
*  are you doing? This was high school, I think this was like sophomore or junior year maybe
*  in high school, what are you doing? And so I'm just hanging out, he says, you got to
*  go to study. I'm not going to study, it's summer, school's out. It means you don't work
*  on school. And he was like, found something in the paper that says, look, there's the
*  local college, there's this advanced math class, why don't you go take it, just don't
*  waste your summer. Anyway, so I ended up going. And it was actually a computer programming
*  class. So they were teaching like, you know, basic programming language, and most of the
*  people taking it were older than I was. And they were looking for jobs in this whole,
*  you know, computer programming, which I didn't even know what it was. Most people had no
*  idea what it was. And so I started, you know, just following along. They had computer
*  terminals, CRTs, they didn't have, you know, before that, they didn't even have screens
*  much before that, but they had had screens. And I started, you know, learning how to
*  program. And I was like, this is unbelievable. If I, you know, I had a very systematic mind
*  always, and I never liked to do anything twice. Once I figured out how to do something, I
*  never wanted to do it again. And so I can write down a set of instructions, and the
*  computer will do it for me tirelessly over and over again. This is amazing. And what
*  if I can get it to think? What if I can get it to do all the work I don't want to do?
*  Because I used to have to, you know, go to, you know, think about going to med school,
*  I was an A student, I was constantly working, working. And if I can just program the computer
*  to gather the information and think, then I can just go and ask good questions. And I
*  can be creative, I could always be doing something new. If the computer can do like a lot of
*  this other work for me, be my thought partner, as it were, and help me be creative and help
*  me solve new problems, as opposed to me ever doing the same thing over and over again.
*  It's gonna say you've been asking the same question ever since.
*  I have, I'm a man with a mission of obsession to really, and then of course, as society
*  and as the technology evolved, the whole area of artificial intelligence, which had already
*  been started in the 50s, nobody really talked about it and knew about it at the time, as
*  I started on my own learning how to program and starting to reading and studying about
*  the field, and of course, there wasn't, you know, the internet at the time, but I just
*  started doing more and more. And so while I went to college for biology, I ended up
*  programming the computers for the department, I ended up doing all the ecological simulations,
*  I did analog to digital interfacing for the physiology department, I did ecological simulations,
*  I helped them build a whole curriculum around the use of computers, this is back when it
*  was only Apple IIs, Apple IIs was a hobbyist computer, they were being used in schools
*  and all over, I became the computer person.
*  You were like V, at the time, there was like the computer guy, he was always a guy at the
*  time, I guess.
*  It was like, who is this guy? We have this computer, we don't even know what to do with
*  it, and he's building a whole curriculum out of it. I was tireless and I was constantly
*  programming, I loved it, everything I saw I would try to program it. And all my Apple
*  too, using basic and then Pascal, and so anyway, and it was, I loved it, and the dream of that,
*  in fact, for my senior thesis in college, I ended up studying medical expert systems.
*  I was doing really well in school, my college wanted to promote me, I was like the number
*  one person to go to medical school, my parents of course wanted me to do that, so there was
*  this intersection between medicine and what I really wanted to do, which was AI, artificial
*  intelligence and computer science. So I ended up doing a thesis on medical expert systems,
*  and there had already been a bunch of work on it actually, and I did that. And then at
*  the last minute, actually I shouldn't say last minute, it was the end of junior year,
*  I thought I had a degree, I was going to graduate with a degree in biology, and I said, you
*  know what, I don't want to be a medical doctor. This caused a lot of pain for my dad, mostly
*  for my dad, and I said I want to be a computer scientist, and my dad didn't even know what
*  it was. It's his fault. What does that mean? I mean, what are you talking about?
*  The only reference that a lay person would have is sometimes there were commercials,
*  and they would talk about computer data processing, and there were these people typing on keyboards.
*  You're taking up a degree in medicine to be a typist? Are you kidding me? You just
*  couldn't comprehend it. I ended up getting, last year I got a minor in computer science,
*  ended up going to graduate school, and just sticking with AI for my whole life, and it's
*  been great, and I'm not done yet. We're doing some really great things at Elemental Cognition
*  and advancing the field and pursuing that dream. Of course, the Watson achievement was
*  tremendous for me, but I've done so many projects in the space and still going strong.
*  Eventually, my dad came around. Many years later, he ended up having some health problems,
*  and we were together sitting in a doctor's office lamenting over what he had been going
*  through with his health problems, and he said to me, I'm so glad you didn't become a doctor.
*  In fact, what's interesting was, he ended up, I ended up getting him a computer. He ended up
*  working at a place where he was helping them do some organization, data organizing. This was
*  later in his life. He was mostly retired, but doing some work. He would come to me and say,
*  maybe you can get the computer to help us do this. I sat down with him, and it was remarkable
*  because it turns out he was a natural information designer. I would draw some sketches about how
*  to organize the information, and he would start making comments that indicated that he intuitively
*  understood what first, second, and third normal form were for databases. I was like, I got to tell
*  you, Dad, this is quite remarkable. He was a good information designer, good app designer. I would
*  do the programming, but it was so interesting how we found each other much later in his life.
*  He must have been good at construction. I asked about your parents because you can't see me, but
*  I'm wearing the only piece of clothing that I took from my dad's closet after he died, and it is an
*  IBM shirt. I don't exactly know how old it is, but on the back of it, it says, we work hard to earn
*  these stripes. It has the old striped IBM logo on it. Anyway, I don't even know what the logo looks
*  like these days, but he was an IBM guy. I had a very different experience than you. You're one of
*  those people who just fell in love with programming from the beginning. My experience was growing up
*  my dad, who was a great father, but he would come home during those days when I was in middle school
*  and all around there. We had a computer. It was like the computer room. You had your room where the
*  computer was. Ours was the guest bedroom, which was upstairs. It was near my brother's in my room.
*  He would come home from work, throw his stuff down, and pretty quickly get to that computer.
*  Then he'd be sitting there staring at the computer forever through dinner or until dinner and then
*  after dinner. I just thought I wanted to go out and do stuff and throw the ball around. He threw
*  the ball with me and stuff. It's not a sad story, but it turned me off from staring at a screen all
*  day. Of course, the irony is I'm sure I stare at a screen all day more than he did these days,
*  but it really made me very uninterested in programming. I blame him for that. Maybe I wish
*  he was in construction or something because I got a really late start on this stuff. That's great.
*  We have this IBM bond, you and I, that you really don't know about because we don't need to talk
*  about Watson and your great success doing that stuff because you've moved on and you've started
*  Elemental Cognition, which you've been doing for a few years now. IBM was good to me too
*  because it had earned my dad a lot of money and hence we had a pretty good life growing up.
*  Dave, and his name was Dave too, by the way. Is it accurate to say that your long-term ambition
*  then really was started in those days, essentially to create what you call thought partners,
*  which are available to everyone to help us essentially be more creative and think more
*  clearly? Be more creative, think more clearly, amplify our intelligence in a positive way,
*  help do more critical thinking, complement our intelligence. We have cognitive biases,
*  we have cognitive weaknesses, we have tremendous strengths. I think computer intelligence,
*  if it's not an alien intelligence but actually one that's compatible with our intelligence,
*  can complement our thinking and our creativity and ultimately our productivity.
*  Look, you look at science fiction all the time and when you look at them and it's inspiring,
*  computers and machines are working together. My favorite is a lot of Star Trek scenes where they
*  Scotty or Geordi or the engineer or the captain and they're just speaking to the computer and the
*  computer is like, no, I get it, let me help you out, I'll organize this information, that information
*  for you, I'll explore that hypothesis for you, I'll read up on that and summarize it for you.
*  You can do the work and make the decisions and take responsibility for those decisions. I think
*  that's one of the challenges we have with machine learning today or systems that are strictly data
*  driven and don't have symbolic or conceptual representations is they'll spit out answers and
*  then you really can't engage in the intelligence and ultimately how are you liable for the decisions.
*  I think purely data driven systems are incredibly powerful and useful in so many areas.
*  When it comes to human decision making, humans, you got to let me into that decision process.
*  You have to have a compatible conceptualization. It doesn't mean the intelligence isn't more
*  advanced in one form or another, but ultimately as humans serving humans that have to uphold and be
*  responsible for our value systems, we have to get in there and understand what's going on. We have
*  to ultimately take responsibility. We have to look for designing systems that can take advantage and
*  leverage the machine's power to see patterns in data, but ultimately bring those patterns
*  into conceptualizations that we can reason about, understand, and engage with and ultimately be
*  responsible for. The machine's ultimate native intelligence or capabilities is really a separate
*  issue from its ability to communicate the right thing in the right way with the human.
*  That's exactly right. In fact, it's hard to think about, it's hard to really separate
*  our perception and practical use of intelligence absent from communication or distinct from
*  communication. Understanding and intelligence and communication for humans are very integrated.
*  We're going to get into language. We're going to talk a lot about language here.
*  I guess it was a few years ago, still when I was in academia in neuroscience. My specialty is not
*  language at all. In fact, my biggest shortcoming on the AI side is probably language as well,
*  so I'm going to be leaning on you a lot here. I was talking with a friend, a friendly acquaintance
*  who had gone through one of those somewhat early data boot camps to learn machine learning.
*  Deep learning had just started. Anyway, he came out and said, look, here's what you need to do.
*  This is the only thing left to do is natural language processing. This was a few years ago.
*  Is there an NLP arms race right now in the industry? What's your experience of this?
*  That's interesting. The machine learning techniques today are very powerful.
*  For any given problem, you can go off and if you have enough data, you can train these systems and
*  they can build complex models. For any narrow enough task, you can get machines to be humans.
*  I should say for any narrow task that is well-defined enough and has the data,
*  the data represents the phenomenon that you ultimately want the machine to represent,
*  machines will probably be humans. Language is a different story because,
*  and I think I'm pausing because you have to kind of tell this carefully,
*  but language is something humans use to share our understanding of things.
*  Often the information that is essential for the communication isn't always, or I should say,
*  sufficient, necessary for the communication, isn't actually in the language itself because
*  the two humans or the set of humans communicating share so much knowledge. They share,
*  not just knowledge, but they share perception. They share background,
*  tremendous amount of background information. They share the same mental processes and
*  internal features. In other words, they're all brains. They're human brains with similar
*  perceptions, often with similar backgrounds and so forth. The language ends up becoming a very
*  concise way to just orient the communicators in a very common space. Now as ML folks or as AI folks,
*  we go and we say, no, it's right there in the words. Actually, there's not that much in the
*  words. What is it that you expect to get out of the words?
*  Yeah. You kind of think of it like almost a key to, so I have to produce this,
*  translate whatever the heck is in my head, spit it out my mouth and have it land in yours
*  to kind of unlock our shared understanding of what we actually mean.
*  Correct. Right. The words are almost pointers into the shared experience we already have. In fact,
*  the more experience, global and local, that you might have, the less work I have to do to communicate
*  with you because I can just sort of almost like an index into the database rather than the content
*  itself. I think as historically, we've approached NLP as if the words were the content themselves,
*  when in fact, they're pointers to the experiences, to the world models, the mental models
*  that humans have evolved in their brains. I actually think, and we can revisit this
*  later too, that we're just not very good with words. I think that we're barely symbolic.
*  And if we were better with language, I know language is the greatest thing and one of the
*  things that separates us as humans from all other things, but it's like we're barely there. So I
*  don't know. It's like we're pointing. I agree with that too. There's the kind of the short and
*  the long, right? The great and the terrible of language. And I think we, because we're so good
*  at understanding each other, because we end up surrounding ourselves frankly with other people
*  who have similar thoughts and similar experiences, because that's a natural social thing. So we find,
*  by and large, we find success in communication. But when we step back a little bit, it's actually
*  really hard. So much time is put into, if you think about it, into communication. I mean,
*  journalists, educators, movie makers, your job, so much of our job is about some set of humans
*  getting another set of humans to understand them. The vast majority of management, government,
*  it's communication, communication, communication, to get other people to understand, to overcome
*  their biases, to be precise enough, to be clear enough, to be unambiguous enough. It is very hard.
*  But you're interested in building a thought partner and that thought partner needs to
*  understand whatever it is that the language is pointing to, needs a deeper understanding of that.
*  Right. It needs to understand and it needs to at least be able to communicate
*  its evolving mental model and engage with you to refine it, sharpen the gap. So I think of it as
*  almost like talking to a realist. And I'm sure you've met people like this. Sometimes people have
*  complained about me being like this. Boy, it's really hard to talk to you. Why? Well, because
*  you pick everything I say apart. You challenge every single word and it's like you're slow and
*  you don't really understand me and I think I'm being really obvious and it's just really painful
*  to talk to you. This is because I'm trying to really understand what you're saying. This may
*  underlie many of my scuffles with my wife. I don't want to assume anything, especially when
*  the communication is not just small talk, but it's really important. By that, did you mean this?
*  Where were you going with that? It could be that you were implying this versus that.
*  So what are you really, and because you're building this precise mental model that you're
*  going to make a decision over, the decision is important and all of a sudden the communication
*  is not that casual anymore. You don't walk away with the feeling that you communicated.
*  You want to walk away with the precision that you communicated. In fact, there was one
*  piece of research I like quoting because we think we communicate so smoothly and clearly with one
*  another. They actually studied people's dialogue because I think we use dialogue to actually
*  understand. I don't even think we say, we wrote this piece, we wrote this document and
*  this is a, sometimes you don't know what people are getting from it until you have the back and
*  forth, until you have dialogue. Dialogue is literally one of our most powerful tools
*  for aligning and sharing understanding. Even giving a talk where you're showing pictures
*  and speaking through the pictures and walking everyone through it, there's still something
*  missing there. Of course, there are different levels of expertise with-
*  There's different levels, depends who's in the audience, again, how much shared background they
*  have, are they used to your language, are they used to you, do they have a common foundation.
*  It's interesting because even when I remember taking an executive communication class when I
*  was at IBM and they were saying, 80% of your communication is nonverbal. I was like, what?
*  What do you mean? I'm putting so much time into the words and the precision.
*  In a 30 minute or hour talk, it's mostly going to be how are you standing, your cadence, your tone,
*  some big keywords, and it's interesting. There was a study and they talk about repairs,
*  a repair being a clarification or a full correction or reformulation of what was said
*  in the dialogue. There's one repair for every 36 words.
*  Yeah, I believe it.
*  For every five conversational turns, roughly about 36 words, there's one repair. In a task-oriented
*  dialogue where you're really trying to communicate precisely what you need to do, there's one repair
*  in every two and a half conversational turns. We are constantly going back and forth to
*  understand each other. With my children, it's basically a 0% non-repairing conversation rate.
*  It's all repair. I just start with repairs. I come from the academic world and I don't really
*  know how it is in industry, but everything boils down to some, it ends up in some philosophical
*  discussion about what you mean by the terms you're using. It's really hard to get beyond that. It's
*  comes down to definitions and how you're operationally defining that term that could have
*  700 different meanings. This brings us to your latest work here. Maybe we should dive into it
*  and you can turn our language keys so that we understand it a little bit better. Then we'll
*  revisit these broader questions. Sure.
*  Your latest work is along this same tract. This is how naive I am. I actually had to look up
*  the phrase machine reading comprehension just to make sure that I knew exactly what it was.
*  What is machine reading comprehension and what is the current state of machine reading
*  comprehension in AI? The concept of machine reading comprehension
*  or sometimes referred to as MRC, it's also just machine reading, is really this idea that
*  computers can read and understand. They can comprehend language in the way we would expect
*  a human to comprehend language. At least that's the philosophical or broad expectation.
*  The text can be in any form. It's unstructured text from what I understand, right?
*  Right. It's not an unstructured meaning. It's not a database with columns and field names and
*  semantics directly assigned. A typical structured or a canonical structured form is a table where
*  you say this is your name and this is your birth date. It could be anywhere from a sonnet to some
*  local vernacular. Sure. A narrative story. I think what's commonly used now are short stories.
*  It's a typical thing. I think that's probably good. Of course, you could use Wikipedia articles.
*  You could use North Times articles. You could read and understand. There's this reality. In fact,
*  I remember talking to the folks at AI too many years ago after coming out of the Watson experience
*  and saying, you know, so Watson understands. I said, come on. Watson doesn't understand.
*  I mean, it was a landmark and no one thought we could answer these so accurately, these
*  factoid questions when the questions were asked in such obtuse ways. But, you know, this is not
*  understanding. This is the way humans understand language. And in fact, you know, we can't even
*  understand. Machines, I shouldn't say we. We as a scientific community, we can't even understand
*  first grade reading comprehension stories and who we can. And of course, what's fascinating is you
*  go off and you look at how some of these systems work on Wikipedia. And, you know, it's remarkable
*  you could cherry pick a lot of questions and if you phrase them right, they nail them. But then
*  you give the same systems a first grade or second grade reading comprehension test and they flop.
*  And you look at that and you say, wait a second. If a person were able to read a complex Wikipedia
*  article and answer questions about it, you know, comprehension questions, you'd expect that person
*  to do pretty well on a first or second grade reading comprehension exam. So what's going?
*  Something's up. And even then, you can certainly write these exams where it's easier to do pretty
*  easier to do pretty well by doing text based matching and looking at patterns of words and
*  answering multiple choice questions. But, you know, you're a parent and you know that your kids will
*  sit down and they will do the same thing. In fact, in fact, I remember when my daughter was in first
*  grade and she had a, you know, a first grade science text and it was about electricity.
*  And she read it and it says something like, you know, electricity is produced by water,
*  water flowing over turbines. And, you know, that was what the story was about. And it went into some
*  more detail around that. And then there's a question, you know, how is electricity created?
*  And so my six year old comes and says that what created is kind of like produced. So I could match
*  that where in the text it says electricity is produced by water flowing over turbines. But
*  I don't know what electricity really is. I don't know what turbines are. I don't know how water
*  flowing over turbines makes electricity. I mean, I can get an A on the test, but I don't understand
*  this at all. Right. So like, that's a really poignant story because this is essentially what's
*  going on. You know, they're learning patterns that exist between the surface language
*  in the paragraphs and the surface language in the questions. So if you ask a why question,
*  it learns that answers to why questions are probably sentences that follow a sentence that
*  says because. And so you learn these patterns just like a lazy student didn't actually want
*  to understand the material and is taking guesses. Right. And so how do you learn now
*  that those machines, besides the fact that when you train them on Wikipedia, you ask the right
*  questions, they do well, you bring them back to first grade or second grade and they do miserably
*  on very similar questions, questions like why or where or how and all of a sudden they're not
*  doing well. That means the same thing's not going on. Understanding the way we think about is clearly
*  not going is really not going on. It's more like the cheaper, you know, text matching, even though
*  they find complex patterns very rapidly. Moreover, though, now you ask them why. Why is that answer
*  makes sense? And there's no mental model there. There's no logical model. There's those like,
*  well, I match these words. These words frequently appear around those words. And
*  these question words frequently appear around other words that have the same, you know,
*  vector representation. And it's like, what? This is not a human understanding.
*  So machine reading comprehension on one hand sets this lofty goal, but then how it's practiced,
*  how these experiments are set up are just not ambitious enough.
*  Isn't it? Aren't there benchmarks to essentially? So an MRC program is measured by how it answers
*  questions posed to it, right? So there's the comprehension part, that's the reading and
*  understanding, but that's measured by the questions you pose to it. I don't know how many
*  benchmark tests, data sets and things like that there are, but my understanding is that you're not
*  satisfied with sort of the common operational definition of comprehension in that respect.
*  Right. If you set them up properly, you can get at deeper understanding. But if you just look at
*  what happens to be hard in some contexts, and you set up these tests, you realize that really not
*  measuring understanding, like we're not stepping back, going kind of going to first principles and
*  say, wait a second, what does it really mean to understand the text? What does it really mean to
*  understand? If you were looking at your child and saying, how am I going to test their understanding
*  for any arbitrary story? What would it mean to have understood it well? Let me not try to pick
*  a question so much as to step back and say, what sort of model would you have built in your head?
*  So we had this view and again, going to the some of the cognitive
*  cognition literature and the learning literature, we sort of went back and we came up with this
*  template of understanding, which is to kind of put a stake in the ground and say, look, this isn't so
*  hard to appreciate when you put yourself in that position. If your child read and your human child
*  read and understood it, you'd expect them to do a few things and put in simple terms, you'd expect
*  them to say, who were the characters in the story? Like, what are the entities? What did they do?
*  What actions did they take? Where were they relative to each other? You don't have to know
*  geography, but like, where were they relative to each other? So their location and so many times,
*  but it's a draw me a timeline. So here's another way to think about it. Draw me a map.
*  What's on the map? You know, what are they relative to? Give me, draw me a map.
*  But it's a causal map as well.
*  But well, there's a couple. So we separate causal. So first, give me a spatial map.
*  You have any idea what's going on spatially here? You know, who's close to who? Where's the ball?
*  Where's the soccer? I see no net. Yeah. Where's the, you know, where's the schoolyard?
*  You know, just give me a relative stick spatial map. Like if you understood it,
*  you should be able to draw that. And then the perfect piece, some basic facts about spatial
*  relationships should be in there if you understood if it's a spatial story of any kind, obviously,
*  a timeline, what happened when? So what does this happen? First, it happened first. This
*  was this happening during that. Give me a timeline to what events occurred and what order.
*  What caused what? People took actions, things happened. Did they cause other states or things
*  to change? And what motivated the action, the people to take the actions that they did or the
*  agents to take the actions that they did? Why? Like why? Why? So you have the causal view,
*  the temporal view, the timelines, the map, and why did the characters do what they did?
*  Or the agents do what they did? And if we step back and try to be honest with ourselves,
*  that's the most basic level of understanding. Did you say, no, you didn't really understand yet?
*  If you don't know who did what, why, and where and when.
*  Yeah, it's almost like reducing the dimensions of the story into its principal components,
*  and figuring out what those components are.
*  Right. And this is how I first wrote this temple of understanding was,
*  motivations cause agents to perform actions at some rate through space and time to achieve
*  physical or psychological goals affecting the physical or psychological states of other objects
*  or agents. I mean, if you can instantiate that thing for any given story, you have a basic
*  understanding. A simple way to say is who did what, why, and when.
*  Yeah. You can't boil it down too simply because there's a lot of complexity in narrative and
*  it's no- There ends up being a tremendous amount of complexity to actually do that. And so therein
*  lies the rub. So when we look at- we did these great experiments where you take the current state
*  of the art and a Wikipedia article, we have this one article about the production of minting various
*  kinds of coins. And you ask the system, it's like a long, multi-page article, where were the cents
*  produced? And the system goes, oh, the United States Bureau of Mint. And it's this big article,
*  and you're like, oh, that's brilliant. You go to the first grade story about Fernando and Zoe
*  buying a couple of plants and putting it in their home, and say, where did Fernando and Zoe buy the
*  mint plants? Completely wrong. Not even close. And the examples go on and on and on about this.
*  And again, that's not even the main point of getting any particular question right or wrong.
*  It's really stepping back and admitting the systems aren't doing that. They're not understanding that
*  and they can't communicate that back to us. I've seen you demonstrate some technology. I
*  guess it's not this latest instantiation that's basically a question answering system. I think
*  you do it through a password protected part of the Elemental Cognition website, maybe. Unless that
*  demo is available. I don't even know. Is it available for people to play with?
*  I don't think we have it. I think there are different points in time where we made it
*  available. I don't think it's available right now. But yeah, I think I know what you're talking
*  about, right? I mean, that's kind of our principal demo that demonstrates this notion of natural
*  learning where the machine tries to read a story. It knows that it has to instantiate this template
*  of understanding. It's got to go through this and say, okay, who are the characters? What are
*  they doing? When are they doing it? What happens? When these things happen, what effects do they have?
*  And it actually tries to build out that whole structure and realize all the entailments of
*  that structure. And it tries to build that and knows where there are gaps. It knows that it can't
*  answer it well or it can't answer it confidently. Or it can't answer in a way that makes logical
*  sense, that's consistent. So it starts interacting with the user to help it.
*  You know, so I think this is what's going on, but I'm having this trouble, that trouble.
*  Can you help me out? Can you confirm it? If I'm not right, what am I missing? Can you help teach
*  me? Almost like you would with your kid. You know, with your child. Like, you know, Charles says,
*  I think this is what's going on, but I don't really know why. Why would people do this?
*  So you, I guess you scoured the literature and put together what you found to be a good
*  template of understanding. And it's not, you know, the actual technology is not that clear to me.
*  I know that it's not all deep learning, of course, and that there's a lot more than
*  deep learning. We do do a lot of deep learning, but I think of it as a hybrid architecture. So
*  we want to squeeze as much juice as we can, if you will, out of the linguistic structure itself.
*  And this is where, you know, encoder decoder language models are very effective.
*  It's interesting because I think the way we ended up modelling this in our implementation,
*  I think, may be very reflective of how humans use kind of these two sides of their brain,
*  because I think humans definitely sort of, it's kind of like condiments think fast,
*  things slow. I mean, I think we're actually language model. I mean, we're statistical
*  language models, but we're also deductive reasoners. We're both. And you could see that
*  in how we behave with language. But I'm sure you know, someone who could string together,
*  you know, beautiful set of words sounds really great. And then when you probe them on the logic,
*  it's completely broken. I mean, that's, that's a great, you know, generative language model right
*  there. Then you also find people who can think carefully and clearly, but can't put the words
*  together. Yeah, you're talking about you're basically describing different sorts of aphasias.
*  Right. So it's, and it's so interesting, because I think there's these different things that go on
*  is the formulation of the model in your head. There's, there's like the experience where you
*  can put together with it, an intuitive model that can get translated into language almost,
*  almost statistically, we are tremendous at that. And then there's the person who puts together,
*  but we also create these intricate logical models of what goes on. And these are almost separate
*  abilities. Of course, they're related. And now you can get someone who's really good at building that
*  rich logical model. And someone's really good at generating the language. And it's called
*  can almost be separate. So you're getting this language that sounds great, you probe it, and
*  there's a very shallow or broken model there, because someone who you it's painful to talk to
*  them, because they're not good at putting it in words. And then you find out, wow, they have a
*  really deep understanding of what's going on. But that was painful. Yeah. But so you guys are using,
*  you know, the, you know, the transformer models and encoder decoder models for language, but
*  is there also a big well of database hard coded information that that constantly gets updated?
*  So there's not so there's not so there's not right, there's a growing well, I would say,
*  because one of the things we're very careful not to do is manually encode symbolic knowledge.
*  So one of the one of the big research thrusts in what we do is try to automatically generate rules.
*  And we were doing a bunch of work in that space, using a combination of curated data, some curated
*  rules in combination with language models, to now generate structured or semi structured rules,
*  that we can now combine or drive into our symbolic reasoning engines, we have a symbolic reasoning
*  engine that does probabilistic abductive inference. And then we have our language models,
*  almost as I was saying, almost very analogous to what we were talking about before, which is
*  a human gets hints from language, they get hints. Look, I read a bunch of stuff, I'm making a
*  hint. Look, I read a bunch of stuff, I'm matching some of the I understand some of the words,
*  like I have some logical interpretation of some of the words, I don't understand others,
*  but they line up. And so now I have an hypothesis. If now if I could just get some help,
*  associating meaning with some of these words, I can stitch a logical model together.
*  And then I can reason over. And that's really that's a really strong analogy for what our
*  system's doing. At that level, is that understanding? Is that I mean, do we have
*  to talk about the definition of understanding? At this point, you know, the natural question arises
*  in me that let's say we let's say you're successful in building a full on thought partner,
*  the way that you envision it. I don't know about you. But in my experience, the more so I used to
*  be pretty interested in chess, you know, and I enjoy playing. But as soon as I see an AI,
*  like when it chess become dominant chess, I'm actually less interested in it, because it seems
*  like less of a challenge, less of a problem. And I know that that's a terrible, you know, it's just,
*  they're just gonna be one by one gonna be knocked knocked down. And I'll end up not interested in
*  anything. You remind you remind me when I was giving it I was giving a talk at high school,
*  and a young lady came in and asked me a question about AI and she and she was wearing a soccer
*  uniform. She just came from a you know, a soccer game and came to my talk. And she said, well,
*  you know, what's gonna motivate us if AI starts doing all these things? And I said, you know,
*  you play soccer. And she said, Yeah. And I said, do you think you're the best soccer player in the
*  world? She goes, No. Will there always be people better than you? Yeah. Will you keep playing? Yes.
*  Why? Because I love it. Oh, there you go. Yeah, of course. Of course, we will always love that's
*  Yeah, that's one. I agree. That's one aspect of it. But yeah, I mean, part of part of loving some
*  things the way that we love some things is is because of the challenge. And there's this sort
*  of mystery, right? That, oh, to be the best, you know, you have to try. But but if we, you know,
*  if we build a thought partner, if that part thought partner has the level of understanding
*  required to help us be creative, to help to guide us to be better thinkers, more rational,
*  you know, at that point, can I can't I just ask the thing to make progress? You know, what,
*  why is it like helping me to be on my project? That's a great, I mean, that's a great and,
*  and that's a great, actually, existential question. I think about that all the time.
*  And I think when it comes down to is, I've been thinking about this for years,
*  this, I think AI really does force us to wrestle with some of you know, if we're willing to think
*  about it enough, forces us to wrestle with some of these existential questions, you know, what,
*  what's our value? What do we do? What is our contribution if machines sort of get smart about
*  these kinds of things? And I have a lot of thoughts on this. This is like a couple hours
*  in and of itself. But let me try to like what it comes down to for me is we exist in a community
*  of other humans, we are designing and defining ourselves in our future, machines can never do
*  that for us. It's just, it has nothing to do with it. It's not about can you solve the problem.
*  It's more can you choose what you want to be? Computer can't choose for us.
*  We have to choose for ourselves. So what do we want? What are our value systems? What do we
*  want to create? What problems do we want to solve? Where do we want to put our energy and why?
*  And these become the more fundamental questions that we have to ask ourselves, which are
*  self defining questions. And when I look at I have a very optimistic view about this,
*  you know, when I look at it, I think, as we define ourselves and our future, our values and what we
*  want to be, we'll make better decisions. If we're not limited by our cognitive capacity,
*  think of how many things when you think about what's going on in the world today,
*  for example, with the enormous amount of noise that's out there, like internet is fantastic.
*  It's great. And it's terrible. It's terrible. Yeah. And fantastic. I agree. If you had the
*  technology, what would you do? So I'm not, I'm not asking like, how would you do it?
*  What would you do? Those are the kinds of questions we should be asking ourselves. We often
*  get stuck in how because solving these problems are really hard. But what would we do if the
*  technology, you know, was there? But what do you envision? Do you envision you're sitting in your,
*  whatever, at your in an armchair with some of your homemade wine, which we don't need to get into,
*  but you're thinking hard, you know, and you have your thought partner and you want the,
*  you know, thought partner to go. Presumably, you're working on a project or you have some
*  direction that you're I'll give you an example. It's like, let's follow through with the internet
*  one. Oh my God, I've read three articles on this, on this thing. They seem to be completely
*  contradictory to me. I don't know what's going on. Can you read 1000 articles on this topic? Oh,
*  yeah. And summarizes for me and compare and contrast. You know, tell me what's likely true
*  and what's likely is it and give me the evidence supporting each thing and organize it so I can
*  make sense out of this stuff. So that you can then do what I make a decision. But at that point,
*  it's a small step to say that the system that you've made is actually much better informed
*  than you are. And I'm not trying to push you on this. I'm just trying to understand what the
*  difference is. So at that point where they bring back all the information to you and you think,
*  okay, now I can make the decision based on what I want to do. Wouldn't why wouldn't you ask the
*  computer what the best decision is at that point? That's not that's not how it works, right?
*  And here's what I mean. Let's take kind of a corporate environment, for example, where I'm
*  given a responsibility and have a bunch of people working for me. Yeah. And I need to get educated.
*  And I need to get educated quickly. I need to know the pros and the cons and everything else.
*  I need to have some deep understanding, second and third order implications.
*  And I get a staff together and I say, please study this for me. And put this stuff together.
*  And in the end, I hear it hopefully is communicated effectively to me. This is no easy job, by the way.
*  And depending on how complex the topic is. And then in the end, I have to be responsible
*  for the decision, not the people who gathered and sorted and understood and organized and
*  summarized the information for me. I have to make a call. And that calls my responsibility.
*  And now I have to consider what are what are what are my company's values? What are my company's
*  goals? How am I liable? You know, how am I responsible for this decision? And what is what
*  is what does it mean for me? And very often, that's about making a conscious and willful choice
*  about how things should happen. And for me, you know, and my company and my team.
*  And you could you could that's that's that's a small example. You could take that at a macro level.
*  Where are humans taking their society? Where are they taking their values? Where are they taking
*  their policies? Or for a family? Same thing. We have to make calls, you know, the experience
*  you probably heard some of my talks I had with my dad, I mean, I have all the doctors and all the
*  nurses and all the residents telling me, you know, here's the information we have, we say pull the
*  plug. Well, he had a he had a do not resuscitate order, correct? Yeah, they asked me to sign a
*  do not resuscitate. Yeah. And I wouldn't do it. Yeah. And and I said, just give me the information
*  and like, you know, you know, you know, you don't you don't understand that, you know, he's brain
*  dead. I said, Look, I get to make the call that I'm making the call. And I want you to give me the
*  information. And I want you to tell me what you know, what's likely to happen, what's likely to
*  happen there. And then I'm going to make the call. And you know, I had my mom was saying, you're
*  going to end up in a situation with the vegetable. My sister was saying, like, you got to trust these
*  people and all these kinds of stuff. And I and I said, Look, in the end, in my mother, father,
*  divorce, even though my father was there, I said, This is my decision. And I have to do something I
*  have to shape my own future here. And and I got to gather the information, I got to make the call.
*  And I have to live with consequences. This particular story has a happy ending, which we
*  should share, by the way. Yes, it did. I I I went, you know, went through that with the doctors,
*  and they were actually quite frustrated that I wouldn't sign it. And they they actually made
*  me make then all the intermediate decisions about what drugs to give him and whether or not to
*  ship him to another hospital. And they would. And so they made so because I wouldn't kind of go with
*  the overall decision, it kind of went down to do we give him this drug or not, it'll have this
*  consequences, that consequences, do we move them from this hospital to that hospital, you know,
*  if this happens, that happens, whatever. And so it ended up going through, going through and making
*  all those decisions in about, I don't know, 18 or 24 hours later, he was sitting up in bed with zero
*  brain damage because they told me he was going to be a vegetable. Zero, not nobody, no brain damage.
*  That's a case that they're transferring, transferring all of the responsibility onto you
*  from this distributed, non essentially non responsible system of a hospital.
*  That's right. But but but they were but they were hopefully, you know, and you know, hopefully,
*  honestly, but but I think so. But they might have been emotionally frustrated. But I mean,
*  they were being honest about, you know, the different the information that they were giving
*  me. So they were acting in some sense, albeit emotional ones, but but but thought partners,
*  you know, OK, well, what if this what if that tell me how this works, tell me how that works.
*  But I had to make the calls. And that's the most human thing you ever have to do is struggle with
*  your value system, struggle with your future, struggle with your relationships and really
*  defining who you who you want to be. Right. So the so values and morality is not going to be
*  part of the thought partner system. It'll inform you that it's in the end, it's all about who's
*  there, you know, who's the responsible party, you know, who's who's making the call and who's in the
*  position to make that that call. And that's that's not analytical. It involves analysis,
*  but is definitional. So when I said to my sister, if you know, if my if he's a vegetable,
*  I'm going to be the guy sitting by his side. That's my that's what I'm going to that's what
*  I'm going to be. I'm defining my future. I'm making that call.
*  One of the things that humans are very bad at is introspecting our own motivations. And, you know,
*  we're bad at estimating probabilities. And we're terrible actually at reasoning. And we actually
*  make decisions based on emotions and then and feelings and then, you know,
*  only post hoc do we rationalize it. Does that matter?
*  I think it does. I mean, I think that's kind of, again, where the story about my
*  dad is very poignant, because, I mean, there's a lot of emotions going on in that in that case,
*  on both sides, of course, sure. I have a tendency on under pressure to get even comes out in my
*  tone, my voice. And as I under pressure, I tend to get extremely precise in both my thinking and
*  my language. The you know, more so than normal, it's just a kind of an interesting thing I
*  observed about myself. You know, the doctors are making calls based on statistics. And that's a
*  reasonable thing to do. I don't get me wrong. It's a very reasonable thing to do in a situation like
*  that. We have an emergency of people coming out. We don't have all the data on everybody.
*  And we have a few features, you know, age, you know, some history, how, you know,
*  cardiac arrest, how long did the you know, did the ambulance take all this kind of stuff.
*  And then there's a probability and it's like 90, 98% sure that, you know, this, you know,
*  your dad's brain dead. And so it's a an average statistical average, if you will,
*  which is very different than forcing yourself to do the very step by step rational decision making,
*  the deductive reasoning. So, you know, I was saying things like, but okay, I get that. But for me,
*  and my dad and my family, this isn't about the probabilities. You know, it happened in a hospital,
*  so not in a hospital, it happened in a restaurant, you know, 200 people in there, whatever, you know,
*  a few of them would have survived given those odds, how do you know he's not one of them? So
*  let's go through this. So so then I go to that, where is the where's the deductive evidence?
*  There's brain dead. Well, his pupils were dilated, the guy said, and I said, Okay,
*  I'm just going down a logical flow chart. Are there other reasons why his pupils might have
*  been dilated might be dilated? And this is like a 15 second pause. Well, we do give him a drug that
*  can dilate his pupils. Yeah, that's crazy. We went into the ambulance. Okay, let's move on.
*  Where's the deductive evidence, right? The actual cause and effect evidence, I'm not going to make
*  a call like this. 98% chance of just not. Right? Because it sounds like a lot to all of us. But,
*  you know, there's like four people in that three or four people in that restaurant who
*  would have survived it. How do you know he wasn't one of them? So in that case, a thought partner,
*  like a machine that you made would not have made the mistake of forgetting about the drug
*  delivered in the ambulance that could lead to the dilated pupils, correct?
*  And that's right. And the list goes on about how to just write the thought what the thought
*  partner would do is say, Look, you know, here's some correlative evidence. And let's try to build
*  the logical flowchart. What is all the evidence? Where does it lead lead us to?
*  Do you have direct evidence cause and effect evidence? No, you don't. You only have some
*  correlative evidence. Here's the probability in that evidence, right? And and understood enough
*  about how to reason in a way through this, you can make a better decision. And to do that sort
*  of calmly and efficiently with no ego. Yeah. Until you build ego in my friend.
*  Yeah. I mean, it's easy to slip into the dystopian, you know, when they when they become conscious and
*  have values and personalities. And so it should we should be clear that this is not what you're
*  describing. No, not not not at all. In fact, like, you know, it's like when you know how these things
*  are programmed. I mean, there's no there's no chance of any consciousness emerging out of this
*  you know, you for its information processing, but it's being done in a more advanced way in a way
*  that can translate it to acquire the knowledge more fluently, more efficiently, I should say,
*  and can deliver it more fluently. And in ways that we can understand and use to make explicit
*  decisions and evidence rationalized decisions. Can you I mean, so this whole thing is based on
*  language, it goes through language. And we've already talked about language being a pointer,
*  and opening up shared understanding. Can you envision a scenario where the rationale for
*  so so you have a thought partner, it comes to you with this information, and it can explain to you
*  the rationale, how it came to that decision, can you envision how it came to that conclusion? Can
*  you envision a situation where language just isn't good enough to convey?
*  So absolutely. In fact, just as as it's a good question, because we talk about language so much,
*  and often I think of language essentially as a means of communicating, we have words,
*  but then we just have, you know, language, which is this, you could have some you could have a
*  pictorial diagrammatic language. Yeah, all kinds of languages. And some depending on the information
*  much more efficient than others. So timeline is a great example. I mean, you know, you know,
*  tell me what happened when and it's like, there could be a lot of events, and they're overlapping
*  and some, you know, and you have partial information about when one started, when ended,
*  and what's during what happened during what and all this kind of stuff. So just get you show me
*  the timeline is to show me the timeline. A map is another great example, enumerating all the spatial
*  relationships between every item in the map. So it's very difficult. But if you ask it a question
*  about the map, it could answer it efficiently. If you want to get a holistic view, you just show
*  the map, you show the map, you don't list out every spatial relationship, you don't list out
*  every temporal relationship in language. And these are just a couple of examples. But when you have
*  relationships between different variables, and you know, you draw a graph, I mean, so we know,
*  as humans, the different ways that are effective in communicating different kinds of knowledge.
*  And I think a system like this ultimately has to make use of that, because it's about efficient
*  and effective communication that reflects the shared understanding, it isn't necessarily about
*  words. The words come in are important in a couple of ways. A lot of our knowledge, our,
*  the way we think, and the way we, the way we communicate, and the way we acquire
*  knowledge is through language. So language is just the medium through which we express so much
*  of what we think and we understand. And so it's a way to get information into the system,
*  if it could read, and it could read effectively, it could acquire information quicker. If it could
*  interact with humans and extract information from humans in ways that they're fluent in, and are
*  happy to do, then we can more efficiently acquire information. That's one reason language is important
*  is on the input. And the other way language is important is in fact on the output, answering
*  questions, you know, through language, and dialoguing through language. But that certainly doesn't
*  limit the other forms in which you might try to communicate. How far along are we to realizing
*  your thought partner goal? Or how I should say, how far along are you? It's hard. I mean, we've
*  made a lot of progress unhappy with our progress, but it's clearly a challenging, it's clearly a
*  challenging problem. We're at a point where we have an architecture. We talked about earlier,
*  a hybrid architecture that both, you know, has dialogue, it, you know, exploits these powerful
*  language models. It has the usual NLP stack of syntactic and semantic parsing. It has a special
*  kind of reasoning engine we developed for reasoning about language understanding.
*  These are, you know, and we're assembling this and putting this together and starting to see how it
*  works. But there's a long road. We don't even know what we don't know yet. I mean, the different
*  components that we put together are probably, and we're experimenting with this now, useful for other
*  things, just improving general NLP tasks, improving knowledge acquisition tasks, improving the way we
*  train and teach computers. So we're, we're optimistic that a lot of the components that we've been
*  working on to assemble the system are sort of independently useful and can have business impact.
*  But the thought partnership vision is, it's a ways off. It's a challenging, challenging thing.
*  And I think one of the things you have to always keep in mind to set your expectations clearly
*  is how difficult it is for humans themselves, I think, to communicate effectively.
*  I don't know. I don't know who you're talking to, buddy. But, uh, yeah, no, I know I can't,
*  I can't barely string together coherent thoughts myself. So, so there's a long way to go. Is that,
*  is that what you want on your tombstone? Or do you want Watson on your tombstone or both?
*  No, I mean, I think that what I think that you're pushing me on this, but it's good. I think that,
*  when we get there, when we get there in my lifetime, my hope is that we will make very
*  significant progress in the next, I would say, you know, five to six years, assuming, you know,
*  we continue to get the sort of investment we need, where people will look at the work that we've done
*  in elemental cognition and say, this will, this approach is the one, this will get us there.
*  And we start to see it have impact on business and other, and science and other technical
*  directions around this. And, and, and we'll see that, you know, this, this is a promising approach.
*  I really would love to start, you know, we do a lot of work with reading comprehension.
*  I think in, in, in less time, we will start to see tools that will interact with, with,
*  with children, you know, you know, to do reading comprehension to both help them
*  think critically, understand and read while learning, you know, from the child, learning
*  how to dialogue, learning the, the essential background knowledge necessary, building that
*  knowledge base while helping them read, helping them think, helping them develop that basic
*  understanding. And so I, that's a really fun and exciting project for us, because, you know,
*  we're adding value in a fun way in the educational space, while, while exercising and developing this,
*  this, this system called this natural, natural learning system, that's learning how to read,
*  you know, more effectively, and then, and then using that reading to learn how to grow its
*  understanding over time. But I don't know any other way, I don't know how not to work on this. I mean,
*  there's no way there's a, there's a future, there's no way there's a future where humans,
*  the machines aren't doing this. This is just an unacceptable future to me.
*  Right, right. That's a, although that's a pretty good tombstone to have David
*  Ferrucci taught your child to read. That's not bad. You know,
*  Yeah. And, and, and learn to read on its own, right? That's, that's the,
*  we'll get there. I think we're gonna get there.
*  Yeah. I'm just curious from the AI perspective, you know, if there's anything that you would
*  like to see coming out of neuroscience that you think would actually be a benefit to you,
*  because you I know that you do pay attention and, and to cognitive processes and the literature,
*  but it's not your main focus when you're developing these things, if I'm not mistaken.
*  Yeah, it's not our main focus. We don't, we, so we think about, we think about how people learn.
*  We think about how they think, but at a more abstract level, we sort of care less about
*  how the brain literally does it. So is it, is it fair to say you care
*  more about the psychology level of understanding it?
*  Um, maybe, you know, it depends what you mean by that exactly.
*  You're becoming really precise. I better watch out.
*  You know, I'm being careful because I'm not sure exactly what you mean by that.
*  But, you know, it more abstractly, how, how do they, you know, how do they, so if you think about
*  the child reads, what, what types of things did they struggle with understanding and why,
*  what things are easier than to understand? What models do they build in their heads?
*  Sort of what questions are they able to answer? When, you know, when you do that now, of course,
*  that is related certainly to how the brain works and, um, and how it organizes information and
*  the power of some of the pattern matching and the inductive processes that I could only imagine go
*  on there and then the deductive processes. But in the end, I, I, right now I have to work
*  with Silicon, right? I have to work with a certain architecture and I have to make sure that,
*  which is sort of an interesting point, I have to make sure that in fact,
*  so I don't want to create a human brain, right? I want, I want to create an intelligence that
*  we can hold to a higher standard with regard to its rational powers.
*  So it strikes a different balance between, you know, I love the, the kind of economy,
*  think fast, think slow. Like it's, it's like, our brains are very powerful inductive machines
*  and induction is sort of by nature prejudice. It's by nature, you know, I'm going to, I'm going to
*  spit back with my particular experience and the data has taught me, we, we often choose to rise
*  above that and say, you know, it doesn't matter what the patterns I experienced were, I need to
*  deal with this individual right in front of me. Very similar to my father's case. We have these,
*  we have this average statistic we've observed, but now I have to deal with the particulars of
*  the person in front of me and make the right call. Now I have to get into the kind of slower
*  rational reasoning, reasoning approach, the inductive, you know, essentially prejudicial,
*  fundamentally biased to the data that you experience is faster.
*  And it's very effective as well. It can be very effective.
*  It can be very effective in the general case. And, and so we've evolved that way and it's a
*  little bit harder for us to do the more formal thinking. Of course, the more formal thinking
*  is the foundation for science. It's the very foundation of it is to lay out all the evidence
*  and produce a deductive argument and presentation and communication around that and proof, if you
*  will. Although that always starts with a guess. So I think you're back, right? You know, that's
*  awesome because you're right back to kind of how we think about, you know, EC's elemental
*  cognitions architecture is we, we too will start with a guess. But then we will fit it into a
*  framework. And so that's exactly right. And so you have those two sides. So
*  studying cognition at that level gives us some confirmation, but it's that, oh, that makes,
*  you know, that makes sense. So because the, because the human model is a great model,
*  it doesn't quite in terms of, in terms of functionally, not, not how it does it, but what it does.
*  It's a great model because we're effective, but I would, with, with our system, you know,
*  I would strike a different balance in this sense that hold it to a higher standard for
*  rational thinking. So I think that's where we often fall. We, we, while it's fast
*  and got us to survive, it's not always the best, especially when we get into making
*  a decisions, justifying and evidencing those decisions, advancing science,
*  then we really want that combination and we want to figure out how to use those two things
*  effectively. So it's in that way that we think about, we think about cognition.
*  It's not in the wet wear, you know, how would the neurons do it?
*  Would you, so this is a terrible question I realized, but if it were
*  decreed and it was absolutely certain that we could not, let's say, make AGI in silicon,
*  that it actually took brain tissue, would, would you still work toward developing a thought partner?
*  Yeah, so that's a great question too, because I think that's a different kind of intelligence
*  so that, that you would get to, but it's, anyway, so there's a couple of ways to kind of think about
*  that question. One is, I think if you think about artificial general intelligence, or if you think
*  human, I want to create a human, or at least I want to create an artificial intelligence that has
*  an easier time understanding the world the way humans do, there's definitely an issue, there's
*  definitely a question around that and we've thought about that, which is, you know, our brain generates
*  a lot of internal features. There are not a variable externally, this is, we talked about
*  this before, this is why language is actually secondary. What primary is, is that we've built
*  very similar models and then we've learned to use language to kind of communicate and orient
*  ourselves around that so that we can, we can interoperate together and we can communicate
*  effectively with each other, but one of the reasons we can do that is because we both have sort of
*  similar brains, we generate a lot of the same internal features, whether they're psychological
*  or emotional, or we generate very similar patterns in response to the same input reasoning patterns
*  as well as sort of emotional, you know, pattern, emotional responses. These features are not
*  external features, these are generated by our wetware, by our brains as machines generate these
*  internal features and they're compatible because your brain generates them, my brain generates them.
*  This is a big part of understanding, is figuring that out, so, or generating those similar things.
*  So now that you ask yourself the question, now well gee, how do you, it's going to be really hard
*  to get at some of those things if you don't actually have the wetware, the machine that generates
*  those things internally. It's a good question, I don't know the answer to that. In some sense,
*  I'd like to imagine that you can design a machine that is almost, actually it's better off to be
*  distant from those internal features. At the same time, there may be, it may be very hard to
*  develop a shared understanding without them. And sometimes I think, rather than any, we do some,
*  we do obviously statistical language stuff and then we do symbolic reasoning when we combine these
*  things, but if you wanted to create an AGI that was strictly a neural driven AGI, you'd probably
*  need to generate all those internal features, right? You would probably, if you really wanted
*  to get AGI vis-a-vis that, I think it's actually hard to do without bringing in the rest of the
*  machine, you know, the rest of the, what I mean by the machine in this case is the human,
*  bringing in all the other pieces, all those internal features and those perceptions and,
*  you know, and then you're off on this really weird path where now are you just kind of creating
*  another human? Do we really want another human?
*  Too many, there are too many already, damn it.
*  Well, I mean, you know, it's, anyway, those are all interesting questions. Like I don't,
*  yeah, I don't dismiss that stuff. I think that's very interesting. And I think there are
*  legitimate and important questions there when we think about AI more broadly. It's not the particular
*  approach I'm taking, but it's, I recognize that it's a significant part of the overall study.
*  Has your conception of intelligence changed over the years or have you had pretty much the same
*  directional arrow? Oh, it's certainly changed. I think that probably, as I understood more and
*  more about machine learning and how that works, it's definitely changed. I think that
*  it's not as much, it's a hard thing to say, but it's not as much of a mystery in the following sense.
*  I think you sort of imagine how it arose,
*  you know, like the basic fundamental architecture has never, I'm not saying it's
*  anywhere near entirely clear, but it's never been clearer. And in some ways, it's been
*  diminished a bit. What, I'm sorry, what is the it? The notion of intelligence is a little bit less,
*  a little bit less mysterious. Yeah. A little bit less, what do I want to look for? What's the word
*  I'm looking for? Intractable? Yeah, yes, also, but there's another word I'm looking for, which is,
*  I think we had this, we looked at it as just incredible. The reverence that we've had for it.
*  The tremendous respect, this mysterious, incredibly powerful thing that we can never wrap our own
*  minds around, intractable, impenetrable, just this incredible force of nature.
*  Whereas as now, maybe it's actually not that big of a thing. And really, the more interesting
*  thing is going to become struggling with the concept of consciousness. And what is that? And
*  what does that really mean to us? Like if, if we break down intelligence and understand it,
*  where does that leave us? Existentially? Yeah. Well, it's now we're just getting started, man. So
*  the other way to look at that is maybe we've, like the Dunning-Kruger effect in psychology is when
*  you understand just a little bit, then all of a sudden you have a bunch of hubris about it and
*  you think you feel overconfident that you actually understand what's going on. And so there is a risk
*  that we're actually, with just with respect to understanding intelligence, that that's where we
*  are on the curve right now is thinking that we actually do understand it. Yeah, we know enough
*  to be dangerous. Yeah, I agree with that. I mean, I see that as a risk, but I think it's
*  I still think it's been tremendously enlightening. The last 20 years have been enormously
*  enlightening and certainly turned, you know, certainly helped us a little bit. But you're
*  right. There's so much we don't know. And I think when you look at a lot of the work that's going
*  on, you know, it's it's overhyped in some sense. But but at the same time, there seems to be
*  some light, we crack the surface a little bit, the sunlight shining through. And of course,
*  our minds go crazy, wet and start thinking, did we figure this out? And if we did, like,
*  we're going to crack the rest of this, then where does that leave us? Because so much of our so much
*  of our ego and so much of our self worth, right comes from our intelligence. Speaking of let's
*  just let's wrap up here. I know that I've kept you long enough. But I really wanted to ask you
*  about the difference between academia and industry and whether you know, you know, is there something
*  that you wish like academics would be better trained on when they enter the tech industry
*  world? Should I even send my kids to college? Wow, that's that's Yeah, today, that's really
*  an interesting question. You know, sometimes even when I interview people.
*  Well, when you interview someone, is it terribly obvious that they're either from the academic
*  or from the industry world? You know, if I didn't, if I didn't know, can I tell? Yeah, sometimes. Yes,
*  actually, sometimes. I mean, I think people can come from the academic world to have a more
*  practical view of what they're doing and why they're doing it. Yeah. And some are very,
*  very driven. They're there. It's really more of an incentive. It's really more of an incentive.
*  You see it in their incentives. You know, they're very focused on, you know, what is what is it going
*  to take to get a paper accepted, as opposed to as opposed to build something that works? Yeah,
*  yeah, they're different things. They're, they are very different things. And sometimes people
*  are more strongly academic minded, don't even think about the difference. They're just strictly
*  focused on, you know, the one incentive of how do I get the paper published, getting things to
*  work outside of some some simple or narrow examples are often a lot more difficult than you have to
*  be more general about it. And, and I look for people who, when I hire anyway, I look for people
*  who actually come to me with that frustration. You know, with the idea that look, I mean, I've
*  been working, you know, if I have an academic working a lot in research, and they come to me
*  exactly with that frustration, I immediately my ears perk up, I'm immediately interested.
*  Because it means you know, you have a smart person working academia that has done some good work.
*  But they they, they realize that it's been narrow, the incentive structure, you know, causes that,
*  and they want to really put it to the test, and build systems that are more general and more
*  stable and actually solve problems. And so I always interested in that, because I always look
*  for that mix. And I did that on the Watson project, I did that I always look for that mix,
*  how do you get the right mix and sort of engineering problem solving, practical, you know,
*  and then but also sort of deeply knowledgeable. And that's, of course, the academics often are
*  deeply knowledgeable about their field. So it's always interesting to be able to get that to be
*  able to get that mix. Is there something grittier about someone in industry versus academia? I mean,
*  I'm not trying to draw a hard line, because there isn't one, you know, but I'm really just
*  wondering, you know, is academia obsolete? No, no, not at all. I mean, I think there are just
*  different aspects. I mean, I think that, well, first of all, look, there's one change I've seen,
*  at least, which is, and I and it's interesting, because I've interviewed people come in with a
*  tremendous academic pedigree, which is, you know, at one time, all you had to have. Yeah. And then
*  and then and and now if you give me someone with a great academic pedigree, and then and then that
*  goes up in someone else who comes in and says, I built this, here's the robot I built. Yeah.
*  You know, here's why and how and, and I got this thing to work. And here's how I did it. And they
*  represent their knowledge and adaptive their understanding through building an actual thing.
*  Yeah, I mean, that went hands down. Sure. Yeah. For me, for me, I don't really care what school
*  they went to anymore. And there are some people who are so motivated, they they and so good that
*  they they come in with that. And they they have the whole balance, right? They have the knowledge,
*  they have the skill, they have they have the practical sort of hands on problem solving
*  perspective. You know, those are really powerful thing. And then all of a sudden, you know, the
*  pedigree just doesn't matter that much. At the same time, you know, people who have spent good
*  time in academia, can come in with a tremendous wealth of knowledge is absolutely critical
*  to what you need to solve a problem. So it's really sort of how do you how do you create the
*  right diverse set of skills and talents and perspectives to be successful? It's always tricky.
*  Yeah. Well, Dave, so people can go learn more about what you're doing at elemental cognition.com. And
*  of course, a link to the paper that we talked about in the show notes. My last question,
*  you're not on Twitter, are you? Is that part of the the noise to be avoided?
*  I have to I don't do a lot of tweeting. Look, I'm sort of a I'm sort of a heads down, you know,
*  kind of person. And elemental cognition is sort of a heads down kind of company. I mean, we,
*  we feel good when we make progress solving the problem. And I think we get a point,
*  we have something we really want to share or say, you know, we'll do we'll do that. So we're not,
*  we're not driven by like, keeping the buzz going, you know, at the same time, you know, we we want
*  people to know about what we do want people to know about our mission. But it's not the it's,
*  you know, it's not the big driving force. Yeah, I think it's probably wise on your part to lay
*  off the Twitter, not the not that we need to perseverate on that. But anyway, I really
*  appreciate you spending so much time with me. And I wish you luck moving forward. And I can't wait
*  to see the next steps in the thought partner trajectory. Thank you. It was a real pleasure.
*  I really enjoyed the conversation. You're very easy to talk to. It was fun.
*  Brain inspired is a production of me and you. You can support the show through Patreon for a
*  microscopic two or $4 per month. Go to brain inspired.co and find the red Patreon button
*  there. Your contribution will help sustain and improve the show and prohibit any annoying
*  advertisements like you hear on other shows. To get in touch with me, email Paul at brain inspired
*  co the music you hear is by the new year. Find them at the new year.net. Thanks for your support.
*  See you next time.
