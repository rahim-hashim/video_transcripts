---
Date Generated: April 21, 2024
Transcription Model: whisper medium 20231117
Length: 2941s
Video Keywords: ['Science', 'Technology', 'Education']
Video Views: 6093
Video Rating: None
---

# BI 067 Paul Cisek: Backward Through The Brain
**Brain Inspired:** [April 18, 2020](https://www.youtube.com/watch?v=L8I9jEUsu3o)
*  I think the problem that the new AI faces is that just like the old AI, it's still
*  completely missing the semantics.
*  One thing I have to say is it's fun.
*  I got to tell you, it's so much fun.
*  Like, do you remember when you were first a graduate student and you just walked into
*  this huge, incredible maze of fascinating facts, right?
*  Well, once you become a PI, you sort of get sort of, you do your field, you read some
*  paper but you know exactly what they're going to say, you know.
*  This was like being a graduate student again.
*  This is Brain Inspired.
*  Hey, it's Paul.
*  Welcome to the show.
*  This is the second part of my conversation with neuroscientist Paul Chisik.
*  We continue right where we left off from the last episode.
*  So you need to listen to that episode to get the full context for this one.
*  As a very course recap though, Paul likes to approach understanding brains by going
*  forward through evolution, that is using a phylogenetic refinement approach to trace
*  out the incremental steps evolution has taken to shape our brains and behavior.
*  And he also likes to go backward through the brain, focusing on how we control ourselves
*  and the environment by taking actions through feedback mechanisms.
*  Today we start by talking about how AI figures in all of this, framed by his recent debate
*  with Blake Richards, about whether we should use normative approaches to study brains.
*  And we go on from there.
*  Hyeongjoon, Mike, Dan, and Mikkel, my gratitude to you guys for your support on Patreon.
*  I am putting the finishing touches on the first installment of the Brain Inspired course.
*  I promise it's happening.
*  Anyway, for you Patreon supporters, you'll soon get an email on how to access the course.
*  And I'll announce on the podcast when it becomes available to everyone else as well.
*  I have plans in the work also to start occasionally posting little episodes just for Patreon supporters.
*  These Patreon only episodes will likely be a little shorter and lean more toward the
*  cultural side of science, with a little more time devoted to advice and lessons people
*  have learned and so on, but also with plenty of science and big questions.
*  So go to BrainInspired.co to learn how to support the show through Patreon.
*  It's super cheap for you and it helps me more than you know.
*  Also find today's show notes there with links to a lot of the excellent books that Paul
*  recommends and his own work on the topics that we talk about.
*  Be well out there and keep pushing onward everyone in science and in life.
*  Enjoy the show.
*  I actually had invited you on the show before I knew and you informed me that you were going
*  to be doing this big debate with Blake Richards at the Neuromatch Unconference or whatever
*  it was called.
*  It was framed as a debate.
*  And in fact, I enjoyed the back and forth.
*  I haven't seen a quote unquote debate that has, I think it needs to be, they need to
*  stop framing them as debates because no one treats it like that and you know people want
*  to see blood.
*  Well you know the thing is, so the thing is, I think you know a lot of people were all
*  excited about seeing blood, but it's a little difficult to try to have a debate in public
*  with someone you like, you know, and really go after the jugular.
*  It's a little embarrassing.
*  I think I don't, neither of us really wanted to be too brutal.
*  And I don't think we're really in so much in opposition.
*  It's more of a, I feel it's fine for people to take that approach.
*  It's not what I'm going to do because I don't think it's promising.
*  But I wouldn't want to discourage multiple approaches.
*  Now you know, by the way, Blake and I, we don't know each other for that long, but our
*  kids are friends in school.
*  In fact, I was just told that they were playing together, well not within two meters, but
*  they were playing together, they saw each other at the park just now.
*  Yeah.
*  So no one got like a bloody lip or anything after this.
*  Nobody got a bloody lip.
*  It's kind of disconcerting to see that as we were talking, there was this chat box going
*  through with people going, we want to see blood.
*  We want to see them.
*  Both participants dead by the end.
*  Wow.
*  I know people love their game of Thrones or whatever.
*  I don't know what the new things are.
*  But even like the chat, like why would you, I don't know, I have opinions about having
*  a chat on during a talk because it just means you're not paying attention.
*  It was distracting.
*  If you're chatting.
*  Anyway, but so I enjoyed the back and forth and yeah, these debates, they, it's a false
*  dichotomy.
*  They pit a false dichotomy often where the people debate, quote unquote, debating or
*  trying to go pro versus, you know, bro and con on an idea.
*  They really end up overlapping in their thought processes a lot.
*  I don't know.
*  So there's, there's gotta be a better way to frame it than debate.
*  Well, the thing, the thing with, with the, the AI approach is I am actually quite against
*  it.
*  I mean, certainly I myself don't think it's, it's promising.
*  I don't think normative explanations are, are really the way to go because we have no
*  idea what that objective function should be.
*  And I think it's probably impossible to come up with it in a way that will lead us better
*  than good old hypothesis driven experimentation.
*  Let's just back up and say what the normative approach is.
*  So you mentioned objective function.
*  Well, the idea that we try to understand the brain in terms of how it optimizes a particular
*  problem that's formally defined with some kind of metric that tells you how well you're
*  doing on that metric, presumably defines a kind of a space of systems where you can say
*  this one is better than, than this one.
*  You don't, as Blake pointed out, you don't expect to find the global optimum.
*  You just expect to find a way of trying to understand the system in terms of how it is,
*  especially in particular learning in the system of how it's actually trying to at least improve
*  its performance according to that metric.
*  My critique of it is that I just don't, it's a little bit like the problem of definitions.
*  Now, instead of having defined a thing that we will now explain, now we start with an
*  objective functions that we will then try to optimize or relate to neural activity or
*  changes in learning or something.
*  And again, I think that if you start with it, your chances of getting it right are not
*  so good.
*  So I think that, again, I wouldn't say that people shouldn't do it.
*  And of course, people will use that approach.
*  Learning is very exciting.
*  But I think, unfortunately, like cognitive psych, cognitive revolution, much of it is
*  asking questions that I don't think I myself, in my opinion, don't think are good questions.
*  And it's again, because they start, you know, things like how do you encode knowledge?
*  Right. And I think that's not where, again, if, you know, an evolutionary viewpoint will
*  suggest those are not problems that will really, you know, they're not the fundamental
*  problems that subdivide and lay the foundation of the brain.
*  One thing that you must appreciate some and that has been celebrated in large, I think,
*  by the neuroscience community that AI does well is they, so yes, they define the task
*  to be performed.
*  Let's say it's object recognition or something.
*  But then, you know, they don't constrain their network.
*  So then the network performs the task.
*  And then you can look back and see how, you know, so there are some constraints in how
*  you set it up and the objective function and the architecture.
*  Then you can look back. And it turns out that a deep learning network that was constrained,
*  you know, by early simple and complex cells from a yubel and weasel and through the neocognitron,
*  but without a lot of other constraints that were being added into networks.
*  If you train it on a task, it actually, the neural representations.
*  Yeah, yeah.
*  Look like, yeah, yeah.
*  And so that's fantastic.
*  Yeah, it's fantastic.
*  But so you must appreciate that aspect of it.
*  Yeah.
*  Well, yeah, because I think especially in those cases, the function was probably sensibly
*  defined. The idea that you want to categorize images in particular ways is, you know, I
*  maybe critique the foundations of putting everything in terms of knowledge building
*  and representation.
*  But of course you have to categorize, if only because some things are things you want to
*  approach, some things are things you want to avoid, some things you don't care about.
*  Right. And so categorization is a pretty good etiologically relevant problem that the brain
*  is probably solving.
*  And networks, you know, highly redundant and distributed systems will come up with a
*  mechanism for doing that categorization, which doesn't necessarily have to have human
*  interpretable elements.
*  So I think that's a good example.
*  The bigger problem, I think, is that just because that does work for some problems can't
*  be assumed to be a framework for neuroscience as a whole.
*  And in particular, I think the problem that the new AI faces is that just like the old
*  AI, it's still completely missing the semantics.
*  Like I was saying before, the old AI and sort of cognitive theories and old AI systems
*  always had this problem of lacking semantics.
*  Again, simple grounding problem.
*  Yeah. Chinese rules.
*  And the reason is because the problem was defined from the outside.
*  The problem was the system was designed to solve the problem defined by an external agent.
*  And it's solved essentially the syntactic aspect of the problem.
*  Right. And that's what current neural networks are doing.
*  You know, whether it's supervisor and supervised learning, the problem and the metric and the
*  objective functions defined externally to the to the algorithm.
*  That's the it's the the designer or the researcher, the programmer that defines what the
*  problem is. And then the thing has to solve it.
*  Right. So just like good old fashioned AI, it doesn't have the internal semantics.
*  It doesn't know what the meaning of the symbols is or the vectors or intermediate tuning
*  functions or whatever you want to call them.
*  So and it's again, it's because the problem was you've sort of taken a chunk of behavior,
*  defined it to be a a explicit chunk that, let's say, represents objects in the world.
*  And then you've placed it on its own.
*  Right. Now, what I what I'm saying is that suppose you think about it in terms of a control
*  system. Right. So suppose that you have an animal and it needs to run around the world and
*  find food, avoid threats, not bump into things.
*  Should we say a robot instead of animal?
*  Yeah, let's say robot. Right.
*  So now, well, it needs to have categories.
*  But what are the categories that it really needs now?
*  It doesn't need categories for objects so much as categories for policies for interacting
*  with the objects. These are the things you avoid.
*  These are the things you approach. Right.
*  Those those are more fundamental categories.
*  Right. And, you know, it's like, all right, so you need to have a system that perceives
*  the affordances in your world and a system that prioritizes based on your current needs,
*  which are the ones that are that you're going to now take advantage of or not.
*  Right. And so the categories are sort of action flavored.
*  Again, if you define it that way, there's never really a meaning problem because the
*  system does have goals that there is kind of needs that are being met.
*  There's something at stake. Yeah.
*  Something at stake. Right.
*  And and whatever you discover about your world, you discover it because it actually does
*  have meaning to your interaction that then helps your state.
*  For example, and it's interesting that when you look at the ventral stream, right, I
*  always thought of the ventral stream as ending in the intro temporal cortex where you have
*  these sort of high level representations.
*  But then there's a couple of papers that I read that made me rethink that.
*  Wait a minute. One was a paper by Ben Hayden and a student.
*  You. You.
*  Yeah. I forget his first name.
*  And they wrote a paper that's that's saying, you know, that's what the orbital frontal
*  cortex is really doing is is using the same approach as has been used to explain the
*  ventral stream in terms of a kind of a categorization system.
*  But it's categorizing sort of value based things.
*  It's like things that are good to approach or things that are less good to approach,
*  et cetera. And it's actually using the same algorithm, this entangling algorithm that
*  people have used and used to interpret ventral stream processing.
*  And I thought that makes really good sense.
*  But the other thing that caught my eye is that at the same time, I read a paper from
*  Barbara Finley and Uchiyama, who describe evolution of the neocortex as essentially
*  the elaboration of two separate sheets, one which is spatial and one which is non-spatial.
*  And it so turns out that the temporal cortex and the orbital frontal cortex are part of
*  that non non-spatial system.
*  It's essentially a continuous thing.
*  It just so happens that in primates, the temporal ring really bulged out so much that it
*  bent forward and created this this slope.
*  And you get it atomically speaking, it's these two streams.
*  And of course, that fits very well with my my thinking about affordance competition,
*  et cetera, because it's all about action selection.
*  So so you can in that context, it makes good sense to have these category sort of clustering
*  algorithm like things that that process the images in a way that categorizes them into
*  into action specific things.
*  But, you know, you can think about the evolution of this thing in terms of key stimuli.
*  So the old theological concepts of certain combinations of cues in the world that tell
*  you this is good, this is bad.
*  And it doesn't have to be an object as so much as things that are pointy are bad or
*  things that are, you know, big and moving fast, big and expanding bad.
*  Right. So those kinds of categories are quite fundamental.
*  So in that context, it makes good sense to have some of these types of representations.
*  Right. But if you if you start with that and build and try to build.
*  So anyway, so within the control context and the idea that you need categorization for
*  action selection because you can't do multiple things at once, it makes perfect sense to
*  have these kinds of feature extraction type mechanisms.
*  And I think one could come up with a relatively solid sort of evolutionary story of how that
*  might have been elaborated over over time.
*  And then look at certain species and see what what are their equivalent visual systems.
*  Right. But the thing is, but again, it's it's within that context.
*  Once it's within that context, no problem.
*  Right. But if it's just about building knowledge, then it's going to be hard to you know, it's
*  not about, you know, why are you recognizing a pattern this way?
*  Why are you categorizing the world this way?
*  If you don't answer that question, then I don't think that I think you're going to get
*  the same trap that good old fashioned AI did that the semantics is never in there.
*  That's because it's because we desperately need to caption images, don't you know?
*  Well, yeah, yeah. But the thing is, it's great for that, right?
*  So for the for the for the engineering aspect, it's great.
*  And that's why there's a lot of, you know, resources poured into it.
*  But we shouldn't necessarily just because it's so good for a problem that we've defined,
*  we shouldn't necessarily assume that that's the right way to break up the brain.
*  This is where we begin the problem.
*  Now, let me say one other thing about that is that reinforcement learning work is actually
*  doing very much like what I would what I would say is the next question.
*  So, yeah, yeah, it's kind of like, yeah, a lot of the things I say, well, hey, well,
*  reinforcement learning, a lot of that work is about kind of solving the control problem.
*  Right. And the objective function there is survive.
*  Right. Which which. Yeah, that's a pretty good one.
*  And then the question is, find ways of dealing with your world in such a way to make sure
*  you don't not survive. Right.
*  And so I think the reinforcement learning work is actually quite promising because it
*  does address the problems of semantics.
*  And and the way I see it is that the semantics has to come from within.
*  Right. The semantics of the representation can only exist once that representation is
*  part of a control system. Then it's trivial.
*  Right. It has meaning because it does something for you.
*  If you if you don't have it inside a control system, then then it's hard.
*  Then it can't have been right.
*  So I think the reinforcement learning work does actually do that.
*  And I think the reinforcement learning work is helping us.
*  And it is actually sort of leading towards strong AI, where you could actually have
*  real, real AI, because it doesn't let go of the semantics along the way.
*  It keeps the semantics in the system and and then just elaborates the system.
*  But always with the semantics.
*  Would it be fair to say that a let's say a non-reinforcement learning system, its
*  meaning is only it's not within itself, but it's only within us because we're the
*  users and we're the kind of kind of.
*  Yeah. Yeah. Like, you know, a word processor is is very useful for us, but it's not an
*  AI system. Right.
*  And it's just like a image labeling system.
*  Right. It has no use for itself.
*  You know, even AlphaGo didn't really have the use of maybe winning.
*  Although I guess AlphaGo did was essentially trained still with a with a with a
*  reinforcement learning approach.
*  It was. So in a sense, whatever representations it had did have meaning in the
*  context of the goal.
*  There was still a state that was being controlled.
*  There can be meaning without subjective awareness of that meaning, I suppose.
*  And I don't need to we don't need to go down that road.
*  But yeah, yeah, it's not going down that road.
*  But yeah, no.
*  But I think the problem is that if you take a chunk of behavior outside of the control
*  loop, you risk not having not not having the semantics that the semantics, I think,
*  come from the control control aspect.
*  And I'm not sure you could do it without that.
*  Maybe you can, but I don't think people are doing that.
*  So that's why I think a lot of a lot of the AI excitement is likely to lead to another
*  big bursting bubble when we have better, better image classification algorithms and
*  maybe more robust robots.
*  But but, you know, I want to I want a robot butler.
*  I want to write.
*  I want to be able to have a conversation.
*  Yeah, I didn't appreciate that.
*  AI winter referred more to the economic aspect of everything than the actual progress
*  being made, which I, you know, and I don't know that there's going to be another AI
*  winter in that sense, because, you know, there are very useful technological industrial
*  applications.
*  Yeah, I don't think people are not going to be able to get jobs.
*  I do think there's going to be a bit of a backlash to the hype.
*  But, you know, it's interesting because I looked into once before giving a talk on
*  some of this stuff, I looked into where the hype is coming from.
*  The hype is not coming from the people doing the AI.
*  Right.
*  They know better.
*  They know better than to promise, you know, some of those promises that were made in
*  the 60s, you know, nobody's doing that.
*  It's more the media and the industry that's that's hyping it up.
*  Yeah.
*  OK, just to stick with reinforcement learning for one more notion here.
*  So just going back to the first, you know, enclosed membrane, self-sustaining organism,
*  you know, could you consider reinforcement learning the original objective function in
*  that sense?
*  Well, no, I think survival would be the original objective function and just metabolic
*  state, etc.
*  You mean like an objective function for learning?
*  Well, no, like so.
*  I'm sorry.
*  So objective function for seeking for going up the food gradient, up the solution
*  gradient to find more food because it's depleted its resources.
*  It's consumed all of the nutrients in its region and has to move.
*  Right.
*  So then there's one that developed some sort of motor ability through random chance
*  through evolution and thus can randomly move into a new environment.
*  And could we consider that an objective function?
*  Well, I don't know.
*  I don't know if it would help us to call it an objective function.
*  I mean, I think what you described already, you know, so I think there's an interesting
*  link here with dopamine, you know, this idea that dopamine state tells you to stay put
*  versus go somewhere else in across all kinds of animals, including jellyfish, etc.
*  Right.
*  The idea that if that was one fundamental role of dopamine, then within a system that
*  keeps you local, it would be helpful to have on top of the general tonic level of things
*  are pretty good, so stay local.
*  It might be useful to have, hey, just now was really extra good.
*  Right.
*  And if the animal detects that and reinforces somehow what it just did in the present
*  context, it's reinforcement learning.
*  Right.
*  It's now not just intake rate.
*  It's specifically little punctate bits of things are extra good now or not as good as
*  the background rate.
*  Right.
*  And then that brings in the basic dopamine aspect, which has been associated with this
*  kind of reward difference in expected versus actual rewards.
*  And so maybe that's a connection as to why within that system that deals with local
*  behavior, there's this kind of associative learning through reinforcement, you know,
*  where you reinforce certain action actions in a certain context because they do have
*  a little extra burst, little punctate burst of goodness, you know, from signal by dopamine.
*  I mean, you know, it's possible.
*  It makes some predictions as to where things should, you know, how dopamine receptors
*  maybe should be distributed in the brain.
*  Yeah.
*  And, you know, there's algorithms, reinforcement learning algorithms.
*  And, you know, it did.
*  Reinforcement learning, I guess I don't know exactly where it's been proven to occur,
*  but certainly by the earliest vertebrates we have it.
*  And the circuits have been retained for essentially until now.
*  The dopaminergic reinforcement learning?
*  The dopaminergic system, the basal ganglia and the ventral tigmental area.
*  These things are, these structures exist in lamprey.
*  Homologous structures exist with the properties that one would expect.
*  They've been, the circuit has been fairly well established.
*  And, you know, so that's, unless they happen to evolve the same circuit as we did, it was
*  there for half a billion years, you know, and it's a very fundamental thing.
*  And also there's good reason to believe that that was one of the reasons that our
*  ancestors did make it through those turbulent times because they were able to do these,
*  learn a little bit about their environment.
*  There's a lot of elaboration.
*  Yeah.
*  But so you don't, you don't see the need to posit 37 objective functions that define a
*  general theory of brain.
*  I just don't know what those objective functions would be.
*  Yeah.
*  Like, let me give you an example.
*  I mean, you know, people talk about evolution as, as a optimization process, but it's a
*  weird optimization process because it keeps changing what the objective function is.
*  Yeah.
*  Right.
*  Yeah.
*  And, and if you were to create an objective function that, let's say you were to do
*  a genetic algorithm and you want it to happen to create the same kind of path that we
*  actually took in our history, you'd have to keep changing its objective function in, in
*  all kinds of ways that contradict each other.
*  So, you know, the objective function 300 million years ago is not the same as the one
*  hundred million years ago, right.
*  And they actually point in different directions.
*  And, and I think you'd have to go through so many, you'd have to add so many terms in
*  that function to drive the system to something that produces what looks like our brain,
*  that I would say, why not just look at what the history actually is?
*  And what would you get more out of having to go guided your system to do to do this
*  rather than just follow what actually did happen, which we do have data on.
*  I think I remember Blake saying the question was, do we need to look to evolution?
*  And I think his position was, no, we don't need to.
*  And I guess that's a truism almost.
*  But for AI, we don't need to, I think.
*  Well, but his interest is the brain as well.
*  Yeah, I think for the brain, we kind of do.
*  I think we do. I don't think ultimately and I think it comes down to that thing that we
*  started at the very beginning talking about, right.
*  Definitions. Right.
*  I think we are I think it's extremely hard to come up with the right definitions.
*  Right. And that's true for any science.
*  Right. And I kind of see as evolution gives us gives us a crutch that lots of other
*  scientific fields don't have.
*  Right. Is that we have a pretty good idea about what type of process produced the thing
*  we're interested in, the brain.
*  Right. And we have data that we can use to infer the stages in which things appeared.
*  Right. And if we have that data.
*  Why not use it? Why not let it guide us to define things?
*  So so getting back to Fela generic refinement, I think the idea is that you don't define
*  problems or systems until you get to the state where they become relevant.
*  So so something like mobile animals, certain things become relevant when you have mobile
*  animals, certain things become relevant when those animals encounter predators or get out
*  on land and suddenly vision is expanded dramatically.
*  Yeah. You know, or they take the nocturnal route because the diurnal route is being
*  dominated by gigantic thunderous lizards.
*  Right. There's there's certain things that happened that led the brain to develop in
*  certain ways. And we can reconstruct that.
*  And that now tells us gives us a guidance of what are the definitions that we should of
*  the problems that we should be asking.
*  And I just kind of feel like if we all agree that the brain evolved, then why not use that?
*  Because it's hard, Paul. It's hard.
*  There's not like a table of all the behaviors throughout all the evolutionary steps.
*  It takes someone to actually do the work and spend 20 years writing a book.
*  Well, no, but people are doing it.
*  Yeah. Right. And I'm not saying me.
*  Yeah. I'm saying there's a whole field of people doing this stuff.
*  Maybe we should give them more funding.
*  Maybe we should funnel some of that big AI money to the people doing comparative embryology or something.
*  The lamprey.
*  The lamprey people.
*  The shark. We need more shark researchers.
*  More sharks. Yeah.
*  Well, you know, small sharks maybe.
*  But no, I mean, I think there is actually a lot of knowledge there.
*  And there's debates.
*  So there's a few outstanding major debates in the field that are very vibrant.
*  And who knows which way things will go?
*  And it's kind of important to know.
*  But I actually think, yeah, I actually would call for more investment in that type of research if you're really interested in about the human brain,
*  because it will make some of those mysteries dissolve, just like, you know, life dissolved, the mystery of life.
*  So, you know, I think but it is actually there.
*  Like, I spent a lot of time reading this stuff and I haven't even really, you know, I mean, I've scratched the surface.
*  Yeah. But there's always more.
*  And it's actually a very daunting challenge to choose what to read.
*  But I have people help me.
*  I mean, Steve Wise, for example, has helped me tremendously, suggesting to me where to go.
*  And I contact some of the people whose stuff I read to find out where to go next, etc.
*  I mean, the knowledge is there.
*  And a lot of it, again, is just exploding now with techniques.
*  Genetic techniques, for example, are establishing the phylogenetic tree very solidly now.
*  The timing can be estimated, at least.
*  The fossil evidence is always difficult, but there's so much more than that.
*  And development in particular, because development is so constrained, because evolution can only make such small changes.
*  The developmental biologists actually can tell us a lot of things about what happened.
*  That constraint is key.
*  Yeah. And so I actually think, yeah, it's a lot of work, but, you know, lots of people are already doing the actual hard work.
*  Right. And I just think that for us, we just need to.
*  We just need to read more of it.
*  You know, I mean, it's, you know, I'm doing it, but anyone can do it.
*  One thing I have to say is it's fun.
*  I got to tell you, it's so much fun.
*  Like, do you remember when you were first a graduate student and you just walked into this huge, incredible maze of fascinating facts?
*  Right. Well, once you become a PI, you sort of get sort of you do your field, you read some paper, but you know exactly what they're going to say.
*  You know, this was like being a graduate student.
*  So there's a naiveté, that's a joyous naiveté.
*  Oh, it's a joyous. And it's just so much more of it.
*  Now, it's also incredibly confusing, right, because you don't even know what the words mean.
*  Right. They use all kinds of jargon.
*  You don't know it. But I mean, that's how it was in graduate school.
*  Yes. Yes.
*  You know, so again, it's actually for me, it was a lot of fun.
*  You know, as I tell people, it's the most fun I have is when I read some of this stuff, because I'm once again a student.
*  With this whole world being opened up, but you can't do this.
*  You can only do this so many times in your career.
*  Right. Where you just take on an entire new field because it's so massive.
*  I mean, how many more do you have in you?
*  I'm not doing the research. I'm just reading.
*  But you have to spend you.
*  But you are immersed because you have to be.
*  And yes, it's joyous, but it is a huge chunk of your mental resources.
*  Right. Yeah. Well, again, but actually tell you the truth.
*  You know, other than time constraints, I don't feel it takes me away.
*  In fact, I think it's shed light on some of the some of the rest of my work.
*  Some of the rest of my work, I think makes more sense to me.
*  I know I know slightly better maybe how to do monkey cortical neurophysiology.
*  Yeah. So it's really pays off.
*  And I think I would highly recommend anybody doing it.
*  And I thought, you know, I thought doing it, doing a sabbatical was was was a good idea.
*  Now, I have an actual project.
*  I actually want to write this book, which is it's going to be kind of a very, you know, speculative book.
*  And in order to do that, I have to learn all this stuff.
*  Right. You know, it may be harder for myself to motivate.
*  If I didn't have a long term project, it'd be hard to just say, oh, instead of doing what
*  instead of publishing the next paper, I'm going to go read some something about some creature
*  that I didn't hear about until last week.
*  That would be harder to motivate.
*  But but I don't know. I actually think it's extremely worthwhile.
*  I mean, I would say my thinking has grown a lot.
*  By reading all this work, I'm just hoping I pick the right authors to follow.
*  Yeah, I'm hoping that you steer me.
*  You point me to the right authors once you do all the work of finding them.
*  So, yeah, well, I hope I hope so.
*  I'd be very sorry.
*  I can tell you if Puelas is wrong and I'm going to be in big trouble.
*  Yeah, I'm also buying Grillner, buying into Grillner.
*  But, you know, you look at the quality of the work and you can sort of feel like you can trust it.
*  They're serious and they're careful.
*  And their methods are sound.
*  They're not they're not off the deep end.
*  They're not just some cook somewhere spouting some nonsense.
*  Right. Yeah. So so it's kind of it's a worthwhile investment.
*  And it's like I said, it's it's a lot of fun.
*  Speaking of graduate school, you got into neuroscience through
*  computer science, from what I understand.
*  So you started in computer science and and you were a graduate student with Steve Grossberg
*  or in Steve Grossberg's lab.
*  His name's been coming up more and more frequently lately, it seems like.
*  But I really wasn't that familiar with him.
*  Naive days of grad school.
*  Not that I'm much less than Jeff Schallmester.
*  Jeff Schallmester. Not really.
*  Not I mean, there's there's just a maybe I just missed it because it's not the one that keeps
*  recurring and recurring. But I came through sort of Mark Summers lab.
*  So I was Bob Wirtz and Mickey Goldberg and that, you know, and and Shaw.
*  But do you think that that Steve Grossberg is you think he's not as well known as he
*  should be? Well, you know, in some fields, he's actually extremely well known.
*  But I don't think in neuroscience he's as known as I think he should be.
*  And I what I mean by that is that there's certain things that he said that people are
*  only getting around to now.
*  Yeah. In particular, about dynamical systems and circuit level stuff.
*  It's not about representations, it's about dynamics.
*  You know, he's been saying that since the 60s.
*  And I think a lot of neuroscience has sort of rediscovered some things he said a long
*  time ago. And, you know, it's it's always difficult.
*  You know, I mean, he was writing in the Journal of Applied Math.
*  I mean, you know, who in neuroscience is really going to come across those papers.
*  Right. So so I don't think in neuroscience and in computational neuroscience,
*  he's not he's appreciated as much as he should be because people are, you know,
*  reinventing a lot of the things he said.
*  And I think he had a really good attitude about, you know,
*  modeling. So my other supervisor, that was Dan Bullock, and he's got a great quote.
*  You know how when you know how people do a model and it has many parameters and it
*  explains a lot of data, people say, well, you know, you've got a lot of parameters.
*  You can explain a lot of data with this.
*  Then Bullock has a great response to it. Oh, yeah. Try it.
*  Because what they try to explain is everything from neuroanatomy to psychology
*  to neurophysiology.
*  And yes, the models have a lot of parameters.
*  But the data is the data set is not just large, but broad and across level.
*  It's really, really hard to actually do it with all those constraints.
*  Right. And I would add to that still the evolutionary constraint.
*  Right. Which actually I got that I got onto that from them.
*  They they did models, for example, the spinal cord in which they reconstructed them
*  following some of the stages in evolution of certain cell types appearing, etc.
*  And and I think that's yet another constraint.
*  And if you go through all that and you focus on the dynamics and behavior and
*  physiology, etc., then you're going to come up with pretty good models.
*  So I think that attitude is, I think, something that that I got from them
*  and Grossberg. And
*  I think a lot of people would, you know, a lot of people would, I think,
*  benefit if they if they knew more of Grossberg's work, which is not to say that
*  I agree with everything that he's written because he's written so much.
*  Yeah, I know. I'm going to try to get him on the show.
*  But it's like what I got to read all this.
*  You should. Yeah, you should.
*  He'll be back. I think you'll be very happy to do it.
*  Anyway, I'm glad actually that his name is coming out more because it seems to me
*  when I was there, a lot of what I learned was through their lens.
*  But, you know, once I got out, it seemed like people just how can people not know
*  that he addressed this problem? Right.
*  Yeah. That must happen so frequently, really.
*  Well, yeah, it does.
*  I mean, you know, sometimes he addressed it and maybe they disagree.
*  But often people are just not aware of it.
*  And I think it's because, you know, I there's so many so much thing.
*  So many things you can read.
*  Well, so he was is and was has always been a very well read researcher.
*  And you are, too.
*  I heard Jeff Hinton was asked.
*  I don't remember the question, but he said that he likes a piece of advice
*  that one of his advisors had given him one time, which was don't read the literature,
*  because if you read, you won't go on and figure it out yourself.
*  You should go figure it out yourself. Don't read the literature.
*  So what's the right amount of reading?
*  So I might I would say something different.
*  I would say, read the stuff that people in your field aren't reading, I would say,
*  because the stuff that everybody's reading, you're going to hear about that anyway.
*  Right.
*  I mean, if you're in the field of,
*  you know, cortical primate decision making or something, you're going to read those
*  papers, you're going to come across across them.
*  Right.
*  So read the other stuff.
*  Read the stuff that people in your field
*  aren't reading, because then you'll have something new to offer, some new, new idea,
*  a new perspective from a different field.
*  And and you know, so, you know, in a sense, I would agree with Hinton's attitude.
*  It's like, you know, don't just follow in with your field.
*  Don't just follow along with what everybody else is doing and just say, all right,
*  well, this is the current thing that everybody's talking about.
*  You know, if this thing gets 17000 Twitter retweets, then that's what I should read.
*  Well, you're going to hear about that anyway.
*  You're not going to add something to that conversation.
*  Probably go read something else.
*  And then and then maybe you'll discover a
*  connection that neither field knows knows about.
*  Right.
*  But will anyone listen to you is the problem.
*  I think that you're taking a gamble when you venture off because you do need to feed.
*  That is a problem.
*  Yeah. Well, that is a problem.
*  Yeah. But I mean, you know, as long as I can survive and get my grants renewed,
*  I want to do what I want to do.
*  Right.
*  And it's true.
*  It would bother me if nobody cares.
*  Obviously, it would bother me if nobody cares.
*  But certainly following along with what everybody else is saying.
*  Again, I don't see myself making contribution to that.
*  I would just repeat, you know, I would just repeat the same thing everybody else said.
*  It's not fun either.
*  Well, yeah, and I don't think I would do any better than what they're saying.
*  Right. Right. Right.
*  I mean, there's a lot of incredibly brilliant people out there.
*  I think the easiest way actually to to to have an impact is to make a connection
*  that that between two large groups of people that haven't talked to each other.
*  You know, it's actually amazing that some of these connections aren't there.
*  Yeah, there's still a lot.
*  There's a lot of fruit to be had.
*  Yeah.
*  You know, the stuff that I read about brain developments, like,
*  how is it that I wasn't taught that when I was in grad school?
*  It didn't exist.
*  A lot of it.
*  Right.
*  You know, yeah.
*  Why is it hard to find a good question?
*  You know, we so when you're in grad school, you I got lucky because I was so naive.
*  And I had such a great advisor to let me do my own project that I don't know.
*  That doesn't seem to usually happen.
*  But that was born out of my naivety, his
*  misplaced trust in my ability to like take it somewhere.
*  And I don't know that it was, you know,
*  necessarily a good question, because a lot of fun.
*  But but, you know, connecting these disparate, otherwise disparate areas that
*  where the real benefit that you can give to you can contribute.
*  You have to be able to ask the right question.
*  And that's hard.
*  And I think you've done that very well with your phylogenetic
*  refinement approach.
*  And it's because people see your this thing and they're like, oh, yeah,
*  oh, that makes a lot of sense.
*  Do they?
*  Do they? I mean, I've had very positive
*  comments from people who have contacted me about it, but it just seems like a lot
*  of people, it's just not on the radar.
*  Well, I was going to ask you the same thing.
*  But I think this approach is becoming it's getting louder and louder coming from
*  different quarters. You've just you've been at it for a long time.
*  Yeah. Well, yeah.
*  Anyway, getting back to asking a good question,
*  I think that's very hard because you have you usually will inherit questions from
*  your advisors, right? And so, you know, I did to a large extent
*  inherit questions and it was mostly like, for example, that they exposed me to Gibson.
*  Right.
*  Reading Gibson was a big deal that that changed the way I approached when I came
*  in from a computer science background, I had a certain attitude
*  and Gibson changed and reading Gibson changed that.
*  And you have to be sort of plastic when you start out.
*  Right.
*  And and you I think you have to be lucky to be steered in the right direction.
*  About humble.
*  Humble. Yeah, humble.
*  Yeah, I think you have to you have to be willing to give up on things.
*  I believe you you held dear.
*  That's harder to do later on, though, right?
*  That's harder to do later on.
*  I've done it a couple of times.
*  I mean, Graziano turned me from from some when I first was in,
*  heard of his ideas, for example, I didn't buy it and I became convinced over time.
*  Some other ideas that I've let go of because over time I've gotten exposed.
*  But again, it comes down to reading other stuff.
*  Again, it's just read other stuff.
*  You get those ideas.
*  I think one problem, though, and maybe problems, the wrong word,
*  because it can be great if it's done well.
*  But you start off in grad school, you're exposed to the questions that your
*  advisors were exposed to, you know, you make some advances.
*  But then all of a sudden you're like two years in.
*  And it's almost like the rest of your career is set because, you know, in your
*  case, let's say you've been exposed to Gibson that started you on this track.
*  And yes, you got lucky, let's say.
*  And, you know, you crossed all the T's.
*  You got lucky. You worked hard.
*  You were smart. So, you know, you could make advances.
*  You're humble. Supervisor.
*  Supervisor.
*  But but all of a sudden it's like, oh,
*  you could really like define the rest of your career based on you can kind of see it.
*  Right.
*  And so it's hard to know, like, well, when should I?
*  Yeah. When should you settle into it and when should you keep exploring?
*  Yeah. Well, that's that's a tough one.
*  And again, I think it's actually a good idea to every once in a while, snap
*  yourself out of whatever you've sort of settled into and and try something
*  different just to get another perspective, to see whether what choices you've made
*  are good ones. And I don't think I've really done that because I've pretty much
*  followed, you know, I'm pretty much, you know, the idea of the brain as a control
*  system was essentially my first year in graduate school from Richard Dawkins,
*  actually, but other people that I've read like Ashby and Powers and Gibson.
*  Right. And since then, I've just become one of these
*  card carrying, you know, body and activism types.
*  And and and just it's colored everything I do.
*  And what if it's completely wrong?
*  So, yeah, that's that's that's a tough one.
*  You know, but and maybe when I when I say I read other stuff is still the stuff that
*  I choose to read is still the stuff that satisfies those interests rather than
*  other interests. Right. I'm not reading linguistics because I don't think that's
*  a good place to start, but it certainly would be different.
*  So I don't know. I mean, I
*  I mean, frankly, I think, again, just reading what your field doesn't is already
*  enough to help deal with that because because you'll snap out of certain things
*  that you otherwise wouldn't and you have a better shot of learning something new
*  that that helps you in a new direction.
*  It almost it takes takes very little external input to sort of jar the internal
*  milieu that's already going on. So, yeah, yeah, yeah.
*  Plus, I mean, plus it's fun.
*  And otherwise, I'd just be keeping up with people who are much more productive
*  and much smarter on doing all the same thing.
*  Right. Right. So it's not a good strategy.
*  Speaking of being humble and naive, my last question, Paul, is
*  what's an example of something that you used to believe that you now, looking back,
*  consider naive?
*  I have an answer.
*  It's almost embarrassing to say it.
*  That's the best.
*  Yeah.
*  So before I went to grad school, I was at Microsoft.
*  Right. I had I already knew I wanted to go to grad school.
*  But for a time I worked at Microsoft and I was offered the opportunity to stay.
*  And this was the early 90s.
*  So I had stayed, you know, you calculate how much money I would have had.
*  It's kind of disturbing. Right.
*  Yeah.
*  The naive thing was I didn't want to delay going to grad school because I didn't
*  want all the good questions to be answered.
*  Right.
*  As if they wouldn't be.
*  Because we would have it solved by now.
*  Because you'd have that.
*  And if I waited 10 years or something, sure, I'd have, you know,
*  12 million dollars in the bank, but the whole brain would be figured out.
*  And what would be fun to do?
*  Yeah.
*  So now I think about it.
*  That was pretty stupid.
*  Because, you know,
*  I could have I could have used that money, you know.
*  So that was pretty naive.
*  That's certainly something I consider naive.
*  You were thinking about a scientific.
*  No, I like that. I mean, you I mean, the more naive things, the better.
*  Yeah. You can't top that.
*  Yeah. That's about as naive as you can be.
*  Paul, thank you so much for taking so much time.
*  I've really enjoyed diving into your work.
*  And I'm looking forward to having you back on the show in 2057 when you're done
*  with your book.
*  Yeah, that's true.
*  It should be. Yeah.
*  I'll be rewriting Chapter one at that point.
*  That's right. Right.
*  I'll be deciding, wait, I got this all wrong.
*  Got to start from scratch. Yeah.
*  Yeah. OK.
*  Anyway. Well, thank you again for inviting me.
*  It was a pleasure.
*  Brain Inspired is a production of me and you.
*  You can support the show through Patreon for a microscopic two or four dollars per
*  month. Go to brain inspired dot co and find the red Patreon button there.
*  Your contribution will help sustain and improve the show and prohibit any annoying
*  advertisements like you hear on other shows.
*  To get in touch with me, email Paul at Brain Inspired.com.
*  To get in touch with me, email Paul at Brain Inspired.co.
*  The music you hear is by The New Year.
*  Find them at TheNewYear.net.
*  Thanks for your support. See you next time.
