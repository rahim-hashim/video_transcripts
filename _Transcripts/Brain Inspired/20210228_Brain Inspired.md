---
Date Generated: April 20, 2024
Transcription Model: whisper medium 20231117
Length: 6396s
Video Keywords: ['Science', 'Technology', 'Education']
Video Views: 12772
Video Rating: None
---

# BI 099 Hakwan Lau and Steve Fleming: Neuro-AI Consciousness
**Brain Inspired:** [February 28, 2021](https://www.youtube.com/watch?v=wa2KvnQ9mro)
*  For me, I feel the lack of clear definition is exactly what attracts me to the problem,
*  because in some ways, then that means that someone needs to clarify those concepts.
*  People coming into the field are going to be key for this. I think Hakwan rightly so
*  reminds us as a community that we shouldn't be complacent, that we do need to cultivate
*  and mentor people who are interested in this area and for them to gain the right kind of
*  skill set to come into it.
*  Stephen, I agree on a lot of things and maybe that's one thing I like working with Steve
*  is that his more positive outlook often complements my darker pessimistic outlook to things.
*  Screw it. I've got tenure, I've got awards in various fields and now I can do this consciousness
*  and maybe the field needed that outsider effect.
*  Yeah, I mean maybe it needed some of them and not others.
*  That's the diplomatic way of saying it.
*  Let's go down the list here.
*  Welcome to the Consciousness Extravaganza podcast.
*  I'm Paul and I believe there should be a podcast called the Consciousness Extravaganza podcast.
*  I'm too busy to make that, but I am happy to have Hakwan Lau and Steve Fleming on for this
*  episode about one of my favorite topics, consciousness.
*  Hakwan runs the Consciousness and Metacognition Lab at the University of California, Los Angeles,
*  and Steve runs the Meta Lab at University College London.
*  They're both interested in pursuing cognitive computational accounts of consciousness and
*  the related phenomenon of metacognition and striving to inch the science of consciousness
*  forward on solid empirical footing.
*  And indeed they collaborate frequently on these topics.
*  As you probably know, there are a plethora of consciousness theories out there and there
*  are more being added all the time.
*  The main one Hakwan and more recently Steve have been associated with is the family of
*  higher order theories.
*  Roughly higher order theories posit that some part or circuitry in the brain has the capacity
*  and mechanism to represent what's going on in other parts of the brain.
*  So frontal cortex may be able to represent the contents of visual cortex and some of
*  the associated statistics of that visual cortex processing.
*  So the second order metacognitive representation in frontal cortex in this example is like
*  perceiving our perceptions.
*  So like I said, there are many other genres of consciousness theories.
*  Some are well known, including the global workspace theory, integrated information theory,
*  and so on.
*  And we touch on a few of these as we go along.
*  Since both Hakwan and Steve are developing computational models to explain some aspects
*  of consciousness, we talk about one of each of their recent models.
*  In Hakwan's case, he is working on a reality monitoring model, which addresses how a higher
*  order system of awareness might develop or might have evolved as a sort of reality monitoring
*  system, asking if your perceptions are from external reality or are generated by your
*  internal processing, your imagination.
*  So he likens this to a generative adversarial network, or GAN, in AI.
*  These are the kinds of networks that are responsible for producing some of the AI-made artwork,
*  like portraits of people who don't exist and so on.
*  The way a GAN works, it has two subnetworks that interact.
*  So one network is called the generator, which produces data.
*  In the AI art example, this would be a potential work of art.
*  In the other subnetwork, the discriminator judges or discriminates whether the generated
*  data or art is good enough to accept as art.
*  So in Hakwan's reality monitoring model, your brain's generator provides the perceptual
*  data and your brain's discriminator judges whether that data is from the outside world
*  or from your imagination.
*  And Steve, on the other hand, is working on what he calls a higher order state space model.
*  So whereas Hakwan's GAN-like reality monitoring model distinguishes between reality and imagination,
*  Steve's model signals when awareness of some object is present and then can probe the content
*  of that awareness.
*  So for example, I'm aware I'm seeing something and that thing is an apple.
*  But the model also actively signals when there's an absence of awareness.
*  And in that case, there's no content to determine.
*  There's no difference between being unaware of an apple and unaware of an orange in the model.
*  So that's obviously a very cursory introduction there.
*  They describe the models a little bit more.
*  We discuss them.
*  We also compare their models a little bit.
*  We discuss consciousness in AI, functions of consciousness, and lots of other topics.
*  And there's a big chunk in the beginning here where we discuss the nature of studying consciousness.
*  So the social and career aspects of studying something so unknown and often controversial
*  as consciousness is and how it differs from other research areas and how it also doesn't
*  differ so much.
*  So we talk a little bit about a lot of topics.
*  So I hope that this wets your appetite to dig deeper in the show notes to learn more.
*  So go to braininspired.co.uk, podcast slash 99.
*  Start the podcast on Patreon if you find that it's expanding your mind and advancing
*  and improving your mental world or some of your own research, which I hope it is.
*  You get to hear full versions of all the episodes and there's some other stuff you get too.
*  So go to braininspired.co and click on the Patreon button there.
*  I find it unbelievably satisfying and gratifying to talk about so many fascinating topics.
*  I hope that you take a moment from time to time yourself to just appreciate what a privilege
*  it is to spend some time thinking about these things and listening to so many good people
*  like HaKwon and like Steve.
*  Enjoy.
*  So I have been reflecting lately about alternative ways that my vocational path could have turned
*  out.
*  And back in my graduate school days, I was, you know, if I'd continued that line of research,
*  I was on my way to a research career much like you guys have perhaps.
*  But once I got my PhD and went on to a postdoc, that changed.
*  So I had Megan Peters on and HaKwon, Megan was in your lab for a spell there and now
*  has her own lab.
*  But I thought it'd be fun for episode 99 before I hit 100 here to have on some people that,
*  you know, resembled what could have been my counterfactual path.
*  So thanks for coming on, guys.
*  It's a real pleasure to be here.
*  Yeah, thank you.
*  Likewise.
*  So we're going to get into some gritty detailed research topics about consciousness.
*  And Steve, before I even begin here, I just saw on Twitter the other day, which I try
*  to spend as little time as possible on, but I saw that you're actually coming out with
*  a book about all this, which you didn't tell me about before we set this up.
*  What's the book?
*  So it's not specifically on consciousness, but it's more on metacognition and self-awareness.
*  So it's entitled Know Thyself.
*  And yeah, it's coming out, being published by Basic Books in the US at the end of April
*  and also John Murray's in the UK, similar time.
*  And it covers the whole range of work on metacognition ranging from animals all the way up to humans,
*  covers the development, evolution of metacognition.
*  So lots of interesting things to talk about, but perhaps a little different to consciousness
*  itself.
*  It's probably got your political leanings work in it as well, I would imagine.
*  It has.
*  Yeah, OK.
*  So there's a lot of stuff that we're not going to get to that is in that book that we won't
*  talk about on the show.
*  Maybe I can convince you to come back on.
*  I'd be delighted to do that.
*  Well, we'll see.
*  Hang on.
*  I'll ask that question at the end of this interview and see if you have the same reaction.
*  OK.
*  So but before we get into the real research topic stuff, just for my own personal benefit,
*  I want to ask about the social aspects and just your personal feelings and reflections
*  about studying consciousness.
*  And I know you've both studied a lot of things other than consciousness, things like metacognition,
*  and it's all interrelated, etc.
*  But so I'll start with this.
*  So a lot of people on my podcast have either on or off air suggested that we shouldn't
*  even be studying consciousness.
*  And there are various reasons.
*  So I want to throw a few reasons out to you here.
*  And you guys can you can rebut and respond to why those are not good reasons to not study
*  consciousness.
*  So one, the first thing, and I think Jeff Hawkins said this even on an episode, is that
*  there are many more important things to be studying, which are actually tractable.
*  And so consciousness shouldn't be one of those things that we should be focusing on.
*  What say you?
*  I think the maybe first reaction I would have is like to each their own.
*  And I think science works because different people do different things.
*  If we all rush to like, OK, now Covid is the biggest problem and everyone just go and work
*  on it.
*  Science wouldn't really work.
*  I mean, different people should pursue different things.
*  And that's like one of the beauty of being a scientist.
*  The perks.
*  And we can just do whatever we want.
*  And to the extent that is actually somewhat tractable.
*  And so the other part is really the tractability part.
*  Then I actually don't find it so intractable.
*  Ultimately, we are measuring something that is not so so elusive or navelous.
*  It's a little bit trickier.
*  We are really measuring something via self-report.
*  And but but so are many other people in psychology.
*  I mean, if you study episodic memory, if you study just memories in general or or psychiatric
*  diseases, quite often self-report is a key component.
*  And we are just studying that.
*  We're focusing on that and via that inferring people's subjective experience.
*  And so it's a bit like you can't you can't touch the stars, but you can you can you can
*  watch them through your telescope.
*  And likewise, I can't directly measure your experience, but I have a pretty good indirect
*  inference through inference through your subjective reports or self-reports.
*  That seems to me not not I don't I don't quite see why is by nature intractable.
*  Steve, you're I mean, I know that you've always been interested in consciousness, but
*  you've kind of winded your way there through metacognition.
*  So there's a case to be made that metacognition is more tractable.
*  And maybe the study of confidence and how that relates to our social being and stuff,
*  maybe that's more important than phenomenal experience.
*  And by the way, when when we say consciousness, we're just going to assume we mean phenomenal
*  subjective experience, not waking sleeping states throughout the entire episode here.
*  So what do you think are there when someone says, well, why are you doing that?
*  Aren't there more important things to do?
*  No, I would echo a lot of what I said there.
*  I think that it is a privilege for each of us in science to in some sense decide what
*  we find most interesting.
*  And we want to be excited to get up in the morning and go and work on that problem.
*  And everyone's going to have a different sense of what the most interesting thing is.
*  I mean, I think that the idea of tractability, though, I get somewhat frustrated with this
*  because I think the transitive notion of being conscious of something is completely tractable.
*  As Hacquam was saying in the lab, you know, we can start to measure that in terms of
*  subjective reports.
*  There's some things that we're aware of at certain times that other things we're not
*  aware of. We can ask subjects that we can model their reports in various in various
*  ways. We can start to write our models of the mental or neural representations that
*  would enable that kind of report.
*  The frustrating thing sometimes is that people assume that if we're doing that kind of
*  research, then we must have to also solve the mind body problem at the same time.
*  Right. So you must somehow have to like grapple with that at the same time.
*  And I think that that's that's just not the case.
*  Like that's we don't have to grapple with it any more than someone researching emotion
*  or episodic memory has to grapple with it.
*  Right. So we're looking at something quite constrained and there's a type of
*  computation that allows the human mind to become aware of certain mental states and
*  not others. And that's what we're we're interested in.
*  I mean, I suppose tractability has a lot to do with operationalizing terms and phenomena
*  that you're studying.
*  So I was going to ask about, you know, people argue that there is just no scientific way
*  to study subjective awareness.
*  So you guys kind of address that with the tractability question.
*  But what about the question of consciousness not being a well-defined phenomenon?
*  And don't worry, I'm not going to ask you to both define consciousness.
*  I'll ask you a related question in a few minutes.
*  But but yeah, what you know, what about the aspect that it's not well-defined enough or,
*  you know, it's not characterized enough, let's say, you know, to to to really study it?
*  I think that that that lack of precise constrained definition that would also affect other
*  fields of cognitive neuroscience.
*  So, you know, everything we're studying here in terms of psychology is a somewhat fuzzy
*  concept. But as long as it's grounded in empirical criteria, don't tell psychologists that
*  though. They don't like that.
*  I think I think the reason it seems different is that it goes back to my previous point
*  that like people interpret it as meaning grappling with the hard problem, the intrinsic
*  subjectivity aspect.
*  And I think that I don't know, I used to be bothered by this a lot and I'm less bothered
*  by it now.
*  That's interesting.
*  Yeah, for me, I feel the lack of clear definition is exactly what attracts me to the problem,
*  because in some ways, then that means that someone needs to clarify those concepts, like
*  what Tovang has done for memory, then someone should be doing this work for consciousness.
*  And again, it kind of relates to my trying to find a trying to find ways to make myself
*  useful.
*  Having a bit of philosophy background and I my work is quite interdisciplinary.
*  I feel this is exactly the kind of groundwork that I like to do.
*  Whereas if you put me in a field that is very well defined, everything is already written
*  in equations. I would just be doing the kind of geeky work that that may not be so suitable
*  for me because I'm not really a geek and I'm more like a soft interdisciplinary jack
*  of all trades that actually would fit my skill set.
*  So I think and that might fit some other of your audience skill set, too, if they like
*  to think about concepts and try to pin them down, sharpen them, make them tractable, then
*  that's a good field to be in.
*  Is it fair to say then that you might prefer or enjoy working in a Kuhnian pre-paradigmatic
*  sort of state if that rings a bell to you?
*  Yes, I think roughly.
*  Yeah, more or less. So, yeah.
*  Because once once the paradigm is set, then we're all just doing what is Kuhn call it,
*  then we're all just kind of worker bees or whatever doing the normal science.
*  So it's called normal science.
*  Yeah, that's right.
*  Hakwan, you've read and written recently about gurus and backscratchers and how research
*  and consciousness progresses and with a little model.
*  Tell me about gurus and backscratchers.
*  Yes, that's a little fun and maybe slightly provocative paper that's putting it mildly.
*  But the terms gurus and backscratchers were not like we calling names of other people.
*  They are actually technical terms in the literature.
*  So Dan Sperber created the term gurus to refer to.
*  I mean, they are both both terms come from discussions of philosophy of science.
*  So in a sense, they are technical terms.
*  So we're just quote unquote using them and not trying to come up with insults for other
*  people and we didn't refer to any current contemporary colleagues.
*  It has an insulting ring to it, those terms though, don't they?
*  I think so.
*  I think that might be part of the intended effect to raise awareness of these problems.
*  But we don't particularly point fingers and say you are a guru, you are backscratcher.
*  I think in some ways we all are.
*  That's the point.
*  I think it's just to me the hardest problem, if the field has a problem that is really
*  intractable, that is not to me is not even the problem of mind body problem itself.
*  Even that I think that we may be able to inch towards it.
*  The really difficult challenging problem for me is the socio historical aspect.
*  Just we have so many people who think of this as not a normal scientific problem because
*  and then they just come along and then they just say whatever they want.
*  And then usually they are already very well established in the careers through some other
*  means and then they just come in and say things that clearly they would not have accepted.
*  It will be up to their standard if you judge it by their previous research.
*  But they feel, well, I come into the study consciousness, this is my
*  last retirement party or something and they are going to go big or go out and just say
*  something rather radical and unhinged.
*  And then the rest of the field usually, yeah, often-
*  Those are the gurus.
*  Yeah, those will be what Dan Sperber called the gurus.
*  And because of the status and it creates, I mean, usually the reaction is too kind.
*  Some people like myself, you can probably already guess I'm not a big fan of this kind of stuff.
*  But then at the same time, there's a combination of appeasement or other motivations that
*  mostly people are not as negative about this kind of high sounding, high-brow speculation than I am.
*  And we somehow accept that.
*  And some people even benefit from that by associating themselves with these gurus.
*  And that's what we call the backscratching.
*  And I think both phenomena are very rampant.
*  And I'm not saying I'm completely free from any of this.
*  I'm part of this field.
*  And I probably at some point, some other people would say,
*  I used there you are being a 2% guru and 5% backscratcher or maybe more.
*  But I think we all are guilty to some extent.
*  But the point is not to just say individuals have done anything wrong,
*  but that really creates a kind of vicious cycle to some extent.
*  The people often don't take the literature seriously and they just come in and say something.
*  There are so many new ideas, like really too many new ideas relative to
*  new experimental paradigms and new findings that are robustly replicated.
*  Yeah, so the overall idea though is that you have an expert in some field, let's say,
*  oh, let's say physics, just to be random, who then decides, well, I need to,
*  consciousness is not solved.
*  So here I'm a physicist.
*  People respect my opinion, or they don't even have to think that,
*  but they offer an opinion.
*  And because they are a renowned physicist, then a bunch of backscratchers say,
*  well, we must trust that person in all realms of science because they're an expert in their
*  own domain.
*  The point is that that potentially hinders progress in consciousness science.
*  Is that right?
*  Yeah, I think in that case, I can mention that Dan Sperber actually used the case of Nobel Laureate
*  Roger Penrose as an example of a guru.
*  I mean, we didn't say that he was one, but Dan Sperber, who coined the term, said that he might.
*  His case exactly in the domain of a theoretical physicist coming to speculate on the nature of
*  consciousness might be considered an example.
*  The backscratching is, the gurus I can actually understand better.
*  The backscratching is in some ways more complicated.
*  So those people, some of them I think are genuinely impressed by the statue of those people.
*  And some of them actually probably have something to gain by being up in a popularizing domain and
*  by mentioning these great names, they kind of associate themselves at that level.
*  But I don't think anyone is so evil to just wake up in the morning and look at a mirror and say,
*  okay, today I'm going to be a sycophant.
*  I don't think people do that.
*  I think it's somewhat maybe implicit in the way that the game is set up or the competition is set
*  up because there isn't a whole lot of public funding in this field.
*  So it's not like you can actually just write a normal NH Grant and just say, okay, I'm going to
*  attack Roger Penrose ideas and probably wouldn't really work.
*  And so because of the lack of the paucity of public funding, a lot of it becomes private funding.
*  And I think private donors are in some ways more easily impressed by this kind of association.
*  If you drop names and say, oh, I had tea with Sir Penrose last month at Oxford and we discussed
*  about this idea, that probably wouldn't fly so much in an NH review panel, but it might fly better
*  in a private donation situation.
*  Of course, I'm just making entirely hypothetical.
*  I did not have tea ever with Sir Penrose, but I meant...
*  So you can see that why this kind of backscratching might be somewhat unique in our field.
*  Yeah, it's complicated.
*  But I wonder whether one of the reasons for that situation, which I also recognise in
*  consciousness science, is because it has been taboo for so long for junior researchers to be
*  getting in and doing this from the word go.
*  And so in a way, traditionally it's been the case that people only turn to consciousness
*  when they are feeling secure and senior enough to risk doing so.
*  And I wonder when that's the point where this intersection of freedom and security and concern
*  for legacy thinking, okay, this is a big problem, I'm just going to go for it late in life.
*  That's kind of been a feature of consciousness science, but I'm optimistic that that hopefully
*  will shift now that it's becoming more accepted and okay for early career researchers to be
*  working on these problems.
*  They're now getting steeped in the methods, doing good psychophysics, good modelling and so on.
*  I don't know, maybe I'm just being naive and too optimistic, but I hope that that position
*  that Hacquan sketched there will eventually dissolve and will become a field more like
*  other fields where that's not so much of a problem.
*  I mean, it's kind of interesting because you could actually then in that model think of
*  the gurus, we have to have something, have some gratitude toward them for bringing
*  regular sort of more inside, like the neuroscience type and psychology type of research on
*  consciousness, bringing that into the more normal domain of science because they were established
*  and were able to like say, screw it, I've got tenure, I've got awards in various fields and
*  now I can do this consciousness and maybe the field needed that outsider effect.
*  Yeah, I mean maybe you needed some of them and not others.
*  That's the diplomatic way of saying it.
*  Let's go down the list here.
*  Okay, well let's talk about the great unknown a little bit and we're going to start kind of
*  broad and then we're going to get down into some of the recent models that you guys have
*  established here by way of talking about consciousness in AI.
*  There are a bunch of different theories and as has already been mentioned, there are more and
*  more and more and more and it seems like there should be less and less, but it seems, you know,
*  the amount of theories keeps growing.
*  Eventually, we'll let's say eventually we'll have a satisfying explanation for consciousness, right?
*  How much of, you know, all of the current and historical debates among the different theories
*  do you think will just disappear like into oblivion versus, you know, will all of or many of the
*  theories still have, you know, carve out some space within the eventual acceptable explanation?
*  You know, is it going to be like the Elan Vittal that people always refer to when, you know, that
*  used to be considered important for an explanation of life that then just disappeared once even
*  though we still don't know what life is, but we still accept some, you know, surrounding
*  characteristics and such?
*  Yeah, I would think that the analogy to vitalism and explanations of life is a useful one
*  and I would be of the opinion that when we have an explanation of the functional aspects of
*  consciousness, then this notion of a kind of independent fundamental magical property
*  that's irreducible will the force of that problem will start to dissolve.
*  So I think just like we understand things like DNA and homeostasis and so on, we don't go around
*  saying, okay, we understand all that, but what's the magic of life?
*  You know, why is this thing living?
*  I think that that will dissolve.
*  Now, I think, like I said before, the mind-body relationship is going to remain a feature of
*  psychology of just doing science of the mind.
*  I don't think that's going to disappear, but I think its force as a problem in science
*  will diminish in a similar way.
*  I don't know what Hakram thinks about that.
*  He's more steeped in the philosophy than I am.
*  Well, I dabble more, but it doesn't mean I have anything more informed to say than you already did.
*  That's that slightly slouched spine of his coming through there.
*  He dabbles.
*  Yes.
*  Now, I'm trying to dodge the bullet or something.
*  No, I think we think very similar on this issue, Steve and I.
*  I feel that historically, the kind of cognitive approach has always been there and,
*  I mean, not always, but has been there for half a century or more.
*  I think it's been making steady progress.
*  I think my money will be on that.
*  I will bet on this eventually dominating and accounting for the more tractable aspects.
*  There will be some tractable aspects and some people might be panpsychists and
*  they believe in metaphysical views or they think metaphysical views are
*  interesting or valuable.
*  I don't deny that, but I just feel that those might be what we think are the
*  less scientifically tractable problems.
*  If you talk about the kind of consciousness that we study, we basically try to distinguish why some
*  brain processes are introspectable and some are intrinsically not introspectable even when you try.
*  Some are accessible, not just access at the moment, but some are in principle
*  possibly accessed by you and some are just closed for that access.
*  Figuring out these kind of issues, I think the cognitive approach will ultimately dominate and
*  we will start to make good predictions and maybe applications.
*  Essentially, then people will be like the case of life.
*  Then people will think, okay, these guys have pretty much figured out the
*  important aspects that are worth figuring out and there might be some
*  philosophical leftover problems and that's that.
*  Do you think an ultimate explanation will feel intuitive or do you think we'll just have to get
*  used to it like relativity or even the idea of gravity, right?
*  Newton's gravity, we all grew up with it so it doesn't feel foreign, I suppose.
*  But when you really think about it, it doesn't make any sense.
*  We just have to get used to physical laws, right?
*  Then I guess they start to feel somewhat intuitive and do you think that that's going to happen
*  with an explanation of consciousness?
*  Yeah, I think so.
*  I think that's the beauty or the attraction of the cognitive approach because once you try to
*  write down exactly what are the cognitive mechanisms, essentially you are saying that
*  the brain essentially computes or do something that is akin to computation and then that can
*  implement certain kind of agroforms.
*  Once you write down the agroform, you can actually write the program yourself.
*  Once you're at that level of understanding, things are going to be intuitive enough.
*  Intuitive as in my computer is intuitive.
*  I don't understand every component of it but I understand, okay, some memory presumably
*  work in this mechanistic way, the display probably work in this mechanistic way.
*  I get enough of a grasp but I don't think it's a philosophical mind-bending situation
*  like quantum physics.
*  Do you have something to add?
*  No, I would agree with that.
*  I think it's very hard to know what intuitions will be like.
*  I think the intuition of someone 200 years ago walking into this room seeing us on a
*  Zoom call across three continents would be very, very bizarre.
*  I think that I would agree that at the moment I can see how people can be taken in by the
*  force of an explanation saying, well, there's going to be something left over.
*  There's going to be something that's counterintuitive about a mechanistic explanation of
*  consciousness but I think as that research program progresses, as that framework gets
*  explanatory power, then the intuitions will evolve with it.
*  All scientists have biases and good science, best we can do is try to work around those
*  biases but all science also still proceeds by guesses about what's true and then you
*  can test those guesses and the circle continues.
*  I want to ask you both two questions and they're related.
*  One is what do you want to be true about consciousness?
*  This relates to the biases.
*  The other is I don't know if you have a specific label like when you wake up in the morning
*  as Haakuan did about 20 minutes ago.
*  If you wake up and you have a ready-made label for what you actually believe to be true
*  and what you want to be true and what you believe to be true don't necessarily need
*  to be the same thing, especially if you're a really good scientist.
*  What do you want to be true about consciousness and what do you believe is true?
*  They can be the same thing, I suppose.
*  What are the options?
*  You both subscribe to the higher order theories of consciousness.
*  I don't know.
*  You could say that you want and believe higher order theories to be true and that's fine
*  and we can move on.
*  Let's say I read about integrated information theory and then I read about higher order
*  theory and then I read about global workspace and there's something attractive about all
*  of those. Panpsychism is not in that list.
*  We can come back to that.
*  There's something a little bit sticky about a lot of those ideas and then I move on to
*  the other one and it's the new bright shiny thing and then I know that makes sense.
*  This aspect makes sense and then some things don't make sense.
*  I'm constantly grasping for the right question and what even is the right question.
*  Yeah, I think that I want the, like we just talked about in the previous segment, I want
*  the cognitive framework to be a useful one for explaining consciousness because I think that if
*  we can't make progress within that framework, then I don't think we're going to have something
*  that looks like a satisfactory explanation. I think that to test those kind of frameworks,
*  we need to be looking at contrast cases. We need to not just be thinking of consciousness
*  as a fundamental property of systems. We need to be looking at empirical data that contrasts
*  and where this is on the worst states and so on. That's what I would like to be true because
*  otherwise it's hard to make progress on what an explanation would look like.
*  Yeah, I have something similar. I would really want animal models to be sufficient.
*  That is, I really want that if someone studies consciousness in monkeys or even rodents,
*  I would really want that to be a good model. I basically go about assuming that they have
*  very similar conscious experiences as we do because to my mind, that's really the only way to do
*  good neuroscience. I mean, human neuroscience is important, but so far we rely a lot on those models.
*  Oh no, I'm anticipating what's coming about what you believe.
*  No, I actually don't firmly believe otherwise, but my collaborator and mentor and friend,
*  Joe Ladeau, often gives me a slightly hard time about this because he of course studied rodents
*  and is known for that work very much. But he actually is a skeptic. Sometimes he would say,
*  I'm sure they have some experience. He wouldn't even say sure. He would say,
*  presumably they have some very simple experience. But when it comes to some of the more self-referential
*  experience like emotion, do you think a monkey would ever feel full blown jealousy like grown
*  adults would? Like elaborated like we do. Yeah, exactly. When we think about even small children,
*  they might have some sort of simple envy, but as far back as I recall, my emotional life wasn't as
*  rich as it was now. So there are differences that might ultimately limit the usefulness of the
*  animal models. So I try to make peace by studying the simple Gabor patches and hope that the monkey
*  would see it the same way I do. But that's a bit of wishful thinking we would never fully know for
*  sure. Yeah. Yeah. That's interesting to hear that you're maybe debating on either side of Joe with
*  that. Because I would probably side more on Joe's side in the sense that, and this is shading into
*  our work on metacognition more than maybe consciousness itself. But I mean, we've been
*  looking a lot and Hakwan has too, at the involvement of these anterior prefrontal regions that are
*  considerably and there are new subdivisions that can be discovered in frontopolar regions that
*  don't seem to exist in even the macaque. So the involvement of those in kind of creating this
*  computational platform for higher order thought, which that's one broad brush way of thinking
*  about what those regions are doing. That to me also puts me on maybe the side of like,
*  we're going to need better techniques for human neuroscience. We can't just rely on animal model.
*  That's interesting. I think, yeah, we, I think Steve and I basically agree on virtually everything.
*  But this is maybe one point that our views might diverge. I'm more of a deflationary higher order
*  person. And sometimes people would quiz whether are you really a higher order theorist? I say like,
*  what's in the name? My view is my view is I would like to think of the higher view as a
*  kind of happy medium between a global workspace view and a local sensory recurrency type of view.
*  So I'm not very, so there's a joke question we ask ourselves or each other in the higher order
*  theorist community is how high are you? And I think Steve probably is higher than I am.
*  I thought as high as he is. Oh man, I'm super low if that's the
*  I want to be high. Not super low, not as low as the local recurrences.
*  I mean, not literally. I know it is evening here. Yeah. I mean, I do, I do think that like,
*  you know, just a knee jerk focus on say one brain region is not, not so helpful, but I think that
*  the fact that the greatly expanded, for want of a better word, association cortex in humans,
*  so the prefrontal parietal system, even with respect to other primates, right? So just in terms
*  of sheer cortical neurons that obviously we don't understand how that's working in any given level
*  of detail in relation to say higher order thought, but the fact that we have more recursive power there
*  and seem to be able to generate these rich metacognitive models, that to me would feel like,
*  I still think we can get a lot of insight into the computations underpinning such models in the
*  monkey, but I wouldn't be so sure about going down to rats to do that. Yeah, same here. I think
*  rodents are a little bit different, but I think for monkeys, I think to kind of anticipate or
*  put it in a quick brush, I think our difference may be that I, unlike Chris Frith, I think that
*  consciousness is not explicit metacognition. So all these like further high up hierarchy,
*  like involving frontal pole and all these mechanisms, they may be important for explicit
*  metacognition, that is explicit reflection, you think about yourself in a situation, etc.
*  But I think for consciousness, in my view, and in some others view, is just a kind of very minimal
*  implicit metacognition that your brain does regardless of what your intention is. So as
*  I'm just like looking at the screen, basically, my brain is already deciding that the sensory activity
*  reflects the state of the world right now, rather than my own imagination. And that happens
*  automatically. And that breaks down in dreams, etc. And so I think that I would assume is
*  rather implicit is presumably common between us and the macaques. But I yeah, this is some wishful
*  thinking that we haven't been able to fully test it out. I'm not sure we ever can.
*  I'm often when I wake up from a dream, I often have the thought like, you know, the thought that
*  that didn't make any sense, or it was just readily acceptable in a dream. And then I wake up and think,
*  well, now I feel like that was not I was not very confident that that's what I should have been
*  doing, you know, or something like that. So I don't know, we won't get into dreams here.
*  You know, because that's what no one wants to hear about each of our dreams. We could we could
*  discuss last night. But let's I think lucid dreaming is an interesting case. Oh, yeah,
*  I have a friend who's really into that. So you're not writing grants about that. Are you Steve?
*  No, I just I've only ever had one lucid dream when I was sleeping on a boat when I was
*  overtired. But it was Yeah, it was great. What did you get to do?
*  We'll just leave it that I just swam around the boat like a like I was flying around the boat. But
*  I mean, there is, you know, amazing recent work has only been a two or three studies on this, but
*  showing that the neural correlates of lucidity in dreams seems to be very similar to neural correlates
*  of waking metacognitive reflection. So that would, you know, seem to line up in the sense that
*  this kind of, you know, second order reflection on our experience seems to
*  be absent in dreams, but that it can come back online when we're lucid.
*  Yes. So essentially, yeah, I think lucid dreaming is a great example of preserved
*  explicit metacognition at the in that is coupled with a failure of implicit metacognition.
*  Right. So what I mean is dreams are almost by definition, according to the way we've been
*  talking, is failure of implicit metacognition, as in you're confusing your endogenous, spontaneous,
*  sensory firing as if it's reflecting the state of the world right now. So you if your implicit
*  metacognition is doing its proper job, it should not be letting you see things as if they are in
*  the outside world. So just having the qualitative sensations in dreams means that your implicit
*  metacognition fail. And mostly when it happens, your explicit metacognition also fails. So in
*  dreams, you don't know you're dreaming, you just it just happens to you. But lucid dreaming is a
*  case when they come apart. So I might going back to what I talked about earlier, I would think that
*  probably monkeys would not have lucid dreams, because they may not really have explicit
*  metacognition at that level. But I presume that they dream qualitatively, they have qualitative
*  subjective experiences when they dream that is they have implicit metacognition, just like we do.
*  So training a monkey to lucid dream, that's one thing you can pitch to the NIH.
*  That's on my second graduate career life. It was hard enough to report to wager on their decisions.
*  Jesus. Let's um, anyway, now you've got me thinking about lucid dreaming monkeys. Let's back up here
*  and let's just talk about higher order theory just for a second, because then we'll and then
*  we'll get into your models that you've both been working on. And then we'll bring it into AI perhaps.
*  So I don't know where I got this. I think I copied and pasted this from one of your
*  your papers here. So here's my little definition of what a higher order theory is. A mental state
*  x is conscious, if and only if one has a higher order representation to the effect that one is
*  currently representing x, whose paper is that from? Can you get Can you tell?
*  Must be hack ones. It's got if and no, you're both prolific. So you probably don't even remember that.
*  No, I think it's Steve. I think I don't I don't use these x y c's in my
*  I don't use it. Maybe I wrote it. Maybe I wrote it. Anyway,
*  so does that sound right? I mean, is that that roughly is what a higher order theory is, correct?
*  Yeah, I would say so. More or less. Yeah. So I mean, this is distinguished from okay, so the whole
*  distinguishing distinguishing factor about a higher order theory is that we have to have some
*  second order sort of process that is representing our first order perceptual processes and ongoing
*  things like in early sensory cortices that there and that's why areas like prefrontal cortex are
*  brought into this because they're sort of higher in the hierarchical structure of the brain. So
*  that by the time it gets to prefrontal cortex, you're able to then I don't know, is it right to say
*  have a model of those representations? What's the difference between a second order having a
*  second order representation and having a model? I mean, I would say that a model has parameters.
*  So I think both are going to be involved in a higher order theory. So you can think of a model
*  as being broader than a representation. So a model would say represent the signals and noise
*  statistics of perception, whereas a higher order representation would point to particular content.
*  So it'd be like more like targeting particular content. But I think that both are needed,
*  right, to effectively form beliefs at this higher level of the network. So we do a lot of our
*  modeling, for instance, in Bayesian networks, and there it just becomes very transparent how
*  knowing the statistics of lower levels in the network enable you to create useful representations
*  at the higher order level. Yeah, I would think very similar. Likewise, I think the model,
*  there is a distinction between an explicit model based kind of representation. And I think that
*  might be where our views diverge a little bit. I think the higher order mechanism does not really
*  have to have an explicit model, as if you just record all the neurons from the whatever mechanism
*  and circuits responsible for the higher order stuff, you don't necessarily be able to extract
*  the whole model from there. And rather, it might be just a more implicit procedural mechanism
*  that somehow can refer to the first order sensory activity and through some sort of downstream gating
*  mechanism, essentially decide that the first order sensory activity is reflecting the world
*  right now, or is it just something else? Is it just noise, etc. So to me, to me, it's more
*  deflationary, the higher order stuff, I wouldn't call it an explicit model person. So it's kind of
*  so like V1 isn't necessarily modeling the incoming visual information, but it is but the visual
*  information is captured within V1. Is that the same then the second order representation is
*  capturing the incoming, you know, visual information that's processed over, you know,
*  a few different layers in cortex. We're talking about visual awareness now, of course. So that's
*  so like like V1, V1 doesn't have a model of the world. Is that analogous to the higher order
*  representation not having a model of the earlier first order activity? I think so. I'm
*  I'm saying sometimes I got into trouble saying that because they also other people would say,
*  well, then extra striated areas receives input from V1, right, then is extra striated area
*  a representation of V1? I would say in that case, no, because to extra strike content to the extent
*  that you try to understand content is challenging. But I think more people would be inclined to think
*  is more appropriate and more useful to think of extra striated areas as still referring to features
*  of stimulus in the world. So extra striated areas are not about V1, it receives input from V1,
*  but it's ultimately still about the things in the outside world. Whereas I think the high order
*  mechanism is ultimately really about the first order sensory activity. It basically tried to say
*  what this activity is truthfully representing the world right now. And this is just noise,
*  this is just my imagination, etc. So it's about the nature of the stimulus, not the stimulus,
*  but about the nature of the active the first sensor activity itself. So even with this aboutness,
*  you don't have to build an explicit model, though, you can just function as if it is about
*  the first order sensory activity. But there does need to be a pretty clear separation and processing
*  them. The reason why I ask is because, you know, my recent very amateurish thoughts about the recent
*  shiny light in my head is, you know, let's say you had like whatever brain you have, let's say you
*  have one that's not elaborated as much right doesn't have a granular prefrontal cortex or
*  something or the newer elaborated prefrontal cortical areas, but it ends, let's say it ends
*  at V2, right. And so then would you would you be would you have subjective awareness of, you know,
*  the contents of V1 if like a brain ended at V2, for instance, and what you're saying is, no,
*  I mean, this is an impossible question, obviously, but what you're saying is, is no, that that. So
*  my idea, right, is like, wherever the brain ends, is where it's going to loop back around,
*  and then it's going to be about, you know, where that recurrence begins, right. So if you're,
*  if you just have V1, then you're about brainstem, right, just those low emotions, right. And there's
*  some phenomenal experience, blah, blah, blah. But what you're saying is that there needs to be some
*  sort of clear distinction between the sensory first order representations and the higher order
*  representation. Yeah, I would think so. I think you're Yeah, so basically the answer directly,
*  I think if you just have V2, we would say that you probably won't be conscious per se.
*  Damn it, this means I have to go on to a new shiny idea now.
*  I mean, I'm wondering whether one useful way of thinking about the difference between just a
*  hierarchy or not just but but say the perceptual the ventral stream, for instance, where there's
*  multiple areas that are in a quasi hierarchical arrangement. And what we mean by higher order
*  representation is that there's an important aspect to those higher order representations that
*  are that are tracking something second order, which I think is the connection here to
*  metacognition. And I, you know, I know you had Megan Peters on recently, and she's been working on
*  similar ideas that you know, one key aspect of the computations that support subjective experience
*  is this ability to track confidence in first order representation. Yeah. So there needs to
*  be some aspect that's tracking the second order statistics of these first order representations,
*  it can't just be, you know, the next level in the hierarchy, receiving input from lower level.
*  Do you guys think about, you know, the minimum necessary conditions to call something a higher
*  order to have, you know, to have some area that is about some first order representation is deep?
*  Do you think in those terms? Or is that is is a minimum necessity? Is that still kind of a fuzzy
*  notion? Yeah, I would say it's still quite graded. And I don't think there's going to be a sharp
*  dividing line. I think there's going to be more, more elaborate, second order representations that
*  can, you know, form. I think one important aspect in the way that we've been modeling or thinking
*  about modeling this is the notion of abstraction. If you can have a higher order representation
*  that's tracking very abstract facts about the system. So not just say, individual aspects of
*  perceptual content, but something about the signal to noise statistics across the whole system.
*  And that provides the useful background conditions to track what hack on was talking about, like,
*  am I imagining something or am I perceiving it? So it's those kind of abstract,
*  second order statistics that I think are important. No, I think my view is very similar. I try to stay
*  away from these like very sharp, hard and fast logical terms like sufficiency and necessity.
*  Sometimes we don't really use them right. And they're too rigid. But basically, roughly,
*  my view is similar here with Steve. Well, why don't we talk about both of your recent accounts
*  of, you know, your recent modeling accounts of, of what's going on here. And then and then we'll
*  bring it into bring in some AI fun as well afterwards. So, how quads since you, you know,
*  you've already mentioned a few of the ideas that you've presented in your reality monitoring
*  account. Maybe I'll just leave it open to you. Can you describe, you know, this general adversarial,
*  generative adversarial network reality monitoring account of consciousness that you've proposed?
*  Yeah, so I always see that my my views are almost never original. And it's basically just some
*  variant of David Rosenthal's view or some other high order theories that we've kind of stolen from
*  or borrowed from from philosophy literature. But my my job, I feel has been to try to express those
*  ideas in more mechanistic terms, by mechanistic meaning more more in actual neurobiological
*  implementation of some sort of algorithm. So, so I so a lot of people sometimes misunderstood the
*  high order theory as if also you have this little thought in your head about you you being in a
*  certain state. And so that seems to require a lot of cognitive demand for you to be conscious. So,
*  you have to be capable of having thoughts and but but if you really read into the literature,
*  they don't really mean that what they meant is just they needed a word to say some representation.
*  And I think Rosenthal in particular argue where the representation should be more thought like than
*  perception like, and some other people would think actually it's more perception like so it's like a
*  higher order in a sense, looking at a first order sensory activity. And I and I find that
*  more appealing, but I'm just not so sure about the thought and, and perception distinction anyway.
*  And as a non philosopher, I'm not obligated to resolve all these issues from Emmanuel
*  Kant about these distinctions. So I just thought, well, what do I think about in terms of neural
*  circuits, what it does? And presumably it does something like when you have a prefrontal circuit
*  that monitors your first order sensory areas, and then signals to itself, to oneself, what that
*  you are actually having a reliable and legitimate first order sensory activity, what does it do?
*  And the first thing that came to mind is, well, you need to do that to distinguish between your
*  self generated imagination versus your external externally stimulated sensory perception,
*  because they activate pretty much the very similar neuronal population and create not identical,
*  but but highly similar activity. So almost like there is a need for your brain to resolve the
*  ambiguity. When you're imagining a cat, you shouldn't hallucinate cat being out there.
*  And you also see sometime it breaks down when you hallucinate or when you dream exactly you
*  mistook your, your own internal, internally generated activity as if it's triggered by
*  the external world. And so that I think might be what the high order mechanism,
*  whatever is thought like a perception like it's doing. And once you think about that, then, then,
*  then we can borrow some lingo from from current AI turns out that actually Sam Gershman at Harvard
*  also wrote about this kind of stuff. He's also recently been on on this podcast, but he published
*  way too many papers were good papers. He's so prolific that he probably never
*  didn't get to get to this point. But he also made a similar point that as in my preprint,
*  that presumably to for your brain to to be able to do predictive coding, you need some engine like
*  that, because that's an engineering argument. So in the past decade or more in the AI literature
*  that has really exploded. A lot of people are realizing that while the the older feed forward
*  only neural network models are great, but they're not really sufficient. And we should look into how
*  people do things in the brain. And of course, our brains are capable of something akin to
*  predictive coding, we have top down processes. So people then start to build in those top down
*  processes in the in the network models. And then they kind of hit a little bit of a hurdle at some
*  point because they realize that you can you can engineer those feedback connections and try to
*  make it like like having top down processes. But the problem is training those networks take a lot
*  of time. So then the generative adversarial network becomes a trick that Ian Goodfellows
*  come came up with, I think, in his PhD or something like that. And and he he suggested,
*  well, actually, one very easy way is to build your top down generative model. But alongside build
*  another thing called a discriminator. And the discriminator is kind of like a critic to the
*  generative model. So when the generative model generate a top down, initiated representation,
*  that is like an imagination or imagery, then you have a discriminator that tried to look at it and
*  say whether it's good enough. So essentially, the discriminator is like a forgery detector.
*  If it looks at your imagination and say, well, your imagination is no good, it's nothing like a real
*  externally triggered real thing at all, then it would penalize the generator. And if the
*  discriminator fails to catch the forgery, then the then the generator would then win a point over
*  the discriminator. So then you pick the two of them against each other, and then they would
*  actually compete. And then they as they compete, they would both learn from each other. And then
*  they would both grow very fast, kind of like rivaling siblings. So in that sense, it becomes
*  an engineering trick just to train the networks. And you can borrow from that idea, then presumably
*  our brain in order to have predictive coding, if it's not entirely genetically hard wire,
*  then maybe we have a similar engine too. And Sam Gershman and I both feel well, then that might
*  be exactly the prefrontal cortex function is to I mean, part of some circuits in the prefrontal
*  cortex might be exactly playing this discriminator role to look at your own sensory activity and say,
*  well, this looks like imagination and it looks all it doesn't look like imagination. It looks like an
*  externally triggered representation. And it has a function of then stimulating the growth of your
*  predictive coding capacities and also then allows you to not confuse imagination with reality.
*  And so that's a kind of long roundabout way of saying seems like a lot of modern concept,
*  but it's really just a way of saying where how does the higher order thoughts or how the
*  perceptions in the philosophical literature, how did it came about? Also, maybe there's a
*  very concurrent neuroscience, neurobiological story to that.
*  I just had the thought where you're speaking about the discriminator and generator, and I don't know
*  the answer for this in AI. And by the way, I don't know if it was Ian Goodfellow's PhD work,
*  but I know that he and his friends were out for beers and they conceived of the idea. And then he
*  went home that night and wrote it up and thus that was born. It was so it was one night.
*  Anyway, that's what you can do when you're in AI. Don't you want to be in AI instead?
*  You just go code it up. But anyway, I thought about the sort of the granularity of both in the
*  again, generative adversarial network and in our subjectivity, like so a waking hallucination,
*  right? So you're kind of cut off from the incoming stimuli or your, I guess you could say your
*  discriminator is rejecting the generators input, right? Or not? I don't know, how would you say it?
*  Is it rejecting the generators in an awakening hallucination? Is the discriminator rejecting the
*  incoming input or just cut off from it? So yeah, I would think that in that case,
*  I would say the forgery detector, the discriminator gave your generative model
*  such an easy pass. So your generator is just generating something dodgenously that doesn't
*  look like external trigger input at all. It should have been easily spotted as a self-generated forgery,
*  but your implicit metacognition that is your discriminator presumably went to sleep
*  and over lunch and just gave it an easy pass and consider it a very obvious forgery as real.
*  And that's why you hallucinate. And in that case, though, you have to because hallucinations last
*  more than 300 milliseconds, right? So then the winner of that battle has to be granted
*  the winning spot for some time for hallucination to come to completion. So there must be some,
*  once you cross that threshold, it must be resonant at those levels. Sorry, I feel like I'm
*  being really unclear because I guess when you're talking about unclear topics and I don't have the
*  good right vocabulary, then it just sounds like a mess anyway. But does that make sense to you?
*  No, it does. It is quite tricky. And to be fair, it hasn't been explained very clearly in the
*  literature. It's our job. We haven't done our job well. It's quite confusing. But I would think of
*  the yeah, you're right. So basically, we're saying that during your entire dream or your whole
*  episode of hallucination, your discriminator will be failing his job almost consistently.
*  It just suddenly, presumably some mechanism there is not working properly. It gave us such a low
*  threshold for considering what is it to be a legit representation of the state of the world
*  right now. And some people would think it's very implausible, but actually it fits a little bit
*  with the known physiology of dreams, right? So people would sometimes say that in dreams,
*  your prefrontal cortex seems to be not very active. And sometimes it's being used as an argument
*  against high order theories and so on. Because your high order thought theories, and for you to
*  be conscious, you need the prefrontal cortex to be active. And I would say, well, actually, no,
*  if you think about it exactly, in this case, we would want the prefrontal cortex to be not working
*  properly for a long period of time, because you have a failure of implicit metacognition.
*  So the low activity in prefrontal cortex actually should count in our favor. At least I would think
*  so. Okay. Okay. All right. Well, let's go ahead and Steve, let's bring in your higher order
*  state space model here. So this is the idea that you have that higher order representation
*  is basically the output or the state of a generative predictive model. And you can
*  correct me what I just said. And then I'd love you just to describe that work.
*  Yeah. So I feel like we're kind of creeping up on the higher order view from a different
*  direction. So our strategy on this was to work backwards from the properties of what we can
*  measure in the lab, which are these subjective reports of awareness. And these kind of reports,
*  you know, you come into one of these experiments, you might be flash stimuli, and you can use things
*  like masking or flash suppression to control whether people perceive things or not, then
*  you might be asked to rate your awareness of seeing things on some scale. And the interesting
*  thing there is that you can create situations that look like the stimulus is being processed
*  to some degree, affecting behavior in various ways, but people still are unaware of it sometimes or
*  aware of it other times. So there seems to be some property of awareness that's dissociable from
*  the general job of perceptual processing. And one way of approaching this is to think, well,
*  what people are actually reporting in those kinds of experiments, what the data we can gather on
*  consciousness in the lab is basically a factorized representation that they can apply to all different
*  types of content. So I can ask you, are you aware of the dog? And you can respond to that. I can
*  equally ask you, are you aware of the Gabor patch? You can respond to that. So what I mean by
*  factorization is there is some property of awareness that you can interrogate, you can
*  compute over and respond accordingly. And you can apply that to tell me about your awareness of all
*  manner of things, perceptual things, your memories, your emotions, and so on. And so what is interesting
*  about this is when you start thinking about awareness as being this factorized state in a
*  in a generative model is that the state space becomes very asymmetric. So what I mean by that
*  is that by definition, when there's the absence of awareness, there's also the absence of
*  perceptual content lower down the state space. So another way of saying that is like being aware
*  of a red thing is a similar state as being sorry, being unaware of a red thing is a similar state
*  as being unaware of a blue thing or being unaware of a dog, for instance. So this this kind of
*  state of being unaware of things is asymmetric to the state of being aware of things. So that all
*  sounds quite lofty and philosophical, but when you start writing down a model like this, basically
*  what you can do is treat this state, this aspect of the generative model as the most abstract level
*  of the system. So it's effectively creating some higher order commentary on whether there is content
*  lower down the generative model. And effectively what it's doing is kind of tagging the situation
*  in the perceptual generative model as having signal in it or having nothing in it. And so this is
*  where I think there's interesting commonalities with that one's view, because there's this idea
*  of some kind of higher order monitor that's tracking whether there's signal or noise lower
*  down the system. And so this then allows us to create various empirical predictions
*  about the existence of this symmetric higher order state, which should track both commentaries of
*  things being there, of seeing things, of perceiving things, but it should also symmetrically track
*  our comments of being unaware of things. And this is where we diverge from, say, the global
*  workspace theory. So global workspace theory would say there's some kind of threshold where
*  when you become aware of things, you get broadcast through the brain. Whereas in our model, you need
*  these higher order states to be tracking not only the broadcast of content, but you also need them
*  to be actively representing the absence of content lower down the system. So it's these
*  active representations of absence that we've been working on. And this is work that's been
*  done by my PhD student, Matan Mazur. And there's interesting data suggesting that in these prefrontal
*  regions, both in monkeys and humans, there are these neural representations that actively
*  represent the absence of stimulation. And that would be very consistent with this idea that you
*  have higher order states that are tracking the properties of lower order generative models.
*  So it's not tracking the content within the absence. So it's not tracking the
*  infinite amount of things that could be present. It's just tracking that there is an absence
*  That's right. So one prediction that we're trying to design experiments to test at the moment is
*  that these, you can think of them as kind of low dimensional abstract codes of whether things are
*  in the lower order generative model. And one prediction is that those codes should generalize
*  over content. So my representation of the absence of red should be similar to the representation,
*  the absence of blue and so on. And so we're testing those predictions in
*  imaging experiments at the moment. So maybe Steve, since you're the last one to describe
*  the model that you've been working on, what do you hate about Haakon's ideas here?
*  I just, I kind of want you guys to go back and forth and I'm sure you've done this before because
*  you know, you're friends. What do you see as sort of important differences and maybe some of the
*  similarities, you know, between your higher order state space theory and Haakon's reality monitoring?
*  Yeah, sure. So I think actually we're missing a key aspect of Haakon's model, which is this
*  ability to distinguish reality from imagination or top down generation from perception. So
*  we're working within this hierarchical generative model. It's a Bayesian network,
*  but we can run it top down. We can kind of allow the model to quote, imagine.
*  Hallucinate and it won't tell the difference from perceiving. So we're actually working on this. So
*  a postdoc in my group, Nadina Dykstra, she's working on extending the model to incorporate
*  this idea that the abstract representations, and this connects back to what I was saying earlier
*  about the need to have second order statistics represented at these abstract levels as well.
*  So if you can represent the precision of lower levels of the system or something about the
*  noisiness of that signal, then you might be able to not only represent are you seeing something or
*  not, but you can also represent another dimension, which is not only am I seeing something or not,
*  but is it perceived or imagined? So you get this kind of 2D higher order abstract representation.
*  And that is something, you know, we're borrowing heavily from Haakon's model and hopefully we'll
*  be able to expand the higher order state space 2.0 will hopefully have exactly that kind of
*  aspect to it. Haakon, do you agree with that assessment?
*  Yeah, totally. I feel it's exactly the same way that I think when we do models
*  of a realistic circuit, we usually focus on one aspect or one task, and then we build a model and
*  then the model can only do that one task. So in our case, our model, we haven't actually,
*  we've been building it, it hasn't been very successful yet in the actual implementation.
*  So Taylor Webb in my lab has been doing these kind of neural network modeling.
*  But I think we'll exactly get to the same point that when we can actually get the
*  reality perception distinction, then we will start to worry about the other aspect,
*  which we've been building a different model in parallel because of my formal work in
*  metacognition. We also want to know when does the higher order state decide that there is
*  nothing out there, no meaningful information, everything is just noise, or when does it have
*  meaningful information. So ultimately, I think the higher order state has to do this kind of
*  at least three option distinction, whether it's just noise or whether it's internally generated
*  or externally triggered. And I think eventually our two models presumably will then at that point
*  converge and become something very similar. This kind of comes to the point of modeling
*  the broader aspects of modeling and having, is it likely that we will have 40
*  models, a family of 40 models that together account for let's say consciousness, or do we
*  really need to combine them and build an uber model? Because like you just said, you have these
*  models that account for very specific things. So why not have 40 different models accounting for
*  the 40 different specific things that don't actually need to be joined up to serve as a
*  satisfying explanation for consciousness? I mean, I think a lot of the cognitive models
*  in the literature at the moment are, there's more commonalities there than there are differences.
*  So this is something we write about in our higher order state space paper that a lot of the work on
*  the initial data that kind of prompted these global workspace frameworks from Stan to Hen's lab.
*  We can think of that in a slightly different way, which is rather than it being broadcast
*  through the system, this is reflecting prediction errors at lots of different levels of the network
*  when it concludes that it's quite seeing something. And so in a way, it's just a different way,
*  a different mathematical way of formulating the same idea that you need something that's global,
*  something that's abstract, something that's hierarchical. The kind of labels that we attach
*  to models in different papers, I think are less important. I think it's the concepts that underpin
*  them and there's a lot of commonalities in the literature. Let's talk about AI consciousness.
*  Let's go and bring it in, I suppose. So Haakuan, a few years ago, you wrote a paper with Stan,
*  DeHaan and Sid Coyter called, What is Consciousness and Could Machines Have It?
*  That includes some of the reality monitoring ideas, but also kind of combining higher order theory
*  and global workspace theory. And Nicholas Shea has written about this as well. So maybe even before
*  that, I should start. Neither of you have an issue with the idea of developing consciousness
*  in machines necessarily. It's possible, correct? Are you both pro consciousness and machines?
*  Haakuan Wang Yeah, I think in the way that we are committed to a cognitive approach, ultimately,
*  that's the kind of bullet that we have to bite. If you think the cognitive neuroscience of
*  consciousness could be complete, that must mean that some algorithm implemented by biological
*  machines would be sufficient to describe the consciousness. And then to the extent that they
*  can be implemented in biological machines, it's very likely you can find some other substrate
*  to implement it. So in that sense, some sort of robot would do the same algorithm and that if
*  that's ultimately what matters, then it is a very strange and unsettling entailment implication of
*  the theory. But I think we have to accept that to the extent we're committed to this kind of approach.
*  And Steve, you're just pro?
*  Steve McLaughlin I mean, yeah, I would agree with that, that I think a consequence of the cognitive
*  approach is that you accept a broadly functionalist view of how it could be implemented. I think,
*  though, that the kind of unsettling aspect, it comes back to this idea about what our intuitions
*  are for whether experience is something magical and holistic that can't be broken down into its
*  component parts. And I think once we make progress on the kind of computational cognitive
*  theories of consciousness, then the aspects of that functionality that we might think are useful
*  to have in machines will become will seem less magical, I think.
*  Mike McDonald Yeah, there's an overriding overarching
*  desire to make things less magical, I think, in the science of consciousness in general,
*  in the neuroscience of God, which I think is a great thing. I mean, because we don't want it
*  to be magical. So Ha Kuan, I alluded to that paper, where you guys talk about potential
*  computations from higher order theory and computations from global workspace theory.
*  Each of those has computations that might be important to implement consciousness in a machine.
*  Do I have that right?
*  Ha Kuan Yeah, more or less. So the co authors are
*  Stanley N and Sikhu there. And we published the paper, I think 2017, if I'm right, or 18.
*  I come to think that the paper is not successful. In some ways, the paper is a way to reconcile my
*  kind of view with Stan's. And I think we agree that so global broadcast and the kind of implicit
*  metacognition that I'm going for are two different components. And we agree that it's kind of
*  orthogonal, but I think we never fully agree which one is more priority, which one is kind
*  of later stage. Yeah, which one is the which one is more primary. So I like to think that the
*  implicit metacognition is more primary. And then from the implicit metacognition, that mechanism
*  of that, you know, reality discriminator, or the high order engine, then signals what kind of first
*  order information should be broadcasted and how they would impact our latest stage, high level
*  cognitive reasoning, belief formation, etc. So the implicit metacognition is the more primary is
*  the gating mechanism. If you like, it's the is the is the mechanism of consciousness then on that
*  view is exactly at the interface between perception and cognition. So I would think global workspace
*  is a great account of higher order cognition, but has maybe not as much to do with the raw
*  subjective experience per se. Subjective experience are what is being gated at this interface
*  for perceptual signals to enter higher higher order cognition. I think we we try to dance around
*  a little bit. I think Stan has that slight different idea. Stan probably think that
*  global broadcast is more primary. And from global broadcast, and on top of that, you can have
*  explicit metacognition. I kind of agree, but I don't think explicit metacognition is really what is at
*  the heart of the problem of these kind of subjective experience issue. So there we have a bit of
*  disagreement, we kind of dance around a little bit. And in the paper, I think we did an okay job
*  that basically when you have three authors who each have their own views, and we end up and also
*  writing for science, we have to be reasonably accessible, it cannot be kind of this kind of
*  nitty gritty arguments back and forth. I feel like that comes through. That comes through in
*  the in the writing. It's yeah, you can kind of tell. Yeah, yeah, I feel that I feel a little bit
*  contrived and constraint there sometimes. But I think we did we did as much as we can, given our
*  different views. But I think, despite that, I think even if you let me go back and edit it,
*  have a final word, and I added that paper and take out what I don't agree with Stan and etc.
*  And have my have my say, I think that would still be unsatisfying. I come to think that maybe one
*  extra ingredient is needed. In that paper, we never really talk about the nature of the first
*  order representation. And that's where I my maybe lack of complete loyalty to the higher order cam
*  is coming through. I think the higher the mechanism is ultimately important in the form of an implicit
*  metacognition. But I think the nature of the first order representation is also important.
*  In particular, we need something what I call analog first order representation. So if your
*  first order representations are just digital, they're just like this signal signals red,
*  this signal signals green, this signal signals blues, etc. Then you won't really have qualitative
*  experiences, even if your your higher order engine refers to the first order activity and say, okay,
*  this red signal is now correctly representing your world right now, then I think you will be
*  aware of red, but you will not have qualitative experience. And I think the qualitative experience
*  probably come about when you have a kind of analog representations. When you know that red is more
*  like pink and orange and purple and brown than silver and gold and black and white. So you have
*  this kind of a graded kind of color space in your head, you know that some colors are more
*  similar than each other, some colors are more different from others. And that again, is borrowing
*  an idea from the philosophy literature, sometimes called the mental quality space theory. And so
*  essentially, you know what it is like to be to be seeing red, because when you see red is redder
*  than everything else you've seen. And red looks the way it does, because it looks kind of pinkish
*  and orangish and, and not so bluish. So you have these kind of automatic similarity relations
*  encoded in the in the in the representation in your repertoire. And we we have actually through
*  this kind of modeling exercise, we have fought it through and probably if you have first order
*  sensory analog signals, you can almost get it for free. And I think that would never emphasize that
*  point. I think I come to think it's maybe the most important when it comes to building conscious AI,
*  we never emphasize that in that paper. Sorry, I just dropped a completely new new idea that I
*  probably haven't expressed it in print yet. No, no, that's okay. I just don't know where to go.
*  It will be in my book, but but my book will not be out for a while.
*  Are you writing a book?
*  Yeah, I have a but it's a monograph is not a pop. It's not a trade book. Okay. So I so I yeah,
*  I will have a monograph contracted with OEP hopefully out this year or next year.
*  Oh, okay, great. Well, by the way, congrats to both of you on the book. Steve, I didn't say
*  congrats. That's it's awesome. Thanks. So I recently bit the bullet and, you know,
*  consciousness in AI is becoming a more popular topic, I suppose I recently bit the bullet and
*  watched a Joshua Benjio talk about the consciousness prior, and his idea that basically,
*  if you just add attention in various forms of it to deep learning, that is the key to develop
*  consciousness in AI. And he relates this to the system two of Daniel Kahneman's system one and
*  system two. I just I kind of want to, I've been avoiding it because it's just kind of hyped and,
*  and I watched the, you know, video, I never read the consciousness prior paper, but my,
*  my biases were correct. It's hyped is basically what I might what I conclude here. But I'm
*  wondering if you guys feel the same way, like what your take is on the, the consciousness prior
*  of Benjio, and maybe not necessarily even that in particular, but just in general, this idea of,
*  of this recent push about building consciousness in AI and using the deep learning framework to
*  do it and so on. I mean, I don't. So I think inevitably, whenever you use the two words,
*  consciousness and AI in the same sense, it's going to get hyped. Right. So I, but I do think that
*  the proposal from Yoshua Bengio is highlighting some important ideas that, you know, are shared
*  with general higher order approaches, this idea we need to create abstracts, communicable
*  representations about our mental states, I think is-
*  That's not a new idea, though.
*  Sure.
*  Right. I mean, so I just want, I don't want to, I don't want to like downgrade
*  Yoshua Bengio, but I just, I just had the thought like, is this a case of the guru and back
*  scratchers, you know, and, and I, so I don't want to single anyone out, but-
*  Yeah. So I mean, okay. So if it, yeah, if it's a sociological point about overhyped,
*  one particular paper being overhyped, then I don't know, I don't have a strong opinion on that.
*  By the, the broader point about thinking hard about the functionality, you know, why
*  we would need to have something that looks like awareness in AI.
*  Yeah.
*  Why would we want it in the first place? So deep learning is being fantastically successful
*  in lots of domains, but it doesn't look conscious in any sense of being able to
*  comment on its processing, explain what it's doing and so on. So I think when we start
*  thinking about those things, like what is it that consciousness enables us to do as humans?
*  The function of consciousness.
*  The function. And I think one aspect that is often overlooked here is, is the social function. It's
*  the ability to compactly converse like we're doing now, share ideas about particular topics. Now,
*  obviously that's very, very abstract, but just at lower levels, you know, the ability of two
*  agents to comment on, have I seen something clearly over there? Do I think it's a predator
*  or not? And so on. You get down to this notion of kind of sharing of metacognitive representations
*  that Chris Frith has written a lot about. And people like Bahadur Barami have done really
*  beautiful experiments showing that when you have two people who are sharing metacognitive content,
*  they can like their confidence in simple perceptual decisions. They can reach answers
*  jointly that are better than any one of the two of them could have reached alone. So there's this
*  notion of kind of pooling metacognitive information is really important for the function of social
*  groups. And so I think that kind of aspect, that more functional aspects of awareness
*  is going to be useful for to think about how could we get that into AI. So we're actually doing some
*  work in collaboration with the Oxford Robotics Institute. We're just getting this started to try
*  and think about these questions, like on a practical level, how could we build in say abstract notions
*  of am I confident about that? I know what I'm doing in a robot and can it communicate that to its human
*  companion? It's collaborator. And would that be useful for and so obviously this is quite a long
*  way away from thinking about intrinsic subjectivity, but it's a more functional notion of awareness that
*  I think is important to think about for AI as it becomes more and more integrated into society,
*  we're going to want the kind of abstract interactions that we have with each other
*  to also be present in AI system. So you write about this and kind of as a response to
*  another paper that is talking about the need to set out some of the guidelines for what would
*  actually constitute a good science of consciousness. And within this paper you talk about what we need
*  to also think about the function of consciousness. And this is where you bring up the idea that it's
*  important for social interactions. And you just talking about that actually reminds me some of the
*  early work in like infants and how infants use imitation to learn. So even an infant,
*  and I'm not talking about infant consciousness here, but can tell when someone that they are
*  preparing to potentially imitate, they will imitate them if they seem like an expert,
*  if they seem like they know what they're doing, you know, trying to open the door, for instance,
*  or something like that. That's when they'll discriminate between imitating someone when
*  they do seem to know what they're doing versus when they don't seem to know what they're doing.
*  So socially that seems like in the same sort of wheelhouse there, because you do need to know
*  whether that person seems to be able to, you know, know how to open a door. If they don't,
*  then imitating them knocking their head against the door or something is not going to help you.
*  So that does seem to jive with the shared experience and coming to an overall better
*  conclusion functionally using a social mechanism. Yeah, I mean, there's a potentially beautiful
*  symmetry that I think is still more of, you know, on the hypothesis side than supported by
*  large body of evidence, but there's a potentially beautiful symmetry between
*  awareness of our own mental states, metacognition of ourselves and,
*  you know, mentalizing about others. And I think in infancy, this is a really fertile period for
*  when you need those kind of dual self-modeling, other modeling things to interact, right? So
*  not only do you need to know what other people know, you need to know when you don't know to
*  ask for help. So the idea that you hit the limits of your own ability and you need to then turn to
*  some adult who is competent to help you and you need to pick the right person. And so all of that
*  seems like foundational to bootstrap yourself up towards, you know, becoming a functioning member
*  of a human social group. I have such an individualistic bent that I'm always resistant to the
*  social account of the functionality of consciousness. And I feel like I'm just giving up
*  more and more and relenting and seeing the value in it. And maybe that is, you know,
*  a theory of mind and the usefulness of being social. It probably is just because I'm so
*  reluctant to accept it, you know. I'm just very individualistic. Do you guys feel that too? Like,
*  I want consciousness to be about me. I want it to be an individual thing, you know?
*  I may be more like you. I'm a loner. I'm a socially awkward loner.
*  Oh, I'm sorry. You're calling me a socially awkward loner. Okay.
*  But I'm saying I am. I didn't say you are.
*  No, I'm like you in the sense that I also think the functions of consciousness
*  actually relates back to this issue in a sense that I think I don't think the whole enterprise
*  of AI consciousness is overhyped. I feel that if you particularly want to talk about Yoshua
*  Benz, I think he behaved very well. I mean, in the sense that he wrote the paper that he cited the
*  right references. He didn't just jump straight to New York Times and wrote a high profile piece,
*  which actually sometimes happens in our few, as you know. So I think the way that he's overhyped
*  is just because his statue is so high that people really look up to what he does in AI field.
*  Not his fault.
*  But I think it's not his fault. But on the other hand, I see that where you see it may be
*  unsatisfying. Probably that comes down to the fact that in our field, we kind of treat the
*  deepest issue in consciousness research is about explaining subjective experience.
*  It's about explaining the qualitative aspects of subjective experience or what it is like to
*  have an experience. But all these other aspects that relate to attention and higher cognitive
*  control, explicit mental cognition, they are important too. And so as like helping others,
*  interacting with other social agents of your member group, etc. And I think all these are
*  important. And so they are not overhyped per se. But I think there's a bit of lack of satisfaction
*  for people who are in this field and field. But how does it explain the qualitative nature of
*  the experience? But I think that AI is not completely silent about that either, as I
*  mentioned earlier, I think there are AI models that could have focus on that a bit more. I think
*  the problem is that the appeal may not be so clear, right? So most of us thinking about
*  consciousness, thinking about building AI robots that will be conscious, we are mostly thinking of
*  having them that can do these higher cognitive functions better, they can play better chess,
*  they can be better childcare takers, and that sort of stuff. But I do think there is some work from
*  the computer science literature that actually I find inspiring. It's actually a colleague in the
*  UC system at UC San Diego, Senjoy Dasgupta. He's a computer scientist who had looked into the
*  very low level sensory systems in let's say fruit flies. And then he's been learning also what does
*  that algorithm do. And that's where earlier I mentioned the analog signal comes from. And
*  as a computer scientist, he found that actually the fruit fly has a coding system that seems to
*  be very analog and very, for lack of a better word, very smooth and mixed. That is the different
*  labeled lines are not independent lines. So that is very different from let's say the Mantis Shrimp
*  color vision system. The Mantis Shrimp, as I remember, has over a dozen different color channels,
*  but they're kind of independent. So it turns out that the Mantis Shrimp has more photoreceptors
*  types than we do, but they are not very good at discriminating between the colors because they
*  act as almost like independent detection lines. Whereas in the fruit fly, the humble fruit fly,
*  actually they have a more mixed model that is like opponents and in fact, it's very randomly
*  projected. So you have a very mixed, almost analog and spatially smooth code for all fraction. And
*  then Senjoy looked into what it does. And it turns out that this is actually a very good mechanism
*  for two kind of difficult computer science problems. It turns out that if you just mimic
*  the fruit fly system, you can actually outperform current or at least some current popular computer
*  algorithm. So there might be some functional difference. Sorry, taking this a bit in the
*  opposite direction of what Steve was going. I think there are these high order social and
*  high cognitive functions that are important, but AI might also contribute to the more simple,
*  qualitative subjective experience problems too. And meanwhile, discover functions for those things.
*  I guess my worry about going, I mean, that's fascinating about the fruit fly. I didn't know
*  about that work. I guess my worry about going down the road of thinking that quality spaces is
*  sufficient is that we're ending up in this realm of not really being able to know and not being
*  able to really test what, you know, if we built that kind of system, would it be aware? Well,
*  it wouldn't necessarily have the ability to comment on its experience and so on. So I don't know.
*  Do you think, Hakon, then that the quality space is the more important thing or the higher order
*  representations are more important? I don't know. This question is very vague, but.
*  Actually, just basically following the same playbook from David Rosenberg, my
*  favourite philosopher to steal ideas from, I think you need both. So in a sense that so
*  David Rosenberg also has his own version of mental quality space. So the idea basically being
*  your higher order theory explains your awareness of your sensory signal or your awareness of your
*  sensory process. So you have awareness, but the content may not be qualitative unless you also
*  have mental quality space. So combining two, then we avoid this kind of problem about whether you
*  can just build a little gadget with a mental quality space with analog signals. In that sense,
*  you would say, well, this creature is capable of potentially having qualitative experiences,
*  but without awareness, it's not going to fly. So if you have both, though, then you don't have
*  this problem of not knowing because you can then ask this creature to do these tasks. Basically,
*  show them pink and ask the creature to tell me what it is like in terms of coming up with five
*  other colour that will be similar and five other colours that would not be so similar
*  or do novelty detection. So these will ultimately be cognitively testable tasks.
*  Guys, as you know, I have about a thousand other questions, but I see that we're coming up on time.
*  So I want to bring it back out. Recently, I've talked about the difference between academia and
*  industry and the need for academics to secure their legacy. And I'm wondering if you guys think,
*  are consciousness researchers, do you think that they're more or less concerned with their legacy
*  relative to other, let's say, neuroscience and psychology researchers studying more,
*  quote unquote, tangible things? My guess is they're less concerned with their legacy.
*  What do you think? I actually think it might be the opposite.
*  Yeah, I think that maybe historically they've been more concerned, but this comes back to this
*  problem about people only feeling like they can turn to it later in life. So I feel like,
*  in a way, the causality is reversed. People who are concerned with legacy, maybe then think,
*  okay, let's try and tackle this problem. And if I solve it, then my position in history is secure.
*  Whereas I think now that, like I was saying earlier, now that there's more
*  younger people getting into it and making it their field of study and realizing all the problems
*  that come along with it, they're sufficiently humble by that to not be any different to any
*  other branch of cognitive, I think. Yeah, I think the same. I think the
*  concern about legacy is there, I think is on some people's minds, but I think it creates an
*  interesting bifurcation. So people are either too shy to get into it, or if they are brave enough
*  to get into it, they probably think that their legacy is already guaranteed. Or they are mostly
*  hoping to shoot for the moon and really even magnify their already great legacy into something
*  greater. I think that is the kind of attitude that may not be so healthy for the field.
*  As I mentioned, Steve and I agree on a lot of things. And maybe that's one thing I like working
*  with Steve is that his more positive outlook often complements my darker, pessimistic outlooks
*  to things. I feel, yes, the few has definitely gone a long way. I think the cognitive approach
*  has been attractive to a lot of people and we seem to be doing work that are deemed useful by even
*  people outside of our immediate discipline. But I'm sort of an activist. I always feel there is an
*  intrinsic vicious cycle there because of this taboo issue and this legacy issue that if we are
*  not careful, we will very easily fall back to what we historically was. And I think now and then there
*  are optimisms. So in the 90s, I think something could happen. Like I said, Francis Crick helped to
*  rejuvenate the field and created a lot of media attention. But as he's no longer around with us,
*  and I think now some of the problems kind of come back. I mean, the media attention that was
*  generated in the 90s now in some ways has become sometimes a bit of liability because of the issues
*  I talk about. People just want to compete for media attention rather than peer respect because
*  peer respect does not really matter as much because when your media attention is so huge,
*  you don't really care about what your critics think about your theories anymore. And I think
*  that happens sometimes in the field. So I think, I mean, I don't want to end on such a sour note.
*  I think things are still going overall pretty okay. Now and then there are problems creeping
*  up. I think if you just collectively keep up the attention and to make sure that we are going in
*  the right direction, we should be fine. But I think this is in some ways really community work.
*  It's not like anyone, one person can fix this like Francis did.
*  I paused in case you had a comment on that, Steve. So don't worry,
*  Ha-Kuan. We're not going to end on such a pessimism here.
*  That's right. Yeah.
*  I mean, I guess I had one. I think people coming into the field are going to be key for this. I
*  think Ha-Kuan rightly so reminds us as a community that we shouldn't be complacent,
*  that we do need to cultivate and mentor people who are interested in this area and
*  for them to gain the right kind of skill set to come into it and be ambassadors for
*  rigorous consciousness science. I think one thing that I sometimes see people getting
*  a misconception over is the idea that because it's the science of consciousness, you can
*  afford to kind of go down the rabbit hole of all the papers, the more speculative end of the papers
*  and kind of wake up every morning and rehash the mystery of subjectivity.
*  And I think that we're going to need people to come into the field who have
*  the training and the skill set in psychophysics, in
*  maths, in computational modelling and so on. So just the bread and butter of doing good
*  cognitive computational neuroscience. So I think if we have those kind of people coming in and we
*  show that we are wanting to do that kind of work, then I think we're going to be okay.
*  I mean, you guys have talked about how there's a community among consciousness researchers
*  and that it's like kind of a family. But like any other science, there's a broad diversity of
*  staunch opinions within that family, like any family, I suppose. And so I'm kind of curious
*  how you view whether people are more prone in the consciousness family to dig their heels in and
*  cover their ears and say, I don't hear you when someone's talking about their favourite
*  theory of consciousness and just dig their heels in, stick with their own theory and to hell with
*  the rest of them. Or is there more of an openness within the consciousness family of kind of making
*  these different theories compatible or either considering the actual merits of other theories?
*  I'm just asking within the consciousness family of science relative to other neuroscientific
*  endeavours where, again, all opinions are pretty staunch.
*  I think my experience about this may be shaped by my subjective experience. I like to think that
*  the field has one unique characteristic that is actually both at once similar to what you described,
*  but there's also an antidote to that. I think it's because we always have philosophers'
*  amongst us and some very good philosophers who actually know the empirical literature like
*  Net Block. And I think that creates always a culture where we almost, like philosophers,
*  we believe in sparring. We believe in really arguing really hard with your opponent. So in
*  some ways we dig our heels, we argue, but we always engage. And I think to the extent that we engage
*  and we listen to our critics, we try to come up with better arguments. I remember as a trainee
*  going to the same conference, ASSC, we all go to, every year when I come back home, I kind of get
*  this very juvenile feeling, or next year I'll have a better reply to that argument. And then
*  you go back and do the work and then next year you see each other again and in fact you present your
*  new evidence and say, last year you asked me this, this year I have to reply. And I think that culture
*  in some ways makes it very almost like, makes things sometimes look sectarian from the outside.
*  But I think actually it's extremely healthy. And I think if we keep that, we should be fine.
*  I'm optimistic about that.
*  Yeah. I think the debate is generally healthy. I think that because it's a young field,
*  there is going to be a natural proliferation of theories, ideas, models. And I get the sense that
*  that kind of ship is turning a corner slightly in the sense that there's more focus now
*  on identifying differences between models and comparing them. So the Templeton Foundation is
*  running these adversarial collaborations where that's been done at a large scale.
*  I think that's a great initiative because that really allows people to kind of, hopefully,
*  start identifying commonalities and differences in a more rigorous way. So yeah, I think we've
*  got to remember that this is a pretty young field in terms of actual empirical science being done
*  at a relatively broad scale.
*  Well, you guys may be in a family. I could have been in your family. Instead,
*  I decided to be homeless. So I get to-
*  You are in a family, Paul. Come on.
*  Oh, that's what I was looking for. Oh, you lie. You consciousness researchers lie.
*  From my homeless vantage point, it looks like a very warm and fuzzy family.
*  Just to end, Steve, let's start with you. I just want to know if you're working on anything in
*  particular that is going to come out pretty soon that I should be looking for.
*  Yeah. Recently, we've felt, I think, pretty energized in our group because we've started
*  to identify ways of testing the higher order state space ideas. Unfortunately, those
*  imaging projects that we got going have been put on hold because of COVID, but I'm hoping that
*  data collection on that is going to be restarting over the summer, and we should have data on this
*  towards the end of the year. Really, this is focusing on this idea of the telltale signatures
*  of a low-dimensional abstract code, a magnitude code that tracks awareness in a way that
*  generalizes over different types of content. We hope to see those signals in prefrontal and
*  parietal cortex. If we can start identifying those signals, I think that will really provide a rich
*  test bed for these models. You're going to be doing some contrastive fMRI analyses on it?
*  Yeah. We're more really looking at representational similarity. Really saying, does the
*  neural pattern that tracks presence versus absence, does that seem to generalize over
*  different types of content? A lot of this, we're working in a predictive coding framework, so we're
*  kind of giving people priors, so giving them cues about whether they should expect to see something
*  or not. Then we also, orthogonally to that, give them cues about what type of content they should
*  expect. One type of cue might tell you, okay, on this trial, you're not likely to see anything
*  at all, but if you do, it will be a house, for instance. We can orthogonally manipulate priors
*  on awareness and priors on content. Then we can start to look at where the prediction errors at
*  those different levels track these different levels of the model. Cool. Haakwan, I know you
*  have a book in the works as well. You told me offline, but that's not going to be out for a
*  little while. What can we expect from you? Yeah. I think for most of my colleagues, the past few
*  years, I think one of my better known piece of work is the neural feedback work that we do in
*  fMRI using machine learning methods combined with online closed-loop fMRI. Basically, we create a way
*  to non-consciously reduce excessive emotional responses or affective responses. We've been
*  using it to, for instance, reduce your excessive physiological responses for, let's say, spider
*  phobia. Before the pandemic, we started this project and collaborated with clinical folks.
*  We actually started to run clinical trials. That, to me, was very exciting because I feel
*  related to the issues we talk about too. For the field to gain more legitimacy, ultimately,
*  making useful clinical application is the way to go. I thought I would just focus on that. We have
*  somewhat been stuck because of COVID. During the pandemic, then I worked on sharpening the methods.
*  I thought, okay, my career would just become honing in on that and just making it work.
*  During the pandemic, I also wrote my book. As I said, the pandemic has done weird things to a
*  lot of us. After writing the book, I feel now more ready to think about the theoretical issues
*  again. The perceptual reality monitoring theory has been extended. We are now more ready to talk
*  about the more raw, touchy-feely subjective experiences. We have theoretical models.
*  I'm hoping to start a new line of basic research again to test that. For people looking for
*  postdoc positions, I hope you will stay tuned and watch for announcements. I should be hiring
*  soon to do also this kind of new work. Very good. Well, guys, this has been a real treat for me.
*  The cycle continues for you. This is episode number 99 here. I'm really glad that I got to spend it
*  with you guys. Maybe I'll have a new 100-episode cycle coming up here. Thanks for spending the
*  time with me and for going so long as well. Yeah, thank you so much.
*  Yeah, thanks so much, Paul. This was really fun.
*  Brain Inspired is a production of me and you. I don't do advertisements. You can support the
*  show through Patreon for a trifling amount and get access to the full versions of all the episodes,
*  plus bonus episodes that focus more on the cultural side but still have science.
*  Go to braininspired.co and find the red Patreon button there. To get in touch with me,
*  email paul at braininspired.co. The music you hear is by The New Year.
*  Find them at thenewyear.net. Thank you for your support. See you next time.
