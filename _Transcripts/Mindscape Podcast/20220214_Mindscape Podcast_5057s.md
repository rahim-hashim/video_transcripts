---
Date Generated: June 08, 2024
Transcription Model: whisper medium 20231117
Length: 5057s
Video Keywords: []
Video Views: 21923
Video Rating: None
Video Description: Patreon: https://www.patreon.com/seanmcarroll
Blog post with audio player, show notes, and transcript: https://www.preposterousuniverse.com/podcast/2022/02/14/184-gary-marcus-on-artificial-intelligence-and-common-sense/

Artificial intelligence is everywhere around us. Deep-learning algorithms are used to classify images, suggest songs to us, and even to drive cars. But the quest to build truly “human” artificial intelligence is still coming up short. Gary Marcus argues that this is not an accident: the features that make neural networks so powerful also prevent them from developing a robust common-sense view of the world. He advocates combining these techniques with a more symbolic approach to constructing AI algorithms.

Gary Marcus received his Ph.D. in cognitive science from MIT. He is founder and CEO of Robust.AI, and was formerly a professor of psychology at NYU as well as founder of Geometric Intelligence. Among his books are Rebooting AI: Building Machines We Can Trust (with Ernest Davis).

Mindscape Podcast playlist: https://www.youtube.com/playlist?list=PLrxfgDEc2NxY_fRExpDXr87tzRbPCaA5x
Sean Carroll channel: https://www.youtube.com/c/seancarroll

#podcast #ideas #science #philosophy #culture
---

# Mindscape 184 | Gary Marcus on Artificial Intelligence and Common Sense
**Mindscape Podcast:** [February 14, 2022](https://www.youtube.com/watch?v=ANRnuT9nLEE)
*  Hello everyone and welcome to the Mindscape Podcast. I'm your host Sean Carroll. If you've been paying attention to
*  advances in technology, science, or just news in the world,
*  it's hard not to be impressed with recent progress in artificial intelligence, mostly driven by
*  neural networks and deep learning, machine learning kinds of techniques.
*  We've really been able to do things with AI that when I was your age we just couldn't do. For one thing,
*  artificial intelligence programs are easily able to kick the butts of human beings when it comes to games like Go and chess,
*  which was considered very far away not too long ago.
*  Another example is GPT-3, which you may have heard of, which is one of these language
*  processing things where you can ask it a question or you can give it a prompt in some sense and it will respond or will continue
*  on in the vein of the words that you gave it on the
*  basis of the fact that it has read a lot of things and it sort of looks for correlations between them.
*  And finally, and maybe most importantly for our everyday lives, AI is everywhere around us in
*  vision recognition, recognizing the images that are in front of us, maybe even self-driving cars or something like that someday.
*  But certainly recommendations, what music to listen to, what movies to watch, etc. AI is at work.
*  So on the one hand is very impressive. On the other hand,
*  none of these are going to be confused for a human being.
*  None of these versions of AI are going to pass the Turing test in some very advanced way. You can sort of
*  jigger up versions of the Turing test that are passable by modern AI, but it's not a full-blown
*  general intelligence, right?
*  AGI, artificial general intelligence, the kind of AI that would really fool you into thinking it might as well be human.
*  So today's guest, Gary Marcus, thinks that he knows why we're not able to do that.
*  More importantly, he thinks that we're moving in the wrong direction or focusing on the wrong things to make progress in this particular direction.
*  The idea is that there are certain kinds of things that neural networks, deep learning is good at
*  looking for correlations in gigantic data sets. There's other things that it's not good at. It's not good at
*  understanding in some vague sense that we would like to define. It's not good at common sense, at understanding how the world works at a basic level
*  so that in individual
*  circumstances we can apply our knowledge in a kind of reliable way.
*  That's why self-driving cars turn out to be harder than we thought they would be. The world out there is a messy place and you need
*  a picture of the fundamental way the world works, not just a set of correlations in your computer or at least,
*  that's what Gary would say. And he even has advice for how we can make progress in the right direction because there's been a shift in how
*  artificial intelligence research has been done. In the early days, it was symbolic. You would try to define symbols,
*  variables in your AI program that represented different things and then look for relationships or try to define relationships between the different variables.
*  Whereas it's almost more
*  mindless today, the deep learning algorithms just take in a whole bunch of data and then spit out correlations between them. As Gary points out,
*  the best deep learning algorithms actually are hybrids.
*  They actually make use of the symbolic approach as well, but he still thinks we should be going a lot further in that direction.
*  We really need that kind of
*  understanding-based approach to make artificial intelligence of the kind you would recognize as
*  human-like in some sense. And that might not just be something we want to do because it would be cool,
*  it might be important technologically going forward. So we're gonna dig into that. It's a lot of fun. Gary's a very opinionated guy.
*  He actually started out in neuroscience and psychology before moving into AI,
*  so he really cares about how real human beings think. He wants to make computers think better than they do today. So let's go.
*  Gary Marcus, welcome to the Mindscape Podcast. Glad to be here. So we're gonna talk about artificial intelligence. Let me give a
*  my impression of the history very very briefly so you can tell me whether I'm correct or not.
*  Like there was, there were go-go days of artificial intelligence,
*  maybe in the 60s and 70s where we were first getting computers that were very, very
*  well-known, and then we started to see a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  And then there was a lot of people who were interested in AI.
*  I would like to clarify for the record that I love AI.
*  I'm not someone who thinks AI is impossible.
*  I'm not someone who thinks AI is impossible.
*  I use the word gadfly.
*  I would probably use the word skeptic.
*  And to that point of that skepticism,
*  And to that point of that skepticism,
*  there's both a specific and a general.
*  The general thing is,
*  one of my favorite graphs I ever saw was
*  the prediction for how far away,
*  let's say, artificial general intelligence might be,
*  although the term is new,
*  but the idea has been around for a long time,
*  that how many years until we have AI
*  that's actually, let's say, as smart as people,
*  and we can talk about whether that's even the right criterion.
*  and we can talk about whether that's even the right criterion.
*  And you look, and it's basically always,
*  people say it's 20 years away.
*  They always say 20 years from now.
*  So that itself is a little bit of an object lesson.
*  So that itself is a little bit of an object lesson.
*  And then there's a question about,
*  what is it that we actually have now?
*  What have we made progress on and what not?
*  And I know as a physicist, you have respect for the data.
*  And you have respect for the fact
*  that there's different kinds of data
*  and different kinds of measurements.
*  And there are some measurements on which
*  the kind of, I'll call it the orthodox
*  quirtzwylian notion
*  of exponential growth seems bang on.
*  And one example
*  for that is chess playing.
*  Another example is Go playing.
*  On these board games there has really been exponential growth
*  or even superexponential growth.
*  You look at Go now as compared to when I was a kid.
*  Computers couldn't play it at all.
*  I could beat a Go player now.
*  There's no way I could beat AlphaGo.
*  So on those kinds of things,
*  there's been exponential growth.
*  On some other things,
*  growth has been slower
*  than the popular media would have it.
*  So, you know,
*  if you had read the popular media
*  over the last few years, you'd probably think
*  that vision was solved.
*  That we now know how to do computer vision.
*  The reality is, we have actually made progress there,
*  but we have not solved the problem.
*  So I'm looking at you right now
*  on a Zoom-type call.
*  And I guess your audience isn't looking at the image that I am,
*  but I can instantly parse what's going on there.
*  And not just label the objects,
*  but I can actually answer questions.
*  Like there are things on the wall
*  and I can make guesses about how they might be mounted there.
*  And I would be surprised if they started floating around the room.
*  Like I have an understanding
*  not just of the entities, but how they relate to one another.
*  I see like stuff like that.
*  And I can see that there's a lot of things
*  that are going on there.
*  And I can see how they relate to one another.
*  I see like stacks of books and their paper on top of them.
*  And I understand why the paper is not floating
*  and not following.
*  So I have this kind of integrated with physics
*  understanding of the scene.
*  And AI does not have that.
*  And that's part of what perception is.
*  So every other month,
*  there's another study now
*  showing these so-called adversarial attacks
*  and so forth, showing that these vision systems
*  can be fooled.
*  In fact, mostly they rely on texture
*  and things like that.
*  So one of my favorite examples from recently was a
*  what was that? I think a fire truck
*  overturned on a snowy road.
*  And the system said with great confidence
*  that it sees a snowplow.
*  And it did that because on a snowy road,
*  you know, if there's a vehicle, it's likely to be
*  a snowplow.
*  And these systems are very much driven by textures
*  that they see and by kind of probabilities
*  of what things are generally likely.
*  They don't have an overall understanding
*  of the scene.
*  Another example was a
*  I'm trying to remember what it was.
*  It was something or other.
*  I think it was
*  an apple with the word
*  iPad on a piece of paper in front of it.
*  Right.
*  iPod. And so, you know, it thought that that was
*  an iPod because the word
*  was written on the page.
*  So, you know, perception is not
*  actually solved, but there has been actual progress.
*  So that's the intermediate case.
*  The first case was true exponential progress.
*  Then there's perception where there are pieces of it
*  where there is exponential progress
*  and pieces of it not so much.
*  And then there's natural language understanding
*  and reasoning. And I would say we have
*  not really made progress at all.
*  GPT-3, which we may want to talk about,
*  gives the illusion of having natural
*  language understanding, but I don't really think that
*  it does. And
*  we are nowhere near, for example,
*  an all-purpose general assistant.
*  We're nowhere near to having the kind of
*  language you would want if you had a domestic
*  robot. I have a cartoon in my book
*  where
*  somebody says put everything in the
*  living room away and the robot ends up
*  cutting up with a saw on the couch.
*  Because it doesn't understand what
*  it is that we would mean by put everything
*  in the living room. We have no
*  candidate solution for that problem.
*  It's not just that we made no progress. We don't even
*  know how to make progress on that.
*  Now you're making me sad that there's no artificial intelligence
*  podcast editor. That would make my
*  life a lot quicker.
*  That would be great. And I mean, you know, even
*  there, there's an example of like
*  how AI does actually help
*  in some ways. So now there are tools, if you
*  want to put together PowerPoint slides
*  for an online
*  talk in this crazy era in which we are living
*  that will automatically transcribe
*  and do a pretty decent job and then
*  go find where the breaks are in the word.
*  So like, there are a lot of places maybe
*  that you wouldn't even expect where AI
*  is actually helping now. There are also some
*  that's hurting now. We should talk about those too.
*  But, you know, AI
*  is real now and it wasn't before.
*  And that's both a blessing
*  and a curse because some of it's reliable
*  and some of it's not. And, you know, there are
*  all kinds of problems with it. But
*  just on that first question,
*  you know, of history and where we are
*  now, what I would say to wrap up
*  a long-winded answer
*  is we made a lot of progress
*  on a lot of things, but there are some core
*  problems which are mostly about
*  understanding the world and what people are
*  talking about. We really haven't made
*  that much progress. And it could be
*  now we really are for the first
*  time 20 years away
*  where all those other times we
*  weren't.
*  Or it could be we're still like 50
*  years away. The
*  core question of how you use
*  common-sense knowledge
*  of the world in order to interpret things.
*  So again, back to the scene in your room.
*  Also there's a place where the
*  lighting values are higher. And I can guess
*  that that's outdoors. Like making all those
*  inferences about what I see
*  and what's likely to be going on. We just don't
*  know how to do that yet. And maybe
*  it's good to... We're able to get
*  into details a little bit. The audience likes the details.
*  So let's try to understand why
*  there has been this progress. And
*  as far as I can tell, the
*  overwhelming majority of recent progress in
*  AI has been driven by neural
*  networks and deep learning algorithms.
*  Is that fair? And what does that
*  mean? It's
*  true, but
*  with some caveats. So
*  first of all, there are older
*  techniques that everybody takes for granted.
*  But are real and are
*  already out there. Second of all,
*  there are things like
*  AlphaGo that are actually hybrid
*  models that use classical
*  tree search techniques enhanced
*  with Monte Carlo techniques
*  in order to do what
*  they're doing. So they're not just straight
*  multi-layer perception. It's a
*  stereotype that people have of neural
*  networks. You have some inputs.
*  They feed into a hidden layer that does some
*  summation and activation
*  function goes to an output. They're not just that.
*  They actually borrow some
*  important ideas about search, for
*  example, and symbols
*  from classical AI. And so they're actually
*  hybrid systems and people don't
*  acknowledge that. So there's a second caveat
*  I would give you. The
*  third caveat I would give you,
*  we can come back to the second, but the
*  third caveat I would give you is, yeah,
*  most of the progress has been with deep
*  learning lately, but most of the money
*  has been there too. And it was really
*  interesting to see, I mean,
*  I don't just mean like 60% versus
*  40. I mean like 99.9%
*  of the investment right now,
*  literally, is in deep
*  learning. And
*  classic symbol manipulation AI
*  is really out of favor. And people
*  like Jeff Hinton say don't spend any money on it
*  at all. And so it was really interesting. There was this competition
*  presented at the NeurIPS conference,
*  which is the biggest conference
*  these days in the AI field,
*  just a month or so ago,
*  on a game called NetHack
*  that has various complications in it,
*  and a symbolic system actually
*  won in an upset victory over all this
*  deep learning stuff. And so
*  if you look back at the history
*  of AI and the history of science more generally,
*  sometimes things get counted out too soon.
*  It is true that deep learning
*  has made a bunch of progress, but
*  the question is, you know, what follows from there?
*  No, I'm not actually trying to make any value
*  judgments. I just would like to explain to our
*  audience what the options are. Like
*  what do you mean by deep learning? What is that?
*  And what is that in comparison
*  to symbolic manipulation?
*  So deep learning is
*  fundamentally
*  a way of doing statistical
*  analysis on large quantities
*  of data. Or at least that's, you know,
*  it's forte. You can actually use it in a bunch
*  of different ways. But most of the progress
*  has come from that. And
*  what's impressive about the
*  recent work is it allows us
*  to learn
*  from very large quantities of data.
*  The classical AI system
*  really didn't do a lot of learning
*  at all. They were mostly hand coded.
*  And sometimes that's the
*  right thing to do. So we don't need
*  to learn how to do
*  navigation. We need to learn
*  some details, but we don't need to learn
*  how to do navigation for the purpose of
*  one of the most useful AI things out there,
*  which is route planning, telling you how to get
*  home from whatever crazy place you
*  wound up in. Right. Right. That's not a deep
*  learning driven system. But
*  there are other systems where
*  if you can glom on to all the
*  data that's out there, you can solve certain problems
*  very effectively. And that's what deep learning has been
*  good for. So an example
*  of that is labeling your
*  photos in, let's say, the
*  Apple Photos app or Google Photos
*  or something like that. There, what you really
*  want to do is to get user data
*  measured in the billions
*  or trillions of examples
*  and have a system that can
*  extract from all
*  of that data what is the
*  most likely label for this image
*  given the other images
*  that are in my database that have been labeled.
*  That's a kind of typical use of deep learning
*  that's very good at. And speech recognition
*  is similar. So
*  I hear this word, lots of people
*  have said it in lots of different ways.
*  And I hear this particular sound.
*  Is it like that collection of things
*  that I've heard before or this other collection?
*  It turns out deep learning is far
*  in a way the best
*  and in some ways simplest way to solve
*  a whole bunch of problems like that. Sometimes
*  it's only a little bit better than the other solutions
*  and it gets more press maybe than it deserves.
*  But it usually is the best for these
*  problems where we have billions and billions
*  of training examples. It's usually
*  the right way to go. But what is it? How
*  does it work? It's statistics, it's correlations,
*  but how does it find these correlations
*  in ways that we couldn't do a few
*  decades ago?
*  The basic idea is not actually new.
*  Something I should clarify first.
*  The mathematics
*  around this has actually been around for decades.
*  People had the idea to do it
*  before. Basically, you're
*  just trying to figure out, I have an error,
*  how can I reduce the error that I've
*  made before? Adjusting
*  weights between
*  a bunch of things that we call nodes that are
*  supposed to make us think
*  of neurons. We can have a whole separate
*  discussion about whether they have anything
*  really to do with neurons. But they're at least loosely
*  inspired by neurons
*  and you're adjusting the weights between
*  them, how loudly they talk to one another.
*  If one of them
*  talks too loudly to the other
*  one, you find out over time, well,
*  I should make it talk a little bit more softly and this one
*  should talk more loudly. You're basically
*  doing that on a mass scale
*  and it turns
*  out to work really well.
*  The math was rediscovered
*  a bunch of different times. There's actually
*  a debate in this
*  mailing list called Connectionist right
*  now about that history and people periodically
*  have these debates. There's no question
*  that it's been around for a long time. What really
*  happened is that
*  people develop GPUs
*  for video games
*  that allow you to do a lot
*  of the relevant mathematics
*  in parallel at the same time
*  and that allowed people to do
*  this deep learning thing at a scale
*  that they didn't really even dream of
*  20 years ago.
*  So that was a major thing. There's this
*  paper called the Hardware Lottery.
*  I'm trying to think of it. I think her name is Sarah
*  Hooker. Wrote this really interesting
*  piece about how
*  the, and I confess I've
*  only read the summary of it, I haven't read it yet.
*  But her
*  thesis is basically, you can have these
*  accidents of history where a
*  particular architecture or something like that
*  is available at a particular moment
*  and people just run with it. And there's a little
*  bit of that going on here
*  and I don't know if she makes this observation or not, but
*  it connects with what she says.
*  Where
*  it's kind of partly an accident
*  of what we figured out how to
*  parallelize first that has made
*  deep learning as popular as it is.
*  So, you know, it was clever
*  to try to use these chips that were built for
*  something else for the purposes of deep learning
*  and it really changed deep learning. I've made
*  a remark earlier about
*  unfairly dismissing things too soon.
*  Deep learning was unfairly dismissed
*  too soon. So, I
*  have dismissed its ability to do
*  sort of deep cognition, but that's a separate question.
*  Its ability
*  to do basic pattern recognition
*  was actually in doubt. So, in
*  the early 2000s,
*  Jeff Hinton, who's his big star now, was
*  kind of like, he gave a poster at this conference
*  and nobody came. They were like,
*  you know, this stuff doesn't really work.
*  We understand the math.
*  It's kind of cool. It has its own kind of elegance,
*  but you're not really getting it to work.
*  So, like, forget about it. And to his credit,
*  he stuck with it. And
*  once people like
*  him could use it at scale,
*  it turned out that this technique
*  is actually lousy with small amounts of data,
*  but it's brilliant with large amounts
*  of data. And so, it was actually
*  like a perfect storm. So, one of it was, one
*  aspect of it was getting these chips, which
*  made a huge difference. Another was, we didn't
*  have databases
*  with trillions of examples,
*  you know, in, let's say,
*  the year 2000. You know, the internet
*  is the other major technology that has
*  driven deep learning. The internet means that
*  you have large amounts of data.
*  And in some sense,
*  from your description, it sounds like
*  I mean, it's artificial intelligence,
*  but it's kind of dumb,
*  and kind of straightforward, right? I mean, you have all these
*  perceptrons, these nodes,
*  and they have weights, and they just float
*  to whatever is the best at fitting the
*  data on the training set, without any
*  deep understanding of what
*  has happened. As you were talking about, you know,
*  papers do not float in the air, or anything like
*  that. So, what's the, how
*  would you characterize the alternative of
*  the symbolic approach,
*  I guess, is what you're calling it?
*  Yeah, well, let me, before
*  I do that, let me say that I think
*  that we need elements of the symbolic
*  approach. I think we need elements
*  of the deep learning approach, or something like it.
*  But neither by itself
*  is sufficient. So, I'm a big fan of
*  what I call hybrid systems that bring
*  together in ways that we haven't
*  really figured out yet, the best of both
*  worlds. But with that
*  preface, because people often
*  in the field like to misrepresent
*  me as that symbolic guy. And I'm more
*  like the guy who said, don't forget about
*  the symbolic stuff, we need it to be part of the
*  answer. So, the symbolic
*  stuff is basically
*  the essence of computer programming, or
*  algebra, or something like that. I mean, what's
*  really about is having
*  functions where you have
*  variables that you bind to particular
*  instances and calculate the values
*  out. So, simplest example would be
*  an equation in algebra. Y equals X plus
*  2. I tell you what X is, you can
*  figure out what Y is. And there, it
*  doesn't matter which X's you have seen
*  before. You have this thing that is defined
*  universally, is the way a
*  logician might put it, universally for
*  everything in some domain, right?
*  Any
*  physicist would grasp that immediately.
*  Or any programmer
*  or any logician. And that is
*  the essence of what allows programs to
*  work. So, we're using a tool called
*  ZenCaster, and it's putting
*  the bits together of your
*  image such that I can see you in vice
*  versa, and it's doing this in real time,
*  because there are functions that can do
*  that across any image. Then
*  we might do some image processing if we're on zoom
*  to do segmentation, we could talk about that.
*  But the basic thing there is I have
*  a function that says, you know, for any
*  set of bits, I will do this
*  function, and I don't care if this
*  image is one that I saw before.
*  And similarly, if I type something in the chat box,
*  it doesn't matter if I come up with a novel
*  sentence or familiar sentence.
*  Whereas the deep learning stuff is all about
*  similarity to the things that you have seen before.
*  And so, it's really a different
*  almost thesis about what
*  cognition should be. And I think the right
*  thesis is actually, our brains
*  anyway can do both. We can do
*  the logical abstract
*  stuff, and I actually did experiments
*  all the way back in the late 90s
*  on human infants showing they could do the abstraction
*  even at seven months old.
*  So there's this ability for us
*  to do abstraction, which allows us to
*  be computer programmers or to do
*  logic. And there's also
*  this like heavy
*  statistical analysis that
*  we humans do. We're not quite as good as the machines
*  at it, but we can do a lot of it. So we
*  know that the word inextricably is often
*  found by linked or bound, but
*  never by water. And we know a lot
*  of statistical things too.
*  Not at the same scale, but we're good at
*  it. And we use it. So we
*  use it in parsing sentences,
*  for example. We make predictions about
*  what the other person's going to say. But
*  then again, if the other person surprises us, we
*  can usually figure it out. So like, you know,
*  a lot of comedy is based on saying something that
*  isn't expected and having
*  to make sure that the listener, you know,
*  figure out that thing. And if you had a system that
*  only kind of makes predictions,
*  like you can think of GPT-3,
*  which is the most famous language system
*  right now, as a really amazing
*  version of autocomplete.
*  And autocomplete is, you know, pretty
*  useful, and we autocomplete to other
*  sentences. But we also have to deal with the unexpected.
*  And for that, symbols are actually really
*  good. And this is why we need
*  to bring both of these traditions
*  together. Well, you had the example in one of your
*  videos that I really liked of
*  children learning how to
*  make the past tense in English,
*  where there's a rule,
*  right? You had E.D.
*  I podcast, I
*  podcasted. But then there's all these irregular
*  ones where it doesn't follow the rule.
*  And so it's kind of like for the
*  regular verbs, it's symbolic
*  kind of manipulation. And for the
*  exceptions, it's more like a deep learning
*  kind of thing. Exactly. And that's
*  actually what my dissertation
*  in 1993
*  was about exactly that. It was
*  about these split systems. And in fact,
*  I wandered off from AI for a long time
*  because I just found it kind of really not
*  very inspiring. And came back
*  around the time of Watson,
*  because I was surprised that Watson actually
*  won at Jeopardy. I can tell you
*  why I think it won. But I
*  was surprised, and not often surprised.
*  And as a scientist, when I'm surprised,
*  that really like wakes me up.
*  And so I was reawoken
*  to AI in 2012,
*  I guess, or so, by
*  Watson. And then around the same time, deep
*  learning was popular. And I was like, oh man, I've seen this
*  movie before. Because the
*  the stuff that I was
*  working on for my dissertation, which included
*  those regular and irregular verbs,
*  which Steve Pinker called the fruit fly
*  of cognitive
*  psychology or something like that.
*  All that, it's the same
*  issues, went into
*  my thesis looking at how children were
*  doing things, as come up again
*  now, when we try to figure out, well, what can
*  deep learning do for us and not? And it's
*  like it can do the irregular verbs, but it's not so great
*  with the regulars. Right. But it's
*  interesting because, I mean,
*  so deep learning is very, very good at some things.
*  Obviously, we've had tremendous success
*  playing chess, playing Go,
*  protein folding is a new success,
*  that the Alpha... Can we pause
*  there though? So, like, sure.
*  The
*  the success in the
*  protein folding and the success in the games
*  actually depend on hybrid
*  systems. And the
*  media coverage of that,
*  and even the internal
*  understanding the field doesn't, I think,
*  realize how much the hybrid stuff
*  is important. So, for example,
*  if you just took the deep learning part
*  of AlphaGo and did not have
*  all of the search stuff, the Monte Carlo search
*  there, it wouldn't be that good.
*  And similarly, AlphaFold
*  has a whole lot of
*  very careful structured representations
*  around
*  the nature of the three-dimensional
*  geometry that it's trying to solve.
*  And it's not just a simple
*  multi-layer perception, I'll put in arbitrary
*  data, get arbitrary data out,
*  and I'm good to go. So, you know,
*  oftentimes the field
*  kind of pumps up the deep learning
*  and doesn't really talk about the other
*  piece of it. I'll give you one other example,
*  which is OpenAI
*  had this example of a system, quote,
*  solving the Rubik's Cube with deep learning.
*  But if you actually read the paper,
*  the part of it that I would think of as
*  solving, which is like knowing which face to turn
*  when, was done entirely by a symbolic
*  algorithm. And they didn't mention that.
*  The deep learning was doing the motor control.
*  And it was, you know, it was a nice contribution
*  to motor control, not as nice as they made
*  it out to be, but it was, you know, a real
*  thing to be able to get the system to
*  turn in one hand
*  the Rubik's Cube at the right time. But the
*  cognitive part of what should I turn
*  in the Rubik's Cube, which is kind of the part that makes it
*  interesting to the average
*  person, they pick it up and they can do the motor control,
*  but they don't know how to do the other part.
*  That was done by a symbolic system.
*  None of the media accounts talked about
*  that. And, you know,
*  there is a
*  a mystique associated with deep
*  learning right now. But often it's actually
*  just part of the picture that does, just
*  gets completely lost. Sure. But
*  what I'm trying to get at is the fact that
*  these successes don't easily generalize.
*  You know, we think about chess
*  and Go as quintessences
*  of intelligent thought,
*  right? But in some sense, they're
*  really, really simple. The rule set
*  is very, very simple. And
*  it's mostly a matter of
*  having enough capacity and computing power
*  to think about it.
*  And, you know, obviously there's a tremendous amount of cleverness
*  that goes into designing the algorithms, and I take
*  your point that it's a hybrid kind of algorithm.
*  But my impression
*  is that if you change the
*  rules of the game by a little bit,
*  right, you change the rules that you're allowed to
*  use with the stones with Go, or how
*  the pieces move in chess,
*  the algorithm that was the world's
*  champion at the regular rules
*  wouldn't be able to adapt
*  very easily to the new rules,
*  whereas a human being
*  could adapt pretty quickly
*  because it has more heuristic understandings
*  of what positions are strong and things like that.
*  Yeah, I pretty much
*  agree with that. I mean, it could start
*  over and learn a new game. I think that
*  the things that DeepMind
*  have built are pretty
*  good at games where there's a closed
*  world and you can gather
*  an infinite amount of data for free.
*  So, related to
*  your point in trying to scope out what is the generalization
*  and generalizability,
*  so these systems
*  are good at closed worlds
*  where the rules are, you know,
*  ideally haven't changed in 2,000
*  years and you can play it yourself and you get
*  as much data as you want. They don't generalize
*  as much to the real world because
*  you usually don't have the same kind of
*  fixed set of rules and it
*  actually is costly to get data. So,
*  you know, if you're trying to figure out what I
*  should do today with my life,
*  you can't get infinite data and you can't
*  solve, you know, I have,
*  well, I'll give you an example of an article that I
*  had somebody made. The article was
*  with Ernie Davis in
*  the ACM journal and the
*  illustrators came up with this great picture
*  which was,
*  we were talking about common sense reasoning
*  and the importance of it. They had the picture
*  of a robot on a tree cutting
*  the limb from the wrong side such that if
*  it succeeded in the cut, the tree
*  limb was going to fall down and so was the robot.
*  So, that's an example of something you can't
*  get by infinite self-play, right?
*  You don't want to fall out of a lot of trees.
*  You want to have some other way of getting
*  to that source of knowledge.
*  If you
*  can work in a hermetically sealed
*  problem where there's kind of no
*  influence from the external
*  world and it's always the same,
*  then you can use this kind of brute force
*  approximation.
*  But if you have to deal
*  with things you can't expect, it's problematic.
*  Now, it's problematic for other
*  approaches too. Nobody has a great
*  AI solution to dealing with the
*  unknown. So, you might
*  remember when long-term capital failed.
*  It was a billion dollar epic mess
*  up. You know, a bunch of Nobel Prize
*  advisors had a model
*  of what they thought would work
*  and they didn't realize that you could
*  have problems with the Russian bond
*  market that would influence this other stuff.
*  That wasn't a deep learning failure.
*  That was a failure of models though.
*  And we don't know
*  how to make models in
*  general versatile enough to
*  deal with the unknown. I mean, I was not
*  a fan of Rumsfeld, but his point about
*  unknown unknowns is actually a good
*  one. Human beings
*  are better at dealing with unknown unknowns,
*  at least in some cases,
*  than any technology that we've
*  currently developed.
*  If you imagine trying to make
*  a domestic robot right now, Amazon's
*  got something that they're talking about.
*  There's a lot of stuff that comes up that
*  nobody's anticipated. And if
*  all you're doing is kind of looking stuff up
*  in a database of what you've seen before,
*  at some point that breaks down. So, to your point
*  about generalization, you know, nothing
*  really unforeseen happens
*  and go if you've played yourself 20 million
*  times. But in the real world,
*  you know, it's snowing in Vancouver
*  and that's not really happening. I need to
*  cross the street and I can't even see
*  the street and now what do I do?
*  Systems aren't really built
*  around that. Well, we can see that
*  there's going to be some tradeoff
*  between
*  letting the algorithm learn
*  by itself versus giving it
*  some structure, right? Like you mentioned for the
*  protein folding where it's
*  not just consider every
*  configuration in space. There's some pre-existing
*  ideas
*  that are built in there. My impression is
*  that for chess and Go,
*  the lesson was don't
*  spoil the algorithm by teaching it
*  human tricks because it'll learn faster
*  just by playing against itself.
*  Yeah, I mean, that's true
*  at this moment in the history of AI.
*  I don't know that it's always true. So,
*  another weakness in current AI is
*  we just don't know how to leverage existing
*  knowledge. We don't know how to specify it
*  and we don't know how to use it. There are some
*  domains where it's actually fine. So, we can do
*  like taxonomy. So, if I tell you
*  that a penguin is
*  a bird, you can make a bunch of inferences about that
*  and realize that it's going to breathe and reproduce.
*  There's some things
*  where we can take a bit
*  of knowledge and extend it further.
*  But we don't know how
*  first of all, in the case
*  of Go, we don't
*  often don't know how to represent the expert
*  knowledge. In some cases, we do. We don't
*  really know how to use it.
*  It turns out in that domain right
*  now, as you say, but with
*  emphasis on the right now, it
*  is easier just to do brute force and just start
*  over basically than
*  to have a bunch of expert Go players
*  tell you stuff. Although, you know,
*  there was actually an expert Go player on
*  the auth, there was an author on one of these papers
*  and probably did say some things. And there
*  are some things that are built
*  in because we know how to do them.
*  We know that it's rotation invariant.
*  Again, as a physicist, you know what I'm talking about.
*  I can rotate the board, I can flip
*  the board, and basically I
*  wind up in the same
*  conceptual space.
*  So there are some
*  things we know how to build in. But
*  here's another example.
*  General intelligence ought to be able to read
*  Wikipedia and use all that information
*  in order to make all kinds
*  of decisions, like to help us with material science
*  or medicine or whatever. And
*  we don't have systems that
*  can do that. They can sort of like
*  take the results of
*  hard won human knowledge
*  or it could be about almost any
*  domain and put them in.
*  So right now the systems, right now
*  the systems that we have
*  are mostly kind of blank slates.
*  They get whatever
*  they know by having all of those
*  nodes line
*  up and balance out
*  in the right kinds of ways
*  without any
*  or without much
*  influence from the knowledge
*  of the world. And
*  it's cool when it works. And in
*  some domains it works well, others
*  it doesn't. But like here's another
*  case. Driving. You
*  would like to be able to just put in the
*  rules of the California driver's code
*  and stick it in with your
*  deep learning system. But we don't
*  have to do that. We just don't.
*  Well language is a great example. And I
*  you've alluded to it several times already.
*  GPT-3
*  is the system that everyone
*  talks about these days.
*  Maybe you can tell us just how
*  GPT-3 works. What's so
*  interesting about it. To be honest
*  I'm less impressed with its results
*  than many people seem to be.
*  Well I may be even less impressed
*  than you are. But many people are. It's true.
*  To me
*  it's a kind of parlor trick that's actually a
*  mistake in the evolution
*  of AI. What it
*  does is another
*  of these systems, more complex than the one
*  that we talked about before. But it's still basically
*  about setting weights for connections.
*  It has some prior structure around
*  attention to help it know
*  about relations between words over
*  certain space and time.
*  There's things called positional encodings.
*  And we don't have to go into all
*  the technical details. But the
*  basic
*  framework if you will
*  is you get a prompt.
*  It sees some set of words.
*  And then it predicts what might
*  follow. And in this
*  way it's kind of like the mother of autocomplete.
*  So you can type in anything
*  and it will continue
*  in that same style.
*  In some ways it's astonishing.
*  You type in something that looks
*  like a movie script and it'll like continue
*  often with the same
*  characters in the same format and
*  all of this.
*  As a kind of surrealist
*  generator, it's fantastic.
*  Type in
*  part of a story and it will continue the story.
*  So why
*  am I not enamored of
*  it when it's capable of doing
*  some really cool things? Like I would
*  not dispute that it can do really cool things.
*  Also, like it's
*  capable of being really grammatical,
*  which earlier systems were not.
*  And it's kind of astonishing in how it does that.
*  Nonetheless, I think that it's misguided.
*  And I think it's misguided because there's no
*  real semantics there. There's no
*  underlying understanding
*  of what it is talking about.
*  And
*  this manifests in different ways.
*  So it
*  will give you fluent speech. I wrote an
*  article that was supposed to be called GPT-3
*  Bullshit Artist.
*  The editor
*  wouldn't let me call it that, so I think it's called GPT-3
*  Bloviator.
*  But they did allow us to have our
*  conclusion, which is that it's a fluent spouter
*  of bullshit. And we
*  had examples like this. You're thirsty,
*  you have some grape juice, but not
*  enough. So you look around,
*  you find some cranberry juice, you sniff it,
*  you pour it into
*  glass, and then you, and GPT
*  autocompletes. And so it says, then
*  you drink it, which is plausible, statistically
*  speaking as a continuation. And then it says
*  you die.
*  Most people don't die by having
*  cran grape juice. It's usually pretty
*  harmless stuff. So the system doesn't
*  actually understand anything about toxicology
*  or why you might die. It's just
*  statistically speaking, the probability
*  of the word die after you sniffed
*  and you're thirsty, and some corpus
*  that it is learned from
*  happens to be high. And I think
*  that illustrates what's really going on there,
*  is it's just looking for corpus,
*  through the corpus for correlations.
*  It doesn't understand what these correlations are about.
*  And that leads it to
*  a weird position in terms of what it does.
*  You can't type in an idea and have
*  it formulate that idea in words, which is
*  what classic computational linguistics
*  tries to do. It can only do this
*  game, and then people work around this game of
*  I'll feed my thing in and hope that
*  it continues. And what they wind up, for example,
*  with is a lot of toxic
*  speech. And DeepMind just had like
*  ten people working on the problem,
*  and actually there are hundreds in the field, trying to make
*  these things not be as toxic.
*  And there's no solution there,
*  because you just have the correlations.
*  You don't have an underlying
*  system where you can
*  query it the way you could query a database.
*  You can query a database and say,
*  you know, how many people of this
*  age group are here or whatever. You can't
*  query GPT and say, are you
*  making a toxic remark, or
*  are you singling out a group?
*  It doesn't know. It's just statistical
*  correlations between words.
*  And so people are trying to put all these band-aids on top
*  of it to make it less toxic.
*  But it's not going to happen. The technology
*  does not really afford that. And then it has
*  a truthiness problem. So it's very
*  fluent, and so it's easy for
*  it to make stuff up and you not notice.
*  And so, you know,
*  it will make up whatever,
*  you know, anti-vaccine
*  stuff if that happens to be in the database.
*  It has no idea what it is that
*  it's spouting.
*  And again, no band-aid that will
*  solve it. Sorry? You had an analogy, which I thought was
*  very illuminating, with the guy who won
*  a Scrabble tournament in French,
*  even though he spoke no French, because
*  he just sort of memorized a list of French words that
*  would be really useful in Scrabble.
*  Yeah, I went back to that book by Fatsis
*  to try to find, I think
*  those people called them word tools or something
*  like that. Like, they don't know what the words are,
*  so they're just using word tools. And that's exactly
*  what's going on. That's why I don't even like playing
*  Scrabble against other people, because if they're good,
*  then they've memorized all these little words that, you know,
*  fit very well. Two-letter words, man. That's where it's at.
*  Terrible. But, I mean,
*  the thing that got me, I was actually
*  before I understood what it really was about,
*  and I had seen some of the hype about GPT-3,
*  I thought maybe it would be fun
*  to do a podcast where I interviewed
*  GPT-3 and had it, you know,
*  voice-synthesized. But then I realized
*  the very basic fact that it has no memory.
*  So, it doesn't remember
*  what you just talked about one question before,
*  and so there's really just no...
*  After five minutes, it becomes highly unamusing.
*  I'm doing an art project
*  around that notion,
*  and, you know, I did some interviewing
*  of GPT, and I would ask questions like,
*  are you a computer? And it would say yes.
*  And it would say, are you a person?
*  And it would say yes. Like, the very next sentence.
*  Like, it doesn't remember.
*  So, why not? Why can't you
*  just add some memory in there?
*  What is the conceptual leap that makes it hard?
*  Or is it just that...
*  That's a really good question.
*  That's an A question there.
*  For sure.
*  What is it about the nature
*  of the system that makes it non-trivial
*  to just add memory? And some people have tried
*  certain kinds of things.
*  It's just built from the foundation
*  in a different way. It's built from the foundation
*  to correlate little bits of information,
*  like the probabilities of these words
*  following those words.
*  And it's not built
*  to have a representational scheme.
*  It does not contact
*  a representational scheme about
*  these are the entities in the world
*  and these are their properties. It's just built
*  on a completely different path.
*  And maybe
*  there's some way of merging them.
*  And I do think that ultimately the answer
*  to AI is going to come from
*  merging at least some of the insights
*  from the GPT tradition with some
*  of the insights from the more classical
*  AI tradition. But I don't think
*  it's going to come literally from merging
*  GPT with these
*  other systems. Because GPT does
*  not have the internal representations
*  that you need.
*  It'd be like saying, I've written this
*  big computer program,
*  but I'm not going to let anybody else see what's inside
*  of it. And now I just want you to
*  hum and I hope that they match together.
*  No, they're not.
*  Yeah, there needs to be some planning to make them
*  work together. There needs to be some planning around what are going to
*  be what we call technically the interface
*  conditions.
*  And it doesn't have
*  an API
*  to use computer gate terms.
*  An API where you can
*  say, hey, what are the
*  people that you're talking about right now? What are the assertions
*  that you made about them? What are you
*  presupposing? And you can't build
*  the API because it isn't there.
*  I mean, it seems to me like that's... It sometimes looks like
*  it's there because it looks like it's
*  coherent, but it's a superficial illusion
*  of the fact that it's drawing on this
*  vast database of things that people have said.
*  You can't build the API to do it.
*  It seems like this is a pretty
*  strong argument just by itself for
*  deep learning
*  or that kind of statistical correlation
*  to be a tool used by
*  a symbolic manipulator. Like, you know,
*  you need some view of the world
*  that is represented symbolically,
*  but then by all means
*  have some deep learning
*  help you with what the correlations are
*  to predict what's going to come next.
*  Well, there's a narrow version of that
*  and a broader version, I guess.
*  The narrow version I think is actually wrong, and the broad
*  version I think is right. So the broad version
*  is, yeah,
*  we need to have symbol systems
*  rely on learning
*  systems to do some of their grounding about
*  what those symbols are about.
*  I think that's the broader argument that you're making. I think it's
*  just right. The narrower version, I don't
*  think GPT itself is actually the right
*  tool for doing the grounding
*  because it doesn't
*  have those interface
*  conditions. It hasn't been built from the ground
*  to land in the right place.
*  So like, you're trying to have
*  two sides of the bridge or the tunnel, I mean,
*  meet up, and it just wasn't
*  built that way. But I
*  think the idea of building that tunnel
*  is right.
*  Let's figure out what
*  these systems are good for. Like, there are
*  lots of opportunities in the world to be
*  tracking correlations,
*  but you need to have respect, I think,
*  for where you're trying to wind up. And
*  as a cultural matter,
*  as a sociological matter, the
*  deep learning people
*  for about 45 years
*  have been, or no, actually like 60 years,
*  have been aligning themselves against the symbol
*  manipulation tradition.
*  This is why we're on the podcast. We're going to change that.
*  Sorry, say again? This is why we're having
*  this podcast. We're going to change it.
*  I was about to say, it might be changing
*  a little bit. So Jeff Hinton, who's the best
*  known person in deep learning, has been
*  really, really hostile to symbols. It
*  wasn't always the case. In the
*  late 80s, he wrote a book about bringing them together.
*  And then he, at some point,
*  went off completely
*  on the deep learning side.
*  Now he goes around saying deep learning can do everything.
*  And he told the EU, don't spend any money
*  on symbols and stuff like that. But Jan
*  Lacoon, one of his disciples, actually said
*  in a Twitter reply to me yesterday,
*  you know, you can have your symbols if I can
*  have my gradients. Which actually sounds
*  like compromise.
*  So I was kind of excited to see that.
*  That does sound good. Sometimes people can like
*  say they're on opposite sides, but really be
*  pretty close to each other.
*  There's one example I want to get on the table
*  because it really made me
*  think. And I think this is the time
*  to do it, which is the identity
*  function. You talk about this in your
*  paper. So let's imagine you have
*  some numbers. They go through
*  a process that spits out an output
*  from the input. And every single
*  time, the output is just equal to the input.
*  So you put in 10010,
*  binary number, and it puts
*  out the same number. And you make the point
*  that every human being sees
*  the training set. You know, here's five
*  examples and goes, oh, it's just the identity function.
*  I can do that. And extrapolates perfectly
*  well to what is meant. But computers
*  don't, or deep learning doesn't.
*  Yeah, deep learning doesn't. I don't think it means
*  that computers can't. But
*  it means that what you need to learn
*  in some cases is essentially
*  an algebraic function or a computer
*  program.
*  Part of what humans do in the world,
*  I think, is we essentially synthesize
*  little computer programs in our heads. We don't necessarily
*  think of it. But the identity function is
*  a good example. I'm just, my function
*  is, I'm going to say the same thing as you.
*  Or we can play like Simon says
*  and then I'm going to
*  add the word Simon says to the ones that go
*  through and not the ones that don't go through. Very
*  simple function that five-year-olds
*  learn all the time. And
*  it's done as a function
*  that applies to a whole bunch of different inputs. So
*  you can say Simon says touch your finger to your
*  nose or Simon says put your phone in front
*  of your nose or Simon says put your
*  wrist strap on your head or whatever.
*  Your viewers can't see me doing these
*  ridiculous things but I'm glad you're laughing.
*  And so you can do this on a
*  infinite set of things. And that's really
*  what functions are about and what programming is
*  about. Doing these things with an infinite
*  range.
*  Identity. This is the same
*  as that. You learn the notion of a
*  pair in cards. You can do
*  it with the twos and the threes and the fours. And now I make
*  a new deck. I don't have twos and threes
*  and fours. I have, I don't know,
*  stars and guitars. And you can tell me the pair
*  of guitars means two guitars.
*  You've taken that new function, put it in a new
*  domain. That's what deep learning does not
*  do well. It does not go over
*  to these new domains. There are some caveats around that
*  but in general that's the weakness of this system.
*  And people have finally realized that
*  nowadays people talk about
*  extrapolating beyond the training set. But
*  the paper that you read,
*  I don't know which version but I first was writing about this
*  in 1998, is really
*  capturing that point. It took a long
*  time for the field to
*  realize that there are actually different kinds of generalization.
*  It also goes back to the past 10 stuff.
*  So people said there's no problem. Our
*  systems generalize. And I said no,
*  there is these special cases. And finally now
*  they're saying oh there are these special cases when
*  you have to go beyond the data that you've seen before.
*  And really that's the essence of everything
*  where things are failing right now.
*  So take driving. You know, these
*  systems extrapolate,
*  or sorry, interpolate very well in known
*  cases. And so they can
*  change lanes in the environments they've seen.
*  And then you get to Vancouver on this crazy
*  snowy day that nobody predicted and you don't
*  want your driverless car out here. Because
*  you now have to extrapolate beyond the
*  data and you really want to rely on
*  your cognitive understanding of where the road might
*  be because you can't see the lane marker anymore.
*  And that's the kind of reason they can't do it.
*  Your identity function example,
*  it raises an interesting philosophical
*  question about what the right
*  rule is. Because it's not like
*  the deep learning algorithms just
*  made something up, but you gave an example
*  where the training set all
*  with a bunch of numbers that all ended in a zero.
*  And the other ones were, you know,
*  random and so we figured it out. But
*  the deep learning just thought the rule was
*  your output number always ends in a zero.
*  And the thing is that that
*  is a valid rule. You know, it didn't
*  just completely make it up, but it's
*  clearly not what a human would want
*  the conclusion to be.
*  I've been talking about this for 30 years.
*  I've made that point in my own papers, be the
*  first person to ever ask me about it.
*  Which brings to my heart.
*  It's really a deep
*  and interesting point.
*  It's not
*  that even when these systems make an error,
*  it's not that they're
*  doing something mathematically
*  random or something like
*  that. They're doing something
*  systematic and lawful, but it's not the way that
*  we see the universe. And in
*  certain cases, it's not the sort of functional
*  thing that you want to do.
*  And that's very hard for people to grasp.
*  So for a long time, people
*  used to talk about deep learning and rule
*  systems. It's not part of the conversation now
*  as much as it used to. But they
*  would say, oh, well, the deep learning system learns
*  the rule that's there. And
*  what you as a physicist understand, or what
*  a philosopher would understand, is that
*  rules are under determined by data.
*  You need something, you know, there are multiple
*  rules. An easy example is
*  if I say 2, 4, 6, 8, what comes next?
*  You know, it could be 10, but it could
*  be something else. And you really want some
*  more background there. So it turns out
*  that deep learning is
*  mostly driven by the output nodes,
*  the sort of
*  nodes that at the end giving the
*  answer. And they each learn things independently
*  of one another. And
*  that leads to a particular style
*  of computation that is good for interpolation
*  and not so good at extrapolation.
*  And people make a different bet.
*  And I did these experiments with babies to show
*  that even very young people
*  make this different bet, which is we're
*  looking for tendencies that hold across
*  a class of items. We're looking
*  for the rule. That's just how we're built.
*  Sometimes that gets us in trouble.
*  There's a word, apophenia, which is like
*  looking for patterns that aren't even really there.
*  And, you know, so sometimes
*  it doesn't serve us well.
*  But it very often does.
*  Language is a great example where it serves us
*  really well. You learn a grammar,
*  and then you can apply it to any words
*  that you can throw in that grammar, even novel words.
*  So if I told you that this thing was
*  called a blicket, then we can start talking about blickets
*  right away. I can say, how much did
*  that blicket cost you? Would you sell me your blicket?
*  Would you recommend the blicket? Is there
*  something, an alternative to the blicket? Like,
*  you know, you're off to the races with one
*  training example because you put
*  it in the context of something that is rule
*  governed, where you have a grammar that
*  tells you not only the syntax, blicket,
*  the plural, the morphology,
*  is going to be blickets, and it's going to tell me
*  how I can use it with Verdin now. But also
*  semantics. You can know that I probably
*  mean an individuatable object,
*  a single object that I can go
*  and count. And you know all this kind
*  of stuff right away because you have
*  a world model that you map
*  your language onto. That's what it's
*  really about. Well, and this is exactly
*  where I was going to go with this because
*  what is the way
*  that clearly, with all this setup, we
*  need to give our world model to our
*  computer friends, to our artificially intelligent
*  friends. And how do we do
*  that? Is it that we human beings need to
*  formalize our sort of
*  manifest image of the world, our picture of
*  common sense, and then turn it into a bunch
*  of symbols? Or is it,
*  and I'm sure that I think I know what the answer to this
*  is, could we deep learn our
*  way into common sense? You know, could
*  we just, is there a way of letting
*  computers figure out the
*  same kind of common sense that we have?
*  I take a view that I think is a little like
*  what Kant was trying to say in the Critique
*  of Pure Reason, although I'm never sure I've completely
*  understood that book. But
*  he talks about having basically
*  prior knowledge of
*  space and time.
*  And I'm sorry I haven't read your book, which would be
*  super relevant at this point.
*  But I think,
*  and so I like to hear your take on it, but
*  my view is you can learn a
*  lot, but that you need a framework
*  to learn,
*  to embed that
*  knowledge. And so minimally,
*  I think you need to know that there is space,
*  that there is time, that
*  there is causality, that there are enduring
*  objects in the world, and some
*  other stuff. But like stuff like that.
*  And I believe that there's some reasonable
*  evidence from the animal literature and
*  the human-infant literature to think that
*  these things are inhumans innate.
*  I think you need to start with that,
*  or else you just wind up with GPT.
*  And in fact,
*  I think GPT is a brilliant experiment,
*  unintentional, but brilliant.
*  On the idea of, could you
*  just learn everything, like let's say from
*  words, or from, you know, people haven't
*  really done it from pixels, but I think you'd wind up in the same
*  place. And I think the answer is no.
*  You don't wind up with the
*  API I'm talking about. If
*  you don't have prior notions about
*  enduring objects that you're talking about,
*  then you just, you're just in correlation
*  soup. And it's, you know,
*  made the best job of correlation soup that
*  I've ever seen, but it's still correlation
*  soup, and it doesn't really connect
*  to those things, which means that it can't
*  know that it's ridiculous to say
*  I'm a computer and a person in one breath
*  or two breaths. You know,
*  it doesn't have the framework to know
*  that, like, things don't tend to change too much
*  over time, because it doesn't know what time is.
*  So I don't fully have
*  an answer to the question that you posed a minute ago,
*  but I think it starts by
*  saying we're going to learn some stuff,
*  but it's going to be relative
*  to a framework
*  where we have some basic knowledge
*  about the world to start with, that there are
*  these enduring objects, etc.
*  I mean, just to emphasize how
*  tricky all this is, you know,
*  I think it maybe undersells the difficulty if we
*  just think about there's space
*  and there's time and there's objects and they have
*  solidity, because there's also,
*  you know, number one, there are relationships between
*  these objects. There are functions
*  that they have. You already mentioned causality.
*  You mentioned the fact that
*  there are values. You don't chop
*  up the sofa to put it away, because that's something
*  that is already away in some sense.
*  And so it's going to be quite
*  a trick. And I guess what you're saying
*  is, and I think I agree,
*  you're saying that the computers are not
*  just going to learn all that stuff
*  by looking at correlations, but
*  there's still a tremendous program out there
*  in front of us of figuring out what it is we want them
*  to know ahead of time.
*  Yeah, so the most
*  impressive shot on goal, to use
*  a kind of cliche I've heard a bunch lately,
*  You're in Canada.
*  Right. No, but I heard
*  it a lot around the vaccines. And
*  rightly, I think people said, this is going to
*  work. There's so many shots on goal.
*  And they did.
*  We underestimated
*  the human capacity to ignore data,
*  but that's another issue.
*  So,
*  Doug
*  Lannet built this thing called
*  Psych, which you may or may not know about.
*  CYC. He's
*  done it for the last 35 years, and it was
*  an attempt to put all of
*  common sense knowledge, or a large
*  fraction of common sense knowledge, in machine
*  interpretable form. And
*  it hasn't been the home run that he
*  thought it would be. And
*  I think people have drawn the wrong lesson
*  from his lack of a huge obvious
*  success. So, like, he didn't build Google
*  with it, right?
*  And, you know, he may have hoped that he would
*  have.
*  But I still think what
*  he was trying to do was right. I think
*  maybe it failed because it started too
*  soon with a different set of tools
*  than we would do to use the project
*  that he was trying to do. But I think the project
*  is right. That we're not going to
*  solve this AI
*  problem, or general intelligence
*  problem, without having a
*  lot of knowledge in formats
*  that the machine can leverage. So,
*  you know, you need to know
*  if you're predicting about grape juice
*  and cranberry juice that they're both juices
*  and that other things being equal, you can
*  mix juices together and you won't die
*  or whatever. And there's a
*  question about, like, what level of specificity you
*  want all of that stuff to be in.
*  Do you want to derive everything from
*  like, you know, quantum mechanics?
*  Do you want to have intermediate representations
*  at the level of juice, which is what
*  people do? But you need
*  some kind of knowledge that
*  machines can reason over. And
*  he built something like 1100
*  micro-reasoners that
*  reason over things like economics
*  and, I don't know if beverages
*  are in there or not, but lots
*  of little domains and
*  desires. The most impressive thing he has,
*  and I write about this in an
*  article called The Next Decade in AI,
*  I can give a reference to his article, which might be
*  in Forbes or Fortune, I'm sorry, I can't remember which.
*  He goes through this example with Romeo
*  and Juliet, where
*  the system is actually able to
*  reason about something complicated, like
*  what Juliet thinks is going to happen
*  when she drinks this potion that's going to
*  fake her death. That's really sophisticated
*  stuff. And he shows that
*  his system that has this common-sense
*  knowledge can make good inferences around
*  that. And nothing in the deep learning tradition can
*  do anything like that. And it's
*  a proof of, you know,
*  conceptual proof, proof of concept
*  that if you have the right knowledge,
*  you can actually get machines to do really rich
*  inference. But there's also an asterisk
*  around it, which is like,
*  it doesn't just read the Shakespeare
*  and make this inference. Rather,
*  he has converted the Shakespeare
*  into a set of logical propositions,
*  and then the system is able to reason
*  over those logical propositions.
*  And so I
*  guess the skeptic would say,
*  well, that's the whole, he's left out the whole
*  problem. I'm a little bit more optimistic.
*  I think he has left out a huge
*  problem, but also showed that another part
*  would be solvable if we do one piece
*  of it.
*  But there's a whole interesting
*  set of issues that don't even get talked about that much,
*  which is like, can we really
*  do this without knowledge, sort of like
*  what he was doing? My answer would be
*  no, we need something like what he's
*  doing. But also that he did
*  it in the 80s, and we know a lot about, for example,
*  statistical representation of information
*  that he didn't have the tools
*  to use then. So
*  like, you want a lot of distributional information.
*  You don't want to just discretize
*  things into logical bins. You also want to know
*  like what's typical. And he
*  doesn't have a lot of that kind of stuff represented.
*  So you do it differently if you did it
*  now, but I think what he was trying
*  to do is still of the essence. I'm reminded
*  of several years ago, Chris Anderson, who was
*  the editor of Wired at the time, wrote
*  some little piece saying that theory is
*  dead in science. My least favorite article
*  of Chris Anderson and of Wired of all
*  time.
*  His logic was, look,
*  if we have enough data, we can just figure out what all the correlations
*  are. Who needs a theory? And I
*  wrote one of the responses, and I said,
*  look, Tycho,
*  Tycho, I should say, Brahe, the famous
*  astronomer, collected a lot of data.
*  And people like Kepler,
*  his protege,
*  found some correlations
*  in the data and constructed some very
*  useful rules. And that was good. But
*  it was when Isaac Newton came along
*  and invented a theory to explain
*  why Kepler's rules were there,
*  that was when we really understood something
*  because then we could talk about
*  going, extrapolating beyond
*  the data sets, et cetera. So in some
*  sense, maybe the
*  worry about, or the problem
*  with deep learning is that we're too good
*  these days at being Tycho
*  and Kepler because we're able to
*  manipulate these huge data sets. But
*  true understanding won't come until
*  we are able to abstract
*  a simple set of rules, which will
*  be a little bit more robust than the original
*  data sets.
*  Well, I mean, I think even getting to Kepler would be
*  progress in that most of
*  the work in the sort of
*  AI scientific discovery stuff
*  builds in the answer in some way or another.
*  And like, you know, what Kepler did
*  that was awesome was to kind
*  of come up with his own answer. He wasn't
*  like, you know, choosing from three templates
*  or something like that. Like, there's a really cool paper
*  by Josh Tannenbaum and
*  Charles Kemp, where
*  they see data and they
*  infer, like, does this follow a
*  ladder or a circle or whatever, these kind
*  of conceptual relationships? But all the choices
*  are built in the beginning. And
*  like, it's still a cool
*  paper, but the
*  really cool paper, which nobody knows
*  that, right? I don't know either,
*  would actually induce that these are
*  even the logical forms that you should think
*  about. And that's, you know, maybe that's closer
*  to what Newton did. But I would give
*  Kepler some cred for that, too. You know,
*  there's a problem some people sometimes
*  talk about about extracting
*  like what the variables are that you even want to talk
*  about. And I
*  think that's often the
*  critical thing, where sometimes, you know, there's a
*  billion different choices and you need to know this
*  is the one that I care about.
*  It's actually impressive even
*  that children ever learn what integers are,
*  for example. This is the kind of thing
*  that, you know,
*  if I were still a professor,
*  I retired as a professor young, but
*  if I were still a professor, I would be telling everybody
*  work on this problem. How is it that
*  Susan Carey is actually telling people
*  this? How is it the kids actually figure out what
*  integers are?
*  Right. That's an example of a
*  kind of conceptual
*  apparatus that's incredibly valuable.
*  And yet, you
*  know, it's not obvious that it's innate.
*  Like, it's pretty obvious that
*  number is innate. So
*  so many animals have some conception
*  of approximate number. You
*  know, that 12 is more than 7.
*  Any animal can, most animals
*  can figure that out. But
*  knowing what a discrete, countable
*  system is where you can have infinity
*  and all, that's a pretty cool intellectual
*  accomplishment. And kids do it.
*  They do the same thing when they learn to read.
*  Some kids don't, but most of them do.
*  A harder example is fractions.
*  You know, the median split on SAT
*  is apparent, SAT math
*  is apparently, do you really get what fractions
*  are or not?
*  Do other primates understand
*  integers? Say again? Do other primates
*  understand integers?
*  You can get them to count.
*  You know, the extent to which they understand
*  integers is not totally
*  agreed on.
*  They do some controversy in the literature.
*  They can at least do things like
*  remember a sequence of small integers.
*  You know, whether
*  they get to the point of realizing
*  hey, I could just keep going with this forever
*  if you just teach me the right words for it.
*  I don't know.
*  It goes right into what I wanted to ask
*  next, which is the extent to which being
*  inspired by biology and evolution
*  and actual human reasoning
*  is useful, right? Like evolution
*  is not
*  goal-directed. It was not set up to try to build
*  a perfect computer, and the human brain is
*  really good at driving and talking
*  and not so good at playing
*  chess or multiplying big numbers together.
*  Do you think that
*  we can take how
*  evolution got us to where we are
*  as inspiration for
*  this program of hybrid
*  systems? I think the right word is inspiration.
*  There's this field of biomimicry,
*  and I think that the
*  moral of that story
*  is there's often useful stuff, and then
*  there's stuff you don't want to copy.
*  You don't want to
*  build your theory about how to support
*  objects around the human spine. It's a
*  terrible solution to supporting
*  this heavy thing on the top of the stock.
*  That happens to be there
*  because we were quadrupeds, and it was
*  kind of evolutionarily cheap in the sense of
*  being likely to rotate
*  the quadruped
*  90 degrees, and then you're vertical, and
*  you're biped, and it's great. But really,
*  a tripod would have been a whole lot better.
*  You don't want to copy everything
*  about our design. In fact, I wrote a book
*  called Cluj, which was about all the
*  things I think are lousy about human cognition,
*  starting with
*  or focusing on things like confirmation bias,
*  where we notice evidence for our own theories.
*  Our
*  political system right now is an epic
*  morality tale in
*  confirmation bias and how bad that is.
*  You don't want your AI system to be
*  subject to confirmation bias, where
*  it comes up with theories, notices
*  evidence for those theories, pats itself on
*  the back, and ignores the counter theories.
*  It's the last thing in the world you would want
*  an AI system to do.
*  We don't want to copy biology,
*  but we do want to learn from it.
*  There are things that
*  people still do way better than machines,
*  even though there are things we do really poorly.
*  We wouldn't want to copy
*  the memory systems of people because they're not
*  that great. On the other hand,
*  they're Q-driven in a way that's kind of
*  cool, and maybe we can kind of do that
*  in AI now.
*  The way that people can
*  understand semantics
*  in relation to a syntax,
*  that's really interesting. We don't know how to do that
*  with machines. Maybe we'll figure out a
*  better way to do it than people do, but right now
*  the only game in town is people,
*  so let's see if we can learn from them.
*  Can you explain that issue
*  without defining
*  the words semantics and syntax as you're using them?
*  That issue is
*  how do you relate the meanings
*  of words to
*  the ways in which you assemble them,
*  and derive the meaning of a
*  sentence in terms of its parts?
*  It turns out
*  that GPT
*  can actually replicate the assembly of the parts
*  into a grammatical sentence, but it
*  can't relate that to a situation
*  in the world that is being described
*  by its sentence.
*  It certainly can't go
*  backwards. It can't actually go in either
*  direction. You can't give it a situation
*  in entities and expect a sentence
*  that will validly describe them,
*  nor go the other way and get
*  the sentence and figure out. Whereas you
*  and I, that's what we're doing, sometimes
*  imperfectly, but we're trying to grasp
*  each other's meanings. You're building
*  a model. What is Gary's actually
*  saying there?
*  We have a limited bandwidth
*  and whatever, but we get there.
*  The machines don't
*  really have that capability right now
*  in a general way.
*  You went back to Kant
*  a little while ago, but how much innate
*  knowledge in the human
*  brain is crucial to this kind
*  of reasoning that we do in extrapolating?
*  Is that something that
*  would help us figure out what to build in
*  to a good hybrid AI system?
*  First thing I'll say is it's controversial.
*  Nobody knows. I spent
*  the first two-thirds of my career
*  as a developmental psychologist
*  slash cognitive psychologist thinking very
*  deeply about this. I wrote a book called The Birth
*  of the Mind, which was about how you
*  might get innate structure given
*  the tools of developmental
*  or molecular biology and what we
*  know about developmental neuroscience
*  and so forth. I thought
*  about these things a lot. The honest answer
*  is we don't know exactly what's innate.
*  The best work I think is done by Elizabeth
*  Spelke, a developmental psychologist at Harvard.
*  There's a lot of work out there.
*  My best guess
*  is that we have at least about a dozen
*  things that are innate, and there could be a lot more.
*  A dozen things
*  include things like the ability to
*  represent these abstractions that we were talking
*  about, the ability to
*  distinguish between types and tokens.
*  Know that this water bottle, as
*  opposed to water bottles in general,
*  space, time, causality,
*  all those kinds of things are like
*  form a bare minimum, and I've written about
*  this occasionally.
*  Then you could have a lot more. I often point
*  to the last chapter
*  of Pinker's first popular book,
*  The Language Instinct, where he runs
*  off a list of like 15 things that includes
*  like, I think I'm quoting verbatim,
*  a mental rolodex. Maybe that is
*  innate.
*  Some things you might be able to derive if you
*  had others. If you had a cost-
*  benefit system, which I think is
*  innate, and you had
*  abstract variables, and
*  you had a few other things, maybe you could acquire
*  some of the others. Then there's
*  a tension in the developmental psychology
*  literature, or actually
*  I won't just call it a tension, a mistake,
*  a foundational mistake, which is that
*  people think that if something could
*  be learned, it is not innate.
*  But that's wrong, right?
*  There may be many things that could be
*  learned, but maybe biology
*  has chosen, so to speak, and you know all the ways in which I'm being
*  anthropomorphic, but has
*  alighted upon solutions that make those innate
*  because it's a whole lot safer or faster
*  or whatever. So you think about
*  a baby ibex scrambling down a mountain.
*  It is not working out
*  online, the physics of objects
*  and slopes and stuff like that.
*  That is there in that, maybe.
*  Or a honeybee can calculate
*  the solar azimuth function and
*  extend it to lighting that it's never seen before
*  if you do the right experiments.
*  So there's clearly innate stuff there
*  about physics and observation
*  and so forth.
*  So there may be a lot more than
*  just the ten or so things that I'm talking about.
*  But even those ten have made me
*  kind of like public enemy number one
*  in the machine learning world where
*  they want to learn everything from scratch.
*  They're like, why is Gary Orleys on about this
*  and this stuff? How do you know?
*  I think that
*  one of the biggest problems
*  with the field of AI is that right now
*  it is dominated by a group of
*  people who do machine learning.
*  There's the old saying of, to a man who has a
*  hammer, everything is a nail.
*  The people in machine learning have made
*  astonishing progress in some ways in the
*  last decade. So they think that the
*  tool that they have is the tool.
*  Whereas I think the right thing is to say,
*  congratulations, that is an awesome tool.
*  We thank you for it. Now let's see how we
*  could use it in combination with other tools
*  to do even more awesome things.
*  It's been a bitter battle
*  getting people to even think about that.
*  Well, one of the possible ways of
*  thinking about this is
*  I kind of
*  don't want to think of Alpha Go
*  or whatever as intelligence
*  almost at all. It's very good
*  at playing Go, but it doesn't
*  remind me of a human being in
*  very many other ways.
*  I would say that
*  intelligence is multi-dimensional.
*  And some of the
*  things that I think are fairly
*  counted as intelligence are
*  like doing that kind of computation.
*  It's fine, call it.
*  As long as you realize that it is multi-dimensional
*  and there are other dimensions where
*  it's not even showing up to bat.
*  So one definition of intelligence
*  is like adaptively
*  solving unknown problems.
*  And it doesn't have that at all.
*  Well, and is the general
*  goal of trying to make AI
*  equal the capacities
*  of human intelligence the right goal?
*  Or should we just be saying...
*  We shouldn't do equal. We should go for exceed.
*  Andy Kahneman and I had this conversation
*  and we sort of came up with this phrase
*  together in a way. It was a panel or whatever.
*  Which is humans are a low bar.
*  I think that was his conclusion.
*  And yet we still can't exceed it yet.
*  We
*  surely
*  should want our machines not to be human
*  level intelligence, but to be way smarter
*  than us. So we want
*  if we're going to trust AI as much
*  as we seem to want to,
*  it's
*  got to be good.
*  If we're going to put it in charge
*  of stuff, whatever that stuff is,
*  it better be able to not
*  be subject to confirmation bias.
*  It better not just perpetuate
*  racist stereotypes
*  from the past, but actually be able to put
*  values so that it's not just interpolating
*  but extrapolating to the
*  world that we want to have.
*  And that means it's going to be better than
*  most people or better than all people.
*  That should be what we're aspiring
*  to. What we're settling for
*  now is we've got these cool tools and they can
*  do some stuff and sometimes they actually
*  tell people to
*  commit suicide or say racist
*  things or whatever. And we're like,
*  but I get really good recommendations from Amazon
*  so it's okay. And that's where
*  we are now. I'm not super
*  thrilled with that.
*  But I guess what I'm getting at
*  is, as you said, I completely
*  agree that there's very many different kinds
*  of intelligence. And
*  computers
*  are going to be, it's going to be easier to make
*  computers good at some kinds of intelligence
*  than it is at other kinds of human
*  intelligence. And
*  I'm sure both are
*  important, but how do we balance
*  just putting computers
*  to work at the things they're good at
*  versus trying to nudge them
*  to become good at other things that we human
*  beings know and love? I think
*  it starts with what you just said.
*  I often make your point a slightly
*  different way, which is that
*  people talk about
*  artificial intelligence as if it
*  was one thing. But it's actually
*  many things. It's actually a whole
*  family of algorithms and
*  also databases and
*  so forth that
*  have different properties.
*  They're good at some things, they're not good at other
*  things. That's going to change over time.
*  So there's the AI of 2022
*  is different from the AI of
*  2019. And I sure as hell hope
*  that the AI of 2025 is better
*  than what we've got right now because it's problematic
*  now. And
*  you can't just talk about it like
*  it's a magic wand. It's not.
*  It's a set
*  of tools that are more or less appropriate
*  to certain problems. And so
*  it's totally fine to use current
*  AI for photo tagging.
*  2025 AI will be even better
*  at it, but the cost of getting a
*  mislabeled photograph is generally not
*  that high unless then again
*  you're using it for surveillance, in which
*  case maybe it's really high. So
*  I just saw another one of these examples of
*  somebody who went to jail because an
*  AI system misread something.
*  I think in the book we gave an example
*  of something in China that
*  gave somebody a speeding ticket
*  because their face, they were an actress,
*  their face was on a bus that went fast
*  or whatever.
*  And so the wrong person
*  got convicted of the crime.
*  The tools
*  are really appropriate and then not
*  appropriate depending on how they get used.
*  So you can tolerate the error in
*  photo tagging if you're not using it to identify
*  criminals. If you're identifying criminals then
*  you probably need at least 2025
*  AI or whatever,
*  2030 AI because the stakes
*  are so high. Same thing with
*  suicide prevention. You can write a little
*  chat bot that will make people feel better
*  some of the time. But
*  when the stakes are high, I don't think
*  the tools we have right now are up to it.
*  Driving is another example.
*  It's easy to build a car
*  that can follow a lane. You can have like
*  70 hours of training data and Nvidia show
*  this and you can follow a lane and that's great
*  but it doesn't mean that you know what to do on
*  a snowy day. And so
*  we have to be very careful
*  about the laws around driverless
*  cars and like right now I think
*  Elon Musk is beta testing on public roads.
*  I don't think that's cool. There have been some accidents.
*  And so
*  understanding that
*  AI is actually a heterogeneous thing
*  rather than a single magic wand
*  is important. Now that makes it hard
*  right because people want a policy
*  that's sort of about AI
*  writ large and that doesn't
*  match the reality of we are
*  incrementally developing science
*  and engineering to make things better
*  and we understand some of it and not
*  others. And without asking you
*  any predictions
*  about time scales or anything like that
*  do you see any obstacles to AI
*  being just as good at human beings
*  as human beings are at
*  writing poems or symphonies and
*  so forth? In principle no.
*  I mean
*  they're not going to have the emotions
*  that might drive some of that stuff.
*  It's actually not that
*  hard to write knockoffs
*  of Bach without the
*  emotional resonance.
*  One piece of your question is
*  specifically about creativity
*  and then there's the larger question. So many
*  particular things that we would define as creative
*  we can already build machines
*  to do without that connection
*  to the underlying emotional
*  impulse that might lead to something. And so
*  vocals are hard because vocals are
*  really about emotion.
*  Synthesizing a drum beat
*  Logic 12
*  or whatever is the latest edition can do that pretty well.
*  There's a humanized
*  function to add a little random variation
*  and make it sound like a person.
*  There are certain things that we can do
*  very very well.
*  And in some cases have been able to actually
*  for 30 years. People
*  are reinventing them with deep learning but people already
*  knew how to do some of those
*  things. The larger
*  question
*  Sorry one more thing. There are some
*  artistic endeavors that I think are way beyond
*  current computers though.
*  Like a movie
*  where you have to have coherence over a long
*  period of time. So
*  GPT can actually make advertising
*  jingles that are like two liners pretty well.
*  But it can't keep the coherence
*  that you would need for
*  an ordinary
*  film. You could make something
*  from the late 60s
*  with pharmaceuticals involved that seemed interesting.
*  But long form is not
*  the strong point of what we have right now.
*  And it won't be for a while.
*  But I don't think anything
*  in the realm of cognition
*  is impossible.
*  We are just me out computers
*  and we don't quite
*  understand how those me computers work. But there
*  are information processors that
*  take in information
*  and manipulate it and come up
*  with outputs. That's what our brains do
*  and computers get better
*  at that. And I don't see any
*  principled argument that says
*  500 years from now people will still
*  be smarter than machines. I just don't see it.
*  500 years is much safer than 20 years.
*  I think you've chosen wisely about your
*  time horizon there.
*  But I guess
*  there are people who are worried about
*  existential risks from
*  AI taking over and have different values
*  than us.
*  Do you share those worries? I mean clearly giving
*  AI's value systems
*  at all and recognizable to us
*  is a tricky situation.
*  So I wrote this piece
*  in 2012 called Moral Machines
*  for the New Yorker.
*  I was one of the first people to talk about
*  trolley problems in
*  AI.
*  Where in my particular case
*  in the New Yorker article was a school bus
*  is out of control, you're on a highway,
*  should you sacrifice yourself?
*  A lot of people picked up on this later. Obama talked
*  about it.
*  I now have some regret around that.
*  It's all your fault. We've narrowed it
*  down. Okay.
*  There were a couple of us who wrote about it around the same time.
*  But I was one of the first. But the thing
*  is that the real challenge right now is much
*  lower to the ground than that.
*  And it's not often that you actually come up
*  with the school bus.
*  But
*  Asimov's basic laws
*  don't do harm. Just think about
*  that one.
*  The model that we have now
*  is around images. I show you a bunch of images
*  and you learn
*  from those images to recognize another.
*  It doesn't work. That model doesn't work
*  for harm. I can't show you a bunch
*  of pictures of harm and really get you to
*  grasp the concept of what harm is.
*  We just don't know how to program
*  what harm is. We don't
*  know how to program really any
*  human values into
*  current technology. And it's actually related
*  to stuff we've been talking about throughout the whole
*  conversation, which is it's kind of an interface
*  thing. We don't know how to
*  specify these things in terms
*  that learning systems would understand. And we
*  can't really do it entirely
*  with innate set of rules either. There has
*  to be some learning. We have to give some examples.
*  There was
*  a film called Chappie a few
*  years ago where a robot learned
*  its values.
*  And you know
*  one of the lines in the film is
*  the robot is trying to figure
*  this stuff out all out. And
*  the robot's been captured by a bad guy.
*  And
*  the robot's master
*  has said you can't kill
*  people. And the bad guy is
*  a bit disappointed to discover that the robot
*  knows this.
*  Because the bad guy would actually
*  like the robot to kill people. But the bad guy
*  is clever. And he works around and he says yeah
*  you can't kill people. But it's okay
*  to harm them. And the robot
*  is sort of left
*  to construct its own
*  ethical values based on
*  the input that it's getting. And the reality
*  is that we will get to
*  a point where
*  maybe we already have gotten to a point where
*  it would be really nice to have
*  AI systems with values. And we have not
*  gotten to the point where we know how to program that in.
*  So one of the problems with GPT-3
*  is all the toxic language that it
*  produces. Because it's like trained on
*  the worst of Reddit and stuff.
*  And we just don't
*  like that already it would be nice to constrain
*  these systems such that
*  they would follow some set of values. And you
*  can argue about what that set of values would be. But
*  we don't know how to do it.
*  DeepMind just had like 10 people, 20 people
*  working on this problem and came up dry.
*  Nobody knows
*  how to actually constrain these
*  systems to values.
*  They don't have the APIs
*  to plug into. And you know it's
*  a problem. So when you come to the
*  existential stuff,
*  I'm not worried about it now.
*  I'm not worried about in the short term
*  robots taking over the world.
*  They don't care about us. They have no
*  motivation to do so. And
*  they're frankly dumb right now.
*  The ability to win a Go
*  doesn't count for anything.
*  Go is actually a great example because Go is about
*  territory. And
*  they don't actually understand anything
*  about territory in the real world.
*  Getting better at Go has not made them
*  any more desirous of human
*  territory nor taught them anything about
*  that would be useful in an actual
*  battle. I'm not too worried
*  about those kinds of things in the near term.
*  In the near term, I'm worried about
*  misapplication of the AI
*  that we have now.
*  But you know in the 100 year
*  time frame, it could
*  be an issue. I think it's fine that
*  we have a few people around
*  thinking about these issues now.
*  Maybe they never come to pass.
*  But it's good to have some thought
*  into it. It's not an urgent need.
*  I'd like to end on an optimistic
*  note. So maybe
*  what if everyone listened
*  to you? What if you were
*  not the bad boy of AI and everyone said
*  you know what? More symbols, more variables,
*  more hybrid approaches to
*  join up to our deep learning. How
*  would you see AI going
*  in the next few years?
*  I wrote a piece for The Times
*  where I argued for a CERN
*  for AI. If people really listen
*  to me, that's what they would do.
*  I would get people to gather around
*  a particular problem.
*  In part because otherwise
*  if you have a large sum of money, then people
*  just do their own thing and don't actually coordinate.
*  The problem that
*  I would coordinate them around is having
*  an AI that could read and understand
*  the medical literature. I think it would be
*  enormous value in that. You could also
*  maybe think about doing the same thing around climate change.
*  Read and understand material
*  science and so forth.
*  Either of those would be fine
*  in my view.
*  Have people coordinate
*  around machine reading. I'm
*  not talking about keyword matching, which we can do
*  very well right now. Having
*  a system read a scientific
*  literature, come up with experiments
*  based on
*  what it reads, come up with novel solutions
*  and so forth. I think that could change the world.
*  It would certainly push
*  AI forward. If I were king for the day,
*  that's what we would do.
*  That's how people know it. They can spread the word.
*  Gary Marcus, thanks so much for being on the Mindscape Podcast.
*  Thanks. This was really fun.
*  you
