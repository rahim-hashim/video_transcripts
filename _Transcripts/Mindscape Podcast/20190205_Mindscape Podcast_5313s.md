---
Date Generated: February 13, 2025
Transcription Model: whisper medium 20231117
Length: 5313s
Video Keywords: ['artificial', 'cars', 'ethics', 'intelligence', 'robots', 'selfdriving']
Video Views: 10777
Video Rating: None
Video Description: Blog post with audio player, show notes, and transcript: https://www.preposterousuniverse.com/podcast/2019/01/21/episode-30-derek-leben-on-ethics-for-robots-and-artificial-intelligences/

Patreon: https://www.patreon.com/seanmcarroll

It’s hardly news that computers are exerting ever more influence over our lives. And we’re beginning to see the first glimmers of some kind of artificial intelligence: computer programs have become much better than humans at well-defined jobs like playing chess and Go, and are increasingly called upon for messier tasks, like driving cars. Once we leave the highly constrained sphere of artificial games and enter the real world of human actions, our artificial intelligences are going to have to make choices about the best course of action in unclear circumstances: they will have to learn to be ethical. I talk to Derek Leben about what this might mean and what kind of ethics our computers should be taught. It’s a wide-ranging discussion involving computer science, philosophy, economics, and game theory.

Derek Leben received his Ph.D. in philosopy from Johns Hopkins University in 2012. He is currently an Associate Professor of Philosophy at the University of Pittsburgh at Johnstown. He is the author of Ethics for Robots: How to Design a Moral Algorithm.
---

# Episode 30: Derek Leben on Ethics for Robots and Artificial Intelligences
**Mindscape Podcast:** [February 05, 2019](https://www.youtube.com/watch?v=OqqL8YM4Ibo)
*  Hello everyone and welcome to the Mindscape Podcast. I'm your host Sean Carroll and today's episode [[00:00:00](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=0.0s)]
*  we're gonna see where the rubber hits the road in moral philosophy. [[00:00:05](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5.36s)]
*  And I mean that quite literally. [[00:00:09](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=9.0s)]
*  You've all heard about self-driving cars and you may have heard about the idea that self-driving cars are going to have to solve the [[00:00:10](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=10.68s)]
*  trolley problem. This famous thought experiment in philosophy where you can either continue to do something and [[00:00:17](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=17.72s)]
*  several people will die or you can take an action to prevent your current course of action and do something different and [[00:00:24](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=24.12s)]
*  fewer people will die. Is it okay to [[00:00:31](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=31.8s)]
*  intentionally kill a smaller number of people to save a larger number of people? [[00:00:34](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=34.800000000000004s)]
*  You might not think that this is something you are going to need to deal with [[00:00:39](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=39.56s)]
*  but it's simply an illustration of the kinds of problems that all sorts of robots and artificial [[00:00:43](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=43.120000000000005s)]
*  intelligences are going to have to deal with. They're going to need to make choices and in the extreme examples [[00:00:49](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=49.2s)]
*  they're going to need to make hard choices about how to cause the least harm. As one example should a self-driving car if there are [[00:00:55](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=55.04s)]
*  two bicyclists in the way and it judges that it's going to have to hit one of them, should the self-driving car [[00:01:03](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=63.92s)]
*  target a bicyclist with a helmet rather than one without? On the theory that wearing a helmet makes that bicyclist more safe and [[00:01:10](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=70.24000000000001s)]
*  therefore in some sense that person should get punished for wearing the helmet that doesn't seem right. [[00:01:18](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=78.16s)]
*  These are moral intuitions that lead to really hard problems and we have to face up to them. [[00:01:23](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=83.28s)]
*  Today's guest Derek Lieben is a philosopher who has written a new book called Ethics for Robots [[00:01:28](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=88.48s)]
*  where he tackles exactly these questions not just self-driving cars, but the general idea of what kind of moral [[00:01:33](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=93.66s)]
*  decision processes should we program into our artificial intelligences? [[00:01:41](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=101.2s)]
*  I think it's just a fascinating topic to think about because on the one hand Derek's book involves big ideas from moral philosophy, [[00:01:46](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=106.24s)]
*  utilitarianism versus deontology, [[00:01:54](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=114.32s)]
*  John Rawls's theory of justice, things like that. On the other hand very down-to-earth questions about game theory, the prisoner's dilemma, [[00:01:56](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=116.8s)]
*  Nash equilibrium, Pareto optimality, other sort of economic and rationality oriented ideas need to come into play here. [[00:02:04](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=124.92s)]
*  So to me it's a great example of how the abstract theorizing of philosophy suddenly becomes frighteningly relevant to real-world decisions. [[00:02:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=132.6s)]
*  Personally, I do not own or have plans to buy a self-driving car in the near future, but I do think they're coming. [[00:02:21](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=141.44s)]
*  Moreover, artificial intelligences of all sorts are all around us and have an increasing effect on our lives. [[00:02:27](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=147.28s)]
*  Therefore we should be thinking about these issues and now's a good time as any. Let's go. [[00:02:34](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=154.24s)]
*  Derek Liebman, welcome to the Mindscape Podcast. [[00:02:42](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=162.56s)]
*  Thanks so much for having me. [[00:02:45](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=165.92000000000002s)]
*  So this is a topic, you know, ethics and morality for robots and artificial intelligence. [[00:02:47](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=167.92000000000002s)]
*  Everyone's thinking about this. We all know self-driving cars are coming and we're thinking about how to make them work. [[00:02:54](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=174.48s)]
*  So I'm going to start with Derek. [[00:03:00](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=180.48000000000002s)]
*  So Derek, you're going to be talking about the idea of a self-driving car. [[00:03:02](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=182.48000000000002s)]
*  So, could you just give us your short version of why it's necessary even to talk about ethics or morality for robots? [[00:03:06](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=186.48s)]
*  I mean, do they even have ethics? Should they just do what we program them to do? [[00:03:14](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=194.48s)]
*  Yeah, so that's a great place to start. [[00:03:18](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=198.48s)]
*  As we are starting to develop more and more autonomous, more and more efficient, more and more efficient, more and more efficient robots, [[00:03:20](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=200.48s)]
*  in the fields of transportation, medicine, warfare, they are starting to make these decisions that are going to have impacts on human health and safety and opportunity. [[00:03:28](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=208.48s)]
*  And for that reason, we need to start thinking about which actions are permissible for them to do and which actions are impermissible. [[00:03:36](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=216.48s)]
*  And I think that's a great place to start. [[00:03:42](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=222.48s)]
*  So, Derek, I'm going to start with you. [[00:03:44](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=224.48s)]
*  So, I'm going to start with you. [[00:03:46](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=226.48s)]
*  So, I'm going to start with you. [[00:03:48](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=228.48s)]
*  So, I'm going to start with you. [[00:03:50](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=230.48s)]
*  So, I'm going to start with you. [[00:03:52](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=232.48s)]
*  I'm not sure if we can use certain words to talk about these machines like responsible or not or blameworthy or not. [[00:04:22](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=262.48s)]
*  But that's sort of beside the point for me. [[00:04:30](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=270.48s)]
*  What I'm most interested in is what are the rules that we're actually going to be using to program into these machines because that's definitely something that we need to do. [[00:04:34](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=274.48s)]
*  Yeah, you have used the word decisions as if the robots have the ability to make decisions. [[00:04:44](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=284.48s)]
*  Is that an important word or is it just a way of talking about the fact that the robots are going to be doing something and we have to decide what we want it is for them to do? [[00:04:50](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=290.48s)]
*  That's interesting. [[00:04:58](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=298.48s)]
*  For me, it doesn't really matter whether you call this a decision or an algorithm. [[00:05:00](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=300.48s)]
*  I know that maybe some people might think of decision as something performed by an agent with free will who could have done otherwise or something like that. [[00:05:06](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=306.48s)]
*  But for me, that's not too important. [[00:05:16](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=316.48s)]
*  I'm not going to get hung up in those kinds of issues. [[00:05:18](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=318.48s)]
*  And I mean, certainly some moral theories would like a Kantian about ethics is going to say that only an agent with free will who understands what he or she is doing is capable of being a moral agent who makes decisions at all. [[00:05:22](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=322.48s)]
*  But that may already reveal something about my normative assumptions that I'm making. [[00:05:36](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=336.48s)]
*  And that's something that I think we'll see as we move forward that every kind of choice you make in programming a machine is revealing something about your normative assumptions. [[00:05:43](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=343.48s)]
*  It's sort of impossible to stay free of ethics, to just say, well, I'm going to avoid ethics entirely. [[00:05:55](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=355.48s)]
*  Right. Well, I agree with that very much. [[00:06:02](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=362.48s)]
*  And also, I'm happy that it doesn't matter whether we attribute free will to the robots because I hate talking about free will and yet I end up doing it all the time. [[00:06:05](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=365.48s)]
*  So I'm glad that we can avoid doing it for this one. [[00:06:13](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=373.48s)]
*  OK, so let's get one thing out of the way, which I'm sure you've been hit with before. [[00:06:17](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=377.48s)]
*  What about Isaac Asimov? Didn't he explain to us how to make moral rules for robots? [[00:06:22](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=382.48s)]
*  Doesn't he have three laws and should we just implement them? [[00:06:28](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=388.48s)]
*  Yeah. So Asimov did propose these three rules, which are really just one rule and two sort of subservient rules that say obey people and protect yourself. [[00:06:30](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=390.48s)]
*  But the first rule is really the one doing all the work. [[00:06:42](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=402.48s)]
*  And it says don't cause or allow harm to other humans. [[00:06:45](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=405.48s)]
*  Now, on the face of it, that's actually pretty good. [[00:06:49](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=409.48s)]
*  And I think the reason why there's a sort of variety of moral theories and a variety of different kinds of rule systems that we've constructed is that most of them do pretty well in normal circumstances. [[00:06:52](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=412.48s)]
*  It's a bit like I'm going to make the first of many analogies to physics here because I love to make analogies to physics and now's my chance. [[00:07:07](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=427.48s)]
*  It's a bit like using classical mechanics in most normal circumstances, where if you're not going very fast and you're dealing with moderately sized objects, this works really well. [[00:07:16](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=436.48s)]
*  However, when you get to these very sort of extreme situations, you start to see differences between the moral theories. [[00:07:28](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=448.48s)]
*  And the problem with Asimov's laws is that it's vague about certain situations like the violation of property rights, the violation of dignity, insulting people. [[00:07:37](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=457.48s)]
*  Does that count as harming them? Does trespassing on their property count as harming them? [[00:07:51](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=471.48s)]
*  Does blowing smoke in someone's direction count as harming them? [[00:07:55](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=475.48s)]
*  And also it fails in these situations where every action either does or allows harm to others. [[00:07:58](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=478.48s)]
*  And these are called moral dilemmas. [[00:08:05](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=485.48s)]
*  So in some scenarios, you can't avoid either causing or allowing harm to others. [[00:08:07](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=487.48s)]
*  And Asimov's law simply breaks down. [[00:08:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=492.48s)]
*  Yeah, to be perfectly honest, the question that I asked you was because I feel I have to ask it, but I think you're being far too generous to Asimov's laws. [[00:08:15](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=495.48s)]
*  I think they're just silly. [[00:08:25](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=505.48s)]
*  You know, the idea that a robot cannot, through inaction, allow a human being to come to harm is entirely impractical. [[00:08:26](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=506.48s)]
*  Like human beings come to harm over the world all the time. [[00:08:36](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=516.48s)]
*  Every robot would instantly spring into action trying to prevent every human from stubbing its toe, right? [[00:08:39](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=519.48s)]
*  Yeah, exactly. And this is a point that philosophers like Peter Singer have made for decades is that almost everything we do in the world has some kind of effect on people that we might not even be aware of. [[00:08:44](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=524.48s)]
*  I mean, Singer has gone to great length to show that the way that we eat, the way that we travel, the way that we spend our money is actually having effects on other people. [[00:08:55](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=535.48s)]
*  We could be doing other things with that money, with that food, perhaps driving around in our cars. [[00:09:06](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=546.48s)]
*  We're not thinking about the damage that we're doing to the environment and the future generations. [[00:09:11](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=551.48s)]
*  Yeah, and I also like the idea that you need to be a little bit more specific. [[00:09:16](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=556.48s)]
*  I think a lot of people do have this idea that, you know, the example I used in my book, The Big Picture, was in Bill and Ted's Excellent Adventure, we heard the moral rule that you should just be excellent to each other. [[00:09:22](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=562.48s)]
*  And I tried to make the point that's fine, but it's not quite good enough. [[00:09:35](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=575.48s)]
*  You haven't told us what excellence is. [[00:09:39](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=579.48s)]
*  And actually recently I got into a Twitter conversation with Ed Solomon, who is the screenwriter for Bill and Ted's Excellent Adventure. [[00:09:41](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=581.48s)]
*  And he was very pleased that even if I was saying it's not a good rule, that it made it into this kind of consideration. [[00:09:48](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=588.48s)]
*  And certainly when robots are on the scene, we have to be a little bit more specific, a little bit more clear, a little bit more quantitative maybe about what constitutes a morally correct action. [[00:09:54](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=594.48s)]
*  Is that right? [[00:10:04](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=604.48s)]
*  That's right. And I agree that what you're describing is a theory that you are familiar with, virtue ethics. [[00:10:05](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=605.48s)]
*  And it basically says, do whatever a noble person would do. [[00:10:11](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=611.48s)]
*  Now, this kind of works if you have a good exemplar of a noble person handy, but then there are all sorts of problems like, how do you know that this person is actually a noble person whose behavior we should be trying to imitate or not? [[00:10:16](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=616.48s)]
*  And if you are a noble person in one culture, that might be a very, very terrible person in another culture. [[00:10:31](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=631.48s)]
*  And so that leads to this problem that you were just talking about, which is where do we look for these more precise, more quantitative, more formal approaches to designing moral algorithms? [[00:10:39](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=639.48s)]
*  And there's a lot of different places. [[00:10:51](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=651.48s)]
*  One place we might look is in human judgments and try to model machine behavior after human behavior. [[00:10:53](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=653.48s)]
*  Another place we might look is to moral theories and try to actually take a historically important theory and implement it into a machine. [[00:11:01](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=661.48s)]
*  And so, yeah, in your book, Ethics for Robots, there's a wonderful little book. [[00:11:10](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=670.48s)]
*  Everyone can read it and it really delves into a lot of fun things. [[00:11:16](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=676.48s)]
*  Am I accurate in saying that you contrast three major approaches here, utilitarianism, libertarianism, and contractarianism? [[00:11:19](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=679.48s)]
*  That's right. Yeah. [[00:11:28](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=688.48s)]
*  So I talk about some historically influential moral theories, utilitarianism, Kantian ethics, contractarianism, and you could also include some other ones. [[00:11:29](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=689.48s)]
*  You could talk about virtue ethics if you're interested in that or not. [[00:11:42](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=702.48s)]
*  But like I said, I don't think that virtue ethics is going to be specific enough to make it into this club. [[00:11:46](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=706.48s)]
*  Be a virtuous self-driving car is hard to actually implement in real life. [[00:11:52](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=712.48s)]
*  It is unless you're doing something like let's train the machine in a sort of bottom up approach, as they say, to be like human beings around us. [[00:11:56](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=716.48s)]
*  There's actually a few people like Wendell Wallach and Colin Allen who have proposed that this is a good approach to designing moral machines. [[00:12:07](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=727.48s)]
*  However, my objection to that is that you're probably going to incorporate all of the terrible biases and limitations of the humans that you're using to model your machine's behavior on. [[00:12:16](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=736.48s)]
*  So these machines are probably going to wind up being terribly racist and sexist and not thinking very clearly about consequences versus rewards and so on. [[00:12:30](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=750.48s)]
*  And also isn't it just a little bit circular, you know, to what it means to be moral is to be like a moral person? [[00:12:42](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=762.48s)]
*  I mean, I think I'm on your side here, if I'm right, that you would say we need to be much more explicit both for robots and for human beings about what it objectively means to be a moral person. [[00:12:49](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=769.48s)]
*  Yes, I totally agree. [[00:13:02](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=782.48s)]
*  So it sounds like we are both on board with virtue ethics not necessarily being a good approach here. [[00:13:03](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=783.48s)]
*  So maybe we can move on to what you were talking about, which is these historically important moral theories. [[00:13:09](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=789.48s)]
*  And in your book, The Big Picture, which I also am a fan of and I recommend you talk a lot about how these theories are constructed around our moral intuitions. [[00:13:15](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=795.48s)]
*  Right. And there might be different consistent internally consistent set of sets of rules that you could get from different kinds of moral intuitions. [[00:13:27](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=807.48s)]
*  And I actually think that is completely correct. [[00:13:36](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=816.48s)]
*  I think that that is historically where these theories have emerged from. [[00:13:39](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=819.48s)]
*  And the question is not so much, well, which intuitions should we rely on? [[00:13:44](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=824.48s)]
*  But what is, I think, the evolutionary function of the intuitions that we are using in the first place? [[00:13:50](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=830.48s)]
*  Right. So one answer to this question is there's a lot of these different internally consistent sets of rules. [[00:13:57](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=837.48s)]
*  That are all more or less intuitive in some kinds of circumstances. [[00:14:05](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=845.48s)]
*  But we have no way of evaluating one versus the other. [[00:14:11](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=851.48s)]
*  If somebody wants to be a utilitarian and say it's wrong to, for instance, buy a cappuccino when you should be giving money to famine relief. [[00:14:16](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=856.48s)]
*  And another person wants to be a Kantian and say, well, your intentions when buying the cappuccino were just to have this delicious coffee beverage. [[00:14:26](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=866.48s)]
*  And therefore it's only a side effect that these people were harmed. [[00:14:34](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=874.48s)]
*  Right. How do we how do we resolve the disagreement here? [[00:14:39](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=879.48s)]
*  Now, you could just say, well, these are just equally important and equally coherent sets of rules. [[00:14:45](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=885.48s)]
*  But I think a better approach is let's look at the evolutionary function of the intuitions that they are drawing on and say, is there a sort of unified framework that matches that that evolutionary function, that original goal of the system? [[00:14:52](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=892.48s)]
*  Right. So, I mean, I think we agree on a lot of things, but maybe we disagree on the meta ethical question of whether moral rules are objectively real or not. [[00:15:11](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=911.48s)]
*  Is that right? That's right. [[00:15:22](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=922.48s)]
*  And a lot of this hinges on what we mean by real. [[00:15:24](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=924.48s)]
*  So I think I mean usually does. [[00:15:26](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=926.48s)]
*  Yeah, exactly. [[00:15:29](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=929.48s)]
*  So when I say real, I mean real in the sense that smoking is bad for your health is a fact. [[00:15:30](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=930.48s)]
*  It's a real fact about human beings. [[00:15:36](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=936.48s)]
*  And it's dependent on certain goals that we all share. [[00:15:38](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=938.48s)]
*  Now, this is something that I think you're inclined to agree with, too, that if we talk about morality as a set of as the philosopher, [[00:15:42](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=942.48s)]
*  Philippa Foot once said, hypothetical imperatives as a set of sort of if then statements, if you want to be X, then you should do Y. [[00:15:49](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=949.48s)]
*  If you want to be healthy, then you should exercise and eat right and so on. [[00:15:59](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=959.48s)]
*  Then we can have objective answers to this question within the domain of us all sharing these goals. [[00:16:03](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=963.48s)]
*  However, I am in agreement with you. [[00:16:09](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=969.48s)]
*  I completely agree with that. Right. [[00:16:11](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=971.48s)]
*  Yes. And so outside of these goals that we all share, there's no way of talking about one state as better or worse than another one. [[00:16:13](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=973.48s)]
*  And that's where I think you and Sam Harris might disagree here. [[00:16:21](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=981.48s)]
*  And I'm on I'm on your side of that debate. [[00:16:25](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=985.48s)]
*  He has this thought experiment that he calls the worst possible suffering for everyone, where he says, imagine everyone is in total misery all the time. [[00:16:27](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=987.48s)]
*  Clearly, that's objectively bad. [[00:16:36](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=996.48s)]
*  But I think it only makes sense to say that that's objectively bad if we already are within the realm of caring about suffering and avoiding suffering. [[00:16:38](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=998.48s)]
*  Yeah. I mean, to me, it's it's much like saying, well, aesthetic judgments are objective because if we imagine the world's ugliest painting, everyone would agree that it's ugly. [[00:16:49](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1009.48s)]
*  Therefore, it must be objective. [[00:16:58](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1018.48s)]
*  But I think that, you know, I think it's OK to admit that we're contingent human beings made of atoms, obeying the laws of physics. [[00:17:00](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1020.48s)]
*  And this thing called our moral goals are very reliant on historical contingent things. [[00:17:07](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1027.48s)]
*  Right. Like different people could have different goals. [[00:17:17](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1037.48s)]
*  But happily, we agree on a lot and we can build from what we agree sensible moral systems. [[00:17:19](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1039.48s)]
*  Yes, I totally agree. [[00:17:26](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1046.48s)]
*  Now, the question is, is which of these internally consistent and all apparently intuitive different sets of rules should we select from? [[00:17:28](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1048.48s)]
*  And we're going to have to make choices like that, because, as you said, in constructing a self-driving car, we're going to have to decide which paths that result in collisions are better and worse than others. [[00:17:41](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1061.48s)]
*  And it would be easy. It would be really wonderful if we could just avoid all collisions that would be make my job completely unnecessary. [[00:17:54](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1074.48s)]
*  And I am fine with that. If it turns out that we can avoid any kind of harm to anybody, then we don't need ethics at all. [[00:18:04](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1084.48s)]
*  But every time the machine is going to be evaluating one path and comparing it to another, it's going to have to decide which collisions are better and worse. [[00:18:14](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1094.48s)]
*  Now, there's a lot of ways of doing it. It could, for instance, consider the driver and the passengers of its vehicle as more valuable than the other passengers. [[00:18:24](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1104.48s)]
*  It could consider swerving to hit someone as better or worse than continuing straight. [[00:18:33](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1113.48s)]
*  It could view hitting and injuring more people as better or worse than fewer people. [[00:18:40](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1120.48s)]
*  Now, some of those might seem more obvious to you or less obvious to you, but the problem is how do we actually resolve this? [[00:18:47](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1127.48s)]
*  And I think we need a moral theory. And for that, like I was saying, we need some framework of comparing these theories. [[00:18:53](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1133.48s)]
*  And I think the only way of doing that is by saying which moral theory actually fulfills this meta ethical assumption that moral theories evolve for the purpose of promoting cooperative behavior amongst self-interested organisms. [[00:19:01](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1141.48s)]
*  If you have a moral theory that actually is better at creating cooperative behavior than other kinds of theories, then that's the one we should use, I propose, in designing these vehicles. [[00:19:17](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1157.48s)]
*  Yeah, maybe it's good to clear up this issue before moving on to the specifics of the different moral theories. [[00:19:29](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1169.48s)]
*  There is this objection out there to this kind of discussion that says, you know, who cares? Cars are not really going to be solving trolley problems. [[00:19:35](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1175.48s)]
*  If they see something bad, they're just going to hit the brakes and stop. And so this is all kind of irrelevant. [[00:19:43](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1183.48s)]
*  And I suspect that's just an impoverished view of how moral reasoning works. [[00:19:48](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1188.48s)]
*  And, you know, people get annoyed with philosophers for inventing trolley problems because they say, well, I just don't want to play this game. [[00:19:54](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1194.48s)]
*  But it's a way of sharpening our intuition. Right. [[00:20:02](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1202.48s)]
*  And actually, it'd be useful here if you laid out what a trolley problem is. [[00:20:04](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1204.48s)]
*  It's conceivable that some of the audience doesn't know. [[00:20:09](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1209.48s)]
*  And you also, you know, you mentioned in the book that Harry Truman's decision to drop an atomic bomb on Japan was very much like a trolley problem. [[00:20:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1212.48s)]
*  Yes, that's right. So these do happen. [[00:20:20](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1220.48s)]
*  And unfortunately, they happen quite often in certain professions, like in medicine and warfare and business, where you have to make choices about causing harm to one person or allowing harm to many other people. [[00:20:22](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1222.48s)]
*  Now, this sounds very strange to most people like me because I think about myself as well. [[00:20:36](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1236.48s)]
*  I'm just going through my day. I'm buying cappuccinos. I'm grading papers. I'm watching Netflix. [[00:20:44](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1244.48s)]
*  What am I doing that is causing harm to others? [[00:20:50](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1250.48s)]
*  But in fact, if you're buying that cappuccino, you are weighing your own pleasure against the happiness and suffering of other people who you could be donating that money to. [[00:20:53](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1253.48s)]
*  If you're watching Netflix, you could be doing more with your time. If you're eating meat, you're making judgments about whether animals are valuable or not. [[00:21:05](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1265.48s)]
*  And the trolley problem is one of the scenarios that was constructed in order to demonstrate the differences between well, initially between doing and allowing and killing many people versus one or I should say sacrificing one person to save many. [[00:21:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1272.48s)]
*  So in the scenario, you have a runaway train going down a track. It's going to hit five people. [[00:21:32](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1292.48s)]
*  And the only way to stop it is to either divert it onto a sidetrack where a single person is or in another condition to push a large man in front of the train and stop the train. [[00:21:37](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1297.48s)]
*  Now, it turns out that most people say it's permissible in surveys to switch the track onto the single person, but not permissible to push the large man to his death, even though that seems strangely inconsistent. [[00:21:49](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1309.48s)]
*  If you are only considering the consequences, then it is the same exact effect. It's killing one to save five. [[00:22:03](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1323.48s)]
*  But if you think that actually performing some kind of physical intrusion into somebody else's personal space is important, or if you think about this in terms of a causal chain or your intentions or something like that, then maybe there is a difference here. [[00:22:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1332.48s)]
*  So the trolley problem is one way of comparing what different moral theories might say. Utilitarians will say you always kill one to save five. [[00:22:29](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1349.48s)]
*  A lot of deontologists, what they have in common, aside from just focusing on rights and duties, is that they often say that there's nothing wrong with just standing back and allowing things to happen. [[00:22:38](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1358.48s)]
*  So this is called the difference between a positive and a negative obligation. [[00:22:51](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1371.48s)]
*  The philosopher Robert Nozick made a big deal about this and said, look, it's wrong of me to push you in front of a train. [[00:22:56](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1376.48s)]
*  It's not wrong for me to allow you to be hit by a train. [[00:23:03](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1383.48s)]
*  And so according to most deontologists, it's wrong for me to push you in front of a train. [[00:23:07](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1387.48s)]
*  And it might also be wrong for me to pull that switch, causing the train to go on to you. [[00:23:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1392.48s)]
*  But I think that it's a very helpful illuminating thought experiment precisely because how we viscerally think about what's morally right and wrong might not match with what we say we're thinking, right? [[00:23:17](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1397.48s)]
*  I mean, that's what the pushing the guy off of the footbridge really brings home, that if you would ask someone abstractly, given two choices, would you let five people die or one person die? [[00:23:30](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1410.48s)]
*  But then when you make it concrete, you have to actually kill this one person to save the other five that are unwilling to do it. [[00:23:43](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1423.48s)]
*  And that's kind of OK. And I don't think it's irrational. [[00:23:51](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1431.48s)]
*  It's just revealing that our moral intuitions are not always very coherent or matching up with our moral cognition. [[00:23:54](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1434.48s)]
*  Yeah. And as someone who's taught ethics classes to undergrads for a long time, I'm very familiar with how sort of inconsistent people's everyday moral intuitions are. [[00:24:01](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1441.48s)]
*  You know, if you press just a little bit on some of these trolley problems like, well, it's OK to pull the switch. [[00:24:13](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1453.48s)]
*  And in fact, you should pull the switch to kill one, but to save five. [[00:24:20](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1460.48s)]
*  But what if it was your mother or your father or your best friend on the sidetrack? [[00:24:23](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1463.48s)]
*  Well, then people change their minds and then you ask them, well, why do you think it's OK for you to value your parents over other people's parents? [[00:24:27](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1467.48s)]
*  It's surely these people are equally valuable. [[00:24:36](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1476.48s)]
*  You're not saying that your family is actually better than other people's families. [[00:24:39](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1479.48s)]
*  But it turns out that people's moral judgments actually are sensitive to this. [[00:24:44](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1484.48s)]
*  There was a fantastic experiment done by some researchers led by April Bleszki-Rechik. [[00:24:47](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1487.48s)]
*  And they found that alternating by condition, the genetic relatedness of the person on the sidetrack will alternate whether people are willing to pull the switch almost exactly as you would predict by the proportion of their genetic relatedness. [[00:24:54](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1494.48s)]
*  So brother, cousin and so on. Right. [[00:25:09](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1509.48s)]
*  But most of the- [[00:25:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1512.48s)]
*  What is the biggest joke? I would sacrifice, you know, my brother or sister to save two cousins or something like that? [[00:25:13](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1513.48s)]
*  Yeah, exactly. I think that was a quote from, was it Herbert Spencer or something? [[00:25:19](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1519.48s)]
*  Yeah, that's right. [[00:25:23](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1523.48s)]
*  And so that but most moral theories agree that genetic relatedness should not play a role. [[00:25:24](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1524.48s)]
*  And in fact, if you press people and ask, well, are you saying that people who are genetically related to you are better than the people who are not genetically related to you? [[00:25:30](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1530.48s)]
*  They'll say, well, of course not. That's silly. I would never say that. [[00:25:39](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1539.48s)]
*  But by acting in that way, they are revealing that judgment is is actually playing a role in their behavior. [[00:25:42](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1542.48s)]
*  Yeah. And I think that, again, part of it is that a lot of people have a presumption, whether it's explicit or implicit, that the right kind of ultimate moral theory will be something utilitarian. [[00:25:49](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1549.48s)]
*  But maybe it's not like maybe it's perfectly OK. [[00:26:02](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1562.48s)]
*  Or at least let's let's say not maybe, but let's say I can imagine a perfectly coherent moral theory that very explicitly gives more credit to people who are closer to me when I come to saving lives. [[00:26:06](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1566.48s)]
*  Yeah, exactly. So I mean, the utilitarian has to say some really, really weird things. [[00:26:18](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1578.48s)]
*  But the other moral theories also sometimes have to say some really, really weird things. [[00:26:24](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1584.48s)]
*  And this is what you have to get used to is that no consistent set of rules is going to give you everything that you want all the time. [[00:26:28](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1588.48s)]
*  And my theory that I'm advocating also tells me some things that I really don't like and I think is really weird. [[00:26:36](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1596.48s)]
*  Why don't we give you a chance to explain what your theory is, which is not completely yours. [[00:26:44](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1604.48s)]
*  You know, we've building on quite a tradition here. [[00:26:49](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1609.48s)]
*  That's right. So I am advocating a moral theory that is drawn from a tradition called contractarianism. [[00:26:51](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1611.48s)]
*  And the most recent version of this that I'm using was from a philosopher, an American philosopher named John Rawls. [[00:26:59](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1619.48s)]
*  And in his book, A Theory of Justice from 1971, he proposed that the best way of designing a fair society is to imagine that we were in this original position where I don't know who I'm going to be. [[00:27:08](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1628.48s)]
*  I could be anyone. [[00:27:24](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1644.48s)]
*  And in this kind of idealized bargaining position, he thought we would all come to agree on certain basic distributions of what he called primary goods. [[00:27:26](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1646.48s)]
*  And the distinction between primary goods and secondary goods are that when you don't know who you're going to be, you don't know if you're going to be male or female, tall or short, handicapped or perfectly abled, and so on. [[00:27:42](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1662.48s)]
*  And if that's the case, you don't know what kinds of particular things you're going to value. [[00:27:59](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1679.48s)]
*  Are you going to like television? Are you going to like coffee? Maybe, maybe not. These are all secondary goods. [[00:28:03](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1683.48s)]
*  Primary goods are the kinds of things that all human beings value no matter what, that all human beings have to value in order to pursue any kind of goal at all. [[00:28:08](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1688.48s)]
*  And this list includes things like your life, your health, your opportunity, and essential resources for survival. [[00:28:17](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1697.48s)]
*  So no matter what you want to do with your life, if you want to be a juggler, if you want to be a lawyer, if you want to be a physicist, you need to have essential resources, opportunities, and health. [[00:28:25](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1705.48s)]
*  Right. And so going back to our previous discussion, these are the kinds of things that we have, as you could call them, common ground that all human beings, as a matter of fact, just by virtue of being human beings, care about. [[00:28:37](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1717.48s)]
*  And if someone says, well, I don't value these things, I could say, yes, you do. You're a human being and you pursue goals. And so you care about your health and safety and opportunity. [[00:28:51](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1731.48s)]
*  You know, I actually took a class with John Rawls in graduate school. [[00:29:03](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1743.48s)]
*  Really? [[00:29:06](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1746.48s)]
*  Yes. It was very funny because I, well, I audited the class, but I went to the sections and everything. [[00:29:07](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1747.48s)]
*  And I remember one day walking with a friend of mine across campus and the people who had actually taken the class for a grade were coming out of the final exam. [[00:29:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1752.48s)]
*  And I just ran into them. And so I said, hey, you know, how did the exam go? And they were like, it's very fair. [[00:29:23](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1763.48s)]
*  Which, of course, you know, my friend who was like, that must not be your physics friends. Those must be your philosophy friends because no physics people have ever come out of an exam saying that was very fair. [[00:29:29](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1769.48s)]
*  But Rawls' whole thing was just as fairness and trying to make things as fair to everyone as possible. [[00:29:39](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1779.48s)]
*  That's right. And Rawls is primarily known as a political philosopher because the kinds of things he was talking about designing from this original position were mainly policies and the structure of our government and the way that we were going to do things. [[00:29:44](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1784.48s)]
*  And so he was talking about the structure of our government and social institutions. [[00:29:57](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1797.48s)]
*  However, towards the end of the book, he talks a little bit about using this as a framework for individual decision making. [[00:30:00](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1800.48s)]
*  And that's what I want to be doing. I want to say that we can also use this as a way of thinking about what kinds of actions are wrong and what kinds of actions are permissible. [[00:30:06](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1806.48s)]
*  And from the original position, Rawls said we would all agree on a certain distribution principle that he called the Maximin principle. [[00:30:17](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1817.48s)]
*  And the Maximin principle has a history from game theory. And in this context, it just means we would agree on a distribution which makes the poorest person as best off as possible. [[00:30:25](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1825.48s)]
*  Yeah, actually, I do want to get into this, but I realize now while you're saying this, there's a prior thing I want to just touch on very quickly. [[00:30:37](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1837.48s)]
*  You mentioned the fact that I think that Rawls himself would have cast his theory as a political one, right, a way to organize our society. [[00:30:45](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1845.48s)]
*  In fact, only a well-ordered society, right? I mean, he admits that there would be cases where things were in extreme distress where you'd have to violate his principles. [[00:30:55](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1855.48s)]
*  But the idea, as I understood it, was that we could disagree on basic moral conceptions. [[00:31:02](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1862.48s)]
*  But if we agreed to live together in a liberal democratic polity, then he had these rules for how to reconcile our different moral conceptions. [[00:31:09](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1869.48s)]
*  And so you're going a little bit farther because you want to actually use this as a theory of morality as well, right? [[00:31:19](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1879.48s)]
*  That's right. Well, I want to say that there are many kinds of values that are equally comparable to each other. [[00:31:26](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1886.48s)]
*  However, those all exist within a kind of space that is constrained by essentially this moral decision procedure. [[00:31:33](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1893.48s)]
*  So there are lots of equally good distributions according to the Maximen principle. [[00:31:42](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1902.48s)]
*  And within that space of equally good distributions of primary goods, [[00:31:50](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1910.48s)]
*  then we could go ahead and impose many different kinds of values and have interesting disagreements about which sets of values are the ones that are best. [[00:31:55](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1915.48s)]
*  But importantly, that's all occurring within the space of a sort of Maximen constraint. [[00:32:04](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1924.48s)]
*  But it is a little bit, it's asking a bit more, right? [[00:32:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1932.48s)]
*  Because the original position is something where we forget some things about ourselves, like you said, and we remember other things ourselves, the difference between primary versus secondary goals. [[00:32:15](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1935.48s)]
*  And this seems potentially more problematic if we want to get out of it moral rules rather than just a political system. [[00:32:26](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1946.48s)]
*  I mean, if someone is in real life very religious and has some religious convictions that strongly flavor their ideas of right and wrong, [[00:32:34](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1954.48s)]
*  are those convictions things they will have to forget in the original position? [[00:32:43](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1963.48s)]
*  Yes. [[00:32:47](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1967.48s)]
*  Wouldn't that lead them to something that's a moral theory coming out of the original position that is very different than the one they actually have? [[00:32:48](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1968.48s)]
*  Probably, yes. [[00:32:56](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1976.48s)]
*  But I don't think we need John Rawls to convince us that religion is irrelevant to ethics. [[00:32:57](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1977.48s)]
*  I think just some basic assumptions about what we mean by making moral choices can do that. [[00:33:04](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1984.48s)]
*  And this goes back to the dialogue, Euthyphro, by Plato, where he plausibly demonstrated that, [[00:33:10](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1990.48s)]
*  look, even if God were to say that slavery and child abuse and rape are morally good, that doesn't make them good. [[00:33:17](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=1997.48s)]
*  And so usually when I'm talking to somebody who thinks that morality is based on a certain set of religious beliefs, [[00:33:25](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2005.48s)]
*  it takes about 90 seconds of talking to them to get them to finally admit, well, yeah, I guess you're right, [[00:33:32](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2012.48s)]
*  that it doesn't really matter what the religious text says. What matters is something else. [[00:33:39](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2019.48s)]
*  I think that I'm on board with the, I can never pronounce it, Euthyphro? [[00:33:45](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2025.48s)]
*  Oh, Euthyphro, yeah. [[00:33:50](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2030.48s)]
*  Euthyphro, dilemma and why there should be some criteria for morality other than what God says. [[00:33:51](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2031.48s)]
*  But nevertheless, I think that I could imagine that the actual moral beliefs that a religious person has are different, [[00:33:57](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2037.48s)]
*  not because God gave them the moral beliefs, but because their religious beliefs affect how they think about the world, right? [[00:34:05](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2045.48s)]
*  How they think about the ontology of reality. [[00:34:13](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2053.48s)]
*  If you believe that human life begins at conception, you might have different views on abortion than if you believe it's just a bunch of cells obeying the laws of biology. [[00:34:15](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2055.48s)]
*  Yeah, that's interesting. I think the abortion case is difficult. [[00:34:24](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2064.48s)]
*  And in fact, people often, when they're talking about ethics, jump right to abortion, which is one of the most complicated moral topics. [[00:34:29](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2069.48s)]
*  And I usually like to point out, well, you're basically starting off at the introduction of the book and just skipping right to the most complicated problem at the very end, right? [[00:34:37](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2077.48s)]
*  There's a lot of stuff that goes on in between. [[00:34:47](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2087.48s)]
*  And most problems that we face are actually, I think, ones that are plausibly ones that have good moral answers to them. [[00:34:50](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2090.48s)]
*  Like, as I mentioned, slavery, child abuse, rape. [[00:35:02](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2102.48s)]
*  And then, of course, we get to more difficult cases like charity and eating animals and driving a fossil fuel vehicle, which I think are, in fact, things that most people think are maybe morally permissible or wrong and are probably mistaken about. [[00:35:06](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2106.48s)]
*  And then we get to very, very difficult cases like abortion, which even if there is not an answer to that, and I think there is probably an answer to it, but even if there is not, that doesn't invalidate everything that sort of went up to that point. [[00:35:25](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2125.48s)]
*  OK, but I'm just trying to get on the table the idea that when we get back to the self-driving cars killing people, solving their little trolley problems as they're going down the street, I could at least imagine that people have deep-seated moral convictions that wouldn't qualify in Rawls's conception as primary goods, and they might object to having those conceptions stripped away from them as we put them in the original position. [[00:35:43](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2143.48s)]
*  I think that's correct, but I also think it's correct that any group of people who are interacting with each other are going to have certain beliefs that are not respected in the process of sort of interacting. [[00:36:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2172.48s)]
*  And so if we are having a civil society together, just to take the political case, then inevitably some of your beliefs are probably going to come into conflict with other people's beliefs and with the institution. [[00:36:25](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2185.48s)]
*  So if I have a religious objection to say, oh, I don't know, respecting other people of different races, then you're going to say as a government, no, you actually have to treat everybody equally, and it doesn't matter what your religious beliefs are. [[00:36:37](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2197.48s)]
*  Right. Yeah, no, I think that the reason that I harp on this is I've become very, very interested in this potential conflict between fundamental moral positions that individuals might have and the goal that we presumably share of living in a liberal democratic society. [[00:36:55](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2215.48s)]
*  I think that we tend to paper over them these differences a little bit, but I think we should respect that they could be true conflicts and have to eventually say something like what you just said, which is that, yeah, you know, suck it up. [[00:37:13](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2233.48s)]
*  Some people are going to have to make compromises if we're going to live together like this. [[00:37:24](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2244.48s)]
*  Yeah, I totally agree. And I think instead of phrasing it, I mean, I would also say suck it up, but that's sort of the Pittsburgh in me. [[00:37:29](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2249.48s)]
*  I think that a more congenial way of phrasing that is that you might have very different religious values from me, but there are a set of values that we all share. [[00:37:37](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2257.48s)]
*  And what we need to base a moral theory on is the sort of universal grounds that we all have in common, the kinds of things that enable all human beings to pursue their goals. [[00:37:51](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2271.48s)]
*  You're right. That's a much more public relations friendly way of putting the point. [[00:38:01](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2281.48s)]
*  I think that's a wise way of doing it. OK, so I know you want to get to the Maximian principle. So do I. [[00:38:06](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2286.48s)]
*  But maybe even before we do that, I was I really liked in your book the casting things in terms of game theory and prisoners dilemmas and Nash equilibria and Pareto optimality and all these other buzzwords. [[00:38:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2292.48s)]
*  And I think that, you know, this is why we have an hour long podcast so we can actually explain a little bit how you're thinking, because it's a very helpful conceptual tool. [[00:38:26](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2306.48s)]
*  So why is game theory something that is useful tool to have in mind when we think about these issues? [[00:38:34](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2314.48s)]
*  Yeah, I am really excited to talk about this because I think this is the way forward in resolving these kinds of tensions between different consistent moral theories. [[00:38:41](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2321.48s)]
*  This kind of tool was only available to philosophers for the last 50, 60 years or so. [[00:38:52](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2332.48s)]
*  A lot of times when I tell people that I work on designing moral frameworks for machines, they'll say, well, haven't philosophers been doing this for thousands of years and they haven't gone anywhere? [[00:38:59](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2339.48s)]
*  Now, my first response is, well, just because a problem has been around for a long time doesn't mean it can't be solved. [[00:39:11](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2351.48s)]
*  And the second response is actually that I think the tools for solving these kinds of problems have really only emerged in the last 50 or 60 years or so. [[00:39:18](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2358.48s)]
*  And when we talk about evaluating theories based on which one promotes cooperative behavior, there's been a lot of talk in the history of philosophy about promoting cooperation. [[00:39:27](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2367.48s)]
*  The British philosopher Thomas Hobbes talks a lot about if we didn't have any kinds of rules, we would need to invent ones in order to cooperate. [[00:39:39](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2379.48s)]
*  You and I have been talking about living in a civil society and cooperating together. [[00:39:47](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2387.48s)]
*  But exactly what does this mean when we talk about cooperation? [[00:39:52](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2392.48s)]
*  Well, there is a very, very technical way of describing this, and you can describe it as a kind of improvement from simple self-interested behavior. [[00:39:56](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2396.48s)]
*  Self-interest is actually a very powerful tool. [[00:40:08](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2408.48s)]
*  And in the 1950s, John Nash described a certain method of showing how self-interested agents would come to certain equilibria in interactions, in games with each other. [[00:40:11](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2411.48s)]
*  And of course, games doesn't just mean poker and blackjack, but it can just mean any situation where two or more people are interacting and there are gains and losses for those people. [[00:40:25](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2425.48s)]
*  But poker was a big influence, right? That was a big inspiration. [[00:40:36](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2436.48s)]
*  That's right. Well, it also includes games too. [[00:40:39](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2439.48s)]
*  So what we're talking about here are cases, say, where the prisoner's dilemma is usually phrased in terms of sort of cops and robbers drama, where let's say that I arrest you for drug dealing and I know you're dealing drugs, but I don't have enough evidence to convict you. [[00:40:42](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2442.48s)]
*  Now, I make you a deal, you and your partner, a deal in separate rooms. [[00:41:03](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2463.48s)]
*  I say, look, if you will confess to the crime, I'll let you off free, but I'm going to put your partner away for good. [[00:41:08](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2468.48s)]
*  And you know, if you both stay quiet, that you actually get, let's say, a very low sentence. [[00:41:16](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2476.48s)]
*  If both of you confess, you both get a medium sentence. [[00:41:23](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2483.48s)]
*  And it turns out in this kind of scenario, a lot of people might think it's intuitive that you should stay quiet. [[00:41:27](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2487.48s)]
*  But according to Nash, you should both confess. You should both squeal on each other. [[00:41:35](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2495.48s)]
*  Right. Now what's weird about this is you've got a conflict where it turns out there is an improvement. [[00:41:41](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2501.48s)]
*  If both of you confess to the crime, then you both get a low sentence, but I'm sorry, a medium sentence. [[00:41:48](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2508.48s)]
*  But if both of you were to stay quiet, you would have both gotten a low sentence. [[00:41:57](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2517.48s)]
*  Now, that's what's called a Pareto improvement because it is improvement for everyone. [[00:42:02](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2522.48s)]
*  It doesn't make anyone worse off. And these are the kinds of improvements that economists think are the bare minimum for rationality. [[00:42:07](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2527.48s)]
*  Like if I finished my lunch and you're hungry and you're sitting next to me, I've still got some leftovers. [[00:42:14](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2534.48s)]
*  It seems obvious that I should just give some to you. It doesn't make me any worse off. And it makes you better off. [[00:42:19](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2539.48s)]
*  It's a Pareto improvement. It's from the Italian economist Vilfredo Pareto. [[00:42:25](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2545.48s)]
*  And so I'm defining cooperation problems as ones where self-interest here and self-interest I'm measuring as a Nash equilibrium. [[00:42:31](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2551.48s)]
*  In fact, there are lots of different situations that have different kinds of Nash equilibria. [[00:42:41](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2561.48s)]
*  You could have multiple Nash equilibria, but there exist Pareto improvements from Nash equilibria. [[00:42:46](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2566.48s)]
*  And if I understand it correctly, the Nash equilibrium is one where one person cannot unilaterally change to get a better outcome without hurting somebody else. [[00:42:53](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2573.48s)]
*  But a Pareto improvement would be where if we all change at once, we will all be better off. Is that right? [[00:43:06](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2586.48s)]
*  That's exactly right. And that's the challenge is that Pareto improvements from these Nash equilibria are not things that self-interested rational agents can do. [[00:43:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2592.48s)]
*  They're not capable of it. And so there's a cooperation problem. [[00:43:22](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2602.48s)]
*  Exactly. There's a problem. And this is the challenge that Thomas Hobbes in the 1600s was describing is that people in their own self-interest are going to be led to these issues. [[00:43:25](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2605.48s)]
*  And they're going to be led to these outcomes that are actually not optimal for everyone. [[00:43:35](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2615.48s)]
*  And so we need this sort of third party, as he called it, a Leviathan, or maybe a set of rules or a government to come in and force us to act in a way that's for everybody's mutual benefit. [[00:43:40](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2620.48s)]
*  Right. And being as he just alluded to, being Pareto optimal is kind of something that nobody could disagree with. [[00:43:52](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2632.48s)]
*  It's not necessarily the final answer to our right thing to do. [[00:44:02](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2642.48s)]
*  But if there is something where if everyone acted in a certain way, literally everybody would be better off or at least the same, then how could anyone object to that? [[00:44:06](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2646.48s)]
*  Exactly. And it's the kind of thing that you would expect in the evolutionary history of our species and other species as well would motivate certain adaptive traits to emerge, [[00:44:16](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2656.48s)]
*  would actually lead certain traits to emerge to force us to cooperate in places where we wouldn't have cooperated before. [[00:44:27](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2667.48s)]
*  And I liked that. And you go on to propose the repugnant prisoner's dilemma as an illustration of how straightforward utilitarianism can lead us wrong. [[00:44:36](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2676.48s)]
*  And it's kind of a version of the utility monster thought experiment that I guess was it Nozick who proposed that? [[00:44:47](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2687.48s)]
*  Yeah. [[00:44:53](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2693.48s)]
*  And where, you know, if one person can become way better off and everyone else suffers just a little bit, straightforward utilitarianism would say, yeah, sure, you know, like, let everyone suffer just a little bit because this one person would be so much better off. [[00:44:54](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2694.48s)]
*  And you would argue that that's probably not the moral strategy we want to pursue. [[00:45:08](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2708.48s)]
*  That's right. So the great thing about defining cooperation in this very formal sense is that we could actually go on and test which of our moral theories produce more and less cooperative solutions. [[00:45:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2712.48s)]
*  So I think that in most cases, like the prisoner's dilemma, the regular prisoner's dilemma, it turns out that utilitarianism, contractarianism, natural rights theories, Kantian ethics, they all produce the correct result. [[00:45:27](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2727.48s)]
*  They all produce mutual cooperation, which is great. [[00:45:42](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2742.48s)]
*  And I think our moral intuitions, this sort of mixed bag of cognitive mechanisms that have over time evolved to make these kinds of choices that they also in most situations do a great job of promoting cooperative behavior. [[00:45:45](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2745.48s)]
*  But you can. [[00:46:02](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2762.48s)]
*  Except maybe libertarianism does not get the same answer for the iterative prisoner's dilemma? [[00:46:03](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2763.48s)]
*  Maybe. Yeah, it depends on how you define causing the harm. [[00:46:08](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2768.48s)]
*  So if you this is something that Nozick talks a lot about in Anarchy, State, and Utopia, which was his rebuttal in the 70s to to Rawls. [[00:46:14](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2774.48s)]
*  He said that if you're talking about causing harm as sort of doing something where if you had done otherwise, she would have been worse off than in this case, maybe the prisoner's dilemma is not an instance of causing harm to the other person if you confess or if you stay quiet. [[00:46:23](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2783.48s)]
*  But it's it's difficult to say in that scenario, really what counts as causing harm. [[00:46:45](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2805.48s)]
*  But but nevertheless, I think I take your point that for the conventional prisoner's dilemma, we have a national equilibrium where everyone defects. [[00:46:51](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2811.48s)]
*  But most sensible people would say that both players in the game are better off if they cooperate. [[00:46:59](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2819.48s)]
*  And so we can have that as a starting point of agreement and work from there. [[00:47:05](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2825.48s)]
*  Yeah. And it's even more than most sensible people. [[00:47:09](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2829.48s)]
*  It's over time. If you are a utilitarian or a contractarian playing prisoner's dilemma over and over and over again, you will get better outcomes. [[00:47:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2832.48s)]
*  If you're measuring this in money, you'll make more money over time. If you're measuring this in children, you'll have more children over time. [[00:47:21](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2841.48s)]
*  Right. OK, so now I'm going to finally let you tell us what a good contractarian believes. [[00:47:28](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2848.48s)]
*  I mean, you mentioned the Maximin principle, but how is it different? [[00:47:35](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2855.48s)]
*  How would a good contractarian approach something like a prisoner's dilemma or other sorts of games differently than a straightforward utilitarian would? [[00:47:38](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2858.48s)]
*  Right. So the Maximin principle says that we should prefer the distribution that makes the worst off person as best off as possible. [[00:47:46](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2866.48s)]
*  And usually what that means is you have a set of outcomes, you attach values to each of those outcomes, number values, and in each of the outcomes you pick the worst off, [[00:47:56](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2876.48s)]
*  then you put those into a group and select the highest of the lows. And that's the one you pick. [[00:48:07](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2887.48s)]
*  Now, in terms of distribution of money, that's fairly straightforward how you count and quantify those distributions. [[00:48:15](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2895.48s)]
*  But in terms of other kinds of goods, it might be a little more complicated. [[00:48:22](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2902.48s)]
*  However, utilitarians have been spending years, decades, centuries to try to convince us that pleasures and pains can be quantified and counted. [[00:48:27](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2907.48s)]
*  And added up. [[00:48:39](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2919.48s)]
*  And added up. That's right. And so the utilitarian wants to run essentially a summation function over all of this and just pick the highest of the sums. [[00:48:41](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2921.48s)]
*  Now, those usually produce very similar answers. So in the prisoner's dilemma, it turns out that adding up all the outcomes and running Maximin both say that we should cooperate with each other. [[00:48:51](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2931.48s)]
*  Right. [[00:49:04](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2944.48s)]
*  But in other scenarios, you could arrange it. And I mentioned or you mentioned the repugnant prisoner's dilemma that I set up. [[00:49:05](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2945.48s)]
*  You could arrange scenarios where the sum of all the payoffs for people is actually not either what Maximin would say, nor is it what Pareto optimality would predict over just self-interested behavior. [[00:49:13](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2953.48s)]
*  And for that reason, I think that the Maximin principle is actually the better principle than the utilitarian one. [[00:49:28](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2968.48s)]
*  And it's precisely because of this possibility that there could be great gains for one person, but other people have to suffer because of it. [[00:49:35](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2975.48s)]
*  Exactly. And that's almost a prediction of the theory, not necessarily a motivation for it. [[00:49:43](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2983.48s)]
*  The real motivation for it is that it produces cooperative behavior in all scenarios. [[00:49:51](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2991.48s)]
*  The prediction is that it will make the worst off as well off as possible. [[00:49:56](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=2996.48s)]
*  Usually this makes a lot of sense intuitively. So in a, like you said, a liberal democracy, very often progressives are wanting to benefit the poor before we benefit the rich, that they should have priority. [[00:50:03](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3003.48s)]
*  So this is often called a prioritarian principle. However, there are some other situations where it wouldn't be prioritarian. [[00:50:19](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3019.48s)]
*  I think the real where the rubber meets the road here is when you start actually attaching values to outcomes and you have to do that by saying here are the primary goods. [[00:50:26](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3026.48s)]
*  What I was saying earlier are the kinds of goods that all human beings from the original position would care about our health, our safety, our opportunity. [[00:50:38](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3038.48s)]
*  And you try to quantify them and then calculate the effects of your action on those goods. [[00:50:48](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3048.48s)]
*  So if I say, look, if I'm going to punch you in the face, I might get some amount of pleasure from that. [[00:50:55](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3055.48s)]
*  If I'm some sadistic weirdo, I don't want to do that. But if I did, but it would be terrible in terms of your health and opportunity. [[00:51:01](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3061.48s)]
*  And that loss to you in primary goods is not equivalent or not made up for the gain to me in the secondary goods that I get, namely my pleasure or something like that. [[00:51:10](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3070.48s)]
*  And so when we talk about applying this to self-driving cars, what we need to do is we need to have a way of quantifying the effects of every collision on the health and safety of the passengers, of pedestrians, of people in other cars. [[00:51:22](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3082.48s)]
*  And then what a contractarian would do is run a maxi-men function over all of that and say, here are three different collisions. [[00:51:38](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3098.48s)]
*  What are the worst health outcomes in each of these collisions? And I'm going to pick the best of the worst case scenarios. [[00:51:46](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3106.48s)]
*  So that sounds like something sensible. But just before we dig into that, I think it's safe to say that in the political sphere, where Ross was originally talking about, his reasoning does seem to lead us to quite a redistributive way of running society, right? [[00:51:54](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3114.48s)]
*  And very worse off people have to be improved by any inequality that we allow. [[00:52:14](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3134.48s)]
*  So it's a very different world than where we actually live, where in modern capitalism in the United States, there's plenty of people suffering a lot with the idea that there's other people who are doing really well off because of economic growth. [[00:52:20](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3140.48s)]
*  And that's what a trade-off we're willing to make. [[00:52:33](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3153.48s)]
*  That's right. Now, if you're utilitarian, you care very much about the suffering of these other people. [[00:52:36](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3156.48s)]
*  So you use the word suffering. But if I'm a contractarian, I don't care about their suffering. [[00:52:42](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3162.48s)]
*  I care about the distribution of primary goods, namely their health, their safety, their essential resources. [[00:52:47](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3167.48s)]
*  And so what I care about is making sure that the worst off people in the population are brought up to a minimal level of let's just call it normal functioning. [[00:52:54](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3174.48s)]
*  Right. Now, there's a lot of discussion about this in bioethics, about what is normal functioning, how do we quantify normal functioning. [[00:53:04](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3184.48s)]
*  But that's essentially what a contractarian is trying to do, to bring everybody up to a minimal threshold of opportunity and safety, but not happiness. [[00:53:11](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3191.48s)]
*  In fact, contractarians don't care about happiness. [[00:53:21](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3201.48s)]
*  Happiness is not the good that we are calculating. [[00:53:24](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3204.48s)]
*  Well, but for Rawls, certainly wealth that you an individual has would be among the goods that we do calculate. [[00:53:29](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3209.48s)]
*  Right. When we talk about the difference principle saying that inequalities should only be allowed to the extent that everyone is better off. [[00:53:37](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3217.48s)]
*  Wealth is among the things that makes us better off. [[00:53:44](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3224.48s)]
*  Sure. But only to the extent that wealth is able to get you the essential resources you need to pursue goals. [[00:53:47](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3227.48s)]
*  If you are a masochist and you enjoy suffering, that's fine. [[00:53:53](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3233.48s)]
*  As long as you have enough essential resources to continue being a masochist, then that's all a contractarian cares about. [[00:53:58](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3238.48s)]
*  Sure. That's right. [[00:54:05](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3245.48s)]
*  I'm just trying to because when we get to the self-driving cars, there will be competing conceptions of what the cars should be doing. [[00:54:06](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3246.48s)]
*  So I just want people to know there are analogous competing conceptions in the political arena. [[00:54:14](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3254.48s)]
*  Rawlsian, at least at face value, would be much more democratic socialist, whereas a libertarian would be much more capitalist in terms of how the economy should run itself. [[00:54:20](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3260.48s)]
*  And these are both plausible theories that we can argue about. [[00:54:33](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3273.48s)]
*  That's right. Yes. [[00:54:36](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3276.48s)]
*  Good. So then when we come to the cars, you're going to try to implement some kind of Maximin algorithm in the mind of a self-driving car. [[00:54:37](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3277.48s)]
*  That's right. So I think there needs to be a database of collisions and the effects of these collisions on most people of comparable, let's say, size and position. [[00:54:49](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3289.48s)]
*  Right now, this is something that you might think is really, really complicated and even maybe a little bit silly. [[00:55:02](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3302.48s)]
*  But I think the alternative is even sillier. [[00:55:09](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3309.48s)]
*  So right now, a lot of the major car companies have the official position of just saying, well, we think all collisions are bad and we want to avoid them all equally. [[00:55:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3312.48s)]
*  But I think that's an incredibly ridiculous position to take because not all collisions are equal. [[00:55:21](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3321.48s)]
*  Obviously, getting hit by a vehicle moving at two miles an hour is better than getting hit by a vehicle moving at 20 miles an hour. [[00:55:27](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3327.48s)]
*  And I want vehicles that are evaluating different paths to say that one collision is better than another. [[00:55:36](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3336.48s)]
*  Yeah. No, I think that I don't even quite understand the resistance to this way of thinking. [[00:55:43](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3343.48s)]
*  I mean, if someone says, what is the best economic system? [[00:55:50](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3350.48s)]
*  And someone else said, well, it's the system where everybody is wealthy. [[00:55:53](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3353.48s)]
*  That would not be very convincing to anyone. [[00:55:56](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3356.48s)]
*  You're like, well, that's not the world that we have to make some hard choices here. [[00:55:58](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3358.48s)]
*  And the cars are we should at least anticipate the reality that cars are going to be making some hard choices. [[00:56:01](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3361.48s)]
*  I think that's the reality that is slowly coming. [[00:56:07](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3367.48s)]
*  But I think it's sort of a public relations nightmare for an industry that is already working hard to just convince people that these things are safe at all, much less to convince them. [[00:56:10](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3370.48s)]
*  That they should be evaluating which kinds of collisions are better and worse. [[00:56:22](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3382.48s)]
*  And there is, I think, a point I think that you made this point in the book that hadn't quite sunk into my brain before reading it, which is that neither you, the human driver, nor the car, the artificial intelligence can say with perfect certainty what the outcome of a decision is going to be. [[00:56:27](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3387.48s)]
*  Therefore, even if it's rare that someone actually gets run over, a car will constantly be talking, will be making decisions between higher risk and lower risk actions. [[00:56:45](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3405.48s)]
*  And that is really quite down to earth and it's going to be common. [[00:56:55](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3415.48s)]
*  Right. [[00:56:59](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3419.48s)]
*  That's right. I mean, I've talked to a few people who are designing autonomous systems in the in the mostly in academics, not in the industry. [[00:57:00](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3420.48s)]
*  Industry doesn't want to talk at all about this kind of stuff. [[00:57:08](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3428.48s)]
*  And I can understand why. [[00:57:11](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3431.48s)]
*  But a lot of the people in academics who are working on this technology, I talked to, for instance, Benjamin Kuiper's who is building along with his former postdoc, Shonjin Park, they built this wonderful autonomous robot that moves around the halls of the University of Michigan and detects obstacles and slows down and tries to avoid them. [[00:57:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3432.48s)]
*  And Park used this system called model predictive control where it essentially casts out a net of many, many, many possible paths, many per second. [[00:57:32](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3452.48s)]
*  And then it prunes those paths based on the likelihood that each of them is going to result in a collision. [[00:57:43](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3463.48s)]
*  Now, likelihood is a really great method to evaluate paths. [[00:57:49](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3469.48s)]
*  I want to take the paths that are least likely to result in collisions. [[00:57:54](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3474.48s)]
*  But once again, I think we need more than just likelihood. [[00:57:57](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3477.48s)]
*  I think we also need to say a likely collision with a pedestrian is worse than a likely collision with a tree. [[00:58:01](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3481.48s)]
*  Right. Yeah, exactly. [[00:58:08](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3488.48s)]
*  So, I mean, is there how simple and straightforward does this suggested algorithm become when it comes to things like trolley problems or things like babies versus grownups or anything like that? [[00:58:10](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3490.48s)]
*  I mean, there still seems to be a lot of wishy washyness there. [[00:58:22](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3502.48s)]
*  Yeah, so it will tell us what kinds of information is relevant in making this database in the first place, which I think is really important. [[00:58:25](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3505.48s)]
*  So there was a recent experiment conducted by the MIT Media Lab that you're probably familiar with. [[00:58:34](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3514.48s)]
*  It was just published in Nature a couple of weeks ago, and it was called the Moral Machine Experiment. [[00:58:40](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3520.48s)]
*  Right. Yes. [[00:58:46](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3526.48s)]
*  And so what they did is they asked people to make choices about self-driving car trolley problems where they alternated things like the genders, the age, the social status of all the people involved. [[00:58:47](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3527.48s)]
*  So would you rather run over two doctors and a homeless person to save one obese man and a dog or something like that? [[00:59:00](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3540.48s)]
*  Now, the contractarian, as well as most moral theories, are going to say all of that information is irrelevant or most of it is irrelevant. [[00:59:09](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3549.48s)]
*  So whether a person is a doctor or a lawyer, whether a person is a Muslim or a Christian or an atheist, all of that is irrelevant. [[00:59:17](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3557.48s)]
*  But what is relevant are things like your physical position, your physical size, and maybe your age, because that information actually tells us about the effects of this collision with you. [[00:59:27](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3567.48s)]
*  And so this is important in figuring out what kinds of databases are going to be discriminatory against people and what kinds are not. [[00:59:40](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3580.48s)]
*  Well, you brought up this very interesting question. [[00:59:49](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3589.48s)]
*  Do you discriminate against people on motorcycles who are wearing helmets versus those who are not because presumably a collision with someone wearing a helmet will hurt them less than someone not wearing a helmet? [[00:59:52](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3592.48s)]
*  So we actually punish them in some sense. [[01:00:04](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3604.48s)]
*  Yeah. And that's one of the more counterintuitive predictions of my theory. [[01:00:07](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3607.48s)]
*  My theory says a lot of things that I find pretty intuitive, but a few things that I find counterintuitive. [[01:00:11](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3611.48s)]
*  And unfortunately, if my theory is correct, I just have to say, well, so much the worse for my intuitions here. [[01:00:18](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3618.48s)]
*  Part of the problem might be the use of the word punish. [[01:00:24](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3624.48s)]
*  So that's a little bit misleading. [[01:00:28](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3628.48s)]
*  If the car is going to evaluate a collision with a bicyclist without a helmet as worse than a collision with a bicyclist with a helmet, [[01:00:30](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3630.48s)]
*  that doesn't mean that it hates the one with a helmet or that it thinks that the one without a helmet deserves to die more than the one without a helmet. [[01:00:42](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3642.48s)]
*  It's only saying this path is less dangerous than that path. [[01:00:52](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3652.48s)]
*  And the reason why I agree with you, that seems really weird, is that you think, well, the person with the helmet was being safe. [[01:00:58](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3658.48s)]
*  She's the one who left the house that day taking precautions. [[01:01:06](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3666.48s)]
*  Why should she have the car target her or punish her more than the other one? [[01:01:10](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3670.48s)]
*  And the answer is, I think we need to stop using words like target or punish and just say that the path that leads to you was evaluated as better than the path that led to her. [[01:01:17](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3677.48s)]
*  OK, good. [[01:01:30](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3690.48s)]
*  And so I think, yeah, there's two big looming questions I'm not quite clear on here yet, but I think we can clear them up. [[01:01:31](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3691.48s)]
*  One is you seem to be saying that the contractarian just treats every human being equally, roughly speaking. [[01:01:36](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3696.48s)]
*  Maybe there's some health differences, like maybe a strong person wouldn't be as bad to get an accident with as a weak person because they're more likely to survive it. [[01:01:44](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3704.48s)]
*  But this is contrary to how many people's intuition goes. [[01:01:55](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3715.48s)]
*  One of the aspects of the MIT study, if I remember correctly, was that different people from different parts of the world gave different answers for injuring women versus men, young people versus old people, et cetera. [[01:02:00](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3720.48s)]
*  But you're saying that you're advocating being ignorant over all of that. [[01:02:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3732.48s)]
*  That's right. So if people are preferring to collide with men over women, my response would be that's sexism. [[01:02:17](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3737.48s)]
*  And that's not something we want to incorporate into our machine. [[01:02:24](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3744.48s)]
*  Right. Good. And but so some people are not going to agree with this, right? [[01:02:27](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3747.48s)]
*  You're going to have to try to convince them, but that's OK. [[01:02:31](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3751.48s)]
*  Well, yeah. So I have to keep stepping back until we find some grounds that we could agree on. [[01:02:33](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3753.48s)]
*  So I'll step back one step and say, OK, well, what moral theory are you using? [[01:02:39](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3759.48s)]
*  And in just about any moral theory, you're not going to value men more than women or vice versa, not utilitarianism, not Kantian ethics, nothing. [[01:02:44](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3764.48s)]
*  And if they say they still do, well, I'll take a further step back and say, OK, well, how should we even make decisions in the world? [[01:02:55](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3775.48s)]
*  Right. Should we just base off of the things that we all have in common? [[01:03:01](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3781.48s)]
*  And if we're agreeing on that, then I'm going to say contractarianism is the best way of cooperating based on the values that we all share. [[01:03:05](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3785.48s)]
*  But OK, what about babies versus grownups? [[01:03:15](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3795.48s)]
*  Babies versus grownups is difficult because a grownup, and when we're just talking about collisions, is more likely to survive a crash than a baby. [[01:03:18](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3798.48s)]
*  And so in that case, the baby should be preferred, but not because we love babies more or they're more adorable, but because they are more vulnerable. [[01:03:28](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3808.48s)]
*  OK, that makes sense. Good. [[01:03:37](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3817.48s)]
*  And then the other looming issue was, you know, let's be explicit about how we come down on the various trolley problem kind of scenarios here. [[01:03:39](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3819.48s)]
*  It sounds like contractarianism doesn't really care if one person versus five gets injured because of an active choice versus a passive one. [[01:03:48](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3828.48s)]
*  Right. It's just it is a consequentialist point of view at the end of the day. [[01:03:57](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3837.48s)]
*  Yeah, that's right. That's right. [[01:04:01](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3841.48s)]
*  So I do think we need to evaluate outcomes based on the distributions they produce. [[01:04:03](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3843.48s)]
*  And that is a kind of consequentialism. [[01:04:09](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3849.48s)]
*  And so in that way, I think utilitarianism and contractarianism are sort of cousins in this respect. [[01:04:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3852.48s)]
*  But I think the biggest difference is their way of quantifying the goods. [[01:04:18](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3858.48s)]
*  Do they quantify happiness and suffering or primary goods? [[01:04:23](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3863.48s)]
*  And do they run a summation function or a maximin function? [[01:04:27](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3867.48s)]
*  Right. And so in, say, the trolley problem, in most cases, they're going to agree. [[01:04:31](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3871.48s)]
*  But in some cases, they're going to disagree. [[01:04:36](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3876.48s)]
*  And in some cases, I find it really weird. [[01:04:38](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3878.48s)]
*  So here's here's one of those one of those cases. [[01:04:41](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3881.48s)]
*  So according to my theory, this is something that a friend of mine, the philosopher Susan Anderson, pointed out. [[01:04:43](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3883.48s)]
*  She pointed out that according to maximin, it would be better for the car to swerve into a crowd of 50 people and give them all a broken leg [[01:04:49](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3889.48s)]
*  rather than to swerve into a brick wall and give the passenger in the vehicle two broken legs. [[01:05:00](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3900.48s)]
*  Why is that? Because 50 broken 50 broken legs is or rather 50 single broken legs is better than one instance of two broken legs. [[01:05:05](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3905.48s)]
*  And I find that so strange. I find that crazy. [[01:05:17](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3917.48s)]
*  But once again, I just have to say, just like Jeremy Bentham did in the seventeen hundreds. [[01:05:22](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3922.48s)]
*  Well, my theory says it, so I have to accept it. [[01:05:26](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3926.48s)]
*  Jeremy Bentham was talking about homosexuality and he said, according to my theory, I guess it's all right. [[01:05:29](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3929.48s)]
*  Even though according to him, it was really weird and gross. [[01:05:35](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3935.48s)]
*  He had to accept it. [[01:05:38](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3938.48s)]
*  Well, I mean, I agree with what the consequences are. [[01:05:40](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3940.48s)]
*  Sometimes when our moral theories give us highly counterintuitive or weird sounding suggestions, [[01:05:46](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3946.48s)]
*  we need to say, well, maybe I have the wrong moral theory. Right. [[01:05:53](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3953.48s)]
*  Well, that's actually something that Rawls thought. [[01:05:56](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3956.48s)]
*  So Rawls agreed that we need to go through this process called reflective equilibrium, [[01:06:00](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3960.48s)]
*  where we sort of tune our own intuitions to the theories that we are developing. [[01:06:05](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3965.48s)]
*  However, that's where I think I would diverge from Rawls. [[01:06:11](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3971.48s)]
*  I would say that, look, if there's a matter of fact about which actions create more cooperative behavior than others, [[01:06:14](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3974.48s)]
*  then just like if the doctor tells me to stop smoking and I really, really want to, I have to say, well, look, [[01:06:22](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3982.48s)]
*  it's a matter of fact which actions are right and wrong or which actions are healthy and unhealthy. [[01:06:29](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3989.48s)]
*  But I could still say I don't want to do that or even I'm not going to do that. [[01:06:35](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=3995.48s)]
*  However, there's still a fact of the matter about what the right thing to do is. [[01:06:40](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4000.48s)]
*  Right. But we do need to make some choice about whether or not we've discovered that fact of the matter through our moral theorizing [[01:06:47](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4007.48s)]
*  or whether or not we should be a little bit less confident that you've chosen a function, right? [[01:06:53](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4013.48s)]
*  A utilitarian chooses a function over all utility, which is to say, add it all up and maximize it. [[01:07:00](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4020.48s)]
*  And you've in some sense chosen a function over utility, which is to say, [[01:07:06](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4026.48s)]
*  look at the utility of the worst off person and maximize that. [[01:07:09](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4029.48s)]
*  Right. And maybe there's some happy medium so that the 50 people don't get their legs broken. [[01:07:13](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4033.48s)]
*  I don't see how that would work, although I'm open to thinking about it. [[01:07:19](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4039.48s)]
*  So there are a lot of people who want to sort of have hybrid versions between these two. [[01:07:24](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4044.48s)]
*  But once again, the problem is if you want to mix the two theories together, [[01:07:29](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4049.48s)]
*  I think you need to have a third theory that tells you when do you take the utilitarian choice, [[01:07:33](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4053.48s)]
*  when do you take the contractarian choice. [[01:07:38](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4058.48s)]
*  And I just don't know what that third theory would look like. [[01:07:40](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4060.48s)]
*  Well, you say that now when the 50 people are suing you because they all have their legs broken, you might feel differently. [[01:07:43](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4063.48s)]
*  Oh, no, don't say that. Yeah, actually, someone at a conference recently joked to me that if I'm wildly successful beyond my dreams [[01:07:48](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4068.48s)]
*  and this actually was used in self-driving cars, I could be responsible for millions of injuries and deaths. [[01:07:57](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4077.48s)]
*  And I laughed because I know that that's not going to happen. Right. [[01:08:05](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4085.48s)]
*  But there was a part of me that sort of was a little bit afraid. [[01:08:10](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4090.48s)]
*  I mean, I know that the car companies need to work out some kind of solution to this. [[01:08:14](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4094.48s)]
*  And the problem is they're not talking about what they're doing. [[01:08:19](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4099.48s)]
*  Yeah, I mean, you could equally well say that if you succeed beyond your wildest dreams, [[01:08:23](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4103.48s)]
*  you'll be responsible for saving enormous amounts of death and suffering in the world. Right. [[01:08:27](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4107.48s)]
*  Sure. Sure. So let me ask you, if you don't mind, are you I assume you're sort of taking the more utilitarian approach here. [[01:08:31](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4111.48s)]
*  I got from what you were what you were saying that generally you take a sort of utilitarian, [[01:08:39](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4119.48s)]
*  although utilitarian constructivist from your from your big picture book, [[01:08:45](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4125.48s)]
*  but still more or less good old fashioned utilitarian approach to most kinds of decisions like this. [[01:08:49](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4129.48s)]
*  Actually, no, I'm just trying to give you a hard time because I just try to figure out what the right thing to do is. [[01:08:54](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4134.48s)]
*  I don't really have strong substantive moral theory myself. [[01:08:59](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4139.48s)]
*  Like I don't believe in utilitarianism because I totally buy the utility monster kind of responses or the repugnant conclusion. [[01:09:05](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4145.48s)]
*  I mean, Derek Parfit had this very similar argument that it would always be better just to have more children, [[01:09:15](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4155.48s)]
*  just had like more and more killed kids because there can be more and more people having happiness. [[01:09:22](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4162.48s)]
*  And I'm extraordinarily skeptical of the idea that we can, number one, calculate individual utility for people. [[01:09:27](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4167.48s)]
*  Maybe that's possible. But then number two, add them up on some commensurable scale seems like the wrong thing to do to me. [[01:09:33](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4173.48s)]
*  So I'm almost to the point where I'm willing to accept some kind of deontology rather than some kind of consequentialist way of thinking. [[01:09:40](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4180.48s)]
*  But I'm not quite sure what that would be. That's interesting. [[01:09:48](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4188.48s)]
*  So you mentioned two objections to utilitarianism. [[01:09:51](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4191.48s)]
*  One of them is that it doesn't match your intuitions on some weird cases. [[01:09:54](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4194.48s)]
*  And the other one is that it's just very, very hard to implement. It's hard to calculate pleasures and pains. [[01:09:58](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4198.48s)]
*  Right. Now, if I'm a utilitarian, I might say as to the first one, well, so much the worse for your intuitions. [[01:10:03](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4203.48s)]
*  And in addition, I might point out now I'm being a utilitarian for some reason, by the way. [[01:10:09](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4209.48s)]
*  But I might also point out that any moral theory is going to say really, really counterintuitive things. [[01:10:15](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4215.48s)]
*  So I'm not sure why we should care if there's a crazy sounding scenario with an alien where it doesn't seem to match our intuitions. [[01:10:21](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4221.48s)]
*  Do we do you expect that there's going to be a moral theory that matches all of your intuitions at some point? [[01:10:30](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4230.48s)]
*  No, but I actually do by Rawls's point on reflective equilibrium, because as a moral anti-realist, [[01:10:35](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4235.48s)]
*  I think that we're getting our our starting point for morality are our moral intuitions. [[01:10:41](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4241.48s)]
*  As a cognitive realist, I understand that those intuitions might be incoherent. [[01:10:46](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4246.48s)]
*  And therefore, there's work for moral philosophy to do in from our moral intuitions, building them into the best fit, sensible, logical, coherent system. [[01:10:52](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4252.48s)]
*  But I think so I think it's evidence when the system that I've tried to build is wildly in conflict with the intuitions I started with. [[01:11:04](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4264.48s)]
*  That might be either because I got to get rid of that intuition or because I did a bad job building a system. [[01:11:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4272.48s)]
*  I'm open to both possibilities. [[01:11:18](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4278.48s)]
*  I think that's fair. I think that's fair. [[01:11:20](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4280.48s)]
*  I think my main concern about using intuitions and evaluating the theory is that I'm so aware of the history of strong intuitions that have been false. [[01:11:22](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4282.48s)]
*  Sure. That I just give them virtually no evidential weight whatsoever. [[01:11:34](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4294.48s)]
*  I mean, in addition to that, you know, there's a lot of intuitions I have right now that I suspect almost any moral theory is going to tell me is wrong. [[01:11:38](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4298.48s)]
*  Like I said, I love the taste of meat. [[01:11:47](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4307.48s)]
*  Yeah. But any plausible moral theory is going to tell me that if I can lead a happy, healthy life without eating meat that I really should. [[01:11:50](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4310.48s)]
*  I love driving fossil fuel vehicles. [[01:11:58](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4318.48s)]
*  I love it. But most moral theories tell me that if that has terrible effects on the environment and I really don't need to be doing that. [[01:12:01](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4321.48s)]
*  I live in a city. Yeah. I have public transport. [[01:12:10](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4330.48s)]
*  So again, most moral theories are going to tell me things I really don't want to hear. [[01:12:13](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4333.48s)]
*  And I definitely agree. [[01:12:18](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4338.48s)]
*  We have to be open to throwing out this or that moral intuition or at least dramatically changing. [[01:12:20](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4340.48s)]
*  And I think this is what makes human beings pretty cool is that we don't only have our moral intuitions. [[01:12:27](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4347.48s)]
*  I mean, they're where we start, but we also have our rational cognitive capabilities and we can there's feedback, right? [[01:12:32](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4352.48s)]
*  We can go from rationality to alter our moral feelings. [[01:12:39](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4359.48s)]
*  And, you know, it could happen like I'm a I'm a meat eater and I drive a fossil fuel car. [[01:12:44](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4364.48s)]
*  I want to get rid of the fossil fuel car, but I don't I'm not going to get rid of eating the meat. [[01:12:49](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4369.48s)]
*  But I'd be happier if we could make artificial meat and wouldn't have to kill any animals to do it. [[01:12:53](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4373.48s)]
*  Sure. I could see that. [[01:12:58](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4378.48s)]
*  I mean, and the problem in appealing to pure or rational sort of corrections here is that if there's nothing outside of our intuitions that we're appealing to to correct them, [[01:13:00](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4380.48s)]
*  then I'm not sure how we escape the inevitability of an internally coherent system that's just completely mistaken. [[01:13:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4392.48s)]
*  Yeah, I don't necessarily believe that the word mistaken has any reference there in the world. [[01:13:22](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4402.48s)]
*  I think that I can understand that. [[01:13:27](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4407.48s)]
*  I think I think then we're just coming to blows about whether we think the function of moral theories is to produce cooperative behavior among self-interested organisms [[01:13:28](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4408.48s)]
*  or whether it's to produce sort of satisfying solutions according to our contingent intuitions that we all happen to share or some of us happen to share. [[01:13:39](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4419.48s)]
*  Yeah, I think that in both senses, we're trying to be, you know, coherent and rational, either individually or collectively. [[01:13:49](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4429.48s)]
*  So I think it's it's, you know, it's interesting to me is that people have strong disagreements about moral realism versus anti realism. [[01:13:56](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4436.48s)]
*  And those disagreements are almost entirely uncorrelated with their ideas about what actually is and is not moral. [[01:14:07](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4447.48s)]
*  That is really fascinating to me, too. [[01:14:14](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4454.48s)]
*  I find that in talking to most sort of well-educated people in my friend circles that they are explicitly moral relativists, but implicitly utilitarians. [[01:14:16](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4456.48s)]
*  Interesting. [[01:14:30](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4470.48s)]
*  And yeah, well, usually, like you said, sort of good utilitarians where they want they're willing to sacrifice one person to save many, [[01:14:31](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4471.48s)]
*  but then they also don't want to sacrifice cappuccinos and fossil fuel and eating meat and so on. [[01:14:38](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4478.48s)]
*  Right. [[01:14:44](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4484.48s)]
*  So if you push them far enough, maybe they'll admit that explicitly, but then they might fall back on relativism and say, well, it's all relative or something like that. [[01:14:45](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4485.48s)]
*  Well, they're relative when it's convenient. [[01:14:53](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4493.48s)]
*  Yeah, for me, utilitarianism is an example of something that I reason to myself out of as far as I'm concerned. [[01:14:55](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4495.48s)]
*  You know, I think that it sounds superficially like the right thing to do, but I think the objections to it are good enough that I'm looking for something better. [[01:15:00](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4500.48s)]
*  Yeah. [[01:15:08](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4508.48s)]
*  Just in case you're curious, I was going to bring this in. [[01:15:09](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4509.48s)]
*  I have a survey that was conducted by the website, PhilPapers.org, run by David Chalmers and his group. [[01:15:11](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4511.48s)]
*  And they asked professional philosophers, do you accept the category of normative ethics described as consequentialism, deontology, or virtue ethics? [[01:15:20](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4520.48s)]
*  Right. [[01:15:31](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4531.48s)]
*  And it's roughly split as 25% accept or lean deontology, 23% and some change accept or lean towards consequentialism, 18% towards virtue ethics, and 32.3% other. [[01:15:32](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4532.48s)]
*  And this actually reminds me of a poll that you took and you described in one of your blog posts about your survey of interpretations of quantum mechanics. [[01:15:50](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4550.48s)]
*  Yep, not a lot of consensus. [[01:15:59](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4559.48s)]
*  Yeah, not a lot of consensus. [[01:16:01](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4561.48s)]
*  And you called this a huge embarrassment for the field of physics. [[01:16:02](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4562.48s)]
*  And I kind of feel that way about my own field in some ways. [[01:16:07](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4567.48s)]
*  I must admit, I feel like it is a little embarrassing that these are not just theories that are sort of fun to think about, but they actually make a difference in how we live and how we design artificial intelligence. [[01:16:11](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4571.48s)]
*  And it turns out that there's not a lot of consensus in the field where there should be. [[01:16:28](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4588.48s)]
*  Do you have the numbers there for moral realism versus anti-realism? [[01:16:33](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4593.48s)]
*  I actually might. Yeah, hold on a second. [[01:16:36](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4596.48s)]
*  Because that was also a Phil Papers survey question. I remember that. [[01:16:38](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4598.48s)]
*  Yes, I do. [[01:16:42](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4602.48s)]
*  I think that most philosophers are realists, right? [[01:16:43](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4603.48s)]
*  That's right. 56.4 accept or lean towards realism, 27.7 anti and then 15% other. [[01:16:45](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4605.48s)]
*  All right. I mean, it's it is interesting. [[01:16:53](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4613.48s)]
*  I think it's more embarrassing that we don't have a consensus on quantum mechanics because quantum mechanics should be easier than ethics or morality. [[01:16:55](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4615.48s)]
*  But it's more important that we don't have a consensus on ethics or morality. [[01:17:03](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4623.48s)]
*  Right. That's where the analogy might end is that most of the versions of quantum mechanics, if I understand it, make essentially similar predictions. [[01:17:09](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4629.48s)]
*  However, the moral theories, although one could say in 99% of cases, most of the moral theories probably make the same predictions. [[01:17:17](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4637.48s)]
*  It's just these rare scenarios of especially involving, say, opportunity cost, what you could be doing instead of what you're doing right now, that there's the biggest and most important disagreements. [[01:17:26](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4646.48s)]
*  Sure. And if you're that crowd of 50 people is going to get their legs broken, it's extremely relevant to you that your car is programmed one way or the other. [[01:17:38](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4658.48s)]
*  So just to sort of wrap it up, put a bow on it. [[01:17:46](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4666.48s)]
*  I guess we glossed over a little bit about the implementability of this plan. [[01:17:49](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4669.48s)]
*  I mean, you sketched out a sort of a database idea where we would have all these different possibilities. [[01:17:56](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4676.48s)]
*  How real world is this prospect of making contractarianism the way that our self-driving cars go about making moral decisions? [[01:18:02](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4682.48s)]
*  Yeah, that's a terrific question. [[01:18:10](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4690.48s)]
*  And my answer is I don't know. [[01:18:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4692.48s)]
*  But if you are working on this kind of technology out there, I would love to hear from you. [[01:18:15](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4695.48s)]
*  I want to know how plausible is it to be able to design autonomous vehicles and other autonomous systems that can quantify the effects of these actions on primary goods and then run maximum functions over them? [[01:18:21](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4701.48s)]
*  Right. [[01:18:35](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4715.48s)]
*  I've talked to people in the field who say, well, it seems like this might be plausible. [[01:18:36](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4716.48s)]
*  I see my job as saying if we are going to design autonomous systems, here's what they need to be capable of doing. [[01:18:40](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4720.48s)]
*  And if they are not capable of doing this, then we should slow down or maybe even halt the development of this technology. [[01:18:49](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4729.48s)]
*  Right. Right. [[01:18:56](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4736.48s)]
*  And I think that's especially relevant in the domain of autonomous weapons systems. [[01:18:57](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4737.48s)]
*  Well, good. [[01:19:01](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4741.48s)]
*  So here are my final two questions, which could be short answers or longer. [[01:19:02](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4742.48s)]
*  But one of them is you brought up an issue in the book that, again, I was sort of surprised because I hadn't even thought of it. [[01:19:08](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4748.48s)]
*  Is it a problem if an artificially intelligent system does things that seem to be ethical to us, but it can't articulate why it's doing them? [[01:19:15](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4755.48s)]
*  This is an issue for deep learning systems, right, where it can recognize a picture, but it can't tell us why it recognized a picture. [[01:19:28](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4768.48s)]
*  A human being would be able to articulate an answer that might not be the correct reason why they did something, but at least it can try. [[01:19:34](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4774.48s)]
*  Should we expect the same from AI? [[01:19:41](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4781.48s)]
*  That's a great question. [[01:19:44](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4784.48s)]
*  And the answer is I'm not sure. [[01:19:45](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4785.48s)]
*  There's a Kantian position here that says that it's not a real decision getting back to the very first thing we talked about. [[01:19:48](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4788.48s)]
*  It's not a decision you're responsible for unless you can actually articulate the reasons for it. [[01:19:57](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4797.48s)]
*  You can't you can tell me why you did it. [[01:20:03](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4803.48s)]
*  Otherwise, you're just sort of an animal or a child acting on instinct. [[01:20:05](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4805.48s)]
*  Now, to me, it doesn't so much matter if you can articulate it. [[01:20:10](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4810.48s)]
*  What matters is, are you following the Maximin principle? [[01:20:14](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4814.48s)]
*  And I think the best way of doing this is actually constructing the Maximin principle in these autonomous systems in what's called a top down approach. [[01:20:18](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4818.48s)]
*  However, I'm also open to the possibility of what you might call approximating a Maximin principle through these more bottom up methods. [[01:20:27](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4827.48s)]
*  If the machine learning system produced outputs that always matched the Maximin principle in the kinds of cases we observe, [[01:20:37](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4837.48s)]
*  and we had good reason for thinking that it would continue to run this program that approximated Maximin in future cases, [[01:20:45](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4845.48s)]
*  then I would say that would be, let's say, close to good enough or sufficient in that case. [[01:20:53](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4853.48s)]
*  I think so. [[01:21:00](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4860.48s)]
*  I mean, I think it's a little bit too much to demand of our AI systems that they be articulate moral philosophers, as long as they seem to be doing mostly the right things. [[01:21:01](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4861.48s)]
*  Right. [[01:21:10](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4870.48s)]
*  Well, as long as it says something like, the reason why I chose this path instead of this path is that the worst collision in this path is better than the worst collision in that path. [[01:21:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4872.48s)]
*  It doesn't need to say need to say something like, I traveled into the original position and I realized from there that Maximin was actually out there. [[01:21:22](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4882.48s)]
*  Yeah, that'd be too much to ask. [[01:21:29](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4889.48s)]
*  And the other final question was something you already alluded to, a potential difference between the everyday life circumstance of a car driving around and trying to avoid accidents, [[01:21:30](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4890.48s)]
*  and the everyday but not everyone's life case of people at war, or machines that were intentionally built in order to inflict harm in a certain way. [[01:21:42](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4902.48s)]
*  How do the moral considerations change? [[01:21:54](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4914.48s)]
*  I realize it's a huge topic, but you know, maybe a simple introduction to the differences between that and everyday life in wartime. [[01:21:56](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4916.48s)]
*  Well, the smallest cases that this might be applied in right now are what you could call security robots. [[01:22:04](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4924.48s)]
*  Right. [[01:22:11](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4931.48s)]
*  And in fact, these are currently being used in some airports in China and other places in East Asia, where they have in some cases, taser technology equipped with them. [[01:22:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4932.48s)]
*  And so there are good things about this kind of technology. [[01:22:23](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4943.48s)]
*  And in fact, if someone is harming another person, it is good if a robot could step in and actually neutralize the threat. [[01:22:28](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4948.48s)]
*  But the problem is in doing so, it needs to be capable of identifying when there is a threat, what kind of threat this is, and what the proportional amount of force is to neutralize that threat. [[01:22:38](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4958.48s)]
*  And so contractarianism does make predictions about this. [[01:22:51](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4971.48s)]
*  If you could quantify the kind of harm being done by that threatening agent, that enemy agent, and you could say usually that neutralizing the threat is better than just say killing the agent, because that would be certainly making the agent now worst off. [[01:22:55](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4975.48s)]
*  Yeah. [[01:23:11](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4991.48s)]
*  But neutralizing the threat would be the best of all possible outcomes. [[01:23:12](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4992.48s)]
*  And so you could imagine security robots and in the extreme military robots being designed with their goal of neutralizing enemies and neutralizing threats, because I think that would be the Maximin approach to it. [[01:23:17](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=4997.48s)]
*  Right. [[01:23:32](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5012.48s)]
*  But Maximin seems to, maybe I'm just not conceptualizing it correctly, but it seems to fail us a little bit when literally our goal is to kill people. [[01:23:33](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5013.48s)]
*  That's right. [[01:23:43](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5023.48s)]
*  And I think that you could imagine cases, and I do imagine cases, where the ideal autonomous robot in war would be commanded to kill an enemy soldier and the robot would say, no, thank you, but I am going to apprehend him and take him into prison. [[01:23:45](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5025.48s)]
*  Right. [[01:24:05](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5045.48s)]
*  That's the goal. [[01:24:06](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5046.48s)]
*  I mean, going back to our good friend, Immanuel Kant, he famously and shockingly said, if God commands you to kill your own children, the correct response is, no, I'm not going to do that. [[01:24:07](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5047.48s)]
*  And we tell people in military ethics and the ethics of war that if your commanding officer tells you to kill innocent people in war, the correct answer is no, but I am happy to do other things that are not war crimes. [[01:24:19](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5059.48s)]
*  Do I remember correctly that in the book you suggested or at least wondered out loud whether or not it might be okay to ultimately have autonomous self-driving cars or drones and so forth, but not in the theater of war, that autonomy should be always in the hands of human agents who actually can take responsibility? [[01:24:35](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5075.48s)]
*  That's right. [[01:24:57](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5097.48s)]
*  There was a letter that was recently signed by a number of people who work on ethics and political philosophy, and these group of people were arguing that autonomous systems should not be used in war at all. [[01:24:58](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5098.48s)]
*  Now, I wouldn't go that far, but I would agree that the kinds of capabilities they would require in order to be, I don't want to use the word responsible, but in order to make the right choices in war are unlikely to happen anytime soon. [[01:25:13](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5113.48s)]
*  And so all of my claims are a big hypothetical, which is if we are going to design these kinds of machines, these are the kinds of capabilities that they would require. [[01:25:31](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5131.48s)]
*  And I'm willing to do that for military robots as well with more skepticism that they are actually going to achieve this level of sophistication than in the case of medical technology or transportation technology. [[01:25:43](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5143.48s)]
*  It's very interesting to me because I see a philosophical version of what happens in physics, in particular in science more generally, where concepts that we could have ignored at earlier times are forced to the forefront of our attention by the progress of technology. [[01:26:00](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5160.48s)]
*  And so it's a wonderful thing. I think it's a wonderful thing for philosophy that our discussions about morality are being sharpened a little bit by the fact that we can't be wishy washy. We can't be fuzzy about them. We can't just say be excellent to each other. [[01:26:17](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5177.48s)]
*  We need to tell machines that will listen to us quite literally how to behave in a wide variety of circumstances. [[01:26:34](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5194.48s)]
*  Yeah. And, you know, I have to admit a friend of mine pushed me on this position that I take. He said it was kind of bullshit what I'm doing because if I was taking a strong moral stance against autonomous weapons systems, can I say bullshit on your program? [[01:26:41](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5201.48s)]
*  Absolutely. [[01:26:56](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5216.48s)]
*  Absolutely. [[01:26:57](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5217.48s)]
*  Okay, so that it's kind of bullshit in that I am being hypocritical or I am I am not really caring about the use of this technology that in fact I'm just saying if you're going to build it, here's the right way of doing it. [[01:26:58](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5218.48s)]
*  And I'm sensitive to that objection. I'm very sensitive to it. I'm not convinced that autonomous weapons systems or autonomous vehicles or autonomous medical care bots are actually a good idea in the long run. [[01:27:14](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5234.48s)]
*  And I'm sensitive to the fact that maybe this position I'm taking is a little bit too corporate. [[01:27:28](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5248.48s)]
*  Well, but that that being said, if any corporations would like to pay me large amounts of money, I am more than available. [[01:27:38](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5258.48s)]
*  Very good. Well, I do hope they take you up on that. I think that I certainly on your side in thinking that this is something where we should face up to the problems rather than ignore them. [[01:27:44](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5264.48s)]
*  Derek Lieben, thanks so much for being on the podcast. [[01:27:53](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5273.48s)]
*  Thank you, Sean. [[01:27:56](https://www.youtube.com/watch?v=OqqL8YM4Ibo&t=5276.48s)]
