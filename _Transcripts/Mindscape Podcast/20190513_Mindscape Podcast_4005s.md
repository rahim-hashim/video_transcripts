---
Date Generated: June 10, 2024
Transcription Model: whisper medium 20231117
Length: 4005s
Video Keywords: ['robots']
Video Views: 7485
Video Rating: None
Video Description: Blog post with audio player, show notes, and transcript: https://www.preposterousuniverse.com/podcast/2019/05/13/episode-46-kate-darling-on-our-connections-with-robots/

Patreon: https://www.patreon.com/seanmcarroll

Most of us have no trouble telling the difference between a robot and a living, feeling organism. Nevertheless, our brains often treat robots as if they were alive. We give them names, imagine that they have emotions and inner mental states, get mad at them when they do the wrong thing or feel bad for them when they seem to be in distress. Kate Darling is a research at the MIT Media Lab who specializes in social robotics, the interactions between humans and machines. We talk about why we cannot help but anthropomorphize even very non-human-appearing robots, and what that means for legal and social issues now and in the future, including robot companions and helpers in various forms.

 Kate Darling has a degree in law as well as a doctorate of sciences from ETH Zurich. She currently works at the Media Lab at MIT, where she conducts research in social robotics and serves as an advisor on intellectual property policy. She is an affiliate at the Harvard Berkman Klein Center for Internet &amp; Society and at the Institute for Ethics and Emerging Technologies. Among her awards are the Mark T. Banner award in Intellectual Property from the American Bar Association. She is a contributing writer to Robohub and IEEE Spectrum.
---

# Episode 46: Kate Darling on Our Connections with Robots
**Mindscape Podcast:** [May 13, 2019](https://www.youtube.com/watch?v=3exiJVuYLc0)
*  Hello everyone and welcome to the Mindscape podcast. I'm your host Sean Carroll and today
*  we're talking about robots. We've talked about robots before on Mindscape, it's a natural
*  topic to imagine, but usually we're talking about what kinds of robots there are, what
*  they might be doing. Today we're going to be focusing on the human side. What do human
*  beings do? What should they do when they interact with robots? How should we think about robots
*  when we're dealing with them? Today's guest, Kate Darling, is a researcher at MIT's Media
*  Lab. She's not an engineer who builds robots, she actually comes from a social science background.
*  She's interested in what people should do with the fact that we tend to treat robots
*  as if they are human beings. We tend to anthropomorphize them. We assign, attribute feelings and emotions
*  and ideas to robots even when we know that they don't have them. You don't need to be
*  working like a human being as a robot to get another human being to treat you like
*  a person. We treat our Roombas like people, our laptops like people. So what's up with
*  that? What does it mean for the future? This is an area where, as very often when technology
*  is advancing, the legal system and the philosophy and how we think about these things lags behind.
*  So anything we can do to get into our brains what's going on ahead of time will be very
*  useful. There's one other thing, a big announcement that I have and I wanted to share with you,
*  which is that it looks like Mindscape will be getting ads, getting advertisements at
*  some point. We're not absolutely sure and I don't exactly know when it's going to happen,
*  but that seems to be the direction in which we're moving. I don't know when it's going
*  to happen so, but I'll let you know ahead of time. I did go back and forth on this.
*  I will be honest because obviously ads are good because they get money and I like money.
*  I will never try to pretend otherwise. On the other hand, you have to admit they kind
*  of get in the way of the actual substance of the podcast. I won't try to pretend that
*  either. What pushed me over the edge into agreeing with the ad thing was the idea that
*  it would get a lot more listeners. It's not that I'm just taking ads. I'll be joining
*  a network which will publicize the podcast elsewhere, bring it to new audiences. And
*  that I really think is important. Maybe I'm flattering myself, but I enjoy what we're
*  doing here. I think it's interesting. It's important. I want to share it with as many
*  people as we can. And a big part of that is just getting the word out and being part of
*  a network would be a huge benefit to that. It won't change the substance. It won't change
*  who I get, how long the podcast is, what we talk about, any of those things. It'll just
*  be a new platform on which the podcast will appear. So I'll keep you updated on that.
*  Who knows? Robots. Maybe the robots will be reading the ads. Maybe someday all podcasts
*  will just be invented by robots and then consumed by robots. Maybe the human beings will just
*  rake in the bucks at the end of the day. I don't know. That's what we're here to find
*  out. So let's go.
*  Kate Darling, welcome to the Mindscape Podcast. Thanks for having me, Sean. So you're at the
*  MIT Media Lab and you work on robots. So I think that for many of the people in the audience,
*  they will instantly assume that you spend your time building robots or programming robots
*  or something like that. But that's not what you do. In fact, you come from a legal background.
*  So tell us a little bit about how you got here and sort of what your job is there in
*  the robotic space.
*  Yeah, I sadly don't build robots. I'm very bad at that, although I have tried. I made
*  a little solar powered robot that it moves. It charges itself so that it moves a little
*  bit. And that was my great accomplishment in robotics. But yes, it's correct. I have
*  a legal and social sciences background. And the way I got here, the short version is I
*  love robots and found a way to do something with robots. But the longer version is probably
*  that I have always been interested in how systems shape human behavior. And so that's
*  what originally drew me to law and later to economics and then to technology, because
*  all three of those are systems that shape human behavior.
*  Sure. But you have a wonderful story that I'm sure you've told hundreds of times about
*  the first time that you started thinking about how human beings react to robots.
*  Yes, yes. I was still a law student at that point and I had bought a PLEO, which is this
*  baby dinosaur robot that came out in 2007. This Japanese toy. I bought it because, like
*  I mentioned, I've always loved robots. This was a really cool one. It had all these motors
*  and touch sensors. It responded to your touch. It was supposed to develop its own personality
*  depending on how you treated it. It had this infrared camera in the snout. And then one
*  of the things it had was a tilt sensor so it knew what direction it was facing. It knew
*  it was upside down and it would respond to that by mimicking distress and crying. It
*  was a pretty realistic depiction of being in pain. I thought that was super cool and
*  I would show it off to my friends and be like, oh, hold it up by the tail, see what it does.
*  One of my friends held it up for a very long time and it started to bother me. So I asked
*  him to put the robot back down and then I started to pet it to make it stop crying.
*  And that just kind of blew my mind because I wasn't a very maternal person but also
*  I knew exactly how the robot worked and I was compelled to be kind to it anyway. So
*  that really sparked my curiosity about human-robot interaction and our psychology around interacting
*  with robots. And I soon discovered that it's not just me.
*  Yeah. And it's interesting because it's teaching us, I mean at the surface level at least,
*  that insight doesn't teach us anything about robots. It teaches us something about human
*  beings and how we interface with the world.
*  Absolutely. I think the most fascinating thing to me about robotics is what robots can teach
*  us about human psychology, human behavior and interacting with each other.
*  So what do we know? I mean, can we at this point identify the features that your little
*  dinosaur had that made you connect with it? I mean, there's a whole gamut of different
*  kinds of robots from Roombas to self-driving cars. I mean, maybe even we should count the
*  little Tamagotchis, the little pets that get sold to kids, right? And to different levels,
*  we anthropomorphize these for some reason or another.
*  Oh, yeah. And it's not just robots either. We have this inherent tendency to anthropomorphize
*  anything. One of the first things a baby learns to recognize is a face, whether that's a
*  real face or just an image. And we have this deeply ingrained tendency to project ourselves
*  and our own human-like qualities onto other entities, whether that's our pets that we
*  see the dog looks guilty, whether the dog actually looks guilty. We don't really know.
*  Or I recently read the example that people will see a monkey yawning in the zoo and be
*  like, oh, the monkey's bored when really it's just showing off the teeth that can rip your
*  face off. So we will often just make all these assumptions about others. Kids develop
*  relationships to stuffed animals. We respond to a lot of different things. One of the things
*  that we respond pretty strongly to is movement. We've kind of been conditioned through evolution
*  to be able to recognize whether something is an agent and moving or whether something
*  is an object because we've needed to detect natural predators. At least that's the reason
*  that the evolutionary psychologists give us. But it's definitely true that studies show
*  that we're very quick to detect animal movement and autonomous seeming movement much more quickly
*  than other types of movement. And the thing about robots is that they move in exactly that type of
*  autonomous way. So robots are physical. They exist in the physical world, which we can respond to as
*  physical creatures. They move in this autonomous way. And then there are different design elements
*  that you can layer on top of that to make people really respond to robots as though they were a
*  living thing. So the baby dinosaur with the cute big eyes being one example. But people will even
*  have a response to the Roomba like you mentioned. Do you have a Roomba? I do not have a Roomba. No.
*  Do you have one? Are you attached to it? I used to have one. We did name it, which a lot of people
*  do. And I was just talking to the people from iRobot that made the first, the Roomba, the first
*  successful robot vacuum cleaner. And they say most people name their Roombas, they'll send them in to
*  get repaired and they'll want the same one back. They'll be like, oh, Merrill's sweep is broken.
*  We don't want a new one. We want you to fix Merrill's sweep. And people feel bad for the Roomba
*  when it gets stuck somewhere. So even just like a very simple robot that just moves around your
*  floor will cause people to have this emotional response. So it's probably multifaceted, right?
*  Because I think I don't know anything about Tamagotchis, but I do remember thinking that,
*  you know, hearing about the fad when they came out and people would care for their little virtual
*  pets and they were not embodied, right? They were just like on a screen. Have studies been done about
*  the different roles of being anthropomorphic, having a face, moving all these different aspects
*  and how they play into how we project organic nature onto these robots? Well, yeah, there's,
*  there are entire fields that research this. So the precursor to human robot interaction is probably
*  human computer interaction, which looked at, you know, the ways that people, or among other things,
*  the ways that people treat computers like social entities and like that.
*  We certainly blame them when things go wrong. Let's put it that way.
*  We blame them. Yeah, but people are also polite to them. There's this really great study that,
*  you know, Cliff Nass and some other people did back in the day where they showed that if you
*  do a task on a computer and then you're asked to rate the computer's performance,
*  and you're asked to rate it using a different computer, you'll be more honest and more negative
*  about how the computer performed than if you're asked to do it on the same computer because you
*  have this instinct that you don't want to hurt the computer's feelings. So people are weird.
*  We are such suckers for anthropomorphizing anything. You know, you mentioned the Tamagotchi.
*  There's also that video game portal that was really in like 12, 15 years ago. I don't remember.
*  In the game, you have this companion cube that comes with you. That's just this cube.
*  That's just with you in every level. And then at the very end of the game, I think it's safe to
*  spoil it, given how old the game is. You're supposed to incinerate the companion cube to
*  complete the last level. And the game designers were surprised to see that a lot of people would
*  sacrifice themselves instead of burn up the cube, just because we do become attached even to virtual
*  things. But physicality is an additional layer to that because there have been studies that show
*  that people will treat something in their physical space with even more empathy and even more like a
*  social actor than something on a screen.
*  Interesting. And so I will prod you to give the other example that I've heard you give.
*  It's not quite a study that you did, but you asked a bunch of people to sort of bond with some robots
*  and then you asked them to do horrible things.
*  This is with my friend Hannes Gosselt. We took the baby dinosaur robot that I now have four of at home.
*  We took five of them. We did this workshop at a conference. I think we had like 30 people and we
*  split them into five teams and gave them each a robot and told them to name the robot and play with
*  it. And we had them personified a little like they built like hats out of pipe cleaners and stuff.
*  We had them personified a little like they built like hats out of pipe cleaners and like did a
*  little fashion contest. And then after like 45 minutes of them bonding with this robot,
*  we unveiled a hammer and a hatchet and we told them to torture and kill them.
*  And it was so dramatic that we actually had to start improvising because we
*  had kind of expected there would be a split in the room. Like some people would be like,
*  oh sure, I'll hit it. It's just a machine. And some people would be like, no.
*  And we wanted to kind of escalate the violence and see if that split of people changed. That was
*  our original plan. But with this particular group, this was all adults between probably
*  25 and 40 years of age and they all absolutely refused to hit the robots. So we had to be like,
*  okay, what are we going to do? Okay, you can save your group's robot if you hit this other
*  group's robot with a hammer. And so they tried to do that and even that they couldn't, they just
*  couldn't do it. So we were finally like, okay, everyone, we're going to destroy all the robots
*  unless someone takes a hatchet to one of them. And this guy stood up and like took the hatchet and
*  the whole room kind of stood around and we watched him like bring the hatchet down on the robot's
*  neck. And it was very dramatic. Like people winced, they like turned their faces away. We could see
*  this on the photos that we took on our phones afterwards. It was really interesting. And then
*  there was this like half serious, half joking moment of silence for the fallen robot. So like
*  super dramatic workshop, you know, very interesting, not science, like you mentioned,
*  this was just a workshop, like there's so much going on there. But it did make me very curious
*  what would happen to look at some of the factors in a more scientific setting. So it led to some
*  research that I did later on at the Media Lab with Palash Nandi and Cynthia Brazil, where we
*  were looking at the correlation between people's empathy and their willingness to hit a robot.
*  Oh, okay. Good. Well, I definitely want to get into that. But first, this was done at a conference. I
*  mean, did the people there were robot people, I guess, or what kind of people were they?
*  I don't know. It was like one of, it was called Lyft. It was one of those like
*  innovation conferences that brought together designers and technologists and all sorts of people.
*  Okay. And what part of me wants to say, you know, that our immediate reaction, we're not in the room
*  killing the robots, we're hearing about other people who are reluctant to do it. And our
*  immediate reaction is, oh, that's silly. It's just a robot, right? You know, because we're outside the
*  milieu. But in some sense, it's a feature, not a bug, right? Like it's part of who we are that our
*  brains attribute, you know, meaningfulness and moral agency even to objects that we know are
*  completely inanimate. I'm glad you feel that way. I do as well. There are some people who say this
*  is a bad thing, and we need to discourage it. We need to educate people that they're dealing with
*  robots. And to some extent, I think that, you know, there are some problems that can arise.
*  For example, people's behavior might be able to be manipulated through technology. And we might
*  design technology in order to sell people products or even worse. And so those are some effects that
*  I think we need to be cautious about. But generally, I do not think it is a bad thing when a child is
*  kind to a Roomba. Like I love that people's first instinct is to be kind to another entity. And I
*  think that's part of humanity. And I think that's something that we should encourage and not
*  discourage. Yeah. And empathy. I had a podcast interview with Paul Bloom recently, and it's
*  clearly been a very good podcast because I keep referring to it in subsequent podcasts. But he's
*  a Yale psychologist. He wrote a book called Against Empathy. And his point is that empathy
*  is usually very unevenly applied. We have a lot of, it's much easier for us to be empathetic with
*  people like ourselves. And that distorts our view of the world. We're nicer to people like ourselves,
*  and you know, we're kinder, more just and so forth. And so instead of being empathetic,
*  he wants us to just be rational about what it means to be moral or immoral. I pushed back on
*  that a little bit because I actually think that empathy is very important to being a rational and
*  moral person in the sense that it becomes too easy to ignore people unlike ourselves when we don't
*  try to have empathy with them. You know, sure, it's easier to be empathetic with people like
*  ourselves. But it's necessary and important to try harder to be empathetic with a wider range of
*  people. I was not at the time thinking about robots at all, but you're saying that there's
*  some kind of relationship between how people interact with robots and how they're more widely
*  empathetic. Is that true? Well, that's certainly what our very preliminary research indicated.
*  We did look at the correlation between people's tendencies for empathy and how willing they were
*  to hit a robot. And there seemed to be a connection there that people who have very low empathic
*  concern for others, that they are much more likely to hit a very lifelike robot and people who have
*  high empathic concern for others are more likely to hesitate or even refuse. I find Paul interesting
*  because I both agree and disagree with him. Kind of like you, I think that, yes, it's true that it
*  can be problematic that we only relate to people who are like us or things that are like us.
*  It can be problematic that we relate to robots because we see ourselves in them and not to
*  certain other people that we don't see ourselves in. We've dehumanized entire swaths of other people.
*  And of course, we don't want to do that in favor of things like robots that don't inherently
*  deserve our empathy. But there does seem to be something to the emotional part of empathy in
*  that it kind of lights a fire in you and makes you care at all. Like you said, I think that it's too
*  easy if you're a completely rational human to just not even care. And so I've noticed myself
*  since becoming a mother that I've become so much more empathic generally, not just towards other
*  mothers because I've had this experience. But like, for example, in Boston in the winter,
*  I was walking around with a stroller and people sometimes don't shovel the sidewalks in front of
*  their houses. And I was like, what if someone's in a wheelchair? I suddenly realized how awful it
*  must be to be a disabled person in Boston. And so I feel like this emotion that I have extends and
*  makes me try and think of other people as well in situations where I might not have cared before.
*  **Matt Stauffer** Yeah, the flip side of both of those things, your example of having become a
*  mother and the fact that we feel empathetic towards robots, is that it points, it reminds us
*  of how involuntary a lot of who we are actually is, right? Like how much a lot of who we are is
*  a bunch of impulses that comes up from beneath the surface, not rational, careful cogitation about
*  right and wrong at a highly philosophically sophisticated level. **Samantha N. Bhutiawala**
*  Oh, absolutely. And I think that makes it somewhat uncomfortable. It's somewhat an
*  uncomfortable realization, but I think it's very important to understand how our relationships work,
*  why we empathize with another entity, how communication works, and that relationships
*  can be very one-sided. We're learning so much about ourselves by looking at how people interact
*  with robots or even computers, because we're starting to realize that it's all about ourselves
*  and all about us projecting ourselves onto others. And we might learn more about our human
*  relationships, human-to-human relationships in this as well. **Matt Stauffer** I mean, I mostly
*  want to talk about that side of things, but you did bring up, you tossed off the comment that
*  we know that robots don't actually deserve our empathy. So obviously I'm going to ask,
*  do we really know that? As robots become more and more lifelike, do we reach a point where we start
*  saying they do deserve our empathy? **Sara Hickman**
*  Sean, when I first got to MIT, I was like, oh yeah, I'm going to be in the place where they're
*  developing the cutting edge robotic technology. I'm so excited. And then I got here and all the
*  robots are broken or they're falling over. And we're nowhere close to developing a robot or an AI
*  that can feel anything. I do think that it's worth considering whether we need to treat robots a
*  certain way because it might have an impact on our own behavior. But I don't, like, I feel like the
*  whole robots deserve inherently rights because they have consciousness. Like that's a fun
*  conversation to have in a bar over beer. But it's not really, I don't think we're going to be there
*  anytime soon. It's not practical. **Sean Riggins**
*  Yeah. And I agree. And I think that most of what you are thinking about isn't even trying to get
*  there, right? I mean, you're not trying to build the most human conscious robot or even talking
*  to people who do. You're interested in how we deal with robots that are by complete agreement,
*  very mechanical in their insides, right? That there's no sense in which they have feelings.
*  **Kate Pleasant** Absolutely. They don't feel anything, but we feel for them. And that's the
*  interesting thing. **Sean Riggins** And so what are the applications here? I mean, what good is a robot?
*  What is it knowing that we care about robots? **Kate Pleasant** There are a couple interesting
*  applications. There have been some very hopeful results in health and education with using robots.
*  For example, for quite some time now, researchers have been looking at how to engage autistic
*  children with robots because they've noticed that kids on the spectrum will sometimes respond to
*  robots in a way that we haven't really seen before. And the leading researchers in the field
*  say that this is probably because the robot is a very social thing that they treat as a social
*  actor, but it doesn't come with all the baggage that an adult or another child would have.
*  **Sean Riggins** Very sympathetic. **Kate Pleasant** Yeah, very sympathetic robots.
*  Yeah, I agree with that. Maybe not just kids on the spectrum. But the interesting thing
*  is not only will they engage with the robot, they will engage more with other people in the room as
*  well if you bring in a robot to play with them. And until now, it's been kind of these one-off
*  studies in the lab that they've done, but they just recently last year did a longer-term study
*  in people's homes where they put robots in kids' homes that they interacted with for like half an
*  hour every day for a month. And their social skills went upward, like in a way that, you know,
*  on every measure that we care about for these kids, normally this would take thousands and
*  thousands of dollars worth of therapy to get those results, and they got it with having them
*  interact with a robot. Now, the catch was that the skills decreased again when they removed the robot
*  at the end of the study, but that shows that there's so much potential here. Another example
*  I like to make is... **Sean Riggins** Sorry, can I just ask for that? What kind of robot was it that
*  they were interacting with? **Meghan Lennon** I think they were using the Jibo platform. Jibo
*  was a consumer product for a few years, but recently shut their doors, but it's still
*  used as a research platform. So I think they were using a Jibo. **Sean Riggins** Did it look like a
*  human being or looked like a pet or looked like a fantasy creature? **Meghan Lennon** Oh yeah, the
*  Jibo looks kind of... I think the closest thing would be like the Pixar lamp. So it's... **Sean
*  Riggins** Oh, okay. **Meghan Lennon** It has a head that swivels and looks at you, and it has
*  this animated dot in the middle of the head. The head kind of looks like a fried egg, but it's
*  very cute. It will move around. It really gives you the sense of interacting with a social being.
*  It's actually really cleverly designed because when you try to make a robot too human-like or
*  too close to something that people are intimately familiar with, it really disappoints people's
*  expectations because they're expecting it to behave a certain way. Whereas if you have this
*  Pixar-like animated thing, people are much more willing to suspend their disbelief.
*  **Sean Riggins** But also it's interesting because one might have guessed that they would
*  want something that was sort of tactile, you know, like furry like a cat or a dog or something like
*  that, but this is manifestly technological. **Meghan Lennon** It is. There are furry ones,
*  which is the second example I was going to talk about. I suspect they use Jibo for this study
*  because like I mentioned, robots are always broken around here. And so it's very, very difficult to
*  do a study in people's homes where you can't go and fix the robot and have that consistency
*  that you need for a long-term study. And so they needed to use a product that was
*  stable enough to do that with. And I think Jibo was, that's why they chose Jibo.
*  **Sean Riggins** Sure. And so what was this other study?
*  **Meghan Lennon** So the other, not necessarily a study, but the other robot that I think
*  has a really promising application is the PARO baby seal robot. Have you seen that one?
*  **Sean Riggins** No.
*  **Meghan Lennon** So this one has been around for quite some time. They use it in nursing homes and
*  with dementia patients. It's really cheap. **Sean Riggins** I might have seen it actually.
*  **Meghan Lennon** And it's been like on TV shows. It was on the show Master of None.
*  You know, it's gotten some fame, but it's super cute. It makes these little movements. It gives
*  you the sense of nurturing this baby seal. This one's furry and soft to the touch. And it turns
*  out to be really important for people who are in a situation where they're just being taken care of
*  by others, that's their whole life now, to be given this sense of nurturing something that's
*  psychologically really valuable for them. And I know it sounds super creepy and people are like,
*  oh, that's so weird and gross that we're giving old people robots instead of human care. But what
*  it really is, is a replacement for animal therapy. And it's been very promising. They've been able to
*  use it instead of medication to calm distressed patients. And so it's really hard to argue that
*  that's a bad thing, especially if you stop considering it a human replacement and start
*  looking at it as animal therapy in a context where we can't use real animals for a lot of reasons.
*  **Matt Stauffer** Yeah, I mean, I can't imagine people objecting to this. I don't get it. Why
*  would you object to this? It's a therapy. Is that sort of the dark side of this fact that we're
*  reacting on the basis of our instincts, not our rationality? I mean, people have a reaction that
*  there's something intrinsically off-putting or inhuman about a robot compared to some other
*  strategy? **Samantha Nare-Bhagia** Well, I think people are just struggling with this.
*  In kind of Western Judeo-Christian society, we have this really stark divide between things that are
*  alive and things that aren't alive. And so our rational brains are like, oh, robots are machines,
*  they're not alive. And yet, our subconscious behavior towards robots is very much treating
*  them like living things. And I think that's just very confusing to people. And we haven't really
*  sorted that out in our culture yet. **Matt Stauffer** Yeah. For the autistic kids,
*  is there any more detailed explanation of what it is, where the benefit is coming from? Why is it
*  that the autistic kids are becoming more social in the presence of this robot?
*  **Samantha Nare-Bhagia** I don't know. I'm not intimately familiar with that. I'm not sure that
*  the researchers actually know, for example, why the robots work so well with these kids. But it's
*  certainly an opportunity to learn those things by studying this in more depth. **Matt Stauffer**
*  Right. I mean, for the dementia patients or for the elderly, you offered an explanation,
*  which makes perfect sense to me, that there's something that you get value from not just being
*  taken care of, but you're taking care of something, right? You are nurturing that something that is
*  valued. I suspect, having no expertise here whatsoever, that we do a disservice to a lot
*  of elderly people by removing all of their responsibilities and authority, right? Just
*  by trying to take care of them and put them in a very soft space where they can't do anything or
*  be hurt. And giving them a little bit more autonomy would be better along many dimensions.
*  **Samantha Nare-Bhagia** Yeah. And I guess people don't like it because they say it's fake autonomy.
*  They're not actually nurturing something. And so they don't think it's authentic. But I feel like
*  the way that we treat people now, we stick them in front of a TV or we medicate them or,
*  like you said, we give them no autonomy. I think that this is so much better than what we're
*  currently doing. **Matt Stauffer** And then on the other side, there's the fact that like it or not,
*  we're being surrounded by robots, right? Like forget about using them for therapy or whatever.
*  You know, whether we're working in factories or driving cars or in our kitchen,
*  do we human beings have to adapt to a new sort of way of dealing with the world when so much of
*  the world is robotic and able to move around? **Samantha Nare-Bhagia** Well, yeah, of course. I mean,
*  that's also why I'm confused. There are some people who are like, oh, it's bad that we treat
*  robots like living things. We need to educate this out of people. But I think that's just going to be
*  the new normal, really, because robots are moving into shared spaces right now. They've been
*  behind factory walls and now they're coming into all of these new areas, like you said,
*  into people's houses, into people's workplaces, into transportation systems, hospitals, the military.
*  And I think we have to roll with it and try to think about whether there are any
*  you know, harms that could come from people's interactions with robots, whether we need to
*  think about consumer protection issues, for example. But I really think we need to be
*  leaning into the positive effects as well within the knowledge that, you know, this,
*  I know this sounds like technological determinism, but you know, it's true. These robots are coming.
*  **Matt Stauffer** It's coming. No, I'm with you there. Yeah. **Samantha Nare-Bhagia** I can't stop that.
*  **Matt Stauffer** I mean, do you know much more about robots than I do? Do you know,
*  do you have a picture of what it's going to be like 50 years from now in terms of how roboticized
*  our daily lives are going to be? **Samantha Nare-Bhagia** 50 years from now? Oh, that's a tough one.
*  **Matt Stauffer** You can change the number, but yeah. **Samantha Nare-Bhagia** Like if you go 50
*  years back, like, and think of all the changes. Oh my goodness. Well, I do think that we are going
*  to see more robots in households and workplaces than we do right now. Like right now we're at the
*  very beginning of people like having a Roomba, having an Alexa. You know, recently some companies
*  have closed their doors that were trying to create household robots that do a little bit more than
*  those two things. They tried and failed, but I think that they're just slightly before their time.
*  I think we will have more and more robots in the household. They may not be Rosie from the Jetsons.
*  They may be more single task robots, but that's definitely going to be a huge shift. I think we
*  are entering an era of human robot interaction. **Matt Stauffer** Right. And it's interesting to me that
*  we treat them, we keep saying anthropomorphic, but that's not exactly right, right? Because
*  we don't necessarily treat them like people, but like animals, right? Whether or not they are
*  explicitly made to look animal-like. So once we get, not like you say, Rosie from the Jetsons,
*  but just super Roombas, it's going to be inevitable that we make them more and more human in our minds.
*  **Sameera Kishore-Murthy** I think so. Well, I'm so glad you mentioned animals though, because
*  I think one of the fallacies that we still have is that we constantly compare robots to humans and
*  artificial intelligence to human intelligence. And whether that's in stock photo images, if you do a
*  Google search for AI, you get all these human brains, or whether that's talking about robots
*  and job replacement. I think we're still very much thinking of robots as like recreating human
*  abilities and human tasks, whereas it's so much better to think of them as an animal equivalent.
*  I'm doing research on a book on this right now that looks at all the analogies between our history
*  of animal domestication and how we're integrating robotic technology now and in the future,
*  because it's a much better analogy given that AI has such a different skill set than we do,
*  and we could be partnering with it instead of trying to replace ourselves.
*  **Matt Stauffer** Right, right. And that reminds me of, so I'll say something controversial here,
*  and I'll get comments, I'm sure, but there's the issue of how we treat animals, just as there's
*  the issue of how we would treat robots, right? And I have a lot of vegetarian followers, I'm
*  not a vegetarian myself, but I get that this is an important issue and we should think about it,
*  and I try to be open-minded and listen to both sides. But one argument is, if you had, if you
*  raised a pig, let's say, from a little piglet and it became your friend and your pet, then you
*  wouldn't want to kill it and eat it. And I actually, I agree with that, and I don't think that it's a
*  contradiction, or I don't think it's a moral failing. I think that it's just the fact that we
*  grow attached to things, animals, robots, people that we get to know. And the problem with killing
*  it is not that there's some intrinsically bad thing about killing it, but it's that our feelings
*  are hurt when that happens. And I think that's a perfectly sensible moral way to live.
*  **Sara Hossain** Wow, yeah, that is controversial, Sean.
*  **Sara Hossain** I agree and disagree with you. So I think it's true that if you look at the history
*  of animal rights and how we've treated animals throughout history, and today, we're such hypocrites,
*  and we love to tell ourselves that we care that animals have consciousness or that they suffer,
*  but if you look at throughout history which animals we've protected and which animals we
*  don't really care about, we don't really decide according to biological criteria. I grew up in
*  Switzerland. People eat horse meat in Switzerland. In the States, people would never eat horses. We
*  have too much of an emotional connection to horses. But if you tell that to a European,
*  the European is like, well, what's the difference between cows and horses? They're both the
*  same. They're both delicious. Why not eat them both? So culture...
*  **Sean Erman** Right. So I'm actually, I'm trying to argue for both sides. I want to say the
*  Europeans are completely right. What is the difference between horses and cows? But I want
*  to say, look, if as an American, if the culture you grew up in says that you are turned off by
*  the concept of eating horses, then don't eat them. I completely agree that we are hypocritical
*  in practice. I'm not in favor of pain and suffering, and I think that we're terrible
*  to animals, especially farm animals and so forth. And I'm all in favor of attempts to be nicer to
*  the animals that we do raise for livestock. But that's a different moral dilemma than whether
*  or not it's okay to eat them if they are raised humanely. **Megan Larkin**
*  I mean, so you think that just because we are hypocrites, it's okay to eat them?
*  **Sean Erman** No, I think it's okay to eat them. I think that the thing that is not okay
*  is mistreatment, right? **Megan Larkin** But you're okay killing them.
*  **Sean Erman** Yeah. Yeah. **Megan Larkin** So do you make sure that the meat you eat is
*  humanely raised? **Sean Erman** Well, you know, I prefer it, but I'm also a realist about what
*  actions I can take will change the world. I would like laws that make it not okay to mistreat
*  animals. That's the same thing with energy conservation, right? Or water conservation.
*  I'm in favor of social collective action to fix these problems, not me trying to be individually
*  virtuous. **Megan Larkin** That's a good answer. It's a good answer. I have to say, though, I have
*  too much of Paul Bloom's emotional empathy when it comes to animals. I've been a vegetarian since
*  January. **Sean Erman** Oh, okay. Good for you. **Megan Larkin** Yeah. I can't do it. You know,
*  I was breastfeeding and realizing that animals, you know, nurse, there's many animals that we also
*  nurse their kids. And I was just like, I can't eat them anymore. **Sean Erman** Yeah, look,
*  it's a very emotional issue. I think it's okay. And it's in flux. I did see someone I was following
*  on Twitter, I won't say who, tweeted something about how it's better for the planet if you eat
*  more plants and less meat. Okay. And there were so many responses in the comments along the lines of
*  don't tell me how to live my life. I'm going to have a big old steak. And, you know, just like
*  comments that were pictures of steak. And they were like, like, I eat steak. This is not a thing.
*  But people who eat meat are just as, you know, sensitive and defensive snowflakes as vegans or
*  vegetarians are. So I don't use any moral high ground anywhere. **Megan Larkin** I think they're
*  defensive because they know there's no justification for eating meat. The way that I know there's no
*  justification for my carbon footprint and flying all over the place, like I do. **Sean Erman** But
*  you do. Right. Yeah. So again, like I do it too. And I know it's bad. I know that me stopping
*  doesn't change anything really. So again, I'm in favor of, you know, collective action to fix this
*  problem. But anyway, okay. I think that, you know, this is, it's important because, you know, it
*  relates back to the robots because you, you know, you said, and I think it's correct, we're nowhere
*  near making artificially intelligent conscious robots that we would have to afford the same kinds
*  of protections to that we would people. But in the living kingdom, right, in animals or plants,
*  there's a continuum of consciousness and feelings. And we have to draw that line somewhere. And
*  eventually that is going to become a much harder question in the world of robots, I would think.
*  **Megan Larkin** Sean, I have a question. Why do physicists always want to talk about
*  robot consciousness? Is that a physics thing? Like what, it's always the physicists.
*  **Sean Erman** I don't think so. I mean, I actually don't want to talk about it, but I mean,
*  it's a, it's one extreme, right? Like one extreme is the Roomba, another extremes,
*  artificial intelligence. And I just, I'm just making the point that there's a continuum between
*  them and we are changing. So it's actually, I think the philosophers like to talk about it more than
*  physicists because they will instantly leap to the thought experiment, whether or not it's anywhere
*  close to technologically feasible. **Megan Larkin** Ah, yes, it's true. Philosophers also like this.
*  It's true. Well, you know, as a philosophical discussion, I think it's absolutely warranted and,
*  you know, relevant. I just, like I said, I don't think it's very practical to discuss
*  if we're looking at the, you know, near and medium term future or, or as far as my eye can see,
*  because we're not going to like, we're not going to have machines that approach the consciousness
*  of, you know, anything that we believe would warrant rights. **Matt Stauffer** Yeah, of a mouse.
*  We're not going to come anywhere close to a mouse, right? **Megan Larkin** Yeah. And we don't give the mice rights either.
*  **Matt Stauffer** Yeah, no, exactly. That's right. So perfectly fair. And so I do,
*  and I do think it's important because of what the implications of your work and what you're studying
*  is much more about how humans deal with robots that we all agree are not conscious. So there's
*  plenty to be learned there. You've given the examples of autistic children, elderly dementia
*  patients, but then we have to start talking about the sex robots, right? You know, people using robots
*  for perfectly healthy grown up people using robots for other kinds of purposes, let us say. So
*  do you study that? Is that something you learn about? **Megan Larkin** I don't study sex robots. I personally
*  would have no problem with it, but it's very hard to do sex research in America. I don't know if
*  you've tried. **Matt Stauffer** I don't have to do either sex or drugs research, but I've been told by
*  friends who do. It's very, very difficult in both cases. We're very pure technical. **Megan Larkin** Yeah,
*  you can't raise money for it. You get ostracized by your colleagues. You know, I also, the sex
*  robots, you know, I think they also get a disproportionate amount of attention. I mean, like
*  you said, we have to talk about the sex robots. Everyone wants to talk about the sex robots. I
*  think there's some interesting issues when it comes to sex robots, and it does tie into some of my
*  questions around does the use of technology influence human behavior? But there's also a lot
*  of kind of sensationalist headlines around the sex robots. We don't really have any yet. **Matt Stauffer**
*  Well, that's right. We have sex dolls, I guess. Right. I mean, the reason why I thought of it was
*  because this issue that we don't need to sort of make the robots look human in order to
*  anthropomorphize them. I presume that for sex robots, it would be, since they don't really exist
*  yet, but it would be beneficial to make them look human, right? Or at least, you know, some exaggerated
*  version, whatever someone wants. **Megan Larkin** Oh, yeah. That's the sex robot that you want?
*  Because if you go into a sex shop and you look, the sex toys made for women, a lot of them don't
*  really look like, you know, there's dildos that look like dolphins. They don't look like penises.
*  **Matt Stauffer** Women don't want it to look like men. **Megan Larkin**
*  I don't know. I'm just saying there's a lot of different form factors out there. But I think,
*  jokes aside, I mean, you're right that the sex dolls or the sex, quote unquote, robots that we
*  are seeing on the market do resemble female bodies and, you know, specific types of female bodies,
*  which also raises some issues around objectification of women and whether, you know,
*  you know, I don't believe that sex robots are any sort of replacement for human sexual relationships,
*  but they might be something that, you know, people who don't have human sexual relationships
*  use because they're better than nothing, or they might be something that supplement people's sexual
*  relationships. So I'm not necessarily concerned about humans being replaced here, but I am a
*  little bit concerned about kind of the form factors. And we just don't know whether people's
*  behavior towards sex robots is something that is, you know, a healthy outlet, for example,
*  for sexual behavior that we don't want performed among humans, for example, pedophilia, or whether
*  it's something that might perpetuate and normalize that behavior and make them want more of it. We
*  don't know. And like I said, it's pretty impossible to research. So.
*  **Matt Stauffer** Well, yeah, I guess that's what I was going to say. So,
*  you're right. We don't know. You could easily imagine it going either way, right? That
*  having this kind of robot either is an outlet and therefore keeps real people safe from harm, or it
*  encourages it. And this is an empirical question. You should study it. But you're saying that it's
*  just hard as a practical matter to actually study things like that. **Samantha N. Boucher**
*  It is. I mean, in other countries like Germany and Canada, they are doing some research on
*  pedophilia, which you like absolutely cannot even do in the United States for a lot of reasons,
*  including legal reasons. But as an empirical question, it's also a very difficult one to get
*  at. It's like the violence and video game debate. It's like the pornography debate. We have some
*  research that tries to get at those questions, but it's not very conclusive. It's very difficult
*  to do these studies. But I do think that robots warrant reconsideration of these issues because
*  of their physicality. I think that it's so much more immersive and visceral to engage with a robot
*  than with something on a screen. **Matt Stauffer**
*  And violence is another example, which is probably analogous to sex in some sense. We could imagine
*  that certain people have violent tendencies, and maybe that could be ameliorated if they could take
*  it out on robots, right? Or it would just encourage them. And it's very analogous to the video game
*  debate also. But is that something where we're able to do research a little bit easier than the
*  sex question? **Sara Hickman**
*  I mean, the research is, well, anything is easier to do research on than sex in America.
*  But it is a very difficult question. So some of the research that's been done on human-robot
*  interaction is starting to look at connections between people's tendencies for empathy and how
*  they treat robots, like my research, but also other people are looking at this. But the question of
*  does interacting with robots change people's empathies is a more difficult one that we would
*  like to get at. And I'd like to think of ways to get at it. But it's going to take a while. And
*  that's unfortunate because we kind of need to know sooner rather than later as robotic design gets
*  more and more lifelike. We want to know whether we need to be policing people's behavior or having
*  age restrictions on certain robots or even having legal protections for certain robots that restrict
*  what people can and can't do with them in order to prevent people from becoming desensitized to
*  certain behaviors. **Jarad Larkin**
*  Yeah. So do you collaborate with psychologists, sociologists? Are they studying these questions
*  of how people's behavior can be changed by dealing with robots in different ways?
*  **Sara Hickman** Some people have reached out to me.
*  I've stopped doing experimental research. I haven't done any for a few years because I'm
*  writing a book right now. But when I get back into it, I'm planning on getting in touch with
*  people because yeah, of course, this is very interdisciplinary work. You need people who
*  understand the technology, people who understand the psychology. And that's another difficulty in
*  academia even though nowadays we claim that interdisciplinarity is important. The institutional
*  structures don't always support it that well. And I'm fortunate to be in a... The Media Lab is pretty
*  good at it. So I'm hoping that I can do some interdisciplinary work on this and collaborate
*  with a few other people later on. **Jarad Larkin**
*  Yeah. You mentioned it's happening, right? Like it or not. And as slow as academia is,
*  the legal system is a whole other thing, right? I did have a podcast with Ulta Charo, who is a
*  bioethicist and law professor. And biology is an area where things are changing very rapidly with
*  gene editing and designer babies and so forth. And the law is always slow to catch up. Do you think
*  how good a shape is the law in when it comes to our future robot interactions?
*  **Sara Hussain** Oh, terrible. It's terrible. And sometimes it's good that the law is so far behind.
*  We would not have the internet as we know it if legislators had gotten their hands on it early.
*  At the same time, now we're dealing with some consumer protection issues and privacy issues
*  and other issues because the law is still behind even though we've had the internet for a while.
*  So it's a constant struggle, especially as the pace of technology really picks up. And biology
*  is a great example. I mean, wow. Yeah. We need people who understand policy and law and understand
*  technology and biology and the cutting edge of innovation to be working together on this.
*  That doesn't happen nearly enough. I'm part of this community of people who are trying to
*  bring policymakers and roboticists and people who work in those fields together with some limited,
*  but some success. We have this conference that's happening every year. There's about to be a
*  policy workshop in DC with a bunch of policymakers. But yeah, it's a constant struggle. The law is
*  usually based on superstition or lobbying and not based on evidence. It's also reactive, right?
*  Some terrible thing happens and then we pass a law against it if it's a new kind of terrible thing.
*  Oh yeah. And we need more evidence-based policy. But for that, we also need to have the data,
*  to have the evidence that we can point to. And in human-robot interaction,
*  there are still some open questions that haven't been explored.
*  I mean, so both what are those open questions and what are the biggest legal issues in your mind?
*  Like what do you wish legislatures or at least congressional staffers were worrying about?
*  Well, okay. So do you mean like in my personal focus and work or do you mean generally in robotics?
*  Whichever you want to do first.
*  Well, so my personal focus is really on the ways that people treat robots like they're alive,
*  even though they know that they're just machines. And so the thing that I really want policymakers
*  to be aware of is, or in particular, consumer protection agencies is the fact that if you take
*  the persuasive design that we've developed on the internet, you know, getting people to click on
*  buttons because there's specific color, using all these tricks from like casinos to get people to
*  interact with their devices more. If you take that and apply it to a social robot, it's just
*  putting it on steroids. A social robot is going to be so persuasive and so engaging to people.
*  And that plus capitalism is a recipe for some consumer protection issues that I think we will
*  need to figure out sooner rather than later. So that's what I think. But beyond that, there
*  are many, many issues in robotics that really warrant legal consideration from autonomous
*  weapons systems to responsibility for harm to automation and the workforce. I mean, there are
*  so many, oh my gosh, algorithmic bias is a huge one that fortunately has gotten a lot of attention
*  recently. And there are a lot of great people working on that. But yeah, there's a lot going on.
*  Actually, why don't you, maybe not every listener is very familiar with that. What is the issue with
*  algorithmic bias? So we like to think of artificial intelligence as being more neutral in its
*  decision making than a person. And so if you're a company and you want to remove bias from your
*  hiring process, for example, you might think it's a good idea to take an algorithm to train it on
*  what people have been successful in your company in the past and have it pick applicants instead
*  of having a human pick the applicants who might be swayed by certain names or who knows what.
*  So companies have actually done this. Amazon did this recently and got in trouble for it because
*  it turns out if you train an algorithm on historical data, which is what we do with all the
*  algorithms, it will incorporate a lot of biases. Like for example, the fact that you didn't used
*  to hire a lot of women in your company. And so now the algorithm is going to be like, oh, well,
*  women haven't been successful in this company. So we're going to weed all those out. So that's a
*  very simple example, but we are seeing these systems being deployed in areas like criminal
*  justice, making decisions over whether people get out on bail or not, making decisions over
*  whether people get hired or fired. Entire rating systems for people's professions have been
*  automated in this way. And it's heartbreaking because not only is the data usually biased,
*  but we usually don't have any insight into how these decisions were made. A lot of these systems
*  are developed by companies. It gets contracted out. It's in a black box. It's protected by
*  intellectual property. Or even the algorithm is so complex that humans can't understand how it came
*  up with the decision. We just trust that it was a good one. But there's a lot of problems with this,
*  obviously. And data scientists, fortunately, are turning some of their attention towards this.
*  So basically, it's a garbage in, garbage out problem. If you train a computer on
*  your biases, it will end up reflecting your biases.
*  Oh, yeah, of course. That's the problem with technology in general.
*  Oh my gosh, we have someone here at the lab, Joy Boulamwini. She does fantastic work. She's
*  a woman of color, and she was programming something that required facial recognition,
*  and she realized that the system couldn't recognize her face because her skin is black.
*  She had to wear a white mask so that the system would see her. And we've seen this over and over
*  again, like photography, automatic faucets in the bathroom, facial recognition. Technology is built
*  by largely young, affluent white men who don't think to incorporate different types of skin
*  color as just one example. And it's not their fault. Everyone has blind spots based on their
*  life experience, but we need more diversity in the teams that are building technology.
*  We need to be aware of the biases and the data that we're training these systems with.
*  And currently, I think we trust technology a little bit too much to be neutral.
*  Yeah, and that goes back to something you mentioned in passing that I want to come back to about
*  responsibility and where we put it. We have increasing...
*  Things are being done in our world increasingly by automated systems, by robots, by computer
*  programs. And sometimes things will go wrong. Sometimes it's inevitable. Sometimes it could
*  have been avoided. But we have a system when human beings do something and it goes wrong
*  for blaming them and holding them accountable and holding them responsible. And what do we do
*  when the thing that went wrong is an automated system rather than a biological one?
*  Well, it seems like an unsolvable problem. We do have some things that we can draw on
*  throughout history to look at this. Now, of course, this isn't going to help in cases where we have a
*  black box problem, which is why the EU, for example, is looking at trying to have transparency in
*  algorithmic decision making so that you can at least see what happened in the decision making
*  process. But when it comes to just these autonomous entities making a decision and causing harm,
*  we can look at animals. We do have a history of assigning responsibility when animals have caused
*  harm in the past. I think of this, in particular, when we think about robots and automated weapons
*  systems and things where robots are causing physical harm, we've dealt with this issue
*  throughout history ever since the first laws known to humankind. What happens if my ox wanders off
*  and gores someone in the street? Who's responsible for that? And we've had different solutions to
*  that. It's not like we found the one golden solution. It depends a little bit on culture
*  and other things. But we can look at that history and try and draw from it a little bit.
*  I was just recently talking to someone at a conference who is involved in some,
*  he's a roboticist, works on autonomous vehicles. And he was talking to some lawmakers in England
*  about how to regulate autonomous vehicles. And they were like, oh, oh, well, we have this law
*  of the horse from the 17th century that perfectly applies here. And so they started drawing on that.
*  So it's not as new a problem as people think. Right. Okay. Yeah, actually, that does make sense.
*  And again, it fits in with the, we can learn something about how to deal with robots by
*  studying how we deal with animals more broadly. Right. I mean, I personally love that analogy.
*  It doesn't hold for everything, of course, but it's pretty cool. Well, I wanted to, I mean,
*  before I forget, you mentioned the usefulness of these robots to autistic children or elderly
*  people with dementia. I want to think about the usefulness of robots in a similar way to
*  specifically intentionally anthropomorphize robots to, you know, grown up regular people,
*  or, you know, the majority of people who we think about. Will we be having increasing robot nannies
*  or robot school teachers anytime soon, robot nurses? Well, whether we will or not
*  is a different question from whether or not we should. Yes. I personally think it's so boring
*  to just try and, you know, create robots that do something that a human can do. And that's just,
*  like, I understand that sometimes there's a need for that. Like, for example, for some,
*  for a dangerous job that, you know, we don't want people putting themselves in danger for. Yeah,
*  we might want a robot to automate that. But the robot nannies or, like, people taking on a
*  function that, like, relies a lot on human social factors, I think that's, I think that would be
*  better done by humans, honestly. It's not where robots have the comparative advantage, right?
*  No, absolutely not. And, you know, people argue, oh, well, you know, there are certain countries
*  where we have overaging populations and, you know, we need robots to take, because we don't
*  have enough people to do care-taking jobs, for example. But, you know, sometimes the problem
*  there is very strict immigration policies and general, you know, issues in society that
*  maybe aren't best solved by throwing robots at the problem. Like, maybe we need to have a broader
*  conversation about what it means, for example, for blue collar workers who've lost their job to not
*  want to become a nurse because it's too feminine in their minds. You know, we have a lot of jobs
*  and care-taking in the U.S. that, you know, where people are needed. And at the same time,
*  we have people complaining that all the jobs are getting lost. And it's, you know, there's a lot
*  of culture and identity and other conversations tied up in that that we might want to be having
*  rather than turning to technological solutionism. I guess, yeah, okay. I mean, it's a good point,
*  because I think that one of the first things that people, I think we've touched on this already,
*  first things people think of when they imagine the robotic future is robots that are exactly like
*  humans, right, that are conscious and look like human beings. And even if we put that aside and
*  say, well, the robots don't need to be like humans, they can be different and have different things,
*  you're saying that we still, you know, look at the functions filled by human beings and say,
*  what if we filled them by robots? And we might want to say, well, what if they're just completely
*  different functions that robots are much better at? And we should be looking at those.
*  Yeah, I think it's so much there's so much more potential and thinking outside of the box here,
*  and thinking about, okay, what are robots good at? And how can we use that to make us more productive
*  or have us do our jobs better? I think that's there's way more potential and thinking like that
*  than in thinking of how do we recreate human ability? Yeah, right. In Pasadena, right near
*  Caltech where I work, there is, I don't know if it's the world's first, but there is a burger joint
*  that has a robotic burger flipper, right? It puts the spatula over the burger, flips it,
*  you can watch it, right? They sell it as a as a tourist attraction. And I was never quite sure
*  why that was an improvement over anything. But I mean, there's some novelty effect there, I guess.
*  There's some novelty effect. But so okay, to close, then I'll just give you an opportunity to
*  prognosticate a little bit like what or maybe if not, you're directly predicting the future.
*  Give us a little bit of a hint for those of us who are not following the latest robot
*  news, what we should keep in mind or keep an eye out for in the robotic future, not just building
*  robots, but you know, the legal and moral and personal issues that you care about.
*  Well, I think one of the things that people really have trouble wrapping their minds around, but
*  as people have more robots in their lives are getting more comfortable with is this idea that
*  we do treat robots sort of like living things, even though we know that they're just machines.
*  And so having that awareness, I think it helps you not be surprised by it and helps lessen the
*  confusion that we're seeing in our society, at least. But another thing that I really want to
*  point out is that the way that artificial intelligence currently is being built and
*  learns is by collecting massive amounts of data and feeding it to the machines. And you know,
*  we now have the processing power to be able to do something with that data. And like, there's some
*  really cool things, but a lot of data collection, I think is could be problematic for people's
*  privacy interests. And that's something that people don't think about a lot. And even I see
*  in myself, like, we have an Alexa at home, it's so practical to be able to order diapers hands free,
*  you know, when you have a toddler. And so we're, you know, we don't mind having a microphone in
*  our home that's listening to everything, because we've traded our privacy for that functionality.
*  And so we don't have an incentive to curb that the companies don't have an incentive to curb that.
*  So we need legislation to have our backs. Because, you know, I think privacy is really
*  important, you know, maybe not for me, but for the poor families who get targeted with
*  advertisements trying to sell them, you know, scammy loans or education programs. There are
*  so many examples of how people get exploited by all this data collection. And I think we need to
*  be aware of that. Yeah, I kind of have a fatalistic attitude towards this. I suspect that
*  privacy is just going to go away. You know, I don't think it should. I think we should try to
*  protect it. But people don't seem that interested in trying to protect it versus the pressure to
*  give it up and the possible benefits they can get from giving it up. But like, we have to fight,
*  Sean, like we do, we do, it doesn't impact you maybe as like a very privileged white man.
*  But it impacts so many people who are, you know, less fortunate. And they're just, I just, we have
*  to fight this. And I don't believe that privacy is dead. I believe that privacy is an entire spectrum.
*  And that if we just let this unbridled capitalism roll over us, they're going to find newer and
*  newer ways to just exploit people. And so we need to at least try to slow it down a little.
*  All right. I will take that as a good optimistic message that we should, you know, even if it's a
*  pessimistic message in the sense that there's this huge problem facing us, it's an optimistic one that
*  we can fight and do something about it. I think so. All right. Kate Darling,
*  thanks so much for being on the podcast. Thanks, Sean.
