---
Date Generated: June 08, 2024
Transcription Model: whisper medium 20231117
Length: 5295s
Video Keywords: []
Video Views: 8092
Video Rating: None
Video Description: Patreon: https://www.patreon.com/seanmcarroll
Blog post with audio player, show notes, and transcript: https://www.preposterousuniverse.com/podcast/2021/07/19/156-catherine-dignazio-on-data-objectivity-and-bias/

How can data be biased? Isn’t it supposed to be an objective reflection of the real world? We all know that these are somewhat naive rhetorical questions, since data can easily inherit bias from the people who collect and analyze it, just as an algorithm can make biased suggestions if it’s trained on biased datasets. A better question is, how do biases creep in, and what can we do about them? Catherine D’Ignazio is an MIT professor who has studied how biases creep into our data and algorithms, and even into the expression of values that purport to protect objective analysis. We discuss examples of these processes and how to use data to make things better.

Catherine D’Ignazio received a Master of Fine Arts from Maine College of Art and a Master of Science in Media Arts and Sciences from the MIT Media Lab. She is currently an assistant professor of Urban Science and Planning and Director of the Data+Feminism Lab at MIT. She is the co-author, with Lauren F. Klein, of the book Data Feminism.

Mindscape Podcast playlist: https://www.youtube.com/playlist?list=PLrxfgDEc2NxY_fRExpDXr87tzRbPCaA5x

#podcast #ideas #science #philosophy #culture
---

# Mindscape 156 | Catherine D’Ignazio on Data, Objectivity, and Bias
**Mindscape Podcast:** [July 19, 2021](https://www.youtube.com/watch?v=55sqv5etvgM)
*  Hello everyone, welcome to the Mindscape Podcast. I'm your host Sean Carroll.
*  Everyone knows, I think, that even though words like data and algorithm carry a certain patina of objectivity with them,
*  in the real world it's often the case that neither the collection of data, nor the analysis of data, nor the use of algorithms are completely objective.
*  They have biases built into them because all of these facts about the world or ideas about how data should be analyzed are created by human beings.
*  And human beings have their foibles, right? And we see this in action in ways both sort of profound and trivial.
*  There are algorithms that decide who people should hire, who should be suspected of committing crimes.
*  Something we'll talk about in this podcast is crash test dummies. When car crashes are done by car companies to test them for safety,
*  it used to be that all of the crash test dummies were modeled after men.
*  None of them were in the shape or sizes of women, and as a result, you could actually figure out ex post facto,
*  the designs of seat belts and things like that for cars were noticeably less effective for women than for men.
*  So as objective as we might try to be, we're going to fall a little bit short.
*  Think of it this way. This is one of the ways I like to think about it. You're standing somewhere right now.
*  You're in a room where you're outside. You're in your car. Look around and imagine trying to describe your immediate environment to somebody else in a completely objective way.
*  You can imagine doing that. Maybe you think you can do that. But the fact is you can't. You can say objectively true things, right?
*  There are true things to say about the world. You're in a car. It's a Toyota, whatever it is. But you're making choices along the way.
*  There are an infinite number of things you could say that are objectively true.
*  But it's you who are always going to be a little bit fallible and have your biases, have your history and your interests and so forth that choose for you what features of the environment matter.
*  How to divide up the environment into the interesting facts, the uninteresting facts, etc.
*  That right there is a way that non-objectivity creeps into how we characterize the world around us.
*  So Catherine D'Ignazio, today's guest, is a graduate of the MIT Media Lab and is currently an assistant professor in MIT's Urban Science and Planning Department.
*  And she's written a book with Lauren Klein called Data Feminism. And it's not just feminism.
*  It's really the intersection of data and algorithms and how we are biased and how we can fight it.
*  So Catherine is someone who is pro-data, pro-algorithms. Her message is not that, you know, science and technology are tools of the oppressor or anything like that.
*  They can be used to make the world a better place.
*  But they're not always used in that way and not necessarily even for pernicious reasons, right?
*  You know, our biases is sort of a negative connotation word, but our individuality about who we are and therefore how we see and conceptualize the world creeps into how we talk about it and what we do about it, whether we like it or not.
*  So being a little bit more conscious, being a little bit more cognitive, being a little bit more aware of what's going on can help us understand the world better.
*  That's something we all want to do. So let's go.
*  Catherine Degnasio, welcome to the Mindscape Podcast.
*  Thank you. Thank you for having me.
*  So Data Feminism, the title of your book, and I'm sure we'll get into both of those.
*  And I'm also sure that most of the audience, their eyes focus on the word feminism, right?
*  That's the thing that is going to get people vibrating with either positive or negative valence.
*  But I'd like to start with the word data a little bit, because, you know, I'm a physicist.
*  We have something that we have in mind when you talk about data coming from experiments, finding the Higgs boson.
*  But in the modern world, the big data world where we're constantly being surveilled and tracked and things like that, data means something a little bit different, or at least the connotations are a little bit different.
*  So what do you, in your idea, should our audience have in mind when you just say data something?
*  What are the things that are flying around in this sphere of ideas?
*  Sure. Yeah, yeah. Yeah, no, great question.
*  So the data that we are referring to can include, you know, data from scientific experiments, of course.
*  And obviously, you know, most people, when you say data, their minds go to quantitative information.
*  And so, of course, data includes that.
*  You know, our definition is pretty expansive.
*  It's just it's information that's collected in a systematic way.
*  And so it's like it's a collection of similar things at some level.
*  So, you know, if you think about just anything that you can put in a spreadsheet, basically, and also includes things you can put in a spreadsheet.
*  So, I mean, in many of the, you know, the most interesting sort of big data things are image data, for example.
*  Right.
*  And so thinking about things that are not necessarily just rows and columns as we encounter them in databases, but also images, videos, audio, different kinds of things like that, which which ultimately can be sort of analyze both quantitatively and qualitatively and decomposed into various kinds of parts and then deployed to do different things and even create new things.
*  So like images that are generated from other images and things like that.
*  So and then, of course, in data feminism, when we're bringing a feminist lens, we also argue for thinking about qualitative data, including qualitative data as a very equal counterpart to quantitative data.
*  So not kind of creating a hierarchy out of quantitative and qualitative data and also really arguing for the value.
*  For really valuing lived experience as a form of empirical data.
*  And that's something that really comes from feminist theory and thinking and thinking about like, well, for people that have been excluded from, you know, the historical canon.
*  How do we share our stories and our data and our kind of evidence that we bring to the table?
*  It's often been through stories and personal experiences and lived experiences.
*  So really kind of thinking about those as as empirical data, obviously of a different nature.
*  Like, it's not the same thing to say, like, this is my story.
*  And like, that's that's your physics experiment.
*  Those two things are different.
*  But yet at the same time, sort of not like denigrating it because it's not some kind of like a generalizable thing that everyone in the world has also experienced.
*  And if I'm remembering correctly, you work at the MIT Media Lab. Is that right?
*  So I graduated from the MIT Media Lab, but I'm actually now a professor in the Department of Urban Studies and Planning at MIT.
*  And different departments.
*  Are you a data scientist? There are people who call themselves data scientists.
*  This almost seems redundant to me, but it's clearly a growing field.
*  Yeah, I would say I'm a data scientist.
*  I would also say, you know, my I'm really trained less as a data scientist and more as a software programmer.
*  So I come out of software and database programming.
*  And so more from the realm of like systems development and application development is where I spent a long, long time and sort of came into data science through doing all this database programming, but also always being interested in art and design.
*  And so for me, those two things always came together actually in maps.
*  And so I'm a long time sort of cartographer and map maker.
*  And in fact, that's the course that I teach for our this main course I teach in urban studies and planning is our GIS and spatial analysis course.
*  We actually I just had a podcast a couple of weeks ago with Jordan Ellenberg, who is a high powered mathematician, geometer.
*  And we talked about gerrymandering and the mathematics of figuring out whether a map is gerrymandered or not.
*  There's maps are surprisingly sciencey.
*  I think it's it's a very, very good topic.
*  I love maps for exactly this reason.
*  Like they bring together science.
*  They bring together art and design.
*  They bring together.
*  I'm a very visual person, so they bring together the visual side.
*  But yeah, so so that's why I love maps is because I feel like they're like the integration of these two sides of the brain that they don't often talk to each other.
*  They don't often encounter each other professionally, but then we find them together in maps.
*  And so that for me is very exciting because it can cause both like, I don't know, friction, but also brilliance.
*  But so when we say the word data as an adjective, I mean, there's sort of data as a noun which you helped define.
*  But I guess what I'm getting at is because you know it already.
*  I'm trying to get for the non-expert.
*  What are the steps involved in collecting, analyzing, presenting data?
*  Like, I think you got into the whole data feminism thing through the issues surrounding data visualization, if that's not wrong.
*  So like, what does a data scientist do to go from the raw stuff of reality to some presented data?
*  Sure.
*  Yeah. So, you know, there's there's this exercise I do with students where we start off and it's an exercise that's about thinking carefully about the ways that we take the world and sort of capture the world is what Joanna Drucker would say is like instead of taking data.
*  We actually are capturing data and super simple.
*  And so I go out and I tell them to just go walk around for like 15 minutes.
*  And your job is that you're going to classify people's shoes.
*  And so you have to develop some taxonomy, our classification scheme for shoes, and you have to collect at least 10 or 15 rows of data about those shoes.
*  And so the interesting thing is, I mean, it's ostensibly a very simple topic.
*  Like most people in the American universities have shoes like you can find lots of them.
*  They come back with their data, but everybody has a different categorization scheme.
*  They have different ways that they've classified shoes.
*  And so this starts to open up some of the complexities.
*  I think once we start to look at the ways in which we count things, the ways in which we aggregate things, the things that we find as being meaningful are not the same as the things that another person would find as being meaningful.
*  So this idea, you know, I often find myself in the position of teaching newcomers about data science and about data analysis methods and trying to help them.
*  Like I'm a really, you know, the whole thread of my work is about data literacy.
*  So trying to sort of expose people who don't consider themselves to be technical to start to understand how to use some of these methods.
*  And in fact, they are not that complicated.
*  There's as my colleague, Rahul Barkov says, they're fancy ways of counting.
*  And so but so this is what becomes interesting though, is that just the humanness of data.
*  Right. So even when we think that we're being so precise and so objective, there's still an infinite number of ways to classify shoes.
*  You know what I mean? And that's not to say we should never classify shoes.
*  Right. And it's never useful to classify shoes because it certainly is.
*  And, you know, particularly if you're a shoe company.
*  But but it's more just so that we can start to be aware of some of the limitations of what we can and can't do with data and understand that data in their essence are really they are a reduction of the world.
*  Like no row of data about a shoe is ever going to describe the rich, rich complexity of a shoe.
*  And that's OK. Like, like, we don't need data to do that.
*  But it's just important that we remember that it's a reduction in complexity and that we don't confuse the data for the thing itself, because that's sort of where we can get into all sorts of troubles is when we think like, oh, these data that we just went out and downloaded from the web,
*  it just represent these raw facts, because in fact, they're not like they've been shaped and formed by the institutions that have set out resources and ways of collecting them that develop some classification scheme that may have done their own analysis and aggregation on them and so on.
*  So that's I love this example of the shoes, because it's exactly what got me interested in your book in the first place.
*  The recognition that there's this tension between a discourse of describing the world with perfect objectivity and rigor so that none of our biases creep in with the reality that that's not a possible thing to do.
*  Right. And so we might strive for might aspire to it.
*  But even when we do something as innocent as collecting data on shoes.
*  Well, you know, who classifies what shoe as what is an immediate choice.
*  The choice to look at shoes rather than socks was a choice.
*  And so there's all these choices that are flavoring what we are choosing to talk about what of the infinite number of facts about the world we're choosing to collect and then display.
*  Exactly. That's exactly right.
*  That's exactly right.
*  And it's like it's sort of like with any classification scheme, like it could have been done differently.
*  And so like the important thing is to think about, well, like, how could it have been done differently?
*  And why was it done the way that it was and understanding some of those motivations, not necessarily because those motivations are going to always be nefarious or something like it often is not that like there's some evil institutional person behind it.
*  It's just that it was done in a certain way for a certain purpose, which means there were things that were left out.
*  Right. And there were these paths that weren't pursued.
*  There were these other data that were not collected.
*  And so it's sort of drawing our attention to in a way like the paths not taken.
*  And we talk a lot in data feminism about missing data, for example, as a way of reflecting on what are the structures and the powers that are shaping the data that we inherit and the data that we ourselves collect.
*  Well, and one of the points you make is that not only does this human choice about what to do come into the data we collect, but then how to analyze the data, what to do about the data, how to visualize the data.
*  Right. These are all involving human choices.
*  Yes, exactly. Yeah.
*  Every stage of the process, like it's like all of these, you know, the pipeline, as it were, all of these stages of that process, there's these sort of very intentional choices.
*  There's this kind of particular set of actors that are working with the data.
*  There's certain goals. There's certain audiences that they want to reach.
*  So, yeah, it's sort of like a lot of the book is sort of deconstructing, I think, some of the myths about data science, which often are things that like data scientists themselves, like they know this really well.
*  They're familiar.
*  It's more like in the popular perception of data science or of statistics or of algorithms that you have to like deconstruct that, that there are these like kind of perfect black box systems that are going to always perfectly predict certain things.
*  It's almost more the popular narratives that we are sort of trying to deconstruct because any data scientists, I feel like who's worth their salt, like they know their data intimately and they also know the limitations of their data intimately.
*  And if they are responsible data scientists, they're not going to be going out and like making these wild claims with data that have all these limitations.
*  There's actually a joke within physics that nobody believes a theory that comes from a theoretical physicist except the person who proposed it and nobody and everybody believes an experimental result except the person who did it because they're very familiar with all of the things that came in along the way.
*  That's fabulous.
*  Do you think it's generally true that people on the street are a little bit overly trusting of the data they see presented to them?
*  Yeah, yes, I think so. And in fact, you know, I countered this in my own classes. So prior to arriving at MIT, I taught at Emerson College in the journalism department.
*  And so I taught data analysis and data visualization to journalists. And they journalists are a group of folks like they would come into the class saying like, I'm not good at math, like I, you know, I can't do numbers, which I actually looked at their standardized test scores was completely alive.
*  They'd all did great on standardized tests. They're fine. But, you know, they have this image of themselves is like, I'm a word person and not a math person.
*  And one of the having that image, honestly, of themselves, I think inhibited their ability to be skeptical of numbers because they were overly, you know, they'd like download some data on the Internet and immediately just believe that the data were true.
*  You know, first of all, like not to kind of understand that like they need to do in the same way journalists are taught to do a verification process for what kind of quotes and facts that they put in their articles, the same process needs to be done on a data set that you're
*  inheriting from another actor institution that you're that you're using. So a lot of it was like kind of teaching them about that process. And just having not worked with numbers intimately, like there was a kind of over trust or like an over placing of confidence in the numbers that they inherited.
*  And a kind of slippage where they imagined that I could see it in their writing when they would write with numbers, they were often throwing in like, you know, there's like a decimal point, they would give all of the decimal eight places.
*  You really don't need to know it's like fifty four point three point seven five six percent, whatever. But, you know, like there's this like need they felt to assert themselves with numbers and precision and things like that. But it was precisely because they were insecure about it.
*  So, so I mean, I think these are ways that that yes, I think there is this kind of placement of faith, particularly when people feel a kind of level of insecurity or under exposure to data or to math or statistical ways of thinking that like they're like, oh, I'm it triggers this like, oh, I'm not that kind of person.
*  I need to place my trust in the people that are that kind of person. And so a lot of what I did was sort of breaking that down and saying, no, in fact, like you can use, you know, you can you can do this work and, you know, particularly like basic descriptive statistics or within everybody's reach.
*  And, you know, teaching a kind of a skepticism, which is a healthy skepticism, but but a but a deeply important one, I think right now, particularly in the climate of misinformation and sort of, I don't know, sort of bad information actors on the Internet.
*  That is also happening with data as well. So thinking about how do we have a healthy skepticism and a kind of a citizen interrogation of data sets and being able to do a kind of a power analysis of a particular issue to understand how data may have been impacted by structural bias and by issues of power and things like that.
*  So and that's that's knowledge they can draw from like that's worlds they've been exposed to. But those are pathways that they're not just going to come naturally. I think those are sort of muscles and skills that need to be taught.
*  There are streaming services that turn our brains off mindless entertainment. And then there's Wondrium, the streaming service that blows your mind. Wondrium has thousands of audio and visual learning experiences to feed your curiosity that goes so much farther than what you'd find searching the web.
*  All of your favorites from the great courses are there, plus collections from Kino Lorber, Magellan TV, Craftsy and more. For example, I love the life and works of Jane Austen. You can find out so many facts.
*  Like one episode is completely devoted to Lady Susan, which wasn't even discovered until several decades after Jane Austen's death. After watching this course, I'm thinking of Austen's novels in new ways.
*  So join me and experience your own mind blowing moments with Wondrium. And right now Mindscape listeners get a special offer, a free month of unlimited access to the entire library. Go now to Wondrium.com slash Mindscape to sign up today. That's W-O-N-D-R-I-U-M dot com slash Mindscape.
*  Wondrium.com slash Mindscape.
*  Well, and you have examples in the book of famous mistakes that people made, you know, in the media with data, for example, the one that struck me was 538 doing a story. I think it was on, you know, mass shootings in Nigeria. They plotted the number of them.
*  But secretly what they were plotting was just the number of stories about mass shootings in Nigeria, even if they were about the same shooting. But I guess everyone makes mistakes. That's fine. You know, we all misread things. But there's something about putting it in a chart and making it look all objective that makes us just more likely to say, well, that's just the facts. There's no mistakes there.
*  Exactly. Exactly. And so this is sort of one of the things we talk about in regards to data visualization, which is that our methods for data visualization are almost like dangerously seductive, right? Because, you know, you can do these really dazzling things. We use these very precise lines. We use these geometric shapes. And then, you know, it looks by all accounts to be quite true. And, you know, it's like, how could we ever question that? But in fact, in fact, it's not just about the data. It's about the data. It's about the data. It's about the data. It's about the data. It's about the data. It's about the data. It's about the data. It's about the data. It's about the data. It's about the data. It's about the data. It's about the data. It's about the data. It's about the data. It's about the data. It's about the data. It's about the data. It's about
*  Exactly. This, you know, there's this like kind of fundamental misstep in that project, where the journalist sort of confused the object of analysis, basically, is like, instead of actual kidnappings, we were actually talking about media reports of kidnappings, which are really quite different things and unfortunately, led to the retraction of the article and things like that. So, yeah.
*  So let's move our way into slightly less innocent mistakes about, or not mistakes, but ways
*  in which we think about and present the data.
*  You have a wonderful quote that you sort of secretly agree with yourselves, both of you,
*  which is that data is the new oil.
*  Explain what that means to the people who generally say it and also to you.
*  Sure.
*  Yeah.
*  So this is such an interesting metaphor.
*  And this metaphor was circulating a lot when we first started writing the book.
*  And I feel like I hear it now less, so I don't know.
*  I should do some Google trends thing on it or something.
*  But probably anyone who's following conversations about big data has probably heard this phrase.
*  It was first said, we actually traced its history.
*  So it was first said in the mid-2000s, around 2006, 2007.
*  And then it was really boosted.
*  I think of it as a meme.
*  And so it was boosted as this sort of meme by The Economist magazine in, I think, 2011,
*  2012 type time, when they did this whole issue on data and all of the kind of great profits
*  that can result from extracting data, using data from social media to infer various things,
*  to kind of fuel what you might call business intelligence these days in the corporate world.
*  The metaphor is really interesting because we can think about oil and we can think about
*  data.
*  And in fact, the verbs that we use about both of those things really line up.
*  So we extract, we mine, we clean, we refine, we process.
*  All of these are metaphors for oil.
*  They're not metaphors.
*  They're things we do with oil.
*  And they're metaphors that we use for working with data, which is very interesting because
*  oil is quite extractive.
*  Oil is an industry of extraction.
*  And so the interesting thing with the metaphor when folks like The Economist are using it
*  is they're using it in a very positive way, meaning profit.
*  We can extract this natural resource, sort of quote unquote natural resource in the case
*  of data and then kind of clean process, analyze, deploy, whatever.
*  And it will yield great riches and profits.
*  But then the question is always sort of like for whom.
*  We raise that a lot in the book is that we call them who questions.
*  And that's often what feminism is good for.
*  It's good for asking who questions.
*  So it's like even in the case of oil, you look at like, well, who benefits from oil?
*  If we're saying data is the new oil and that's a good thing, who has benefited from oil in
*  the past?
*  And what kind of sort of externalities have been sort of created in the process and who
*  has borne the brunt of those?
*  So yeah, we sort of unpack that metaphor and don't disagree in the sense that I think it's
*  an apt metaphor to describe what a lot of corporations are doing with data.
*  If you look around the companies that are making the most money with data right now,
*  are the companies that are, you know, they have the resources to collect, store, analyze,
*  deploy, et cetera, these data driven products and services.
*  But it's again, like I think that metaphor of extraction is something we really need to
*  think about in the process.
*  Well, I guess, I mean, to me, it's quite obvious that oil as a concept is a double edged
*  sword, right? As many good things about it has helped industrialize the world, increase
*  the standards of living.
*  It's pretty clear that's also bad things about it, climate change.
*  And I would argue that it also has led to massive inequality.
*  Some people get rich, others don't.
*  And the data is the new oil metaphor lines those up.
*  And it sounds exactly right.
*  I mean, is it exactly right?
*  I mean, do you think that the data has this property that it will have just as many bad
*  effects? I shouldn't even say just as many.
*  I don't know what the metric or the measure is, but very noticeable bad properties, bad
*  effects, just like it will have good effects.
*  Yeah, yeah, I think so.
*  I mean, I think in the sense of thinking about like, what are the negative externalities
*  created by this, the data economy, which some people I don't know if you've heard this,
*  the other metaphor people have been using is the fourth industrial revolution around
*  the sort of big data economy.
*  I don't know. I scoff a little bit at that, I have to say.
*  Because again, it's like, you know, for who?
*  Like, who is benefiting?
*  And, you know, one of the interesting things with data and the negative externalities
*  that these that are created by the kind of big data economy is that the externalities
*  are very similar in a sense, like the negative externalities are very similar to the ones
*  that oil creates. So like, if you think about like even just, you know, the oil is
*  one of Facebook's data centers that was located in New Mexico, cost something like
*  30 million dollars a month in electricity to operate.
*  Like, this is a tremendous sort of ecological, energy intensive sort of earth
*  exploiting thing to be basically just fueling us to talk to each other.
*  Yeah.
*  And so like, so thinking about like there are environmental sort of costs to the
*  cloud, as it were.
*  But then there's also all of these social and sort of inequality generating costs as
*  well. And we go into some of those in the book, too, wherein, you know, if the power
*  of data is really centralized in the hands of large actors, because that's where the
*  resources to be able to mobilize it, they're going to use it to their benefit and they're
*  not necessarily going to use it to the benefit of sort of human health and wellness, more
*  generally speaking. Right.
*  And so I think that's a big problem.
*  If like, if it's really the corporations and to some extent, the like elite governments
*  and universities that can mobilize data and the rest of us are just kind of left in the
*  dust.
*  And that's a concern centered on, I guess, data plutocrats, if that's a category of
*  people we can point to.
*  But you also make the point in the book that even among data scientists, people who we
*  would imagine are trying to be objective in finding true things about the world, there
*  are value systems that have sort of sneaky choices built into them.
*  So there are values that are valorized.
*  I guess I'm running out of vocabulary words here.
*  But things like ethics and fairness and accountability and all of these sound great,
*  but they can be used to actually silence voices that are being slightly contrary and so
*  forth.
*  Whereas other values like fairness or equality, sorry, fairness was the other category,
*  justice or equity are not talked about as much.
*  So maybe say something about how it's not just the plutocrats.
*  It's even the scientists who sort of maybe even innocently stumble their way into putting
*  these values in a hierarchy.
*  Totally.
*  Yeah, thanks for bringing that up.
*  One of the observations that we make, so first of all, I think one, it's very encouraging
*  that there's a lot of computer scientists right now and a lot of folks in the technical
*  community are talking about what commonly goes by as fairness, accountability and
*  transparency.
*  There's kind of a whole set of conferences that are organized on these topics.
*  What that work tends to cover is work that's looking at algorithmic discrimination,
*  discrimination and bias in large data sets and training data sets, running from text,
*  natural language processing to images and things like this.
*  And then thinking about how do we make these things more fair?
*  But then there's been some interesting pushback on that work and we're not the only ones
*  doing that saying, well, why are these particular values, this fairness, accountability and
*  transparency, why are these the set of things that we're organizing around?
*  And in particular because a lot of work, and this is not all the work in the space.
*  I don't want to be caricaturing it or something.
*  But a good amount of the work in this space, when you take a concept like fairness, often
*  what these things try to do is say, okay, well, let's say we have some discriminatory
*  system like credit lending or something like that.
*  And what we're going to do with our speculative algorithm is we're going to just tune these
*  knobs and levers so that race, any kind of racial bias or any kind of gender bias or
*  sort of like excluded from the system.
*  You kind of get the bias out of the system.
*  And so it's not to say that those approaches have no value.
*  But at the same point, one of the things we would say in response is that they're actually
*  not addressing, they don't come with a full kind of conception of the problem.
*  So they're not coming with a kind of like root cause analysis of like, how did we get
*  to this point?
*  And how do we get to the point that lending to Black folks in the United States is perceived
*  as higher risk?
*  This is through centuries of like really deliberate sort of design and discrimination
*  that's been built in from kind of like the very beginning post-Civil War.
*  So we've had all sorts of, in a way, big data tools like redlining, racial covenants,
*  both informal and formal legal tools that have worked to keep that kind of system in place.
*  And so just looking at it, let's say like mathematically within a kind of a closed system,
*  unfortunately fails to account for these hundreds of years of history, right?
*  And almost invariably, the work that starts with this fairness lens is like starting from
*  like time equals t equals zero or something.
*  And rather than saying, actually, we need to take like the past 200 years into account.
*  That needs to be part of the model.
*  That's part of the data.
*  And so that would be the approach that's about equity and justice.
*  And so that's in a way like the challenge is like, how do we do that?
*  And we can't do that in a race blind and gender blind way.
*  It just doesn't work like that because history is not race blind and gender blind, right?
*  We have to acknowledge that past and we have to account for it somehow in our systems.
*  So like, yeah.
*  I mean, maybe we can get some good examples on the table here to ground people's thoughts.
*  Because I think that there is an extremely naive point of view, which would just say, look,
*  if you have an algorithm or if you have a data set, by definition, it can't be biased.
*  It's just a computer.
*  I mean, I say that's a very naive point of view, but I know people who have it.
*  Of course, in the real world, algorithms are trained on data sets and who chooses which data
*  set and what history is the data set reflecting.
*  One that struck me very vividly recently was a fun thing going around Twitter.
*  I think it came out after your book.
*  But take a sentence like, she won the Nobel Prize and put it in Google Translate into a
*  language that does not have gendered pronouns and then translate it back to English.
*  And it's always he won the Nobel Prize when it comes back.
*  Right.
*  And so maybe explain more about how and examples of how these biases from history or from ignoring
*  history creep into purportedly objective algorithms.
*  Exactly.
*  Yeah.
*  And so see, this is great because I think this goes back to the other topic we're talking
*  about was that kind of faith, like an overplaced faith in the systems that they are objective
*  and that they are going to work like that.
*  So, yeah, so these things happen because again, it goes back to the human constructed nature
*  of the data sets.
*  Right.
*  Like if we're going to make, let's take facial recognition, which we talk about in the book.
*  Right.
*  So if we're going to make a system that recognizes a face and is able to distinguish a face as
*  opposed to the background in a photograph, we need training data because like the system
*  needs to be able to look at a bunch of images and have ones that have already been annotated
*  and say, oh, this one's a face and here's the exact location of the face.
*  And looking at many, many, many of those is going to teach the system where the face is
*  and what faces look like and so on.
*  But one of the things that, for example, I look at the work of Joy Buolamwini, who's
*  a colleague at MIT.
*  She's a Ghanaian American.
*  She has very dark skin and she sat down in front of her.
*  She was just got an off the shelf facial detection library.
*  She wanted to use it in a class project and it just wouldn't see her.
*  Like it wouldn't put that little box around her head.
*  You know, she's just like, that's weird.
*  She got her white friend, recognized the white friend.
*  She got an Asian friend to recognize the Asian friend.
*  And then she took a, she had this like white theater mask and she put on this white theater
*  mask and then it recognized her face.
*  Right.
*  So we have to think about like what's happening behind that system.
*  Is it that the engineers are racist?
*  No.
*  Like the engineers were not like sitting there being like, yeah, we're definitely going to
*  discriminate against black women.
*  But what happened is they had, you know, she's a very technical person.
*  So she dug into the guts of the system and she ended up writing a really important paper
*  called Gender Shades with Timnit Gebru where they audited the training data libraries that
*  are used to train these kinds of systems.
*  And in the resulting analysis that they looked at, it's sort of like the problem of like what
*  data are available is often like the biggest source of bias.
*  Because in the case of face data, these are celebrity and like political people profiles.
*  And so these are often the data that are used to then train the data sets.
*  But then we can think about, well, like who's celebrity, who's a political person,
*  what kinds of racial and gender biases are those?
*  It's going to be like mostly white people, mostly men.
*  And in fact, what they found is something like, and I'm forgetting the exact figure,
*  but something around like 88% of the faces in this benchmarking training data set were
*  what they called pale and male.
*  So they were actually, they didn't rate their race.
*  They were looking just at the skin color.
*  And so of course, a system that's trained with that kind of training data
*  is going to fail really badly.
*  Like it works great for white men because that's the kind of user group that's really
*  centered in that kind of system.
*  And then it works really terribly for black women because there are so few
*  images of black women in the training data.
*  And so it's sort of like thinking about, it's not really that the algorithm is biased,
*  it's that we are biased, right?
*  And it's even also that even in building the system, so it's like the engineers weren't
*  sitting there being like, ha ha ha, like let's be racist and sexist,
*  but also they didn't have mechanisms to seek out the inherent racism and sexism that will
*  show up inevitably.
*  They didn't have the tools to look for it.
*  They didn't have the checks and balances to be able to like check for it before it ends
*  up that the black woman discovers it on the tail end of things and then, I mean,
*  sort of exposes it from there.
*  And so that's, I mean, it's sort of like how we end up with these things is because as
*  we build human systems, we're pulling from the human and social world to train those
*  systems.
*  And that world is not a race, you know, it's not a, it's, you know, hopefully one day
*  we live in a world where like we are not, you know, racist and sexist, but right now
*  the world is racist and sexist.
*  That's the, you know, garbage in garbage out.
*  Right.
*  Exactly.
*  That's what we have to deal with.
*  So we have to look for the garbage, basically.
*  You can't travel faster than the speed of light, but what you might be able to do is
*  hire people at nearly the speed of light with indeed.com.
*  Indeed is the job site that makes hiring incredibly simple.
*  Everything you need is in one place, including interviewing.
*  Indeed's hiring tools help you cut through the noise to hire faster and smarter.
*  Indeed Instant Match will provide you a list of quality candidates whose resumes are on
*  Indeed the moment you post a sponsored job.
*  And then you can invite them to apply right away.
*  According to Indeed data, candidates you invite are three times more likely to apply
*  for your job than those who only see it on search alone.
*  Plus with Indeed Instant Match, 90% of employers get quality candidates from Indeed's
*  resume database as soon as they sponsor a job post, according to Indeed data.
*  So get started right now with a $75 sponsored job credit at indeed.com slash mindscape.
*  That will be an upgrade to your job post at indeed.com slash mindscape.
*  All for valid through September 30th.
*  Terms and conditions apply as indeed.com slash mindscape.
*  Well, I mean, this really highlights, I think, what to me is the big looming philosophy
*  question here, epistemology question, or whatever you want to call it, about objectivity.
*  I mean, I can very crudely distinguish between three attitudes that we might have towards
*  objectivity.
*  One is objectivity is good, and we have it basically, like the science and computers
*  and data are pretty objective, even if we humans are flawed.
*  Another attitude is, you know, objectivity is something we should aspire to, but we don't
*  have it, we should be extra careful in trying to get there.
*  And a third attitude is the goal of being objective is just misplaced.
*  In the first place, we shouldn't even try, we should recognize our, you know, individual
*  non-objective goals.
*  So where do you sit in that classification scheme?
*  Yeah, that's helpful.
*  So I think what I would say, you know, and this is where I think some really interesting
*  feminist theory comes in too.
*  So folks like Donna Haraway and Sandra Harding, who have thought through these questions
*  really deliberately and specifically, we can draw a lot from them.
*  So they have the ideas of something called feminist objectivity.
*  Donna Haraway specifically talks about situated knowledge.
*  And so, you know, I think where we can't end up, you know, and in fact, like, I don't
*  think any, well, I shouldn't say that.
*  I don't think any feminists would say, but I'm not, I guess I won't speak for all feminists,
*  because maybe some of them would say this.
*  Exactly.
*  But like, I think where we can't end up, I find it untenable, a position of like, oh,
*  to everyone their own individualistic truth and truth is super subjective and everything
*  is relative.
*  You know, I think that's untenable because, you know, we have to be able to arrive at
*  some collective shared understanding about the world.
*  And, you know, I think it's very dangerous when truth is just fiction.
*  We've seen that happen recently.
*  And so like, so I think that's a very dangerous world to me.
*  And yet at the same time, I think we have to recognize that the current conception of
*  objectivity as it exists has also been exclusive.
*  Like it hasn't been including all people, all genders, all races, all communities into
*  this sort of fold of objectivity.
*  And this is something feminists have critiqued for ages.
*  A more recent critique is Ruha Benjamin, who calls this imagined objectivity.
*  And it's not to say that, you know, we've done some just incredible scientific
*  achievements following these kind of tenets of scientific method.
*  And yet in particular, when it comes to how sort of science and objectivity meet the human
*  and social world, our disciplinariness is really constraining for us.
*  This is what's sort of leading to, I think, some of the confusion on the part of the
*  technical community for like, how do we do this better?
*  And often when you're trained in computer science, you're not given, like you haven't
*  really been trained in like gender studies.
*  You haven't been trained in history of race in the United States.
*  And so like we're finding these places where like our knowledge is haven't been able to
*  like these bodies of knowledge need to meet each other and they haven't met each other
*  and been incorporated.
*  So I guess like in your schema would be, I think, sort of somewhere in the middle.
*  You know, I think one of the things that Haraway would tell us is that all knowledge is
*  situated.
*  So we're all in a particular kind of context.
*  We're in a particular country, geography.
*  We have a particular value system.
*  And so the strategy that feminists would advocate for what you would call strong objectivity or
*  feminist objectivity, which is something we pull in more people from more places.
*  And we try to recognize more of our own blind spots, more of the ways in which our kind of rigid
*  conception of objectivity may be excluding people.
*  And so how do we bring people to the table to understand the kind of cultural and social
*  boundaries of our knowledge better?
*  So maybe that's a little bit long winded.
*  That's what we're here for.
*  It's like the feminist or my feminist kind of heritage would be like we don't like throw
*  objectivity out the window.
*  We definitely don't throw away like collective knowledge making.
*  But we do have to include more people.
*  And we do have to break down some of the exclusionary norms that end up pushing people
*  out of the quote unquote sort of objectivity.
*  I guess my temptation upon hearing things like that, which was actually very eloquently said,
*  and a lot of me wants to agree with it.
*  But then there's part of me that wants to say also make it even more complicated by saying,
*  you know, look, the charge of the electron is equal in magnitude and opposite in sign to the
*  charge of the proton.
*  And that's not situated anywhere in particular.
*  Like that's absolutely universal.
*  So I guess I want to sort of imagine a spectrum of situatedness of knowledge where there's some raw
*  physical facts that are pretty much universal and some social statements we could make that are
*  pretty obviously situated.
*  Is that a fair way of thinking?
*  Yeah, I think so.
*  I mean, I think like it's sort of like the closer that we get, you know, the closer we get to the
*  human and social world.
*  I think this is where we try to take sort of like physical science sort of laws, for example,
*  and then apply those to human circumstances.
*  I just feel like it's a recipe for disaster because like these kind of laws will not.
*  And people try to do that.
*  Like they try to find, well, like, like what's the underlying grammar of a city or what's the,
*  you know, underlying law that guides this particular kind of human behavior?
*  And the thing is that like humans are just more complex than electrons.
*  I mean, maybe one day we could get there.
*  You know what I mean?
*  In terms of like having some laws that are going to be like, oh, Katherine, you know,
*  right after this podcast is going to go pat her cat.
*  Like we just know that about her.
*  But I think this is what happens in the human world is that just things are messy and there's
*  so many variables that the scientific method way of kind of like, well, let's, you know,
*  take this one model and exclude all this other stuff and just pursue this one kind of model
*  of reality down this particular path.
*  Like I think that can work for certain kinds of things, but it really can't generalize and
*  apply as a method to all these different human and social circumstances.
*  So those are things that we really need to understand as being contingent.
*  And or like we may discover, you know, some laws, as it were, facts that might apply in
*  more contingent circumstances.
*  So this kind of applies generally speaking in Western cities that have this particular
*  mode of transportation.
*  You know what I mean?
*  And that's still really useful knowledge, but it's also really useful to know that it's
*  only in this kind of city.
*  It's not through all kinds of cities.
*  So I think that's where the kind of proponents of unqualified objectivity can be irritating
*  to feminists is, you know, where you're trying to make these somewhat absurd, like universalist
*  statements or generalizations and imagining that it's always about generalizing and
*  universalizing.
*  Right.
*  And so like kind of understanding when is universalization and generalization appropriate
*  and when is it more appropriate to just understand something that's deeply situated and
*  contingent in its own circumstances?
*  And that's where a lot of, you know, more qualitative social science research comes
*  in and things like that.
*  I guess what my response to that, and I apologize for talking too much myself, but I'm
*  working through this in real time because it's interesting.
*  It goes back to the shoes that we started with.
*  Right.
*  And I guess one could say even with the electron and the proton, you claim that's an objective
*  fact about the world, but someone chose to describe the electron as one particle and
*  the proton is one particle rather than, you know, different combinations or different
*  agglomerations.
*  You know, there was some carving of nature that was human that we turned that we showed
*  was useful after the fact.
*  It's not completely arbitrary.
*  There are good reasons to do it.
*  But it's still done by human beings.
*  The difference being that when it comes to fundamental physics, it's pretty easy to
*  do that, right?
*  It's pretty straightforward to agree on how to carve up nature that way.
*  And when it comes to human beings or even shoes, that's going to be much less easy,
*  much more prone to sneaking in our biases as some objective measure of something.
*  Yeah.
*  Yeah.
*  And I think variation, you know, and so like it's sort of like with the proton and the
*  electron, it's like it's going to be able to behave in a particular way.
*  And that's like super repeatable.
*  And then you can kind of demonstrate just even through repetition that this is pretty
*  much always how this particular thing works.
*  Whereas with shoes, like if I do the shoe experiment in my neighborhood versus in a
*  different place versus with a different group of students, like it's going to be different
*  every single time, you know?
*  And it's going to be different actually in a similar way, I guess I should say, because
*  that's why I use it as a learning exercise.
*  And so that but that's what gets interesting to talk about is that that sort of variation
*  in that.
*  But it's not but it's not repeatable.
*  You don't always come back with the same result.
*  And so like and that's for me what makes it sort of data in the social and political
*  world the most interesting thing, because they are contingent and yet they are still
*  useful.
*  Like we can still do meaningful things with them.
*  Like I said, like their reduction of the world, but hopefully they're a helpful reduction
*  that we can still use in some meaningful way.
*  Well, thank you for indulging my descent into the philosophical rabbit hole there.
*  But I do want to come back to the data and the feminism.
*  Let's get some more examples on the board, because we talked about how you make an algorithm
*  for facial recognition or for translation.
*  It can be biased.
*  You make the point in your book that even the choice of which data to collect smuggles
*  in all sorts of presuppositions, right?
*  And there's a whole bunch of data sets that either should exist or should do exist, but
*  don't include data that they should exist.
*  I mean, how should people collecting the data be thinking about these issues?
*  Yeah, absolutely.
*  And I think people collecting data have a special, it's almost like a special kind of
*  responsibility, right?
*  Because you really do have to be thinking about not only sort of how are we going to
*  collect the data and store the data, but how are we going to store the data?
*  How are we going to provide information like metadata about the data in terms of who's
*  going to come later?
*  One of the, I think, really complicated things, I find things like open data really
*  interesting and useful, and then also complicated because it's sort of like you have governments
*  that are collecting data and they're collecting it for a really specific institutional purpose.
*  And then they're publishing it often with really bad metadata.
*  You don't know where it's coming from or what the columns represent.
*  You have to do all sorts of legwork to figure that out.
*  But then imagine that we can then use that to do something else that the data were not
*  intended to do.
*  So in fact, this is why I have a lot of, I think, admiration, I would say, for people in
*  sort of library sciences and in fields that are about stewarding information, because
*  it's really about thinking about how do we become good caretakers of information and
*  not knowing at the time of collection all of the possible ways that the data might be
*  used in the future.
*  And so I think that's sort of what is complicated about it because of course you can't
*  anticipate all those possible future uses and other data sets that people might want
*  to combine it with to infer something else or whatever.
*  So yeah, I mean, I think, but that's not to say like we shouldn't collect data.
*  It's just to say that we have to do it with some of those caveats and things in mind.
*  And then I think, you know, in particular for data feminism, we think in particular
*  about ways that structural forces of inequality like sexism and racism can enter that
*  process.
*  So really tuning into, well, like, what are those ways that they enter?
*  You know, really obvious one is when you collect gender data, for example.
*  And so like, you know, when we are collecting gender data, thinking about, you know, first
*  of all, why are we collecting gender data?
*  But then also thinking about how, you know, it's, you know, 95% of the time when I'm
*  filling out a form that is asking for my gender, there's only a binary choice, but
*  there are far more than two genders.
*  And so kind of thinking very carefully about how some of our categories that we've sort
*  of naturalized, so like this, we inherit this like received wisdom of like there's two genders.
*  Well, in fact, there aren't like empirically speaking, there are not two genders.
*  Same with race, sort of thinking about like, how do we ask people to enter their race or
*  how do we racially identify people?
*  All of these are, I think, really fraught.
*  But then even things that are not overtly related to say racism, sexism, gender, race,
*  identity categories, but then that can often be used to infer those things.
*  So like, for example, in the United States, things like zip code, you can infer somebody's
*  race with something like 80% accuracy from their zip code because our country is so sacred,
*  so segregated.
*  And so thinking about ways that even collecting something like a zip code can be racialized,
*  right?
*  Like somebody could use that or a system, even not even intentionally could end up
*  differentiating people and sorting people by sort of racially by proxy through the zip
*  code.
*  And so like these are the things to think about as we're collecting data is like what
*  data may reveal about us or about these different group based identity categories that may
*  have implications downstream for whatever system you're building.
*  On the specific issue of the two genders, I just want to, for the audience's benefit,
*  say two things.
*  Number one, one of the very first podcasts I did was with Alex Drager, and we talked
*  about all the different ways one can fail to fit into the natural categories of the
*  two genders that we're most familiar with.
*  But then also in your book, you have this lovely chart that I had never seen before.
*  I don't know if it was brand new with you, if you got it from somewhere that traces all
*  the different ways you can end up in between the traditional polls of biologically male
*  and female.
*  And this is not even about gender, right?
*  This is about sex.
*  It's just just biology.
*  Like you forget about psychology.
*  I love, yeah, I love that piece.
*  It's a piece from Scientific American and it's called like Beyond XY.
*  XX or something like this.
*  And yeah, it's a beautiful flow chart.
*  The designers and the research team looked across all the most recent literature in sex
*  differentiation and showed how, again, like our received wisdom is that like there's like
*  three, maybe three sexes, male, female, and intersex.
*  And that it's like biologically determined at birth and then it's just set, right?
*  But it's a beautiful flow chart visualization that shows how, no, in fact, sex is differentiated
*  and it unfolds.
*  It's dynamic.
*  Over time.
*  So it kind of over time.
*  Yeah.
*  And it's, I just love the piece because it really complexifies even in, I would say,
*  gender studies because in gender studies, there is also received wisdom of like, okay,
*  well, gender is more of your identity and sex is a kind of biologic thing.
*  So even in gender studies, that's the received wisdom there.
*  And so I love that that piece is kind of complexifying and challenging that received
*  wisdom.
*  Well, the word complexifying is great here because that's a lot of the work that you've
*  set for yourself, right?
*  Taking clear and easy distinctions and sort of saying, well, it's not always quite that
*  simple.
*  And the other thing I love about the chart is there's nothing in there about your feelings.
*  It's about a mutation in this gene leads to a decrement in this particular hormone.
*  And it's hard to look at the chart and go, oh, you're denying the science.
*  It's exactly the opposite of that.
*  Yeah.
*  No, it's like showing the science.
*  It's bringing us up to date about the science, actually.
*  And the one other example I love because it's just so direct and relatable is the crash
*  test, crash test dummies and how they've affected ideas about safety in cars.
*  So I'll let you tell that story if you want to.
*  Sure.
*  Yeah, no.
*  So for years and years, we only used male sized crash test dummies.
*  So these were based on the statistical average of the male body.
*  And in fact, what this led to is that pregnant people and women were 40% more likely to be
*  injured or die in a car crash because we hadn't been basing it on women's bodies.
*  And so this is just such a clear and also such a harmful example of the ways in which
*  just a simple thing of not considering gender differences in bodies.
*  Yeah.
*  Right?
*  Just such a clear example of that.
*  It's very similar too to like, it was only fairly recently that the NIH mandated that
*  you have to have equal numbers of women and men participants in health research studies
*  because for many years, women were excluded from scientific research due to, evidently,
*  their menstrual cycles.
*  I'm just imagining a bunch of men being like, oh, I don't know, they're weird.
*  They have menstrual cycles.
*  We definitely have.
*  We can't include that because that might mess everything up.
*  But I just find that so interesting because men have hormonal cycles too.
*  So it's like their hormones will mess things up with women's well.
*  But anyway, so yeah.
*  But so I think that's a really clear example of this sort of thing where, again, it kind of
*  goes back to this objectivity question of true objectivity.
*  Maybe that would be great, but we don't have it yet.
*  There's all these ways that we have excluded and we are still pretty far from
*  including.
*  And so we have to work on that.
*  Well, the crash test dummy example is a good one because it seems like such an obvious mistake.
*  Like if you're trying to be objective and you know that different people come with different
*  size and shaped bodies, isn't the most objective thing to do to try to get the cross-section that
*  is as fair as possible of bodies.
*  But these human flaws, and again, sometimes they might be pernicious, but sometimes they
*  might just be as human beings were finite and fallible.
*  They get in the way.
*  I remember the story and I'm not going to get the numbers right.
*  So I'm sure someone on the Internet will correct me.
*  But Sally Ride, when she was the first female astronaut on the space shuttle, like they
*  packed something like for a week long trip, thousands and thousands of tampons because
*  all you had to do was ask somebody, right?
*  Even if it was all men doing the planning, how hard would it have been to ask?
*  That got in the way.
*  Right.
*  I love this story.
*  Yeah, I know.
*  I think this story is hilarious.
*  But it shows you how things like social stigma, how they shape our inability to even have these
*  conversations, right?
*  Like the fact that they couldn't ask her until she opened up some cabinet and a bunch of
*  any other woman in the world.
*  Yeah.
*  Yeah.
*  So yeah, I think that's the thing too is sort of like, if you just think about that as a
*  bias, but then magnified up into what do we take on as a research subject in terms of
*  like who's going to work on menstruation, for example.
*  Like if the majority of scientists in health, let's say, think that menstruation is weird,
*  like they're not going to work on it.
*  Or who's going to work on transgender health or who's going to work on segregation.
*  So it's sort of thinking about like how these taboos, norms, stigmas, values, sort of human
*  and social things, they creep in and they affect things.
*  They are pernicious ways, but they're not often, I would say the majority of the time,
*  they're not often from like some one person's brain being like, I'm going to do this.
*  They're systemic.
*  Again, this systemic inequality that we swim in every day.
*  And so that's why it's important to have both a theoretical and kind of a material
*  understanding of like, what is this water that we swim in?
*  How do we look out for those things?
*  And how do we ultimately create better work and better science by being able to recognize
*  those things and take steps to avoid them in our own work?
*  Well, this is, I might be going, I might be generalizing too glibly here, but I think that
*  this is one of the reasons why the tension about feminism is particularly sharp in my own field of
*  physics and closely related fields of computer science and philosophy and things like that,
*  because on the one hand, there are fields that valorize objectivity and treating everyone
*  equal would be part of that goal.
*  But on the other hand, they make all their money out of simplifying things, right?
*  And boiling things down to the essence and treating things like spherical cows.
*  So this messiness of saying, well, there's actually a lot of heterogeneity in the sample
*  and things like that is anathema a little bit to the method that has been so empirically
*  successful in these fields.
*  So there's a mismatch there, which maybe shining some light on it will make it a little bit better.
*  I don't know.
*  Yeah.
*  No, I think that's a really important observation.
*  And I still, I hold hope for sort of like the quantitative fields.
*  In a way, what I hold hope for is when I see really excellent mixed methods research,
*  because I think we do get somewhere by simplifying.
*  We just have to remember that we simplified.
*  I was like, again, we can't confuse the representation of reality for the reality itself.
*  We can't confuse the spreadsheet for the truth out there.
*  Even though they are interconnected, they have a relation there that's important.
*  And so I think for me, I get excited when I see sort of the mixed methods.
*  And one example, like a concrete example I'll give of this is Mary Gray has a book called Ghost Work.
*  She wrote it collaboratively and her co-author is a computer scientist.
*  She's an ethnographer and qualitative researcher.
*  And it's all about the sort of behind the scenes labor of the platform economy.
*  Uber basically.
*  It's like all the people that work behind the scenes.
*  They showcase a lot of folks in India, for example, that they're kind of like helping
*  the algorithm along the way as it does things.
*  They do human in the loop sort of approvals of things and checks on people's licenses
*  and stuff like that.
*  And it's this really lovely study because Mary will be doing ethnography and interviewing people
*  in their houses in India.
*  And she'll surface an interesting question.
*  And then her partner will go test that computationally and quantitatively across a bunch of
*  network data.
*  And then he'll surface some interesting questions and then she'll kind of go try to validate
*  those in interviews.
*  And I get really excited by this kind of work because I feel like it's building on sort of
*  the strengths and the limitations of both things.
*  Because the qualitative work is like, okay, you can interview like, even if you interview
*  100 people in India, it's not like you've interviewed the population.
*  And then for the quantitative work, it's like, okay, you've got all this really great coverage
*  like in terms of scope and scale.
*  But then what explains the variation?
*  Right.
*  And so I love how they can kind of go back and forth and make it really sort of multiscalar.
*  And so that for me, like it's a little bit closer, at least when we're talking about
*  things in the kind of human and social and political realm, to being able to use these
*  kind of different fields methods to their best advantage and in a complementary way.
*  Again, to get further to like kind of what is really explaining some of this variation
*  on the ground.
*  Yeah.
*  And again, if you put it that way, who can object to the idea that we should have these
*  richer methodologies that we might find something out?
*  I don't know.
*  But people can be a little bit resistant, which I guess brings me to this question I
*  probably should have asked within the first five minutes.
*  But how do you define the word feminism?
*  What does that mean to you?
*  That's one of the points of contention, which maybe it shouldn't be, right?
*  Like whether or not you think it has a positive or negative connotation.
*  Sure.
*  Yeah. So for us, and I should say there's many feminisms.
*  And Lauren and I in the book are specifically pulling from intersectional feminism.
*  But at the very basic line, if you are a person who believes in equality for all genders,
*  you're a feminist.
*  And that's kind of like the basic definition.
*  And that is feminism.
*  Is it a belief that all genders are equal?
*  And there's a kind of a corollary to that.
*  Because if you believe that all genders are equal and then you look at the world around you,
*  you can see that that equality has not been realized in the world.
*  And so feminism compels you to take action to realize the world in which all genders are equal.
*  And so I would say those are two kind of indisputable things about feminism.
*  And then the sort of specific type of feminism that we draw from is called intersectional feminism.
*  And this is an idea out of black feminism in the US, where black feminists said,
*  we cannot consider, gender inequality can't explain our reality and it can't explain social
*  inequality because we have to take race into consideration.
*  And so that would be the basic idea of intersectionality.
*  And so this idea that we include both sexism, racism, and then since then,
*  other sort of forces of oppression.
*  So thinking about like classism or colonialism and so on.
*  So that we can try to think simultaneously, uncomprehensibly about how those forces
*  intersect to produce social inequality.
*  So that's the, and I should say intersectionality is pretty well established at this point.
*  So that was around in the late 80s, early 90s.
*  And that is, I would say, the dominant kind of sort of feminist thought today.
*  I think physicists would be a lot more in favor of the idea of intersectionality if it were just
*  labeled non-linearity.
*  I think that's what we would call it.
*  That's great.
*  Okay.
*  Maybe I'm going to say that next time I speak to physicists.
*  If I understand it correctly, it's just the idea that whatever discrimination you face for being,
*  let's say, a woman and black is not just the discrimination you face for being woman,
*  plus the discrimination you face for being black.
*  They can interact non-linearly.
*  Yeah, they can enhance.
*  Exactly.
*  Yes, that's exactly right.
*  Yeah, yeah, yeah.
*  Okay.
*  I'm going to totally remember this when I'm talking like to, is this just in physics?
*  Or anything, anything, any field that likes to have equations.
*  Non-linearity is literally a feature of equations that just says you can't add things and get the
*  sum of the effects, it's not the sum of the causes.
*  That's all it is.
*  Exactly.
*  Kimberly Crenshaw's points is like, it's not reducible to these two separate things,
*  but they combine and interact.
*  The Schrodinger equation is linear, but human beings are absolutely not.
*  Pretty obvious.
*  But the less obvious thing, I mean, the thing that, you know, your definitions of feminism
*  seem pretty transparent and unassailable, but the tricky part is that you use the word equality
*  without quite digging into what that means.
*  And if I pretend to be, you know, hard nosed about this, I say, well, what do you mean?
*  I mean, the average height of women throughout the world is not equal to the average height of men.
*  They're not equal in that sense.
*  So what do you mean?
*  That men and women, all genders should be treated equally?
*  Yeah, I mean, equality of access.
*  To services, happiness, equality of opportunity, to promotions, to et cetera, and equality of power.
*  I mean, I think like this is when we'll know that we have achieved feminist goals is when
*  we look at who's in power and it's representative of the population, you know?
*  And, you know, it's just still, we're so far behind on so many things, like in terms of both,
*  if you just look at like political representation or if you look at like the wage gap,
*  just all of these different markers and measures.
*  And so I think that to me would be like, this is when we'll have achieved the world in which
*  we have equality of genders is when all those measures of power have equal representation.
*  Yeah, I mean, this is tough for me because, well, I had Elizabeth Anderson on the podcast
*  and equality is her thing.
*  I'm not sure if you're familiar with her work, but a leading theorist of equality.
*  And people also, you know, the audience members I can tell from YouTube comments, et cetera,
*  some of them get their hackles up because they hear the word equality and they instantly
*  read that as equality of, I don't know, wealth, right?
*  Like everyone has the same amount of wealth or the same about anything.
*  And Anderson's point of view is much more nuanced and it's more about, you know,
*  the equality of the ability to become who you want to be or something like that, right?
*  I can at least imagine a world where men and women have exactly equal equality to become who
*  they want to be and a bunch of women decide that they don't want to be in positions of power,
*  right?
*  I mean, maybe that's not the actual world because we're very far from this thought
*  experiment of equal opportunity, but I worry about measuring it or judging it that way
*  because, you know, that's an outcome based measure.
*  And I want the, it's like as a poker player, I know that you can do the perfect strategy
*  and the outcome might not be what you want or what you anticipate, right?
*  So maybe we have to be a little bit more nuanced about the kind of equality that we're shooting
*  for here.
*  Yeah, I mean, I like Anderson's definition.
*  There's sort of equality to equal opportunity to become who you want to be or something like
*  this.
*  I might have mangled it, so sorry, Elizabeth, but yes, I think that's the thrust of the idea.
*  No, and I like that.
*  And I do think like, as more women come into power, we might have, we might see a transformation of
*  the institutions themselves.
*  Like, yeah, maybe the institutions themselves will change and look really different from what
*  they looked like before.
*  Because I do think there is a, you know, if you look at, you know, other strains of feminism
*  sort of that are like kind of pushing women into the corporate workforce, they're like,
*  being like, hey, be more like men, you know, like, you know, wear your power suit and whatnot.
*  And so I certainly don't think like the goal is not just to like fill the pipeline, which is
*  sometimes where I get frustrated with that line of research, particularly in STEM, which is not to
*  say it shouldn't exist.
*  Like, I'm glad people are thinking about this, but it's always framed as like, where are the women?
*  Not like, why are the men taking up so much space?
*  Like, it's always like the women's problem that they're missing.
*  And so, so yeah, I mean, I think like institutions themselves will change as different people,
*  both women and people of color come into power.
*  And that's possibly a really awesome thing.
*  So for example, I'll say like, I run a research lab at MIT, and I'm trying really carefully to
*  run it in a feminist way and an inclusive way.
*  We have a kind of a handbook, we have a set of values and norms.
*  We try to do our best within our little scope and sphere of operation to push back against
*  some of the sort of toxic academic culture that exists at universities, both for students and
*  faculty and staff and so on.
*  And so thinking about the ways in which and those are small interventions, but I think they are
*  potentially transformative.
*  So I think it's right that institutions themselves may transform in the process of realizing this
*  sort of like equality of access, opportunity, sort of freedom to become the person that you want to
*  be and so on.
*  Well, and this leads in very well to sort of what I wanted to ask for the last big topic here,
*  which is we talked a lot about how not taking feminism seriously or not taking fairness in
*  some broad sense, justice seriously can lead to these unintended biases in data analysis and
*  processing and so forth.
*  So what about the flip side?
*  I mean, can we use data or the analysis of data or the collection of data to bring about a better
*  world?
*  Can we be proactive data feminists in that sense?
*  Absolutely.
*  So yeah, and I'll give you a really concrete example, which is my next project.
*  I'm working on it now and it's going to be my next book.
*  And in fact, I sent the book proposal today.
*  So congratulations.
*  So one of the things I'm looking at, you may remember from the book, we talked about the case
*  of Maria Salguero, who she is collecting data about feminicide in Mexico.
*  So this is gender based violence.
*  This is when women are killed basically for being women.
*  And this could be what we call in the United States, like intimate partner violence.
*  But it also happens in not only with intimate partners, but with other family members.
*  It happens with sort of drug trafficking, et cetera.
*  And this is a big topic in Latin America.
*  Governments are passing laws about feminicide and yet governments are not systematically
*  tracking feminicides themselves.
*  And they're not kind of putting into place the apparatus that would enable them to,
*  like medical examiners to label deaths as feminicides and things like that.
*  And so Maria Salguero sort of steps into this.
*  And for five years, she's been collecting data about feminicide from news reports.
*  And she's ended up with the largest public database of feminicide in Mexico.
*  And she's ended up sharing data with journalists, NGOs.
*  She's actually testified in front of Mexico's Congress a couple of times.
*  And we talked about this as this really interesting example of sort of feminist counter data.
*  So like kind of collecting data in order to hold institutions and societies accountable
*  for things that they are missing, that they are not doing.
*  And since then, I ended up, well, I was living in Argentina, in fact, on sabbatical and ended up,
*  I was really interested in this.
*  I started asking around.
*  I started meeting other groups.
*  It turns out she is not at all the only one who's doing this.
*  There's many groups across Latin America who are doing this work.
*  They range in size from people like Maria, who are individuals, to large nonprofit
*  organizations who are mapping and monitoring gender-based violence.
*  And so I've been interviewing them.
*  And in fact, my lab has started building technology to help them detect feminicide
*  better from news reports because invariably they're using news and social media to tally
*  these killings.
*  And so this is a case that is, I think, really interesting for how we can collect back or
*  monitor back or really think about how do we use data, aggregate these statistics as a way
*  of holding governments accountable and building political will towards social change, to really
*  making a case and building an evidence base for changing policy.
*  And so yeah, so I think that's one example.
*  I think there's many other examples out there.
*  And in fact, especially, and that's one of the reasons I'm also so committed to data
*  literacies because I think these are tools that can and should be in the hands of human
*  rights groups, journalists who are doing some of the most interesting sort of accountability
*  work with data right now, social movements, community-based organizations, and so on.
*  So I think there's a lot of power and potential of data science really for good.
*  That's what we're talking about here is data science for justice.
*  Yeah.
*  So yeah.
*  And I think along those lines, there was one of the points you made in the book that
*  your sort of suggestions to do better, which really struck me was the blending of reason
*  with emotion or let emotion exist.
*  Don't be afraid of it.
*  And you can elaborate on why we should do that and what it means, but it reminded me of
*  a talk I heard at MIT by Evelyn Fox Keller, your MIT colleague, about the dawn of the
*  scientific revolution and how it wasn't completely organic.
*  Francis Bacon and friends would sit down in the coffee shops and decide what was science
*  and how we do science and how we go about it.
*  And they made up a bunch of rules for sounding more objective than we really were, right?
*  Like write in the second person, explain that things couldn't have been any other way.
*  Don't tell about all the mistakes you made.
*  Just tell the successes.
*  And it's not as necessary or inevitable.
*  It was a choice that was made along the way.
*  And so, I mean, maybe give your point of view of why it's okay or even good to not always
*  try to sound as objective as we could possibly be.
*  Totally.
*  Yeah, no, I love that.
*  And this goes back to the objectivity thing, in fact, right?
*  Because it's sort of like when we have to put on that cloak of objectivity,
*  we are sort of trying to push that other stuff under the rug.
*  The mistakes that we made, the ways in which our own blind spots were maybe uncovered during
*  the process, and what a kind of feminist approach would say is, in fact, we want to see that.
*  That's part of being transparent, in fact, is not only telling this heroic
*  genius story of some kind of discovery or new knowledge that we made, but also showing those
*  bumps in the road, showing the blind spots, showing how we got help and support.
*  That is, in fact, much more transparent along the way.
*  And then one of the things that we say about reason and emotion is that
*  a kind of feminist maxim is whenever you encounter a binary, you should be deeply
*  skeptical of it, because usually it's hiding a hierarchy.
*  And usually it's empirically false, right?
*  There's a gender binary man and woman, that's hiding, it's just false.
*  There's more than two genders, right?
*  And we say this about the reason and emotion as well.
*  That's often a binary that we encounter of reason, on one hand, emotion.
*  On the other hand, it's hiding a hierarchy.
*  Reason is usually seen to be on top, and emotion is this messy thing that's gendered,
*  that more women are emotional or whatever.
*  And yet, there's this wonderful line from Patricia Hill Collins, where she says a feminist
*  knowledge would be about valuing reason, emotion, and ethics equally, on an equal playing field.
*  And the reason to do that is, again, acknowledging the situatedness of our own knowledge and our
*  own selves.
*  It's about being transparent about our own limitations and the fact that we have emotional
*  motivations for studying things, for doing things.
*  And then when we communicate our knowledge, communicating with emotion is a way to make
*  it accessible to other people as well.
*  So using emotion and communication is a way of being inclusive.
*  And this, I think, comes in particular with data visualization, which can be seen as a very,
*  you know, it can be seen as a very scientific, technical, specialized way of communicating,
*  which is fine if what you're doing is in a scientific journal or something.
*  But when you're doing it more for a general public, it can be sort of, it's like a gated
*  community.
*  It's like letting certain people in, and it's prohibiting access by other people.
*  So thinking through about what are ways that we engage emotion, that we engage multimodal
*  sensibilities for people to access whatever sort of messages with the data.
*  So like emotion brings all these things into our toolkit of communication that are lost.
*  If you're like, no, it must be neutral, objective, and appear scientific or whatever.
*  So, yeah.
*  Well, you know, David Hume, one of my favorites, said that reason is and should be the slave of
*  the passions, right?
*  I mean, reason gets us what our passions tell us is what we want in some sense.
*  So they need to go hand in hand.
*  It reminds me a little bit what you just said of controversies in news reporting from wars
*  and things like that, where you're allowed to say how many people died, but the government
*  will try to prevent you from showing pictures of those dead people.
*  I mean, in principle, no extra information is conveyed, but the emotional resonance is
*  a very different thing.
*  And that matters, right?
*  Yes, absolutely.
*  Yeah, there's a really interesting piece.
*  Now I'm forgetting the author, but it's something like numbing with numbers or something.
*  It's this really interesting piece about how often we are moved by the story of one sort
*  of unjustified death, but not by numbers describing kind of mass death.
*  It's a really interesting philosophical reflection on that.
*  And then also how do we communicate the scale of things like a genocide or these kind of
*  mass extermination events at a scale that is meaningful to us as human beings and doesn't
*  just numb us into feeling disempowered by the scale of it or something like that.
*  But I always like to end the podcast on a more or less optimistic note, if possible.
*  So maybe say one more thing about how great it is that we can use data and analysis and
*  all these things to make the world a better place.
*  Not end on death.
*  Yeah, so I think Lauren and I talk a lot about, in data feminism, we try to describe some of
*  the ways that our data are infected and polluted by the inequalities that we encounter in the world.
*  But at the same time, we do posit and advance the idea that data are also part of the solution.
*  And so really thinking about ways in which we put that power of data into the hands of the people
*  who can really make social change and use data to challenge those same structural inequalities
*  that keep showing up over and over again in our data sets and in our systems.
*  And did I hear correctly there? This is the most important controversy of all that we
*  haven't even mentioned. You treat data as plural, not singular?
*  Well, we flip back and forth. So when we talk about big data, we tend to use singular because
*  it's like big data is blah, blah. But if I'm talking about a data set, like a collection of things,
*  then I tend to use plural. But I probably slip and go back and forth.
*  There you go, complexifying the world for us once again.
*  Yeah, I feel like physicists should like complexity, right?
*  No, kicking and screaming. They would like everything to be a perfect sphere as far as
*  it's concerned, honestly. But complexifying is a good thing for human beings when they're not
*  doing physics. And with that in mind, Catherine D'Ignazio, thanks very much for complexifying
*  our worldview here on the Mindscape Podcast. Lovely. Thank you. It was a pleasure.
*  Thank you.
