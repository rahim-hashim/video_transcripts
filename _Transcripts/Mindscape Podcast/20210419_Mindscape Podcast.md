---
Date Generated: June 09, 2024
Transcription Model: whisper medium 20231117
Length: 5538s
Video Keywords: ['bias', 'rationality']
Video Views: 18539
Video Rating: None
Video Description: Patreon: https://www.patreon.com/seanmcarroll
Blog post with audio player, show notes, and transcript: https://www.preposterousuniverse.com/podcast/2021/04/19/143-julia-galef-on-openness-bias-and-rationality/

Mom, apple pie, and rationality — all things that are unquestionably good, right? But rationality, as much as we might value it, is easier to aspire to than to achieve. And there are more than a few hot takes on the market suggesting that we shouldn’t even want to be rational — that it’s inefficient or maladaptive. Julia Galef is here to both stand up for the value of being rational, and to explain how we can better achieve it. She distinguishes between the “soldier mindset,” where we believe what we’re told about the world and march toward a goal, and the “scout mindset,” where we’re open-minded about what’s out there and always asking questions. She makes a compelling case that all things considered, it’s better to be a scout.

Julia Galef received a BA in statistics from Columbia University. She is currently a writer and host of the Rationally Speaking podcast. She was a co-founder and president of the Center for Applied Rationality. Her new book is The Scout Mindset: Why Some People See Things Clearly and Others Don’t.

Mindscape Podcast playlist: https://www.youtube.com/playlist?list=PLrxfgDEc2NxY_fRExpDXr87tzRbPCaA5x

#podcast #ideas #science #philosophy #culture
---

# Mindscape 143 | Julia Galef on Openness, Bias, and Rationality
**Mindscape Podcast:** [April 19, 2021](https://www.youtube.com/watch?v=NRRRKzAJbHg)
*  Hello everybody, welcome to the Mindscape Podcast. I'm your host Sean Carroll and
*  today here on the podcast we are going to get rational. I know that a lot of
*  you are thinking well it's about time but we always try to be rational, right?
*  As I make the point early in the podcast there are very few people who actively
*  brag about not being rational but it's easier said than done. We would like to
*  be rational but how do you do it? How especially do you admit that you are not
*  always rational and reorient your mindset in a way that helps you be more
*  rational, whether it's in sort of high-level problem-solving for some
*  scientific or technical job you have or just in your daily life? Who to date? What
*  to have for dinner tonight? We would like to be rational about all of these
*  different choices and so today's guest Julia Gallif is a star in the
*  rationality community, someone who has given a lot of thought into these real
*  efforts that an individual can put into becoming more rational themselves. She
*  doesn't want to just give you a list of ways in which you're not rational, that's
*  that's been done quite a few times. She wants to give you useful actionable
*  advice for becoming more rational yourself. So she has a new book coming
*  out called The Scout Mindset. Julia's metaphor here is that in a war, there's a
*  military metaphor going on here, so you have soldiers. The soldier's job is to
*  say, look, I have a goal, I'm gonna achieve that goal. Nothing's gonna stop me from
*  achieving that goal. The other mindset you might have is that of a scout. The
*  scout doesn't have a goal to sort of take over a certain installation or
*  whatever, win a certain battle. The scout's mindset is to gather all the
*  possible information with an open mind, to be as open as possible to all the
*  different pieces of information, whether or not they reinforce what the scout
*  already believes. And so Julia is hoping that we can all develop more of a scout
*  mindset. So a lot of the podcast episode is going to be exactly that. How do you
*  develop this scout mindset? How do you actually train your brain to be open to
*  all the information? But there's an interesting sub conversation we have
*  about justifying what seems to be obvious, namely that being rational is
*  good. So there are people out there, both explicitly and implicitly, who make
*  arguments that, you know, rationality is overrated, right? That in fact, there's
*  something maybe evolutionarily useful in being irrational in the right way. And
*  so Julia is going to defend the position that we should be rational in the right
*  way all the time. And, you know, that's an extreme position, but I think I'm
*  pretty much in favor of this one. I think that we're on the same side. So I
*  try my best to play the devil's advocate here, but overall, I'm more or less
*  sympathetic. So occasional reminder, you know, we have a web page for the podcast,
*  preposterous universe dot com slash podcast, which is a good place to go
*  because we have full transcripts for every episode. And they're searchable.
*  You can search for any word that has ever been said on Mindscape and find it in
*  the transcripts on the website. And the other thing is, you know, we like it when
*  you leave reviews of the podcast on places like Apple podcasts and things like
*  that. iTunes reviews, they help raise the visibility of the podcast. If you think
*  that the podcast is a special experience that you want for yourself and don't want
*  to share it with anyone else, then don't bother. But if you think that Mindscape
*  is something that would be great if other people were listening to, then
*  raising the overall level of exposure is a good thing. So I'm very grateful to
*  everyone who does that. So let's go.
*  Julia Galliff, welcome to the Mindscape podcast.
*  Thank you. So great to be here finally.
*  So like many of my guests, you have put yourself in a position apparently
*  intentionally. So what I mean by that is you're defending the idea we should all
*  be rational. I had David Baltimore in the podcast a while ago. He's a Nobel
*  Prize winning biologist. And we had tried, I didn't know and knows this yet.
*  Sorry, David. But while we were talking before the podcast, we had trouble
*  getting his microphone to work. And I said, like, once you win the Nobel Prize,
*  don't you constantly get crap for like not being able to get the microphone to
*  work? Do people say like, Oh, I thought a Nobel Prize winner should get to do
*  that. And he's like, Oh, yeah, you have no idea. It's just like that. So
*  similarly, don't you open yourself up when you're being pro rational as an
*  explicit position? Like every single thing you do, people are going to say,
*  well, that wasn't very rational, was it?
*  That is, yes, that is true. And that is despite the fact that I, I try to
*  emphasize that I, I definitely don't think of myself as perfectly rational, I
*  don't think anybody is perfectly rational. And I also try to have kind of
*  a light touch in the way in which I'm like pushing for rationality. So for
*  example, what I mean by that is some, some advocates of rationality or
*  intellectual honesty or other things in that space, they, they will often claim
*  explicitly that everyone, it's always better to be rational, or it's always
*  better to be to see, you know, the truth. And I don't think that they can actually
*  support that. I've never heard anyone give a defense of that claim that seems
*  sufficient for how strong a claim that is like, how do you know that everyone's
*  always better off being rational? Like, how do you know that everyone's
*  happiness is maximized with a rational view of the world in every situation?
*  That's a strong claim. So I try not to make that strong claim. And instead, I
*  will make more nuanced claims that I think I actually can defend. Like, I, I
*  think we can be pretty confident that people would be better off being at
*  least somewhat more truth seeking than they are by default. And I can like
*  explain why I think that's the case, that like on the margin, we would be better
*  off with more truth and less self deception. But that's different from
*  claiming that everyone is always better off all the time, you know, being, being
*  rational. That is true. But as someone who's studied a little bit of the human
*  psychology required to try to be more rational, I'm sure you understand that
*  the arguments that people are responding to are not always the arguments that
*  you're actually making.
*  This is true. This is true. And that's something that I mean, it's, it's always
*  like a little bit frustrating when people aren't hearing the thing that you
*  intend to be saying. Yeah. But I have I've come I've made my peace with it to,
*  to a large extent in the last few years, just by basically, I just realized that
*  communication is always imperfect and always involves some amount of making
*  assumptions about what the other person meant to say. And so if, if most other
*  people in my in a position similar to mine, like if most other people who are
*  advocating for rationality or truth, or, you know, intellectual honesty, if most
*  of them are saying a more extreme thing, then it's, it's kind of understandable
*  how my listeners unless they're really going out of their way to be super
*  careful, which most people aren't most of the time, it's, it's understandable why
*  they would hear the extreme thing, even if I'm not saying the extreme thing. So
*  I've just kind of, I've made my peace with accepting that people are going to
*  hear the thing I'm not saying. And I just kind of preempt it to whatever extent I
*  can, and then try to explain what I'm actually saying.
*  That is very general, we should always aim to be, you know, that generous to our
*  listeners. But anyway, I don't want to worry too much about the flak you're
*  getting for this, because in some sense, let me come from the opposite direction.
*  I mean, is there anyone who self professes to not be rational or to not
*  even try to be rational or not value rationality isn't rationality is one of
*  those ultimately benign values that everyone thinks they're upholding?
*  So I know, I understand why you say that. Because there are tons of people who
*  claim to be rational or prize rationality or prize, you know, you should
*  always change your mind in response to the evidence, like I always change my
*  mind in response to the evidence. And, and I think very few of those people come
*  close to upholding the value that they're professing. And in fact, I kind of
*  learned this the hard way when I was researching my book, because I would
*  find these quotes, you know, I was always looking for like good quotes or good
*  examples for the book. And I'd find these amazing, you know, quotes about
*  intellectual honesty is saying things like, you know, it's so important to
*  change your mind, you know, when you notice you were wrong. And then I would
*  look at the source of the quote, and I'd be like, this is so I'll just give you
*  one example, I forget the exact quote, but it was something like, you know, I
*  always it's so important to change your mind in response to the evidence. And the
*  source of the quote was Rudy Giuliani.
*  I was like, I can't, I can't put this in my book. So I get what you're saying. And
*  I basically agree, except that there actually are a bunch of people who
*  explicitly reject the principles of rational thinking or intellectual
*  honesty. And I think they they kind of fall into two categories. One category
*  is the kind of postmodernist or, or maybe in a sense, romantic school of
*  thought, where, you know, there really isn't any objective truth. And it's,
*  like all that exists is different people's subjective realities and any
*  attempt to try to ferret out the objective truth or try to, you know, like,
*  figure out why two people disagree is doomed to failure, or it's, you know, an
*  agent of like, white supremacy or, or bigotry to to even talk about their
*  their being in theory, a correct answer. So that's one sense in which people
*  disagree with me. And then another sense is, I think just kind of a classic, like,
*  a feeling that it's more noble to stick to your guns and never change your mind.
*  So there's a metric, a scale in cognitive science called active open mindedness,
*  which was invented and popularized by John Barron. And it's just a questionnaire
*  about, like, what, how do you think people should think? So it's not really
*  measuring how you do think, but it's just, what do you think good thinking is?
*  And so it includes questions like, should people change their minds when they
*  encounter evidence that contradicts their beliefs? And so this is the kind of
*  questionnaire where you would think, of course, everyone would say yes to all
*  these questions. But in fact, a lot of people say, no, you shouldn't change your
*  mind. And I think they're thinking of like, I don't know, deeply held political
*  or religious beliefs or something, and they see value in just sticking to your
*  guns no matter what. So that's another kind of person who I think disagrees
*  with even the theory, like the theoretical idea of trying to adjust your
*  beliefs to the evidence.
*  Well, I think that second category interests me. I suspect that the first one
*  is pretty thin on the ground. Like, you have to really work hard to find people
*  who actively think that being rational or finding the truth is an instrument of
*  oppression. I think that the existence of such people-
*  I don't know, they do a good job of finding me. I don't feel like I have to
*  look that hard for them.
*  Well, exactly. I mean, I think that the few examples that there are might stand
*  out. It's like, you know, it's like using Twitter as a proxy for public opinion,
*  right? You know, like you're not hearing the unbiased sample in some sense. But
*  the second one is interesting to me because I can almost see it. I mean, I
*  don't agree with that attitude, but I get where they're coming from in some
*  interesting way. And I think we'll get there down the road. But I do have this
*  feeling that when people say, you know, let's be rational, you can have two sets
*  of people who are both saying, let's be rational, let's stick to the evidence,
*  let's draw conclusions based on reason, even though they utterly disagree with
*  each other, right? And they're using it to bash the other side. So in some sense,
*  do we agree on what it means to be rational, for one thing? Is there a sense
*  in which all right thinking people know what they mean when they say, let's be
*  rational?
*  No, I think there's like tons of disagreement about that, about what people
*  mean by that word. Some people mean something like, I don't even, it's so
*  hard to even try to summarize what different people mean by this. I think in
*  practice, what a lot of people mean is, you should believe what I believe. And so
*  anything that disagrees with me is not rational. That's like a common
*  colloquial interpretation of the word. There's, there's, you know, kind of
*  technical definitions you could give from philosophy or, or like economics or
*  computer science even, of what in like a theoretical rational agent would, how a
*  theoretical rational agent would update their beliefs in response to new
*  evidence, or how a theoretically rational agent would make decisions to maximize
*  their utility, like to pursue their goals effectively. I, but, but you know,
*  any attempt to apply those definitions to actual messy human reasoning in the
*  real world is going to involve, it's going to be messy. So I, you know, I don't
*  try that hard to come up with a definition of how to think rationally
*  that is all comprehensive. But you know, there are heuristics, like, I think, I
*  think it's pretty arguable that, you know, trying to at least sometimes look for
*  reasons you might be wrong is probably going to make your thinking more
*  rational than if you didn't do that at all. So these, you know, these seem like,
*  like pretty weak claims, like, well, who would disagree? But I think that the
*  disagreement is just in, in practice, how often do people actually try to live out
*  these principles? So as you pointed out, there are tons of people who will agree
*  like, yeah, of course, of course, one should do these things, but then practice,
*  people rarely do. So I'm more arguing that, like, we should try to do these things
*  that, you know, that you probably agree already in principle are good.
*  Well, you already alluded a couple of times to the idea of being willing to
*  change your mind in the face of evidence, right? And this is closely related to the
*  constellation of ideas that goes under the umbrella of Bayesian reasoning,
*  right? So let's assume that there are people in the audience, it's probably not
*  very likely, but there are people in our audience who don't even know what Bayesian
*  reasoning is, have never heard the word Bayes. How would you talk about that way
*  of thinking?
*  Sure. So Bayes rule is just a simple theorem in probability theory about
*  basically how you should revise your confidence in a particular hypothesis in
*  response to new evidence. And it's pretty simple. It's just, your your
*  confidence should go up in proportion to the ratio of how likely is that evidence
*  in a world or like how likely would you guess that evidence is in a world where
*  your hypothesis is true compared to how likely would that evidence be in a world
*  where your hypothesis is false? So, you know, of course, in the real world, we
*  don't have perfect information. Like, we don't know exactly how likely how likely
*  is it that this paper in support of wearing masks for coronavirus? How likely
*  would that paper be to exist in a world where wearing masks helps compared to
*  a world where wearing masks doesn't help? We can only guess at those things. So
*  in practice, Bayesian reasoning just means kind of making an attempt at least
*  to make these guesses and at least get a sense of like, you know, the order of
*  magnitude. Like, is this paper a lot more likely in the world where wearing masks
*  helps than the world where it doesn't? If so, then that's, you know, it's kind of
*  strong evidence in favor of wearing masks. Or to take another example, if if
*  you read an article about how a woman in a tech company was passed over for
*  promotion because she was a woman, how strong is that evidence in favor of the
*  hypothesis that the world of tech is really sexist? I would argue it's not
*  that strong evidence just because when I imagine the world in which tech companies
*  are not more sexist than average, I think even in such a world, it's pretty
*  likely that there would be at least one case of a woman who was passed over for a
*  promotion due to her gender just because in any big industry that kind of thing is
*  going to happen at least sometimes. And so I would argue that that article
*  doesn't provide very strong evidence for that hypothesis about tech being
*  sexist. So that's like that's one aspect in which I think trying to be Bayesian,
*  trying to at least be mindful of Bayes rule when you're evaluating evidence
*  leads to thinking that even though it's not precise and mathematical, at least
*  isn't I think an improvement over default human reasoning. Yeah, one of the
*  examples of this kind of reasoning that I find people like the the basic idea
*  that yes, if the data you're collecting is more likely in one hypothesis than
*  another hypothesis, you should increase your credence in the one that's likely
*  under. Okay, right. Most people are going to go along with that at an intuitive
*  level. But in theory, in theory, no, what I mean is most people will say they're
*  going to go along with it, right, rather than actually going along with it. Yes,
*  well, they'll they'll say they're going to go along with it. But then in maybe
*  this is where you're really going to go with this. I don't mean to preempt you.
*  But I think there are a lot of situations where people explicitly reject
*  that reasoning. If the context is like, well, I don't know exactly in what
*  context people reject it. But but for example, I've talked to people who will
*  agree with that reasoning in theory, and in some contexts. But then when we're
*  talking about, like, I don't know, something political, like is Trump going
*  to win? Or could the Democrats be right about about immigration or something?
*  Then they will insist that, you know, nothing could change their mind or any
*  evidence that comes up, even if I think it's like way more likely in the world
*  where the Democrats are right, they will insist that it's zero evidence at all in
*  favor of the Democrats being right. So they will reject the idea of trying to do
*  that kind of estimate in their head, if it leads to conclusions that, you know,
*  would force them to downgrade their belief in a particular strongly held
*  hypothesis.
*  Well, the specific kind of mistake that I was going to mention is when there is
*  some evidence, and I would argue that it does go against someone's belief and all
*  that they think is necessary is to say, no, I can account for that evidence under
*  my belief.
*  Yeah, exactly.
*  What they don't do is say, well, if the evidence had been the other way, I would
*  have counted that as huge evidence for my belief.
*  Exactly. Yeah, I think it's like, I think it's a theorem that if the if the
*  opposite evidence would have increased your belief, then this evidence should
*  decrease it.
*  Exactly. Right. No, that's that's a great example of a kind of, like, common
*  sense implication of Bayes rule that people very often neglect and that, you
*  know, even if you're not plugging numbers into a formula, just just being aware of
*  the rule and the fact that it implies that, you know, if if X would have been
*  evidence that you would update on, then the opposite of X should it has to like
*  force you the other way.
*  Right.
*  Even that can, I think, provide a really useful corrective on your intuitive
*  reasoning on its own.
*  But what do you think?
*  Go ahead.
*  Sorry. I was just going to say an example of this that maybe you're familiar with,
*  but maybe some of your listeners haven't heard is during World War Two, there was
*  a lot of concern that Japanese Americans were going to betray the U.S.
*  in support of Japan.
*  And Governor, I think it was Governor Earl Warren in California was one of the
*  strongest proponents of this, you know, Japanese Americans are treacherous
*  hypothesis. And he was presenting this testimony to Congress that we should
*  intern Japanese Americans.
*  And someone points out, you know, there hasn't really been any evidence of
*  Sutterfuge from Japanese Americans.
*  And he said, ah, well, I take that to be, you know, even more confirmation that
*  they are planning Sutterfuge because they're being so sneaky about it that that
*  they've managed to hide all the evidence.
*  You're just like, OK, so it's evidence for your view, both if there is and if it
*  isn't any sign of Sutterfuge.
*  Well, yeah, I mean, the way that we see that all the time on social media is
*  everyone is criticizing me.
*  Therefore, I must be on to something.
*  Right. Right. No, that's a great.
*  Exactly. Right. So you're saying that if everyone was agreeing with you, that would
*  mean you were wrong. Yeah.
*  Maybe in theory, you believe that.
*  But in practice, I've never seen anyone actually act that way.
*  When it comes to hiring people, it's as much about who you say yes to and also who
*  you say no to.
*  So it helps. And you can narrow down the list to only great choices first.
*  So you're not choosing between good and bad, but between good and great.
*  That's what you get with Indeed.
*  Indeed is the job site that makes hiring as easy as one, two, three.
*  You post, you screen, you interview all on Indeed.
*  You can get a quality short list of candidates whose resumes on Indeed match
*  your job description faster.
*  And you can only pay for the candidates that meet your must have qualifications.
*  Then you can schedule and complete video interviews on the Indeed dashboard.
*  So Indeed ends up making connecting with and hiring the right talent fast and easy.
*  You can choose from more than 130 skills test or add your own, then add your must
*  have requirements so you only pay for applications that meet them.
*  Get started right now with a free $75 sponsored job credit to upgrade your job
*  post at indeed.com slash mindscape.
*  That's a $75 credit at indeed.com slash mindscape, offer valid through June 30th
*  terms and conditions apply.
*  But I think what we talked about so far is sort of half of Bayesian reasoning,
*  right? Like you have some beliefs, you collect more data and you update them.
*  But then the other half is you have some beliefs, right?
*  The people who don't like Bayesian reasoning always object to the idea that we need to
*  start with a prior.
*  And that seems that seems kind of mystical.
*  Like where do you get these prior opinions from?
*  Do you do you have a spiel on that or do you have a take on how it's necessary to
*  have some prior credences on things?
*  No, not really. I mean, I think the claim that trying to be Bayesian or trying to be
*  mindful of Bayes rule when you're encountering new evidence, the claim is not that
*  that will lead to 100 percent accurate beliefs.
*  It's just that that relative to not using it, it will lead to more accurate beliefs.
*  So everyone we all start out with, you know, prior beliefs and assumptions that just
*  come from whatever they come from, where we grew up and the people we talk to and
*  the whole collection of all the sources we've read and the experiences we've had.
*  And and our the entirety of our life experience has led to us having, you know,
*  a worldview.
*  And then then the question is just given that fact that all of us have these prior
*  beliefs that we can't prove are true and that are probably wrong in lots of ways,
*  given that fact conditional on that, will we be better off if we then try to use
*  Bayes rule in updating on new evidence?
*  Like from that starting point of our subjective and probably flawed beliefs.
*  And I'm claiming that I think we probably will be better off if we update in a
*  roughly Bayesian way than if we don't.
*  But that doesn't mean we will be 100 percent right.
*  It just means we'll be probably be more right than if we weren't using Bayes rule.
*  That sounds like an admirable goal.
*  There are a lot of obstacles to that goal.
*  And you point out some of them.
*  I've seen you put up this slide that you call like the most depressing graph in the
*  world. Oh, yeah, the graph of despair.
*  The graph of despair. Why don't you explain that?
*  So this is a graph made by Dan Kahan, who's a cognitive professor of cognitive
*  psychology at Yale Law School.
*  And it was it was part of a study, a series of studies he did in which he measured.
*  There was a construct that he called, I think he called it science intelligence.
*  But really, it was just kind of a collection of measures of people's scientific
*  knowledge and basic, basic analytical reasoning.
*  Like, can you do very basic logic puzzles?
*  And do you know the answers to basic scientific questions like, you know, which
*  is is the air in our atmosphere mostly oxygen or mostly hydrogen, things like that?
*  And so so he collected this measure and and he also collected measures of people's
*  political affiliation, like liberal or conservative, and their views on the
*  various ideologically charged scientific questions like climate change.
*  You know, do you do you agree that, like, the earth is getting warmer, at least in
*  large part due to human activity?
*  And so what he found was, first of all, the unsurprising part was that political
*  views are correlated with your views on climate change.
*  That's we all know that.
*  Right. But the the surprising part and the part that I made me call this the graph of
*  despair was that if you if you order people to go to the store and you order
*  people on this graph by scientific intelligence at the low end of scientific
*  intelligence, there's very little correlation between political views and
*  climate change. So like low scientific intelligence, conservatives and liberals
*  are both at, I don't know, something like 30 or 40 percent agreement with climate
*  change. But then as you go up the scale, as people increase in scientific
*  intelligence, the the correlation between political views and climate change use
*  increases. And so the the lines of like how conservatives feel about climate
*  change versus how liberals feel, they diverge as you go up the scale of
*  scientific intelligence until in the in the top tier of like the top one percent
*  of scientifically intelligent people.
*  There's just wild disagreement over climate change where Democrats are at almost
*  100 percent agreement and conservatives have fallen from 30 or 40 percent down to
*  about 20 percent agreement with climate change.
*  And so, I mean, it's it's despairing because, you know, there's this view, this
*  kind of idealistic view, I guess, that if if everyone just gets more, more
*  education in science and more training and logic and critical thinking, then we
*  will all, you know, be able to agree on the truth.
*  And actually, this graph shows the opposite.
*  Well, the way that you I don't remember the numbers from the graph, but the way
*  that you reproduce them right there, it sounds like the conservatives did not
*  decrease by that much in their belief in anthropogenic climate change.
*  And whereas the Democrats, the idealistic theory is that they would have is that
*  everyone would have converged.
*  And so even decreasing a little bit is kind of shocking if you expected them to
*  converge in the other direction.
*  So simply being more knowledgeable or more educated doesn't make you more
*  rational in that particular way or not as much as we would like.
*  Yeah, so there's a there's a popular interpretation of this graph that I think
*  is wrong or that I I don't think is supported.
*  The popular interpretation, which I think even Dan Kahan, the creator of this graph,
*  has probably said on occasion, is that being smarter and being more knowledgeable
*  makes you better at finding defenses of the things you want to believe.
*  So he's drawing kind of a causal line in between your scientific intelligence and
*  like reasoning ability on the one hand and your ability to self-deceive along
*  with, you know, in keeping with your political views.
*  On the other hand, I don't necessarily think that causal line exists.
*  To me, the most plausible explanation for the graph of despair is just that people
*  with high levels of scientific intelligence tend to be more politically engaged.
*  You know, they tend to be more educated and they more educated people tend to care
*  more about politics. And the more you care about politics, the more hyper aware you
*  are of what your side is supposed to believe and what the other side is not supposed
*  to believe. And so it's not that you are better able to deceive yourself, it's just
*  that you on this particular topic, you have more of a motivation to get the
*  politically correct answer for your side.
*  Whereas the people with quote unquote low scientific intelligence just aren't really
*  that engaged in, you know, getting the politically correct answer.
*  Yeah, I think I think both of these hypotheses sound actually plausible to me.
*  I'd be interested in collecting more data so you could control for these questions
*  of political engagement and so forth.
*  I do remember Ezra Klein was on my podcast and we talked about political
*  polarization and he made the point that it certainly doesn't go away with
*  increased education. It's sort of the opposite.
*  In fact, it's a broader theme where the pithy statement is nobody knows more
*  about the tensile strength of steel beams than 9-11 truthers, right?
*  Right, right.
*  You can really, really, really be an expert in something and have a completely
*  crackpotty conspiracy theory point of view on it.
*  So I think that sounds more like the position you're pushing against a little
*  bit.
*  I don't know if that's in tension with what I'm saying.
*  Basically, what I think we can conclude from things like the graph of despair is
*  that at the very least, intelligence and knowledge on a particular subject does
*  not protect you from motivated reasoning on that subject.
*  Then the question of whether it actually makes you more inclined to motivated
*  reasoning is kind of a separate question that's trickier to figure out.
*  But I think it's even important just to say the weaker claim that it doesn't
*  protect you because people tend to feel like it does protect them.
*  That, you know, I'm an expert in this topic, therefore my reasoning is going to
*  be really good and objective and my answers are all going to be correct.
*  Well, if it's a topic that you feel kind of ideologically passionate about, then
*  that's probably not true.
*  And in some sense, correct me if I'm wrong here, but this is an example of you
*  changing your mind, right?
*  Like, didn't I read maybe, I think in your book, that at first, when you first
*  got into this set of questions, you were hopeful, like many people are, that better
*  training and logic and probabilistic thinking will make everyone more rational.
*  And you've kind of moved away from that.
*  You have this amazing quote that I want, you know, to get a tattoo of our judgment
*  isn't limited by knowledge nearly as much as it's limited by attitude.
*  Right.
*  Yeah, it's it's that was kind of a hard lesson I learned over the course of several
*  years, just observing people and observing myself and noticing that, you know.
*  Knowledge and and thinking skill are they're like a tool and you can direct that
*  tool to any end, you can direct it to the end of figuring out what's really true if
*  you're motivated to do that, or you can direct it to the end of, you know, coming
*  up with with airtight defenses of whatever position you want to believe if you're
*  motivated to do that.
*  And so just having the tools themselves, I mean, it sounds almost obvious now that
*  I'm saying it in hindsight.
*  And, you know, I guess it's kind of obvious if you look at like an Internet forum
*  where, you know, there's all these people on the forum who are are well armed with a
*  list of cognitive biases and logical fallacies.
*  But all they do with that list is, you know, point out how other people's reasoning is
*  fallacious and you never actually see them question anything that they believe or, you
*  know, turn that turn those tools on their own reasoning.
*  So that's why I guess maybe it's kind of obvious, but I hadn't been fully aware of
*  it. I don't think it's at all obvious, to be honest.
*  I think you should give yourself more credit there.
*  And in fact, the very first episode of the Mindscape podcast was with Carol Tavris,
*  a social psychologist who's done wonderful work on cognitive dissonance and how we
*  rationalize all the mistakes that we make.
*  And, you know, I asked her a little bit after diagnosing all these issues, like, what do
*  we do about them?
*  And she wasn't that programmatic about how to fix all of these problems that were in
*  her. She seemed to think it was part of the human condition and we could sort of valiantly
*  fight against it, but didn't have anything specific really as a as a remedy.
*  But here you are. So now we finally two and a half years later, I think, gotten to what
*  the answers are here.
*  Maybe maybe three years later.
*  Oh, my goodness. Yeah, we're approaching the three year anniversary of when I started
*  doing this podcast.
*  We're not quite there yet, but we're getting there.
*  So and you you you approach this problem by way of a metaphor, a military metaphor.
*  You want to explain what that is?
*  Yes. So I call it soldier mindset.
*  And that's my term for broadly put, the motivation to defend your beliefs or what you
*  want to be true against any evidence or argument that might threaten those beliefs.
*  And the metaphor comes from the fact it comes from the way we talk about reasoning and
*  beliefs and evidence.
*  So it's the language is just it's strikingly militaristic when you actually look at it.
*  So when we talk about beliefs, we talk about supporting our beliefs or buttressing our
*  position, the way you would, you know, like fortify a fortress.
*  And then we talk about, like, poking holes in the other side's case or shooting down
*  an argument as if, you know, we're defending our fort against attack.
*  And then when we talk about changing our minds, the language is almost it's like we're
*  admitting defeat. We've lost, essentially.
*  So just the idea of conceding a point that the word there is to seed, as in seeding
*  territory, like in a battle and then admitting you were wrong or admitting that
*  someone has a point to admit is like to let someone into your, you know, into the
*  gates or into your fortress.
*  So it's all about weakness and defeat.
*  And and so I call it soldier mindset.
*  And it's it's just a term for what we've been talking about, motivated reasoning,
*  also known under other names and other facets as rationalizing or wishful thinking or
*  denial or to some extent, confirmation bias.
*  So so I wrote a book not about the existence of soldier mindset, because other people
*  have covered that admirably already, but about an alternative to soldier mindset.
*  Which you're going to call the your own role.
*  Scout mindset, which is the title of my book, The Scout Mindset.
*  And and and that metaphor is that, you know, unlike the soldier whose role is to attack
*  and to defend, the Scouts role is to go out and explore and see what's really there and
*  and form as accurate a map of the landscape or the situation as possible.
*  And the Scouts, you know, may have preferences like you.
*  You may hope to learn that there is a conveniently placed bridge across the river where you
*  need to cross. But above all, you want to know what's actually there.
*  You don't want to draw a bridge on your map where there actually isn't a bridge in
*  reality. So that is then my term for the motivation to see things as they are and not
*  as you wish they were.
*  I actually really like this metaphor.
*  I mean, well, one thing that immediately came to mind is why do we need a metaphor for
*  just trying to be as rational as possible?
*  Like, couldn't we just put the case forward as cold bloodedly rationally as possible?
*  But maybe that's OK. I'm not going to I'm not going to give you a hard time about that.
*  But the metaphor, even though I'm totally on your side in terms of having the Scout
*  mindset and it's a good thing, it actually suggests reasons why the soldier mindset,
*  the sort of sticking by your guns mindset, might in some circumstances be the right
*  thing. Right. If we take the metaphor too far, I mean, soldiers, if you're in a battle,
*  they serve a purpose. I probably want more soldiers than I want scouts.
*  Right. And part of being a soldier is and it's the it's the downside of being a
*  soldier, but it's the necessary thing is your job is not to think about whether the
*  battle is just or not.
*  Right. Your your job is not to think about the geopolitical strategy.
*  Your job is to be on the ground or in the air or whatever and try to win your momentary
*  little battle. And if everyone was a political theorist and a just war theorist on the
*  battlefield on one side, I don't think that side would have a very good chance of
*  winning. Right. I mean, they would be hesitant.
*  I always think in terms of rather than battlefield metaphors, I think in basketball
*  metaphors usually. So like there are people with scores mindsets.
*  Right. Like when the ball is in their hand, they're looking at the basket and they're
*  going to shoot one way or the other.
*  There are other people who are facilitators and they're not even trying to score.
*  They're trying to set people up to score.
*  And both of those mindsets simplify their jobs in some sense.
*  So is there some self undermining going on in this metaphorical choice?
*  Does it suggest that maybe it's too much work for everyone to have this scout mindset?
*  So a couple of things. First of all, it's definitely not a perfect metaphor.
*  Like I don't know if it would be possible to have a perfect metaphor.
*  I considered other metaphors like I thought about the judge and the lawyer, for example,
*  which some other people have talked about.
*  But the problem with that was that the judge is very passive.
*  The judge is not going out to learn about the world and get new information.
*  They're just considering the information that's put in front of them.
*  So that's kind of not ideal.
*  So yeah, I agree.
*  There's this kind of potentially this connotation that, you know, we in the real world,
*  we actually need soldiers, at least until we've somehow achieved world peace.
*  So that's yeah, you're right about that.
*  And that's maybe an imperfection in the metaphor.
*  But then to the kind of substantive point of, well, don't we need people who are not
*  constantly thinking about things and second guessing things and questioning and but instead just acting?
*  This is maybe a subtle point, but the the metaphor that the terms scout and soldier
*  mindset are supposed to be referring only to how do you decide what to believe?
*  So if you like picture picture an entrepreneur who's they've got this company they've created
*  and there's all these all this uncertainty in starting a company like are we targeting
*  the right market and is our like should our project our product be improved more before
*  we launch it, et cetera.
*  So there's all these uncertainties that you could be constantly wondering about.
*  And I don't think that being a scout means constantly questioning everything you're doing
*  because that would be untenable.
*  Yeah. Basically, you have to like you have to make the best decision you can in like a limited time period.
*  And then you have to act on that decision for some set period of time, either until like a good point
*  when it makes sense to step back and and you know, now it's our quarterly meeting, like what what things
*  should we change or or until some new information comes up where you're like, oh, we've like, huh, our sales
*  numbers this quarter were like surprisingly low.
*  Maybe now is a good point at which we should like step back and revise what we're doing.
*  And so I definitely don't want to define scout mindset as constantly questioning everything.
*  I mean, to define it as like having having a quote unquote map in your head of reality or of how my business
*  like what my business choices are and whether they're justified and have having your map be uncertain in all the
*  places where it should be uncertain.
*  And then, you know, you can like pull that map out when you're making decisions about whether to change what you're
*  doing, whether to pivot, et cetera.
*  But you don't need to be constantly staring at the map and never moving.
*  Does that make sense?
*  No, it does make sense.
*  I just want to keep pushing a little bit because once again, I'm totally mostly on your side here and I'm just trying to sort of
*  push back. It's great.
*  Therefore push back. Right. Exactly.
*  So because one of the things that I find hardest about trying to be rational, especially in a social context, is that
*  we do have finite resources, both of our cognition, our attention, our time, you know, a million different ways.
*  And so there are especially like the one that I'm very familiar with is as a physicist online, there are a lot of crackpot
*  physics things going on out there.
*  And there will be people who just say like, you have a duty to look into this, because if it's true, it's going to be
*  amazingly important.
*  Right. Right. And if you don't, then you're close minded.
*  Yeah, exactly. You know, the M drive.
*  Do you know about the M drive?
*  EM drive?
*  I that sounds familiar, but I couldn't explain it to you.
*  It's a propellantless drive.
*  Basically, it's like that they really believe the the analog of getting in your car and making it move by pushing the
*  steering wheel forward.
*  So like they're just shining light on one side of the spaceship from the interior and making it move.
*  And somehow conservation of momentum is violated.
*  But they say that, you know, in a basement in Texas, someone made it work.
*  So I'm like, no, maybe someone did make it work.
*  There's certainly a chance there's certainly a nonzero credence that something like that works.
*  But I have all this background knowledge called like the laws of physics and things like that.
*  And so this is beyond the pale to me.
*  This is something I'm not going to pay attention to.
*  And I think that's the correct attitude.
*  But figuring out exactly where to draw the line between what's not worth paying attention to and what's worth being
*  open minded about seems to be the tricky part.
*  Yeah, no, that's definitely that is definitely the tricky part.
*  Yeah, I and maybe you already understand or agree with this, but I don't think I don't think there's anything
*  intellectually dishonest or anti-scout mindset in saying, you know, that could be true, but it's like I have limited
*  time and information and that's below the threshold of worth paying attention to or worth investigating now.
*  And to be honest, I think like the people who are urging you, you know, kind of guilt tripping you into trying to pay
*  attention to their fringe claim by exploiting your desire to be intellectually honest.
*  I think that if they were like I don't actually think that they would act that way about any any random fringe claim.
*  Like, I don't think anyone can realistically claim that every single fringe claim is worth everyone's attention.
*  That just doesn't really make sense.
*  So that feels more like an exploit to me, like an attempt to exploit your your desires to be virtuous.
*  Don't worry. It doesn't work. My desires to be virtuous are not that strong.
*  I can easily I can easily ignore it.
*  But but but if I'm trying to be virtuous, it's not that I'm going to start paying attention to them, but it's that I cannot
*  exactly articulate where I should draw the line between crazy ideas worth paying attention to and those that are not.
*  Because I can be very honest and say that some of my ideas are seen as crazy by other people.
*  And so how do I make the sales pitch to them that, well, maybe they're unlikely in your world view, but they're not so crazy.
*  You should ignore them. Like that seems like a very hard meta rationality problem to me.
*  Yeah, it is hard and I don't have like a clear cut answer to it.
*  So sometimes I think in theory, the distinction that we want to be paying attention to, even if we can't always perceive it perfectly,
*  the distinction is between dismissing something because your prior beliefs indicate that it is it is very unlikely to be true versus dismissing something because you don't want to accept it or you like don't want to take it seriously.
*  So one is just like a it's just a cognitive.
*  The judgment call is like purely cognitive and the other hand, it's motivational.
*  And so. So sometimes what I'll do is I will.
*  Sometimes I will pay attention to whether if there were another theory that I liked that was similarly implausible, would I feel motivated to check that out instead?
*  Like I might I don't know. I might like the idea that I'm trying to come up with something implausible, but also desirable.
*  Well, you know, are UFOs really aliens?
*  Yeah, I would like it to know what my desires are about that, actually.
*  I would like it to be true. OK, well, how about the end?
*  Maybe like a clearer, maybe like a better way to express this is just.
*  If if you find yourself saying.
*  That like I don't have like I can't immediately find any flaws in that argument, but I just this like seems too low probability to me to look into, then that's that seems much more intellectually honest to me than what many people do,
*  which is is making making an argument against the evidence that's that's maybe not even very well supported, but they feel like they have to come up with an argument against it to dismiss it.
*  One of the best ways to keep yourself young and your mind sharp is to learn new things, and there's no better way than the Great Courses Plus streaming service with a library of over 13000 audio and video lectures.
*  There's an enormous variety of topics you can get on the Great Courses Plus and with the Great Courses Plus app.
*  It's very convenient. You can watch from your phone, your tablet or even stream to your TV.
*  I've done courses for the Great Courses Plus, for example, the physics of time.
*  You can learn about why there's an arrow of time due to entropy, but also how we tell time, how time works in relativity and space time, a little bit about the neuroscience and the philosophy of time.
*  I had a great time creating and recording these lectures because it's an opportunity to dig deeply into a topic that everybody cares about.
*  So what are you waiting for? Sign up for the Great Courses Plus today to start your 14 day free trial.
*  And for a limited time, listeners of Mindscape can get 20 percent off an annual membership.
*  This is only through the special URL thegreatcoursesplus.com slash mindscape.
*  That's T H E Great Courses plus.com slash Mindscape.
*  Now, that's a very good point. So let me let's try to restate it to see if I understood it.
*  That it's there are times when we're not going to pay attention, give our resources over to seriously contemplating a certain proposition.
*  And the reason why is just because it is so in conflict with our background beliefs that we're going to judge it very, very unlikely.
*  And it's better to offer that as the honest reason why rather than to sort of come up with some barely plausible substantive reason that pretends to meet the argument on its own terms.
*  Yes, that's very well put. I don't think it fully solves your or doesn't fully answer your question, but it's like one of the heuristics that I would use to try to be intellectually honest while still not evaluating every single claim.
*  Maybe another heuristic is just to have to have at least some criteria for whether to investigate or take seriously a claim that are kind of kind of outside view criteria unrelated to whether you personally want to reject it or accept it.
*  So, for example, if if if some smart and reasonable people who I whose judgment I trust in other ways think that this claim is at least promising or at least worth considering, then I think that, you know, if I then still am reluctant to even consider the possibility that it might be true or that it might be worth some people investigating, then that seems a little suspicious to me.
*  Yeah, or maybe that maybe that suggests another criteria. Like if I don't want anyone to investigate it and like if I get angry when anyone takes it seriously, then that kind of gives the lie to my claim that it's just not worth my time.
*  Because I do want like lots of people investigating lots of fringe ideas. And in theory, you should be in favor of that, even if you don't personally have time to look into everything yourself.
*  Well, so yeah, no, but this comes in exactly into where the rubber hits the road kind of question for people in academia, for example, where you it's great to imagine everyone pursuing every crazy idea. But in the real world, you have not just limited resources in terms of your own brain, but you're hiring people and you're giving out grant money and things like that.
*  How do you allocate grant money or jobs to people who are are pursuing ideas you think are probably wrong? Right? I mean, that's a very difficult thing. Because, well, you think it's probably wrong, but there's a chance that it's right. I think it's very hard. It certainly is hard in practice to find people who will say, well, I don't like this idea, but we should hire them anyway, because who knows what I think? Right?
*  Yeah, I mean, these are all very hard questions. And I would, I guess I'd be shocked if there was like an easy clear cut answer to them. Just because it, in practice, it so often depends on, like, the particular logic of the case in front of you where like, sometimes, like if you can, if you can see a very basic, you know, way in which the theory contradicts, you know, extremely well established physics, and the person either doesn't seem to be aware of that fact, or they're not aware of it.
*  their defenses of it seem ridiculous to you, then I think it's pretty reasonable to conclude
*  that there's not some hidden brilliance there that you're missing.
*  In general, I suspect that the bias is towards being too unsympathetic to weird or fringe
*  ideas, just because I think that's human nature. We tend to not like the unpopular maverick
*  who has an idea that tells us we're wrong or that questions the consensus. I think that
*  our bias is towards being less open to that than we otherwise should. Even though I don't
*  want to say everyone should get all the funding and all the attention, I want to err more
*  on the side of being open as a scientific community. I want to err more on the side
*  of letting people investigate things that most people think are wrong.
*  Well, I think you said something there that I think is actually very true and very important.
*  I don't want to let it go by too quickly, which is that one of the techniques we can
*  use when we're judging these ideas that seem to be unlikely, it seems to me, is not just
*  to think about the idea but to think about this person who is promoting the idea, right?
*  And to ask ourselves, when we're listening to them, do they seem like a fanatical demagogue
*  about this? They're so focused on it they can't see the opposite? Or do they start by
*  saying, well, I can see why you would think this is crazy and let me explain to you why
*  it's not. That's always much more persuasive to me.
*  I think so too, yeah. And maybe there's going to be some exceptions, like people who are
*  actually right but just bad at seeing the criticisms. But I think as a general rule,
*  that matches my experience that people who understand and can articulate the reasons
*  why their theory seems implausible or why other people don't already agree with them,
*  if they can understand and articulate those things and still say, I think people are still
*  missing something, that is a good sign to me. Yeah, I would put that on my list of things
*  to take seriously an idea that on the face of it seems wrong.
*  Well, and speaking of which, I mean, you have written a book and I encourage everyone to
*  buy the book. It's very helpful. I think I blurbed it in fact. But I don't want to give
*  away all the book, therefore, here on the podcast, but maybe we can give a flavor for
*  it by giving some very explicit helpful hints or strategies or whatever it is for being
*  more scouting.
*  Yes. Oh, so tips for being a better scout.
*  I think so. I mean, whatever you think is the best way of doing it. I mean, what happens
*  when we put this idea into action?
*  Sure. I mean, there are. So I define scout mindset in this kind of abstract way of, you
*  know, motivation to see what's really there. But we can make that more concrete with some
*  examples. So scout mindset could look like actively seeking out like critical feedback
*  about your beliefs or about how you're how you're doing, like as a boss or as an employee
*  or as a partner and, you know, actually trying to think about the feedback when you get it
*  instead of immediately jumping to your own defense or the defense of your ideas. Scout
*  mindset could look like was we were talking about a minute ago, really trying to understand
*  the views of people who disagree with you and understand them well enough that you could
*  you could explain them and explain them compellingly enough that someone else couldn't
*  tell that you actually don't believe that yourself. So you're not, you know, inadvertently
*  straw manning this idea as you're explaining it.
*  You did have a hilarious example in the book about a liberal writer who said that the
*  conservatives should trust that they completely understood where they were coming from and
*  then did just a terrible, terrible job of giving the conservative point of view.
*  I read it and I admired that this person, this like liberal blogger who will go unnamed,
*  I appreciated that they were at least in theory trying to understand conservative views, but
*  the attempt was just so bad. You know, it was like it was like a cartoon villain caricature
*  of conservatives where they were like, you know, I get I understand you conservatives,
*  you you think that that like the best thing in life is for rich people to become richer.
*  And that's just like, that's what's really valuable. And like, and I understand you think
*  that that poor people are all just terrible people and should die. I'm caricaturing a
*  little bit, but only only a little bit. Right. So that's that's like one facet of Scout
*  Minds that I would say being able to accurately understand and portray the views of people
*  who disagree with you. Kind of a corollary of that is is just being able to point to
*  some critics you have who you think are reasonable, even if you don't agree with them.
*  And so those could be critics in the sense of people who disagree with your political views,
*  your scientific beliefs, but they could also be critics of, you know, your lifestyle choices or
*  your the industry you work in, like if you work in tech or you work in the military. And I think
*  what people typically do and what I certainly am tempted to do is to focus on the unreasonable
*  critics, because everyone has unreasonable critics, like there's always people on the other side who
*  just are terrible. Oh my goodness. Yes. Sorry. Oh my goodness. Yes. Yes. Yes.
*  Yeah. So there's always critics who are either just like completely ignorant or arguing in bad
*  faith. And so, yes, those exist. But if you if you want to be a scout, like you should also be
*  pushing past that to look for the critics who may not be quite so unreasonable. And even if they're
*  wrong, maybe they're wrong in like subtle ways where you can understand how, you know, an
*  intelligent, thoughtful person could hold that view. Let me let me just interrupt. I just thought
*  it was like suspicious. I think we should find it suspicious if you can't find any good critics
*  of your views or your choices. That's exactly I think I like that a lot. And I think that it's
*  I want to relate the last two things you said, because they seem you know, slightly related
*  versions of a similar idea. Like one is the idea that you can accurately model your opponent's
*  point of view. It's like you can run a little virtual machine in your in your head that mimics
*  their point of view. And then the other is that there you can fight a real person who has that
*  point of view that you respect. And I think that both of those are great, all in favor in the spirit
*  of pushing back. I have people who I respect enormously, very, very smart and I disagree with.
*  And so the example that came to mind when I was thinking about this podcast is actually David
*  Chalmers, because I'm finishing up an article about consciousness. And David Chalmers, a former
*  Mindscape guest, is clearly super duper rational, super duper smart, very open minded, very nice guy,
*  like everyone loves David Chalmers. And I completely disagree with him about consciousness.
*  And I don't know what he thinks about me in terms of being rational and smart, but he completely
*  disagrees with me about consciousness. And we've talked about it. And we've tried to be rational.
*  And I don't think that either one of us has changed our minds even a little bit. Is there
*  is there some worry that the fact that there are such rational people who disagree with us
*  points to the existence of these disagreements being something that goes beyond rationality?
*  Is it a mode of communication failure or just such big differences in priors that no evidence can
*  overcome them? Or how should we think about that?
*  So this is another thing on which I've my opinion has sort of evolved over the last 10 years.
*  I think it now seems to me much more likely and much more common that two people, even if they're
*  being perfect scouts, that two people could earnestly try to resolve their disagreement over
*  some factual issue or some at least partly empirical issue and fail to reach agreement
*  on what the correct answer is, even if they're both really genuinely trying and both smart.
*  I think that seemed more implausible to me 10 years ago than it does now.
*  And a lot of the problem is just, as you kind of alluded to, our prior beliefs
*  really do like, even if you're not suffering from motivated reasoning at all, which all of us are
*  to a large extent, but like even in the perfect world where we weren't, your prior beliefs color
*  how you interpret the evidence. And so there are just these tangles of where it's kind of
*  hard to escape this trap where you and I are both, we can't resolve our disagreement because whatever
*  evidence we look at, we can't agree on what direction that evidence should update us because
*  the way we interpret the evidence is colored by our prior beliefs. And so
*  that does happen. And it's philosophically interesting and frustrating.
*  I do still think that trying to strip away the motivation component from it at least improves
*  the situation. And I have made progress on intellectual disagreements that I wouldn't
*  have made if I hadn't been trying to remove or account for the motivational aspect. But
*  I still have to accept that that doesn't mean stripping away the motivational component
*  of reasoning isn't going to mean that everyone is going to be able to agree on everything
*  because of this issue. No, yeah, absolutely not. And I wonder how much of it is just due to the
*  fact that beliefs don't appear in isolation, but they're part of like a network of beliefs.
*  In my book, The Big Picture, I called them planets of belief, where you have a bunch of
*  beliefs that sort of stick together under some mutual gravitational field. And if you think that
*  you're just trying to change someone's mind about one thing, you're actually trying to cause this
*  enormous phase transition in a whole set of beliefs. And it's hard. And maybe it will happen,
*  but maybe it'll take 10 years to achieve. That's so well put. I wish I had thought of that metaphor,
*  the phase transition. But yeah, so, you know, often what we realize, or what I've realized
*  after the fact is that there were people's attempts to change my mind about one node of this
*  graph were doomed because there were other nodes that kind of in the background that I wasn't even
*  aware of that needed to change first or needed to change also in order for that first node to change.
*  So one example that came up as I was doing research for my book was, it was an example of
*  this pastor named Joshua Harris, who he wrote this, he's most famous for writing this book
*  that if you grew up as a Christian teenager in the 90s or early 2000s, you probably had on your book
*  shelves, it's called I Kissed Dating Goodbye. And he wrote it when he was just, I think 21 years old.
*  And it's a book all about how you shouldn't date or certainly not have sex before marriage, but even
*  engage in any kind of romantic relationship before marriage because you have to remain pure
*  for your future spouse. And the book got a lot of, it became famous, but it also got a lot of push
*  back. And a lot of people came forward and said, Joshua Harris, your book completely screwed me up.
*  And now, either like people said, well, I got into a romantic relationship that was, or I got
*  married and it was terrible because I had no past experience to help me make a good choice.
*  Or they said, even now that I'm married, even though I remained a virgin before marriage,
*  now every time I'm intimate with my wife, I feel like I'm doing something terrible because
*  I've adopted this view of sex as dirty and something that corrupts you. And so anyway,
*  all of these stories, people started sharing all of these stories about how they felt this book had
*  been bad for them. And Joshua Harris, who I think is actually an unusually thoughtful and
*  well-meaning person, he heard the stories and he basically just discounted them for years because
*  he just thought, these are exceptions or these are haters, which is true. Every famous book gets
*  haters. So he was able to dismiss them. And then the moment when he first actually took seriously
*  the possibility that his book might have been harming people was after a different experience
*  he had where someone in the congregation where he was a pastor, basically there was a sex abuse
*  scandal and several members of the congregation came out that they had been sexually abused by
*  other members of the congregation. And Joshua Harris was not involved in the abuse, but he had
*  known about it and he had encouraged the victims to settle it internally and not involve the police.
*  And so then he realized after the fact, oh, I was wrong. I should not have made that decision. I
*  should have encouraged them to talk to the police. And he felt terrible about it. And then he had
*  the realization, oh, so you can actually harm people even though you're really well-intentioned
*  and trying to do good. And maybe that sounds obvious, but I think until you really live
*  through it, you don't fully appreciate that that can be true. And then once he had that realization,
*  then he thought, oh, maybe my book actually harmed people because he had all the time when people
*  were complaining to him about the book, he had this kind of background belief that hadn't budged,
*  which is that if you believe you're well-intentioned, then you're not actually
*  causing harm. And now that that node had shifted, he could actually shift the node that was about
*  the book in particular. Does that make sense? Sorry, that was a long tangent.
*  No, it makes perfect sense. I have a favorite metaphor for this also that I actually got from
*  Jennifer, my wife, who did research on changing people's minds for a little while. And so it's
*  the metaphor of the plateau in you're going to laugh, but it comes from barbecue. It comes from
*  cooking pork shoulder, low and slow. So you have your barbecue, your grill or whatever,
*  and you keep it at a very low temperature, 225 degrees, and you put the pork shoulder in there
*  for hours, right? For like seven hours it takes to cook this. And it's all very scientific. You
*  put a thermometer in the pork and you watch the temperature go up and it plateaus. So it'll go up
*  the temperature of the pork linearly. And then at around 150 degrees, it just stops. It just
*  stops getting hotter, even though it's surrounded by 225 degrees ambient temperature, right?
*  So what's going on there? Well, the answer is scientifically what's going on is that
*  it is still changing, but it's giving off steam. So it's sort of evaporating. It's losing its heat,
*  or rather it's exchanging energy with this environment, but by giving off steam,
*  rather than by increasing in temperature. And if you just let it stay there for a while,
*  usually the right thing to do in practice is to wrap it in foil so it stops giving off steam.
*  But eventually if you just let it in there, it would start increasing in temperature again.
*  And so I like this metaphor because you can try to change the minds of a person or a group of people
*  and not be seen, not be apparently getting anywhere. Like nothing is happening,
*  but there is some change going on beneath the surface that will eventually lead to
*  the change that you're looking for, but maybe you don't know.
*  But the point is there's no direct relationship between the internal state of what you're talking
*  into and the external way that they're presenting themselves, right?
*  Right. Or even internally, it probably, the way it often feels to me when someone is trying to
*  change my mind is that my mind's not changing, my mind's not changing, my mind's not changing.
*  And then I guess often there's kind of a state of confusion where the evidence that conflicts with
*  my view has accumulated enough that it's past the threshold of like, okay, well actually maybe I'm
*  confused about what's true here. And then either I stay confused, which is often the outcome,
*  or my confusion resolves into a new and changed view of what's happening. But those are like two
*  phase shifts, I guess, as opposed to a steady linear change in my view.
*  But my hypothesis, which may be completely wrong, is that there are hidden variables
*  in the quantum mechanical language. In other words, there are things going on that are below
*  your own conscious perception that make you closer or further away from that phase transition.
*  Yeah. And do you think those hidden variables are, are they kind of like the nodes we were
*  talking about where there's like unconscious beliefs that are kind of being changed? Or are they?
*  Yeah, I think, you know, I mean, we know that neurons, right, you know, they get a little
*  signal and the signal they get is kind of analog, you know, you get like a couple of beeps here and
*  there, but the output is digital. Like if the number, if the frequency of getting inputs goes
*  above a certain threshold, then the neuron will fire. So I think that what we don't know is how
*  close all of the different nodes in our network are to flipping over and doing that phase transition.
*  Like our external profession is still perfect confidence. And some people really are perfectly
*  confident and other people are one straw away from the camel's back being broken.
*  Right, right. That's, that's very well put.
*  I have no idea if it's true, but that's the theory that I have.
*  Good. One of my favorite chapters, I just want to, you know, hit very quickly. We're approaching
*  the end here, but I do want to get some more of these helpful hints if they want to come
*  to the real world strategies on the table. I think my favorite chapter in the book was about
*  wearing one's identity lightly. Maybe you could say more about that.
*  Yeah. So the last section of the book, the last three chapters are about identity and how
*  beliefs can become part of our identity in the same way that, you know, people are very familiar
*  with how political beliefs are part of your identity for many people or religious beliefs.
*  They're in the sense that, you know, when someone disagrees with them, it feels like a personal
*  attack. It feels like someone is stomping on your country's flag or something like that. And that's,
*  that's like the subjective experience of someone disagreeing with one of those beliefs that are
*  part of your identity. And I think politics and religion are just, you know, the tip of the
*  iceberg. There's so many beliefs that can become part of people's identities. I lived in San
*  Francisco for many years, and so beliefs about like which programming language is best can easily
*  become part of people's identities. And they can argue very passionately and emotionally about that.
*  And so a common piece of advice about this phenomenon comes from Paul Graham, who's a
*  technology investor and an essayist. He's written a lot of great essays. And he has this one popular
*  essay called Keep Your Identity Small, where he says, you know, the more things that become part
*  of our identity is the harder it is for us to think clearly. And so all else equal, you should,
*  you know, let as few things into your identity as possible. And I think I love this advice,
*  and I've, you know, gotten a lot out of it. I just prefer a slight variant on it,
*  which I call holding your identity lightly, instead of keeping your identity small. And the
*  variant, the thing I'm changing here is that I think a lot of people, certainly me, after reading
*  his essay, tried to just avoid identifying as anything. Like, you know, I would say there's a
*  movement that I am a big fan of, and I've worked with called effective altruism, which is about
*  basically trying to apply rationality to the project of doing as much good as possible
*  in the world. And so, you know, by in keeping my identity small, I would be refusing to identify
*  as an effective altruist in case it distorted my ability to think clearly. So if anyone asked,
*  like, Are you an EA? Be like, No, no, no, I'm not an EA. I just am a big fan of them. And I work
*  with them. And I like support their views and agree with them. And, and so basically, it just
*  started, it just started to feel a little bit awkward, like, like, verbally to try to explain,
*  you know, what I mean. And also, you know, I do think, like, if you support a cause or a position,
*  then identifying with it publicly can help bring attention and credibility to it. And it can,
*  you know, you're lending your credibility to the movement by identifying as part of the movement.
*  And so I don't want to, I don't want to encourage everyone to never, you know, join any movement or
*  political party or cause, because then where would we be? And so really, I think the trick,
*  and I don't I don't know how different this is from what Paul Graham actually meant, I'm just kind
*  of disagreeing with the way that his advice is often implemented in practice by people. I think
*  what we need to do is to be able to identify with causes and movements and so on that you value,
*  while still kind of keeping some distance, emotional, ideological distance from those
*  positions. So that entails, for example, always having a separation in your mind between,
*  like, this is what the Democratic Party believes, or like, this is what, you know,
*  liberals believe, or Black Lives Matter, or whatever cause that you're part of. And here's
*  what I believe. And maybe there's a lot of overlap, but they're still two separate things. And,
*  you know, whatever differences there are, I want to notice them. And my support for these positions
*  is contingent in the sense that, you know, I will identify as a feminist, you know, up until the
*  point when I think feminism is causing harm or is wrong. And so just always being aware that that,
*  you know, you're supporting the belief to the extent that you actually think it's true and
*  helpful. You're not, the support isn't the most important thing. Or, I guess I said that wrong,
*  but you shouldn't be supporting it just because it's your tribe. You should be supporting it to
*  whatever extent you think it's right. Yeah, I mean, you can agree, you could in principle agree
*  entirely 100% with the positions of a certain group of people, but agreeing with their positions
*  might not be the same as identifying as a member of the tribe. Yeah, I mean, it's a tricky distinction.
*  Yeah, it's, I mean, it's often internal, like, I don't know, one person I interviewed for the
*  book said, you know, he used to think of him, he used to strongly identify as a feminist.
*  And what that meant in practice was that he would often feel compelled to jump into arguments online
*  where someone was, you know, being wrong and like argue with them to defend feminism. And then he
*  made a conscious attempt to kind of hold that identity more lightly. And so now he says,
*  you know, if someone asks him, are you a feminist, he will probably say yes, because,
*  you know, he agree, he like has a lot of agreement with the group that called themselves feminists.
*  But internally, it feels very different. So if someone criticizes feminism, it no longer feels
*  to him like his tribe is being criticized, like, this group of ideas is being criticized, and maybe
*  I agree, or maybe I don't, but it doesn't feel like a personal attack on me. And if they're right,
*  I want to know if they're right. And, you know, if they're wrong, then I don't necessarily have to
*  like jump in and angrily argue with them. I think this is a tricky issue for me personally,
*  because I do think that, I mean, I know people who go so far as to just reject all isms, right? Like,
*  I don't, I don't like labels. That's part of what I'm talking about. But I get it. But it's just so
*  informationally compact to give me the label on what you want. Exactly. Exactly. That's what I was
*  trying to say. Yeah. So I mean, just at the pragmatic level, I think that labels are always
*  going to be useful in some sense. But then, so I think that's okay. But that's sort of a non
*  emotional response. And there is this much more emotional level where, you know, so Tyler Cowan,
*  sort of our mutual acquaintance, who was also a previous podcast guest,
*  he once blogged about some rules for, I don't know whether it was being more rational or something
*  like that. But what one of his principles is, that as soon as people use the words Democrat or
*  Republican, you know, their IQ dips by 20 points or something like that, right. And I get where
*  that's coming from. And probably as a as a empirical statement, it's quite justified. But it seems like
*  a recipe for a kind of quietism, right? Like, don't, don't get too invested in real world attempts to
*  change things for the better, because that counts as a kind of tribalism. Is there is there some
*  tension there between not ruining our own rationality by identifying too strongly and
*  not committing ourselves to making the world a better place enough?
*  So I think what I'm trying to say is that the your end goal that the thing you you actually care about
*  and are trying to trying to promote in the world is like whatever the thing is you actually care
*  about, like making the world a better place. And so whatever tribes you identify with are hopefully
*  the tribes that you think are going to help with that goal. But you shouldn't you shouldn't be
*  losing sight of what the primary goal actually is, like helping make the world a better place.
*  And so you, you know, you should be on the lookout for ways in which like cheering for the Democrats
*  or supporting whatever the latest Democrat cause is, you should be on the lookout for ways in which
*  those things don't actually help with the goal that supposedly is your main goal, like helping
*  the world be a better place. And so I'm definitely not saying you shouldn't be identifying with a
*  party or a cause. But, you know, there are a lot of ways in which doing the thing that feels really
*  satisfying, if you strongly identify as a Democrat or an effective altruist or whatever cause,
*  doing like the things that feel really good for your identity are not necessarily the things that
*  actually help make the world a better place or further whatever your ultimate goal is.
*  So I have this this the way I think about it is in this graph of with two axes,
*  or one axis is, you know, how strongly does this action
*  like validate your identity? And the other axis is how much impact does your action actually make
*  on the world? And so, you know, some actions feel really good for your identity, but don't help the
*  world like arguing with strangers on the internet, or, or just like, I don't know, putting a bumper
*  sticker on your car or something that's not really all that helpful. Or some things are actively
*  anti helpful or unhelpful, like, like attacking groups that are, are really pretty similar to
*  you, but but have like 10% disagreement with you on what to do. And it can often feel very tempting
*  to attack those groups, right, because they all disagree with you, but not quite. And so that
*  leads to a lot of infighting. And so I think this is an important benefit of holding your
*  identity lightly is that it allows you to stay away from those actions that are that are so
*  tempting for identity purposes, and instead try to find the actions that are actually going to help,
*  you know, make an impact in the world, which might be things like one, one group that I
*  interviewed for the book is a an animal welfare group called the Humane League. And they pivoted
*  from their original focus, which was kind of like splashy demonstrations outside the houses
*  of scientists who are experimenting on animals. That's that was what they were originally focusing
*  on years ago. And then when their new director came in, David Komenheide, he pivoted the
*  organization to focus instead on farm animal treatment. In part because just like running
*  the numbers, there's way more farm animals in the world than lab animals. And so if what you care
*  about is improving the lives of animals, you can have a much bigger impact by focusing on the latter.
*  And the specific strategy that they adopted, which has turned out to be like really valuable,
*  is negotiating with large agricultural companies to treat their animals better. So for example,
*  to agree to stop throwing male chicks into grinders after they're born, which is was like typical
*  practice in agricultural companies, because male chicks can't produce eggs. But there's, you know,
*  billions of chickens who are dying painfully due to this policy. And so anyway, the Humane League has
*  won a bunch of impressive concessions from these large agricultural producers. But it's not that
*  strategy is not very satisfying if what you're thinking is, I want to fight for my tribe of,
*  you know, the animal rights activists against the evil companies who are standing in our way.
*  Because negotiating with evil companies is like exactly the opposite of what is satisfying
*  if what you're motivated by is, you know, fighting for your tribe. And in fact, lots of other animal
*  rights activists, you know, resent them for this for like negotiating with the with the enemy. But
*  in fact, what they're doing is really impactful for their goal. I think that's super important,
*  actually. And maybe I can just say the same thing over again, because I don't think I can say it
*  better than what you just did. But the you know, if what you want is to make the world a better
*  place, you're going to have to work with people you don't agree with or even like, right?
*  Right. So like, or at least resist the temptation to get stuck in unproductive fights that aren't
*  actually worth fighting. Yeah, the purity gatekeeping is one example of not necessarily
*  making the world a better place. But at the same time, so just because I just need to be contrarian
*  here, please, I do think that I try especially and maybe this is just because I'm old and
*  patronizing, but I try with young people who are who are sort of maxly identifying with some tribe,
*  right? Like, yeah, discovered a cause and it could be, you know, one that I agree with or disagree
*  with. But they're really passionate about it. And I have this feeling like I should encourage them
*  to be passionate about their their little tribal identity that they've discovered, because they're
*  sincerely trying to make the world a better place in some way. And they're just not the energy to
*  make the world a better place often comes from these irrational things is what I guess I'm trying
*  to say. And maybe that's not right. But it's George Bernard Shaw, right? No reasonable man,
*  all reasonable men know that they can't change the world. Therefore, all progress comes from
*  unreasonable men. And maybe that's not right. I hate the definition of reasonable. Yeah,
*  I understand. But I do think that there are a lot of people who it goes back to the soldier versus
*  scout thing, right? I think that there's a lot of young activists on the right and the left who are
*  soldiers, right, who are committed to a cause and are passionate about it. And even if I don't think
*  it's the best way to live your life, I think that maybe it's a it's a sensible phase to go through,
*  or at least an excusable phase to go through. Yeah. Well, I mean, again, I think it's important to
*  separate out being extremely passionate and driven to solve some problem in the world from being like
*  extremely passionate and driven to, like defend your tribe and and just like, to really prioritize
*  the former over the latter. And I'm not going to claim that, you know, people who are soldiers and
*  who see the world in black and white and, you know, and aren't like trying to be objective,
*  I'm not going to claim those people never change the world, because clearly many of them have. So
*  it's not like that strategy never works. And, and given that that's kind of the natural state of
*  affairs for humans, I think it makes sense that we see lots of examples of that working. I'm just
*  trying to claim that I think it works better if you could be passionate and driven about impact
*  than be passionate and driven about, you know, defending your ideology or your tribe. I think
*  it would be better. It's hard to prove that because we'd have to like do a study and I don't know how
*  we would even begin to do such a study. But at the very least, I wanted to show some like,
*  at the very least, I wanted to combat the claim that you can't be an effective activist, that you
*  can't change the world while being a scout, because there are some very striking examples of scouts
*  who have changed the world. And I suspect that's a more effective strategy, even though I can't
*  prove that. No, I think it's a perfectly rational claim that you're making whether or not the
*  evidence is for we'll have to gather more evidence. So maybe to end up, let's just ask the meta
*  question, which you explicitly address in your book, is it rational to be rational?
*  Were the word rational is being used in just ever so slightly different senses there? Like is it,
*  if I have some self interest in being happy or fulfilled or whatever, is being perfectly rational
*  about my view of the world the best strategy to get there? This is a great question. And one that
*  is one of the reasons it took me so long to write this book is that I spent a lot of time thinking
*  about potential challenges to the thesis or ways in which like potential downsides to scout mindset.
*  And so to elaborate on the question that you asked, there's these two different senses of the
*  word rational that we haven't really made this distinction so far, even though we've been talking
*  about being rational throughout this whole conversation. But one of the senses is epistemic
*  rationality, which is about, it's basically the sense in which I've been using the word
*  throughout this conversation, like reasoning as accurately as possible, reasoning in a way that
*  makes your beliefs more accurate over time. Then the other sense of the word is instrumental
*  rationality, which is about whether the decisions you make are effectively achieving your goals or
*  not. And so the relationship between epistemic and instrumental rationality is complicated.
*  And many people argue that, in fact, the instrumentally rational thing to do is often
*  not epistemic rationality. Often if you've self-deceived in some way, that will make you
*  happier or that will make you better at achieving your goals of, say, changing the world or of
*  making, like, succeeding at some hard thing, like making your company succeed. And so you need to,
*  you should be trying to be a soldier in order to achieve your goals.
*  I think this is wrong in a number of interesting ways. But the most important point that I want
*  to make is just that in all of the specific cases in which I've seen people say, oh, yeah,
*  you have to be self-deceived, you have to be epistemically irrational in order to fill in
*  the blank, in order to be happy or in order to be motivated to do something hard or in order to
*  change the world. In all of those cases, I have seen other ways you can achieve those goals without
*  having to deceive yourself. And so I think that a large thing that's going on here is just that
*  people are, they're neglecting the possibility of finding other strategies to get the things they
*  want. And they just assume they're kind of like giving up too soon. And they're assuming, well,
*  you got to deceive yourself if you want to be happy or motivated. And so one thing that I
*  spent a lot of time on in the book is pointing out that, oh, you don't like, let's actually take
*  an example from Carol Tavris's book, because you mentioned her earlier in the conversation.
*  And towards the end of that book, mistakes were made, but not by me, which is all about
*  self-justifying behavior. She and her co-author kind of have this fatalistic attitude of, well,
*  you know, we have to self-justify because if we didn't convince ourselves that we had made all
*  the right choices in the past, then we would be consumed by regret and guilt and depression.
*  And I read that and I was like, are those our only two options? Do we really have to choose between
*  convincing ourselves against the evidence that we've made all the right choices in the past
*  versus being depressed and miserable? Like, surely there is a third option here, which is to
*  find a way to be okay with the fact that we've made mistakes in the past and still be happy
*  despite that. And I think this third way exists in basically all of the cases that people have
*  presented to me where they say, you know, you have this choice between being happy and motivated or
*  being rational. I think there's always a way to at the very least make peace with your situation
*  that doesn't require you to tell yourself false things. I think that a place where this comes up
*  in my own experience with scientists is I've known scientists, really super duper good scientists,
*  who will explicitly say that the best strategy for, let's say, a theoretical physicist is to be
*  their own ideas' biggest advocates because, you know, let everyone put forward the best possible
*  view of their own perspectives and the free market will sort everything out. Whereas I tend to think
*  that the best, that the good theoretical physicists should be their own ideas' biggest critics.
*  So they're most, you know, they put forward like, here's all the reasons why this could be wrong,
*  but I believe it anyway, as we were discussing before. And I suspect that the ones who say,
*  well, I should be my own theories' biggest advocates are sort of cheating in the way that
*  you just pinpoint. You're like, they just kind of enjoy being their own theories' biggest advocates.
*  Rather than thinking that's the most rational thing to do.
*  Well, you know, they, it can be a good strategy if your goal is not to like, come up with a
*  true theory and have that theory be accepted. If your goal is just to get a lot of attention or to
*  get, I don't know, prestige or... But I think that's not true. I think that let's give them
*  the credit that they're actually trying to find the truth because I think that is true. These are
*  people who will change their minds. Like just because they're their own theories' biggest
*  advocates today doesn't mean they're at all consistent with what they were saying last year.
*  Right, right. Okay. So really this question is just, should we want, like setting aside one's
*  own selfish, you know, goals for attention or prestige or whatever, should we want other people
*  to be their own biggest, their ideas' own biggest advocates? Is that how we want science to work?
*  And in theory that could work if everyone was, if everyone was sort of, you know, playing
*  the role of an ideal jury and they were hearing the best possible arguments on both sides
*  for the ideas. But in practice, that doesn't actually seem to be the way it works. It seems
*  like very few people are taking the time to delve into the nitty gritty of the arguments for and
*  against the theories. And the, I mean, I'm speaking mostly about social science here because I'm more
*  familiar with that than with physics. So maybe it's a little different than physics, but-
*  It's not.
*  In practice, like ideas get, they get attention and they get publications and they get
*  Ted talks and articles without all that much vetting of whether they're really sound. And so
*  if you have people who are just advocating for their ideas without worrying too much about whether
*  those ideas are true, I don't see that process really weeding out the truth from the false ideas
*  very well. Actually, maybe this is an interesting distinction between social science and physics is
*  that I think in physics, it's maybe clearer more often whether an idea is true or false, that you
*  can do pretty conclusive tests oftentimes. And so if you have someone really pushing hard for an idea
*  and then it fails in experiments, then maybe that's just kind of clear cut and the matter is settled.
*  Whereas in social science, the experiments are so rarely clear cut that you like ideas rise and
*  fall much more based on the advocacy of their proponents than based on the facts from the world.
*  And so the problem-
*  I suspect that the rough life cycle is very, very similar in both cases, but the timescales are
*  much quicker in physics. Because like you say, there can be very definitive evidence. You go,
*  all right, I guess I will move on. Or even a definitive theoretical argument. Whereas in
*  social science, not to mention the humanities, you can cling to your increasingly less likely
*  position for a very long time without it going quite to zero. Right. Yeah, yeah. I guess the
*  timescales, maybe that's what I'm pointing at. Although-
*  But at the end of the day-
*  I don't know. The quality of the evidence that we can get even in the best worlds,
*  or in the best possible cases in like psychology, I don't know. I feel like
*  you can do a bunch of experiments and kind of get whatever answer you want
*  with a little statistical wizardry on either side. So I'm not even sure that like long timescales
*  would help the problem in social sciences. If you have everyone just being a soldier for their
*  ideas, I'm not sure that that process would weed out the true ideas from false, even in long
*  timescales. That's right. So that speaks to the idea that individuals, not just communities,
*  should be tough on their own ideas, which I think is-
*  Ideally, yeah. I think so.
*  I think it's a better way to go. And just to sum it up, you do want to make a case for the emotional
*  rewards of being like this. It's not just an intellectual reward, right? It's a satisfying way
*  to live. I do. Yeah. I mean, I think, I'm not going to deny that it can be comforting to tell yourself
*  false but convenient things, but it is, I and many people I know find it very freeing to
*  be able to follow the evidence where it leads without feeling like you have to believe such
*  and such in order to be happy or in order to feel good about yourself or be motivated.
*  I think it's very liberating to not feel like you have to believe something in order to
*  get what you want. And there's lots of rewards of the joy of curiosity and figuring things out that
*  I think come from Scout mindset, regardless of whether the conclusion is something that you
*  had wanted to be true. There's a thrill of seeing the world more clearly than you had before.
*  And so, as always, I'm trying to be reasonable and nuanced, and I'm not going to claim that
*  Soldier Mindset has no benefits, but I think the benefits of Scout mindset are underappreciated
*  and that if more people appreciated those benefits, then they would find Scout mindset
*  more doable than they think it is. It sounds very good to me, but I promise that if contrary
*  evidence comes along, I will change my mind about that. I would ask for nothing less from you,
*  Sean. Julia Gallo, thanks so much for being on the Mindscape Podcast.
*  It was so much fun. Thanks so much.
