---
Date Generated: January 13, 2025
Transcription Model: whisper medium 20231117
Length: 4159s
Video Keywords: []
Video Views: 667
Video Rating: None
Video Description: Patreon: https://www.patreon.com/seanmcarroll
Blog post with audio player, show notes, and transcript: https://www.preposterousuniverse.com/podcast/2025/01/13/301-tina-eliassi-rad-on-al-networks-and-epistemic-instability/

Big data is ruling, or at least deeply infiltrating, all of modern existence. Unprecedented capacity for collecting and analyzing large amounts of data have given us a new generation of artificial intelligence models, but also everything from medical procedures to recommendation systems that guide our purchases and romantic lives. I talk with computer scientist Tina Elassi-Rad about how we can sift through all this data, make sure it is deployed in ways that align with our values, and how to deal with the political and social dangers associated with systems that are not always guided by the truth.

Tina Eliassi-Rad received her Ph.D. in computer science from the University of Wisconsin-Madison. She is currently Joseph E. Aoun Chair of Computer Sciences and Core Faculty of the Network Science Institute at Northeastern University, External Faculty at the Santa Fe Institute, and External Faculty at the Vermont Complex Systems Center. She is a fellow of the Network Science Society, recipient of the Lagrange Prize, and was named one of the 100 Brilliant Women in AI Ethics.

Mindscape Podcast playlist: https://www.youtube.com/playlist?list=PLrxfgDEc2NxY_fRExpDXr87tzRbPCaA5x
Sean Carroll channel: https://www.youtube.com/c/seancarroll

#podcast #ideas #science #philosophy #culture
---

# Mindscape 301  Tina Eliassi-Rad on Al, Networks, and Epistemic Instability
**Mindscape Podcast:** [January 13, 2025](https://www.youtube.com/watch?v=wBlHFAwsQvM)
*  Hello everyone and welcome to the Mindscape Podcast. I'm your host, Sean Carroll. [[00:00:00](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=0.0s)]
*  There's a kind of history myth that sometimes gets promulgated in, I don't know, elementary schools, [[00:00:03](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3.92s)]
*  maybe, or just folktales we tell each other. According to which, when the first European [[00:00:10](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=10.96s)]
*  explorers landed in the New World, the indigenous folks saw them and thought, oh my goodness, these [[00:00:16](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=16.4s)]
*  are gods coming to visit us and we need to worship them and they're too powerful to deal with. [[00:00:23](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=23.28s)]
*  Turns out nothing like that is actually true. This is a story that the Europeans made up after the [[00:00:29](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=29.76s)]
*  fact to make themselves look good, justify some of the things that happened. Nowadays, we are being [[00:00:35](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=35.84s)]
*  faced with a new set of visitors from another world, namely artificial intelligences, whether [[00:00:42](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=42.08s)]
*  it's large language models or some other kind of constructed program that in many ways can act human, [[00:00:48](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=48.8s)]
*  but has a different set of capacities and we're learning to deal with them. And unlike the myth [[00:00:58](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=58.08s)]
*  of the European explorers landing in the Western Hemisphere, today there are a bunch of people who [[00:01:04](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=64.0s)]
*  quite literally who are very willing to say that these are gods coming to deal with us. I know [[00:01:11](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=71.92s)]
*  there's also plenty of skepticism out there, but there are people who think not only that AIs are [[00:01:18](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=78.56s)]
*  going to be and are human level intelligence and agency, but well beyond that, superhuman, [[00:01:24](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=84.24s)]
*  godlike creatures that we're going to have to deal with. I am myself not of that opinion. I do not [[00:01:30](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=90.72s)]
*  think that that is actually what is going on, but just like the landing explorers, AIs do have [[00:01:36](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=96.24s)]
*  different capacities than we do. They're trained, of course, they're designed, they're made to in [[00:01:44](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=104.0s)]
*  many ways act very human, but they're really not. They're thinking in a different way. They're [[00:01:52](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=112.16s)]
*  capable of some things much better than we are and other things not nearly as good as we are. [[00:01:57](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=117.84s)]
*  So how do we think about this world in which interacting with AIs, interacting with computerized [[00:02:04](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=124.88s)]
*  systems more broadly is going to be a crucially important part of how we live our lives? Today's [[00:02:11](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=131.84s)]
*  guest is Tina Eliassi-Rod, who is a computer scientist whose work spans the space, and this [[00:02:18](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=138.56s)]
*  is why I really like it, from very technical stuff, just how do you better detect certain [[00:02:24](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=144.0s)]
*  nodes or communities in an abstract network that you have embedded in some sort of data, [[00:02:31](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=151.36s)]
*  but then also the human side of how you deal with this stuff, how these computer systems, [[00:02:37](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=157.92000000000002s)]
*  how these AIs are going to affect our lives and we're going to affect them all the way up to human [[00:02:44](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=164.96s)]
*  AI co-evolution. Once we build these systems and then we interact with them and then we use them to [[00:02:51](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=171.52s)]
*  decide how to go shopping or decide how to find a romantic partner, guess what? That affects who we [[00:02:59](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=179.60000000000002s)]
*  are, how we live our lives, and the survival strategies we're going to have to move forward [[00:03:06](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=186.48000000000002s)]
*  in this very brave new world. Again, many positive aspects here. There are things that we don't want [[00:03:11](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=191.36s)]
*  to do, we don't want to bother doing, or it's hard to do for us as human beings that we can outsource [[00:03:18](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=198.08s)]
*  to the AIs. There are other ways in which it's very dangerous. The biases, the bad things that [[00:03:23](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=203.44s)]
*  we have in our own brains can be inherited by the AIs and they can have new failure modes that we [[00:03:30](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=210.08s)]
*  human beings don't have. It's a world that is changing super duper rapidly obviously as a lot [[00:03:36](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=216.08s)]
*  of research is coming in and a lot of influences are out there. It's not all about necessarily [[00:03:42](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=222.16000000000003s)]
*  writing the best program. Some people who are very good at writing programs want to optimize for [[00:03:48](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=228.4s)]
*  making the best money, right? And we have to take that into consideration when we consider what to [[00:03:53](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=233.20000000000002s)]
*  do, how to regulate, how to control, how to optimize for our own actual goals rather than just seeing [[00:03:57](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=237.92000000000002s)]
*  what happens next and living with the consequences. So the more informed we are about what the [[00:04:05](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=245.12s)]
*  possibilities are and how to deal with them, the more we'll be able to do that. So let's go. [[00:04:09](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=249.6s)]
*  Tina Eliassi-Rod, welcome to the Mindscape Podcast. [[00:04:20](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=260.96s)]
*  Thank you. Thank you for having me. [[00:04:34](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=274.32s)]
*  Normally I like to start the conversation with someone talking about the most basic stuff, [[00:04:36](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=276.47999999999996s)]
*  the things everyone knows about. For your stuff, I kind of feel like going in reverse order. We'll [[00:04:43](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=283.44s)]
*  end with the fun stuff about AI and democracy and things like that, but let's start with [[00:04:49](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=289.68s)]
*  understanding graphs and networks and things like that, especially using neural networks to [[00:04:54](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=294.64s)]
*  understand things that human brains can't quite wrap their minds around. So what is the most [[00:05:02](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=302.72s)]
*  general way of stating what it is that you're trying to understand when it comes to thinking [[00:05:08](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=308.64s)]
*  about graphs and networks? Well, when you're trying to understand the phenomena, usually [[00:05:14](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=314.48s)]
*  you have multiple entities, like multiple people, and they have relationships with each other. [[00:05:21](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=321.84000000000003s)]
*  And so when we're looking at graph, like machine learning with graphs or graph mining, [[00:05:26](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=326.56s)]
*  we're trying to find those what we're calling relational dependencies. That the probability [[00:05:32](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=332.24s)]
*  of you and me being friends, given that we both like Apple products, is greater than the probability [[00:05:38](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=338.72s)]
*  of you and me just being friends. Or the probability of me liking Apple products, [[00:05:45](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=345.44000000000005s)]
*  given that we're friends, is more than the prior probability of each of us liking an Apple product. [[00:05:52](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=352.88000000000005s)]
*  So the second one that is, we are friends, you influence me, and so I like Apple products and [[00:06:02](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=362.96s)]
*  I buy Apple products, or I buy this headphone, right, headset. And the first one is that because [[00:06:09](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=369.04s)]
*  we like similar things, we become friends. This notion of homophily or like birds of a feather [[00:06:15](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=375.12s)]
*  flock together. But in a nutshell, like people who work on machine learning on graphs, network [[00:06:20](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=380.71999999999997s)]
*  scientists who are interested in understanding phenomena, network sciences and interdisciplinary [[00:06:28](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=388.72s)]
*  discipline. It is about these relational dependencies and like, what can we find? [[00:06:33](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=393.84000000000003s)]
*  What are the patterns? What are the anomalies in the relationships that get formed? [[00:06:39](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=399.92s)]
*  Dave So for the audience who wasn't there, what Tina is not telling you is that we spent 10 minutes [[00:06:44](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=404.16s)]
*  before the podcast struggling with our Apple products to make the recording work, but we still [[00:06:49](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=409.36s)]
*  use them. So, you know, I guess take whatever lessons from that. Okay, but I guess in the [[00:06:54](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=414.4s)]
*  current era, the issue is you have too much data, or at least in principle, one would like to imagine [[00:07:00](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=420.23999999999995s)]
*  having too much data, there's like so much stuff, right? Is a large part of the worry, like, how to [[00:07:06](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=426.4s)]
*  pick and choose what to pay attention to, what to draw connections between? Tina Yeah, there's some [[00:07:14](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=434.32s)]
*  of that I would say that so I have this thing I call the paradox of big data, which is like, [[00:07:20](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=440.16s)]
*  there's a lot of data, but to predict specifically for what Tina wants, it's difficult, right? You [[00:07:25](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=445.12s)]
*  don't have maybe as much information about Tina. Now, if Tina belongs into some majority group, [[00:07:31](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=451.28000000000003s)]
*  then maybe you can aggregate from the majority and say, well, Tina is part of this flock. And so Tina [[00:07:37](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=457.68s)]
*  will like whatever this flock likes. But really, I feel like the problem these days is more about [[00:07:43](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=463.12s)]
*  exploitation and going with things that are popular than exploration, right? Like in the past, [[00:07:51](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=471.52s)]
*  we would go to the library or the bookstore, you're looking for a book, and you would find [[00:07:58](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=478.56s)]
*  other things. And those were, you know, they basically did the cherry on top of the cake, [[00:08:02](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=482.64s)]
*  right? The cream is like, Oh, yeah, I found this, right? And now we're really not getting that, [[00:08:09](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=489.04s)]
*  right? So when you use all these recommendation systems, whether it's Google or any other [[00:08:13](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=493.52000000000004s)]
*  Amazon, etc. They oftentimes show you what is popular or what they believe you would like. [[00:08:19](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=499.68s)]
*  So in a past life, I worked at Lawrence Livermore National Laboratory, which is a physics [[00:08:26](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=506.40000000000003s)]
*  laboratory. And like when I would do searches there, and this is many years ago, I would get [[00:08:30](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=510.64000000000004s)]
*  more like physics books than like when I lived elsewhere, they would sell me they wouldn't show [[00:08:37](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=517.28s)]
*  me as much physics books, right? Just based on the location, the zip code. And so there's some of [[00:08:43](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=523.4399999999999s)]
*  that that's going on. I feel like that is more of the problem of like not really serving the individual [[00:08:48](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=528.56s)]
*  or exploring as much as possible. So thinking though, like purely like a mathematician or a [[00:08:54](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=534.8s)]
*  computer scientist, faced with these big networks, how should we think about them? What are the tools [[00:09:00](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=540.64s)]
*  that we use to tease out what are the important relationships? Yeah, so, you know, it depends on [[00:09:06](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=546.0s)]
*  what kind of network it is, right? So in social networks, for example, we know that there are two [[00:09:13](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=553.2s)]
*  dominant processes that form social networks. One is closing of what we're calling wedges. So if I [[00:09:18](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=558.16s)]
*  am friends with you, and you're friends with Jennifer, then I will become friends with Jennifer, [[00:09:25](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=565.68s)]
*  right? We close that triangle. And in fact, if you and I have, for example, many common friends, [[00:09:30](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=570.08s)]
*  or let's say me and Jennifer, in my example, we have many common friends, and we are not friends, [[00:09:36](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=576.0s)]
*  then there is something going on that there was lots of opportunities that we could become friends, [[00:09:41](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=581.5200000000001s)]
*  but we chose not to become friends, right? Now, there's also, of course, partial observability, [[00:09:46](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=586.48s)]
*  and that like, maybe I didn't observe it, right? However big your data is, you're not omniscient, [[00:09:52](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=592.4000000000001s)]
*  you don't see things. But we do expect that friend of a friend is also a friend. That's one. The [[00:09:56](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=596.4s)]
*  other one is this notion of preferential attachments, right? That everybody wants to [[00:10:04](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=604.0s)]
*  connect to a star. And so you're interested in, in like, basically, those are the two big patterns. [[00:10:09](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=609.4399999999999s)]
*  And then you look at deviations from that. So a work that was done by John Kleinberg at Cornell, [[00:10:18](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=618.0s)]
*  he's a very well known computer science professor, this is a while back, was think Facebook, [[00:10:25](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=625.84s)]
*  for example, who is your romantic partner on Facebook, and he and his colleagues showed that [[00:10:32](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=632.64s)]
*  basically, you are the center of a flower, and you have petals around you. These petals could be your [[00:10:39](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=639.0400000000001s)]
*  high school buddies or college buddies, etc. They have just more triangles in them. And people who [[00:10:45](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=645.36s)]
*  fall outside of these petals and have a lot of connections to these petals are either your [[00:10:50](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=650.88s)]
*  sibling or your romantic partner. That is, you are introducing them to other facets of your life. [[00:10:56](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=656.0s)]
*  And they showed that when that connections that stopped, establishment of those connections [[00:11:01](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=661.76s)]
*  stopped, it's a leading indicator that you will break up. So you were talking about which connections [[00:11:07](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=667.36s)]
*  to pay attention to, right? So those are some of the things that are fun when you look at social [[00:11:13](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=673.68s)]
*  networks. I mean, biological networks are totally different. So in biological networks, [[00:11:19](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=679.6s)]
*  it's a whole other ball of wax. And there's not like you're not looking for common friends, [[00:11:24](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=684.96s)]
*  you're looking more for like complementarity between different proteins that serve some function. [[00:11:29](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=689.44s)]
*  **Matt Stauffer** So it's interesting because it seems like an attempt to go from [[00:11:34](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=694.72s)]
*  syntax to semantics in some sense, right? You're going from structure to meaning, broadly speaking. [[00:11:39](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=699.52s)]
*  **Sara Hussain** You're trying to understand what is going on. What is the underlying process that [[00:11:44](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=704.72s)]
*  is happening in this network and why these links exist. Now, the one thing that makes studying of [[00:11:49](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=709.2s)]
*  graphs and networks really interesting is that it is not a closed world. So just because like you [[00:11:56](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=716.4000000000001s)]
*  didn't see a link between me and Jennifer doesn't mean that we're not friends. And so for machine [[00:12:03](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=723.6s)]
*  learning where you need both positive examples and both negative examples, which negative examples [[00:12:09](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=729.9200000000001s)]
*  do you pick becomes difficult because the edges or the links or the friendships that don't exist [[00:12:15](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=735.2s)]
*  may because like they don't want to be friends or for other reasons. And so this what are the [[00:12:22](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=742.5600000000001s)]
*  negative examples becomes an important aspect of things. **Matt Stauffer** Well, or as you were [[00:12:27](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=747.84s)]
*  giving the example, I was thinking I don't interact with my romantic partner on social media that [[00:12:32](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=752.4s)]
*  much because we interact in real world. Like we don't need that. **Sara Hussain** Indeed. So there are lots of [[00:12:37](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=757.36s)]
*  assumptions being made, obviously, in terms of like how the network is being observed. And in fact, [[00:12:43](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=763.6s)]
*  this is one of the big differences between computer scientists that study graphs and network [[00:12:50](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=770.32s)]
*  scientists that are typically physicists or social scientists where, for example, they're like, well, [[00:12:56](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=776.96s)]
*  there's a distribution and this graph fell from it versus like the machine learning graph mining [[00:13:03](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=783.6800000000001s)]
*  folks typically don't question where the graph came from. They're like, oh, here's data and they run [[00:13:10](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=790.72s)]
*  with it. Right. And it's just it boggles the mind that like you should think about where this data [[00:13:16](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=796.1600000000001s)]
*  came from, how it was collected, what were maybe the errors in collecting it. And in fact, this [[00:13:21](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=801.6800000000001s)]
*  touches on a sore point for me because what happens is they don't question the data, right? They just [[00:13:29](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=809.12s)]
*  feed it into their machine learning AI models. And then on the other end, they don't measure any [[00:13:35](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=815.84s)]
*  uncertainty. So like if you have something like, let's say a social network that you've observed, [[00:13:41](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=821.28s)]
*  there's all this stuff about like representation learning, right? Where basically I take Tina in [[00:13:48](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=828.24s)]
*  the social network and I represent her as a vector in a Euclidean space, right? Like maybe with [[00:13:53](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=833.44s)]
*  60,000, a vector with 16,000 elements in it. So the cardinality is 16,000. And there's no [[00:14:00](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=840.16s)]
*  uncertainty. They're like, no, Tina falls exactly here. And it just doesn't make sense at all. [[00:14:08](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=848.4799999999999s)]
*  Right. And so then those kinds of models, given that you didn't start with, okay, well, my data [[00:14:13](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=853.28s)]
*  could have some noise in it, some uncertainty in it. And then you don't even capture the uncertainty [[00:14:19](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=859.3599999999999s)]
*  of the model at the end. There are lots of problems that can occur, including, for example, [[00:14:25](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=865.36s)]
*  at the serial attacks or like your model is not just going to be robust. [[00:14:32](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=872.72s)]
*  Well, this sounds just like full employment for enthusiastic graduate students, right? Because how [[00:14:37](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=877.92s)]
*  hard could it be? I mean, it could be hard, but it's very well defined, the problem that you just [[00:14:43](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=883.04s)]
*  set out. I mean, allow for the existence of noise in these descriptions and see how your answers [[00:14:48](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=888.8000000000001s)]
*  change. Yeah, I think in part, one of the reasons that folks, at least in the CS side, the computer [[00:14:54](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=894.08s)]
*  science and the machine learning side, aren't too bothered by it these days is because we are going [[00:15:00](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=900.8000000000001s)]
*  through this era where prediction is everything, prediction accuracy is everything. And so, [[00:15:07](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=907.6s)]
*  there are these benchmarks and it's basically benchmark hacking or state of the art hacking. [[00:15:13](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=913.12s)]
*  And that's basically what is going on. That's the reality of it. [[00:15:19](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=919.76s)]
*  So, there's a lot of that kind of engineering going on as opposed to really thinking about, [[00:15:28](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=928.0s)]
*  what is the phenomena that I'm interested in? How is the data coming to me? What are the sources of [[00:15:31](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=931.92s)]
*  noise? How should I take them into account? Should I even take them into account? And what are the [[00:15:37](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=937.12s)]
*  uncertainties in terms of the predictions that I'm outputting? Let's help the audience understand [[00:15:41](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=941.92s)]
*  the idea of benchmark hacking because that's probably a cool but important one. I mean, [[00:15:48](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=948.4s)]
*  what's a benchmark and how do you hack it? Yeah, so basically, you create a bunch of data [[00:15:52](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=952.56s)]
*  and you get a buy-in from the community that these are good data sets to test a machine learning or [[00:16:00](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=960.64s)]
*  an AI model on. And then, there's a leaderboard and you want to be number one, right? And so, [[00:16:08](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=968.88s)]
*  you hack the systems that exist or you hack your own system, you create your own to be number one [[00:16:17](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=977.0400000000001s)]
*  as much as possible. And that's basically what is going on. And I like this metaphor. So, my [[00:16:24](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=984.72s)]
*  colleague, Barabasi said, it's like there are two camps. There's like a toolbox. It's a finite [[00:16:31](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=991.2s)]
*  toolbox, right? And the machine learning, the AI people, the engineers put tools into that toolbox. [[00:16:38](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=998.0s)]
*  And because it's finite, it's very competitive. That is, my tool beats your tool, even if it's [[00:16:44](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1004.1600000000001s)]
*  like 1% by 1% that it's not clear if it's statistically significant or not. And I may [[00:16:48](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1008.96s)]
*  be keen for only 30 seconds because another tool comes in, right? And then, there's like the [[00:16:54](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1014.0s)]
*  scientists on the other end that just open the toolbox and say, okay, well, what is good for [[00:16:59](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1019.0400000000001s)]
*  whatever, you know, whatever prediction task I want to do? And then, they pick a tool out of that. [[00:17:03](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1023.76s)]
*  And so, a lot of this like benchmark hacking or state-of-the-art hacking happens on the [[00:17:08](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1028.4s)]
*  engineering, on the AI machine learning side, the computer science side, because you want your tool [[00:17:13](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1033.12s)]
*  in that finite toolbox. But on the science side, the physicist or social science side, [[00:17:18](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1038.1599999999999s)]
*  the people who are interested in these models that create the sets of data you have, there's also, [[00:17:23](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1043.52s)]
*  as I understand it, a lot of worry about degeneracy or overdetermination or underdetermination where [[00:17:29](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1049.6s)]
*  very different physical models could give you essentially the same kind of graph or network. [[00:17:38](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1058.32s)]
*  How big of a problem is that? It is a very big problem. I mean, there are multiple angles to [[00:17:44](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1064.08s)]
*  this. So, one is, for example, because of all the hype, oftentimes, people on the engineering side [[00:17:50](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1070.8s)]
*  don't talk about the assumptions that they have made or, you know, the technical limitations of [[00:17:58](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1078.16s)]
*  their system. And in fact, because of that, we have this reproducibility problem. So, not even [[00:18:04](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1084.96s)]
*  a replicability problem, but a reproducibility problem, which is just a code. Can I just reproduce [[00:18:11](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1091.12s)]
*  your code as you have it? Right? And even with your training data, even with like how you broke it up [[00:18:16](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1096.3999999999999s)]
*  with these different like folds or whatever, you know, and so, which is like very, very, a very low [[00:18:24](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1104.3999999999999s)]
*  bar to pass. But that doesn't happen because there are lots of assumptions that are being made, [[00:18:32](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1112.08s)]
*  etc. And then there's this notion of, you know, we are living through this era of like big models, [[00:18:37](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1117.76s)]
*  right? So, I want a model that has many, many, many parameters, you know, even if I don't need [[00:18:46](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1126.0s)]
*  all those many parameters. Or, for example, maybe I do care about interpretability. That is, I want [[00:18:50](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1130.8s)]
*  to know what the model is actually doing. But because, again, for that one or two percentage [[00:18:58](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1138.4s)]
*  point on the prediction side, you let go of it and, you know, you go with the bigger models. [[00:19:05](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1145.3600000000001s)]
*  But yes, it's a big, big problem of, you know, for me, like the lowest bar would be that we require, [[00:19:11](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1151.6000000000001s)]
*  at least with federal funding, you know, and in some of the service that I do for the federal [[00:19:19](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1159.44s)]
*  government, I've been pushing this. I'm not going to be a very popular person, but that if you get [[00:19:23](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1163.76s)]
*  taxpayer dollars in your reports to the government, you have to have a section on assumptions and [[00:19:28](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1168.48s)]
*  technical limitations. Because the problem is the way the peer review culture goes is that if I have [[00:19:34](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1174.48s)]
*  a technical limitation section in my paper, the reviewer will just copy and paste it and say reject. [[00:19:41](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1181.92s)]
*  But the federal government isn't going to do that, right? NSF isn't going to do that. NSF has [[00:19:48](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1188.8799999999999s)]
*  already given you the money and you're doing the ANA report. And so it has to be, come on, [[00:19:52](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1192.48s)]
*  just be honest. Right? Like, I did not test this method on biological networks, and they're very [[00:19:57](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1197.1200000000001s)]
*  different than social networks. So like, caution. Well, this is because what you do for a living [[00:20:02](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1202.96s)]
*  matters a lot to the real world and to money and things like that, unlike the foundations of quantum [[00:20:09](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1209.28s)]
*  mechanics that I do. I don't need to worry about people being overly concerned with the results. [[00:20:13](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1213.52s)]
*  They're all willing to give me a hard time anyway. Okay, so I have this sort of philosophical, [[00:20:18](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1218.8s)]
*  mathematical problem. I don't know. I mean, if I have a graph, a big graph, so some nodes, some [[00:20:26](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1226.08s)]
*  edges that are relationships, and I have a different graph, how is there other measures of [[00:20:31](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1231.1999999999998s)]
*  similarity between them? Like if I add one node to the graph, is it a completely different graph? [[00:20:37](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1237.6799999999998s)]
*  Or is there a metric I could put on there? How much is that even understandable? [[00:20:42](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1242.24s)]
*  Yeah, I love that problem. I've thought about that problem a lot. So the issue there is [[00:20:46](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1246.56s)]
*  similarities in the eye of the beholder, right? And it depends on the task itself. So similarity [[00:20:52](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1252.24s)]
*  is an ill-defined problem. And so you can say, okay, well, I can go with something like an [[00:20:56](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1256.48s)]
*  edit distance. Like, okay, how many new nodes do I have to add to graph number two? And how many new [[00:21:04](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1264.3999999999999s)]
*  edges do I have to add or remove to make it look like the other graph? And then try to solve the [[00:21:12](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1272.0s)]
*  computationally hard problem of isomorphism, in fact, alignment, right? And in many cases, [[00:21:18](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1278.72s)]
*  you don't need alignment. So for example, you can think about two networks, and you have started [[00:21:24](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1284.16s)]
*  a process of information diffusion on it. Like you started a rumor, let's say, right? And you would [[00:21:33](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1293.52s)]
*  just measure like, how similar does this rumor, the same rumor, travel through network one versus [[00:21:39](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1299.2s)]
*  network two. And if like, you know, it travels similarly, let's say, you know, I'm going to [[00:21:46](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1306.0s)]
*  throw some jargons and like the stationary distribution of a random walker that is spreading [[00:21:52](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1312.32s)]
*  this rumor becomes the same at the end, you would say the networks are similar enough, right? And so [[00:21:57](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1317.1200000000001s)]
*  you don't need to have like the sizes exactly be the same. So it could be, for example, you have a [[00:22:02](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1322.64s)]
*  social network of France and a social network of Luxembourg, and you started a rumor in France and [[00:22:06](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1326.8s)]
*  in Luxembourg. And they are processing the same way. And you would say the networks are similar, [[00:22:12](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1332.0s)]
*  even though one is much, much bigger than the other. That makes sense. In fact, because that [[00:22:17](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1337.52s)]
*  because I was going to ask about when you have a big graph, and you somehow coarse grain it, [[00:22:22](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1342.0s)]
*  right? Or, you know, you group subgroups into single nodes, you want to somehow have the feeling [[00:22:26](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1346.8799999999999s)]
*  that it's still representing the same thing, even though you've thrown away a lot of information. [[00:22:32](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1352.88s)]
*  Yeah, yeah. Now, the problem was like grouping nodes. This is a very important problem and has [[00:22:38](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1358.16s)]
*  been studied by lots of people within like graphs. It's called community detection. Basically, you [[00:22:45](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1365.2800000000002s)]
*  want to group similar nodes together and that you can have different functions that you define about [[00:22:51](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1371.3600000000001s)]
*  what similarity there means. It could mean that like, these people just talk to each other more, [[00:22:56](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1376.8000000000002s)]
*  right? So there's more connections between them than what you would expect in a random world, [[00:23:01](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1381.52s)]
*  right? Or just more connections between them than other folks. Now, this kind of community detection, [[00:23:06](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1386.08s)]
*  Aaron Clossett, who's a professor at Colorado, showed that there's a no free lunch theorem there. [[00:23:12](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1392.56s)]
*  And actually, it was Aaron Clossett and others. And I think actually, Aaron was the last author. So [[00:23:19](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1399.28s)]
*  I think the first author is Leto Peele. But you know how it is, you usually just name your friend. [[00:23:24](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1404.32s)]
*  Yeah, I do know. My apologies to the other authors. But they showed it in no free lunch [[00:23:28](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1408.96s)]
*  theorem, which basically means that it is not the case that there's like one particular group of, [[00:23:35](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1415.1200000000001s)]
*  or one particular collection of nodes that you're grouping that would give you the best or the [[00:23:44](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1424.64s)]
*  true communities. You see what I mean? So because when you are doing these grouping of nodes, [[00:23:51](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1431.04s)]
*  you have some objective function that you're trying to maximize. And basically, the idea is [[00:23:55](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1435.92s)]
*  that there is no one peak there. So there's not like one particular community that you can put [[00:23:59](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1439.92s)]
*  Tina on and say, okay, Tina belongs here. That's where she has to sit. And so they become some of [[00:24:05](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1445.1200000000001s)]
*  some of that becomes an issue. But this notion of what does it mean for one network to be similar [[00:24:11](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1451.8400000000001s)]
*  to another network is has its tentacles to community detection to clustering of nodes, [[00:24:19](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1459.44s)]
*  and all of those are L defined. So it really is driven by the task at hand. [[00:24:27](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1467.1200000000001s)]
*  Okay, I mean, I guess I'm spoiled by caring about what probably in your world would be the [[00:24:31](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1471.92s)]
*  simplest possible case because I think about, you know, the emergence of space from some set of [[00:24:38](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1478.4s)]
*  quantum entanglements or something like that. And it sounds all very fancy and highbrow, but [[00:24:44](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1484.72s)]
*  basically something is entangled with something else if it's next to it. And there's this very [[00:24:48](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1488.8799999999999s)]
*  similar spatial or a very simple minded spatial coherence. But of course, in social networks, [[00:24:53](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1493.2s)]
*  I can be connected to people anywhere that makes it a more complicated problem. [[00:24:59](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1499.36s)]
*  Yeah, and that becomes the what we call the small world problem, right, that you or the Kevin Bacon, [[00:25:03](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1503.76s)]
*  or the or the Erdős number, right, like you don't have to go that far out to be connected [[00:25:09](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1509.76s)]
*  to famous people. And so I mean, how good are we these days at detecting real clusters, [[00:25:18](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1518.16s)]
*  communities, figuring out what's going on just from knowing about a graph and the connections [[00:25:25](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1525.68s)]
*  between the nodes? I mean, for downstream tasks that you can like, have some, let's say confusion [[00:25:30](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1530.24s)]
*  matrix where you can draw like true positives, false positives, true negatives, false negatives, [[00:25:40](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1540.24s)]
*  we're actually very good at it. But if it's about like, okay, I found these communities and [[00:25:45](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1545.52s)]
*  these communities make sense. It kind of breaks down into whether they're like hard clustering, [[00:25:51](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1551.44s)]
*  where you put Tina into just one community, or you put Tina into multiple communities. [[00:25:57](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1557.28s)]
*  And then there's a little bit of just like eyeballing it in a way, if you do not have [[00:26:01](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1561.84s)]
*  this downstream tasks that you can say, okay, here are the true positives, here are the false [[00:26:06](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1566.4s)]
*  positives, so on and so forth. But in many cases, it's difficult to place a person in a social [[00:26:10](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1570.0s)]
*  network only in one community, because people are multifaceted. Right. But you started with [[00:26:18](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1578.24s)]
*  an example of being given recommendations by Amazon or whatever. And sometimes it, [[00:26:24](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1584.0s)]
*  the algorithm fails, because it's not picking up our individual idiosyncrasies, it's just giving [[00:26:30](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1590.72s)]
*  us the most popular thing. And is that tie in to the well known problem of polarization or [[00:26:36](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1596.8s)]
*  extramization of network recommendations, like you're, you're, everyone is pushed to some [[00:26:45](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1605.12s)]
*  slightly more extreme set of YouTube videos or Reddit posts or whatever? [[00:26:51](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1611.44s)]
*  I think they're, in part, they just want your attention. And so the objective function is such [[00:26:56](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1616.1599999999999s)]
*  that, you know, they just want to hold your attention. And so they will show you whatever [[00:27:02](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1622.64s)]
*  necessary that will keep your attention. And so if they believe that like my tie to Brandon is [[00:27:06](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1626.8000000000002s)]
*  very strong, that we have a strong relationship, and Brandon found these things interesting, then [[00:27:14](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1634.24s)]
*  they will show it to me as well to just test it to see whether, you know, they can capture my [[00:27:20](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1640.3200000000002s)]
*  attention. And then through that, they can show me more ads, for example. I guess that makes perfect [[00:27:25](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1645.76s)]
*  sense. So like, the point is, if Amazon wants to recommend things to me, it's not maximizing the [[00:27:30](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1650.64s)]
*  chance that I want this, it's maximizing its profit. Exactly. Exactly. And so they kind of go hand in [[00:27:37](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1657.1200000000001s)]
*  hand. And in fact, this touches on this issue that we have written a couple of times about there was [[00:27:44](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1664.72s)]
*  a nature perspective piece a while back, and more recently, an AI journal piece on, in a way like [[00:27:51](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1671.8400000000001s)]
*  human AI co evolution. So if you think about it, when you're using Amazon, when you're using [[00:27:59](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1679.36s)]
*  YouTube, when you're using Google, you're providing data for them, we talked about this, right, and [[00:28:05](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1685.4399999999998s)]
*  they take that data into account. And they make recommendations, those recommendations then affect [[00:28:10](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1690.4799999999998s)]
*  what you do in in the real life. And then you go back and you provide them more training data. And [[00:28:16](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1696.4799999999998s)]
*  so there's this kind of feedback loop that goes on and on. And it's oftentimes not captured in [[00:28:22](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1702.56s)]
*  terms of who's influencing whom most. And one example that I like here is like, think about dating [[00:28:28](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1708.72s)]
*  apps. There was a story recently from Stanford that like most people are meeting on online dating [[00:28:36](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1716.72s)]
*  apps these days, instead of like through college or through their friends, family, etc. Or at the [[00:28:42](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1722.72s)]
*  local bar. Now, those dating apps have recommendation systems, right? And based on those recommendations [[00:28:48](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1728.56s)]
*  systems, perhaps you meet somebody, you partner up and you have babies. And so over time, these [[00:28:56](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1736.4s)]
*  recommendation systems actually have an impact on our gene pool. Oh, okay. Yeah, I've not quite gotten [[00:29:02](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1742.72s)]
*  that far. Right. Yeah, but it's like, as opposed and so and because these recommendations systems [[00:29:08](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1748.4s)]
*  are all about exploitation and not exploration, but maybe you would say like my aunt or my [[00:29:14](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1754.4s)]
*  grandmother or my college were also all based on exploitation and not exploration, right. But there [[00:29:19](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1759.1200000000001s)]
*  is this notion that there are these algorithms that we can't understand what they're doing. And [[00:29:25](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1765.12s)]
*  perhaps 100 years from now, they may influence how our genome is evolving. Well, we are part of the [[00:29:30](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1770.8s)]
*  world and we create the world and it reflects back on us, right? I mean, it reminds me a little bit [[00:29:39](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1779.04s)]
*  of discussions about extended cognition theories, where you count your calculator and your pad of [[00:29:43](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1783.04s)]
*  paper and whatever is part of your brain, because you keep information there, you do calculations, [[00:29:49](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1789.9199999999998s)]
*  etc. And so our our environment and who we are is being increasingly populated by these artificial [[00:29:54](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1794.56s)]
*  algorithms that we put out there. Yeah, I don't know, like how far do we think certain things are [[00:30:03](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1803.9199999999998s)]
*  going and society has to design it like, for example, New York Times had this article a while [[00:30:11](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1811.2s)]
*  back about how this there's a person who's trying to set up a company and online dating company where [[00:30:16](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1816.56s)]
*  on the first or second dates, which are usually not very good, my avatar and your avatar will [[00:30:22](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1822.3200000000002s)]
*  go on the date and then they will report back and only if both avatars are happy, then on the third [[00:30:28](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1828.3200000000002s)]
*  date, we actually go out on the date. And so how much of actually our human behavior are these [[00:30:34](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1834.4s)]
*  things going to take over? So I didn't see this article. What's your actual opinion? Is there any [[00:30:40](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1840.48s)]
*  chance that that would help? I think like, I'm an introvert. So I'm like, and also I'm a computer [[00:30:46](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1846.4s)]
*  scientist. I'm like, Oh, this is great. Let somebody else do the dirty work. And then maybe, you know, [[00:30:51](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1851.92s)]
*  if it's a good day, I'll get out of my cave. And I'll like go and talk to some. But you know, I [[00:30:57](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1857.68s)]
*  for extroverts, they don't like it at all. So my husband was an extrovert. Like, what is what are [[00:31:02](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1862.96s)]
*  you talking about? Am I just the brain in a bath? Like, what's happening? You know, so I think it [[00:31:06](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1866.88s)]
*  depends on where you are in this extrovert introvert. We should also reveal to the audience that Tina has [[00:31:12](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1872.4s)]
*  the good or bad fortune of being married to a philosopher. Indeed, for 30 plus years, it's been [[00:31:17](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1877.68s)]
*  fantastic. So yeah, so the evolution, I mean, I was going to get that later, but it's so good, [[00:31:23](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1883.92s)]
*  we have to talk about it now. Co evolution of humans and AI. And my guess was, when I heard [[00:31:29](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1889.68s)]
*  that phrase, we were thinking more about cultural evolution, right? memes more than genes, but of [[00:31:35](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1895.3600000000001s)]
*  course, they're interconnected with each other. Now that you say it, it's obvious because our [[00:31:41](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1901.6s)]
*  cultural effects of our behavior, our behavior affects how we pass genes onto the next generation. [[00:31:45](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1905.9199999999998s)]
*  So AI is going to be affecting the population genome of human beings. Yeah, and I think in [[00:31:51](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1911.76s)]
*  particular with, for example, generative AI, as it's generating content, whether it's text or video [[00:32:00](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1920.24s)]
*  or images. And there's this notion in the late Dan Dennett, who you had on your podcast, very famous [[00:32:06](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1926.72s)]
*  cognitive scientists called these generative AI models, counterfeit people. He had an Atlantic [[00:32:14](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1934.56s)]
*  article a few years back about it. And so, and also because people treat these generative AI systems, [[00:32:21](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1941.1200000000001s)]
*  these counterfeit people as if they're more objective, somehow, they know more than me, [[00:32:29](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1949.28s)]
*  you know, people tend to give their agency to them. And so and also, these AI systems evolve [[00:32:35](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1955.04s)]
*  faster than us. And so it's not quite clear, not that it's a race, but it's that they're evolving [[00:32:40](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1960.8799999999999s)]
*  a lot quicker, their objective functions are different, like attention, money, etc. than [[00:32:47](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1967.6s)]
*  perhaps the objective function of people, like maybe the good of the society or public good or [[00:32:53](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1973.28s)]
*  something else than just like money or some like GDP or some some some measure like that. [[00:32:58](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1978.8s)]
*  Are we good enough that we could at least imagine some kind of new equilibria that we get into when [[00:33:06](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1986.72s)]
*  we're tightly coupled with our AIs that you know, there is some happier state of being we could at [[00:33:14](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=1994.08s)]
*  least aim for if we're working together well? Or is it is it too much in flux these days to know much [[00:33:21](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2001.28s)]
*  about that? I think these days is too much in flux. But I think for example, there are certain [[00:33:26](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2006.32s)]
*  things that can be done to improve it. Whenever you you or another human being asks me a question, [[00:33:31](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2011.84s)]
*  perhaps I would come back with another question. I'm like, did you mean this, Sean? Or did you mean [[00:33:38](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2018.96s)]
*  that? Right? But for example, with child GPT or these large language models, they never come back [[00:33:42](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2022.8s)]
*  and say like, did you mean this? The reason is that it reduces their utility, right? Me as a human [[00:33:48](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2028.48s)]
*  being, when I ask the question, I want an answer and I want it now. Yeah. Right. Or like, it never [[00:33:54](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2034.24s)]
*  comes back and says, I don't know, or I'm not sure of it. And maybe you would accept that from a [[00:33:58](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2038.8s)]
*  human being, but you don't accept it from a large language model. You're like, oh, you're a tool, [[00:34:04](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2044.16s)]
*  you need to tell me like, I asked you about this, and I want the answer now. And, you know, and so [[00:34:07](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2047.76s)]
*  there's some of that going on. But like the big tech companies could add those features to make it [[00:34:12](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2052.16s)]
*  more equal in terms of this conversation that is going on. But at this point, utility is winning [[00:34:19](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2059.6s)]
*  overall. But utility is tricky. You know, I was talking with chat GPT or whatever the other day, [[00:34:27](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2067.52s)]
*  and I was trying to get it to imagine, and maybe I didn't try too hard. I don't, you know, I didn't [[00:34:33](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2073.6s)]
*  really put too much effort into it. But like, I was trying to imagine a character in a fictional [[00:34:38](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2078.64s)]
*  narrative, who was very insulting, and who would, you know, give out some good insults. And I said, [[00:34:43](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2083.44s)]
*  what are some good insults that it could give out, but it wouldn't tell me. It's like, oh, no, [[00:34:48](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2088.1600000000003s)]
*  you shouldn't give out insults. You should talk to people politely. Like, it's clearly programmed [[00:34:52](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2092.1600000000003s)]
*  not to go down that road. Yes, it is. There are actually other generative AI systems, especially [[00:34:56](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2096.2400000000002s)]
*  for programming, that I've heard where like, it tells you like, okay, if you want to code X, [[00:35:01](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2101.92s)]
*  this is how you code it. And then you code it, and you're like, oh, it didn't work. That was, [[00:35:07](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2107.84s)]
*  you're stupid to the generative AI. Like the human says, you're stupid. And then the human, [[00:35:11](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2111.6000000000004s)]
*  the generative AI says to the human, you're not a good programmer. [[00:35:15](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2115.6s)]
*  So then there's some kind of a, you know, then they get at it. It gets in a loop. But that's only [[00:35:20](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2120.4s)]
*  like for, you know, specific ones, you're absolutely right with with with gpt, it's not going to be [[00:35:24](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2124.4s)]
*  that kind of antagonistic. And I know, this is probably related to the the big worry that a lot [[00:35:30](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2130.24s)]
*  of people have had about bias in AI algorithms. I mean, if you've if you've trained, well, if you [[00:35:35](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2135.68s)]
*  train AI on human discourse, and human beings are biased, then of course, the algorithm is going to [[00:35:43](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2143.12s)]
*  be biased. It's not because the computer is biased, it's because you've trained it on data that is. [[00:35:49](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2149.12s)]
*  And is that is that something that your tools can help us deal with? [[00:35:53](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2153.2799999999997s)]
*  I mean, you can try to find biases. I mean, there's a lot of work in that, like these large [[00:35:58](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2158.0s)]
*  language models are sexist, misogynist, we wrote a report for UNESCO for last year's International [[00:36:03](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2163.2s)]
*  Women's Day about how sexist and misogynist these these large language models are. The problem was [[00:36:10](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2170.48s)]
*  that is whenever like I get somebody asks me that question that, oh, look, humans are biased to the [[00:36:19](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2179.12s)]
*  problem is that I can hold the human accountable, I can sue a human being, who am I going to sue? [[00:36:26](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2186.2400000000002s)]
*  You know, and especially in America, we're very litigious. And so then this gets into accountability. [[00:36:32](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2192.32s)]
*  And if I go there's a lot of work in the government, for example, our government is putting a lot of [[00:36:38](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2198.96s)]
*  our tax dollars into like trustworthy machine learning, trustworthy AI, etc, etc. And to me, [[00:36:43](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2203.68s)]
*  it rings a little hollow because there's no accountability. Like, how can I trust you if [[00:36:51](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2211.76s)]
*  there's no accountability? I feel like they go hand to hand. And so there's some of that going on, [[00:36:55](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2215.84s)]
*  which is like, you know, who am I going to sue? Am I going to open AI? Because it's sexist and [[00:37:01](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2221.04s)]
*  misogynist. Like one of his products is sexist and misogynist. You know, that's not the case right [[00:37:06](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2226.4s)]
*  now. Well, and human beings, I mean, this is an ongoing cultural flashpoint. So I mean, it's there's [[00:37:12](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2232.08s)]
*  a lot of different opinions about it. But human beings might at some point, think of something to [[00:37:19](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2239.44s)]
*  say that we know is inappropriate, and then we're smart enough, or we have enough controls that we [[00:37:24](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2244.8s)]
*  don't say it. Is that a kind of thing that we that it makes sense to try to implement in the [[00:37:30](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2250.88s)]
*  context of a large language model? Perhaps, right? The thing is, at this point, what it gives out is [[00:37:36](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2256.96s)]
*  what's the most probable and what it believes you will like, right? So it's, it's a two function, [[00:37:43](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2263.6s)]
*  it's a two place function, what's probable and what you will like. But yes, you could definitely [[00:37:49](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2269.44s)]
*  do that. And there's this comedian, unfortunately, I forget his name now, but he was saying, [[00:37:53](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2273.6800000000003s)]
*  the secret to a long marriage is to never say what comes to your mind first or second, always [[00:37:57](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2277.84s)]
*  say the third thing that comes to your mind. Right? And this goes back to what you were just saying, [[00:38:03](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2283.2000000000003s)]
*  maybe you should just say this third thing, the third most probable thing. And in fact, [[00:38:09](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2289.1200000000003s)]
*  along those lines, usually the students who use these generative AI tools for like math problems, [[00:38:13](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2293.6800000000003s)]
*  the math homeworks, the first answer is usually wrong. Because a lot of the answers that have [[00:38:20](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2300.8s)]
*  been uploaded into like course hero, etc, etc, they're wrong. Usually it's the second answer. [[00:38:25](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2305.76s)]
*  That's the correct answer. Oh, that's very interesting. Is that actually true? Or is that [[00:38:31](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2311.0400000000004s)]
*  like a feeling that people have? These are just anecdotal, right? Like, I haven't had anybody do [[00:38:34](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2314.5600000000004s)]
*  like a systemic study of this, but that like usually the first answer is not quite there. [[00:38:39](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2319.76s)]
*  Well, it's interesting, because one of the things we discover, you discover we in the in the Royal [[00:38:45](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2325.2000000000003s)]
*  We thinking about these very, very large data sets is a sort of sometimes you can predict even [[00:38:50](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2330.48s)]
*  more than maybe you thought you'd be able to I mean, I want to ask you about this paper that [[00:38:56](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2336.56s)]
*  you wrote about using sequences of life events to predict human lives. That sounds interesting, [[00:39:03](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2343.44s)]
*  but also maybe scary. Yeah, yeah. So in the true like computer science, AI machine learning sense, [[00:39:10](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2350.8s)]
*  we're very good at coming up with names for our system. So we call it life to vex. So we're just [[00:39:18](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2358.88s)]
*  putting your life into a vector space, whether you like it or not. But you're just a vector in [[00:39:24](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2364.48s)]
*  this vector space. Now, basically, the idea is that if you look at these large language models, [[00:39:33](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2373.28s)]
*  right, so they're analyzing sequences. And so as human beings, we also have a life story, [[00:39:39](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2379.28s)]
*  that's a sequence, right. And so I was lucky enough to work with a group of scientists [[00:39:45](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2385.12s)]
*  in Denmark. So if America has a surveillance capitalism, and then mark, they have surveillance [[00:39:52](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2392.7999999999997s)]
*  socialism. So there is there is a department there, Department of Statistics, they call it like [[00:39:59](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2399.04s)]
*  Ministry of Statistics that collects information about people. And so we had information for about [[00:40:06](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2406.4s)]
*  6 million people who have lived in Denmark from 2008 to 2020. And we were like, well, can we [[00:40:12](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2412.8s)]
*  write stories for these people in a way, and then feed it to what is the heart of these large [[00:40:22](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2422.32s)]
*  language models, a transformer model, which is basically just the architecture of a neural [[00:40:28](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2428.6400000000003s)]
*  network that learns association weights for within some context window. And that's what we did. So [[00:40:33](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2433.84s)]
*  but instead of so for example, chat GPT goes online and gobbles up all this bad data that that [[00:40:42](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2442.4s)]
*  or that people have put in all the misogynistic sexist data, we didn't do that. So we had very [[00:40:50](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2450.32s)]
*  good data from this Department of Statistics. And we created our own artificial symbolic language. [[00:40:56](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2456.96s)]
*  And then we fed that artificial symbolic language for these like 6 million people [[00:41:05](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2465.6800000000003s)]
*  into a transformer model. And we then we were able to like predict life events. And so one of them [[00:41:10](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2470.96s)]
*  that caught the media's eye was will somebody between the age of 35 and 65 pass away in the [[00:41:18](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2478.32s)]
*  next four years. And we picked that that age range because that's a harder age range to predict for [[00:41:25](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2485.92s)]
*  like if you're over 65, then it's easier to predict whether you're going to pass away in the next four [[00:41:33](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2493.28s)]
*  years. And if you're younger than 35, it's also easy the other way, right? You're unlikely to [[00:41:38](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2498.0s)]
*  pass away. And so that's one of the things the other prediction test was like, well, you leave [[00:41:43](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2503.52s)]
*  Denmark, you know, so then you can predict for that. But it had this similar technology as these [[00:41:48](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2508.56s)]
*  large language models, which is like you have this one, what they call like predefined where you just [[00:41:55](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2515.04s)]
*  learn based on the data that you have what's likely to happen next, and then you fine tune it [[00:42:02](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2522.48s)]
*  for whatever prediction tasks that you have. What does it mean in artificial symbolic language, [[00:42:08](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2528.8s)]
*  like literally a human language? Or is this like some logical encoding? It's a logical encoding [[00:42:14](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2534.2400000000002s)]
*  because it was because the data that the Department of Statistics has in Denmark is all tables. So it [[00:42:20](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2540.08s)]
*  is not like this kind of sequence. So then you could say, like Tina was born in Copenhagen in [[00:42:27](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2547.2s)]
*  December, blah, blah, blah, right. And we could like generate like a natural language, but that's [[00:42:35](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2555.2s)]
*  difficult. Why would we do that? So then we generated a vocabulary for this artificial [[00:42:39](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2559.8399999999997s)]
*  symbolic language. And then and that was actually a lot of the intellectual property of the work is [[00:42:45](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2565.7599999999998s)]
*  like, okay, well, how do you take these tables, and then create this artificial symbolic language [[00:42:52](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2572.72s)]
*  that then you can give to a transformer model? And and and what's the answer? Are we likely to die [[00:42:57](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2577.9199999999996s)]
*  if we're 38 years old? How do we know? Well, the thing the thing that we found, which was very [[00:43:04](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2584.16s)]
*  interesting, I think, so like the accuracy in terms of the model was about like 78%, etc. And [[00:43:09](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2589.3599999999997s)]
*  I think that's why people were showing a lot of interest in it. Um, but to me, that wasn't really [[00:43:17](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2597.36s)]
*  the takeaway. The takeaway actually was that labor data is a very good indication of whether [[00:43:25](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2605.44s)]
*  somebody in that age range is going to pass away in the next four years or not, [[00:43:31](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2611.6800000000003s)]
*  because health data is very noisy and inconsistent. So even in Denmark, where they have universal [[00:43:35](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2615.76s)]
*  healthcare, it's not like everybody goes to the doctor all the time, and you have good data for [[00:43:42](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2622.64s)]
*  them. And so some of the some of the indicators of like, whether you're going to pass away, [[00:43:47](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2627.6s)]
*  one was whether you're male, we know this, right? Two more crazy things. Oh, yes, I can jump over [[00:43:52](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2632.7999999999997s)]
*  this. No problem. Right. And then the other stuff was basically just what which sector you were [[00:44:00](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2640.72s)]
*  working in, right? So if you're like an electrician, it's a bad thing. It's not a very good thing, [[00:44:09](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2649.2s)]
*  right? As opposed to like an office worker, right? So some of the so the labor data was actually very, [[00:44:15](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2655.2799999999997s)]
*  very helpful, helpful than the health data. How important is it to extract causality from [[00:44:21](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2661.3599999999997s)]
*  these relationships? Like maybe risky or minded people just become electricians? Yeah, maybe. [[00:44:29](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2669.12s)]
*  Yeah, we didn't do any any any kind of causal stuff, right? Like a lot of their work, a lot of [[00:44:35](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2675.68s)]
*  the hype that's happening now in AI and machine learning, they're all on the correlation side, [[00:44:39](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2679.7599999999998s)]
*  not on the causation side. So we didn't look at that at all about what causes what that's very [[00:44:45](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2685.6s)]
*  difficult. And I haven't touched the field of causation in part because I'm married to a [[00:44:50](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2690.24s)]
*  philosopher. So it's like, no, like, I go there. Because every time I try to approach the topic, [[00:44:57](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2697.04s)]
*  I just heard nightmares. And so I haven't gone that way. There are some issues there. Yeah, no, [[00:45:03](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2703.92s)]
*  absolutely. But I guess I mean, it's interesting, is it is it too much to draw a general lesson [[00:45:11](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2711.12s)]
*  that by looking at these large data sets, we might find simpler indications of what we're looking for [[00:45:16](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2716.4s)]
*  than we expected? Like, you know, you might like you might have said, Okay, how many calories is [[00:45:23](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2723.76s)]
*  somebody ingesting is the important thing to look at. But then you look at the data and you learn [[00:45:29](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2729.6800000000003s)]
*  know what is their job. That's what's the important thing to look at. Yeah, I think [[00:45:33](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2733.84s)]
*  that there's some of that I think the best way of using this is perhaps government policy, right? [[00:45:39](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2739.28s)]
*  When government issues a policy, and then like maybe 20 years from that you have, if you have [[00:45:44](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2744.32s)]
*  good data, you could see, okay, what has been some of the correlations that have come about [[00:45:50](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2750.7200000000003s)]
*  based on this policy, and then maybe, you know, the actual social scientists and political scientists [[00:45:57](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2757.36s)]
*  can then draw some causal diagrams from what we find. Because the one thing is, usually like, [[00:46:03](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2763.68s)]
*  from the computer science, AI machine learning, we treat causation and correlation as if binary, [[00:46:12](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2772.7999999999997s)]
*  right? As it's like a coin this way or that way. But that is really not the case, right? It's more [[00:46:18](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2778.8799999999997s)]
*  of a spectrum. And so if you have a model that is producing robust predictions, there is some [[00:46:24](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2784.08s)]
*  underlying causal model, you just don't know it. Yeah. And then maybe that could steer you into the [[00:46:31](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2791.6s)]
*  right direction for that kind of work. But we didn't we didn't look at that for this particular [[00:46:39](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2799.12s)]
*  work. So, yeah, human beings, of course, are examples of complex systems themselves. But [[00:46:44](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2804.96s)]
*  this raises the larger question of human beings will eventually die for whatever reason, [[00:46:52](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2812.4s)]
*  complex systems have their lifespans, right? Or maybe they're infinite, I don't know, but they can [[00:46:57](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2817.6s)]
*  also change dramatically and die. And that's something else you're interested in trying to [[00:47:02](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2822.88s)]
*  tease out in a general way. Yeah, I am very interested in the feedback that we were talking [[00:47:08](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2828.24s)]
*  about. And like, how do we capture that feedback between, for example, when I go and I'm using [[00:47:15](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2835.2799999999997s)]
*  Amazon and Amazon is making me these recommendations, and then I buy things, I tell my friends, [[00:47:22](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2842.08s)]
*  and then all of that data goes back into Amazon. And like, how much does like my contributions or [[00:47:28](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2848.3199999999997s)]
*  my friends contributions amplifying what Amazon is doing? And so there's some of that going on. [[00:47:34](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2854.72s)]
*  And then there's also in terms of like society is a complex system, and the place of these tools in [[00:47:42](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2862.64s)]
*  these systems. So the tools that help us spread misinformation and disinformation make our society [[00:47:49](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2869.04s)]
*  unstable. In that then you're not quite sure what you are reading is true or not. Right. So right now, [[00:47:57](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2877.68s)]
*  with the fires in LA, there's a lot of misinformation and disinformation going on. And [[00:48:07](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2887.2799999999997s)]
*  it's like, who do I believe? And maybe like you believe LA Times, and you believe, you know, [[00:48:13](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2893.36s)]
*  what you read in ca.gov, and so on and so forth, but not what you're seeing on Instagram. And so [[00:48:20](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2900.7200000000003s)]
*  there's this notion of the place of these AI tools within our society, and whether they're making our [[00:48:27](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2907.52s)]
*  society better or worse. And by better or worse here, I mean, stable versus not stable, more chaotic. [[00:48:34](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2914.48s)]
*  And I think we can all agree that we will like to live in societies that are more stable than not. [[00:48:42](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2922.56s)]
*  Right. Yeah. So it so there's some of that that is going on. And I have a new project along those [[00:48:47](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2927.84s)]
*  lines, which actually touches on philosophy, which is called epistemic instability, which is, [[00:48:56](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2936.32s)]
*  what are some stability conditions of what you know? So if you genuinely know that whales are [[00:49:02](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2942.7999999999997s)]
*  mammals, no matter what I show you, perhaps I won't be able to convince you that a whale laid [[00:49:09](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2949.12s)]
*  an egg. You're like, a whale is a mammal and mammals do not lay eggs. Right. And you're very [[00:49:15](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2955.04s)]
*  sure about it. Right. But then you start talking to me and to chat GPT. And maybe if you don't know [[00:49:20](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2960.48s)]
*  something, then you're like, as well as you thought, right, then I then you're malleable, [[00:49:27](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2967.12s)]
*  right? Then I can like change your mind. And then now you have groups of people who are talking [[00:49:32](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2972.7999999999997s)]
*  to these within themselves and with these generative AI tools. And then basically you [[00:49:38](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2978.72s)]
*  go from like individual to groups to this hyper graph notion. And what I'm interested in is when [[00:49:45](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2985.12s)]
*  are phase transitions in this hyper graph in terms of what the society believe, like maybe the society [[00:49:54](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2994.16s)]
*  believe that vaccines are good. Right. And now all of a sudden, the society doesn't believe the [[00:49:58](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=2998.64s)]
*  vaccines are good. Right. And what are the leading indicators of those kinds of phase [[00:50:03](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3003.36s)]
*  transitions in our society as it's being modeled by conversations formally represented as these [[00:50:08](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3008.1600000000003s)]
*  hyper graphs? Yeah. I mean, I guess that's a good example. I hadn't quite thought of the vaccine [[00:50:15](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3015.36s)]
*  thing. The traditional example that I hear for sort of a social phase transition is opinions about gay [[00:50:20](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3020.4s)]
*  marriage, right? Where it was universally against it somewhat rapidly changed to generally four. [[00:50:26](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3026.7200000000003s)]
*  But this is the vaccine stuff is more subtle, right? Because it's not that the whole society [[00:50:32](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3032.7999999999997s)]
*  is going against them, but about half or whatever, right? There's this political polarization and [[00:50:38](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3038.08s)]
*  there's sort of more than one consensus being built up. Is that just my impression or is there [[00:50:42](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3042.96s)]
*  some idea that the modern informational ecosystem lets us have these larger sub communities where [[00:50:49](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3049.12s)]
*  they have their own sets of beliefs different from other communities? Yeah, I think it's the [[00:50:57](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3057.04s)]
*  second one in that like in the past, when you did have people that tend to be on the fringe, [[00:51:01](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3061.76s)]
*  they would people wouldn't hear them. But now, even if you're on the fringe, because of the [[00:51:07](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3067.44s)]
*  information technology that we have, you can connect to other people who are on the fringe. [[00:51:13](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3073.2000000000003s)]
*  And then you believe, oh, no, we're bigger than the fringe. Yeah, we're actually in the middle. [[00:51:17](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3077.6800000000003s)]
*  Right. And then that kind of thing spreads. So, so that is one of the things I'm interested in [[00:51:22](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3082.0s)]
*  regarding gay marriage. One of the things that was interesting is I was talking to a philosopher [[00:51:29](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3089.28s)]
*  who has taught for a very long time at the Ohio State University, and he and he was teaching ethics [[00:51:33](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3093.76s)]
*  and issues related to gay marriage and abortion, etc. And he was saying that with gay marriage, [[00:51:39](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3099.52s)]
*  similar to what you were saying, he saw a shift in terms of opinions for or against gay marriage, [[00:51:44](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3104.32s)]
*  mostly for, but he didn't see any change when it came to abortion. And I think that had to do [[00:51:51](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3111.2000000000003s)]
*  with the vagueness of when is, let's call the thing a baby, right? When is the actual [[00:51:56](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3116.88s)]
*  fetus a baby or whatever, you know? And so, and that vagueness, because like, we could all agree [[00:52:06](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3126.4s)]
*  that maybe like the day before you're about to give birth, obviously, you're not going to do [[00:52:11](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3131.12s)]
*  anything, we all believe it's a baby. But that vagueness is something that doesn't shift the [[00:52:15](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3135.12s)]
*  opinion on, on abortion so much for or yes. And I like that vagueness aspect of it. So there are [[00:52:19](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3139.6800000000003s)]
*  certain things that are vague, and maybe you will never have that kind of phase transition. [[00:52:25](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3145.92s)]
*  And then there are certain things like the vaccine, where like there are people in the [[00:52:31](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3151.2000000000003s)]
*  fringe that our information technology allows them to connect to each other. And so it feels [[00:52:34](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3154.64s)]
*  like a bigger thing. And then maybe there are other aspects of information that really do [[00:52:39](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3159.6s)]
*  make people change their mind, just based on talking to other people. And so they're not as [[00:52:46](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3166.32s)]
*  sure or as stable in their knowledge. So I like the hypothesis that the vagueness of the [[00:52:51](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3171.92s)]
*  proposition makes it harder to have a phase transition. How would we test that hypothesis? [[00:52:58](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3178.32s)]
*  Is that something that we can sort of sift through the data and figure out whether or not that's on [[00:53:03](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3183.36s)]
*  your track? So it's a work in progress right now for us on this. I'm trying to stay away from making [[00:53:09](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3189.12s)]
*  it a psychology or a social science problem, because then you get all these confounding factors. [[00:53:16](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3196.0s)]
*  And that's what I said, it has more tentacles to philosophy. So in terms of what people ought to do [[00:53:22](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3202.0s)]
*  in terms of their knowledge and how sure they are of their knowledge. And so right now, the way that [[00:53:28](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3208.4s)]
*  we're representing the knowledge or like what you know these things as vectors, because I'm [[00:53:33](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3213.92s)]
*  a computer scientist, everything is a vector. It's okay. It's all in your algebra. So basically, [[00:53:38](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3218.72s)]
*  how much does this vector space move in one direction versus another? So as you talk with [[00:53:42](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3222.56s)]
*  others, so you can build these like kind of simulations, right? You can build these [[00:53:49](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3229.68s)]
*  simulations in terms of conversations and see how much the vector space shifts. [[00:53:53](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3233.84s)]
*  So I mean, one thing about complex systems is they can survive a long time, like the human body [[00:54:00](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3240.24s)]
*  you know, fends off attacks pretty well because it's complex enough to catch things. The other [[00:54:06](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3246.3999999999996s)]
*  thing is that they can sort of go into this wild, negative positive feedback loop, I guess, and [[00:54:12](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3252.3999999999996s)]
*  crash, right, like the economy or something like that. So is this something maybe this question is [[00:54:18](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3258.56s)]
*  too vague, but is, are we learning general purpose lessons about complex systems concerning what [[00:54:23](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3263.2s)]
*  features they need to be stable versus what features they need to be delicate? [[00:54:31](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3271.84s)]
*  Yeah, so there's a book by Lademan and Wiesner. And I know that you had James [[00:54:37](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3277.36s)]
*  Lademan on your podcast as well. He's a philosopher at Bristol and Caroline Wiesner is a mathematician [[00:54:45](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3285.44s)]
*  at Potsdam now about what is a complex system. And their book that came out, I think in 2020, [[00:54:51](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3291.52s)]
*  talked about complex systems in terms of features and how there are certain like necessary features [[00:54:59](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3299.68s)]
*  and there are certain like emergent features and then there's some functional features where like, [[00:55:05](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3305.44s)]
*  for example, our human brain is a complex system. And as you were saying, like if it has a shock, [[00:55:09](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3309.84s)]
*  it adapts and it still perhaps can function unless the shock is like catastrophic. And so [[00:55:15](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3315.92s)]
*  what we are not seeing, if we tie this to, for example, the AI models and how they are [[00:55:22](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3322.32s)]
*  operating within the system is we don't know even the role of the AI system, like how much instability [[00:55:28](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3328.7200000000003s)]
*  is it causing in the system, right? How much feedback is it causing in the system? How much [[00:55:35](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3335.76s)]
*  memory does it have, right? Because they're evolving so quickly that it's not quite clear. [[00:55:40](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3340.8s)]
*  So this is like an open area of study of like going through these different features of a [[00:55:46](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3346.48s)]
*  complex system and trying to see, okay, well, how do I measure it for let's say, a chat GPT, [[00:55:53](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3353.2s)]
*  right? In fact, a lot of people say, oh, well, you know, it doesn't have a good memory [[00:55:59](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3359.6s)]
*  based on like what I told it yesterday kind of a thing, right? So memory is one of those features [[00:56:04](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3364.2400000000002s)]
*  that a complex system have. Okay. So I guess, you know, and one of the important applications [[00:56:10](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3370.64s)]
*  here that you have talked about explicitly is democracy, right? Democracy is a complex system. [[00:56:15](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3375.84s)]
*  And democracies do fail sometimes. And I guess one way of putting the worry is that, or at least the [[00:56:20](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3380.4s)]
*  interest is that the introduction of AI as a new feature in some sense, opens the possibility of [[00:56:29](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3389.04s)]
*  a new instability. It could lead to sort of a runaway disaster that destroys democracy, [[00:56:37](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3397.6000000000004s)]
*  not to put it into alarmist terms. Yeah. I think where it comes in, in fact, this is how it links [[00:56:43](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3403.44s)]
*  to my new project on epistemic instability, is that it introduces epistemic instability, right? [[00:56:50](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3410.4s)]
*  Like when my dad was getting his PhD in America back in the sixties, the most trusted man in [[00:56:56](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3416.96s)]
*  America was Walter Cronkite, right? If he said something, you believed him. Now we don't have [[00:57:04](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3424.1600000000003s)]
*  such a thing, right? We don't have a person or an institution where you say, okay, I read it here [[00:57:09](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3429.6s)]
*  and I believe it. And then there's also like, depending on where you are on the left or the [[00:57:15](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3435.52s)]
*  right, you're like, maybe you believe New York Times, you believe Fox News. And so because of [[00:57:20](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3440.64s)]
*  that, I feel like one of the things that we need to do if we value our democracy is teach our kids [[00:57:25](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3445.44s)]
*  critical thinking, right? It's just like, don't believe what you read or what you hear, question [[00:57:31](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3451.92s)]
*  it, right? Does it make sense? Talk to different people and make your own decision and don't give [[00:57:38](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3458.08s)]
*  up your agency. But that's a hard task, right? Thinking is not easy and people don't want to [[00:57:42](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3462.88s)]
*  think in the age of TikTok. Well, is that true? I mean, maybe it is true. I'm certainly willing [[00:57:47](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3467.36s)]
*  to believe that's true. But again, I always worry about comparing eras, right? Because I was a [[00:57:54](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3474.56s)]
*  different person in the seventies and the seventies were also a different time, but I don't know what [[00:58:00](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3480.72s)]
*  things are common between different eras and things are not like, did we really want to think [[00:58:06](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3486.8s)]
*  more back in the 1970s than we did in the TikTok era? I don't know. I think there was less [[00:58:11](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3491.84s)]
*  distraction for sure, right? I then then it is now. I think the dopamine hits that we get [[00:58:16](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3496.88s)]
*  by just scrolling through Instagram, TikTok, etc. is something that has been studied. And, [[00:58:26](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3506.32s)]
*  I'm not a psychologist or a cognitive scientist, but that people it's just like you let your brain [[00:58:33](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3513.92s)]
*  go to motion, you just like spend hours on it, instead of maybe actually sitting quietly and [[00:58:38](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3518.32s)]
*  thinking about a problem. It's boring. Yeah, okay, good. So this is another aspect. So okay, [[00:58:44](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3524.88s)]
*  that's actually nice, despite not really trying to I think that I see a bunch of threads coming [[00:58:52](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3532.08s)]
*  together here like technology broadly, not just AI is giving us new ways to fulfill our own objective [[00:58:57](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3537.28s)]
*  functions. Maybe it's a dopamine hit or whatever, but its objective function might not be ultimately [[00:59:06](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3546.0s)]
*  our flourishing. So there's a absolutely danger mode there. Yeah, in fact, that's such a perfect [[00:59:11](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3551.76s)]
*  thing you said, I always say to my students, what is your objective function? Right? Because we all [[00:59:17](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3557.6000000000004s)]
*  have an objective function and that objective function changes over time. Right? And, and perhaps [[00:59:23](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3563.04s)]
*  if like all of us just think, okay, did my objective function change from yesterday or from last month [[00:59:29](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3569.04s)]
*  or whatever, you know, would be helpful for society. So as a computer scientist, as a machine [[00:59:34](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3574.24s)]
*  learning person, I always think about objective functions. And in fact, I cannot look at a mountain [[00:59:43](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3583.04s)]
*  range now and not think, okay, if you drop me there, will I find the peak or not? The global [[00:59:48](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3588.08s)]
*  peak? Probably not. But you know, like, please drop me at a nice place. You've co-evolved with [[00:59:54](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3594.16s)]
*  your with your network. That makes perfect sense to me. Yeah. So the gradient is with me. Exactly, [[01:00:00](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3600.4s)]
*  exactly right. So, okay, so the you've said many things about this already, but I just want to get [[01:00:05](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3605.6s)]
*  it as clear as possible. The trust, the the community of trust idea that is so central to [[01:00:12](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3612.24s)]
*  a democracy is one of the things that is in danger of being undermined by AI, right? Like you probably [[01:00:18](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3618.7999999999997s)]
*  saw the story about Instagram having its AI accounts, the sassy black lesbian lady who [[01:00:24](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3624.4799999999996s)]
*  was programmed by a bunch of people who were neither black nor lesbian and just pure AI. [[01:00:32](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3632.8799999999997s)]
*  That one was admitted, right? Like they said that was AI. And do you personally worry that people [[01:00:39](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3639.2799999999997s)]
*  are just going to mostly become friends with non-existent human beings in the long term? [[01:00:46](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3646.0s)]
*  I mean, as an introvert, I'm fine. But, you know, I think we see this in society now where like [[01:00:52](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3652.56s)]
*  people aren't as good as interacting with other people, or they're not as not as courteous to other [[01:01:00](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3660.0s)]
*  people. Perhaps as before, I don't know, maybe I'm out of an age now where I'm like, oh yeah, [[01:01:07](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3667.28s)]
*  people are not as courteous as they were before. But, you know, the more you interact with people, [[01:01:11](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3671.76s)]
*  the better you get at them. Unless you interact with them, the worse you get at them. And so if [[01:01:17](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3677.84s)]
*  we don't put a premium on like, oh, look, like Tina can actually pick up the phone and call [[01:01:21](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3681.92s)]
*  somebody and get something done, you know, as opposed to just like sending a zillion emails or [[01:01:27](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3687.76s)]
*  text messages. I think there's a value to that. And I think there is this notion of trust, like [[01:01:32](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3692.8s)]
*  even the most introvert among us, right? There are a few people that we do trust. And so if it comes [[01:01:39](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3699.36s)]
*  to a point where you trust an AI system, that we don't know how it works, and that it's vulnerable [[01:01:46](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3706.1600000000003s)]
*  to attacks, then that is a problem, right? And so if this gets us to this phrase called the red [[01:01:52](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3712.88s)]
*  teaming that we hear all the time now, that, oh, well, don't worry about it. They will red team it. [[01:02:01](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3721.28s)]
*  And so the phrase red teaming came from the Cold War era, right? So the Soviet Union, the red team, [[01:02:06](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3726.0s)]
*  America, the blue team, right? So, and there was a lot of this red team, blue teaming, for example, [[01:02:13](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3733.52s)]
*  for cybersecurity. But this phrase red teaming is not well defined when it comes to these generative [[01:02:17](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3737.76s)]
*  AI systems. And my friend and colleague, Professor Hoda Hedary at Carnegie Mellon, has written [[01:02:25](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3745.68s)]
*  extensively about this, because there's no guarantee, right? So you cannot guarantee [[01:02:33](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3753.52s)]
*  that somebody cannot jailbreak chat GPT. And jailbreaking is basically that chat GPT has put in [[01:02:39](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3759.7599999999998s)]
*  some kind of guardrails, right? Like you shouldn't, it shouldn't tell you how to like rob a bank, [[01:02:47](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3767.52s)]
*  but you can jailbreak that and it will tell you how to rob a bank, right? But there's no guarantees. [[01:02:53](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3773.36s)]
*  It's not like, oh, here's a theorem, the proof QED, go home. You cannot jailbreak this. And so [[01:02:59](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3779.2000000000003s)]
*  if you're getting all of your information from these AI systems that we know can be manipulated, [[01:03:05](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3785.1200000000003s)]
*  and we don't know how they exactly work, then you may not have a shared reality with other citizens. [[01:03:09](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3789.84s)]
*  And that's, I think the worst for democracy. We really do need a shared reality to be able to [[01:03:16](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3796.56s)]
*  withstand our democracy to have to hold it and not lose it. [[01:03:22](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3802.72s)]
*  So how do we get that? What do we do? This sounds very scary, but I'm not quite sure what to do about [[01:03:26](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3806.64s)]
*  it. I guess as a professor to me is education. Yeah, I think actually, you know, educating [[01:03:31](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3811.84s)]
*  the public and I spent a lot of my time educating the general public and, and not just the students [[01:03:38](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3818.64s)]
*  at my university, but general, but but educating the public about how these, these tools work, [[01:03:44](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3824.96s)]
*  what they're good at, what they're not good at, not giving their agency to these tools, [[01:03:51](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3831.6s)]
*  and critical thinking skills. I think that that's the way forward. [[01:03:57](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3837.36s)]
*  But the problem with that, of course, is that the value of getting an education is also susceptible [[01:04:01](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3841.6s)]
*  to this loss of trust. I don't know if you saw the recent people were getting upset because there was [[01:04:06](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3846.88s)]
*  a poll that showed that young men were becoming less and less interested in going to college. [[01:04:12](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3852.88s)]
*  Then someone else pointed out that if you go into the cross tabs, if you look at, you know, [[01:04:19](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3859.6800000000003s)]
*  other questions that were asked, there's actually no relationship between male and female versus [[01:04:23](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3863.6800000000003s)]
*  going to college. It's all about Republican versus Democrat. It's that there are more, [[01:04:28](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3868.8s)]
*  it's a Simpsons paradox kind of thing where most of the young Republicans are male. And those are [[01:04:33](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3873.12s)]
*  the ones who've become very polarized against wanting to go to college. So that's a, that's part [[01:04:39](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3879.36s)]
*  of the problem you've been talking about, right? Like there's a whole new epistemic community out [[01:04:44](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3884.96s)]
*  there that is forming and it seems to be solidifying over time. Yeah. Perhaps we should think about how [[01:04:49](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3889.6800000000003s)]
*  we educate people and maybe they will see the value of, of education, right? And that education [[01:04:58](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3898.7200000000003s)]
*  is about enlightenment. Education is about empowering yourself, right? So education isn't [[01:05:05](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3905.36s)]
*  like a teacher just pouring knowledge into your head. It's, it's about you learning about the [[01:05:10](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3910.4s)]
*  world. And so you could do better in the world. Yeah. You know, like as a teacher, I'm already [[01:05:16](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3916.1600000000003s)]
*  11 and my guitar, right? I just want you to do better. And if you do better, then I will also [[01:05:22](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3922.1600000000003s)]
*  do better. The society will do better and we will all do better. Right. And so I think part of that [[01:05:27](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3927.04s)]
*  is maybe we should rethink about how we sell education. Do you think that AI and associated [[01:05:32](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3932.08s)]
*  technologies can be a force for good in education? Yeah, I think so. I mean, there are certain things [[01:05:38](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3938.08s)]
*  that I have, I have heard. So for example, now there's some privacy aspects to this, but if you [[01:05:44](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3944.7999999999997s)]
*  are a college and you are tracking how students are doing on their homeworks, et cetera, and let's say [[01:05:52](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3952.48s)]
*  Tina took calculus and she didn't do very well on differential equations and now she's taking [[01:05:59](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3959.36s)]
*  machine learning and they're going to talk about differential equations that you could tell Tina, [[01:06:04](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3964.6400000000003s)]
*  oh, you know, maybe you should go brush up on differential equations because they're going to [[01:06:09](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3969.04s)]
*  talk about differential equations. Yeah. Okay. You know, so there's some of that kind of a thing to [[01:06:12](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3972.32s)]
*  like, to, to help you. And then there's also like, basically like personalized tutoring that I think [[01:06:16](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3976.7200000000003s)]
*  AI can be helpful there. Do you yourself use, chat GPT or something equivalent to help figure [[01:06:23](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3983.84s)]
*  things out, to learn about things? I use it for fun. Like, you know, like, like, give me a bio of [[01:06:33](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=3993.2000000000003s)]
*  Sean Carroll in the King James style or whatever, you know, just for, I don't use it. I haven't used [[01:06:41](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4001.6800000000003s)]
*  it for any real, like work stuff or anything. I actually, you don't trust it. I certainly don't [[01:06:47](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4007.76s)]
*  trust it, but sometimes, you know, I did, I did realize that there was a good use case because I [[01:06:58](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4018.0s)]
*  was trying to understand, you know, in, in mathematical things, they will often tell you true [[01:07:02](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4022.48s)]
*  things, but you don't understand what the point of it is. Right. And I was trying to understand type [[01:07:08](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4028.32s)]
*  three, Fondoyman algebras. And so I asked, and I got chat GPT to explain to me not just what the [[01:07:12](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4032.96s)]
*  definition was, but why it was important in this particular case. And that was actually very [[01:07:19](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4039.36s)]
*  helpful. Yeah. Oh, that's great. Yeah. I asked it some stuff about linear algebra and matrix norms, [[01:07:23](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4043.6s)]
*  and it was really bad at it. And I was like, wait, what? Like there's so much about linear algebra. [[01:07:30](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4050.0s)]
*  And that's, I think that's, that's the problem. There's too much. Like you just said, there's too [[01:07:35](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4055.2000000000003s)]
*  much junk out there when you, in some sense, if you get technical enough that it knows about it, [[01:07:40](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4060.08s)]
*  but not so technical, all the good, all the stuff that's been written about it is sensible. Like [[01:07:45](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4065.2s)]
*  no one's going to make up stuff about type three, Fondoyman algebras. What would be the point? [[01:07:50](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4070.0s)]
*  Exactly. Yeah. Yeah. So I guess maybe the point is let's not teach linear algebra to kids. [[01:07:53](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4073.36s)]
*  Cause the whole of machine learning is basically linear algebra and like quantum mechanics also. [[01:08:00](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4080.56s)]
*  So yeah, linear algebra kids, that's, that's your lesson for today from Mindscape. Learn [[01:08:05](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4085.2799999999997s)]
*  more linear algebra. I think it's the key to everything. Yeah, exactly. Exactly. But it's [[01:08:09](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4089.6s)]
*  very good at like, um, basically admin stuff. So if you like, um, show it a, uh, some like a picture [[01:08:14](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4094.56s)]
*  of like Google scholar, like, uh, put it into BibTech, put these, these references into BibTech [[01:08:22](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4102.16s)]
*  that it does it for you. So some of those kinds of admin stuff it's good at. Yeah. I think that [[01:08:27](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4107.76s)]
*  the weird thing is we're trying to use it for creative work, whereas the most obvious use [[01:08:32](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4112.4s)]
*  case is for the least creative things that we don't want to do. Indeed. Indeed. All right. [[01:08:37](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4117.52s)]
*  It's all very complex and it's evolving and it's, it's, uh, there's a lot of degrees of freedom. So [[01:08:44](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4124.0s)]
*  Tina Eliassi-Rod, thanks very much for helping us all figure it out. Thank you. Thank you for having [[01:08:48](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4128.88s)]
*  me on Sean. [[01:08:53](https://www.youtube.com/watch?v=wBlHFAwsQvM&t=4133.6s)]
