---
Date Generated: June 10, 2024
Transcription Model: whisper medium 20231117
Length: 6003s
Video Keywords: ['existential', 'risk', 'poshumanity', 'artificial', 'intelligence', 'alien', 'life', 'extraterrestrial']
Video Views: 24770
Video Rating: None
Video Description: Blog post with audio player, show notes, and transcript: https://www.preposterousuniverse.com/podcast/2020/03/02/86-martin-rees-on-threats-to-humanity-prospects-for-posthumanity-and-life-in-the-universe/

Patreon: https://www.patreon.com/seanmcarroll

Anyone who has read histories of the Cold War, including the Cuban Missile Crisis and the 1983 nuclear false alarm, must be struck by how incredibly close humanity has come to wreaking incredible destruction on itself. Nuclear war was the first technology humans created that was truly capable of causing such harm, but the list of potential threats is growing, from artificial pandemics to runaway super-powerful artificial intelligence. In response, todayâ€™s guest Martin Rees and others founded the Cambridge Centre for the Study of Existential Risk. We talk about what the major risks are, and how we can best reason about very tiny probabilities multiplied by truly awful consequences. In the second part of the episode we start talking about what humanity might become, as well as the prospect of life elsewhere in the universe, and that was so much fun that we just kept going.

Lord Martin Rees, Baron of Ludlow, received his Ph.D. in physics from University of Cambridge. He is currently Emeritus Professor of Cosmology and Astrophysics at the University of Cambridge, as well as Astronomer Royal of the United Kingdom. He was formerly Master of Trinity College and President of the Royal Society. Among his many awards are the Heineman Prize for Astrophysics, the Gruber Prize in Cosmology, the Crafoord Prize, the Michael Faraday Prize, the Templeton Prize, the Isaac Newton Medal, the Dirac Medal, and the British Order of Merit. He is a co-founder of the Centre for the Study of Existential Risk.
---

# Mindscape 86 |Martin Rees on Threats to Humanity, Prospects for Posthumanity, & Life in the Universe
**Mindscape Podcast:** [March 02, 2020](https://www.youtube.com/watch?v=EPpnPoLDB_Y)
*  Hello everyone, welcome to the Mindscape Podcast. I'm your host Sean Carroll and today we're
*  going to have a thought-provoking, if perhaps slightly depressing episode, or at least slightly
*  putting us in the mode of worrying about really profound things. That is the concept of existential
*  risks, or even lesser than that, catastrophic or extreme risks that we face as a species.
*  We all know that something happened in the 20th century. We gained the technological ability to
*  really do enormous harm to ourselves as a species. There was always times back in history when human
*  beings could harm each other, but these days we can imagine human beings truly wreaking havoc on
*  the whole planet or the whole species. That's what we mean by extreme or catastrophic or
*  existential risks. Today's guest is Martin Rees, that's Lord Rees, Baron of Ludlow. He is
*  officially a Lord in the British hierarchy. There he actually sits in the House of Lords and votes
*  and so forth. But Martin is also one of the leading theoretical astrophysicists of our age.
*  He's done enormously good work in high-energy astrophysics, understanding black holes and
*  galaxies and things like that. But over the last decade or two, he's gained an interest in these
*  big questions of human life and where humanity is going toward the future. So he's one of the
*  co-founders of the Center for the Study of Existential Risks at Cambridge University.
*  And that's mostly what we talk about today. There's a lot of risks out there. Of course,
*  the first to really come on the scene was nuclear war, and that's still out there. We talk about that
*  one. But now there's all sorts of bio threats as well as cyber threats as well as artificial
*  intelligence, not to mention the sort of natural disasters of asteroids and solar flares and
*  earthquakes that could cause enormous harm. It's a really interesting science problem,
*  but also a philosophical problem because you're asking how to judge an extremely unlikely event
*  versus if that event happens, it would be catastrophic for everybody. So the classic example
*  of this is what if the Large Hadron Collider created a black hole and destroyed all the earth?
*  Unlikely to happen, but do you really want to risk it? This is a really, really good question. So we
*  get into this, we get into how to deal with all these questions, but then being who we are talking
*  about life on earth eventually turns into life on other planets. And in actually in the last half of
*  the discussion, we're talking about life elsewhere in the universe. Again, something that Martin is a
*  world expert in. And we talk about why we haven't seen it yet, what forms it could take, what we can
*  learn about life on earth by thinking about life on other planets. So we start off with some,
*  you know, down to earth kind of depressing topics, but it's very optimistic and action
*  packed there by the end. So tune in, you're gonna like this one. Let's go.
*  Okay, Martin Rees, welcome to the Mindscape Podcast.
*  Good to be with you.
*  So you've done a lot of work, obviously, on cosmology, high energy, astrophysics,
*  and so forth. But a relatively recent interest of yours has been the future of humanity,
*  the risks that we face. And so I thought we could start just with a worked example,
*  and we can think about how to think about this. So as someone who's done particle physics and
*  quantum field theory, my favorite worked example is what if the Large Hadron Collider created
*  either a black hole or some exotic particle that then ate up the earth? And I was surprised to
*  learn by reading your books that you had actually thought about this, and you were one of the first
*  people to really wonder about this.
*  Well, I got the idea from someone else, but I thought it was a good worked example,
*  because obviously it's very, very unlikely. But the consequence is so catastrophic that I think
*  it's not stupid to query this. And there were some people who, of course, did query this,
*  and they were sort of pooh-poohed a bit by the people at CERN or Brookhead.
*  But although we think it's very, very unlikely, I think it does raise a non-trivial issue,
*  because there was a committee set up at CERN to address this. And they said with great confidence
*  that on the basis of all theories, this wouldn't happen. But obviously, the level of confidence
*  you want is something like a billion or a trillion to one, given the consequences.
*  And the reason I think it's not trivial to worry is that supposing you were up before
*  an American congressional committee to address this issue, and you said, well, the chances are
*  a billion to one, and then they say to you, Dr. Carroll, are you really saying that there's
*  less than a one in a billion chance that you're wrong? Could you say yes to that?
*  Yes.
*  And so obviously, the main risk is that our theory is completely wrong. There's something
*  entirely unenvisioned.
*  A systematic error.
*  Which could happen when you explore a part of parameter space that's never been explored by nature itself.
*  I mean, maybe for the benefit of the audience, could you explain a little bit about what it
*  would be that could go wrong at the LHC that would cause us all to be killed?
*  Well, I think here I'm just quoting the other experts. One idea is a mini black hole.
*  The other idea is turning material into what's called strangelets, which would gradually sort
*  of gobble up the earth and turn it into some compact structure. And these have been speculated
*  about by particle physicists. No one thinks they're likely. And I'm a bit inhibited talking about this
*  because when I have talked about it in the past, there have been newspaper headlines saying that
*  Astronomer thinks that the accelerator will destroy the world. And I don't think that.
*  But I do think that it's something which is quite right that one should address.
*  And I try to think what I would do if I was before the Congressional Committee.
*  I think I would say that the chance is very, very small. But I would then say,
*  would you like to ask me what's the chance that something we discover with this accelerator
*  will solve the world's energy problems forever? And I would say that's very, very small.
*  But supposing that you thought that was a thousand times less small than the chance of
*  destroying everything, then that might tilt the balance.
*  That's right.
*  That's fair.
*  So that's the way I would explain why I actually, despite uncertainty about my theory,
*  feel we shouldn't be inhibited in going ahead with these experiments.
*  Well, OK. That's a very good point. What if that were not true? So I think that this is a good
*  thing because I want to get on the table, what is the theory of thinking about these terrible,
*  terrible things where the chance is very small, the consequences are disastrous.
*  So you're suggesting that one of the things to think about is other really, really small
*  probabilities that might be good rather than bad.
*  That's right. And of course, we're never possible to quantify. But of course, there's a risk in
*  doing anything which may have potential upsides. And so we're used to that in the testing of new
*  drugs and everything like that. And so when we get to these colossal risks, then again,
*  the same argument applies that we might be forgoing benefits, as Friedman-Eisen put it in
*  one of his articles, as a hidden cost of saying no.
*  That's right. The example I used in my book, The Particle of the End of the Universe, was
*  every time you open a jar of pasta sauce, there's a chance that a terrible mutation has happened
*  inside and you're going to release a pathogen that destroys millions of people. It's a very,
*  very small chance, but it never stops us from opening the jar.
*  No, no, no. But of course, one does have to be cautious. And if you think of something which
*  could be globally catastrophic, then you get into philosophical questions about just how much worse
*  an existential catastrophe is than a very bad one. And to express this, if you consider two
*  scenarios, one is the death of 90% of the people in the world, scenario two is the death of 100%
*  of the people in the world. And you ask the question, how much worse is scenario two than
*  scenario one? You could say, well, 10% worse, the body counts 10% higher. But some people would say,
*  it's much, much, much worse, because there you destroy the potentiality of all future developments
*  and future lives. And I think it's fortunate that if we think of all these scenarios that have been
*  discussed, apart from these rather crazy ones from particle physics, it's very hard to imagine
*  anything to wipe out all human beings. That's right. But so you were very careful in your
*  formulation there saying some people would say that 100% is much, much worse than 90%.
*  What is your feeling personally? Well, I do think that's the case, because one thing which
*  we learn as astronomers is that we humans are not the culmination of evolution. It's taken four
*  billion years for us to emerge from the primordial slime, as it were. But the time ahead,
*  even for the Earth, is six billion years. And the universe has a much longer future. And
*  I like to quote Woody Allen as eternity is very long, especially towards the end.
*  So there's no reason to think that we're even a halfway stage in the emergence of complexity.
*  And of course, if intelligence is rare, and as some people think, we are the only intelligent
*  species in the galaxy, then our fate as humans is of galactic importance, not just terrestrial
*  importance. Because if we wipe ourselves out, that would destroy these potentialities. So I think
*  it is much worse to envisage wiping out everyone than having a catastrophe which is a setback to
*  civilization. Yeah, there are definitely implications of these ideas for questions about
*  life elsewhere. I do want to talk about that. But let's stay down here on Earth for just a minute.
*  So I think what you're getting at, and something that I thought of as I was reading your books,
*  is that we talk about existential risks or these terrible disasters, but there really is a hierarchy
*  of them, right? There's one in which a million people die, which is kind of incredibly, terribly
*  bad. One of which most human beings die, one of which all human beings die, one in which all life
*  dies here on Earth, right? And I think it's safe to say we can contemplate all of these, right?
*  There are scenarios for any one of these to actually happen. It's exceedingly rare, but there are
*  scenarios. Right. And I think, incidentally, in my book, which is called On the Future, I do discuss
*  some of these scenarios. And one point I make is that if we think of the intermediate level scenarios,
*  then they will all cascade globally in a way that didn't happen previously. I mean, you may know the
*  book by Jared Darman called Collapse, where he talks about the way I think five different
*  civilizations collapsed. But in none of those cases did the collapse spread globally. Whereas now,
*  I think any really severe setback in any part of the world will have consequences because we are so
*  interconnected by telecommunications, supply chains, air travel and everything else. So
*  things would spread globally. And that's something new. And another thing which worries me very much
*  is that I think society is much more fragile than in the past. To give an example, if we think back
*  to the Middle Ages, when the Black Death bubonic plague killed half the population of many towns,
*  the rest went on fatalistically. Whereas now, if we had some pandemic, once the number of cases
*  overwhelmed hospitals, there'd be a breakdown in social order. People clamor for treatment that
*  wasn't available. And likewise, we're so dependent on electricity, that if there were a massive cyber
*  attack on say the east coast of the United States, then there'd be a complete anarchy within a few
*  days, because nothing would work. And indeed, I think that is realized because I quote in my book,
*  a 2012 report from the American Department of Defense, which talks about that scenario,
*  and says that if it were instigated by a hostile state, it would merit a nuclear response.
*  That would be helpful. It's clear to me that this is going to be somewhat of a downer of a
*  conversation. I should let people know that you're by nature a very optimistic person,
*  actually. You have a lot of optimism about technology.
*  Shevonis breaks in.
*  It does good. Okay. Yeah, so you mentioned the interconnectedness and how that allows,
*  potentially would allow disasters to spread and propagate. But at the same time, there's the
*  related fact that technology has given us a kind of leverage over our future for good and for bad
*  that we didn't have 100 years ago. The 20th century introduced for the first time, I would say,
*  the possibility we could extinguish ourselves.
*  Yes, well, nuclear initially, of course.
*  That's right. So it's a different kind of question that we really haven't
*  been trained to think about. I mean, so you use the number billions to one, let's say,
*  if the probability of wiping out the earth and maybe you got that number by taking one over
*  the population of the earth. But what is that the right calculus? I mean, how should is there any
*  quantitative way of thinking about what we how we should measure existential risk and how we should
*  respond to it? Well, I don't think there is. I mean, when I'm an astronomer, people say,
*  do you worry about asteroids, etc. And of course, I don't very much because they are the one risk
*  you can quantify pretty accurately never any there are how big they are, what the impacts
*  would be, etc. But the point about asteroids is that the probability of impact is no higher now
*  than in prehistoric times. Not our fault.
*  And the concerns that we should have in the forefront of our mind are those which are
*  emergent, and which are probably increasing. I mean, nuclear war was the first one of this kind.
*  But misuse of bio and cybertech are new concerns. And I do think that there is not enough attention
*  given to these. There's a complacency that if something has never happened, you don't worry
*  about it. But if it's an event which is so serious that even one occurrence is too often,
*  we ought to think about this. And I've been involved in setting up a center here in Cambridge
*  to address these extreme risks, the argument being that there's a huge amount of effort going into
*  moderate risks like castagens in food, making trains and planes safer and all that. But these
*  improbable events, which are perhaps growing in probability as time goes on,
*  can lead us into complacency. And we really ought to think about how we can minimize these risks
*  and what we would do if they happened. And I think in our center, if we can reduce the probability of
*  any of these things by one part in a thousand, then the stakes are so high that we'll have more
*  than earned our keep. That's right. I'm morbidly fascinated by solar flares, which are certainly
*  natural phenomena that has happened many times. But I met a lawyer who had worked on a commission
*  where they thought about this. And he was very concerned about the idea that once every thousand
*  years, there's a solar flare that would be big enough to wipe out the entire electrical grid
*  here on Earth. And so it's a kind of a combination of a naturally occurring thing and the vulnerability
*  that we have invented for it. But again, the time scales are things that we're not adapted to
*  dealing with. No, that's right. But that's an example of something where solar flares can
*  affect satellites, for instance, they can be hardened to minimize that impact. And it's
*  worth doing. But you're right in saying that there are some natural phenomena whose consequence is
*  greater now, because we depend on electricity more. And of course, the consequences of an earthquake
*  in Tokyo is more now than it would have been 300 years ago, because of the number of people involved.
*  Well, and in Los Angeles, where I live, of course, you know, the buildings won't fall down,
*  because the buildings are now cleverly constructed, but we will not have electricity or water for a
*  long time if the earthquake hits in the right place. And that's the real worry. You're safe
*  here in Cambridge, right? No earthquakes? That's right. There was a reported one of magnitude 2.8
*  in England a few times ago. And it's reported a budget rig fell off his perch, but nothing much
*  worse than that happens. That would not even be reported where I'm from. So, but there's also a
*  philosophy question here that I thought you highlighted in the book, even when you use
*  phrases like a billion to one chance. What does that mean? Right? Because it's not like you've
*  done it a billion times and seen it happen once. So how do we interpret phrases like there's that
*  kind of probability? Well, of course, there's some context where it is meaningful. I mean, if you
*  have something where you know a roulette wheel is being turned or something like that,
*  then you can quantify it. But of course, as you say, we can't quantify any event's probability
*  if it hasn't yet happened. And that's the reason why we may tend to underestimate. So we can't
*  quantify it. Well, I tend to be a subjectivist about probability. I don't know if you have
*  strong feelings about the philosophy of probability, but as a good Bayesian, I think that all
*  probabilities are more or less on the same footing. They're related to our credence that
*  something is going to happen. But it's just harder to accurately pin them down. Well, that's not true of
*  coin tossing or things like that. There are some where probabilistic estimates do make sense and
*  well, that's our basis. See, if you're an eternalist, if you think the past, present and future are
*  equally real, then the outcome of a coin toss is just your ignorance about the future. You can
*  relate it. You can obviously, you know, give a sensible justification for having a certain
*  credence. But I don't think it's philosophically different. But it does raise the question, you
*  know, the famous question in science is what are the error bars on your error bars, right? So when
*  you say what is a billion to one, could it be just a million to one, something like that? Is this the
*  kind of thing that your center worries about? Well, I think it could be because the probability
*  that our theories are wrong is, of course, not all that small. Right. Right. And always hard to quantify.
*  That's why utter surprises can't be ruled out. And of course, they can't be quantified. Yeah.
*  And that is, I think, the reason why we can't completely dismiss the concerns about doing
*  something for the first time, which has never happened in nature. Of course, when we talk about
*  these physical experiments, in many cases, and indeed, I remember writing papers about this,
*  you can say that nature has done the experiment already, cosmic rays of high energy have collided
*  and nothing disastrous has happened. And the fact that we can see stars, white dwarfs that are not
*  turned into strangelets tells us something. But if there is a potential effect that we can create
*  artificially, which we don't think ever happened in nature, then we perhaps should be open minded.
*  Yeah. But we shouldn't take this too far. I don't think there's anything in nature which is a
*  temperature of a milli-degree Kelvin. But we didn't have huge worries when we first cooled
*  something down to a very low temperature like that. I suppose because we had very great
*  confidence in the physics. Yeah. It does have to do with the, it's a theoretical, I don't want to
*  say bias, but we have to proceed on the basis of thinking that our theories capture some truth,
*  right? Capture some element of reality. When we make these otherwise, like you say, we couldn't
*  do anything because anything would bear some worries in some sense. But I like this example of
*  the Large Hadron Collider and how we can ask whether nature has done it already because it
*  gives a little bit of an insight to people who are not in this field that we're not flying
*  completely blind. There is a process through which people do think about how you might rule out the
*  likelihood of these existential problems. And you make the very good point in your book that,
*  which I had not really thought of before, that we have an obligation to make that reason public,
*  to inform people about why we think this, right? I mean, you really, you make it clear that we can't
*  just say, let's trust the scientists on this. We need to spread our reasoning widely. Well,
*  I think especially when the scientists want to do the experiment. Yeah, there's an interest in it.
*  So I think in that sort of case, it's good to have some independent group of people who can sort of
*  reassure themselves without having any self-interest in the experiment that it's safe.
*  And although, I mean, I don't want to make a big deal of the accelerator, because I don't
*  worry about it at all, to be honest. Me neither out there.
*  But I think in the case of bio experiments, then this is a real issue. And I need to take one
*  example. There were two groups, which in 2011, I think it was, one in Wisconsin, one in Holland,
*  showed was surprisingly easy to make the flu virus more virulent, more transmissible.
*  And these so-called gain of function experiments were denied US federal funding for a number of
*  years, because they were thought risky. Because first of all, this is dangerous knowledge,
*  which perhaps you don't want to publish, that's one question. But also, there's a risk of error
*  releasing something. And it's now possible to synthesize a smallpox virus and things like that.
*  And we do have to worry about, are there some experiments where the risk of something going
*  wrong is such that you shouldn't even do the experiment. And these are contexts where, of
*  course, there are regulations. And you don't just allow experiments to do anything they like,
*  because we are concerned about prudential constraints and ethical constraints on biological
*  experiments. And there is a regime of regulation, as you know, for the use of modern ideas in
*  genetics, etc. But of course, what worries me, and this is really a theme of my book,
*  is that whatever regulations one establishes, even if they're internationally agreed,
*  can't necessarily be enforced. Regulations on nuclear activities can be enforced because they
*  require special purpose conspicuous facilities. And so the IAEA can monitor these things. But
*  when we think about biological experiments, they can be done in a lab with the facilities that
*  exist in many industries and many universities. And of course, cyber attacks of pretty damaging
*  consequences can be done just by a lone weirdo, etc. And we can't really stop experiments like that,
*  or unethical experiments on human embryos and things of that being done. And my concern is that
*  whatever can be done will be done somewhere by someone. Whatever the regulations say,
*  and enforcing those regulations globally would be as hopeless as enforcing the drug laws globally,
*  or the tax laws globally. And so that's what scares me. And if you ask me,
*  what are the concerns I have in the next 10 years, not being too futuristic, then it is
*  the misuse by error or by design of biotech or cybertech. Yeah, I think that that's exactly
*  right. And it does. It raises a couple questions. So like you say, we have regulatory standards in
*  place. On the other hand, they seem to be easy to circumvent by a bad actor. So is that an argument
*  for actually letting controlled research go on? So I mean, you can't just keep knowledge in the box,
*  someone's going to get it, it must be, it might as well be ours as well as someone else's.
*  It's very difficult to say that we shouldn't do research. I mean, you can obviously control the
*  rate by controlling the funding, but you can't really stop the research. Although obviously,
*  if the research is intrinsically dangerous, or intrinsically unethical, then we should try and
*  ban it. But as I say, it's very hard to enforce these bans globally. And in the context of bio
*  and cyber, I think there is going to be a growing tension between three things we want to preserve,
*  liberty, privacy, and security. And of course, this is an issue that comes up when you talk
*  about regulating the internet and all that. And I think if you want to have global regulations,
*  you run into the problem that different parts of the world would adjust the balance differently.
*  I think the Chinese would care less about privacy and more about security than the United States,
*  and we in Europe are somewhere in between. But these are very serious issues, which need to be
*  addressed. What is the state of international cooperation on issues like this? Well, obviously,
*  in health, there is the World Health Organization, etc. But there's nothing yet established in the
*  internet domain. And the problem there, of course, is that the companies are global.
*  And there are vested interests in doing certain things that other people might not want to be done.
*  That's right. So I think there are serious problems of governance of the internet,
*  which are being addressed. But I think enforcing them is going to be pretty hard. And unless you
*  have really firm controls on the internet, which is quite contrary to the initial spirits and hopes
*  of Tim Berners-Lee and the other pioneers, where they wanted it to be free. But is it even
*  realistic? I mean, could we do it? Could we really regulate the internet that easily? I'm not an
*  expert on this. Well, I was an expert either. But I think one can have sort of censorship of content
*  to some extent. But as regards the hardware and minimizing the risk of hacking, I don't know what
*  can be done. But these are serious issues. And they're going to get more serious. And I think
*  one of the most depressing things I find about the way technology is developing is that the
*  fraction of efforts that go into security and all the things that wouldn't be needed if we were all
*  honest is getting larger and larger. Yeah. And it feeds back into this idea that the
*  time scale, this is sort of mismatch of time scales, right? There's great promise in doing research in
*  biology or medicine or building technological infrastructure. And there's also great
*  dangers. But there's this worry that we race toward the promise and the rewards first,
*  and then try to fix up security later, right? Yeah. Is there some sort of global philosophical
*  shift we might try to work toward to be more secure in our technological advancements?
*  Well, I think we should all campaign for this. But we know how hard it is to get international
*  agreements on anything, climate control and all that sort of thing. Yeah. Yeah. And we're sitting
*  here in the United Kingdom, which has just decided to leave the rest of Europe. It's hard to keep
*  international organizations together. And we're also sitting here just a few days after it was
*  announced that Jeff Bezos' phone was hacked by Saudi Arabia. So even the best protected
*  people in the world could be very vulnerable. That's right. So what about, I mean, maybe it
*  would be useful to get on the table a list of your favorite existential risks in some sense.
*  Obviously, we've mentioned nuclear weapons biology. I'll let you be a little bit more
*  specific about your top 10 dangers. Well, I mean, I think I'd want to avoid the word existential
*  because the idea of wiping ourselves out is something which is relevant only to these rather
*  science fiction like scenarios. Isn't the name of your center the Center for Existential Risk?
*  It is. And I think extreme risks or catastrophic risks would be a more appropriate representation
*  of what we actually do. Good. And as I say, I think these are the downsides of exciting new
*  technologies, bio and cyber being the most prominent. And of course, another class of
*  concerns are those which are emerging from our collective impact on the planet. You know, the
*  climate change is one and associated loss of biodiversity, etc. These are a class of long term
*  threats, which everyone is aware of, which are because of our collective actions rather than
*  a few bad actors, as in the other cases. And again, it's very hard to get effective action
*  because politicians tend to focus naturally on the immediate and the parochial. They think
*  up to the next election and they think about their own constituents, etc. And it's very,
*  very hard to get prioritization of the action which is needed to minimize these global risks
*  where the community, the global community has to act collectively. And we're seeing this in the
*  context of climate change and attempts to reduce CO2 emission. So I'd say two things about this.
*  One thing which I feel quite strongly in the UK context is that we have set a target of cutting
*  our net emissions to zero by the year 2050. This is a very challenging target, you will need some
*  new technology. And I support the idea that we should have a very strong program to develop
*  clean energy, better batteries and all that sort of thing. But the reason I support this is that
*  if we succeed, then we will reduce our emissions to zero with really 1% of the world's emissions.
*  That's neither here nor there. But I think we in Britain can claim to have produced more than 10%
*  of the world's clever ideas up to now. And certainly a disproportionate number. And so if we
*  do have a crash program that does lead to cheaper energy storage, etc, etc. So that India, for
*  instance, can leapfrog directly to clean energy when it needs a grid and not build coal power
*  stations, we will thereby do far more than 1% to reducing world CO2 emissions. And it'd be hard
*  to imagine a more inspiring goal for young engineers than to provide clean energy for the
*  developing world. And so that's why I think there's a strong case for enhancing R&D there.
*  And I would say the same thing about research into plant science and bio, because feeding 9
*  billion people by mid-century is another challenge. And this requires intensive agriculture,
*  if one is to do this without encroaching on natural forests, etc, etc. And here again,
*  it's going to need new technology, which the scientifically advanced countries of which we're
*  one, can make a disproportionate contribution to. So I would say that if we prioritize research into
*  those areas, things to do with clean energy and storage, and into improved plant science, etc,
*  then we'll not only help ourselves, but help the world. So that's one thing. That's a digression.
*  But thinking about the politics, then I know people who've been scientific advisors to government.
*  And they normally get rather frustrated because the politicians have their urgent agenda, etc.
*  And it's very hard to get them to prioritize something which is long term.
*  Scientists are thinking more long term, yes.
*  Yes, okay. But that's why I think that scientists can have more effect if they go public and,
*  you know, become small time Carl Sagan, as it were, because then they do have an influence.
*  And politicians will respond if they think the voters are behind them.
*  And so I think what the scientists can do is to publicize the issues in a way that the public
*  responds to, because then the public will influence the politicians and the politicians will
*  feel that they can take these actions without losing votes. Let me give two examples.
*  One example which is perhaps very surprising is the Pope, because in 2014,
*  there was a big scientific conference at the Vatican, which had all the world experts in
*  climate and people like Jeffrey Sachs and Joe Stiglitz, etc., which discussed these environmental
*  and climatic threats to the world. And that was input into the Pope's encyclical in 2015.
*  And that encyclical got him a standing ovation at the UN and, of course, had an influence on
*  his billion followers in Latin America, Africa, and East Asia, and helped to ease the path to
*  consensus at the Paris conference in December 2015, because the people in those countries
*  knew that lots of people would support this. And as a more parochial version, in this country,
*  one of our less enlightened politicians proposed legislation to limit the production of non-reusable
*  plastics, drinking straws and things of that kind. And he only did that because we'd had a year or
*  two ago the Blue Planet II programs, fronted by David Attenborough, which showed the effects of
*  plastics in the ocean, and especially iconic picture of an albatross returning to his nest
*  and coughing up for its young, not the long for nourishment, but some bits of plastic.
*  And that's become an iconic image, rather like the polar bear and the melting ice
*  flow was for the climate campaigns. And because millions of people saw that, then there was public
*  support for the idea of cutting down the use of plastics, and that's certainly become quite
*  an effective campaign in this country. So if the public cares about something, then politicians
*  will respond, even if it's long term and part of a global campaign. And that's why we should
*  encourage and value the scientists who are able to get through to the wide public, because that's
*  more effective than being a science advisor in-house to the government or politicians.
*  Yeah, no, that's a very interesting point that if a scientist, you're saying if the scientist wants
*  to have a real impact on public policy, then the public should be as much of the target as
*  the politicians. Yes, yes, yes, and because politicians respond to what's in the inbox
*  and what's in their press and what they think their voters want, obviously. And so that can be
*  influenced by charismatic public figures with a scientific background. Good, very good. And so
*  I want to back up a little bit because maybe we can be a little bit more specific as an example
*  of good action here. If you think that Britain wants to be zero emissions by 2050,
*  what would that involve? What kind of technology is that? Obviously, some renewable energy,
*  some storage, but can we be more specific? Well, I mean, I think storage is more cheaper,
*  more efficient batteries, and one has to store the energy from day to night and maybe even
*  seasonally. So we have to do that. I personally think it would include nuclear. Okay. And of
*  course, this is controversial. But I think the problem with nuclear is there's been no real R&D
*  in the last 20 years. The designs being used really date back from the 1960s. And I think,
*  therefore, that in trying to develop clean energy, that should include fourth generation nuclear and
*  things like small modular reactors, which could be put on the back of a truck and
*  and would be 100 megawatts each or something like that. And something like that would be safer.
*  And hopefully, if they could mass produce more economical. So I think that nuclear is probably
*  going to be part of the answer. Without that, it's going to be harder. So do you think that
*  the relevant the related issues there with not emissions, but there's still nuclear waste, right?
*  Do you think that those are solvable? I think they are. Yes. I mean, they are a problem,
*  but I think they can they can be coped with. And incidentally, I think people worry too much about
*  low radiation levels. That's right. Another place where scientists maybe can have a
*  valorous impact somehow. Yes. Yes. Explaining that this is not so bad. Yes. Well, there is a
*  dread factor in radiation, isn't there? Yes. And well, to digress a bit, I think it's very important
*  to have clear guidelines about the dangers. Because suppose a dirty bomb were let off in
*  a street in New York or something like that over here, then people might look at some
*  table of risks and say, we've got to evacuate this street for the next 30 years, something like that,
*  which may be sort of over the top. But after an event like that is not the time to have the debate.
*  You have to have the debate beforehand. And of course, an example of this, which we have studied
*  at our center is the aftermath of the Fukushima disaster in Japan, which was serious. But the
*  evacuation was over the top and probably caused far more distress than the radiation itself would
*  have done. I didn't know that. And I'm over evacuated. They were evacuated. Yes. And I think
*  what ought to happen is there should be sort of guidelines, which are made available for every
*  city about what the extra risk is for a particular level. Because if I was an old villager in Japan,
*  I'd be prepared to accept a significantly higher risk of cancer in order to live out my days in
*  my home rather than being evacuated. And people ought to be given that choice. And of course,
*  oh, people might make a different choice from people with young children. But there ought to
*  be some rational way in which people can make that choice. Otherwise, there'll be an over reaction.
*  And this is something which is particularly the case in terms of radiation. This is just a human
*  foible, though, right? We're very bad at talking rationally about what happens to us in extreme
*  situations or when our lives are at risk. Like once you say to an old villager, if there is radiation
*  here, don't worry about it. You can stay here because you're old and you're going to die anyway.
*  Like this is not an easy conversation to have. Yeah. Well, I think you'd have to actually
*  ask them in advance. You could ask everyone, you know, what risk will you be prepared to
*  tolerate if the alternative is evacuating from your home? Yeah. Yeah. But yeah, I don't know what
*  the answer is. I mean, there certainly is a philosophy question here, a whole bunch of them.
*  I mean, are philosophers heavily involved in your center, you know, making decisions about these big,
*  big questions? Yes. Well, in fact, one of our senior people is a philosopher and is particularly
*  concerned about the issue of future generations because there is a question of what discount rates
*  you apply and how much we should think about future generations rather than as we're now alive. This
*  is a real question. And in fact, we have set up a parliamentary committee to try and raise the
*  amount of attention given in all legislation to the effect in the long term, because many of the
*  things we're doing are having some effect. Obviously, this is something we ought to try
*  and emphasize and not discount the future as much as a commercial transaction does. Have you found
*  politicians more or less responsive to this? Well, I think they are responsive. I mean,
*  as long as a sacrifice for the present generation isn't too much, but it is important, obviously,
*  to think longer term and to realize that even though if you make a commercial decision on an
*  office building or something, if you don't get your money back in 30 or 40 years, you don't do it.
*  Whereas in making a decision that may affect the longer term future, people do care about the
*  life experiences of babies born today who will still be alive in the early part of the 22nd
*  century. They get that. And so we ought to think longer term. I mean, it's hard to predict, of
*  course, because things are changing so fast. And one of the points I make in my book is
*  that it's at first sight paradoxical that in medieval times, when people had very limited
*  horizons, and when they thought the world might end in a thousand years, they still built cathedrals
*  that wouldn't be finished in their lifetime. That might seem paradoxical. Whereas now,
*  we would not do something like that. But the reason it's not paradoxical is that in medieval
*  times, even though they thought the whole world might end, they thought the lives of their children
*  and their grandchildren would be more or less like theirs. So the grandchildren would appreciate the
*  completed cathedral. Whereas now, I think we don't have very much confidence in predicting
*  what everyday life will be like 50 years from now, given the changes that have been in the
*  last 50 years. And so for that reason, I think it's rational to not plan quite so far ahead in some
*  context. But on the other hand, if there's a risk of doing something which is irreversible,
*  long term damage, we ought surely to care about future generations. Does the example of the
*  environment and climate change, it has some sort of depressing aspects in how resistant certain
*  quarters have been to the obvious scientific evidence, right? I mean, how much do you worry
*  about misinformation or just vested interests resisting the scientific findings? Well, very
*  much so, of course. And this is apparent in that context of climate change, where the science is,
*  of course, very difficult, and is very uncertain, we've got to accept it's uncertain. But I think we
*  do have to worry about the reluctance to accept scientific evidence, not in that context alone,
*  but in the dangers of vaccines and things of that kind. That's another example where, of course,
*  there's a lot of misinformation, which is damaging. Being from Los Angeles, I'm well aware of this
*  phenomenon, the epicenter of that particular phenomenon. But climate change is also an
*  interesting case, because it's so different in character than something like a nuclear war,
*  or even a terrorist nuclear bomb, right? It's slowly creeping up on all of us versus a tiny
*  fraction every year that it would happen, right? Yes. Is it a different strategy, different mental
*  space you need to be in to attack these different problems? I think it is. And I think, of course,
*  in both cases, it's hard, because in the case of the sudden catastrophe, then the point is we're
*  complacent, because it's never happened, and you think it never will. It's like in the stock market
*  where it keeps on rising. It will rise forever. And you think, but then there's a sudden fall,
*  there's an asymmetry between the speed of rises and the speed of falls. And similarly, you think
*  that things are going to be okay, and they're not. And one of your predecessors is a Darwin lecturer,
*  Mr. Taleb, who made some rather good points about Pareto distributions, where things are below
*  average 98% of the time. And it's a long tale that's important. Right, yeah. And so,
*  let me let's fill in a little bit, I want to get the all these different kinds of extreme risks,
*  I think is a good way of putting it. And I think most people know about climate change. In fact,
*  we've talked about it on this podcast. There's the nuclear threat, which maybe was larger in the past
*  for a large scale nuclear war. These days, are you much more worried about one bomb being taken
*  into a port and blown up? Yes, that's right. I think we can say that during the Cold War,
*  there was a risk of a real catastrophe, because there were about 50,000 bombs on both sides.
*  And if they had gone off, then that would have devastated much of the northern hemisphere,
*  certainly Europe and North America. And there were at least a couple of moments when it was a non
*  civil war. Indeed. And I think, you know, when we see what people said, McNamara in his later years,
*  and Kenzie saying that the risk was between one in three and evens, and all that, and of course,
*  the other false alarms that we've learned about, I think we realized just how great the threat was.
*  And for those of us in Europe, I think the realistic estimate of the risk during the Cold
*  War was probably one in three or something like that. And it's hard to quantify, obviously, but
*  I think looking back, there was a substantial chance. And I personally think that if people
*  had realized that, there would have been a bit more questioning of the conventional policy.
*  For my part, I wouldn't have been prepared to risk a one in three or even one in six chance
*  of a nuclear exchange of that kind, even if the alternative was a Soviet takeover of the whole of
*  Europe. And I suspect many people have taken that view, but they didn't really feel that there was
*  this real risk. Right. And saying those words out loud is also gets you in trouble politically,
*  maybe a little bit. Well, it might have been for some people at that time. But I think realistically,
*  that will be the trade off that would have been far better to let the Soviets take over than to have
*  destruction of the whole fabric of Europe. So that was the situation over the Cold War. But as you say,
*  that particular scenario is at least in abeyance, because the number of weapons on both sides has
*  been cut by a matter of fact of 10. But on the other hand, there are two things to worry about.
*  When I see three things, one is that there are more nuclear powers, there are now 10 nuclear
*  powers. And the risk of some nuclear weapons going off in a regional conflict in the Middle East or
*  India and Pakistan is probably higher than ever. Yeah. And that would be a regional disaster.
*  Unless it produced some such big forest fires that you had a nuclear winter, but it would probably
*  just be a regional effect. So that's one concern. But the other point is that the
*  global threat may be just in abeyance, because it could be that there's a new standoff in the
*  second half of the century between new superpowers, which is handled less well or less luckily
*  than the Cold War was. So that's a concern. And the third concern is that, again, something we've
*  having meetings about here, that the risk of cyber attacks of the nuclear infrastructure
*  is a new threat, because obviously it's very complicated. So what exactly is the threat there
*  that a cyber hacker could do what? Could trigger false alarms or even trigger
*  bombs going off. This is a, one hopes that it's being addressed. Yes, it is being addressed.
*  But on the other hand, in all these cyber issues, there's an arms race between the cyber attackers,
*  who are indeed aided by AI, and of course, those who are trying to secure things. And so this is a
*  new concern, a new class of risks, quite apart from old fashioned types of false alarms.
*  And the lone actor who's not even a state is a new worry for nuclear weapons, right?
*  Yes, I don't know to what extent a lone actor could realistically do that. But I think
*  a small state certainly could. And presumably it becomes not less likely over time, right? I mean,
*  as technology leaks out, as the information gets out, as more and more plutonium and uranium are
*  flying around, what kind of risk is it possible to quantify that we have? Well, I wouldn't venture
*  to quantify it, but obviously there are far greater risks. This is just another instance of how
*  a small group of people amplified in their impact by modern technology can have a consequence that
*  cascades globally. This is what I worry about. And I put it in my book, the global village will have
*  its village idiots, but they will have a global range now, which they didn't in the past.
*  And is the biological worry bigger in your mind than the nuclear worry?
*  I guess there's two categories, right? There's sort of poisons, like anthrax, that you can spread,
*  and then there's these contagious agents that you can imagine, and they both sound terrible.
*  Yes. Well, I think the consequences of a natural pandemic could be worse. Of course,
*  SARS wasn't a global effect because it only spread to places that could cope with it. I mean,
*  Singapore and Toronto, I think, mainly. But had something like that spread to one of the mega
*  cities of the developing world, like Mumbai or Casablanca or something, then it could have been
*  very, very serious. And that's true of any future one. And of course, we have planned for things
*  of that kind. How would you cope? Would you turn off the mobile phones because they spread panic
*  and rumour? Or would you leave them on so that you could spread information? That's a sort of
*  sociological issue which needs to be addressed in planning for these disasters.
*  Which should you do? Now you have me curious.
*  Again, I don't know, but that's an example of a context where you need social scientists to
*  explore what is the optimum thing. And so I think we do need to worry about natural pandemics and,
*  of course, engineered pandemics. And also, we certainly have to worry about the ethical
*  consequences of new technology. Already, this is an issue with the Chinese experiments on human
*  embryos and all that. The guy got arrested, right? The guy who claimed to have genetically engineered
*  a baby? Yes. And in fact, all the Chinese scientists were equally opposed to what he had done.
*  The balance of benefits and risks was not such as to justify it. Whereas one can justify perhaps
*  gene editing if you remove the gene for hunting disease or something like that. But what he was
*  doing was deemed by almost everyone to be unethical. But we do have to worry about
*  the widespread understanding and availability of these techniques. And of course, we have to worry
*  about the social effects of these if they can be used for human enhancement, for instance.
*  I mean, I think most people think that if you can remove some severe potentiality to disease,
*  then that's okay. But if you can use genetic modification for enhancement, then that's
*  different. But fortunately, that's a long way in the future, because most qualities that we might
*  want to have in human beings, looks and intelligence, etc., they're complicated
*  combinations of thousands of genes. So before you can do that, you've got to first use AI on some
*  very large sample of genomes to decide which combination is optimal, and then be able to
*  synthesize that genome and be confident it's not going to have unintended downsides. So that's
*  fortunate long way in the future. But if that happened, then that would be a new fundamental
*  inequality that we'd have to worry about. And it would perhaps be bad news, because if we think of
*  the effects of medical advances in the last century, they've had a beneficial effect on
*  promoting equality, because the cutting down in infectious disease in Africa, for instance,
*  has had a huge effect on infant mortality. So I think we can say, looking back, that medical
*  advances have been beneficial, not just for those who have them, but for promoting equality
*  worldwide, whereas the future ones may have the reverse effect. In a sense, because we're now
*  inventing medical procedures that are just financially out of the reach of so many people.
*  But it's a good point to sort of segue into something, because there's the topic of extreme
*  risks, and then there's also the topic of extreme benefits. And some of these technologies are
*  ambiguous, they're transformative, you know, human gene editing, AI, more things that we're
*  stumbling across right now in the 21st century. And there's certainly that's certainly an example
*  where some people are going to want to just rush ahead without thinking about the consequences.
*  Well, again, we can't stop, we can't control it, just like we can't control the drug laws or the
*  tax laws. And, and perhaps, you know, in a way, if some crazy people want to do this, perhaps,
*  we should not be too upset, because obviously, every technology in medicine starts off as risky,
*  I mean, heart, heart transplants, and things like that, and then become more routine. And so perhaps,
*  we should not be too upset if a few people try to modify themselves in ways that turn out not to be
*  as beneficial as I hope. And of course, techniques like enhancing your brain by some cyborg implant
*  into your brain, I mean, it'd be great if we could improve our memories and all that. And so it's
*  perhaps a good thing that some people are talking with a straight face about the potential of doing
*  this. I mean, as you said, it's hard, but let's just imagine, because that's what we're doing here,
*  the future. Let's imagine someone develops a diagnostic test that lets you screen
*  thousands of embryos and pick out the one that will be the most intelligent.
*  And let's prospective parents take advantage of this technology. Would that be bad?
*  Not necessarily. Of course, it's interesting to know whether they would seek more intelligence,
*  because there was the famous case of Mr. Shockley, do you remember who?
*  Shockley, the inventor of the transistor. And he set up a sperm bank for Nobel Prize winners.
*  And I'm glad to say there was very little demand.
*  I guess that's true. That's true. But I just wonder, again, on a philosophical level,
*  there's an ickiness factor, but I think maybe sort of engineering babies sounds weird,
*  Dr. Strangelovey, but maybe there's just an inevitability about it also. Like once people
*  get over that, it's going to be taken for granted. Well, of course, we should make babies as good as
*  we can. And I honestly personally don't have a strong understanding one way or the other.
*  Maybe we should. But there is, as you said, there is an icky factor about some kind of genetic
*  modification, even for animals. And I think the reason why GM crops were opposed in Europe was
*  that people associated genetic modification with things like making a rabbit glow in the dark by
*  putting jellyfish genes in it or something of that kind, which did promote the ick factor.
*  It's not like circus animals being given silly clothes to wear. We somehow feel it's not what
*  you should do to animals. And people felt the same about that. And that really perhaps went against
*  the support for the use of GM techniques in very good cases.
*  Certainly, it would be irresponsible not to mention everyone's favorite new extreme
*  risk, which is super intelligent artificial intelligence. Right. Do you have an opinion
*  on whether or not that's a real risk or a fake one? Well, whether it's a risk or opportunity,
*  we don't know. But I think obviously, the question is, whether it's general intelligence,
*  as we know already, what we've had for 50 years, machines can do arithmetic better than
*  any human being. And of course, we've now got machines can play chess better than a human being
*  and do many other things. But of course, there's still many things they can't do as well as a human.
*  And the question is, will they be able to acquire common sense, as it were,
*  they're acquiring greater capabilities and AI is very, very valuable for coping with large
*  data sets and optimizing complex systems, like the electricity grid of a country, the traffic flow.
*  And incidentally, the Chinese will be able to have a planned economy of a kind that Marx could
*  only have dreamt of. Because they have a record, they have records of all transactions, all stocks
*  in the shops, etc. So they could do that. So AI does have tremendous power to cope with complex
*  systems. But as regards to sort of robots, which we can treat as intelligent beings, as in the movies,
*  that's quite a long way away because robots are still rather bad at interacting with the real
*  environment. They can't move pieces on a chessboard as well as a kid can. They can't jump from tree to
*  tree as well as a squirrel can. So there's a long way to go before robots have that sort of agility
*  and interaction. And before they have any feeling for the external world, because someone told me
*  that the Watson computer, which won the game of Jeopardy, was asked, which is bigger, a shoebox or
*  Mount Everest? And couldn't answer. And that's because obviously it doesn't, it understands words.
*  It has no concept of the external world, etc. And to give a machine that concept, and indeed to
*  get you understand human behavior, is a big challenge. It's a big challenge because from
*  a computer's perspective, everything we do is very, very slow. And computers learn by
*  looking at millions of examples of pages to translate or pictures of cats and all that,
*  and recognize them. But watching human beings is like us watching trees grow. It's very slow,
*  and they can't accumulate the amount of data in order to really understand that. And so that's
*  a big impediment. So that's a long way of saying that I think it'll be a long time
*  before we have human general intelligence in a sense of something which behaves like a human
*  being. Although obviously, we will have machines that can cope with huge data sets, and clean up
*  on a stock market and all those things. Right. So of course, yes, the getting artificial intelligence
*  up to general human scales will be hard, right? But do you think it's possible in principle?
*  There's no fundamental objection to it, I suppose, because you could imagine in principle,
*  a machine which has sensors, which allows it to interact with the real world, and maybe even
*  communicate with human beings. So it's not impossible. But whether there'll be a motive
*  for it, I don't know. Because I think one thing we've got to remember is that there's a gap
*  between what could be done, and what there's an economic or social motive for doing. And that's
*  why some things surge very, very fast, and level off. Just to digress for a bit, the most rapid
*  technology ever, really, is smartphones, which have developed and spread globally,
*  within a decade or so. But probably they will saturate now. The iPhone 11 is probably as
*  complicated as you want a smartphone to be. And so probably 20 years from now, we'll be using
*  similar smartphones. And so a sigmoid curve is what happens to most individual technologies,
*  and then something else will take over. And I think we've got to be mindful of the point made
*  in the famous book by Robert Gordon, about the important technological advances. And he makes
*  the point that what happened between 1870 and 1950 in electricity, the railways, television,
*  cars, and all that, that was more important than anything has happened since. And there's something
*  in that. And let me give you another example. Aviation. 1919, Orchard and Brown's first
*  transatlantic flight. 50 years after that, the first commercial flight of the jumbo jet, 1969.
*  And now we're 50 years after that, still a jumbo jet. And the Concorde came and went. And so that's
*  an example of how technology develops, when there's a motive, but then it levels off when things are
*  fine or when there's no economically feasible way of developing them further. And we got to
*  realize that that may happen to some of these information technologies, which have developed
*  very fast. So we can't assume that because they're developing so fast now, that there will be equally
*  rapid changes in the next 10 or 20 years. But there'll be some, obviously.
*  Yeah. In the case of the smartphone, I guess the obvious next big phase transition would be to get
*  a direct interface with the brain and the computer. Right, and that'd be a very big jump, of course.
*  But we're not quite there yet. People are trying, but that's not going to be the next five years.
*  Okay, but still, if artificial intelligence could be, even if it's not precisely human,
*  even if it couldn't pass the Turing test, there is a worry in certain circles that it will
*  nevertheless be powerful. And it won't have the same values that we do, right? You could imagine
*  something that is hyper intelligent along some axes, and yet isn't all that interested
*  in human flourishing. Is that something you worry about?
*  Well, of course, you've got to worry about two things. First, of course, if we have the Internet
*  of Things, then of course, that means that it's hard to avoid a scenario where an AI can interact
*  with the real external world. It can't be kept in its box. But the question is, does it have its,
*  what motives would it have? We just don't know. I think it's not clear it has motives, but it may
*  do things which are contrary to human interest. Obviously, that's a possibility. That's to have
*  any machine. And is there any way to, I mean, we can sit and fret about it. Is there any strategy
*  for planning for that? I mean, many big names have started warning about this, but I'm not quite sure
*  what the actionable thing to do is. Well, I don't see it in principle different from
*  avoiding the misuse of any other technology, really. So don't worry so much about it.
*  Not worried so much about that. Okay, very good. What is your more positive spin on what AI can do
*  for us or what these kinds of technologies will help? Well, I think it's clear it can
*  optimally cope with networks, electricity grids and things of that kind. And obviously, it can
*  help scientists with discoveries. I mean, I think, to take one example, if you want to have a
*  room temperature superconductor, then as you know, the best bets are these rather complicated
*  compounds and things. And rather than do lots of experiments, then maybe AI can explore the
*  parameter space and come up with that and ditto with drug development. So I think it can help.
*  And perhaps moving closer to our own field of science, I think it may help us to understand
*  some fundamental problems in cosmology. Because supposing that some version of string theory is
*  correct, and that theory applies to the early universe, it could very well be that it's just
*  too difficult for any human being to work through all these alternative geometries in 10 dimensions,
*  etc. Which you know, far more about than me. It may just be too difficult. But on the other hand,
*  just as a computer learned to play world class chess in three hours, given just the rules,
*  then it could very well be that a machine could do the relevant manipulations and calculations
*  in order to follow through the consequences of a specific string theory, on the variety of string
*  theories. And of course, if it turned out that at the end, it spewed out the right mass of the
*  proton, things like that, then we'd know it was on the right lines. And this may be a kind of
*  scientific discovery, where it never gives any human an insight, because it's just too complicated.
*  But nonetheless, we would know that it was developing a correct theory. And so I think we
*  have to bear in mind that there may be some theories which are correct, but we can never sort
*  of have the insight into them, which we hope to have into the theories of physics at the present
*  time, simply because they're too complicated. But nonetheless, we would have confidence in them,
*  because it would do a calculation, and it would come out with answers which you could compare
*  with experiment for numbers of neutrinos, masses, etc. And if we had such a theory,
*  we would then believe its predictions about the early Big Bang. Right. Because the hosting
*  theory is supposed to apply to the conditions early on. And so we would have reasons to be able to
*  decide whether many Big Bangs are not one, what were they like, and all that. And so I think this
*  is just an example of how in science, the capability of a machine to do very, very complicated manipulations
*  of 10 dimensional geometry, which I think is a less remote goal than understanding human behavior,
*  just the kind of thing could be good at. And that could be very, very important for science,
*  the extreme kind of physics that we are interested in ourselves, but also developing temps,
*  high Tc superconductors, and drugs and all that. So I think the power of AI to do not just routine
*  computations, but to explore parameter space and learn is going to be very powerful and beneficial.
*  It's been a while now since the four color theorem was proven. I remember vividly,
*  right, but that's an example of that's an example of a computer doing something and essentially
*  there again, one knew what it was doing. I think in the case of string theory, we may not understand
*  what it's doing. It's just like in some of the very clever moves that the AlphaGo Zero made playing Go,
*  the experts didn't understand how it chose that move. And I think it may be more like that. I
*  think in the four color theorem, everyone knew exactly what the program was doing, etc. Whereas
*  this might be qualitatively different from that in that we don't really understand what's going on.
*  And of course, this is a problem with AI already, because if decisions about whether you
*  should be let out of prison, whether you deserve credit or whether you need an operation,
*  if they're made by machine, then you're a bit worried about that. Even if you have evidence,
*  the machine on the whole makes better, more reliable judgment than human. You feel you're
*  entitled to an explanation you can understand. But of course, that's not always the case
*  when computers are now used. And it may never be the case if they're used to solve these very
*  difficult scientific problems, which involve a huge amount of calculation. Yeah, there's a
*  whole regime in which we can imagine computers solving problems and then not being able to tell
*  us why they got the solution that they did. And scientifically, that would seem to be not really
*  satisfying in some way. Like we would think that there's further work to be done if that's all we
*  have. Well, that's right, because obviously, what the satisfaction of science is that is
*  having some idea and then you get the insight when you realize it's just got to be that way.
*  And that's the most exciting thing that happens to you if you're a scientist. But we would never
*  have that. But nonetheless, if it spews out the correct values for the fundamental constants and
*  things like that, then you'd have to accept that it's really got some insight. What are your
*  feelings about uploading a human brain into a computer? Well, I mean, is it ever going to
*  be feasible? I don't know. But then, of course, the question is, would it really be you? Yes,
*  because I think our personality depends on our bodies and our interaction, our sense organs. So
*  would it be you? So again, what philosophers discuss is, if you're told this has been done,
*  are you happy to say, well, you could be destroyed because you're there? And what happens if several
*  clones are made of you, which is really you? So I think there are all kinds of fascinating
*  philosophical conundrums, which philosophers have talked about for a long time. But of course,
*  maybe one day they will be practical ethics if we can do things like that. But I think we do need to
*  worry about whether this is possible. And of course, if we think a few centuries ahead,
*  then even without that, some human enhancement may have changed human beings. And we're not
*  evolving on the timescale of the Indian selection. We're evolving much faster, or we could do through
*  these techniques. And one other point I make in my book is that when we read the literature left by
*  the Greeks and Romans, then we can appreciate it because human nature was the same then. So
*  we have some affinity with the emotions of those ancient artists and writers. Whereas a few hundred
*  years from now, any intelligences which are still around may have no more than an algorithmic
*  understanding of us because they may be sufficiently different, that they don't have anything that we
*  would call human emotions and human nature. So that's going to be a real game changer. But that
*  could happen over a few centuries. Because of human machine interfaces or? Well, human machine
*  interfaces or drastic genetic modification, human enhancement, which you were talking about.
*  I do think that also longevity is a frontier that might change things dramatically if that happens.
*  Yes, indeed. Yes. And it could. And of course, people are working on this, as you know, and some
*  people think that aging is a disease that can be cured. Some people think it's just incremental
*  improvement. But of course, if aging can be slowed down so that people live to be 200,
*  this, of course, would be a huge sociological change. And again, if this was available to only
*  some subset of people, this would be a huge and fundamental inequality. And the question is,
*  what would go with your multi-generational families or would you delay the menopause and all that?
*  You know, it'd be crucially different. And so we'd have to be very worried about what would happen
*  if changes of that kind were possible. But again, we can't exclude them, I don't think.
*  And of course, we know the people like Mr. Ray Kurzweil, who think that they'll be immortal either
*  by that way or by being able to download them. And they're the people who want to have their bodies
*  frozen in order that they can be resurrected when this happens. And given all the existential risks
*  that we have talked about, that doesn't seem like the best strategy to me. No, but in fact,
*  amused that three people I know, I'd say they're from Oxford, not from my university,
*  my university, have paid good money to be frozen by this company in Arizona. I think it's $80,000,
*  a cut price if it's just your head being frozen. And they hope this company will keep going for
*  a few centuries and then they can be revived. And of course, they've got to have their blood
*  replaced by liquid nitrogen, etc. And they go around carrying some medallions so that people
*  know that that should be done as soon as they drop dead. They're to be saved. Yeah. So I think,
*  I also think actually it's selfish because supposing that this worked, then they'd be revived into a
*  inconceivably different world. They'd be refugees from the past, and there'd be a burden. I mean,
*  you know, we feel that we've got to look after refugees or some people from Amazonian tribes,
*  habit has been destroyed. But if these people voluntarily as refugees from the past,
*  impose a burden on you, then maybe they can put you in a zoo or something like that.
*  It's not clear that it's an ethical thing to do. I think I'm on your side. But you know,
*  for purposes of playing devil's advocate here, there's a similar calculation that we do
*  with the existential risks. If you thought that there is a 100th of 1% chance that 300 years from
*  now we cure aging, and you could literally live forever, then clearly the benefit to you of being
*  frozen, even if it doesn't work very well, is almost infinite, right? There's definitely
*  calculation that ends up with the conclusion you should do everything you can to try to preserve
*  yourself. Yes. And the chances are if you don't do this. Right. That's right. So what about then,
*  as another survival strategy, going into space? I know that Elon Musk has said that one of his
*  motivations for SpaceX is to back up the biosphere by making sure we have some of us on other planets
*  if something goes dramatically wrong here on Earth. Is that a good survival strategy for the human race?
*  Well, I'm a bit skeptical about these arguments. I mean, of course, I'm especially interested in
*  space being an astronomer. And I think AI and miniaturization is going to be crucially important
*  for the science of space exploration. I think there'd be wonderful probes sent into space,
*  Cassini orbiting around Saturn and its moons and all that, and New Horizons, which took pictures
*  of Pluto and all that. But all those were 1990s technology. And so they took 10 years to build,
*  and 10 years to get there. And if we think of how things like smartphones have changed since then,
*  we realize how much better we can do now. And so I really hope that there'll be swarms of miniaturized
*  probes going throughout the solar system to explore in detail. I think that's realistic.
*  And also wandering around on the surface of Mars, etc. But as regards people, then with every advance
*  in miniaturization robotics, the practical need for the people gets less. And that's
*  one reason why, of course, manned spaceflight has not been prioritized so much. Of course,
*  there is a revival of interest. And I personally think that if I was an American,
*  I wouldn't want any taxpayers money to go on NASA's manned program, because there's no practical need
*  for the people. But on the other hand, I'd be glad that Mr. Musk and the others are developing
*  very effective rockets, bring the Silicon Valley culture to a branch of industry that was dominated
*  by the big conglomerates like Lockheed, and doing great stuff there. And I hope that they will be
*  sending people into space. But they'll be adventurous prepared to accept a high risk.
*  Because one reason why the NASA manned program is very expensive was it was very risk averse. The
*  shuttle was launched, I think 135 times. And there were two failures, less than 2% failure rate. But
*  each of those failures was a big national trauma, because it was presented as safe. And they sent
*  up a woman schoolteacher and all that. So I personally think that space should be left for
*  adventurers, the kind of people who hang gliding, or simity, and things like that, and are prepared
*  to accept a very high risk. Right, the risk is just cost for doing business, and that's built in.
*  Yeah, yeah. And there are people, obviously, adventurers who are prepared to take that risk,
*  and even a one way ticket to go to Mars. And but Elon Musk has said he hopes to die on Mars,
*  but not on impact. And good luck to him. And we should admire these people and cheer them on.
*  But I think the idea of mass emigration is a rather dangerous delusion. Okay. Because we got
*  to realize that dealing with climate change, for instance, is a challenge. But it's a doddle
*  compared to terraforming Mars. Right. And so I think we got to accept that there's no planet
*  B for ordinary risk averse people. And so we should encourage people to go to Mars, but
*  that's less comfortable than living at the South Pole. Not many people want to do that.
*  Right. So I think we should encourage it. And I think actually looking further ahead,
*  I think we should cheer on those who do try to establish a colony on Mars, even though it'll
*  be against the odds, uncomfortable. And that's because those people will have every incentive
*  to use all the techniques of genetic modification and cyborg techniques to adapt themselves.
*  We're pretty well adapted to living on the earth, but they be in an environment to which they're
*  very badly adapted. So they have every incentive to use these techniques to adapt to different
*  gravity, different atmosphere, etc. And maybe, if possible, to download themselves into some
*  electronic form. And if they do that, then they may prefer zero G and they may not need to go on
*  the planet at all. So I think that these post human developments, which will happen on a
*  technological time scale, far faster than the Darwinian time scale, they will happen fastest
*  among these crazy adventurers who try to live on Mars. And so the post human era will start from
*  them. It does seem almost inevitable that the first trip to Mars will be a one way trip. Yes.
*  It's so hard to come back. And that is a good argument that it won't be done by a government,
*  right? NASA would never send astronauts to Mars. There could be lots of volunteers
*  to go one way. Right. But the government would be having a hard time. And we and of course,
*  I think as human beings, we should admire and cheer on these people because
*  they may indeed hold the post human future in their hands. And I guess I hadn't thought of the
*  idea that people who dwell in space, whether it's on Mars or in between, would be natural candidates
*  for genetic modification. But it makes perfect sense. So in some sense, indeed, a different
*  species out there. They'd become a different species. And of course, whether they would be
*  entirely flesh and blood, or whether they will by then be cyborgs or even downloads into something
*  electronic, we don't know. But if they become purely electronic, then of course, they won't
*  want to stay on a planet. And of course, if they're near immortal, then the interstellar voyage is no
*  deterrent to them. And so they will spread. And this has relevance in my view, to to SETI
*  projects, because, you know, I think obviously, the search for extraterrestrial biospheres is a
*  mainstream part of science. Now that we know that there are exoplanets out there, many like the Earth,
*  and within 10 or 20 years, we know if any of them have biospheres. But of course, what people really
*  want to know is, are there any intelligent aliens out there? And that's why there are these SETI
*  programs to look for extraterrestrial artifacts, or transmissions, which are manifestly artificial.
*  And I'm an enthusiast for modest expenditures on all these possible searches, because it's so
*  fascinating to all of us. But if you ask what I predict, I predict that if anything is detected,
*  it won't be any sort of flesh and blood civilization on a planet. It will be some sort of
*  maybe burping and malfunctioning artifacts, probably roaming in space. And the reason for
*  that is, let's suppose that there was another planet where things have rather like on the Earth.
*  And what's happened here on Earth is four billion years of Dominion evolution. And we're now in a
*  few millennia of technological civilization. But within a few hundred years, as we've discussed,
*  it may have been taken over by some sort of electronic entities, not flesh and blood.
*  And they, in principle, have a billion year future ahead of them, because they may be immortal,
*  and they can create copies or developments of themselves, etc. And so if there was another
*  planet where things had happened differently, it's unlikely to be synchronized to within a
*  few millennia so that we would detect something like ours. It's not at the same stage of technology.
*  So if it's behind by a few tens of millions of years, then of course we see no evidence
*  of intelligence. We see a biosphere. If it's ahead, then of course we would not detect anything like
*  what's now on Earth. But we might detect some technological artifacts or their emissions.
*  And so that's why if we do detect something, it's far more likely to be something like that,
*  detect some electronic artifact. Yeah, I personally think that the idea-
*  And you have to modify the Drake equation, because the Drake equation talks about the
*  lifetime of civilization, thinking about something with lots of independent fresh and blood entities.
*  Whereas if maybe one's super brain and maybe something entirely electronic, and that could
*  persist for billions of years, even if a civilization like ours can't persist for more than thousands of years.
*  I've often thought that the idea of taking a big radio telescope and listening in on the sky for
*  other advanced civilizations was a very, very, very long shot, because why in the world would
*  advanced civilization waste a bunch of energy being radio signals in random directions?
*  But I think it's sensible to do everything we can, as a byproduct of-
*  It's relatively true. Yes, yes. So I support their efforts to
*  look for radio transmissions, narrow frequencies, and look for optical flashes and things like that.
*  And we should also look for artifacts. We should look for evidence of some star that's
*  orbited by something that's artificially artificial. We should even look for artifacts in our solar system.
*  Something in the asteroid belt that's especially shiny, etc. On the moon 2001.
*  Yes, exactly.
*  I mean, one argument is that if intelligent life formed frequently, it would build
*  self-replicating robots, and it should have filled the galaxy a long time ago,
*  right? And this leads to the Fermi paradigm.
*  Yes. Well, of course, the Fermi paradigm is one important relevant factor about this. But I think
*  a reason why I don't think it's a watertight argument is that if these future entities are
*  electronic, it's not clear they will be expansionist, because we have evolved
*  by Darwinian selection, which favors intelligence, but also favors aggression.
*  And so that's why people talk about a sphere of interest expanding, etc., and the aliens that
*  have come here. But if the intelligences are electronic, then they may be entirely
*  contemplative. They may not want to expand. And so they could be out there without manifesting
*  their presence in an inconspicuous way and come here. So I don't think we can say that the galaxy
*  doesn't contain anything like that. We can say that it can't contain many civilizations which
*  have led to massive expansion, because some would have got here already. But I think if the scenario
*  is that advanced intelligence is electronic, then that need not be associated with expansionism.
*  The flip side of that, though, is I wondered about this out loud. Is it possible that we do
*  gradually or suddenly upload our consciousness into electronics, and then exactly because we
*  don't have all these thermodynamic survival instincts anymore, we stop caring? Even forget
*  about expanding, just existing is less interesting to us in that environment.
*  The survival instinct is no longer there. I think we as human beings probably want to stay as human
*  beings. And I think we'd like to feel that the earth won't change very much. That's why I'm happy
*  about all these things happening fast away from the earth, but preserving the earth as it is,
*  occupied by creatures who are adapted to it. So in addition to human beings, there would be
*  artificial intelligent electronic beings that we could spread throughout the galaxy.
*  We hope they leave us alone. But I think we would want to try and restrain the speed of these
*  changes. Because maybe we can't. Because maybe some group will get ahead and we'll have a very
*  disturbed world where human beings can't survive. And this is one of the dystopian scenarios,
*  obviously. But I think we should hope that things don't change too fast.
*  Your perspective seems to indicate that the Fermi paradox might not be a very big paradox,
*  and in fact that the galaxy might be teeming with different kinds of life. Is that fair?
*  Yes.
*  So what does that say to people who are thinking about the origin of life or the
*  frequency of life? So I know that there's certainly an idea that we haven't seen lots of aliens yet,
*  therefore the very beginning of life was a really, really, really unlikely event,
*  or the beginning of multicellular life was very hard.
*  Yes. Well, I mean, of course, if you take the Fermi paradox seriously,
*  then you could still be happy with the idea that simple life and a biosphere of lots of plants and
*  things could be widespread. And within 20 years, we will know that because we'll have a spectroscopy
*  of planets around nearby stars, which would be sufficient to tell us if there's a biosphere there.
*  But even if a biosphere of plants and simple organisms is common on Earth-like planets
*  throughout the galaxy, it could still be that there are other bottlenecks that stop it getting
*  to intelligence. So I think we can still hope even if we don't think that intelligent life is
*  widespread, that simple biospheres are widespread, because we don't know what the odds were against
*  the evolution getting as far as it has. I mean, if the dinosaurs hadn't been wiped out,
*  would there be an intelligence like ours that emerged by a different group? We just don't know.
*  Well, Europa or Titan might have biospheres, right?
*  Well, Titan might. And of course, the origin of life is a problem which everyone has known is an
*  important problem. But most people, most serious scientists have put it in this sort of too
*  difficult box and haven't worked on it. Evidence of that is that the Miller-Urey experiment done
*  in the 1950s where they put sparks through gas and got amino acids and all that. That experiment
*  was still talked about 40 years later. No one thought of doing much better experiments, whereas
*  now there are lots of serious top-ranking biologists who are thinking about the origin
*  of life and the stages by which it could happen. That's motivated by advances in microbiology,
*  but also, of course, motivated by realizing that there are other places in the galaxy where it
*  could exist. And so I'm hopeful from having talked to a number of these people that we will have a
*  plausible scenario for the origin of life within 10 or 20 years. And that'll tell us two things.
*  It'll tell us first, was it a rare fluke? But it will tell us also in answer to your question,
*  does it depend completely on special chemistry like DNA and RNA? Does it even depend on water
*  as a solvent? Because on Titan, of course, you'd have to have methane and not water. And I think
*  if we understood the origin of life, we would know whether whatever happened at the temperature
*  of the Earth with water could have happened on Titan or not. So we will know how rare the origin
*  of life is and what the variety of chemistries is that could lead to it. So we don't know that,
*  but I think in the next 20 years, we'll know about that. And we also might have evidence for
*  whether there is life. And of course, telescopes now aren't really powerful enough to be able to
*  detect a spectrum of a planet because an Earth-like planet is billions of times fainter than a star
*  it's orbiting. And to isolate the spectrum of that is very hard. So I think the James Webb
*  telescope might do something. But after that, the best bet is going to be the telescope the
*  Europeans are building, which is called the Extremely Large Telescope. We are unimaginative
*  in our magnitude. But this has a 39-meter mirror, which is not one sheet of glass, but a mosaic of
*  800 pieces of glass. And this will collect enough light such that with the right spectrograph,
*  it should be able to separate out the light reflected from the planet from the billions
*  of times brighter light from the star. So we can really see what happens.
*  As the planets are in different phases, then you change that. And so that would tell us something
*  about what's a chlorophyll on the planet and things of that kind. So that's something which
*  will be done within 10 or 20 years. Yeah, that's very, and just so everyone knows,
*  the Extremely Large Telescope is a ground-based telescope. It's ground-based, yes.
*  James Webb is in space. And obviously, it's an advantage in space, but you can have a much
*  bigger mirror on the ground. But incidentally, if you look further ahead, then talking about robots,
*  even if we don't send many people into space, we can have robotic fabricators assembling big
*  structures, either on the moon or maybe in space. And I think one exciting scenario is
*  fabricating one huge, very lightweight mirror under zero G in space, which might even be able to
*  resolve the image of an exoplanet. Okay. Not just to take the light, but see it as an extended object,
*  not just as a point. And I think, make point in my book that the target for this should be 2068,
*  which is a centenary of the famous picture, take my Ed Anders, Earthrise, which was the
*  classic iconic picture of the Earth. Right. And it would be nice if 100 years after that,
*  we had a picture we could put on our walls of an Earth-like planet elsewhere. And it would not be
*  crazy to have by that time, a successor of James Webb Telescope and of the ELT, which could be a
*  huge mirror up in space. How huge is huge? How big do you need for the mirror up in space to do this?
*  I think several hundred meters. Several hundred meters. Okay. That sounds feasible to me. What
*  do I know? But under zero G, it would not be impossible. That's right. Okay. You mentioned,
*  just to sort of start winding things up, we can get back to the ethical concerns a little bit,
*  because you did mention terraforming Mars. Yes. If human beings want to go up there and start living
*  there, there are people who will say that we shouldn't do that. Right. This is a whole part
*  of nature. Yes. And what do you think? Well, I think if there were evidence of life there,
*  then I think most people would feel we ought to sort of preserve that life, like a national park,
*  as it were. If there's no life at all, then I think we could be somewhat more relaxed about it.
*  Similarly about the moon. I mean, I think people talk about mining the moon. In fact,
*  Harrison Schmidt has this idea of really sort of doing a huge amount of open-cast mining on
*  the moon to find helium-3. And I think economically that doesn't make much sense, and probably never
*  will. But on the other hand, the question is, would we be relaxed about it? And I personally
*  would be fairly relaxed about getting materials from the moon in order to build structures on the
*  moon, preserving a few historic sites like where the Apollo astronauts landed and things of that
*  kind. But if there were a biosphere already, then I think most people would feel that one should try
*  and preserve it on Mars or indeed under the ice of Enceladus or Europa, which of course are now
*  thought to be perhaps the most likely places for life in our solar system. And of course, there are
*  probes already being planned to fly through the spray that's coming up, through cracks in the ice
*  there, to be able to do some analysis to see if there's complex chemistry there, and of course,
*  looking further ahead to send some submarine to go down and see if there's something swimming around.
*  So, Mindscape listeners should know we will be talking about exactly that.
*  But I think the reason that's important is, well, not only of its own important exploration,
*  but of course, if we could find evidence that life had originated twice independently
*  within our solar system, that would tell us straight away that life of some kind was widespread
*  probably in a billion places in our galaxy. It would be one of the most important discoveries
*  of all time. It would, it would indeed. But it's got to be convincingly independent. That's why
*  some people would say life on Mars would not be quite so convincing because people have said,
*  maybe we're all Martians, maybe life starts on Mars and then meteorites brought it to the Earth.
*  Or vice versa. Yes, whereas if it was on Europa, I think you couldn't make that argument,
*  it would be independent. And of course, if it was quite different chemistry,
*  chemistry was very different. Then that also would show independence. And so that's why
*  I think searches for even rather boring primitive life anywhere in our solar system are very
*  important because it would have such huge implications for the prevalence of life throughout
*  the entire galaxy. You've made bets before. What would be your bets about both the existence of
*  life elsewhere and our likelihood of finding it? Well, I think I would bet substantial odds on
*  finding evidence somewhere of life, whether it will be in Europa or Enceladus, moons of Saturn
*  or Jupiter, or whether it will be from observations of exoplanets like the Earth.
*  And of course, we shouldn't just look at things like the Earth. We should think, look at other
*  planets and other places. So I think it's not a crazy expectation. Whereas the idea of finding
*  intelligent life or artifacts, which are evidence that there was some sort of intelligent life.
*  Obviously, I'm in favour of pursuing research at a modest level, because it's so fascinating to
*  everyone. If people know I'm an astronomer, then the first question they ask is, are we alone,
*  etc. They all ask that question and they want to talk about it. If you don't want to talk to your
*  neighbour on the plane, you say you're a mathematician. Or a physicist. But even though I favour and
*  support modest searches for intelligent signals of some kind or artifacts, I wouldn't bet very
*  much on that. But it's so important that it's worth the search. Yeah, it's a good point. I'll
*  close by mentioning, I don't know if I've ever told you this, but some time ago, roughly 20 years ago,
*  you gave a public lecture in Chicago and I was in the audience. And so I was already a cosmologist,
*  physicist, giving lectures myself, but you were talking about cosmology and the state of cosmology.
*  And in the middle of the talk, you started talking about the possibility of life on other
*  planets and other solar systems. And as a professional scientist, of course, I was appalled
*  by this. I'm like, that's not cosmology at all. But I realised, of course, like you just said,
*  that is what people care about. And upon reflection, I gathered an important lesson from that, which is
*  that it's okay to talk about what people care about, not just what you as a professional are
*  supposed to be talking about. Yes, yes. But I think it's more than that. I think there are some
*  subjects which are so far from any kind of empirical test that it's not worth discussing them.
*  And they should be left in the too difficult box and left alone. And that may have been true in the
*  past of many of these issues. But I think exobiology is now a serious subject. And so I think no one
*  would regard it as a crazy, flaky, cranky subject, as they might have done in the past. That's right.
*  And the very, the very practice of contemplating the future seems to be a very different thing now
*  than it was 50 years ago. So it's interesting to see how our perspective has changed.
*  It has. But I think that when I'm asked, is there any special perspective that I bring to these
*  issues of the Earth's future, because I'm an astronomer, a cosmologist, it is simply an awareness
*  of the long term future because most people, unless they live in Kentucky or some Muslim areas,
*  are happy with the idea of where the outcome of four billion years of evolution. But nonetheless,
*  frame anything, we're the top of the tree, we're the culmination of evolution.
*  And I think that's something which no astronomer could believe, being aware of the huge ranges
*  of future time and extended space. It's a very good perspective to keep in mind. Martin Rees,
*  thanks so much for being on the podcast. Thank you very much.
