---
Date Generated: June 08, 2024
Transcription Model: whisper medium 20231117
Length: 5262s
Video Keywords: []
Video Views: 12358
Video Rating: None
Video Description: Patreon: https://www.patreon.com/seanmcarroll
Blog post with audio player, show notes, and transcript: https://www.preposterousuniverse.com/podcast/2021/12/06/176-joshua-greene-on-morality-psychology-and-trolley-problems/

We all know you can’t derive “ought” from “is.” But it’s equally clear that “is” — how the world actual works — is going to matter for “ought” — our moral choices in the world. And an important part of “is” is who we are as human beings. As products of a messy evolutionary history, we all have moral intuitions. What parts of the brain light up when we’re being consequentialist, or when we’re following rules? What is the relationship, if any, between those intuitions and a good moral philosophy? Joshua Greene is both a philosopher and a psychologist who studies what our intuitions are, and uses that to help illuminate what morality should be. He gives one of the best defenses of utilitarianism I’ve heard.

Bonus! Joshua is a co-founder of Giving Multiplier, an effective-altruism program that lets you donate to your personal favorite causes and also get matching donations to charities that have been judged to be especially effective. He was kind enough to set up a special URL for Mindscape listeners, where their donations will be matched at a higher rate of 100%. Check out https://givingmultiplier.org/mindscape.

Joshua Greene received his Ph.D. in philosophy from Princeton University. He is currently Professor of Psychology and a member of the Center for Brain Science faculty at Harvard University. His an originator of the dual-process model of moral reasoning. Among his awards are the the Stanton Prize from the Society for Philosophy and Psychology and Harvard’s Roslyn Abramson Award for teaching. He is the author of Moral Tribes: Emotion, Reason, and the Gap Between Us and Them.

Mindscape Podcast playlist: https://www.youtube.com/playlist?list=PLrxfgDEc2NxY_fRExpDXr87tzRbPCaA5x
Sean Carroll channel: https://www.youtube.com/c/seancarroll

#podcast #ideas #science #philosophy #culture
---

# Mindscape 176 | Joshua Greene on Morality, Psychology, and Trolley Problems
**Mindscape Podcast:** [December 06, 2021](https://www.youtube.com/watch?v=xAodsREx-N0)
*  Hello everyone and welcome to the Mindscape Podcast. I'm your host Sean Carroll. So the
*  trolley problem. You know, I swear that just a few years ago, the trolley problem was not
*  of widespread familiarity with people. When I wrote the big picture, I explained the trolley
*  problem and I thought that, you know, some experts knew about it, but it wasn't a widespread
*  cultural phenomenon, which it appears to be today. Maybe because of The Good Place, that TV show,
*  but I'm not sure. Anyway, you know probably by now the basic setup, right? You have a choice
*  between letting a trolley kill five people by running away on the tracks or by doing something,
*  saving the five people, but one other person dies because of what you did. And there's different
*  versions of the trolley problem where what you do is just flip a switch and then one person dies on
*  the tracks or you actually have to shove somebody in front of the train. And when you do psychological
*  quizzes and you ask people about this, they will give different answers on what the right thing to
*  do is depending on the specific action that is called for, even if the outcomes are the same,
*  which is very interesting. So the point of the trolley problem, which was first proposed by
*  philosopher Philip Afoot and then dubbed the trolley problem by Judith Jarvis Thompson,
*  is to heighten our clarity about the competition between two different ways of moral reasoning.
*  There's roughly speaking a deontological way. Deontological morality is based on rules.
*  Certain things are right, certain things are wrong. So killing someone is wrong, right? That's a
*  deontological maxim right there versus a sort of consequentialist version of morality that it's not
*  about the rules, it's about the outcomes of your actions, whatever they may be. And again, very
*  roughly speaking, deontology would warn you against making one of those actions that switches
*  the trolley from doing one thing to another, whereas consequentialism would say save the
*  most people, whatever that would be. So what's interesting is we all have both of these intuitions
*  and they do compete against each other. So one question you can ask is forget about what the
*  right answer is. The trolley problem is never about saying what the right answer is. It's about how we
*  reason morally and therefore you can ask what about what is literally going on in our brains?
*  And that is a question that has been tackled by today's guest, Joshua Green and his collaborators
*  and his lab. Josh is a psychologist, neuroscientist, and philosopher and he is interested in this sort
*  of deontology versus consequentialism debate but also interested in what is happening in our brain
*  when we are thinking deontologically versus when we are thinking consequentially. And it's different
*  parts of the brain that sort of light up and when you get into this it's just intrinsically
*  interesting but then it has enormous consequences for further questions not just about individual
*  personal morality but about tribal cooperation or competition, what Josh calls global metamorality
*  and so forth. So there's a fascinating conversation and there's a bonus that is coming here.
*  Josh thought about what he was doing and realized there were implications for charitable giving.
*  You know he's one of these people, there are many and including myself, who think that
*  there are more and less effective charities to give to but then there are also charities that
*  tug on our heartstrings directly. So he helped found a program called Giving Multiplier.
*  Giving Multiplier will let you have it both ways. They will let you both give to the charity that
*  really is personal and important to you while also spreading some wealth to those effective
*  altruist kind of charities that just have a huge impact as far as the consequences are concerned.
*  And here's the bonus. Mindscape listeners get a special benefit here. So they've set up,
*  Josh and his friends have set up a special code. If you go to givingmultiplier.org
*  mindscape they will match your donations to a higher rate than usual. So this is a good thing
*  to do no matter what but as a mindscape listener you get to do just a little bit more good for the
*  world than usual. So that's a nice thing. I like to think that listening to mindscape helps do a
*  little bit of good for the world because it helps us think about how the world works and make it a
*  better place. But here you can put a cash value on those ideas by giving to the right place.
*  That's an exciting little thing. So with that, let's go.
*  Joshua Green, welcome to the Mindscape Podcast.
*  Great to be here. Thanks.
*  I thought that we would start the conversation by sort of setting out our philosophical
*  convictions on the table here. We'll get more empirical later on but a lot of your empirical
*  work seems to be driven by interest in philosophical questions. So when it comes to morality and for
*  that matter meta-ethics, why we choose our ethical systems, what are your convictions?
*  Where do you come down on utilitarianism, deontology, virtue ethics, all those big questions?
*  Right. So I'll start with honoring you. I'll start in a big picture kind of way.
*  I think of morality first and foremost as a natural phenomenon. So I don't think of morality
*  as something that descends from on high, from outside the observable universe in a theological
*  way or in some other way. And not only that, I think of morality as something that comes up from
*  below. So a useful contrast here is with mathematics where it is plausible to think that
*  basic mathematical truths or complex mathematical truths in some way are completely
*  independent of human minds. That two plus two would equal four regardless of whether or not
*  any humans ever existed. Certain basic facts about the nature of entities and their relations to each
*  other. Those things just are that way and humans may accurately or inaccurately perceive them.
*  When it comes to morality, I think of it as a natural phenomenon, as I said, which arises
*  out of evolutionary processes. And the fundamental evolutionary development is the development of
*  cooperation. So cooperation is really the thing that makes life on earth. And if it exists
*  anywhere else in the universe, interesting. You can have a bunch of organic molecules like RNA
*  and they could just sit there in the primordial soup forever and never do anything. But what
*  makes things get interesting is when molecules start coming together to form larger molecules
*  and those form even larger ones that can make copies of themselves which form cells and so on
*  and so forth all the way up to multicellular creatures and animals and social animals and
*  the United Nations. And morality as I see it is a suite of psychological devices that enables
*  creatures like us to reap the benefits of cooperation. So we have, and to a large extent,
*  it's governed by our emotional responses. We have feelings that make us care about other people,
*  empathy or compassion. And so we can engage in a kind of teamwork that is helpful for survival.
*  We have emotions that make us react negatively to people who are not so cooperative. We can have
*  anger and disgust. We can be afraid of how other people might react to us, negative feelings about
*  our own behavior. And we can have positive feelings about other people's behavior. So
*  we have positive and negative feelings that act like carrots and sticks for ourselves and others.
*  And that's the basic implementation of morality. And it enables teamwork. It enables us to survive
*  in ways that we couldn't if we were all just out for ourselves like sharks.
*  All this sounds very much like what a scientist psychologist would say. And so if I'm part of my
*  job here, my role in the podcast guest is to be the gadfly and push back a little bit. So
*  yes, all that is telling me that humans behave in a certain way and why they behave that way
*  from an evolutionary biology perspective. But it doesn't quite yet tell me that they should behave
*  that way. Is that a reasonable distinction? Yes, absolutely. And I like to start with it is
*  because it sets up the odd, right? That what you're going to end up saying about the odd,
*  I think, depends a lot about your broader picture of the universe. And so we're kind of on the same
*  page there, I think. So when it comes to let's skip meta ethics for a second and talk about
*  ethics, right? So the fundamental moral problem for humans in general is the basic problem of
*  selfishness versus caring about others, right? And that's basic morality, those emotional responses
*  that I described and the structures built around them solve the problem of me versus us, right?
*  The modern moral problem is a problem of us versus them. It's a problem of not otherwise
*  selfish individuals trying to get along in a group, but groups with their own interests and
*  their own values, trying to get along with other groups. And so if morality is the solution to the
*  basic moral problem of me versus you or me versus us, we need something to solve the problem at a
*  higher level, right? And so I think of that as a metamorality, that just as morality enables
*  individuals to live together as a group and cooperate, a metamorality is something that
*  enables groups to live together productively in a larger, more complex and more cooperative world.
*  So the way I think of modern ethics is what should our metamorality be? And
*  my view is that the 19th century utilitarians got this right. I really don't like the term
*  utilitarian, because I think it makes you think of parking garages and it makes, you know,
*  and when you try to explain it in terms of maximizing happiness, it makes you think of
*  smiley faces and things like that. I prefer to describe myself as a deep pragmatist. And I think
*  that that flows naturally from this conception of, okay, let's start with the practical problem of
*  different groups with different values and different interests. How do we resolve that
*  problem? Now, one way to resolve that problem would be to appeal to some universal objective
*  moral truth. And my view is that there probably isn't such a thing, but even if there is, we have
*  no reliable access to it, right? So instead we have to be pragmatic about it. So for practical
*  purposes, I am not a moral realist. That is, maybe there is some abstract moral truth that
*  are akin to the truths of mathematics, or maybe there's a theological version of it,
*  and we just haven't figured out which of the human versions of theological morality is right,
*  or if any of them are. But for practical purposes, we're left with what we can observe.
*  And so then the question is, well, yeah, to resolve our differences, we need to make trade-offs.
*  Well, to make trade-offs requires having some kind of common currency. If we're going to look at you
*  and your groups and their values and say, we want abortion to be illegal, and the other one's like,
*  we think it should be legal, right? To make an all things considered decision about how that ought
*  to play out means to put all of those considerations onto a common metric. Now, you might say that's
*  unfair, that's unreasonable, that's not how my people think about this, but that is what
*  decision making requires. You either have to go one way or the other way. And that requires some
*  kind of common currency. And I think that the best common currency that we have is the common currency
*  of experience. And so the fundamental insight of consequentialism and utilitarianism more specifically
*  is, well, it's really putting two things together. One is that when you think about the things that
*  you care about or that other people care about and keep asking, why do you care about that until
*  you run out of answers? It ultimately almost always comes down to the quality of somebody's
*  experience. So I say, Sean, you're here at work today. Why'd you go to work? And you say, well,
*  I enjoy what I do, but I also need to pay the rent. And you're like, well, why do you need to pay
*  the rent? Why don't you just wander around? You're like, well, this year I'm in Boston,
*  it gets cold outside. And I said, well, what's wrong with being cold? Well, it's painful. And
*  you said, well, what's wrong with pain? And then at that point you'd say, it's just bad. You run
*  out of answers. And so the thought is that when you ask those, why do you care? Why do you care?
*  Why do you care? It ultimately comes down to the quality of somebody's conscious experience,
*  whether it's yourself or somebody else or a human or an animal. And we don't worry about rocks,
*  because rocks, as far as we know, are not conscious and nothing is really good or bad for them.
*  So that's insight number one, is that the quality of experience provides this
*  common currency of value. But then you say, well, okay, but who matters? So, okay, so that's
*  what matters to you, but who really matters? And utilitarianism's insight, which sounds banal now,
*  but really was quite a dramatic advance in the 18th and 19th century, is everyone matters equally.
*  And not even just humans mattering equally, but even animals. So, Jeremy Bentham, the original
*  utilitarian, famously said, we're asking the wrong question about animals. It's not a question of
*  whether they're as smart as us. The question is whether they can suffer. And their suffering is
*  just as much suffering as our suffering. And as a result of this, people like Bentham and Mill were
*  way ahead of the curve on things like animal rights. They were early opponents of slavery.
*  Jeremy Bentham even penned one of the first defenses of what we would now call gay rights.
*  And he said, you know, I know that everywhere around here, people think this is the most
*  terrible thing in the world. But when I try to break it down in terms of the quality of human life,
*  I just don't see why this is so bad. So maybe it's not. And with that thought, in my view, leapt
*  ahead two centuries in moral thinking. So if you take this idea that the quality of experience
*  certainly matters and is our best common currency, and the idea that everybody's experience counts
*  the same, not that everybody has the same experiences, not that everybody has the same
*  culture, not that everybody values the same things, but that nobody is inherently more important
*  because they're them. Right. Then you get this idea, okay, well, what our metamorality, the most
*  sensible metamorality is try to make the world have as little suffering as possible as most
*  happiness or well being as possible, taking into account the suffering and well being of all
*  sentient beings equally. And that that is the core of it. And when you and when you try to live that,
*  I think what you end up being is what I call a deep pragmatist. That is, you are mostly focused
*  on evidence, on trying to figure out what's the best way to do this, working with human nature as
*  it is and not as it ought to be, and aiming for this higher ideal, but very much engaged with the
*  practical details of the world. That's my ethics. Donating money to help people can be a wonderful
*  and selfless act, but how can you feel confident that your donations are improving or saving lives
*  effectively? You could do weeks of research to find the charities that are out there, what programs
*  they run, how effective those programs are, and how the charity might use your money. Or you could
*  visit givewell.org. There you'll get a short vetted list of the best charities they found at saving
*  or improving lives per dollar. GiveWell spends over 20,000 hours each year researching charitable
*  organizations and only recommends a few of the highest impact evidence-based charities they found.
*  Over 50,000 donors have used GiveWell to donate more than $750 million.
*  Rigorous evidence suggests that these donations will save tens of thousands of lives and improve
*  the lives of millions more. And if you've never donated to GiveWell's Recommended Charities before,
*  you can have your donation matched up to $250 before the end of the year or as long as matching
*  funds last. To claim your match, go to givewell.org and pick podcast and enter Sean Carroll's Mindscape
*  at checkout. Make sure they know you heard about GiveWell from Mindscape to get your donation matched.
*  I wish you luck in the rebranding program, trying to convert utilitarianism to deep pragmatism.
*  And I very much am on board with both the focus on suffering and conscious experience and the
*  fact that other people and other kinds of experiences matter. I do want to push just a
*  little bit before we get into the empirical stuff on utilitarianism as even the version of
*  consequentialism that you might need or even that you need consequentialism. I mean, there are some
*  standard worries about utilitarianism and I wonder how you address them. The basic flavor of the
*  standard worry is it sounds like you could destroy one person's life if you make 100 people's life
*  better. And I can imagine a completely cosmopolitan global metaethics that nevertheless
*  said every individual has some autonomy and some rights and I cannot sacrifice the good of that one
*  person even if it would make 100 other people better. So is that, does utilitarianism emerge
*  as a sort of unique conclusion in your mind from hoping that we can find a fair global metaethics?
*  Yeah. So I think that it's important to look at this in multiple levels and I think this actually
*  goes all the way back at least to John Stuart Millen has been a recurring theme and other
*  utilitarian thinkers, most notably R.M. Hare. That is, we don't want people going around
*  thinking that it's their prerogative to sacrifice some people for the greater good of others.
*  Why? Because that would be bad because the world would end up worse off if that was our first level
*  basic disposition, right? And this is part of what I think is the pragmatic element here is
*  we have to understand the biases and limitations in human nature, right? That people are very
*  naturally going to say, oh yeah, the greater good. That's what works well for me, right? What's good
*  for America is good for Ford Motor Company if you're the CEO of Ford, right? We're likely to be
*  biased and we're also likely to overestimate our ability to predict things and our ability to
*  control things. So it is a very bad general idea to have fallible, biased, limited humans
*  in general going around trying to do this sort of engineering where they're sacrificing some people
*  for the benefit of others. That's just a generally okay mode of operation. But now you might say,
*  well, that's kind of, aren't you sort of being a little squirrely here? I mean, but you just said
*  you had to promote the greater good and then you just said, but we shouldn't go around thinking
*  that we're going to promote the greater good. But you have to ask yourself, do you think the world
*  would actually be better if people thought like that? And surely you're thinking the answer is no,
*  right? Well, if you're entitled to think in a sophisticated way about the limits of human nature,
*  why shouldn't you do utilitarian, right? So we want to get off the table the idea that in general,
*  this is a good way for people to think. Now, with that said, in our more reflective and sober moments,
*  do we sometimes have to make tradeoffs? Suppose that there really is an enemy threatening our
*  nation, you know, this is World War II and Hitler's regime is working towards the nuclear bomb, right?
*  Is it then acceptable to conscript some people to fight in a war to keep Hitler from taking over
*  the world? That's sacrificing the interests of some for the greater good, right? And there are
*  many examples of things like this, although they don't usually involve such a kind of direct
*  sacrifice. So in situations where if you really have to choose, if it really is clear that if the
*  interests of some are not sacrificed for the interests of a larger number, then it would be
*  acceptable to do that. But it's very dangerous to have people going around thinking that way
*  in general. And in practice, thinking in a more utilitarian way is much more likely to be about
*  giving of yourself for the benefit of other people than it is about sacrificing other people for the
*  benefit of other people. Okay, no, actually, I mean, that's a very good point. And I think that I
*  appreciate something that I didn't appreciate before, even having talked to you before about
*  this stuff. So thank you. The pragmatist aspect here softens or at least ameliorates some of the
*  more worrisome thought experiments about utilitarianism gone wrong. And you're right about,
*  you know, it would be contradictory to say that utilitarianism leads to a result that is actually
*  making people worse off because that's the whole point. If you do it right, it's not supposed to
*  do that. But let's let's contrast it here. I think in my book, the big picture for which I
*  interviewed you a while ago, I drew the conventional contrast between consequentialist ethics and
*  deontological ethics, which are based on rules rather than outcomes. Subsequently, several people
*  have convinced me that I should also put virtue ethics as a third alternative on the table there.
*  How do you think about those two other options? Well, let me start with virtue ethics, because I
*  think it's it's clear. My answer is clear. I don't think virtue ethics is really an answer.
*  That is maybe you should say what it is to the people who don't. Okay, so virtue ethics sort of
*  is in the Western tradition, it's most closely associated with Aristotle. But really, I think
*  that virtue ethics is kind of the default ethics of human cultures throughout history. So what
*  the Aristotelian idea is, he's not giving you a general theory that says, here's what's right,
*  and here's what's wrong, and why. Instead, he is describing and in some way endorsing certain kinds
*  of moral practices and encouraging people to engage in them. So we'd say, look, it's all about finding
*  the right balance. You don't want to be a coward in your life and unwilling to stand up to the
*  things that threaten you, but you don't want to be rationed, you know, rushing into danger without,
*  you know, protecting yourself, you have to find this in the golden mean and in the middle.
*  And with all of these things, with all sort of matters of human life, it's a matter of balancing
*  these competing considerations and having the virtues which achieve that balance. How do you
*  achieve those virtues? Well, you kind of look at the people who seem to be doing a good job,
*  and you try to emulate them, and you try to practice. You don't just get this from theory,
*  you get this from building habits and making mistakes and learning from those mistakes and so
*  on. And I think all of that is right in a sense, but it doesn't tell you whether or not abortion
*  should be illegal. It doesn't tell you when it's justified or not to invade a foreign nation.
*  It doesn't tell you how much we should be doing to protect future generations from climate change.
*  It's just, it's a nice approach for life in a tribe, right? Where everybody kind of agrees
*  on what's right or wrong, and it's a matter of trying to live up to those standards. But in a
*  multi-tribal world, it doesn't really tell you very much because whose virtue? Is it Vladimir
*  Putin's virtues as a powerful strongman leader? Is it Barack Obama's virtues as a thoughtful,
*  progressive universalist? You just end up having a debate about which virtues to implement. So it's
*  a very natural and comfortable, there's not much to object to there. There are devastating
*  counter examples to it, right? But that comes at the cost of not really giving you a way to answer
*  the question, right? But that's my general take on virtue ethics. When it comes to deontology,
*  this is where I've really, you know, it's motivated a lot of my scientific research.
*  You could go on for hours and hours here, but I'll try to give the very short version.
*  The idea of deontology is that there are certain actions that are, or types of actions that are
*  inherently right or inherently wrong. That which must be done, their duties, that which must not be
*  done because they're wrong or they violate people's rights to put it in more modern parlance.
*  My view of deontology is that it is, as a philosophy, is that it's an actually a
*  rationalization of our moral emotions, right? This comes out most clearly in moral dilemmas that I
*  spent a lot of time discussing that fall under the heading of the trolley problem. These days,
*  the trolley problem has kind of become a meme where it's more of a platform for saying, well,
*  would you save your high school teacher if it meant killing five dogs or something like that?
*  To me, that's not the original interest of the trolley problem, just as a platform for trade-offs.
*  The interesting thing is contrasting different cases. If you ask people,
*  is it okay to hit a switch to turn the trolley away from five people and on to one? Most people
*  say that that's okay. If you ask people, is it okay to push somebody off of a footbridge
*  so that they land on the tracks, they get killed, but it'll stop the trolley from killing five
*  people? Is that okay? And most people say no. What we've learned from a lot of research over
*  the last 20 or so years is that when people say no there, it's largely driven by an emotional
*  response. It's an emotional response that depends a lot on the physics and mechanics of that action.
*  When you push somebody off the footbridge because it's active rather than passive,
*  that is an important part because you're harming the person as a means. It's part of your goal
*  as opposed to as a side effect. You're not just turning the trolley and then it happens that this
*  other person is killed as collateral damage. That's important. And because it's direct,
*  if you push somebody, that actually feels worse than if you hit a switch that would
*  drop them through a trap door onto the tracks. And these different features combine and make us
*  have a negative emotional response. And there's now good evidence that this comes from the basic
*  mechanisms of how we learn and in particular reinforcement learning of the kind that is used
*  in deep reinforcement learning in neural networks and of the kind that we see operating in animals
*  such as rats and monkeys and humans. And basically what's going on there is we learn probably in
*  toddlerhood that it's not okay to do things like pushing people. And so we have negative reactions
*  to those things. So that's empirically what's going on in the footbridge case. Now philosophically,
*  what's going on there? Well, a lot of philosophers look at that and say, aha, this explains exactly
*  what's wrong with utilitarianism. Sure, you could save more lives, but you're violating somebody's
*  rights when you push them off the footbridge. You're using them as a means and so on and so forth.
*  Well, why aren't you violating somebody's rights when you turn the trolley away from
*  the five and onto the one? I think that what's going on with deontology and particularly in
*  Kant is it is an attempt to put a rational gloss on our feelings. And it's not that those feelings
*  are bad, but those feelings are not infallible. Those feelings are adapted to certain kinds of
*  environments and questions, but don't necessarily generalize well to other environments and questions.
*  And this is most clear when we look at the kinds of effects on our feelings that are very hard to
*  endorse. It's more morally acceptable to trade one life for five if you end up doing the
*  sacrificing of the one by pushing that person rather than by hitting a switch. No one thinks
*  that's morally important, but our feelings think, so to speak, that it's morally important.
*  This isn't just about trolley cases. So someone recently asked me about what's going on at
*  Facebook and their failure to rein in misinformation and planning of violent activities
*  leading up to January 6th. And that's very much a real world thing. Why did Facebook not act on
*  that? Well, I would suggest, and this is, I should say, a lot of this really important research was
*  done by my colleague, Fiery Cushman. So I'm going to just credit him with that. If Mark Zuckerberg
*  had to storm the Capitol himself or stand there in front of a right-wing group of migraine
*  militants and hand them guns or ties to tie up Nancy Pelosi and Mike Pence, he wouldn't do that,
*  I don't think. Whatever limitations he may have as a moral person, I don't think he would do that.
*  That is active, direct, pretty direct, and intentional. But instead, the problem that
*  Facebook faced was preventing harm where their role is passive. And it's not very direct. And
*  they're not trying to cause these problems. It's a side effect of their running their businesses
*  as profitably as possible. And so these things that we've learned... So in the Footbridge case,
*  I think we're overreacting. That is, we have more of an emotional response because we've taken this
*  paradigmatically bad action, pushing somebody, something you learn not to do as a toddler,
*  and then artificially attaching it to the greater good. And now it becomes an objection to
*  utilitarianism. In the case of Facebook, it's the opposite. We have this enormously consequential
*  action that is a set of behaviors or choices that's eroding American democracy. But it doesn't feel
*  like pushing somebody off of a footbridge because it doesn't have those basic physical qualities.
*  So you asked me about deontology. I think the problem with... I think that as a philosophy,
*  what it's essentially doing is just trying to justify whatever we feel. And that's not bad for
*  everyday life if it stops people from lying and stealing and cheating. But if we've got difficult
*  moral problems to solve, just rationalizing our intuitions isn't going to help because people
*  have different intuitions about what's right or what's wrong. And different groups have different
*  interests. Now look, if the deontologists ever managed to hit the bullseye, if they could deduce
*  from self-evident first principles what's right and what's wrong, then I would pack up and I
*  would say, okay, you're right. You've proved it. But that hasn't happened. And I don't think
*  that's really what it's about. I think it's about trying to tell a rational story about feelings
*  and that we're better off taking our feelings for what they're worth, but being able to transcend
*  them when necessary. And I think that that's what deep pragmatism does. Well, this leads beautifully
*  into the more empirical side of things because you've already alluded to it. But this idea that
*  there's sort of a... I think probably most people in their informal morality have deontological
*  aspects and consequentialist aspects and probably some virtue ethics aspects in there. But you and
*  your collaborators have been able to say a little bit more about literally what is going on inside
*  the brain when we have these competing impulses. So say more about that. Yeah. So take our responses
*  to cases like the Footbridge case. In one experiment that we did, we gave people cases
*  like that and we asked people to say, don't tell us what's right or wrong here. This is work done
*  with Amitai Shenhov, who's now a professor at Brown. It said, don't tell us what's right or wrong,
*  just tell us how bad do you feel about this action? And their ratings of how bad they feel about it
*  correlate with activity in a part of the brain called the amygdala. And the amygdala is
*  part of a kind of general evaluation system that is especially active when it comes to making a
*  sort of rapid assessment of something. So if you're walking along and you see a stick,
*  but it sort of looks like a snake and you kind of have that feeling where you sort of shudder
*  before you even realize what you're looking at, that is your amygdala detecting, responding to
*  the pattern that's been detected and saying, hey, pay attention to this. This could be bad.
*  And your amygdala plays a role in reinforcement learning. If you are learning to associate an
*  electric shock with a blue square, when the blue square comes on, your amygdala is going to respond
*  before the shock comes. So this is part of our basic learning mechanism here. And we have good
*  reason to think that, and this is again, Fiery Cushman's work, that it's that basic learning
*  mechanism that's responsible for that emotional response. Some really lovely studies done by
*  Fiery and Indra Patel had people do two different kinds of tasks. So one is this task where people
*  are just playing a game where they kind of make choices where they take you essentially to
*  different rooms and then one room can lead to another room. And you can get to a reward,
*  and the levers that you have that take you from room to room, they have different probabilities
*  of taking you to different places. So one way you can end up getting a reward is by getting lucky.
*  That is, you hit a couple of switches that with a low probability get you to what you want. And if
*  you understand the map, you might say, ah, there's a reward there, but the more reliable way to get
*  there is with a different route. Right? So in one case, you're using a map, you're using a model,
*  this is called model-based reasoning. In another case, you just be relying on your habits. You're
*  saying, well, when I hit this lever and then hit this other level, I got the cheese, I got something
*  good. So I'm going to keep doing that. And you can look at the extent to which people in this game
*  has nothing to do with morality, rely on their trained habits versus rely on their knowledge of
*  the map. And it turns out that if you're more map-like, when you play this multi-stage movement,
*  navigation game, you are more likely to have a utilitarian pattern in your moral judgment.
*  So it really is getting at this basic mechanism where you can see these mechanisms operating in
*  humans navigating mazes and in rats navigating mazes. And it's the same neurobiology. It's a
*  really pretty beautifully tight connection there. So where does the feeling come from and where do
*  we see it in the brain? Now, if I give you a footbridge case and I say, okay, don't tell me
*  what's right or wrong, just tell me which action will produce better results, then you'll see more
*  activity in a part of the brain called the dorsolateral prefrontal cortex, which is responsible for
*  conscious explicit reasoning and also in different ways, cognitive control, implementing a rule or a
*  policy and overriding competing influences. If I say, okay, now I want you to make an all things
*  considered judgment, taking into account everything about the footbridge case, you'll see more
*  activity in a part of the brain called the ventromedial prefrontal cortex, which is the part
*  that was famously damaged in in Phineas gauge. If that's a sort of familiar reference point,
*  this was the 19th century railroad foreman who got a spike through his forehead. And as a result,
*  kind of had his moral character dramatically change, even though his ability to use language
*  and reason was preserved. And what the VMPFC seems to do is serve as a hub for signals that
*  bear on decision making, that that's where all of those signals come together. So you've got the
*  amygdala saying, ah, don't do that. And you've got your dorsolateral prefrontal cortex saying,
*  well, but isn't five lives saving five lives better than one? Yeah. And they kind of do get out there,
*  right? And that's, that's the basic picture of what's going on. There have been nice studies
*  showing how you can move these judgments around pharmacologically, other studies showing how
*  different kinds of brain damage lead to different patterns of judgment. So if you have damage to
*  the ventromedial prefrontal cortex, you're far more likely to say that it's okay to push the person
*  off of the footbridge because the amygdala signal can't get out into decision making. But if you
*  have other kinds of damage, then you can have other effects. So we can we can relate these things,
*  you know, pretty clearly to different circuits in the brain that are not specific to morality,
*  and that place similar roles in other domains for humans, and in those same other domains for
*  other mammals. Yeah, I mean, there's an obvious analogy connection or even equivalence here to
*  Daniel Kahneman's famous thinking fast and slow dichotomy where you have system one underneath
*  the surface. Is that right? System one is unconscious underneath the surface stuff.
*  And system two is the more cognitive, rational, deliberative part. And so there's morality,
*  fast and slow, right? That is yes. And that is, in fact, that is the title of one of the
*  subdivisions of my book. Make it up. Yeah. Yeah. So I should have I should have mentioned that as
*  a reference point, but absolutely. And and what fiery has argued is that this distinction between
*  model free learning and decision making and model based learning and decision making,
*  which has been really important in computer science and artificial intelligence, and has also been
*  important in dissecting neural circuits in humans and other animals, that that is really the essence
*  of system one system two, or as I've called dual process, psychology, that it's about
*  two different ways of attaching values to actions, one by attaching values directly to actions,
*  and the other by attaching values to actions via an understanding of the of the causal connections
*  with consequences that you care about. A critical point in here is that this applies not only to
*  actions out in the world, but to mental actions, right? That that that when you when you engage in
*  reasoning or thought, the patterns that you follow are patterns that have been reinforced,
*  right? That you have habits of thought in the same way that you have habits of action.
*  And that explains things like, you know, why why why we get certain tricky math problems wrong.
*  This is I'm thinking of the cognitive reflection test, if that's familiar.
*  That is, we have certain patterns of thinking reinforced that get us to to to miss the boat.
*  And let me let me just push back a little bit on not even push back, but
*  ask about how clean this distinction is a little bit in the morality fast and slow case, because
*  our buddy David Hume explained to us that reasons are the slave of the passions. And even in this
*  more deliberative cognitive way of being moral, which you say sort of affiliates with utilitarian
*  impulses, there still is some judgment about what is the greatest good like where that I mean,
*  you can be instrumental, but you need to have a goal from somewhere. So is it more like teamwork,
*  or is it really two alternatives? It is, there are two different algorithms. But both algorithms
*  involve some kind of evaluation, as you said, right? So you can do things in a more model based
*  or a model freeway and more system or a system two way. But there's but there's but some of some
*  evaluation, as you said, I think you hit the nail on the head is that in model based reasoning,
*  system two, you have to be attaching some values to the outcomes, right? And so that that is the
*  fundamental distinction. It's not is emotion involved or not, but rather it's are you reacting
*  emotionally to a particular kind of action in a particular context or a particular category of
*  action? Or are you reacting to the value of the ultimate goal or goals that are to be
*  gained or sacrificed, right? And I think this was appreciated early on by the utilitarians,
*  but Henry Sidgwick, who's sort of the least popularly well known, but the most systematic,
*  I mean, of three originals, Bentham, Mill and Sidgwick really laid this out nicely. He
*  distinguished between three different levels of intuition and what he called perceptual, dogmatic
*  and philosophical. And perceptual intuition is something like you see somebody do something and
*  you just have a sense that it's wrong, right? Or you imagine some particular action, you have a sense
*  that it's wrong. Dogmatic, or what you could in a less loaded way, you could call it categorical is
*  a reaction to a certain category of action. Oh, that's a lie. Therefore it's wrong, right?
*  Whereas if you categorize it as strategic misdirection or something like that, then he's
*  like, oh, that's not so bad, right? Branding. And then a philosophical intuition is really about
*  attaching value to consequences. So the philosophical intuition is pain is bad, suffering is bad,
*  happiness is good, right? And that is not about a particular case. It's not about a particular
*  action. It's a judgment about ultimate value, right? And so I think you're, so I agree with
*  what you said that it's not that it's emotion here and reason there. It's about different
*  relationships between emotion and reason. Are you using your reasoning in the service of an end
*  that you have ultimately on some affective basis decided is good or bad? Or are you
*  relying on your emotion to sort of pass judgment on a specific action, independent of its
*  consequences? And that can be useful and efficient in certain ways. There are certain things we don't
*  want you thinking about whether or not all things on balance, it's better or worse if he's shoplift.
*  Well, the CBS really need the money more than me, right? In some ways you want to have
*  feelings that just say, no, you can't do that. That's the kind of thing we don't do, right?
*  And a utilitarian can agree with that because the world is better off if we have those feelings,
*  right? But when it comes to our ultimate values, I think it makes more sense to think about
*  consequences. Okay, but yeah, the the CVS examples are really good one. It clarifies something that
*  you said earlier about the pragmatic aspect of deep pragmatism here, because it's almost like
*  you're saying that if you're a super conscious utilitarian, you become deontological in some ways,
*  because the way to get the greatest good for everybody is if everyone acts well and, you know,
*  obeys some sort of social rules. Is there some future reconciliation here? Right. So the
*  reconciliation is this. When it's life within the tribe, when it's basic everyday interactions,
*  when it's mostly at stake is my benefit or yours? Do I get the money or do you? Do I get the money
*  or to CVS? Right? If it's me versus us, that's when you most think fast. You want to trust your
*  gut reactions when it's about serving yourself versus following the rules or serving others.
*  But the metamoral problem is when we don't want to trust our gut reactions, because my tribe has
*  its feelings about what's right and what's wrong. But so does your tribe, right? And we if we all
*  just rely on our feelings about what's right or what's wrong, then we all, you know, it's nuclear
*  war. It's we all perish. Right. So our feelings are pretty good for basic morality, but they're
*  really bad for complicated, high level intertribal morality. And that's when we need to think slow.
*  And that's really the if you ask me to nutshell my book, Moral Tribes, that's the nutshell,
*  for practical purposes, is that in everyday life, when it comes to basic matters of right and wrong,
*  being a moral person, follow your intuitions and don't overthink it. Right. But when it comes to
*  the moral issues that divide us, we need to think more. And we need to step back from our moral
*  intuitions, because it's our moral intuitions that have us at each other's throats in the first place.
*  I think maybe what I'm saying is something a little bit different because you I think very
*  plausibly started by drawing a connection between deontology and our feelings, as you put it,
*  you know, our quick moral reactions. But what I'm asking in this, I'm literally just making
*  this up right now. So I'm sure it's nonsense. But what I'm asking is, given that we want to be
*  pragmatic utilitarians, if that's what you want to be, isn't it plausible or at least conceivable
*  that the way that plays out in the real world, by thinking really, really cognitively about how to
*  make things best for everybody, is to make up a set of rules that people should follow, even if
*  those rules don't map on to directly our feelings, but rather than telling everyone to do a calculus
*  to maximize utility, just say, here are the rules for right and wrong. Yep, that's right. And, and,
*  and, and this has actually been a persistent and persistently ignored theme from John Stuart Mill
*  on, right, that, you know, Mill says, look, our virtues have value, and we should cultivate them,
*  right. And it's, it's only at the sort of higher levels that we need to gain be in the explicitly
*  utilitarian mode. And it would be unutilitarian to do otherwise, because things go worse when we
*  ignore the basic everyday rules, right. But then there are times when those rules maybe need to be
*  reconsidered. So you can have everyday rules about gender roles, right, where, you know,
*  the everyday rules, if you've watched, you know, if you've seen, you know, Fiddler on the Roof,
*  you know, tradition, then who does this, and who does that, right. And you don't have to,
*  you don't need a musical to tell you about, you know, right, that, that those are aspects of
*  everyday life that maybe we want to question and reconsider, right. So, you know, it's, and there
*  are other domains as well, you know, what we choose to eat for, for food, for example, is something
*  where if we just rely on our intuitions, of course, it's fine to, you know, eat a hamburger,
*  even if it's coming from, you know, meat from animals that were tortured on their way to
*  production, right. And that are, you know, worsening our climate crisis, you know, we need to
*  be willing to rethink some everyday things as well. Yeah, this is extremely helpful. You're,
*  you're reinvigorating my youthful enthusiasm for utilitarianism that I was backsliding from for a
*  little while. But, and it also leads directly into what I wanted to talk about next, and again,
*  which you already alluded to, which is that the sort of switch of moral thinking when we start
*  thinking about these global problems, these inter tribal problems, maybe the place to start
*  is with the tragedy of the commons, because I know that you've talked about that a lot,
*  and how that plays into our moral ways of thinking. Yeah, so this comes back to something
*  earlier that is, you know, the tragedy of the commons is that the story comes from
*  Garrett Hardin, who, you know, in the late 60s was worried about overpopulation. And he gave the
*  analogy of, you know, herders with too many animals on their fields, and then they're all
*  the grass is gone, and all the animals end up dying, right. And we need to restrict the size
*  of our of our herds, or else everybody's animals are going to die. And he was imagining that humans
*  that, you know, we're going to overpopulate and destroy their resources. But it's a nice metaphor
*  for the general me versus us problem, right. And, as I said, you know, we have our, our,
*  our behavior on the commons is governed by basic feelings, right, that is, if I care about my fellow
*  herders, then I'm not going to, you know, sneak in some extra sheep and take more of the grass,
*  right. And I'll be angry at other herders if they do that. And I'll be grateful to other herders who,
*  you know, restrict their their herd for the common good. And I'll feel bad about myself if I even
*  contemplate doing this, right. So we have these positive and negative feelings that we apply to
*  ourselves and to others that get us to be able to live sustainably on the commons. Now we face these
*  global commons problems related to things like, you know, climate change, right. And it's complicated
*  because it's not just a bunch of individuals with personal feelings for each other. Instead, it's
*  groups with norms and philosophies, and with hierarchies, and with some individuals in the
*  hierarchy, maybe they can benefit from by saying to hell with the planet, because if you're make
*  your livelihood selling products that produce a lot of carbon, you might just say, you know, I can
*  move to wherever if things get a little too hot where I am, but I sure am raking it in now, you
*  know, selling petroleum products, right. So and then those people can try to influence people and
*  make them think that it's a terrible violation of your freedom and your dignity and your autonomy.
*  If you're asked to make any kind of sacrifices for the sake of the planet, or even to vote for
*  legislation that would rein some of these things in, you know, not at the level of consumers,
*  but at the level of, you know, efficiency standards for cars and investing in green energy
*  and stuff like that. So the basic feelings that we have as sort of herders on the common pasture in
*  everyday life, so to speak, they get they don't transfer up to the larger problem. And that's
*  where we need to understand the problem and make choices that are going to produce good consequences
*  instead of good feelings. Right. I think the way that you put it, which was very vivid and helpful
*  was that, you know, we climb the evolutionary ladder, and then we kick it away, right? Yeah.
*  And that's from some point that is ingrained into us in a very basic way. And we can sort of
*  take in our values from that process without necessarily giving into the first impulses
*  that that process leaves us with. That's right. Yeah. And well said. I'll leave it at that.
*  Well, I'm quoting you. But what it means is, well, so I guess I should be asking you,
*  what does it mean when push comes to shove? You did this, you have this nice data from experiments
*  where you've asked people to play the public goods game, right? And they're asked to sort of
*  give money away. I was surprised both by the average result and by the differences in results
*  under time pressure and not. Yeah. So in that original study, this was work done with Dave Rand
*  and Martin Novak. We found that putting people under time pressure made them more likely to do
*  the us-ish thing to sort of to contribute to a public good rather than doing the selfish thing.
*  It turns out it's complicated. There's been a lot of research on this using a lot of different
*  methods. And it seems the short and long of it, from my understanding, I haven't followed the
*  whole sort of almost decade of literature that's followed this, is that it really depends a lot on
*  your cultural experience, right? So if your dominant response is to be cooperative, because
*  you've lived in a world that rewards cooperativeness with strangers, right? Not just with your friends
*  and family, then that's going to be your dominant response. But if you've lived in a world in which
*  you really can't trust strangers very much, and where your response, when you're put in this weird
*  situation of cooperating with a bunch of people you don't know in this sort of sterilized,
*  anonymous context, then it can even go the opposite way. And I think this was nicely sort of
*  foreshadowed by an amazing study done by Benedict Herman. It was published back in 2008,
*  where he had people do public goods games in different cities around the world. And in
*  very sort of cosmopolitan places like Denmark and Switzerland and Boston, you saw people
*  cooperating right out of the gate. And when they were given the opportunity to punish people who
*  didn't cooperate or to punish other people, and I say punish, I mean, like you pay a dollar to
*  take $3 away from somebody else. Not court or punishment. They're not whipping each other.
*  Yeah. Cooperation was sustained. And in other places, cooperation kind of started out in the
*  middle, and then the cooperators punished the non-cooperators, and cooperation went up. But
*  then in other places where the social world tends to be smaller, so this would be places like
*  Athens, at least at the time, or Oman or Riyadh, people who didn't cooperate punished the
*  cooperators. And there was a kind of, this is known as anti-social punishment. And it was a kind of
*  resentment of coercion, right? That it was, look, I don't know you, I don't know who you are, I don't
*  like this whole game where I'm supposed to put my money on the line and trust you. And I don't like
*  the way that you're trying to make me do this, right? And so I think that what that reflects is
*  a kind of smaller world. So we have social heuristics. We have emotional tendencies that
*  we've learned about when it's safe to trust people and when it's not, and when we should resist
*  social structures that try to get us to trust more than we should, right? And so then, of course,
*  the question is, well, if we have people who are more globally trusting, and people who aren't all
*  trying to live in a world that requires some kind of global cooperation, how do we fix that? Right?
*  And I don't think that the answer is just to say, hey, Riyadh, Athens, be more trusting, right?
*  The trust has to be earned, right? And so what we really need to do is build the kind of
*  social structures that make trust and global cooperation feel like the natural thing to do.
*  And this is a lot of what I'm working on and thinking about these days.
*  Yeah, I mean, this is a huge set of issues that I do want to get into. But there's one thing I don't
*  want to... I want to go back very quickly, because in the public goods game, where, I mean, very
*  roughly, you're just giving people the opportunity to share their money a little bit. It's not a very
*  complicated game. And there's this result that when you ask them to hurry, they're more generous.
*  And when you ask them to think about it, they're like, I'm going to keep the money. Now,
*  yeah. That seems to be a little bit in tension with the idea that we're going to become a
*  more successful global inter-tribal society by engaging our rational thinking faculties rather
*  than just acting in the moment. How do you reconcile those two things? So the public goods
*  game is a me versus us problem, right? It's a basic problem of the tribe. And that is exactly
*  when I think we need to be at least more often relying on our gut reactions, right? We need to
*  be filled with a sense of trust and generosity towards the people who we interact with in daily
*  life, assuming that they will do reply in kind, right? When it comes to national and global
*  politics and the things that divide us, that's where we can't rely on our intuitions because
*  it's us versus them. So I think the intuitive success in the prisoner's dilemma and the public
*  goods game illustrates the first part of the overall larger equations, so to speak, in moral
*  tribes, which is when it's me versus us, think fast. Cultivate those prosocial intuitions,
*  right? But when it's us versus them, where it's different versions of prosociality competing
*  against each other, that's when we have to step back and think in a more reasoned kind of way.
*  I mean, maybe you can say a little bit about the origin of this conceptualization of the
*  world in terms of us versus them. I mean, this is a strong legacy of our evolutionary history,
*  right? That there's a group that we're a member of and we fight for the group. This is like one
*  of our deepest impulses as far as I can tell. Yeah, yeah. I mean, and as we share this with our
*  primate ancestors, I mean, if you look at chimpanzees, they'll patrol their border and
*  if a group of chimps on patrol find a male chimp from another group that is defenseless,
*  they'll just kill that chimp, right? And with humans, it seems to be a very basic response.
*  Catherine Kinsler did some really nice research where she showed that very young infants
*  recognize a native versus an unfamiliar accent of someone who's even speaking a language that
*  they understand. And they're more likely to accept a gift from someone who speaks, say, English in
*  their native English accent versus someone who speaks English with a French accent, right? And
*  you can go the other way. It's not just English and Coors, French or other languages, of course.
*  That we, you know, come into the world, it seems, ready to divide the world into us and them. But
*  there's some flexibility there that we don't, you know, we don't just detect who shares our genes
*  or something like that and only trust those people who are ingruped in that way.
*  We rely on cultural markers to tell us, okay, you're a Christian, I can trust you, or you speak my
*  language, so I can trust you. And because we have cultural flexibility, there's no reason in principle
*  why we all can't belong to the same tribe, right? There are always going to be forces pushing against
*  that. Because someone can say, you know, rather than being third in command for the big tribe,
*  I would be better off being king of a smaller tribe, right? And so the possibility of human
*  hierarchy is always creating incentives to defect at the cultural level. And I think that that's
*  really what's going on with ethnic nationalism in the United States and other countries, is that
*  the world has been moving towards more and more global values, right? But there are winners and
*  losers in that process, right? And someone like Trump or Le Pen in France, or Nigel Farage in
*  England, and so on and so forth, they're saying, no, America first. France is for the French.
*  Don't, we don't want to be part of this. Just us, just those of us who really, you know, got that
*  ussy feeling, right? And they play to that. And then they say, and those other people,
*  they're coming to kill you and rape you, or your sisters and daughters, right? And they're stealing
*  your jobs, right? Those people in China are stealing your jobs. And why would they do that?
*  Well, it's hard to be king of the world. But maybe you could be president if you push those
*  buttons hard enough, right? And so the great challenge is to, we have this open-ended
*  flexibility to see us as very small or very large. And I think the challenge we're facing right now,
*  geopolitically, is between forming a truly larger us and,
*  or breaking off into our smaller, more comfortable uss.
*  Well, it's a challenge in part. I just read, there was a column in the New York Times by Thomas
*  Edsel very recently, quoting some academic paper that claimed that one of the reasons why
*  authoritarianism can be so attractive is just because it gives meaning to people's lives,
*  right? It gives them a purpose. And in a world where the world is so big, you don't meet everybody,
*  and it can seem like there are invisible forces keeping you down, then it is a very attractive
*  position to just circle the wagons a little bit, right?
*  Yeah, absolutely. And I think that's especially true if people feel like they're sliding,
*  there's been this big debate in the social sciences, is the rise of Trump and ethnic
*  nationalism, is it about economics, or is it about tribalism? Is it about hierarchy in us versus them?
*  And I think it has to be both. I mean, the data point more strongly towards tribalism and us versus
*  them because individual variation in economic situation is not a very good predictor of,
*  it's not about my pocketbook and people don't attribute their personal gain or loss of income
*  to what's going on geopolitically. But over the last 40 years, inequality in places like the United
*  States has gone up and up and up. And the prospects of someone who doesn't have a college degree
*  have slowly slid down. And that is Trump's base. And it's basically people without college degrees.
*  And that sense of being left behind, I think it doesn't require you to go down that nationalist
*  route. You can go a more Bernie kind of direction and try to push for a more egalitarian social
*  structure. But for many people, the most comfortable and appealing route is to say,
*  enough of this. I want a country that's just for people like me, look and sound like me.
*  And then that can really be reinforced by these wolf stories about the outsiders who are coming
*  to take your jobs and physically harm you. And then those elites who are in cahoots with them
*  and who are just profiting off of your demise. And then those stories really
*  reinforce that sense of victimization. And it makes that, yeah, let's just be us feeling very powerful.
*  Well, it goes back in my mind to one of the things you said about utilitarianism right from the start.
*  We very naturally, our intuitions, what we grow up thinking in terms of morality and correct
*  behavior, give a lot more weight to people close to us and like us than to people far away. And
*  actually, one of the reasons why, one of my worries about utilitarianism is it has this
*  unrealistic bent sometimes where it says, you know, every person's experience counts precisely
*  equally. And even if that's true, in a sort of God's eye view sense, it's almost impossible
*  to imagine real human beings acting that way, right? Right. Well, so this is the pragmatist
*  part coming back again, right? Is that if you're a good utilitarian, you start with humans as they
*  are and not demand that they live up to some impossible ideal. So my view is, we've made a
*  lot of progress towards that, right? People care a lot more about people on the other side of the
*  world than they ever used to, right? And people's moral circles are much wider than they used to be.
*  And the idea of, at least in the West, not eating meat, which is delicious, because you just care
*  about animal suffering, that would just be absurd. So many of the things that we consider perfectly
*  normal and reasonable now would have just seemed as absurdly idealistic for most of human history.
*  But you're absolutely right that there's no prospect anytime soon of people caring more
*  about strangers than they care about their own friends and family. And maybe in the end, we don't
*  need to go that far in order to have a well-functioning global society. We just need to
*  not burn ourselves up with carbon in the atmosphere or nuclear weapons or a supercharged
*  pandemic, right? So the way I see it is, you don't have to give up on loving your children.
*  Just give up 10% of your income or 1% of your income and devote it to good causes and support
*  political movements that are more egalitarian rather than less egalitarian and that promote
*  global cooperation rather than undermining it. My friend, Charlie Bresler, who runs an organization
*  called All You Can Save, which was started by Peter Singer, ultimately about alleviating global
*  poverty, he has a great phrase that he brings into this, which is personal best. That is,
*  it's not helpful to think of this in terms of trying to be perfect in terms of valuing all lives
*  perfectly equally, caring about strangers no more than or as much as you care about yourself or your
*  friends and family. Instead, just ask yourself, can I be a little less selfish? Can I be a little
*  less tribalistic? And if that becomes easy, then you dial it up. And so with anything else, whether
*  you're on a diet or you're trying to get in shape or learning to meditate or whatever it is, you
*  don't start off by saying, okay, I'm going to be an Olympic athlete tomorrow or be very disappointed
*  if I'm not. Instead, you say, okay, can I run one mile without stopping? And then once that becomes
*  easy, you build up. And so I think Aristotle got one thing right, is that these things don't just
*  happen because we've heard some theory. You need practice. You need to build up. And I think as
*  individuals and as cultures, we can scaffold and we can build up and make ourselves more globally
*  pro-social without breaking the moral bank, so to speak.
*  Let me just throw out a thought that I have no idea whether it's relevant here, but I recently
*  did a podcast with Christopher Mims, who is a journalist at Wall Street Journal about
*  industrial ecology and how stuff gets to us in the modern world. The fun anecdote is if you
*  catch a fish off the coast of Scotland and you eat it from the supermarket in Scotland,
*  chances are after being caught, it was sent to China to be filleted and then sent back,
*  because it's just so much easier and cheaper to do that than to fillet it in Scotland.
*  And so my point is that even though our circles of caring have grown as we become a more
*  interconnected world, we're also so differentiated in what we do that the things that are right next
*  to us or the ways of living that are right next to us can become a little bit invisible.
*  I'm not aware until talking with Chris and reading the book about the truck drivers who
*  are bringing me this stuff, where I could go my life not really connecting in some deep way to
*  the people who serve me food, which maybe 200 years ago would have not been impossible. Is this
*  a new challenge for global morality that we're so complex and interconnected, but also
*  differentiated? Yeah, I think that as technology has become more complicated and the economy has
*  become more complicated, the effects of our choices as agents within the global economy
*  are much more opaque. And sometimes it's just interesting to think of how many different
*  nations participated in making it possible for me to have the iPhone in my pocket or something
*  like that. And sometimes it really matters. I mean, again, to come back to a familiar example,
*  I think very few people would eat factory farm meat if they spent much time in a factory farm.
*  But we're just separated from it. And that's why the companies that do factory farming
*  work very, very, very, very hard to keep footage of what's going on in there out. But to us,
*  it just tastes like a chicken sandwich. And of course, that's one example, but labor practices
*  and reinforcing certain kind of corporate structures. Every time you buy something from
*  Amazon, you're reinforcing a kind of economic structure. And I still buy things from Amazon,
*  but I have concerns about that. I feel bad about it. Sometimes I just want my big box of toilet
*  paper and I don't want to have to organize my life around this. So yes, I agree that the world is so
*  complicated that it's impossible for us to keep track of all the pieces. And yet
*  we'd be a terrible mistake to just throw up our hands and say, oh, it's inscrutable, so I'm just
*  going to do what I feel like. So we have to find some middle ground where we pick our battles and
*  we learn what we need to learn most and make good enough choices.
*  Well, on the more optimistic side, you do, I forget whether it was your research or you
*  were quoting somebody else, but you do have research where when you make people who are
*  nominally in different tribes work together on some task and cooperate, they become much
*  more friendly and trusting of each other. Is that an overgeneralization or is that okay?
*  Yeah, this is actually work that's ongoing. I mean, first of all, this is an old idea.
*  Going back to Gordon Alport in his book, The Nature of Prejudice, where he emphasized that
*  contact between groups, he was mostly focused on race in a cooperative context, can really
*  bring people together. And he gave the example of something that was very salient at the time was
*  the integration of the US military. And we say there are a lot of things wrong historically and
*  today with the US military and other militaries. And the integration of the US military didn't
*  just happen for noble reasons that the United States, many people were happy to have black
*  people risking their lives for the United States instead of themselves or their children. But
*  whatever complex and questionable motives may have been behind creating a more integrated military,
*  it did create a more integrated military and a more integrated culture. I now am doing research
*  on this with a brilliant student named Evan D. Philippus, who's in the organizational behavior
*  program at Harvard Business School and in the psychology department. And we have Republicans
*  and Democrats working together as partners in an online quiz game that we've created.
*  And I can't say anything about the results here, other than I'll say that I'm very excited about
*  this research. Okay, good. I think that the principles behind it could really be helpful.
*  Well, good. I mean, maybe that's the right segue into wrapping up with more
*  programmatic pointers for ourselves and our listeners. Because you actually, as far as I can
*  tell, taken these thoughts about being a good person and global metamorality, etc., and thought
*  about how to make them make a difference in our choices in our everyday lives and our giving to
*  charities and things like that. Yeah. So yeah, since publishing Moral Tribes and doing a lot of
*  this work on very abstract and theoretical issues, I wanted as a moral psychologist and
*  a student of social behavior to work on things to sort of more apply my philosophy rather than
*  then defend it and understand the mechanics behind it, although I'm interested in that as well.
*  And one of the things that's been most important to me personally and philosophically is
*  living utilitarianism, living deep pragmatism, doing what you can to do as much good as possible.
*  And the most straightforward way that individuals who are lucky enough to have some
*  resources, the most straightforward way that people can do good is by supporting charities
*  that are highly effective. So most people don't know this, but the most effective
*  charities are orders of magnitude more effective than typical charities. So to give an example
*  that comes from Toby Ord, you can help a blind person in the United States by paying for the
*  training of a guide dog. And that costs about $50,000. And that's a really wonderful, valuable
*  thing to do. If you can spend $50,000 on a car for yourself or an addition to your house or something
*  like that, you do much more good by helping a blind person get around in the world. But
*  you can fund in other nations, a surgery to prevent an infection from trachoma can be done for about
*  $100. You can prevent somebody from going blind for about $100. So we're talking about something
*  like a thousandfold difference in effectiveness. And it's not to say that the person in the United
*  States who's blind, it's not important that we shouldn't help them. But if you have to choose
*  between helping a thousand people not go blind or one person manage their blindness, to me,
*  there's no question about what we should do. And there are many things that are as effective as
*  trachoma surgery. So things like distributing insecticidal malaria nets that for very low cost
*  can prevent people from dying of malaria, or deworming treatments where a simple pill can
*  prevent, can destroy parasitic infections that are really debilitating and prevent children from
*  going to school and getting an education. And those deworming treatments can cost less than a dollar,
*  right? And make a huge difference in someone's life. So the charities that support these things
*  are enormously impactful. It's kind of mind blowing how much good they can do for so little
*  money, right? And so then I tried- Do you know, by the way, GiveWell?
*  Yes. So actually, GiveWell is an important role.
*  I just want to mention very, very quickly, GiveWell is a sponsor of the Mindscape podcast.
*  Oh, great. Okay.
*  So I suspect I'm recording this week's ahead of time, of course, but I have a high probability
*  that I will be giving an ad for GiveWell during- Okay.
*  I've already given it by now, so I'm glad we're on the right wavelength.
*  So GiveWell was the organization that really pioneered doing this effectiveness research,
*  and the thing I'm about to talk about giving multiplier is making use of their research.
*  So what do you do? So I've been studying the psychology of this for a while. How do you get
*  people to give more and how do you get people to give more effectively? And I initially tried
*  to convince people as a site, experimenting with convincing people the way I was convinced,
*  which is with philosophical arguments of the kind that Peter Singer gave, right? Which is,
*  you know, there's a child who's drowning in the pond and you could save that child,
*  but you're going to muddy up your clothes. It'll cost you some money, but you save someone's life.
*  Is it worth doing that? And it's, of course. And so I present people with this and say,
*  so now are you willing to commit to supporting the most effective charities? And they're like,
*  nah, or a very small number of people will go that direction, but most people,
*  not so much. And then with Lucius Caviola, who's been a postdoc in my lab, we hit on a
*  fundamentally different strategy, which takes a much more deep pragmatist approach in retrospect,
*  which is instead of saying, don't use your money on yourself or on the charities that you love that
*  really speak to your heart. Don't do this. Instead, do this other thing, supporting deworming
*  treatments in Africa and Asia where you have no personal connection to this. We just said,
*  what if you do both? And so we started doing these experiments where we just said, hey,
*  you have a certain amount of money. You could give it all to this charity that you chose
*  or all to this charity that experts recommend, or you can do a 50-50 split. And we found that
*  people love the idea of doing a 50-50 split and that more money ends up going to the highly
*  effective charity if you give people the 50-50 split option than if you don't, right? Even though
*  only half of the money is going to that charity. And so we did some other experiments where we
*  looked at, tried to understand why this is the case. And it seems like it's a kind of heart-head
*  balance that people want to give something to the charity that they love. But then once they've
*  given something, they also really like the idea of being highly effective and competent and
*  using the scientific research to do as much good as possible. And we found that people really like
*  this. If we say, hey, if you make a 50-50 split charity where you pick one and we pick one,
*  or you pick one of ours, and we'll add 25% on top to both donations, people go, oh, wow,
*  that's great, right? And we found not only that, we asked people, we said, hey, would you be willing
*  to take some of what you gave and then use it, you can use it to pay matching funds for somebody else?
*  Right? A lot of people were willing to do that. Not everybody, but a lot. And when we did that,
*  when people put a dollar sort of into matching, it ends up moving more than a dollar towards
*  highly effective charities. So we said, my gosh, well, we should try this in the real world.
*  So we created this website. When I say we, I mean, Lucius Caviola, my collaborator and his
*  awesome web design friends, created this website called Giving Multiplier, which works just like
*  the way I described. So if you go to givingmultiplier.org, you'll see an option first to
*  pick any charity that you like, any registered charity in the United States, and you enter that
*  in, and then it says, okay, so if that's your charity that you chose, your personal favorite,
*  and then we have a list of currently nine charities that cover a lot of different things. So some of
*  it is global health and poverty. So fighting malaria, deworming treatments, vaccines,
*  trachoma surgeries, and things like that. Some things are, we have a climate change charity
*  that seems to be extremely effective. We also have a charity called Give Directly, which just
*  gives money to people in poor nations. The research shows that they do an excellent job
*  of using that money to build businesses and take care of basic necessities and that it ends up
*  helping the whole community. So this is kind of a little bit, some people like this because you
*  might see it as less paternalistic, that you're not saying, here, have a malaria net, even if
*  that's not what you would ask for. Instead, it's just saying, here are some resources that I trust
*  you to do as much good for yourself as you can with it. So we have a lot of different things,
*  but all of these things have either been rigorously tested with experiments shown to be highly
*  effective, or they're more long-term things like preventing the next pandemic, where it's reasonable
*  to assume that a relatively modest investment in research can have a huge impact. So you pick
*  your charity, you pick one of these charities that experts recommend, and then you say, okay,
*  how much do I want to give total? And then we have this little slider where you can decide how much
*  you want to allocate to your personal favorite charity versus the charity from our list. And
*  the only constraint there is you have to give at least 10% to one of the charities that we
*  recommended. And then giving multiplier adds a match to your donation. And I think right now,
*  it's a 50% match if you give everything, if you just make a donation to the highly effective
*  charity, the one that you chose, and we do a 25% match. So if you split 50-50, and you can do
*  things in between. And this has been great so far. We launched this less than a year ago,
*  and it's raised over half a million dollars, most of that going to the world's most effective
*  charities. And we have created a special code that unlocks a higher matching rate for your
*  listeners. So if you put in the code, mindscape, all caps, all one word, then you get a higher
*  matching rate than you otherwise would. So we encourage people to give this a try. And the
*  idea here is that you don't have to be a perfect utilitarian. It's okay to care about. And I do
*  this myself. My wife and I, we give to our local schools and the Greater Boston Food Bank and
*  things like that, that we don't give up on having these more personal connections. So you support
*  something that's personally meaningful to you, but you also give something to something that's,
*  the research says is super effective. And then we help out. And we also have a system where
*  if you donate, you have the option to donate part of your donation to the matching fund so you can
*  pay it forward for somebody else. And a lot of our donors do this. And the whole cycle has just been
*  self-sustaining. So it's been this wonderful sort of virtuous circle of effective giving,
*  where people support things they personally care about, but also go with the research.
*  And it's just been running on its own. And so we're excited for our second holiday season coming
*  up. And hopefully we're going to get over a million dollars and then some. And so I hope
*  your listeners will find this some, will find it useful and gratifying. Yeah. No, I mean,
*  it's a great pitch. And I'm a big believer in both the head and the heart. So I think that's a very
*  good way of saying it. I'm always going to give some money to the local pet shelter where we got
*  our rescue kittens, even though I know that it's not quite as impactful on the world as giving
*  money to help disease or poverty elsewhere in the world. But I also try to do that. And likewise,
*  you and I both like it when rich people give money to our universities to help us do research and
*  support students and things like that. Also, probably not the most impactful. But I think
*  that the idea that you do a little bit of both and everything gets better is a very clever one.
*  Yeah. Yeah. So now thanks. I'm excited about this and looking forward to it.
*  And it works whether or not you are utilitarian, which is the best thing about it because, you
*  know, our opinions change about that. So this is a view this as a way to expand the circle of
*  effective giving and, you know, create a way for people to do a lot more good than they otherwise
*  would while still feeling connected to what they're doing and feeling connected to the
*  other people who are part of this. That when you support the matching fund, you participate in this
*  cycle. Good. Lots for the listeners to think about, feel, and do also. So Joshua Green,
*  thanks so much for being on the Winescape Podcast. Thanks so much for having me. This has been great.
