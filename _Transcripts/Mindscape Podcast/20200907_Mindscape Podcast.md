---
Date Generated: June 09, 2024
Transcription Model: whisper medium 20231117
Length: 4794s
Video Keywords: ['game', 'theory', 'evolution', 'discrimination']
Video Views: 15102
Video Rating: None
Video Description: Blog post with audio player, show notes, and transcript: https://www.preposterousuniverse.com/podcast/2020/09/07/113-cailin-oconnor-on-game-theory-evolution-and-the-origins-of-unfairness/

Patreon: https://www.patreon.com/seanmcarroll

Mindscape Podcast playlist: https://www.youtube.com/playlist?list=PLrxfgDEc2NxY_fRExpDXr87tzRbPCaA5x

You can’t always get what you want, as a wise person once said. But we do try, even when someone else wants the same thing. Our lives as people, and the evolution of other animals over time, are shaped by competition for scarce resources of various kinds. Game theory provides a natural framework for understanding strategies and behaviors in these competitive settings, and thus provides a lens with which to analyze evolution and human behavior, up to and including why racial or gender groups are consistently discriminated against in society. Cailin O’Connor is the author or two recent books on these issues: Games in the Philosophy of Biology and The Origins of Unfairness: Social Categories and Cultural Evolution.

Cailin O’Connor received her Ph.D. in Philosophy from the University of California, Irvine. She is currently Associate Professor of Logic and Philosophy of Science and a member of the Institute for Mathematical Behavioral Science at UCI. Her works involves questions in the philosophy of biology and behavioral science, game theory, agent-based modeling, social epistemology, decision theory, rational choice, and the spread of misinformation.

#podcast #ideas #science #philosophy #culture
---

# Mindscape 113 | Cailin O'Connor on Game Theory, Evolution, and the Origins of Unfairness
**Mindscape Podcast:** [September 07, 2020](https://www.youtube.com/watch?v=k2H4lsW4R54)
*  Hello everyone, welcome to the Mindscape Podcast. I'm your host Sean Carroll.
*  Longtime listeners will know that we've talked about game theory before on the podcast,
*  usually in the context of literally playing games. We've had no less than two different podcasts about playing poker or
*  realizing that there are lessons from poker that you can apply more widely. And in fact,
*  I think both times we noted that John von Neumann, who is the father of game theory, invented it in order to analyze his local poker game.
*  We also talked to Frank Lance, who is a game designer about game theory.
*  But the wonderful thing about game theory is that it applies much more broadly than that. It's a lesson in strategic
*  interactions between agents with different interests, right?
*  So that's obviously a hugely broad kind of conceptual framework in which to think about a whole bunch of different issues.
*  So today we're going to be talking to Kaylin O'Connor, who is a philosopher of science.
*  She has quite a broad portfolio in terms of research interests, but one of the things she's done,
*  she's written a couple of books on applying game theory to both biological evolution,
*  thinking about what species evolve, how they interact with each other, who's predator, who's prey, how they fill different niches and so forth.
*  And also one applying game theory to human behavior.
*  Why do human beings treat each other in different ways? And in particular,
*  can we understand the origin of certain inequities in society in game theoretic terms?
*  Now this is a very difficult thing to do, right? Because when you have an inequity in society,
*  when one group is discriminated against or picked on, another group is privileged and has more wealth or resources or power or whatever,
*  there's probably a whole host of reasons why and people will debate them.
*  But game theory sort of presents an interesting new twist on this kind of problem. Imagine that, you know,
*  you're a kid and you have a sibling and your mom says to the two of you, to you and your sibling,
*  okay, there's a pint of ice cream here.
*  You each tell me what fraction of the pint of ice cream you would like to eat.
*  And if the total that you say is less than the whole pint of ice cream, less than or equal to it,
*  then I will split it up according to the rules that you just suggested.
*  But if you suggest a total that is more than one, right, more than the whole pint, then nobody gets any ice cream.
*  Now, of course, the fair thing to do would be for you and your sibling to say,
*  I each want half of the pint of ice cream, then you would both get half, everybody's happy.
*  But what if you know from past experience that your sibling is always very greedy and they're almost
*  guaranteed to ask for two-thirds of the pint of ice cream, okay?
*  So you could stubbornly ask for your half because that's the fair thing, but then neither one of you get anything.
*  You are both worse off, okay?
*  So because your sibling is going to have this strategy and you know it in a strictly utilitarian sense,
*  it makes sense for you to only ask for a third of the ice cream.
*  So you can see how, by this kind of analysis,
*  inequities creep into the system, even though there's no difference between the two players of the game initially.
*  So I find this all extremely fascinating.
*  It's sort of a completely different lens with which we can use to analyze
*  biology, psychology, sociology, a whole bunch of interesting things.
*  It's nowhere near the whole story in any of these cases, but different angles, different perspectives are always very welcome.
*  So I think you're gonna find this conversation very interesting.
*  The other announcement to make, of course, is that for those of you who've been following along,
*  you know I have this video series called The Biggest Ideas in the Universe and we're done.
*  We have reached the conclusion. Last week was the last video.
*  There were 24 different ideas. The last idea was science.
*  That was a bit more philosophical and meta. The other ideas are things like space, time, matter, gravity,
*  you know, stuff like that. Physics ideas, physics and cosmology ideas. We had some off-the-wall ones.
*  We did emergence and renormalization and
*  criticality and complexity. But anyway, they're all done. There was one, every week
*  there was one video and one Q&A video. You can find them all on my YouTube channel.
*  I do have a YouTube channel. It's youtube.com slash Sean Carroll.
*  There's no M in there, like Patreon supporters or Twitter followers might know about.
*  Sean M. Carroll is what I usually use, but for some reason on YouTube it's just Sean Carroll.
*  Anyway, if you haven't checked them out, I encourage you to check out the videos if you're interested in physics.
*  You get to see me as well as listen to me. Although, yeah, in fact, since I'm drawing
*  and writing equations in most of these videos, I think that the visual aspect of it is actually important.
*  You could also see my improvement as a videographer and cinematographer
*  over time as I figure out how to do the green screen and so forth.
*  And you can also see that I never improve all the way.
*  I was still making pretty elementary mistakes in my video skill package as time went on. But it was a lot of fun.
*  We got a lot of viewers, which warms my heart, and it's always good to see that people are interested in learning new things about the universe.
*  So with that, let's go.
*  Katelyn O'Connor, welcome to the Mindscape Podcast.
*  Oh, thank you so much for having me, Sean.
*  So we talked to philosophers before, we talked to biologists.
*  I'm trying to think. I'm not sure that I've really done any philosophy of biology.
*  Is that fair as a way to characterize what we're going to talk about today?
*  Yeah, I think a lot of what we're going to talk about would fall under that.
*  We also might call some of it philosophy of social science.
*  Yeah, I mean, philosophy of biology is this nice interdisciplinary area where
*  some people in it do things that look a lot like theoretical biology.
*  And some people do things that are asking more meta level questions about the biological sciences.
*  You know, what are the concepts that we use in biology? What do they really mean?
*  What can the methods in biology really tell us or not tell us?
*  So, yeah.
*  And game theory is something that I don't know why, but like in the past year of my life, I just keep running into it in different
*  circumstances. You're not the only podcast guest who will talk about game theory a little bit with us.
*  So, how does, well, let's even before we relate it to biology,
*  why don't you tell us in your brain what, how you think about what game theory is and what it's good for?
*  Okay, so I mean like little history, game theory was really introduced
*  sort of in the starting off in the 1940s and 50s.
*  And it's a branch of mathematics where the goal is to look at strategic interactions.
*  And by strategic interaction, I just mean an interaction where you have
*  multiple actors who have some kind of interest. So these could be humans, but they also could be animals or even
*  something like trees or bacteria.
*  And they interact with each other and it matters to them what the other actors are doing.
*  So it's the branch of math that, you know, tries to represent and tackle
*  this kind of interaction.
*  And the way you do it is by building what's called a game.
*  So this is a sort of simple mathematical model of a strategic interaction and then
*  analyzing that game to try to ask,
*  can it help us predict what certain actors would do when they're in a strategic scenario?
*  Or can it help explain behaviors that we see in the real world, either in humans or in the biological world?
*  And one of the big ideas here is that we can truly exactly
*  quantify the rewards for what we do, right? Like, you know,
*  one of the buy-ins for using game theory for anything is to say,
*  if you do this strategy and your opponent does some other strategy, then here's the rewards that you each get.
*  Yeah, and I think, you know, the way you want to think about this is that people using game theory don't
*  assume that we can exactly quantify what are the real rewards that, say,
*  you and I would get if we managed to coordinate some behavior, if we managed to meet each other at the right time for coffee.
*  I mean, it would be hard to quantify, like, is this a psychological thing, you know,
*  and how much pleasure do we get or, you know, what material goods do we get?
*  But rather the idea is we can use specific numbers
*  to represent those rewards and then by making them precise, we can get a kind of
*  good enough or approximate representation that we can also analyze and better understand.
*  Yeah, I think that's pretty fair.
*  Although, you know, this is going to be one of those things when we start applying it to biology and to psychology and to sociology, where people are going to
*  poke at it in different ways when you reach conclusions they don't want to reach, right?
*  So maybe one of the pokes is, you know, are we overly
*  rigorizing things that are a little bit fundamentally fuzzy?
*  Yeah, there's always this risk with modeling. I mean,
*  modeling can be so useful in that it allows you to make these really precise structures that you can analyze precisely and say very concrete
*  things about them and I'll be talking about the exact same thing.
*  Of course, the downside to that is that you're going to lose some of the precision of the real world or you might represent it
*  incorrectly in some really significant way. And so there are always going to be these deep questions about is your model the right model?
*  Is the conclusion you draw from it going to be supported in a more complex reality?
*  That's okay. As a physicist, I'm very, very used to, you know, overly idealizing complex situations into very simple little models.
*  So that part we're used to that part. So why don't you, I mean, there's some famous games, right?
*  There's the prisoner's dilemma. I mean, we're not thinking, even though you could,
*  we're not really thinking about backgammon or basketball or things like that.
*  But those are, you know, those are more advanced games. The little games that we're analyzing here are
*  basically have these reward tables. So why don't you fill us in on how we should be,
*  what is the mental image we should be having when we're playing these games?
*  Well, I should note that when von Neumann was first
*  inventing game theory, he did, I think one of his first papers was called the theory of parlor games.
*  So he did have some kind of real games in mind, but the games you see in game theory, they're not the fun kind.
*  In fact, I teach a class called evolutionary game theory and I sometimes start the class being like if you're here because you like playing games,
*  scoot on out. This isn't the place for you. No twister here. No twister. It's not going to be world of warcraft or starcraft.
*  So, okay, a game in this sense, I mean you can define it using four elements. So the first thing is
*  the players who is involved in the strategic scenario.
*  The second thing is their possible strategies. What things can those players do?
*  What actions you might say. The third thing is their payoffs.
*  So given some combination of strategies chosen by the players or actions that they do,
*  what are the kind of benefits or detriments to them?
*  And then information is the last thing that helps to find a game and
*  information is approximately what do the players know about what they're doing?
*  So what do they know about the setup of the strategic scenario they're in and what do they know about the other
*  opponents or players they're involved with?
*  So chess is a game of complete information because you see the board, but poker you don't see the other person's cards.
*  Right, right, and we can think of lots of different kind of strategic scenarios where you might have more or less information.
*  So if you're bargaining with someone that's a strategic scenario and you might know
*  what's going to happen to this person if your bargain fails or you might not know that
*  and knowing or not knowing might really shift how you bargain. So if you know that this person
*  doesn't have anything else to do if this bargain doesn't work, they'll be in a really bad situation.
*  You might be a more aggressive bargainer on the basis of that knowledge.
*  I guess it's probably, I mean you should tell me,
*  but it's probably worth doing the prisoner's dilemma in some detail just because it's a wonderful example of
*  some of the concepts that appear but also some of the counterintuitive conclusions you can reach by taking this seriously.
*  Yeah, so the prisoner's dilemma is the most famous and widely analyzed game in game theory because you're right it is
*  a kind of fascinating little game. So you assume you have two players and you assume there's two things that each of them
*  can do and people often call these two strategies
*  cooperate or defect.
*  So tell the little kind of story people like to tell about the prisoner's dilemma.
*  It's like you've got two prisoners
*  and they're each told well you can either rat on the other one
*  or stay silent. So staying silent would be cooperating and ratting would be defecting.
*  And then the idea is okay if they
*  both stay silent they're going to have a really short jail term. Let's say they'll be in jail for a month.
*  If they both rat the other one out they'll have a slightly longer jail term. Maybe they'll be in jail for three months.
*  If one of them rats the other one and that one stays silent though,
*  the one who rats is going to go free and then the one who stays silent will have a very long jail term.
*  They'll get blamed for everything. So maybe they'll be in jail for a year.
*  So those
*  that kind of creates the payoffs of this game and the basic payoff structure
*  which you get out of this little story is that
*  the very best thing for anyone is to be the one who ratted while the other person stayed silent.
*  But you still prefer to both stay silent than to both rat.
*  So basically there's a payoff structure that incentivizes you to defect
*  but creates a situation where if you both defected you would prefer that you had both cooperated.
*  So that's the dilemma of the game that it's individually rational to choose to defect
*  but from a kind of social level you can all do better by not all defecting.
*  And I guess so one of the things that leads to a voluminous literature for this very very simple game to state is that you can imagine either
*  this was truly just a once-off you play the game and in that case
*  I think that just defecting is the right thing to do.
*  But you could also imagine that you're just faced with situations like this over and over again where maybe some cooperation would help you out in the long term.
*  I think the heart of why people are so interested in this game is that in the end it's an analysis of altruism.
*  So if you cooperate in the prisoner's dilemma, you've done something that will lower your payoff but increase the payoff of the other player.
*  So you've done something kind of materially inherently altruistic.
*  And we see humans and animals in the biological world do altruistic things all the time
*  even though the kind of basic structure of the game tells you that's not individually rational. It's not rational to harm yourself.
*  So the question is well if that's not individually rational, why do we see so much altruism?
*  And then there are a few answers. One is the one you just brought up that we're in repeated interactions with others.
*  And that really actually changes the structure of the game.
*  Once you start meeting the same individuals again and again for this kind of interaction, it can change what's rational.
*  It can make it rational now to cooperate on the assumption that cooperating now will get them to cooperate with you later.
*  And then there's other things like if you're looking from a biological perspective,
*  you can see that being altruistic with your kin can end up being selected for.
*  So the genes that would make you altruistic with kin can be selected.
*  Basically because when you're altruistic toward your kin, you're being altruistic towards people who are often other altruists or actors that are other altruists.
*  And so altruists are getting these kind of benefits by dint of associating with each other that then defectors aren't getting.
*  So I think that is really the heart of why the Prisoner's Dilemma is so fascinating, is it helps us answer these questions about altruism.
*  I bet you didn't know that September is National Life Insurance Awareness Month.
*  A lot of people are probably aware of the existence of the idea of life insurance,
*  but they haven't quite gotten around to doing their responsible thing and looking into getting it for themselves.
*  That's where Policy Genius comes in. Policy Genius is an easy-to-use insurance marketplace
*  built and backed by a team of industry experts that makes the whole process a snap.
*  You head to PolicyGenius.com in minutes.
*  You can work out how much coverage you need and you can compare quotes from top insurers to find the best price,
*  apply for that price, and then the Policy Genius team handles all the paperwork and all the red tape.
*  Policy Genius works for you, not for the insurance company.
*  So if you hit any speed bumps during the application process, they will take care of everything.
*  They even have policies which allow eligible customers to skip the in-person medical exam and do it all over the phone.
*  So if you need life insurance, head to PolicyGenius.com right now to get started.
*  You could save $1,500 a year or more by comparing quotes at their marketplace.
*  Policy Genius, when it comes to insurance, it's nice to get it right.
*  It also, I mean the the last point that you just raised sort of brings up questions of the selfish gene or levels of selection
*  kind of thing, right? If I'm altruistic toward my kin, in some sense I'm being altruistic toward my genome, right? I'm helping it
*  survive. Is that is that a valid way of thinking?
*  Yeah, so I mean the way you'd think of it is, you know, from an individual perspective altruism might not look rational.
*  You know, why would I
*  do something for my kin that's gonna, you know, essentially make them more likely to have more offspring and me less likely to have more offspring.
*  But then if you flip to the genes I view, so this was an idea, you know,
*  largely popularized by Richard Dawkins. From a genes I view, if you know, I'm a little altruistic gene,
*  I would like to benefit other genes just like me. That it's, you know, the sort of that the altruistic gene
*  is the point of view we look at. It makes complete sense.
*  And
*  yeah, and so are you gonna are you gonna come down on which is right or are they all right or different circumstances
*  called for different ways of thinking?
*  Um between what oh these perspectives? Yeah.
*  Yeah, I you know, I tend to be the yeah, different circumstances require different kinds of thinking type of
*  person when it comes to thinking about evolution.
*  It gets very heated, you know, these arguments between uh, what levels of selection are valid, especially group selection. I know is extremely
*  um controversial in different circles.
*  Yeah, group selection is very controversial. I mean for people who don't know what group selection is, it relates to the idea that um,
*  you know, in some cases you wouldn't want to think of selection as happening on the
*  unit of an individual but of a social group. That maybe if a whole social group does well,
*  by having certain traits that can explain why we have those traits.
*  Um, now this debate has really used a lot of game theory. So John Maynard Smith did these very influential models.
*  Um, you know game theoretic models where he showed
*  that it's quite hard to get group selection of a certain type to work and that's group selection for altruism.
*  So a lot of people thought well, maybe because altruism is good on a group level.
*  That's why we see it groups that are altruistic tend to be selected. He showed um that that
*  it's hard to get an altruistic group to be selected. Um, now and so a lot of people sort of took that
*  criticism and variations of that to imply that group selection
*  is an important period. But I think
*  it's just sort of a confusion in the literature where people have shown group selection probably isn't the most important explanation of altruism.
*  But that doesn't show that group selection isn't important for the explanation of other kinds of traits necessarily.
*  Okay, okay. I mean that that does make sense and it is a bit of a sorry distraction.
*  I got off on it just because I know that people people really care about this.
*  So we should you know bring it if we bring it up we should say something about it.
*  But let's let's just do a little bit of the lingo that is used in this game theory context.
*  So you mentioned the prisoner's dilemma.
*  Maybe it's worth explaining what a Nash equilibrium is and I don't know if you also
*  want to give other examples of games like the prisoner's dilemma is not the only game that models actual dynamics.
*  For sure. Um, so
*  Nash equilibria, so I mean
*  in game theory you build a game you get this model of a strategic scenario and then the next thing you do is analyze it
*  and what people do is apply what are called solution concepts to the game.
*  There are a bunch of solution concepts, but the most widely used one is the Nash equilibrium.
*  And the idea is you're looking for sets of strategies where when actors are playing them
*  none of the actors want to change and do something else. There's no incentive for them to switch strategies.
*  And those sets of strategies are Nash equilibria.
*  The reason that this is an important solution concept is that if you are at a Nash equilibria, no one wants to switch.
*  It's approximately a stable set of behaviors. And so that means the Nash equilibria
*  predict what actors will do in games and there has been
*  I mean laboratory experiments don't always confirm that people do play Nash equilibria, but they often play them or something
*  like them when you have them actually play games.
*  So that that's why those are so important.
*  And if you're doing a game theoretic analysis, the first thing you do is build your game,
*  whatever model you're looking at, and then the next thing usually is calculate the Nash equilibria of the game.
*  See what they tell you. So for example in the prisoner's dilemma the issue is that
*  the best for both
*  players is to
*  cooperate, but the Nash equilibrium is that they both defect, right? Right.
*  They do better individually. I mean the the equilibrium
*  relies on the fact that they're making their decisions individually.
*  Yeah, so
*  right, that's exactly it. Defect, defect is the only Nash equilibrium of the game.
*  And so that's the kind of prediction for what rational agents would do.
*  But then there's this benefit to not playing the Nash equilibrium in that game, like just a straightforward benefit to everyone playing.
*  So
*  you mentioned
*  other games. I mean there's so many other fascinating games. I actually don't work really with the Nash,
*  or sorry with the prisoner's dilemma myself because it's been so analyzed. You're just,
*  everyone has done everything you can imagine to the prisoner's dilemma. And to me,
*  situations having to do with coordination are really fascinating.
*  So I've done a lot of work more on coordination type scenarios.
*  And these are scenarios where there's common interest between the players. And what I mean by that is that
*  they're going to get shared payoffs.
*  And in coordination games, the way they get shared payoffs is by somehow coordinating behavior.
*  But then the kind of thing that makes the game interesting is that there are multiple ways to coordinate behavior.
*  And the question is how are the actors going to settle on one way or another?
*  How are they going to solve what's called a coordination problem and learn
*  to make their behavior work together? And this is especially interesting when we look at more of a societal level.
*  How does a whole society come to learn to coordinate?
*  And then under that umbrella
*  are what are called bargaining games,
*  which have to do with resource division or splitting stuff. And the reason these games are so interesting to me is that humans are in
*  situations all the time where they're dividing resources, whether it's literal resources like money for your salary or whether it's
*  time and effort. So for example, if you're an academic on a joint project,
*  everyone, you know to get it done, you have to somehow divide the work necessary to be done and that's a bargain.
*  And that takes a type of coordination too. And so that's another set of games that I've been really interested in.
*  Is there some kind of classification scheme for at least all two-person games?
*  I mean there are names to different games and
*  games get a name based on the kind of ordering of the payoffs and what that means for the equilibria of the game.
*  So the prisoner's dilemma is always going to have this structure that we've described where there's a strategy that's
*  rational to take, it always benefits you,
*  but will get lower payoffs for both actors than the other strategy. A coordination game will have at least two strategies,
*  maybe more, and a kind of situation where if actors manage to match their strategies in the right way,
*  they get better payoffs than if they don't manage to match their strategies in the right way.
*  So, I mean maybe an explicit example would be helpful. Is the stag hair game an example?
*  It's a kind of coordination game. I'll show that you pick rather than suggest it was.
*  I'll just give you a totally like here's the basic coordination game. You have two strategies, maybe it's
*  A and B, and let's say these represent driving on the right side of the road and the left side of the road.
*  And you get payoffs if you both pick A, which is both driving on the right side.
*  You get good payoffs if you both pick B, which is both driving on the left side.
*  But if one of you picks A and one picks B, you don't get good payoffs because you crash into each other.
*  So there's a super simple example.
*  And a physicist would say it's an example of spontaneous symmetry breaking because it didn't matter whether we picked left or right,
*  but picking the same one for everybody is definitely advantageous.
*  Right, exactly. And so you can ask why in certain societies or how do we break this symmetry and come to different
*  conventions where we all do something that helps us coordinate our behavior, but ultimately it could have been something else.
*  There are lots of other things we can do in some situations. In this case, there's just two options.
*  But if we look at say what time we start work in the morning,
*  there's actually a lot of options we might have had and
*  societies have to somehow come to a convention about when they're going to do that.
*  If they're going to coordinate things like when is daycare open, when are restaurants open for lunch?
*  Well back, you know back when daycare and restaurants were.
*  We imagine. So look, they're going to be listening to this podcast a hundred years from now. So we got to speak in universal terms.
*  Oh, yeah, I probably yeah.
*  They might not be restaurants anymore, but for different reasons.
*  And I mean somebody you mentioned like when you talk about the times that we start working or the times that we have dinner,
*  there's clearly
*  coordination going on and
*  different societies answer that question differently. I mean, this is probably leaping ahead a little bit,
*  but how much do we actually compare to data in this field?
*  I mean, do we build a model based on imagining a certain dynamics arise in the world?
*  Based on imagining a certain dynamics arise from certain games and then we test it empirically?
*  It really depends what you're trying to do with the model.
*  And so I think this is a big picture thing that I I'm always trying to convince people about models.
*  A lot of people will look at simplified models like games or game theoretic models and say well, this is so simple.
*  What can you even do with this? What can it tell us about the real world?
*  I mean, I think the answer totally has to do with
*  what are you doing with it? What are you trying to find out about? How are you using it?
*  What inferences are you making on the basis of it?
*  And then you also see that depending on what kinds of scientific inferences people are trying to make, what they're investigating,
*  they use games very differently.
*  So sometimes people will build models and compare them to data sets of what real humans do.
*  Sometimes people will look at a human behavior like the fact that say
*  everyone in India drives on the left side of the road and
*  everyone in Canada on the right side and say like, okay, well, can we explain this cross-cultural
*  diversity but inter-country regularity?
*  Well, maybe we can use coordination games to improve our understanding of patterns like that.
*  And in that case, you would be not comparing specifically to a very detailed data set,
*  but essentially looking at some empirical phenomena and then
*  making some comparison between what you see from the model and that. And then sometimes people use games
*  to do
*  almost kind of proof of possibility modeling. Someone will say
*  it's impossible that x and someone else will say, well, no, I can build
*  a game theoretic model that shows that is perfectly possible. So to give an example from philosophy,
*  there's this tradition of natural language skepticism,
*  quine with someone in this tradition who argued
*  you can't evolve
*  language naturally.
*  Basically the idea being like if you want to have a convention for language, you need language to establish it to tell each other.
*  Okay. Well tree is going to mean tree.
*  But modeling work by David Lewis and then Brian Skirm showed that
*  Well, no, you can you can have extremely simple agents learn how
*  to coordinate on a signal to mean something without needing any sort of prior language. So that's
*  modeling where you're just showing a proof of possibility when someone has made an impossibility claim.
*  So yeah, does that make sense? I mean, it's used in very different ways and has very different connections to data
*  across different cases. Yeah, actually, I think it makes perfect sense. I mean in some ways, um,
*  there's a sort of the model building aspect for some particular empirical phenomenon you're trying to understand. But there's also just the
*  theoretical aspect of understanding what it means to be rational. Like you might say like well,
*  there's no rational way to do this. But then when you dig into the details of people
*  playing by the rules of this game, you see some emergent phenomenon that you might not have guessed.
*  Yeah, and I also here's another one like another role that I think
*  often game theory can play. You're looking at certain cultural patterns and you're trying to reason about them.
*  And then sometimes it plays a role as like an aid to reasoning.
*  Um, it's not that you're doing something much different than you would be doing just thinking about okay, what do I see
*  in the data empirically that humans are doing. But it kind of helps you organize your thoughts.
*  So I think of Christina Bigieri's work on norms, which uses a lot of game theory as falling under that kind of category.
*  It's not that she's doing these like super complicated analyses, but she's saying like, okay, well, what if we
*  used a game to think about norms this way and what if we
*  thought of norms as changing the payoffs of the game in that way?
*  So that's like kind of another way you can use games. Right. And in particular we can so that's I don't
*  there's so much to talk about here. I'm sort of tongue-tied about it. But we did say we're going to talk about evolution.
*  So like what is the particular way that an evolutionary biologist
*  would think about the evolution of different traits using this kind of game theory, uh,
*  technology?
*  Yeah, good. And so this really starts to get into
*  the area that I work in more which is called evolutionary game theory and it's it's related to other kinds of evolutionary modeling.
*  So this area really started taking off in biology,
*  uh in large part like due to the work of John Maynard Smith.
*  Um, and the idea was something like this. Okay, we have these games they're representing strategic scenarios.
*  Mostly the way people had been analyzing games up to that point was like assuming
*  okay, the actors involved are rational or semi-rational. They're going to sit down.
*  They're going to engage in some kind of deliberative process and then decide what strategies they're going to play or what actions they're going to take.
*  And biologists thought well we have
*  animals, organisms of all sorts engaged in strategic interactions all across the biological world.
*  I mean they're bargaining with each other. They're coordinating with each other. They're signaling to each other. They're behaving altruistically,
*  prosocially, non-prosocially.
*  But most of them we know aren't sitting down and engaging in a rational deliberation.
*  Instead a lot of their behaviors have evolved.
*  And so the idea behind evolutionary game theory is you instead take a population of agents.
*  You assume they have certain behaviors that they might play in a game. So maybe some of them are altruists and some are
*  non-altruists or whatever it is.
*  And then you ask how does this group evolve and cannot explain things about strategic behavior in the biological world?
*  And so I know that in
*  individual games of incomplete information like poker or even just like a sport out there on the field where you don't know what the
*  other player is going to do,
*  mixed strategies are often the best ones to use where you have a certain percentage of the time you do one thing,
*  a certain percentage you do the other. But I presume that in a complete information game
*  there's something that you should be doing.
*  But then when you introduce the population idea, does it become again something where the equilibria are more subtle where
*  a certain fraction of the population does one thing and a certain other fraction does another thing even though
*  they're both, you know, it's clear that one is overall better, but it's also better that there's a dispersal of strategies.
*  It can depend on the situation. So first of all, I'll point out that sometimes when you have games with complete information
*  it's still best to mix up your strategies. So if you look at rock paper scissors,
*  even though everyone knows exactly everything about how that game works,
*  your best thing is to randomly mix among your strategies. Fair enough. Yes, that's true. So I'll just point that out and then
*  oh, sorry, what was the... I'm just asking is there analogy between that incomplete information and the population dynamics
*  so that you get the equivalent of a mixed strategy in the sense of different members of the population doing different things.
*  All right, so we're sort of like diving...
*  It's like the general introduction that we're diving into like really...
*  That's why we're here. More theoretical stuff.
*  Right, so when you're looking at individual behaviors you have these mixed strategies where
*  sometimes the best thing to do is sometimes do one thing and sometimes do the other thing and that can kind of confuse your opponent or
*  make it impossible for them to predict what you're going to do and then force them to behave in certain ways.
*  Now in biological populations, you can have an equivalent of that which
*  can work in different ways. So one thing is that you might have
*  part of the population that behaves one way all the time and then another part that behaves another way, but all the time.
*  And so that's a kind of thing where everyone has a set behavior or strategy,
*  but there's a mixture or variation if you look across the whole population.
*  Then you could have another situation, which is that everyone kind of learns to have some mixed types of behavior
*  or many individuals end up having some mixed or sorry evolved to have some kind of mixed behavior. So
*  for example escape behavior is often
*  quite stochastic. I mean the
*  bunny doesn't run right all the time or left all the time. They mix, right?
*  And so it's not that there are some right bunnies and some left bunnies, it's that all the bunnies have some randomness in their strategy.
*  Let me pause for a second to talk about the Peloton Bike.
*  If you want to keep your mind sharp, keeping your body in good shape is part of the process.
*  But if you're like me, it can be very difficult to either find time or somehow work it into a schedule with so many other things
*  to do. That's why Peloton is so great. The bike is always there,
*  it's in your house, when you feel the spirit move you, you can hop on it,
*  but you also get the structure of guided classes if that's what you want. You can join live classes throughout the day
*  or choose from thousands of on-demand classes for a truly immersive experience.
*  You can filter what classes you want by your music preference, by the class type, the length, intensity.
*  You can find your favorite instructors and keep going back.
*  And now new riders get to experience Peloton with a 30-day home trial. If you decide it's not for you within those 30 days,
*  they will come pick it up and you will get a full refund.
*  So experience game-changing cardio with the Peloton Bike. Visit onepeloton.com to learn more. That's
*  o-n-e-p-e-l-o-t-o-n.com
*  I guess what I have in mind is something like, you know about Richard Lensky's long-term evolution experiment.
*  I don't know. Is this the one where they keep doing the bacteria?
*  Yeah, they have these bacteria and they just, you know,
*  breed them in generations and generations and generations and generations and like throw away most of them at every step,
*  but keep a certain fraction. And what they found, and it's in exactly the same, you know,
*  culture, whatever it is,
*  food source that they have, and
*  some small fraction of the bacteria evolved a way
*  to eat
*  a different kind of sugar than the other bacteria did. And what was interesting was that this new strain,
*  you know, the sugar that they were eating was less efficient.
*  It was less good as a food source,
*  but of course there was also less competition for it since all the other bacteria were eating something else.
*  And I'm wondering if there's a game theoretic explanation for that.
*  Well, so I don't think that would probably be best
*  handled using a game and the reason, well,
*  I mean, so usually when you're thinking about a game, you're thinking about,
*  right, an interaction between two organisms where they have different choices and they care what the other one does.
*  This, probably you'd want to use a model that was less game theoretic and more about utilizing underused resources
*  if you wanted to think about this kind of scenario.
*  Okay.
*  So I guess, yes, there's limitations on how far the game theoretic paradigm works here.
*  I mean, maybe you could sort of force it in by saying that
*  there's two strategies, eat this molecule or eat that molecule or something like that.
*  Yeah, and that the whole population would want to have a mixture.
*  But yeah, I think it's not a sort of paradigm case where you'd want to, where game theory would be like the best way to go
*  to represent that population, I don't think. If it's like the rest of science,
*  I'm sure that the game theorists just think that they should just use game theory for everything, right?
*  We hammer an awful lot of nails with the game theory hammer.
*  Whether or not it's the best way for sure.
*  So the other terminology I wanted to get on the table was that of an evolutionarily stable strategy.
*  So this is related to but not the same as the Nash equilibrium for these games.
*  Yeah, there are formal relationships. So this is kind of a solution concept for an evolving population.
*  And the idea is to ask, all right, if we have, what strategies where,
*  sorry, what are the strategies where if the whole population is playing that strategy, it's stable to invasion by other strategies.
*  So if we introduced a little mutant who was doing something else,
*  would this population still be stable? Would that mutant die off?
*  And so there's a set of conditions that you can ask about payoffs that will tell you whether a certain strategy is
*  evolutionarily stable. So things like does it do better against itself than other things?
*  Do against it? If so, it's going to be stable because other variants or mutants are going to die off when they enter the population,
*  because they just won't do as well against this kind of dominant strategy that exists.
*  Or if mutants do kind of equally well against the existing strategy,
*  how do the mutants do against themselves versus the kind of dominant strategy?
*  And so you can you can analyze whether a strategy is evolutionary,
*  evolutionarily stable by asking questions about how do mutants do against themselves.
*  And so you can do that by asking questions about how do mutants do against themselves.
*  So you can analyze whether a strategy is evolutionarily stable by asking questions about these relationships between the payoffs that different strategies get.
*  This might be asking too much, but is it possible to give an example of where the national equilibria are or are not evolutionarily stable?
*  Yes. So, oh god, I don't want to...
*  Okay, there are like
*  conditions that one could accurately say about the relationship between these, but I don't want to say it and mess it up.
*  It's the kind of thing I always look up on Wikipedia every time.
*  But I'll give you an example where the Nash equilibrium is the evolutionarily stable strategy.
*  Just the prisoner's dilemma in a totally basic population is also the evolutionarily stable strategy.
*  So if you have a population without adding extra bells and whistles, it will just evolve to always all defectors.
*  And because we can just easily see that because if somehow they manage to coordinate and all become cooperators,
*  they're doing better off, but one defector wanders into the room and individually does better off and then everyone goes,
*  huh, defecting might be better after all or something like that.
*  Right, or the defectors, you know, in the kind of biological situation would start to have more offspring.
*  Yeah, okay. Well, I guess good. Maybe we should be more specific about that.
*  How do we reconcile or marry the usual way of thinking about
*  evolution where we have like fitness and replication rates and survival rates to the game theory lingo in this way?
*  So I'm gonna pull this apart into two things.
*  So when we're looking at evolutionary game theory,
*  some models are really thinking about evolution by natural selection and representing that.
*  And so the idea is you have these actors engaged in strategic scenarios, and then the payoff they get
*  is some kind of benefit that allows them to have more offspring on average. And so you say, okay,
*  next generation, the ones who did well in these strategic interactions are going to have more offspring.
*  There's going to be more of that kind of behavior in the population.
*  And so that will determine a kind of evolutionary trajectory towards increasing the prevalence of strategies that do well.
*  Now people also use evolutionary models to think about cultural evolution and even kind of individual learning within a lifetime.
*  And there you're making slightly different assumptions, but your assumption might be something like this.
*  There are different possible driving behaviors.
*  Some people might try out the right side or the left side, and we'd want to be thinking about like way back when cars or
*  oh, probably actually earlier wagons or horses were first being used on roads. So people might be trying different directions.
*  And
*  people might learn that, okay, when I go to the right, that tends to do better for me.
*  And so I'm going to stick with that behavior more often.
*  So I might learn based on the environment that I'm in to kind of go with one strategic situation.
*  So I'm not like literally having offspring who go on the right side of the road.
*  I'm learning and then I might be passing that learning on to say my offspring or my friends.
*  So there might be some kind of cultural transmission or imitation of successful behaviors.
*  Well, yeah, but so this is important because in the real world,
*  at least in the cultural world,
*  we can not only sort of see the situation we're in and try to optimize for whatever reward we want,
*  but we can also talk to people.
*  We can signal, right? And so part of the fun of this approach to evolution is talking about the emergence of
*  signaling and cooperation and things like that.
*  Yeah, yeah, exactly right. I mean, is it is it do we talk about sort of the robustness of how
*  certain signals develop and it's it's useful and then you can start saying well, well, what if someone starts lying and or being deceitful somehow?
*  Yeah, so, I mean, this is sort of so in the the book that prompted this discussion games in the philosophy of biology.
*  I start basically the first half of it covers
*  signaling behaviors of different sorts and especially the evolution of signaling behaviors as they've been
*  analyzed by philosophers of biology and different biologists.
*  Now, sometimes you can apply these models to like human language,
*  though you have to be very careful because they're really pretty simplified models to think about the complexities of language.
*  So you want to apply them to think about certain particular aspects of language.
*  More often, people are applying them to think about signaling in the biological world like the peacock tail or the
*  large jumps of the antelope when they see a predator and things like that.
*  Right, and so does it work?
*  Do we think that the understanding of why I mean, there's a lot of goofy things out there in the natural world in terms of
*  things that look like tremendous wastes of energy and resources, but that we think of as
*  serving the purpose of sending signals. Is that something that game theory has explicated for?
*  Yeah, it has. Well, I mean, so
*  usually you want to break up this literature on game theory and signaling approximately into two big camps, and one is common interest signaling signals when the two individuals involved kind of want the same thing.
*  So this might be like, you know, my husband is at the beach and he wants me to come and get a bite of the food.
*  And we're talking on the phone and I say, well, how's the weather? And he says, it's really sunny. And I'm like, okay. And then I pack a big sun umbrella.
*  So we both want the same thing. We want the right items there at the beach.
*  So that would be like a common interest type of scenario.
*  A conflict of interest type scenario is the sort of other area of the world where we want to be able to get the right items.
*  So that's the kind of thing that we want to be able to do.
*  So that would be like a common interest type of scenario.
*  A conflict of interest type scenario is the sort of other area that you want to separate off where the sender and the receiver of information may or may not have the exact same interest.
*  So a kind of classic case from economics has to do with hiring in the job market where, you know, a company is trying to hire the best candidates, but every candidate wants to be hired.
*  So some candidates have interests that line up with the company.
*  They both like for that person to be hired, but some candidates don't.
*  They might not be a very good candidate, but they still want to get hired.
*  And so we break signaling up into approximately these two different camps.
*  And then we can explain and understand an awful lot of things in human social world and the biological world using models in these two areas.
*  So if you think about the common interest models, they really make clear why signaling is going on basically all the time.
*  I mean, every I shouldn't say every organism, I don't know, but stuff all the way down to little tiny bacteria and yeasts and plants.
*  And, of course, insects and, you know, all the little wiggly stuff in the sea are signaling to each other.
*  And the reason is that there are all these benefits to being able to coordinate your action.
*  And often the only way to do it is to somehow send information between organisms.
*  And then there are, you know, there's lots of more details of how we can use these models to think about the different ways that we all engage in common interest signaling.
*  And then this other area, the conflict of interest signaling, helps explain some of the stuff that you were just talking about.
*  Why do we see these tremendously weird seeming adaptations like really long tails or I mean, if you ever look at the face of a turkey, it's just a mess.
*  I mean, like this weird blue goo thing all over its face and neck.
*  Like, why or, you know, why do mantis shrimp have 16 different kinds of cones to see a bajillion colors?
*  Presumably, we can help use help explain a lot of those things using conflict of interest signaling theory.
*  And this does seem like a field that is potentially ripe for just so stories.
*  So you can come up with an explanation based on game theory for this or that weird behavior.
*  But then how do you know if it's the right one?
*  I mean, how do we test these ideas?
*  Right.
*  Well, I mean, game theory is in part the answer to how we know.
*  I mean, in many ways, evolutionary modeling, including evolutionary game theory, helped throw out a lot of just so stories in biology and provide at least some sort of way to constrain what sorts of stories or narratives.
*  People could tell about the evolution of certain traits.
*  So going back to group selection, you know, before this work on group selection and altruism, a lot of people would say, oh, well, they'd say things like the reason we have senescence, you know, death of organisms before they had to die is so that the younger birds can live their lives and have more food or whatever.
*  And so that's a very speculative narrative.
*  If you actually take a model and sort of put numbers on what are the benefits to birds from dying young and dying old and how might that translate into offspring, you have a way to think more rigorously about evolutionary processes.
*  Is it also possible then to sort of discuss, I don't know what the word is, some sort of fragmentation of populations into different groups where some groups say, well, we're going to have one strategy, we're going to cooperate with each other, and we're going to signal to each other so we know whether or not you're in the group.
*  Yeah, so we do see that kind of thing happening, especially, I would say, in human groups.
*  So fragmentation or speciation can happen for different reasons.
*  So one can be that you're just in different locations, right?
*  That these organisms are over on an island and then they might evolve to have different behaviors than these ones over on the mainland as a kind of accident of history.
*  Or as you're pointing out, sometimes you can have situations where you can have some kind of signal of in-group membership that identifies members of a population and gets them to treat their own group differently than another group.
*  And people sometimes think that's a way to think about in-group signaling in human groups, you know, that different cultures or different tribes or whatever might dress in different ways, engage in different sorts of behaviors.
*  And that would be used as really relevant information about how you're going to interact strategically with someone who displays in a certain way.
*  Yeah, I mean, let's just boldly hop right from biology to human beings and how they deal with each other.
*  I mean, I've heard it claimed that signaling plays a much more important role in human behavior even than we might guess.
*  Like the only reason people go to college is to signal some achievement or the only reason people wear the right clothes or go to the right movies or whatever.
*  Do you think that's something we can analyze game theoretically?
*  Yeah, well, so I'm guessing these claims really relate to this literature I was mentioning on conflict of interest signaling.
*  So there's this famous paper there looking at job seekers where they say, let's suppose they're kind of high quality candidates and low quality candidates for jobs.
*  And the company wants to identify the high quality candidates.
*  But how can they do that given that everyone's going to portray themselves as high quality?
*  Well, they say, what if it costs more for the low quality candidates to go to college?
*  It might be more work for them is the idea in this original paper.
*  Then maybe we can have a situation where only high quality candidates will be willing to put in the work to go to college.
*  And so the diploma isn't valuable to them because of the knowledge they gain, but rather as a signal of their ability to pass classes and things like that.
*  And so people have applied that kind of thinking to various things in the human realm where people do things that are very hard to do.
*  And then it can become kind of a signal of their abilities to others.
*  I mean, I guess in some sense, it's bringing into question what it means to be rational.
*  Like the you know, you're rational in some sense, you're playing a game and you're getting the reward that you want.
*  But it leads to behavior that might from some remove be perceived as less than completely what we would like to aspire to as rational beings.
*  Yeah. So that's the really interesting thing about this whole literature on conflict of interest signaling and then using costs to, you know,
*  guarantee that signaling can happen. And this really relates also to biology because people bring up the same thing like, well, you know, a peacock tail maybe is reasonable in some way if it can get the peacock to mate.
*  But it costs so much, right? It's a huge expenditure of protein. It's hard to drag this thing around.
*  Can that really be a reasonable adaptation? And I think it depends how you're thinking about these things.
*  I mean, if you count your social environment as a legitimate part of your environment, then it's totally reasonable to expend a lot of costs to signal certain things about you that are going to get you social benefits.
*  Or in the case of the peacock, you know, sort of reproductive benefits.
*  And I mean, that's clearly true that it is part of your environment, right?
*  I guess it's just, you know, well, this is why I'm a physicist, because it's too hard to disentangle all the different factors that are going to enter an equation like this.
*  Yeah.
*  But you want to go even further. I mean, I don't know how much of this is sort of standard lore, but in your other book, you talk about the origins of inequality, the origins of unfairness.
*  And you point out that we can use game theory to explain why it might seem rational to do something that ends up with a deeply unfair division of labor in our society.
*  Yeah. So I mean, I think this is important to understand, because I've seen analyses before where people will say something like this.
*  You know, if we look at bargaining games and bargaining and a whole society, we should always expect to end up at a fair outcome, because nobody is going to accept that they get an unfair amount.
*  And so they're just going to refuse to play and we'll end up with fairness.
*  Yeah.
*  Now, I think that analysis ends up being a terrible kind of analysis, because people are rational given the social situation they're in, or they make choices given the social situation they're in.
*  And this, by the way, really relates to previous work by people like Ann Cudd and Susan Okun.
*  So you can have these situations where, say, you have two identity groups. These could be men and women or maybe identity groups of different races.
*  And they're engaged in maybe some kind of bargaining scenario.
*  You know, who has to pay more or less? Who gets the really prime jobs or not? Who can sit at the front of the bus or has to sit at the back of the bus?
*  So there are these kind of bargaining situations they engage in.
*  And you can use evolutionary models to show that you'll end up at inequitable conventions a lot of the time where one group's getting more and one group's getting less.
*  But if you look at the group getting less at every stage of the way, they were changing, like learning and adapting in ways that made completely perfect sense given the situation they were in.
*  So they're taking the best behaviors they can to get the best payoffs and outcomes they can, given the pressures they're under from another group.
*  And so is it just because, again, from a physics perspective, I think about these in terms of landscapes, of energy landscapes, and you sort of fall to the valley of a minimum and then you're metastable there, even though it's not the lowest point where there could be some even better vacuum that you just can't find?
*  Is it an accident of history that you end up in this inequitable division? Or was this sort of more globally rational?
*  It's just sort of an inevitable thing that some people are going to get screwed by the system.
*  Well, so here's the kind of dialectic I go through in this book.
*  You know, I show that if you take cultural evolutionary models, suppose you have a group of people and they're all bargaining with each other.
*  Well, in these models, most of the time, fair outcomes emerge unless you split the group into different identity groups.
*  And as soon as you do that, you get inequality emerging.
*  And that basically has to do with this way of breaking symmetry.
*  As soon as you have two groups, you have these identity markers where you can say, well, I'm type A, you're type B, and type A's treat type B's in this kind of way.
*  And so that allows inequality to emerge.
*  Now, in those models, it's just an accident of history.
*  There's nothing about either group that makes one more or likely to get better outcomes.
*  So in that case, it really is just this kind of symmetry breaking type of argument by random chance.
*  These people were being more aggressive in their bargaining early on, and these people less aggressive.
*  And then they all learn to end up at this kind of pattern where one group is getting more.
*  So I kind of start showing, all right, all you have to do is break into identity groups.
*  You already have this happening.
*  But then, of course, if we want to think about real cases or slightly more realistic cases, there are often going to be certain factors that can help explain why one group or another is going to end up with more in bargains.
*  So if we're thinking about gender facts about like physical sex differences are sort of an important symmetry breaker in determining who's going to end up with better outcomes.
*  If we think about cultural groups or racial groups or groups like this, there are going to be facts having to do with power, economic advantage, maybe with minority status, all of which might matter to which group ends up with more or less when we come to sort of conventional bargaining outcomes.
*  Yeah, no, I can imagine that, again, the messiness of the history is really crucial here.
*  But maybe it's worth digging in a little bit into the technicalities of this claim, because I remember I was struck by it in your book that just the existence of different identification markers that say you're in one group versus another changes what the rational strategy is.
*  I mean, basically, because there's a new possibility opens up, I will have this strategy against members of my group and this other strategy against other people.
*  Exactly. And so I think that the kind of really core, one of the most core interesting observations to communicate here is this thing where if you think about bargaining, you know, if you and I are engaged in a bargain, there are many different kind of outcomes we can coordinate on ones where I get less and you get more ones
*  where I get more and you get less or ones where we kind of equally split our resources.
*  So in models, these last type, the equal split outcomes are special because you and I can do the exact same thing and perfectly divide our resource.
*  So it's a kind of special equilibria where you're being fair to each other.
*  And other philosophers like Jason Alexander and Brian Skirms and economists like Peyton Young have used this to try to explain why we have norms for justice and fairness, right?
*  So you have the symmetric equilibria that emerge when you don't have groups. And then when you just add identity markers so that I can say, well, I'm this type and you're that type, a whole group of new equilibria become possible,
*  which are equilibria where members of one group get some amount more and members of the other group some amount less.
*  You literally can't evolve these in the other kinds of models.
*  But in models with groups, not only can you evolve that sort of equilibria, but they just kind of pop out of the dynamics all the time.
*  And this is, I mean, you'd want to say it's a super robust finding.
*  It sort of doesn't matter how you do the model.
*  You still get this happening because of this new asymmetry where you're able to identify who's of what sort of group and then treat them differently.
*  Is it worth actually sort of giving you details about an example game in which this would be the case?
*  Yeah, so the bargaining game is a perfect example game.
*  So say we have a bargaining game where you and I are dividing something like people often will say a pie.
*  And let's say here are our strategies in the game.
*  We could each ask for a third, a half or two thirds of the pie.
*  So that's a specific game.
*  We have our strategies, we have players.
*  If we imagine a population playing this game, if we have just a population where people don't have any special identity, you almost always get everyone demanding one half for a lot of the time.
*  Sorry, is there a stipulation that if we both demand two thirds, then we get nothing or something like that?
*  Oh, sorry. Yes, I should have been more specific.
*  So in this game, the payoffs are that you get what you demanded as long as you don't demand too much.
*  So if we both demand two thirds, we're sort of pushing too hard and then we get a poor payoff for pushing too hard.
*  And so the equilibria of the game are that we can either both demand half.
*  You can get a third and I get two thirds.
*  Right.
*  Or you can get two thirds and I get a third.
*  Yeah, which is an interesting point for those who are not professional game theorists in the audience.
*  Like all three of those possibilities are, technically speaking, equilibria.
*  Like one is fair 50-50, but if one person always asks for two thirds and the other one always asks for one third, that also satisfies the criteria for being a stable set of strategies for the game.
*  Right.
*  And if one person asks for 99% and one person for 1%, that also satisfies the criteria for being a stable equilibria of the game.
*  And then so how does this change when you get groups in there?
*  And so, yeah, the idea is when you get groups, the fact that I know you're in one group and I'm in another can give me this extra information where I can say, well, people in that group always demand a third.
*  So when I meet members of that group, I can demand two thirds.
*  I can ask for more.
*  And the idea is that this kind of reasoning can actually help us understand, for example, how we divide up household chores.
*  I mean, like you're very careful.
*  You said, you know, look, there are differences physiologically between men and women on average.
*  So that's some part of the explanatory role being played by that.
*  But there's also just this symmetry breaking that screws some people over.
*  Yeah.
*  Well, so if we look culture to culture, there are cultures that have pretty gender egalitarian norms and conventions where there isn't a big power imbalance between men and women, where there maybe isn't a big difference in how much labor they do altogether.
*  And then there are cultures that are much less gender egalitarian.
*  And so part of the point here is that these are conventions that emerge in part due to chance or, you know, almost random historical factors or accidents of history where we end up with groups that are much more or much less fair once we have a kind of division into two genders.
*  Yeah.
*  And you make the point that I'm not quite sure how to put this, but they get baked in like a tiny little variation in some initial stage can lead to huge imbalances down the road.
*  Yeah, part of the point I make is something like this.
*  You know, the sort of first most important role that gender plays in many societies is as a locus for dividing labor and division of labor is not as important in modern societies as it used to be.
*  So we still divide labor.
*  You know, if I say who in that household is cleaning the windows versus taking out the trash, you'll make certain guesses based on the genders of the people.
*  But if we look at more traditional societies, it was completely necessary to divide labor because there were so many skilled tasks that required doing that were very hard to learn to do.
*  And so having a rule where it's something like, well, women will always be the ones who learn to make rope.
*  Men will be the ones who learn to fire ritual pottery.
*  Women will be the ones to dig tubers.
*  Men will be the ones to build houses, really benefited groups.
*  And so you see these kind of conventions emerging where gender becomes this really important locus for division of labor.
*  And once gender becomes really important in determining who's going to do what, it also then becomes important in determining who's going to get how much and who's going to deserve what.
*  And that's in part because people get different things based on what they're doing.
*  You know, the person who controls food production might, you know, be in a special position within a family or a society to kind of have more for themselves, demand more for themselves.
*  And then it's also just because now you've kind of created this meaningful social difference where people can look at that difference and be like, well, we think that these kinds of people are different than those kinds of people.
*  These ones are the rope makers.
*  These are the ritual pottery makers, the rope makers.
*  They're the kind that deserve less than the pottery makers.
*  And so you can have these kinds of meanings and conventions attaching to these categories that emerge for other purposes.
*  Is it is it a slightly depressing conclusion in the sense that, you know, it was it was sort of not some evil or unfairness in the hearts of people that made this happen, but it was just the dynamics of evolution under the constraints of rational game players?
*  Well, I try to be really careful about that because there is evil in people's hearts sometimes.
*  And there's a lot of discrimination and there's a lot of really morally wrong discrimination.
*  And these results shouldn't be taken as indicating that any of that is less morally wrong than we thought before, for sure.
*  But I think part of the important thing to realize is that inequity kind of has a life of its own or a tendency to arise via very non-offensive things.
*  So if you have groups of people, if you have kind of social identity groups within them and if everyone tries to learn to do what's best for them, inequity can just emerge from those preconditions, which I think helps explain why it's both so common cross-culturally, why inequity based on social identity is the rule, not the exception, but also tells us that it's quite likely to happen and should require.
*  Sort of active responses rather than passive ones.
*  I mean, it won't, you know, I think there's kind of an idea like in thinking about social inequality, like we'll keep trying to fix it and then we'll get to a point where it's fixed and then we're done.
*  And I think this analysis says that's not really the right way to think about it because these really basic forces having to do with learning and strategic interactions are going to push us towards inequitable patterns almost on their own.
*  And so it's going to be the kind of thing that you'll want to you'll have to always be thinking about, always be actively addressing if you don't want to have high levels of social inequity.
*  Yeah, so there is some I mean, but that is that that is the depressing part of it.
*  Well, I mean, there's this it's almost saying that maybe it is saying that an unequal division of resources and rights and privileges in society is the natural thing.
*  It's not saying who is supposed to be the winners and the losers necessarily.
*  But, you know, it's a rich get richer kind of phenomenon.
*  Yeah, well, let's specify that when we say natural, we'd say something like it is the thing that, you know, emerges naturally.
*  But we certainly wouldn't want to give it like the moral weight.
*  You know, sometimes people say natural.
*  It's not the good thing.
*  Oh, that's the good thing.
*  Right. Or we can't help that.
*  It's natural.
*  And we want to say one thing about it, but not the other.
*  Right.
*  I mean, the flip side is you can make an argument that people shouldn't be as touchy or defensive, right?
*  You know, about fixing it.
*  You can just say, look, I'm not saying that people were evil.
*  I'm just saying there was evolutionarily game theoretic dynamics going on, and we need to actively take a role to counterbalance that.
*  I mean, I think it should give us, you know, some reason to think like, and I don't want to say this about, you know, sort of every discriminatory behavior, but some sorts of discriminatory behavior, some reasons to think like if I were in that social position, I'd be behaving the same way potentially.
*  Right. Right.
*  So like, it is weird to say counterfactually, if I had been a man.
*  Yeah.
*  Like a lot of things.
*  You wouldn't be you.
*  Right.
*  Yeah.
*  It's hard to say.
*  If I had been raised as a man, I think I very reasonably would have learned all, you know, I would have been socialized to expect to get more than I'm socialized to expect to get as a woman.
*  And that would have been pretty natural for me.
*  Yeah.
*  And so it maybe gives some locus for extending understanding to people who are behaving in discriminatory ways on that kind of grounds.
*  I mean, that might be something you could take away from this.
*  Well, and also just to go back to the prisoner's dilemma, you know, and the idea that if the two prisoners were allowed to talk to each other and work out a mutual cooperation strategy, then they would, you know, neither one of them would defect and they would get the best possible outcome.
*  In the real world, we like to think that we can take this meta step back and be rational and say, like, OK, well, there was this natural dynamics that ended us up in this equilibrium, but it's not in some sense what we want or what we value.
*  So let's take action against it.
*  Yeah.
*  And actually, I think that's much more plausible when it comes to bargaining than when it comes to the prisoner's dilemma.
*  So actually, in studies of the prisoner's dilemma, like people being able to talk can do something beneficial for them.
*  It does not guarantee that people don't defect.
*  With bargaining, it's different from the prisoner's dilemma in that there are all these different equilibria.
*  Right. In the prisoner's dilemma, the only equilibrium is the kind of bad one in the bargaining game.
*  The fair outcome is an equilibrium.
*  Now, just thinking like we can all sit down and talk about what we want and get something better might not quite get us there, because if we think about an equilibrium, say, where those in one social group are getting more and the others are getting less.
*  Well, the people getting more, you know, it's really good for them to get more.
*  And so just a conversation like, hey, what's happening isn't fair.
*  Let's fix it up.
*  May or may not be persuasive.
*  And if you look at what happens in the real world, this kind of outside model world now, often people engage in all sorts of kind of special thinking or reasoning to justify why they ought to get more in a sort of discriminatory situation, right?
*  To justify the status quo that benefits them and to have some reason why that should be OK.
*  So a discussion alone won't necessarily...
*  I have no idea what you're talking about.
*  I've never met people who did that before.
*  This is some weird philosopher's concoction that I'm unfamiliar with.
*  Yeah.
*  Let me make sure that I actually I think I understand the point of the bargaining game.
*  So we're dividing the pie and roughly speaking, if there's a known group of people who have, you know, for whatever reason, we know ahead of time they're going to demand two thirds of the pie.
*  And I meet one of these people, then the even though the equitable thing would be for me to ask for half the pie, if I know they're going to ask for two thirds, I actually come out better just by asking for one third.
*  And that's sort of the is that a simplification, but fair origin of this bifurcation?
*  Yeah, that's the right kind of thing.
*  I mean, you have a and, you know, also sort of socialization over a lifetime plays an important role.
*  You come to expect these people are always going to demand a lot of me.
*  And if I want to get anything, I'm going to have to make a concession, right?
*  I'm going to have to be accommodating.
*  And so you learn to behave accommodatingly towards some people or in a more like demanding, aggressive way towards other people.
*  Yeah.
*  And and the hope is that in the real world case is a little bit less idealized than that.
*  And there are people who all their lives have been getting two thirds of the pie.
*  But if they if you ask them, they will say they're big believers in equitability and fairness.
*  And maybe you can get them to switch their strategy through hook or by crook.
*  Yeah. And we certainly do see this happen sometimes.
*  You know, it's not the easiest process because I think there is some resistance.
*  You know, if you're getting all the good stuff, why?
*  There's going to be some resistance to going down to a half.
*  But we do see these processes happening where, you know, like
*  there are these educational processes that make clearer to men or clearer to white people like you're getting too much.
*  And you are also a person who holds to fairness norms.
*  You know, most of us are in our society.
*  We think we ought to be fair that that's the right thing.
*  And so if you can really convince people you are getting too much,
*  you're behaving in a discriminatory way, sometimes it can change people's behaviors.
*  I mean, sometimes there's a very common strategy that says, look, just being fair to everyone helps everyone.
*  Right. Like, you know, discriminating against women or minorities or whatever is really hurting society as a whole.
*  But at least in principle, it's not necessarily true.
*  Right. I mean, it might just be there are finite resources and they're being distributed inequitably.
*  And we're making the big ask of asking people or groups of people to say, give up some of what you have in the name of fairness.
*  I think that's very insightful.
*  And I mean, so I understand arguments like feminism is for everyone.
*  And I think to some degree, there are truths to that, which have to do with things like the way people are constrained into certain roles based on their gender or even perhaps their race.
*  Like, you know, if you're a man, traditionally, you're not supposed to love caring for an infant or spend a lot of time with your infant.
*  Right. And so that's a kind of harm to you that you're being taken away possibly from a role you might really like to play.
*  And so in that sense, you know, a movement like the feminist movement is for everyone.
*  But I think it's really too simple to say fairness benefits all of us.
*  And it ignores a lot of the reasons why movements towards fairness like civil rights movement or Black Lives Matter movement or feminist movements meet so much resistance because people aren't dumb.
*  You know, I've been winning this game for years.
*  Well, I mean, OK, maybe maybe we've already answered this one, but maybe just to wrap things up.
*  Does this kind of analysis suggest strategies for making the world a more equitable place?
*  I mean, I don't want to end on like saying, well, it's just the law of nature that we've got we've got to stick with it.
*  I mean, we can appeal to people's senses of fairness, but can we in some sense change the game that we're playing so that it just becomes more rationally self-interested for everyone to be more fair to each other?
*  Good. I mean, so and I even though there's some kind of like, you know, depressing this of the lessons from this book, I certainly also would like to emphasize like we see cultures do better, you know, become more fair.
*  And then we see across cultures, some cultures that are much more fair than other ones.
*  So it's not like it's just hopeless.
*  Right. There are definitely ways that groups of people can be better with respect to equity.
*  So thinking about like the modeling in this book and some of the things it tells us about doing that, one thing that I think is kind of important to realize is that norms can be eroded before you ever see change happening.
*  So norms and conventions tend to be very stable because if everyone in society is adhering to them, there are these kind of forces having to do with payoff.
*  That means everyone else is kind of pushed towards adhering to them.
*  So if everyone in one group is demanding two thirds, that makes it you know, there's always this force pushing everyone in the other group to demand one third.
*  Right. But you can have situations where people stop liking a norm before they stop adhering to it.
*  Right. Everyone's still doing it.
*  But you can have people becoming dissatisfied with it, learning more about it, coming to believe that it's in just.
*  A related lesson to this is that you don't see change happening until actually people change their behaviors.
*  Right. So you can talk a lot, but at the end of the day, you've got to do something different for a norm to change.
*  And this can mean people who are disadvantaged making higher demands and just messing things up for everyone for a while till everyone else adjusts.
*  Or it can mean people in an advantaged situation conceding more, changing what they're demanding, saying, I'm going to give more until we end up at a new norm.
*  You usually see the first one more than the second one.
*  But so I think that's an important thing to think about.
*  OK, we can have people changing their minds about things, but the behavior can still look stable for kind of a long time.
*  And then if we want real change to happen, someone has to actually make that change, like change their strategy.
*  Do something different.
*  OK, Karl Marx, I thought that the philosophers were just trying to describe the world.
*  And here you are suggesting that we actually try to change it as well.
*  I mean, that's a pretty radical move.
*  But I am hopeful. I mean, look, I'll tell you how much of an optimist I am.
*  I like to think that these kinds of analyses, philosophical or biological or whatever, actually do help change people's minds.
*  I think it's very hard to tell.
*  I think that the effects of new ways of thinking are just so diffuse and so delayed in their impacts that you might never know.
*  But I actually do think it matters.
*  I hope you're right.
*  We'll do our part to spread the word.
*  So, Caitlin O'Cotter, thanks so much for being on the Mindscape Podcast.
*  All right. Thanks for having me, Sean.
