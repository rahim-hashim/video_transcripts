---
Date Generated: June 08, 2024
Transcription Model: whisper medium 20231117
Length: 4634s
Video Keywords: []
Video Views: 13372
Video Rating: None
Video Description: Patreon: https://www.patreon.com/seanmcarroll
Blog post with audio player, show notes, and transcript: https://www.preposterousuniverse.com/podcast/2022/04/11/192-nicole-yunger-halpern-on-steampunk-quantum-thermodynamics/

Randomness and probability are central to modern physics. In statistical mechanics this is because we don’t know everything about the distribution of atoms and molecules in a fluid, so we consider a probability distribution over what they might be; in quantum mechanics it’s because the theory only lets us predict measurement outcomes probabilistically. Physicist Nicole Yunger Halpern explains how we’ve been lagging behind at bringing these two theories together, and how recent progress is changing the landscape of how we think about the microworld.

Nicole Yunger Halpern received her Ph.D. in physics from Caltech. She is currently a NIST physicist and Adjunct Assistant Professor of Physics and IPST at the University of Maryland. Her Ph.D. thesis won the international Ilya Prigogine Prize for a thermodynamics dissertation. As a postdoc she received the International Quantum Technology Emerging Researcher Award. Her new book is Quantum Steampunk: The Physics of Yesterday’s Tomorrow.

Mindscape Podcast playlist: https://www.youtube.com/playlist?list=PLrxfgDEc2NxY_fRExpDXr87tzRbPCaA5x
Sean Carroll channel: https://www.youtube.com/c/seancarroll

#podcast #ideas #science #philosophy #culture
---

# Mindscape 192 | Nicole Yunger Halpern on Quantum Steampunk Thermodynamics
**Mindscape Podcast:** [April 11, 2022](https://www.youtube.com/watch?v=LyH4ZfN5ITk)
*  Hello everyone, welcome to the Mindscape Podcast. I'm your host, Sean Carroll. We've all heard of
*  the Clockwork Universe, right? This is the idea that came into physics post Isaac Newton and
*  Pierre-Simon Laplace, the idea if you knew everything that was going on in the universe,
*  you could predict exactly what would happen next. Of course, the Clockwork Universe isn't the current
*  paradigm for thinking about physics. Because of quantum mechanics, which came about in the
*  early 20th century, we know that the world as we observe it is not deterministic. There is
*  unpredictable quantum randomness in the world. But that quantum randomness is not the first time
*  that randomness or probability popped up in the history of physics. There's also famously in the
*  late 19th century, the advent of statistical mechanics. We had thermodynamics. We were
*  inventing in the 19th century, putting together the theory of thermodynamics, the science of heat,
*  and how different kinds of fluids and gases pushed on each other and did work and dissipated
*  entropy and all that stuff. And then Maxwell and Boltzmann and their collaborators figured out
*  that you could understand thermodynamics if you believed in atoms and molecules, if you believed
*  that things you thought of as a fluid or a solid are actually many, many, many little particles.
*  And these particles were more likely to behave in some ways than others. So probability came in.
*  The fact that entropy increases over time is not an airtight rule in Boltzmann's way of thinking
*  about it. It's just very, very, very likely. People didn't like this at the time. There were strong,
*  strong objections. But now it's the accepted lore. So you might think that statistical mechanics,
*  which involves probability, and quantum mechanics, which involves probability,
*  would be close bedfellows, right? They'd be natural partners. And you would think about
*  quantum statistical mechanics and its implications for either pure science or for technology or
*  what have you. Well, they are natural fellow travelers, but it's taken a long time for
*  scientists to really dig into the essence of quantum statistical mechanics. How do you combine
*  together the idea of quantum mechanics, replacing particles and fields with wave functions and
*  observations with probability outcomes with statistical mechanics, the idea that you don't
*  know exactly what the system is doing. You can only make probabilistic assertions about what's
*  going to happen next. Today's guest, Nicole Younger Halpern, is a leading expert, a very young person,
*  but nevertheless leading in the dawning science of quantum statistical mechanics and thermodynamics
*  and its relationship to things like information theory. How much do you know about a system? And
*  the fact that it combines crazy radical ideas with potentially real world experimental implications
*  has inspired Nicole to name her own field quantum steampunk. And she has a new book out called
*  quantum steampunk all about this revolution that's been going on for no more than 20 or 30 years,
*  I would say, in physics, where we're really understanding the frontier of statistical
*  mechanics with quantum at the heart of it. And so again, it has both implications for how we
*  think about the fundamental nature of reality, right, because that's what quantum mechanics is,
*  and maybe implications for things like how biophysics works, right, how DNA works,
*  or maybe for nanotechnology for building little machines at these tiny scales. Thermodynamics
*  and statistical mechanics were driven by the urge to understand steam engines at some sense,
*  in some way, quantum thermodynamics is going to be building steam engines at the molecular scale,
*  thus quantum steampunk and also the outfits are really, really cool. So this is a fun conversation,
*  cutting edge science going on here, but in a slightly different vein that we've had previously
*  on the podcast. I'll give you our occasional reminder that we have a website, preposterous
*  universe.com slash podcast, where you can find show notes, transcripts for every episode, you can
*  search through them, etc. All the archival episodes are just as awesome now as they were
*  when we recorded them. And there is a Patreon that if you want to support Mindscape, you can go to
*  patreon.com slash Sean M. Carroll, and you get both ad free versions of the podcast episodes. Plus,
*  it's the Patreon supporters that get to ask questions for the monthly Ask Me Anything episodes.
*  So plenty of reason to join the Patreon, although I would argue that the reason to do it is less for
*  tangible benefits than for the intangible benefit of being part of the community of
*  Mindscape supporters. We talk on the Patreon page, there are comments, there's a back and forth,
*  there's a whole back channel you don't know about if you're not a Patreon supporter. So join up today
*  patreon.com slash Sean M. Carroll. And even if you don't, that's okay. We still love you,
*  you can still enjoy the episode. Let's go.
*  Nicole Younger Halpern, welcome to the Mindscape podcast.
*  Thank you. It's a delight to be here.
*  I wanted to take advantage of having you here. You have this wonderful quantum steampunk label
*  for both your research and your new book. And obviously, both those words are very evocative
*  in very different ways. I had to laugh a little bit because when I wrote my book about quantum
*  mechanics, about many worlds, etc. I went to Amazon and I looked for word for books with the
*  word quantum in them. And I didn't find that because it hadn't didn't exist yet. But I did
*  find quantum leadership, quantum yoga, quantum healing, etc. So you're you're joining in a long,
*  not so dignified history of using the word quantum mechanics, but you're using it in a much more
*  respectable way. That's the intent. But I before we get to the quantum part,
*  wanted to think about the thermodynamic part of what you do. I mean, actually, maybe why don't
*  you just define what you mean by quantum steampunk? And then we'll then we'll back up a little bit and
*  lay some groundwork. Okay. Well, steampunk is a genre of literature, art and film. It's a genre
*  that combines Victorian settings from the 1800s. So some of the earliest factories, very smoky,
*  foggy London, the wild, wild west men and top hats, and puts those together with futuristic
*  technologies like time machines, dirigibles and automata. So this is genre of sci fi. But I see
*  it as really coming to life in science that's happening now at the intersection of quantum
*  physics, information theory and thermodynamics. Thermodynamics is the study of energy and it was
*  developed during the 1800s, motivated by those smoky factories, the steam engines that were
*  driving them. So quantum thermodynamics, this bringing together of these three fields is
*  partially Victorian, but quantum information is part of the science and technology of the future,
*  people are building quantum computers now. So I see quantum steampunk as really the spirit
*  of quantum thermodynamics and as sharing its aesthetic with this genre of literature, art and
*  film. And especially the word steam is just a wonderful fit because like you say, that was the
*  origin of thermodynamics. Like these days we think of thermodynamics in terms of entropy and the
*  arrow of time and stuff like that, depending on where you come from. But it was really all about
*  building steam engines back in the day, right? Yes, people wanted to be able to pump water out
*  of mines in England. And I'm told, and I've often related this, but maybe I don't know if you have
*  better insight on the history, that Saji Carnot of Carnot Cycle fame, one of the pioneers of
*  thermodynamics, was mostly motivated by his annoyance that British steam engines were so
*  much better than French steam engines. Have you ever heard that story? I can imagine that.
*  I hadn't heard that, but I can well imagine that. Yeah, there's a certain nationalistic pride there.
*  And so he invented, you know, so in a very French fashion, he said, well, let's invent the perfect
*  steam engine and then realize that there would always be some dissipation, a lot of the second
*  law. So good. This is a perfect excuse for me to talk about thermodynamics because, you know, it's
*  crucially important part of science. In some ways, I think that thermodynamics is kind of
*  not given as much respect among professional physicists as it should. I mean, we all love the
*  laws of thermodynamics, but it's not quite considered a sub-discipline. You have particle
*  physicists, you have astrophysicists. There's not a lot of people, at least in the modern US
*  culture that I'm aware of, who are specializing in thermodynamics or statistical mechanics.
*  I discussed that a little bit in the book. Quantum thermodynamics has its root in the 1930s,
*  right after people discovered quantum theory. But it's really blossomed over the past 10 or so years.
*  And a lot of the energy has come from Europe, Canada, Singapore, outside the US. So the US has
*  taken a while to catch on, which is ironic because there was also another spurt of growth in quantum
*  thermodynamics during the 1980s. And a lot of those developments did happen in the US. But over the
*  past 10 years or so, there was a lot less quantum thermodynamics activity in the US.
*  And so that's one reason why I considered going elsewhere for grad school, actually.
*  Europe has this millennia old wonderful tradition of philosophy, and the US has had, let's say,
*  at least a stereotype of efficiency and productivity and applications.
*  So the nature of the quantum science done in the US has been very different.
*  And quantum thermodynamics has historically, especially at the beginning of its lifetime,
*  been more abstract, mathematical, and philosophically oriented.
*  Recently, there have been lots of connections with experiments. Some of my colleagues and I
*  have worked to build bridges with the fields that are more developed in the US, atomic, molecular,
*  nanoparticle physics, condensed matter, high energy physics. And so now there's a lot more
*  interest and enthusiasm in the US for quantum thermodynamics and in association with that
*  thermodynamics, I think. Well, it's ironic, right? Because like we just said, thermodynamics itself,
*  minus the quantum part, classical thermodynamics from the 19th century, was the paradigmatic
*  applied science. It really was almost engineering. I mean, can you tell us a little bit about
*  your favorite stories of the origins of thermodynamics? I mean, how do you think
*  about those developments over the 1800s, which were so influential?
*  Some people, as we discussed, were interested in seeing how efficiently engines could power
*  factories and pump water out of mines. But these practical questions can't really be considered
*  without the fundamental questions that go along with them. For instance, steam is a relatively
*  large classical system. You can think about it without having to think about individual particles.
*  But some thermodynamicists thought it worthwhile to think about individual particles. Atomism had
*  basically been established by them. But still, some people didn't like atomism. And some
*  thermodynamicists said, we can describe a lot of the changes coming about in the world during the
*  industrial revolution by just referring to large scale properties of systems like energy, temperature,
*  and pressure. We have no need to talk about these invisible particles that you can't even see. It's
*  unscientific for you to talk about them. And they sort of beat down on the thermodynamicists who
*  wanted to think about individual particles. But fortunately, those other thermodynamicists
*  did have their say and they ended up being right that there are individual particles out there.
*  And so we can even start to see the legacy of quantum theory in thermodynamics.
*  That's true. Yeah, I mean, there definitely was the hints of atoms and particles that eventually
*  grew into quantum theory. I'm wondering if maybe, you know, we've all heard entropy increases over
*  time, right? The second law of thermodynamics. In a closed system, entropy goes up. But that was not
*  the formulation that people had back in Saji Carnot's time, right? He didn't know about the
*  word entropy. He thought about cycles and engines. And so can you sort of put us back in that mindset
*  of someone who thinks about thermodynamics in terms of cycles and engines? What was his,
*  what was Carnot's big thing that he explained to us? There are multiple ways of casting the second
*  law of thermodynamics. One is very closely related to Carnot's engine, which, as you mentioned, is an
*  idealized engine. It is some working medium that has contact with two different large bodies. We
*  call them heat baths. You can also call them environments, but I like the term heat baths
*  because it makes me think of it was soap and loofahs and so on. So this engine can contact
*  two different heat baths. One is hot, one is cold. And the engine undergoes a cycle, a sequence
*  of four different steps. And at the end of the cycle, the engine ends up in the same configuration,
*  the same state as it was at the beginning. That's why we call it a cycle because the four steps
*  close, the engine ends up back where it was. However, the rest of the world has changed.
*  During the cycle, the engine performs work, useful energy on some other system. So it might be
*  lifting a weight or turning a paddle wheel. And meanwhile, energy has flowed between those two
*  heat baths. So the hot bath gets a little bit cooler and the cold bath gets a little bit warmer.
*  So Carnot calculated the best possible efficiency achievable by this engine or by any engine that's
*  in contact with just two different heat baths of two different temperatures. And so this optimal
*  efficiency is called the Carnot bound or the Carnot limit. When we develop new engine cycles,
*  including in quantum thermodynamics, to evaluate how well the engine performs,
*  we'll often calculate the efficiency and then double check that if our engine satisfies Carnot's
*  assumptions, it does not break the Carnot bound. Because if our engine broke the Carnot bound,
*  then we would basically be breaking the second law of thermodynamics and certainly we would have done
*  something wrong. Or really, really important. But actually, let me guess with very, very low
*  probability. Very low probability. I know that we have our priors on these things. But this is
*  peeking ahead a little bit. But of course, we're going to eventually talk about the quantum side
*  of things. Are the results of Carnot, etc. still just as true in the quantum realm? Or once we
*  have quantum mechanics, can we do better than a Carnot efficiency? One might hope that quantum
*  theory could get around Carnot's bound. To my knowledge, Carnot's bound has not been broken.
*  And there are many papers with many proposals of engines, classical and quantum. And in these
*  papers, the authors check, yes, my engine obeys the Carnot bound. However, there are ways to bend
*  around the Carnot bound, that you could propose an engine that's on the face of it might seem like
*  it should obey the Carnot bound, but actually accesses some slightly different resource, like
*  a certain quantum resource that really doesn't enable the engine to fall under the assumptions
*  of Carnot. And so you can, quote unquote, break the Carnot bound, but actually you're just bending
*  around it and achieving a better efficiency. All right. And so back to the classical world of the
*  1800s, you've used words or there are words hanging around in the background here that would
*  be worth digging into, namely work and heat. And these were tough for me when I was first learning
*  thermodynamics. I mean, clearly we need to understand these two different forms of energy.
*  And I think that no one really does, or at least that I think that people disagree about what the
*  best way of thinking about these things are. Do you have a favorite way of saying what is work and
*  what is heat? Work and heat are the two types of energy that are being transferred between two
*  different bodies. And my way of thinking about them is partially a little loose and intuitive,
*  and then partially we can put some math to say the work that is performed and then say everything
*  that's not covered by this math is heat. So work is energy that's being transferred that's directly
*  useful. You can directly harness it to push a rock up a hill or charge a battery. It's in a sense
*  coordinated, whereas heat is random energy. So it's uncoordinated. It's not doing something useful,
*  although if you wanted to, then you could use it in conjunction with an engine to do something
*  more useful to turn it into work. So heat is more the random jiggling around of molecules.
*  I think heat makes sense, and it even makes sense how you can turn heat into useful work.
*  I mean, that's literally what happens in an internal combustion engine, right? You fire some
*  piston, the gas ignites and it heats up and pushes the piston. But the work part is always a bit more
*  subtle, especially because like you just did, it's so often defined in terms of usefulness.
*  And that seems a bit anthropocentric, right? It seems like human beings and their ideas about
*  what is and is not useful are somehow creeping into this fundamental law of physics. But there's also
*  the related concepts of coordination, which you just used, or organized forms of energy. Are there
*  rigorous ways of defining the sense in which certain kinds of energy or certain energetic
*  substances can be organized and therefore we call that energy work rather than heat?
*  We have in thermodynamics a set of thermodynamic variables,
*  entropy, temperature, pressure, volume, particle number, chemical potential. Some of these numbers
*  are extensive, they grow with the size of the system. For instance, you can have some
*  material that takes up some volume. If you take another copy of the material and put it right next
*  to the first copy, then you've doubled the volume. So those are the extensive parameters. Then there
*  are intensive parameters that don't scale in that way. For instance, if you have one material at some
*  temperature and you take another copy of the material and put it right next to the first copy,
*  everything is still at the same temperature. And we do work when we change those extensive parameters
*  for example, we could have gas in a box, our favorite example of steam, that gas could be
*  capped with a piston and if the gas is expanding against the piston so that it's increasing its
*  volume, then the gas is performing work against the piston, even if it's not particularly useful
*  for me personally to have the piston move upward. Got it. Okay, good. Yeah, so it's not really as
*  anthropocentric as it sounds. But is this question of a clear bright line between heat and work,
*  is that like a useful thing to think about? Do the experts really truly understand it once and for
*  all or is it kind of the thing where researchers are still quibbling over it? Researchers definitely
*  quibble over it in quantum thermodynamics. I've heard less quibbling in classical thermodynamics,
*  but there could be quibbling that I haven't heard. Good. I think that gives us the background we need.
*  And then there are, of course, I want to get on the table the idea of refrigerators, which are somehow
*  related to engines but backwards. Yes, exactly. We could take for instance Carnot's cycle that
*  moves heat from the hot bath to the cold bath and run it backward, we would be moving heat from the
*  cold bath to the hot bath to cool down the cold baths to keep last night's leftovers cool.
*  But this requires some energy is the point. It's never going to happen spontaneously. That would
*  violate the second law. Exactly. If we want to run Carnot's engine backward, then instead of getting
*  work out of the cycle, we have to put work into the cycle. Yeah, okay, good. And so the next big
*  thing, so that's how people were thinking like in the first half of the 18th, 1800s, 19th century.
*  The second half, like you already alluded to, atoms came on the scene, right, in statistical mechanics.
*  And so I think probably people are somewhat familiar with that basic idea that really
*  gases are made of atoms, etc. But what really is interesting, like the profound philosophical move,
*  is that probability and information come into the game. Can you say a little bit about that sort of
*  conceptual shift in how thermodynamics was thought about? Thermodynamics originally was thought about
*  in terms of these large scale quantities that characterize systems, the ones that I just mentioned,
*  external and internal, excuse me, extensive and intensive parameters, like energy, temperature and
*  pressure. But you can go farther and think about some material of many, many particles as, excuse me,
*  some large material as consisting of many, many particles. And in principle, if you know
*  some of the theories that we have a good handle on in physics today, including quantum mechanics,
*  then you could imagine describing all of those particles individually and all of those particles
*  interactions. And in principle, if you had enough computing power, which you won't, then you could
*  recover the large scale behaviors of as large a material as you please. But again, this is very
*  difficult to do. And also, it's not necessarily the most useful. Because when we study thermodynamic
*  behaviors of materials, we don't really care about what all the little particles are doing.
*  We're more interested in what the material is doing, which is well described in many cases by
*  average behaviors of the particles. And we can also not knowing exactly what the particles are
*  doing, use probabilities to say, there is probably the particles are have some probability of being
*  here with these momenta, and some probability of being there with those momenta, we don't know,
*  but we don't actually need to know in order to recover the large scale behaviors of the material.
*  So did people in 1870 think in terms of information theory at all, they certainly began to think in
*  terms of probability people like Maxwell and Boltzmann, etc.
*  There are certainly the beginnings of information theory there, for instance, Boltzmann's H function
*  is a manifestation of entropy, which played a very important role in information theory
*  during the 20th century as invented by Claude Shannon. Claude Shannon explained the father
*  of information theory explained why he when he came up with some function in information theory
*  that quantified uncertainty about something, he wasn't sure what to call it. And he got advice
*  from his friend, John von Neumann, the great Hungarian American mathematical physicist,
*  and von Neumann said, this thing, you should call it entropy, because, well, it's already been
*  called that in statistical mechanics, it already exists. And also, no one really understands
*  entropy. So in a debate, you will always have the advantage.
*  I don't know how many debates Shannon got into, but I hope that he did get the advantage there.
*  But but I don't want to sort of gloss over the profoundness of this move, right? I mean,
*  we start by talking about steam engines. And everyone thinks that, you know, there's something
*  that's going to happen when you light the spark in your piston. But now you've reformulated
*  everything in terms of information, like, you know, entropy is a measure of how much we don't know
*  about the microscopic state of a system. How do you reconcile in your in your soul, you know, how
*  information helps pistons move? That's a good question. Suppose there are multiple ways to
*  approach this question. And one way is thermodynamics is an operational theory. It is
*  about agents who run engines who want to cool systems down or to charge batteries. So they
*  want to accomplish goals. And they can accomplish goals only insofar as they have the information
*  about where a gas is compressed, or. And so information and the gases entropy should come
*  into play when we think about thermodynamics. Okay, that makes sense. I mean, I guess
*  let's make it even more specific by talking about everyone's favorite thought experiment of Maxwell's
*  demon, right? Maxwell's demon in some sense is an operationalization of this relationship between
*  information and thermodynamics and entropy and so forth. So I will let you tell the audience what
*  Maxwell's demon experiment is actually supposed to tell us what it is and what it's supposed to
*  teach us. James Clark Maxwell, who was one of the founders of thermodynamics, came up with a thought
*  experiment in which he challenged the second law. He said, suppose that there is a gas in a box,
*  our favorite system in thermodynamics, there is a partition dividing the box in half, there's a
*  little door in the partition, and there's what he called a finite being. His colleagues named it a
*  demon, which is probably the term that Maxwell should have used because it caught on so well.
*  There's a demon who can open and close the door. This demon will see particles moving at a very
*  high speed coming, let's say from the right, and will let those through. And whenever the demon
*  sees slow moving particles coming from the left, the demon will let those through. But if there are
*  any fast particles coming through the left, the demon won't let those through the door, and so on.
*  Eventually, the demon will separate out the different particles in the gas so that the
*  quick molecules are on the left-hand side of the box and the slow molecules are on the right-hand
*  side of the box. The speed of a set of molecules in a gas is directly related to the energy of that
*  gas and its temperature. The demon will have separated what used to be just one uniform gas,
*  which is everywhere at just one temperature, into two different gases, one hot and one cold.
*  Then the demon could take Carnot's idea and run an engine cycle, and from this temperature
*  difference extracts useful work. So the two gases over the course of the engine cycle will come to
*  be at the same temperature or after many, many cycles. So the demon will have extracted work,
*  while returning the whole gas to the same uniform temperature. Then the demon could do this entire
*  thing again and again and again. So the demon could run a perpetual motion machine and extract
*  as much work as he wanted. That idea violates the second law of thermodynamics. So what is wrong with
*  Maxwell's Demon? Why can't this Maxwell Demon really operate as it operates?
*  Good. So what's the answer? It took us a while, right? It took us a little bit of,
*  yeah, it took us an embarrassingly long time to figure this out.
*  Yes. So the answer that is the most widely accepted was put forth by Charlie Bennett near the end of
*  the 20th century. He was building on the work of Szilard, another great Hungarian physicist,
*  and Rolf Landauer. There's a little bit of debate about whether this is really a satisfactory answer,
*  but I see quite a bit of agreements across the physics community, and I find the answer
*  satisfying personally. So Bennett proposed that the answer is this demon actually records information
*  about which particles are moving quickly and so where the hot gas ends up. And this part, the
*  demon at the end, suppose that the demon has separated the gas into two different gases
*  and has run the engine cycle so that there's just one uniform gas again. We would like to say,
*  according to our story, that everything is reset so that it's exactly as it used to be at the
*  beginning. However, the demon actually has this memory which is now full of information.
*  And in order to actually reset the full system, the demon needs to erase this information from
*  his memory. And Landauer showed that erasing information costs thermodynamic work. So the
*  work that the demon gains by running the engine cycle is lost in the erasure. And so the demon
*  nets zero work after all and can't run a perpetual motion machine.
*  It's interesting. So you say that erasure costs thermodynamic work. The usual way that I think
*  about it is erasure increases entropy. These are probably the same things said in different words.
*  Yeah, basically.
*  Okay. And that's roughly because irreversible things increase entropy, right? Like if you had
*  just the underlying Newtonian or even Schrodinger equation laws of physics, everything is reversible,
*  but something like erasing is irreversible. So that's where entropy has to come in.
*  Exactly.
*  Good. And I guess I see why that explanation from Bennett is pretty darn good, but not entirely
*  satisfying. I mean, you know, what if the demon had a really big ledger and didn't need to
*  erase it for a long time? I'm not completely convinced about this yet. I mean, I'm not saying
*  that you should have a better way of doing it. I think it's interesting that over a hundred years
*  later, we still haven't completely nailed this thing down.
*  Right. And interestingly, it was just recently that experimentalists got to the point of
*  checking the quantitative prediction by Landauer that indicated that the demon really
*  would need to expend basically all the work that he got from the engine cycle in order to erase
*  his memory. So yes, there's still a lot of discussion happening.
*  And this is at least tangentially related to the fact that our laptops heat up, right? I mean,
*  doing computations, we can do irreversible computations, but typically we don't. Typically,
*  we do irreversible computations. And according to Landauer, that's going to heat things up.
*  It's a fascinating connection between information and heat.
*  Yes. So we could do our computation reversibly, but we'd need an idealized system.
*  It would run very, very, very, very, very slowly. So it wouldn't be very practical for doing your
*  taxes, say. And I presume that, I mean, Landauer has a bound, like for every bit that you erase,
*  a certain entropy is created. Are real world computers, like are the laptop that I'm using
*  right now close to that bound? Or are they more wasteful than that?
*  Our computers are way more wasteful than that. Again, it was just not too long ago that during
*  the 21st century that experimentalists started showing that they could approach the idealization
*  that is Landauer's limit. Is this a future frontier for computer builders to try to be more and more
*  efficient and come closer to Landauer's bound? It is definitely a topic that is an inspiration
*  for, say, my community and a topic also of practical interest to, say, a colleague I have
*  who used to be at IBM. So it's being thought about. We are a little ways away, but energy usage is
*  an important topic, and there are a lot of people thinking about it.
*  I mean, it's certainly true that the computers around the world use a lot of electricity,
*  right? Generate a lot of heat. I mean, it's something we might imagine we would like to do
*  better at. Yes, definitely.
*  And then, okay, good. Now I think we're ready to move on to the quantum side of things. So
*  since you've explained thermodynamics to us, please explain quantum mechanics to us now.
*  Of course. Very loosely speaking, quantum physics is the physics of the very small,
*  electrons, atoms, photons, or particles of light and so on. And these systems can act in ways that
*  are very different from the ways in which the large systems from our everyday lives can act.
*  Probably most people who have listened to this podcast have heard about superpositions and
*  entanglement, very strong correlations that quantum particles can share. We can use these
*  different behaviors of quantum systems to perform certain tasks, such as solving certain
*  computational problems much more efficiently than we can perform those tasks with classical systems.
*  Similarly, there are thermodynamic tasks like the extraction of work and the charging of batteries
*  that are similar in spirit to information processing tasks. So we can ask, can we also
*  use quantum resources to perform those tasks better than with classical resources?
*  And I guess if someone asks me what is the most important difference between quantum mechanics
*  and classical mechanics, my answer would have something to do with entanglement. Do you think
*  of it that way also? Or do you think of something else as the most important thing? And what is it?
*  I certainly think of entanglement as very closely bound up in the difference between
*  classical and quantum. There are many different things that we can think of as quantum. Some of
*  those characteristics can be mimicked by classical systems. For instance, an atom can have only
*  certain amounts of energy, so its energy consists of just discrete numbers. However, there are
*  classical systems that have at least approximately discretized energies. Entanglement is something
*  that we don't attribute to classical systems, but there's a very high bar that you can set such
*  that if you prove that some phenomenon meets this bar, then it is truly non-classical.
*  That bar is contextuality. It's pretty difficult to prove that something is contextual, but it can
*  be done. But I think of contextuality as, at least in some cases, the thing behind entanglement
*  that makes entanglement really non-classical and in some cases provides speed up to quantum
*  computers. But I think it's much easier in many cases to talk about entanglement, and it's also
*  very useful. Entanglement is a strong relationship that quantum particles can have. I think of
*  quantum particles that are entangled as a whole greater than the sum of their parts.
*  There is some information in a system of entangled particles that is not in just one particle,
*  it's not in just the other particle, and it's not in the sum of the two particles separately,
*  but it's sort of between the particles, it's in the whole. And what is contextuality? Can you explain
*  that? We would think that if we were performing a measurement in a lab, a measurement of atoms or
*  DNA or anything else, then the color of my shirt doesn't matter. It's not going to affect the
*  statistics of the outcomes of that experiment if we have run many trials. So there are properties
*  that seem like they really should be irrelevant to the statistics of the outcomes of the measurements
*  in an experiment. And in the classical world, indeed, the color of my shirt does not matter.
*  But in the quantum realm, there are some properties that we would expect to be totally irrelevant
*  to the statistics of measurement outcomes that in fact aren't irrelevant. And so we could say
*  that in quantum theory, context matters for measurement statistics. So we say that quantum
*  theory is contextual and classical physics is non-contextual. Can you give us a concrete example
*  of a property that we would expect not to matter classically, but would matter quantum mechanically
*  to measurement outcomes? We can imagine having a system with three observables, three properties
*  that we might want to measure. And properties A and B could commute with each other. So we can
*  measure the two of them simultaneously without any problems. And similarly,
*  observables A and C could commute with each other. So we can measure them both simultaneously
*  without problems. So we could think if we measure one of these observables, then say if we measure
*  A, then whether we measure one of these other observables really doesn't matter and shouldn't
*  affect the outcome statistics. However, observables B and C might disagree with each other. We would
*  say that they don't commute with each other. So they can't be measured simultaneously. But you
*  would think that that wouldn't really affect A, but it can affect the statistics of measurement
*  outcomes. So I mean, this is great and fascinating, but it's also a little bit abstract. I mean,
*  so where do we run into this kind of thing measuring spins or qubits or in a quantum computer,
*  like, or, or like, some kind of experiment in your lab? I don't know. There are,
*  people have shown that contextuality is behind at least some of the speed ups that are achievable by
*  quantum computers with qubits. So that is one example where contextuality is important.
*  Okay, cool. And is it, I mean, is it sort of a version of entanglement? Or is it more like
*  the uncertainty principle? I mean, certainly the fact that different observables can't be measured
*  at the same time sounds uncertainty principle-esque. Definitely. And they're all related. They're all
*  parts of quantum physics. They, I think of contextuality again, as each of them is a very
*  specific property, and they're all closely related. One can involve the other. Contextuality is just a
*  particularly high bar for something, for some property or for some experiment to have such that
*  we can really say is really, really nonclassical. Yeah, it's really quantum. Okay, good. And the
*  other interesting thing to get on the table before diving into applications is that quantum
*  mechanics introduces a whole new kind of entropy, or at least a new way to have entropy. Entanglement
*  brings along with it a kind of entropy that we hadn't anticipated before. Indeed. So if we have
*  two particles that share entanglements, then automatically each one of those particles has
*  entropy just because of that entanglement. And can we read- This entanglement is, the most popular
*  way of measuring that entanglement is with the von Neumann entropy named after the job of the
*  von Neumann who advised Claude Shannon about naming his entropy. Yeah, did that come second? I mean,
*  did von Neumann invent his entropy after Shannon? I don't know who came first. I'm not sure of the
*  years, but at least the two of them, according to this quote from Claude Shannon, she seemed not to
*  be competitors, but more helping each other out. And can I think of it? I've always, I've gone back
*  and forth myself about this. So I have two entangled systems. And like you just said,
*  that means that either one of them, considered by itself, has some entropy. Can I relate that back
*  to information? Can I say that because it's entangled, there's some information I don't
*  have about that subsystem unless I include the whole shebang? Exactly. There's some information
*  that's not in just one of the particles or the other or in the sum of the two measured separately,
*  but that information is spread out across the whole. Good. Okay, good. So we have entanglement.
*  It leads to this new kind of entropy. Let's put it to work. Let's do some quantum thermodynamics.
*  So I have a list of different ways in which this becomes really interesting. I don't know,
*  do you want to pick one first? What is the sales pitch if you're going to say like, not just
*  thermodynamics, but quantum thermodynamics is something that's really interesting to think
*  about for this reason? I think of quantum thermodynamics as encompassing another a number
*  of goals and ways of viewing it. One is that we know that quantum physics can assist with
*  information processing tasks like communicating information and solving certain computational
*  problems. There are thermodynamic tasks, analogous to information processing tasks,
*  like refrigerating and charging batteries. So we can imagine that quantum phenomena might help
*  with some of those thermodynamic tasks. One example that I discuss in my book is an information based
*  engine. This is C. Lard's engine. We mentioned C. Lard just now when we were discussing the
*  resolution of Maxwell's demon paradox. C. Lard showed that if you have information and you also
*  have a source of heat, then you can use that information to turn this random uncoordinated
*  heat into useful work. His initial thought experiment was about classical gas, but you
*  can imagine that this gas might be classical or quantum. And as we know, there are different types
*  of quantum particles. Some are fermions like electrons and the other atoms are the other
*  types of matter around us. Instead, you can imagine that the particles are bosons,
*  so carriers of force or compositions of multiple fermions. And the fermions and the bosons
*  have different tendencies, different properties. So you could imagine running your engine through
*  many, many cycles with each of these types of particles. And you would get out some amount of
*  work on average from each type. The amount of work that you could get from fermions would be
*  in a certain setup would be zero. The amount you could get with classical particles would be
*  greater and the amount that you could get with bosons would be even greater. So what we think
*  of as spin statistics of bosons can help you on average in extracting work from an information
*  engine. Okay, that's very interesting. So fermions are the particles that can't
*  pile on top of each other. Bosons are the ones that like to pile on top of each other. And you're
*  saying that we can use that fact to beat the best possible classical result. In principle. And the
*  basic idea comes exactly from what you said. Since the bosons will have a tendency to pile on top of
*  each other, they'll exert a greater pressure. And that's why they'll do more work. And is this
*  something that is only going to be useful at the microscopic scale? Or can we imagine,
*  you know, using this kind of technology and building it up to greater and greater sizes,
*  so that it's really like an engine that could push a car around?
*  We could imagine I don't expect to have quantum engines in our garages anytime soon.
*  But quantum control is developing. And I, as a theorist, long thought that basically anything
*  I proposed would be laughed at, because theorists have a reputation for proposing
*  experiments that are impossible for the next 20 years. But I've been very pleasantly surprised at
*  how often experimentalists have said, huh, no, yeah, I could do that. Sure, let's collaborate.
*  So I have, again, people have recently performed the land our erasure experiment that we just
*  discussed earlier with a number of different platforms. C Lard's engine is very closely
*  related. To my knowledge, no one has realized C Lard's engine with these different types of
*  particles. Okay, but I have discussed it with some experimentalists. And they say there's at
*  least one lab out there that can probably do the experiment now with some significant thought. So
*  in a few years and 10 years, experimentalists will have much better control. So I don't want
*  to say that they can't do something. And I want to help our audience really just visualize what's
*  going on here. So a real engine like in a car in a gas car, I put gas into it, there's some fuel,
*  and it burns the fuel increases entropy makes the car go. What are the analogous things for your
*  little tiny quantum engine? What is the fuel that makes it go? The fuel you could think of as
*  consisting of two parts. One is the heat from the environment from the heat bath. But again, this is
*  random uncoordinated energy. So we have to have, so to speak, some way of directing it. And we use
*  our information in order to direct it. For instance, in the case of C Lard's engine, we have,
*  once again, a gas in a box. And we could imagine the box being partitioned in half. And suppose
*  that we have one bit of information about the gas suppose in a really simple example, the gas is
*  just one particle. And we know that the particle is on the right hand side of the box rather than
*  the left hand side of the box, then we can hook up a weight. So that if we allow the partition to move
*  so that the particle knocks against the partition and is able to move it farther and farther and
*  farther across the box, as the partition moves, it drags a weight upward. Okay, then we're using
*  our information about which side the particle is on in order to decide where to put the weight.
*  And that's what enables the particle to get heat energy from the bath and lift the weight.
*  By the end of this process, the particle has pushed the partition all the way to the edge
*  of the box so that again, the particle is free to be absolutely anywhere in the box. We have no
*  idea where the particle is. So we have lost our information about the particle. But in exchange,
*  we've lifted the weight. And I don't know that I could necessarily describe very well exactly how
*  you would hook up the weight so that is being dragged. But there's a beautiful illustration
*  in my book that I did not draw a wonderful steampunk artist drew it. So there's more
*  information there. And so in some sense, you're being Maxwell's demon, you're using information
*  to sort of figure out how to extract some work from these randomly fluctuating particles.
*  You could think about it that way. And you could see that as a reason why one could say that
*  Charlie Bennett was building on the work of Cillard and Landar in proposing a resolution
*  to Maxwell's demon paradox. And is this kind of thing? I guess the question for me to get to is,
*  separate out the quantumness of it from the tininess of it. Like part of this whole discussion is
*  just that we're at the level of particles and atoms and things like that. And even if the world
*  had been classical, you would have to think a little bit differently than we do in the big
*  macroscopic world. Another part of it is that the world is not classical, it's really quantum
*  mechanical, and there's contextuality and entanglement. So is this engine you're describing
*  really using quantumness? Or is it just relying on the fact that things are tiny?
*  The engine that I just described in detail can work even if the particle is classical. That's why
*  a group of people who came around much later than Cillard built on his idea by saying,
*  suppose that we move from thinking about one particle to thinking about multiple particles,
*  and suppose that these multiple particles were classical or fermions or bosons. So their results
*  relied on the statistics of fermions and bosons. Fermions obey Pauli's exclusion principle. So as
*  you said, two fermions can't pile on top of each other, they can't be in exactly the same quantum
*  state, whereas bosons do like to be in the same quantum state and so can be bunched up together.
*  Okay, very good. And since this is happening down the level of particles and molecules, etc.,
*  I can imagine this is going to be useful for nanotechnology. Is it useful in biology? Do cells
*  use these kinds of structures to move themselves around? Or is it something only human beings have
*  caught on to? That's something we'd like to know more about. There is a subfield of quantum biology
*  that is enjoying extra energy nowadays. And people approach quantum biology from many different
*  angles. In general, it is very hard to find a way in which non-classical phenomena could be
*  having a large scale effect in biology because biological systems are warm, they're watery,
*  they tend to be large. And in these conditions, quantum phenomena such as entanglement tend to die
*  off quickly. However, quantum theory is certainly relevant to molecules. Molecules are certainly
*  relevant to biology. For instance, I had the privilege and pleasure of working with a chemist,
*  David Limmer at UC Berkeley. He studies photo isomers, which are molecular switches.
*  They're molecules that have one conformation most of the time, but when light hits them
*  and they absorb a photon, they have the opportunity to switch configuration.
*  So he was studying these molecules in many different ways. People have plenty of models for them,
*  but these molecules are small, they are quantum, they're far from equilibrium. And so models tend
*  to be either very, very detailed or to involve some assumptions that we might prefer not to make.
*  So we worked together to try to make a more general model using quantum information theory
*  and derive a general bound on the probability that one of these molecules would switch.
*  You could probably recast that in terms of quantum. Well, this is a quantum thermal machine,
*  a molecular switch. We weren't thinking of it as an engine, but one can often think about such
*  things as quantum thermal machines. So they are out there, although they're large scale effects on
*  biology, as far as we know, are probably fairly small, but there could be plenty to be discovered.
*  I think maybe it's worth helping the audience gain this intuition that physicists have that warm,
*  wet things won't be quantum mechanical, right? Quantum mechanics is the world, the whole world
*  is quantum mechanical, but classical physics is a pretty good approximation to it. And it becomes
*  a very good approximation when things are big and radiating and so forth. I mean, can you explain
*  more about why that is and therefore why intrinsically quantum phenomena are not even
*  more ubiquitous in biology? Sure. And we can use entanglement as a touchstone. You mentioned that
*  you think of entanglement as particularly quantum. I really do too. I think it's a useful way to think
*  in many cases. So suppose that we were interested in finding some biological system that makes use
*  of entanglement. So this entanglement would need to be concentrated and controlled between
*  the few systems that would need to be entangled. It's all too easy to entangle particles. Particles
*  very easily entangle with stray particles, the things that bump into them, especially if the
*  temperature is high, things are jiggling around a whole lot. So even if you start, even if two
*  molecules or one molecule started out with some controlled entanglement that it might be able to
*  take advantage of to perform some tasks, thermodynamic or information processing,
*  probably that entanglement would dissipate very quickly to the environment because of the principle
*  of monogamy of entanglement. One particle can share only so much entanglement with any other
*  particles. So this particle could share a whole bunch of concentrated entanglement with one other
*  particle to do something useful. Or if this particle gets distracted by other particles,
*  then it would end up sharing only a little bit of entanglement with each of the other particles.
*  And there would be no particle with which it shares enough entanglement that it could accomplish
*  something useful from that entanglement. Okay, let me try to rephrase that because this is an
*  interesting perspective that I haven't heard before. Given that there's sort of an upper
*  amount of entanglement you can have when particles are, when there's lots of particles and they keep
*  bumping into each other, they will be entangled. But the amount of entanglement between one particle
*  and any other one particle will be really, really tiny because the entanglement gets spread all
*  over the place. And therefore, this seems like a bit of a leap, but therefore, it's almost as if
*  there's no entanglement at all. Right. Suppose that we as humans, to make this relatively clean
*  example, as humans who have conscious agency wanted to send some quantum message from me to you.
*  So we could do that in two ways. We could have a quantum channel analogous to the internet that
*  we're using now, but quantum. And I could send my message over that quantum channel. Instead,
*  we could just share two particles, two qubits, two basic units of quantum information that are
*  maximally entangled. And we could perform a certain protocol on them such that it would be as though
*  we had one use of a quantum channel. So to do something useful to send a unit amount of
*  quantum information, we would need, or to send some amount of quantum information, we would need
*  at least this one maximally entangled pair. And if we had just a pair of particles that were
*  very distracted by the other particles around them, so that your particle and my particle
*  didn't share enough entanglements, then we couldn't accomplish a useful quantum task.
*  It's interesting because it's always a bit of a surprise to me, even though I know it's true,
*  that the world is so very, very quantum, and yet we have to work really hard to manifest
*  its quantumness in some places. Yeah, that's very interesting. And even before quantum mechanics
*  comes along, there's this, in statistical mechanics, there's this concept of fluctuations,
*  of if we don't know, if we don't have perfect knowledge, information of the state of all of
*  our particles, we can say what usually happens, but there will be fluctuations around that.
*  And maybe you can help us, because I don't think we've ever talked about it on the podcast,
*  but there's been a bit of a revolution in this non-equilibrium fluctuation kind of statistical
*  mechanics over the last 20 years, that is even in the classical world and now these days in the
*  quantum world. Yes. So there's this field of non-equilibrium thermodynamics or statistical
*  mechanics called fluctuation relations. Fluctuation relations can be thought of as
*  stronger versions of the second law. So the second law of thermodynamics is an inequality,
*  as you stated near the beginning of the podcast. According to the second law of thermodynamics,
*  the entropy of a closed isolated system increases or stays constant. So it obeys an inequality over
*  time. And the second law of thermodynamics, in most cases, doesn't tell us how much the entropy will
*  increase. It just says it'll increase by some amount or stay constant. So that's some information,
*  but it's not as much information as one can want. Fluctuation relations are equalities,
*  and they govern systems even very far from equilibrium, which is a state in which the
*  large-scale properties of a system, such as its temperature and pressure remain constant,
*  and there are no net flows of anything into or out of the system. So it's a very idealized state.
*  Most of the worlds that we care about is out of equilibrium. We are far from equilibrium.
*  So fluctuation relations are these stronger versions of the second law of thermodynamics,
*  but from them you can recover what's sometimes called the second law of thermodynamics. And they
*  give us also more information in that if you perform the same experiment many, many times,
*  then you might get different outcomes in different trials. For instance, suppose that you have a
*  strand of DNA. You can trap one end of it in an optical trap using a laser and use lasers to
*  stretch the strand of DNA. And stretching something requires the input of energy. It requires work.
*  So you could imagine starting the strand out in the same way in each of many, many experiments,
*  and in each experiment stretching the DNA strand through the same distance. In different trials,
*  you'll need different amounts of work because in one experiment, a water molecule kicks the
*  molecule over here in this direction. In another experiment or another trial, a water molecule
*  kicks the DNA over there in a different direction. So different trials require different amounts of
*  work. But you can perform many, many trials. And from the statistics predict that in the next trial,
*  you'll need this amount of work. And from some fluctuation relations, we can learn about the
*  probability of the next trial. We can do all sorts of things with fluctuation relations. Some are
*  practical, some are fundamental. I have to mention my institution. I feel sort of legally obliged.
*  The University of Maryland is home to my colleague and collaborator, Chris Jarzynski, who is
*  responsible for Jarzynski's equality, which is one of these fluctuation relations, one of the ones that
*  helps to kick off a lot of excitement about what they could say in detail about these far from
*  equilibrium systems that are usually extremely difficult to describe, except Chris is very humble.
*  So everyone else calls it Jarzynski's equality, and he calls it the non-equilibrium fluctuation
*  relation, which I think is much less convenient to say. But these equalities are remarkable,
*  not only for the reasons I've mentioned, but also because the world far from equilibrium tends to be
*  very difficult to describe. It's chaotic. It's wild. It's difficult to derive very general results
*  that describe lots of non-equilibrium situations, but these fluctuation relations do. And they're
*  not only theoretical constructs that have just been proved mathematically. They've been tested with
*  DNA, with RNA, with other small particles, and extensions of them have been explored with quantum
*  systems and more. But let me just, as an aside, it's interesting that you keep mentioning DNA,
*  because it took a while for this to get into my brain, but we think of DNA as what carries the
*  genetic information inside our cells. But to physicists, it's like a really good spring,
*  or something like that. It's a very flexible molecule for tiny, tiny applications, right?
*  Yes. To physicists, most things are springs. It has nothing to do with which base pairs are being
*  used in the DNA. You're not using that information. That's interesting and fun.
*  And yeah, so this, I mean, I don't know, I'm asking you open-ended questions here,
*  because I just want to hear your big picture views. But to me, these fluctuation theorems,
*  these non-equilibrium statements, these statements about how thermodynamics works
*  when things are not settled down and the gas is not all equilibrated inside the box,
*  we're beginning to get a handle on how that works. And this might have enormous implications for
*  how complex systems work in general. Like you already said, it's hard to get general rules
*  in these cases. And here's a general rule. So can we imagine bootstrapping our way up from those
*  rules that we have to a bigger picture understanding of things beyond individual particles,
*  things which have many moving parts, but aren't simple like a box of gas?
*  Certainly. And I think that the field of complexity science is very exciting. I know
*  you belong to the Santa Fe Institute. So you have a bunch of colleagues thinking about complexity
*  theory. I've even written a paper about complexity theory in that sense recently.
*  The regimes certainly share the importance of information. In some cases, they share the
*  importance of thermodynamics. In some ways, they are a little bit different. Fluctuation relations
*  are especially useful when the system is small. If a system is large, then it will very often
*  behave as it does on average. And fluctuations away from the average have less and less
*  significance. That's one reason why fluctuation relations have been tested with strands of
*  DNA and RNA. They're small enough that the fluctuations are very important.
*  But also, I agree, complex systems are far from equilibrium. So there can certainly be overlaps
*  in those toolkits. I guess the intuition I have, which is very vague and non-mathematical at this
*  point in my thought about it, but what makes complex systems interesting is there can be
*  feedback between different levels. Even though it's a big system, a simple system is the Earth
*  going around the Sun. Even though the Earth has made lots of particles, we can pretty accurately
*  predict where it is. But once systems become complex like a person or a volcano or a hurricane,
*  I can imagine tiny little fluctuations at the microscopic level impacting the macroscopic
*  behavior in interesting ways. That's why I'm wondering whether or not there are connections
*  to be drawn there. That's all I have to say. It's very vague, but I figured I'd put it on the table.
*  There's certainly a lot of work in that direction. I've also had the privilege and pleasure of
*  working with Jeremy England. He and his group have had extremely interesting ideas about
*  fluctuation relations, complex systems, far from equilibrium systems, self-organizing systems.
*  They do very rigorous, far from equilibrium statistical mechanics. In part, it was a pleasure
*  to work with him because he is such a rigorous thinker about statistical mechanics. But the group
*  is inspired by thoughts of life, which is certainly a complex system. There are connections
*  between the fluctuation relations that are particularly visible in small systems and larger
*  such as living systems. I will mention that Jeremy was a former Mindscape guest.
*  We have a good track record there. I know that was a long digression because I just love the
*  fluctuation theorems, but I do want to hit back on a couple more quantum points before we wrap up.
*  One of the fascinating crucial things about quantum mechanics is the role of measurements
*  and observations. We've talked about entanglement a little bit, which I think is pretty central.
*  As a many-worlds person, I think entanglement is the only thing you need to know and you can derive
*  everything else. But certainly, operationally in the real world, when you measure quantum systems,
*  their wave functions look like they collapse. How important is that kind of measurement uncertainty
*  in quantum mechanics, which wasn't there in classical mechanics, to the story of statistical
*  mechanics and thermodynamics at the microscale? How important is wave function collapse to quantum
*  steampunk thinking? It's very important. Measurement is how we learn about properties of a system.
*  We, as thermodynamicists, would like to know how much work we've performed on a system,
*  how much work we've extracted from it, how much heat it's exchanged with its surroundings.
*  But we find out those properties typically by measuring the system of interest. If we measure
*  the system of interest, then we disturb it and we can influence our understanding of how much energy
*  the system has absorbed or let go. There is a problem of defining quantum heat and quantum work
*  that has been the subject of a lot of debate. I have a folder that I keep. Every time I find
*  another paper that proposes another definition of quantum work and heat, I add it to the folder.
*  But I think that that's actually a reason for some of the richness of quantum thermodynamics.
*  I haven't proposed definitions of quantum work and heat myself, but partially because there are
*  definitions that I think are quite useful in different settings that I consider. And I think
*  that this richness can be interestingly contrasted with, say, particle physics.
*  Physicists are stereotyped as being obsessed with unification, trying to take a whole bunch
*  of different theories and show that they're really the same theory so that there's just one theory
*  to rule them all. But in quantum thermodynamics, there are these many different definitions of
*  work and heat that seem to be useful depending on how you poke the system, how you perform work
*  on the system, how you measure the system, whether you have any other systems around to help you in
*  particular ways. And so my opinion, which seems to be the opinion of at least some other quantum
*  thermodynamics, is there might not be any one definition of quantum work and heat to rule them
*  all, but maybe there doesn't need to be. Because there are diverse situations. We approach different
*  systems in different ways with different tools and different skills. And different definitions
*  and measurement strategies can be useful in different situations. So the measurement of,
*  or the disturbance of a quantum system by measurement is extremely fundamentally important
*  in quantum thermodynamics. Good. I figured it was. I wanted to check because sometimes you get
*  surprising answers in this field. That's why you need to do the research, I guess. Okay, so I mean,
*  at the end of the podcast, I always like to allow ourselves to be a little bit more speculative than
*  in the meat of the podcast. So imagine that rather than being a professor and podcaster,
*  I was a venture capitalist. And I wanted to know how to spend my money on quantum steampunk startup
*  companies. Like what are the things that we might hope, even if it's not in the very near term, but
*  what are the real world technological applications that we might want to get out of this? Is it mostly
*  mini engines or timekeeping? Or is there even biological medicinal kind of applications for
*  this kind of technology? I think that's a very important question. And one that I think that
*  the field could benefit from increasingly addressing. As I mentioned, the roots of
*  quantum thermodynamics, and a lot of the quantum thermodynamics that has been done, has been
*  relatively abstract, mathematical and theoretical. Quantum thermodynamics has made its way into
*  experiments, and the number of experiments and the complexity of the experiments has been growing,
*  and it will continue to grow. But thermodynamics did develop hand in hand with the industrial
*  revolution, which was extremely useful. That was useful. Yeah. Also, there are some downsides of it,
*  but mostly, yeah, it was useful. There were certainly downsides. Hopefully, we can learn
*  from past mistakes. So it would be lovely if also quantum thermodynamics could go
*  hand in hand with some quantum thermodynamic technologies. Those on a practical level don't
*  yet exist. The engines that we propose have been really useful for seeing the difference between
*  quantum physics and classical physics through the lens of thermodynamic tasks, similarly to how we
*  understand nowadays how quantum differs from classical by understanding what problems a
*  quantum computer can solve much more efficiently than a classical computer. And the experiments
*  have been proof of principle experiments. But now that we have done some of this fundamental legwork,
*  and we have started on and done a number of proof of principle experiments, I think it
*  definitely would be useful to think about how quantum thermodynamics could help us technologically.
*  One challenge is that quantum systems are very difficult to control. So I like to look for
*  what I think of as quantum analogs of Southern California. Southern California is,
*  it just happens to be very well suited to solar panels. And so if you just stick some solar panels
*  in Southern California, then even though the solar panels need rather specific conditions to work
*  well, they will work well if they're put in the right place. So I think there might be some
*  some situations that might already exist in labs in which we could stick a quantum thermodynamic
*  device and get some benefits. I'm working with a lab on one possible way of realizing this idea.
*  They've asked me not to talk much about it. So I won't say much for now. And also, I'm not the only
*  person thinking in this direction. Alexia Fev in France has also recently written some papers about
*  the possibilities for quantum thermodynamic technologies. I think that all the ideas you
*  mentioned are interesting to think about. I don't know if we will succeed in making
*  many very useful quantum thermodynamic technologies. I hope so. We haven't thought
*  about it too much yet. So I think there's a lot of opportunity. Okay, but I'm not pretending to be a
*  venture capitalist now, not a podcaster. I want you to tell me what one technology I should spend
*  my money on. If you're a venture capitalist, I'm afraid I'm the wrong person to talk to.
*  But yeah, perhaps a quantum engine that is placed in the right way in a lab,
*  so that it can take advantage of the quantum control that's naturally there.
*  Well, not naturally, but already there. And use the resources around it. I should probably,
*  due to the request of my colleague, not say too much more. Okay, I know that is unfair.
*  Are there potential, I mean, just as a sort of guidance as to what to be aware of or looking
*  for as the future comes online. Solar power, is that something that will be maybe useful,
*  these quantum techniques? There are certainly people thinking about how we can
*  capture energy from light more efficiently. For instance, my chemist colleague at UC Berkeley,
*  and quantum physics, energy and information are all relevant. So the quantum thermodynamics
*  community has been historically a little separate from that. But I think it would be useful to
*  create some more of those bridges. Okay, and then on the totally opposite side,
*  not being a venture capitalist anymore. Final question. Are there connections and maybe the
*  answers? No. But are there connections here to other things going on in quantum information
*  theory, like quantum computing, or even applications of quantum information to black holes and emergent
*  spacetime? Absolutely. That's one of the frontiers that I think is most exciting for quantum
*  thermodynamics and quantum steampunk, which I think of as sort of taking that, not only building a
*  theory of quantum thermodynamics, marrying quantum physics with thermodynamics, sometimes with help
*  from information theory, but also taking the mathematical and conceptual tools of this quantum
*  theory of thermodynamics, taking it around to different disciplines, and saying, how can we help
*  with our toolkits? Or can we uncover any new questions? Or can we take inspiration from your
*  field to ask new questions about quantum thermodynamics? So I've had, again, the
*  privilege and pleasure of working with people in chemistry, condensed matter, atomic molecular and
*  optical physics, and high energy and black hole physics. It's definitely a lot of fun to cross
*  these borders and very productive. I often feel very ignorant because I'm usually talking with
*  people whose conversation I don't understand. But it's a wonderful feeling to be able to say,
*  you have this problem. And I think that this tool from quantum thermodynamics could help.
*  I gave the example earlier of the chemist David Limmer, who is thinking about a molecular switch.
*  We used a tool from quantum thermodynamics, a resource theory, a very simple mathematical model
*  for quantum thermodynamics, in order to derive the bound that he was looking for. So those connections
*  definitely exist. Some of my colleagues and I are working on building many more.
*  Yeah, I mean, it's a good situation to be in. It can be challenging, I think, like you allude to,
*  because you're always feeling like you're not the expert in the room. But you have your little area
*  of expertise that they don't have, and so you're useful to people. And that is an exciting position
*  for everyone to be in. Yes, definitely. And I'm looking forward to more. And good luck with the
*  book coming out. Nicole Younger-Helpern, thanks so much for being on the Mindscape podcast.
*  Thank you. It's been a pleasure.
