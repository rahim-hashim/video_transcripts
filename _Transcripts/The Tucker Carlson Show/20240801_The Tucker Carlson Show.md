---
Date Generated: January 05, 2025
Transcription Model: whisper medium 20231117
Length: 6804s
Video Keywords: ['tucker carlson', 'tucker carlson tonight', 'tucker carlson ai', 'fox news tucker carlson', 'tucker carlson joe rogan', 'tucker carlson monologue', 'tucker', 'elon musk tucker carlson interview', 'tucker carlson monologue tonight', 'tucker carlson evolution', 'david pakman tucker carlson', 'carlson', 'tucker carlson and joe rogan on ai', 'tucker tonight', 'tucker carlson x', 'tucker carlson twitter', 'tucker carlson ufo', 'tucker carlson rant', 'tucker carlson ufos', 'ai']
Video Views: 520496
Video Rating: None
Video Description: Tech CEO Amjad Masad on the cults of Silicon Valley. This is the deepest and most interesting explanation of AI you’ll ever see.

Watch more here: https://watchtcn.co/49CDF2t

Subscribe to the new Tucker Carlson Network channel for more exclusive content: https://www.youtube.com/@TCNetwork/featured?sub_confirmation=1

Follow Tucker on X: https://x.com/TuckerCarlson

Text “TUCKER” to 44055 for exclusive updates.

Paid partnerships:

Download the Hallow prayer app and get 3 months free at https://Hallow.com/Tucker

ExpressVPN: Get 3 months free at https://ExpressVPN.com/Tucker

#TuckerCarlson #AmjadMasad #DonaldTrump #tech #AI #SiliconValley #ElonMusk #AllInPodcast #JoeRogan #debate #transhumanism #Christianity #DEI #news #politics 

Chapters:
00:00 Artificial Intelligence
10:01 Bitcoin
22:30 The Extropians Cult
29:15 Transhumanism
34:04 Longtermism
40:24 Are Machines Capable of Thinking?
43:11 The Two Conflicting Theories of Physics
49:46 The Difference Between Mind and Computer
1:04:24 The Abuse of Technology
1:12:36 Virtual Companionship
1:15:12 Do Silicon Valley Elites Acknowledge the Existence of God?
1:26:37 Silicon Valley Turning to Donald Trump
1:40:20 Elon Musk and Free Speech
---

# Amjad Masad The Cults of Silicon Valley, Woke AI, and Tech Billionaires Turning to Trump
**The Tucker Carlson Show:** [August 01, 2024](https://www.youtube.com/watch?v=cNqTu_58sZ8)
*  It does sound like you're directly connected to AI development. [[00:00:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=0.0s)]
*  Yes. [[00:00:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3.68s)]
*  You're part of the ecosystem. [[00:00:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4.18s)]
*  Yes. [[00:00:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5.18s)]
*  And we benefited a lot from when it started happening. [[00:00:06](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6.0200000000000005s)]
*  Like it was almost a surprise to a lot of people, but we saw it coming. [[00:00:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=9.68s)]
*  You saw AI coming. [[00:00:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=14.200000000000001s)]
*  Saw it coming, yeah. [[00:00:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=15.38s)]
*  So, you know, this recent AI wave, you know, it surprised a lot of people. [[00:00:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=16.52s)]
*  When ChatGPT came out November 22, a lot of people just lost their mind. [[00:00:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=21.400000000000002s)]
*  Like suddenly a computer can talk to me. [[00:00:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=26.560000000000002s)]
*  And that was like the holy grail. [[00:00:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=28.88s)]
*  Yeah, I wasn't into it at all. [[00:00:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=29.88s)]
*  Really? [[00:00:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=31.439999999999998s)]
*  I'm sorry. [[00:00:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=31.939999999999998s)]
*  You know what? [[00:00:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=32.94s)]
*  It's terrifying. [[00:00:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=33.44s)]
*  Paul Graham, one of my closest friends and sort of allies and mentors. [[00:00:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=34.24s)]
*  He's a big Silicon Valley figure. [[00:00:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=59.120000000000005s)]
*  He's a writer, kind of like you. [[00:01:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=62.0s)]
*  You know, he writes a lot of essays and he hates it. [[00:01:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=63.6s)]
*  He thinks it's like a midwit, right? [[00:01:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=68.44s)]
*  And it's just like making people write worse, making people think worse. [[00:01:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=70.64s)]
*  Worse. [[00:01:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=73.96000000000001s)]
*  Or not think at all. [[00:01:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=74.96000000000001s)]
*  Right. [[00:01:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=75.96000000000001s)]
*  Not think. [[00:01:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=76.96000000000001s)]
*  As the iPhone has done, as Wikipedia and Google have done. [[00:01:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=77.96000000000001s)]
*  Yes. [[00:01:19](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=79.24000000000001s)]
*  We were just talking about that. [[00:01:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=80.24000000000001s)]
*  The, you know, the iPhones, iPads, whatever, they made it so that anyone can use a computer, [[00:01:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=81.24000000000001s)]
*  but they also made it so that no one has to learn to program. [[00:01:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=87.76s)]
*  The original vision of computing was that this is something that's going to give us [[00:01:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=91.8s)]
*  superpowers, right? [[00:01:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=97.92s)]
*  J.C. Licklider, the head of DARPA while the internet was developing, wrote this essay [[00:01:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=98.92s)]
*  called The Man-Machine Symbiosis. [[00:01:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=104.8s)]
*  And he talked about how computers can be an extension of ourselves, can help us grow, [[00:01:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=106.92s)]
*  we can become, you know, there's this marriage between the type of intellect that the computers [[00:01:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=114.32s)]
*  can do, which is high speed arithmetic, whatever, and the type of intellect that humans can [[00:02:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=121.36s)]
*  do is more intuition. [[00:02:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=125.96000000000001s)]
*  Yes. [[00:02:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=127.56s)]
*  But, you know, since then, I think the, the sort of consensus has sort of changed around [[00:02:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=128.56s)]
*  computing, which is, and I'm sure we'll get into that, which is why people are afraid [[00:02:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=134.88s)]
*  of AI is kind of replacing us. [[00:02:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=138.6s)]
*  This idea of like computers and computing are a threat because they're directly competitive [[00:02:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=140.68s)]
*  with humans, which is not really the belief I hold. [[00:02:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=147.92000000000002s)]
*  They're extensions of us. [[00:02:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=151.08s)]
*  And I think people learning to program, and this is really embedded at the heart of our [[00:02:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=152.28s)]
*  mission at Repled, is what gives you superpowers. [[00:02:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=156.16000000000003s)]
*  Whereas when you're just tapping, you're kind of a consumer. [[00:02:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=160.48000000000002s)]
*  You're not a producer of software. [[00:02:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=164.12s)]
*  And I want more people to be producers of software. [[00:02:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=166.12s)]
*  There's a book by Douglas Roshkov, it's called Program or Be Programmed. [[00:02:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=170.24s)]
*  And the idea is if you're not the one coding, someone is coding you. [[00:02:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=177.04s)]
*  Someone is programming you. [[00:03:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=180.96s)]
*  These algorithms on social media, they're programming us, right? [[00:03:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=182.6s)]
*  So too late for me to learn to code though. [[00:03:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=188.44s)]
*  I don't think so. [[00:03:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=190.6s)]
*  I don't think so. [[00:03:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=191.6s)]
*  I can't balance my checkbook, assuming there are still checkbooks. [[00:03:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=194.08s)]
*  I don't think there are. [[00:03:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=196.4s)]
*  But let me just go back to something you said a minute ago, that the idea was originally [[00:03:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=197.4s)]
*  as conceived by the DARPA guys who made this all possible, that machines would do the math, [[00:03:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=202.88s)]
*  humans would do the intuition. [[00:03:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=208.2s)]
*  I wonder as machines become more embedded in every moment of our lives, if intuition [[00:03:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=209.35999999999999s)]
*  isn't dying or people are less willing to trust theirs. [[00:03:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=216.95999999999998s)]
*  I've seen that a lot in the last few years where something very obvious will happen. [[00:03:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=220.39999999999998s)]
*  And people are like, well, I could sort of acknowledge and obey what my eyes tell me [[00:03:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=225.04s)]
*  and my instincts are screaming at me, but the data tell me something different. [[00:03:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=230.35999999999999s)]
*  I feel like my advantage is I'm very close to the animal kingdom and I just believe in [[00:03:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=235.51999999999998s)]
*  smell. [[00:04:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=241.04s)]
*  But I wonder if that's not a result of the advance of technology. [[00:04:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=242.04s)]
*  Well, I don't think it's inherent to the advance of technology. [[00:04:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=247.35999999999999s)]
*  I think it's a cultural thing, right? [[00:04:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=250.44s)]
*  It's how to, again, this vision of computing as a replacement for humans versus an extension [[00:04:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=253.08s)]
*  machine for humans. [[00:04:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=260.64s)]
*  And so you go back, Bertrand Russell wrote a book about history of philosophy and history [[00:04:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=262.8s)]
*  of mathematics and going back to the ancients and Pythagoras and all these things. [[00:04:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=269.2s)]
*  And you could tell in the writing, he was almost surprised by how much intuition played [[00:04:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=274.64s)]
*  into science and math and in the sort of ancient era of advancements and logic and philosophy [[00:04:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=280.16s)]
*  and all of that. [[00:04:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=288.72s)]
*  Whereas I think the culture today is like, well, you've got to check your intuition at [[00:04:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=290.2s)]
*  the door. [[00:04:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=295.84000000000003s)]
*  Yes. [[00:04:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=296.84000000000003s)]
*  Yeah. [[00:04:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=297.84000000000003s)]
*  You're biased. [[00:04:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=298.84000000000003s)]
*  Your intuition is racist or something. [[00:04:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=299.84000000000003s)]
*  And you have to, this is bad. [[00:05:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=300.84000000000003s)]
*  And you have to be this blank slate and you trust the data. [[00:05:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=303.56s)]
*  By the way, data is, you can make the data say a lot of different things. [[00:05:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=308.44s)]
*  I've noticed. [[00:05:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=311.91999999999996s)]
*  Can I just ask a totally off topic question that just occurred to me? [[00:05:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=312.91999999999996s)]
*  How are you this well-educated? [[00:05:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=315.79999999999995s)]
*  I mean, so you grew up in Jordan speaking Arabic in a displaced Palestinian family. [[00:05:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=317.28s)]
*  You didn't come to the US until pretty recently. [[00:05:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=322.64s)]
*  You're not a native English speaker. [[00:05:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=325.4s)]
*  How are you reading Bertrand Russell? [[00:05:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=326.96s)]
*  What was your education? [[00:05:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=330.59999999999997s)]
*  Is every Palestinian family in Jordan this well-educated? [[00:05:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=331.96s)]
*  Kind of like, yeah, Palestinian diaspora is like pretty well-educated. [[00:05:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=336.8s)]
*  And you're starting to see this generation, our generation of kind of who grew up are [[00:05:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=342.08000000000004s)]
*  starting to sort of become more prominent. [[00:05:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=346.36s)]
*  I mean, in Silicon Valley, a lot of C, Sweden, VP level executives, a lot of them are Palestinian [[00:05:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=349.88s)]
*  originally. [[00:05:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=356.40000000000003s)]
*  A lot of them wouldn't say so because there's still bias and discrimination and all of that. [[00:05:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=357.40000000000003s)]
*  But they wouldn't say they're a Palestinian. [[00:06:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=361.88s)]
*  They're called Adam and some of them, some of the Christian Palestinians especially kind [[00:06:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=363.71999999999997s)]
*  of blend in. [[00:06:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=367.4s)]
*  But there's a lot of them out there. [[00:06:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=368.4s)]
*  So how do you wind up reading, I assume you read Bertrand Russell in English. [[00:06:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=372.15999999999997s)]
*  How did you learn that? [[00:06:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=377.15999999999997s)]
*  You didn't grow up in an English speaking country. [[00:06:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=378.71999999999997s)]
*  Yeah, well, Jordan is kind of an English speaker. [[00:06:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=380.28s)]
*  Well, it kind of is. [[00:06:23](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=383.32s)]
*  That's true. [[00:06:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=384.32s)]
*  Right. [[00:06:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=385.32s)]
*  So it was a British colony. [[00:06:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=386.32s)]
*  I think one of the, you know, the independence day like happened in like 50s or something [[00:06:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=387.76s)]
*  like that, or maybe 60s. [[00:06:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=392.2s)]
*  So it was like pretty late in the, you know, British sort of empires history that Jordan [[00:06:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=394.32s)]
*  stopped being a colony. [[00:06:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=400.28s)]
*  So there was like a lot of British influence. [[00:06:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=401.64s)]
*  I went to, so my father, my father is a government engineer. [[00:06:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=403.92s)]
*  He didn't, he didn't have a lot of money. [[00:06:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=408.76s)]
*  So we lived a very modest life, kind of like middle, lower middle class. [[00:06:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=410.03999999999996s)]
*  But he really cared about education. [[00:06:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=415.58s)]
*  And he sent us to private schools. [[00:06:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=417.84s)]
*  And in those private schools, we, we learned kind of using British diploma, right? [[00:06:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=418.84s)]
*  So IGCSE, A-levels, you know, that's very familiar with. [[00:07:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=424.64s)]
*  Not at all. [[00:07:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=428.64s)]
*  Yeah. [[00:07:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=429.64s)]
*  So, you know, part of the sort of British, you know, colonialism or whatever is like, [[00:07:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=430.64s)]
*  you know, education system became international. [[00:07:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=435.56s)]
*  I think it's a good thing. [[00:07:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=437.56s)]
*  Yeah, there are British schools everywhere. [[00:07:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=440.15999999999997s)]
*  Yeah, yeah. [[00:07:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=441.56s)]
*  British schools everywhere. [[00:07:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=442.56s)]
*  And there's a good education system. [[00:07:23](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=443.56s)]
*  It gives students a good level of freedom and autonomy to kind of pick the kind of things [[00:07:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=445.04s)]
*  they're interested in. [[00:07:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=448.04s)]
*  So I went to a lot of math and physics, but also did like random things. [[00:07:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=449.04s)]
*  I did child development, which I still remember. [[00:07:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=453.72s)]
*  And now that I have kids, I actually use. [[00:07:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=456.48s)]
*  And in high school you do that? [[00:07:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=459.68s)]
*  In high school. [[00:07:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=460.84000000000003s)]
*  And I, I learned. [[00:07:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=462.68s)]
*  What does that have to do with the civil rights movement? [[00:07:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=463.68s)]
*  That's the only topic in American schools. [[00:07:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=468.04s)]
*  Really? [[00:07:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=470.32s)]
*  Yeah. [[00:07:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=471.32s)]
*  I've been learning for 16 years learning about the civil rights movement. [[00:07:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=473.08s)]
*  So everyone can identify the Edmund Pettus Bridge, but no one knows anything else. [[00:07:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=474.08s)]
*  Oh, God. [[00:07:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=476.71999999999997s)]
*  I'm so nervous about that with my kids. [[00:07:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=477.71999999999997s)]
*  No, opt out. [[00:07:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=479.71999999999997s)]
*  Trust me. [[00:08:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=481.71999999999997s)]
*  That's so interesting. [[00:08:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=482.71999999999997s)]
*  So when you, when did you come to the U.S.? [[00:08:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=484.2s)]
*  2012. [[00:08:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=487.12s)]
*  Damn. [[00:08:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=488.12s)]
*  And now you've got a billion dollar company. [[00:08:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=489.12s)]
*  That's pretty good. [[00:08:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=490.71999999999997s)]
*  Yeah. [[00:08:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=491.71999999999997s)]
*  I mean, America is amazing. [[00:08:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=492.71999999999997s)]
*  Like I just love this country. [[00:08:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=493.71999999999997s)]
*  It's given us a lot of opportunities. [[00:08:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=495.56s)]
*  I just love the people, like everyday people. [[00:08:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=496.92s)]
*  I like to just talk to people. [[00:08:19](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=499.0s)]
*  I do too. [[00:08:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=500.0s)]
*  I was talking to my driver, which she was like, you know, I'm so embarrassed. [[00:08:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=501.68s)]
*  I didn't know who Dr. Carlson was. [[00:08:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=505.2s)]
*  Good. [[00:08:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=507.2s)]
*  That's why I live here. [[00:08:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=508.2s)]
*  Yeah. [[00:08:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=510.2s)]
*  I was like, well, good for you. [[00:08:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=511.2s)]
*  I think that means you're just like, you know, you're just living your life. [[00:08:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=512.2s)]
*  And she's like, yeah, I'm, you know, I have my kids and my chickens and my whatever. [[00:08:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=514.92s)]
*  I was like, that's great. [[00:08:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=517.96s)]
*  That's awesome. [[00:08:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=518.96s)]
*  It means you're happy. [[00:08:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=519.96s)]
*  It means you're happy. [[00:08:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=520.96s)]
*  Yeah. [[00:08:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=521.96s)]
*  But. [[00:08:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=522.96s)]
*  So I'm sorry to digress. [[00:08:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=523.96s)]
*  I'm sorry to digress. [[00:08:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=524.96s)]
*  Please. [[00:08:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=525.96s)]
*  Please. [[00:08:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=526.96s)]
*  I'm sitting here referring to all these books. [[00:08:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=527.96s)]
*  I'm like, you're not even from here. [[00:08:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=528.96s)]
*  It's incredible. [[00:08:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=529.96s)]
*  So if we look back to AI and to this question of intuition, you don't think that it's inherent. [[00:08:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=531.9200000000001s)]
*  So in other words, if my life is to something governed by technology, by my phone, by my [[00:08:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=538.36s)]
*  computer, by all the technology embedded in like every electronic object, you don't think [[00:09:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=543.52s)]
*  that makes me trust machines more than my own gut? [[00:09:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=548.72s)]
*  You can choose to. [[00:09:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=553.6s)]
*  And I think a lot of people are being guided to do that. [[00:09:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=554.72s)]
*  Ultimately, you're giving away a lot of freedom. [[00:09:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=561.64s)]
*  And there's a, it's not just me saying that there's like a huge tradition of hackers and [[00:09:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=567.1999999999999s)]
*  computer scientists that kind of started ringing the alarm bell like really long time ago about [[00:09:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=573.9599999999999s)]
*  like the way things were trending, which is more centralization, less diversity of competition [[00:09:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=581.04s)]
*  in the market. [[00:09:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=588.3599999999999s)]
*  Yes. [[00:09:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=589.3599999999999s)]
*  And you have like one global social network as opposed to many. [[00:09:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=590.92s)]
*  Now it's actually getting a little better. [[00:09:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=594.0s)]
*  But and you have a lot of these people, you know, start the crypto movement. [[00:09:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=596.52s)]
*  I know you were at the Bitcoin conference recently and you told them CIA started Bitcoin. [[00:10:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=601.84s)]
*  They got really angry on Twitter. [[00:10:06](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=606.52s)]
*  I don't know that. [[00:10:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=608.64s)]
*  But until you can tell me who Satoshi was, I have some questions. [[00:10:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=609.64s)]
*  Actually, I have a feeling about who Satoshi was, but that's a separate conversation. [[00:10:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=613.72s)]
*  Let's just stop right now because I can't, I'll never forget to ask you again. [[00:10:19](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=619.48s)]
*  Who is Satoshi? [[00:10:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=622.16s)]
*  There's a guy, his name is Paul LaRue. [[00:10:23](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=623.16s)]
*  By the way, for those watching who don't know who Satoshi was, Satoshi is the pseudonym [[00:10:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=626.08s)]
*  that we use for the person who created Bitcoin. [[00:10:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=631.84s)]
*  But we don't know who it is. [[00:10:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=634.44s)]
*  It's amazing. [[00:10:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=635.44s)]
*  You know, it's this thing that was created. [[00:10:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=636.44s)]
*  We don't know who created it. [[00:10:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=638.24s)]
*  He never moved the money, I don't think. [[00:10:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=639.8s)]
*  Maybe there was some activity here and there, but like there's like billions, hundreds of [[00:10:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=641.8s)]
*  billions of dollars locked in. [[00:10:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=645.32s)]
*  So we don't know the person as they're not cashing out. [[00:10:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=647.24s)]
*  It's like pretty crazy story, right? [[00:10:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=650.0s)]
*  That's amazing. [[00:10:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=652.44s)]
*  So Paul LaRue. [[00:10:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=653.44s)]
*  Yeah, Paul LaRue was a crypto hacker in Rhodesia before Zimbabwe. [[00:10:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=654.44s)]
*  And he created something called Encryption for the Masses, EM4. [[00:11:06](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=666.64s)]
*  And was one of the early, by the way, I think Snowden used EM4 as part of his hack. [[00:11:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=671.8s)]
*  So he was one of the people that really made it so that cryptography is accessible to more [[00:11:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=677.9200000000001s)]
*  people. [[00:11:23](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=683.36s)]
*  However, he did become a criminal. [[00:11:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=684.36s)]
*  He became a criminal mastermind in Manila. [[00:11:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=685.36s)]
*  He was really controlling the city almost. [[00:11:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=688.36s)]
*  He paid off all the cops and everything. [[00:11:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=691.52s)]
*  He was making so much money from so much criminal activity. [[00:11:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=693.88s)]
*  His nickname was Sletoshi with an L. And so there's like a lot of circumstantial evidence. [[00:11:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=698.4200000000001s)]
*  There's no cutthroat evidence, but I just have a feeling that he generated so much cash. [[00:11:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=704.52s)]
*  He didn't know what to do with it, where to store it. [[00:11:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=709.92s)]
*  And on the side, he was building Bitcoin to be able to store all that cash. [[00:11:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=712.6s)]
*  And around the same time that Sletoshi disappeared, he went to jail. [[00:11:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=717.92s)]
*  He got booked for all the crime he did. [[00:12:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=721.24s)]
*  He recently got sentenced to 25 years of prison. [[00:12:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=725.64s)]
*  I think Judge asked him, what would you do if you would go out? [[00:12:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=728.96s)]
*  He's like, I would build an ASIC chip to mine Bitcoin. [[00:12:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=732.12s)]
*  So look, this is a strong opinion, loosely held, but it's just like there's... [[00:12:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=736.96s)]
*  So he is currently in prison. [[00:12:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=740.76s)]
*  He's currently in prison. [[00:12:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=742.1600000000001s)]
*  In this country or the Philippines? [[00:12:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=744.6s)]
*  I think this country. [[00:12:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=745.6s)]
*  Because he was doing all the crime here. [[00:12:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=746.6s)]
*  He was selling drugs online, essentially. [[00:12:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=749.2s)]
*  We should go see him in jail. [[00:12:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=751.4000000000001s)]
*  Yeah, check out his stories. [[00:12:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=754.24s)]
*  I'm sorry. [[00:12:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=756.4000000000001s)]
*  I just had to get that out of you. [[00:12:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=757.4000000000001s)]
*  So I keep digressing. [[00:12:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=758.76s)]
*  So you see AI, and you're part of the AI ecosystem, of course, but you don't see it as a threat. [[00:12:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=762.48s)]
*  No. [[00:12:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=769.72s)]
*  No. [[00:12:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=770.72s)]
*  I don't see it as a threat at all. [[00:12:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=771.72s)]
*  And I think... [[00:12:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=772.72s)]
*  And I heard some of your podcasts with Joe Rogan, whatever, and you were like, we should [[00:12:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=773.72s)]
*  nuke the data centers. [[00:12:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=778.08s)]
*  Yeah. [[00:12:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=779.08s)]
*  I'm excitable on the basis of very little information. [[00:13:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=780.08s)]
*  Well, actually, tell me what is your theory about the threat of AI? [[00:13:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=783.4s)]
*  I always want to be the kind of man who admits up front his limitations and his ignorance. [[00:13:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=787.28s)]
*  And on this topic, I'm legitimately ignorant. [[00:13:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=793.5600000000001s)]
*  But I have read a lot about it, and I've read most of the alarmist stuff about it. [[00:13:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=795.2s)]
*  And the idea is, as you well know, that the machines become so powerful that they achieve [[00:13:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=800.48s)]
*  a kind of autonomy. [[00:13:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=805.96s)]
*  And they, though designed to serve you, wind up ruling you. [[00:13:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=808.16s)]
*  And I'm really interested in Ted Kaczynski's writings, his two books that he wrote, obviously, [[00:13:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=813.12s)]
*  as to say, ritually, totally opposed to letter bombs or violence of any kind. [[00:13:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=823.04s)]
*  But Ted Kaczynski had a lot of provocative and thoughtful things to say about technology. [[00:13:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=828.24s)]
*  It's almost like having live and help, which people make a lot of money. [[00:13:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=834.48s)]
*  They all want to have live and help. [[00:13:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=837.16s)]
*  But the truth about live and help is, they're there to serve you, but you wind up serving [[00:13:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=838.4s)]
*  them and inverts. [[00:14:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=842.6s)]
*  And AI is a kind of species of that. [[00:14:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=844.44s)]
*  That's the fear. [[00:14:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=847.72s)]
*  And I don't want to be a slave to a machine any more than I already am. [[00:14:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=849.16s)]
*  So it's kind of that simple. [[00:14:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=853.52s)]
*  And then there's all this other stuff. [[00:14:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=854.52s)]
*  You know a lot more about this than I do, senior in that world. [[00:14:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=856.2s)]
*  But yeah, that's my concern. [[00:14:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=858.48s)]
*  That's actually a quite valid concern. [[00:14:19](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=859.48s)]
*  I would like decouple the existential threat concern from the concern. [[00:14:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=861.2s)]
*  And we've been talking about this, of like us being slaves to the machines. [[00:14:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=866.8000000000001s)]
*  And I think Ted Kaczynski's critique of technology is actually one of the best. [[00:14:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=871.52s)]
*  Yes, thank you. [[00:14:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=878.3199999999999s)]
*  I wish he hadn't killed people, of course, because I'm against killing. [[00:14:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=880.52s)]
*  But I also think it had the opposite of the intended effect. [[00:14:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=884.42s)]
*  He did it in order to bring attention to his thesis and ended up obscuring it. [[00:14:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=887.4s)]
*  But I really wish that every person in America would read not just his manifesto, but the [[00:14:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=894.22s)]
*  book that he wrote from prison because they're just so, at the least, they're thought provoking [[00:15:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=900.52s)]
*  and really important. [[00:15:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=904.68s)]
*  Yeah, yeah. [[00:15:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=905.68s)]
*  I mean, briefly, and we'll get to existential risk in a second, but he talked about this [[00:15:06](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=906.68s)]
*  thing called the power process, which is he thinks that it's intrinsic to human happiness [[00:15:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=910.56s)]
*  to struggle for survival, to go through life as a child, as an adult, build up yourself, [[00:15:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=916.72s)]
*  get married, have kids, and then become the elder and then die, right? [[00:15:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=924.88s)]
*  Exactly. [[00:15:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=929.36s)]
*  He thinks that modern technology kind of disrupts this process and it makes people miserable. [[00:15:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=931.2s)]
*  How do you know that? [[00:15:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=935.24s)]
*  I read it. [[00:15:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=936.24s)]
*  I'm very curious. [[00:15:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=937.24s)]
*  I read a lot of things and I just don't have mental censorship in a way. [[00:15:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=938.24s)]
*  I'm really curious. [[00:15:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=945.24s)]
*  I'll read anything. [[00:15:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=948.24s)]
*  Do you think being from another country has helped you in that way? [[00:15:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=949.24s)]
*  Yeah. [[00:15:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=952.4s)]
*  And I also think that just my childhood, I was always different. [[00:15:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=953.4s)]
*  When I had hair, it was all red. [[00:15:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=958.0s)]
*  It was bright red. [[00:16:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=960.12s)]
*  And my whole family is kind of, or at least half of my family are redheads. [[00:16:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=962.52s)]
*  And because of that experience, I was like, okay, I'm different. [[00:16:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=969.88s)]
*  I'm comfortable being different. [[00:16:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=974.3199999999999s)]
*  I'll be different. [[00:16:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=975.52s)]
*  And that just commitment to not worrying about anything, about conforming or like it was [[00:16:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=978.64s)]
*  forced on me that I'm not conforming just by virtue of being different and being curious [[00:16:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=985.88s)]
*  and being good with computers and all that. [[00:16:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=992.3199999999999s)]
*  I think that carried me through life. [[00:16:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=998.1999999999999s)]
*  I get almost a disgust reaction to conformism and mob mentality. [[00:16:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1004.04s)]
*  I couldn't agree more. [[00:16:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1009.1999999999999s)]
*  I had a similar experience in childhood. [[00:16:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1010.48s)]
*  I totally agree with you. [[00:16:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1011.72s)]
*  We've traveled to an awful lot of countries on this show, to some free countries, the [[00:16:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1013.6800000000001s)]
*  dwindling number and a lot of not very free countries, places famous for government censorship. [[00:16:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1017.36s)]
*  And wherever we go, we use a virtual private network of VPN and we use ExpressVPN. [[00:17:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1023.04s)]
*  We do it to access the free and open internet. [[00:17:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1030.1200000000001s)]
*  But the interesting thing is when we come back here to the United States, we still use [[00:17:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1034.56s)]
*  ExpressVPN. [[00:17:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1037.88s)]
*  Why? [[00:17:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1038.88s)]
*  Big tech surveillance. [[00:17:19](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1039.88s)]
*  It's everywhere. [[00:17:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1041.44s)]
*  It's not just North Korea that monitors every move its citizens make. [[00:17:23](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1043.0s)]
*  No. [[00:17:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1046.72s)]
*  That same thing happens right here in the United States and in Canada and Great Britain [[00:17:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1047.72s)]
*  and around the world. [[00:17:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1051.68s)]
*  Internet providers can see every website you visit. [[00:17:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1053.68s)]
*  Did you know that? [[00:17:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1056.52s)]
*  They may even be required to keep your browsing history on file for years and then turn it [[00:17:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1057.52s)]
*  over to federal authorities if asked. [[00:17:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1062.28s)]
*  In the United States, internet providers are legally allowed to and regularly do sell your [[00:17:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1065.4s)]
*  browsing history everywhere you go online. [[00:17:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1070.04s)]
*  There is no privacy. [[00:17:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1072.6399999999999s)]
*  Did you know that? [[00:17:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1074.06s)]
*  Well, we did. [[00:17:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1075.06s)]
*  And that's why we use ExpressVPN. [[00:17:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1076.06s)]
*  And because we do, our internet provider never knows where we're going on the internet. [[00:17:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1078.36s)]
*  They never hear it in the first place. [[00:18:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1082.8799999999999s)]
*  That's because 100 percent of our online activity is routed through ExpressVPN's secure encrypted [[00:18:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1084.8s)]
*  servers. [[00:18:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1090.72s)]
*  They hide our IP address so data brokers cannot track us and sell our online activity on the [[00:18:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1091.92s)]
*  black market. [[00:18:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1097.44s)]
*  So we have privacy. [[00:18:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1098.88s)]
*  ExpressVPN lets you connect to servers in 105 different countries. [[00:18:19](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1099.88s)]
*  So basically you can go online like you're anywhere in the world. [[00:18:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1105.1200000000001s)]
*  No one can see you. [[00:18:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1108.64s)]
*  This was the promise of the internet in the first place. [[00:18:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1109.88s)]
*  Privacy and freedom. [[00:18:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1112.76s)]
*  Those didn't seem like they were achievable, but now they are. [[00:18:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1115.0s)]
*  ExpressVPN, we cannot recommend it enough. [[00:18:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1119.3200000000002s)]
*  It's also really easy to use. [[00:18:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1121.2s)]
*  Whether or not you fully understand the technology behind it. [[00:18:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1123.2s)]
*  You can use it on your phone, laptop, tablet, even your smart TVs. [[00:18:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1125.8s)]
*  You press one button, just tap it, and you're protected. [[00:18:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1130.28s)]
*  You have privacy. [[00:18:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1133.6399999999999s)]
*  So if you want online privacy and the freedom it bestows, get it. [[00:18:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1135.48s)]
*  You can go to our special link right here to get three extra months free of ExpressVPN. [[00:19:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1140.76s)]
*  That's expressvpn.com slash Tucker. [[00:19:06](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1146.8799999999999s)]
*  So Kaczynski's thesis that struggle is not only inherent to the condition, but an essential [[00:19:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1150.4s)]
*  part of your evolution as a man or as a person. [[00:19:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1155.96s)]
*  And that technology disrupts that. [[00:19:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1160.5600000000002s)]
*  I mean, that seems right to me. [[00:19:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1162.5600000000002s)]
*  Yeah, and I actually struggled to sort of dispute that despite being a tech person. [[00:19:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1164.5600000000002s)]
*  I mean, I was a tech person. [[00:19:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1172.5600000000002s)]
*  I was a tech person. [[00:19:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1174.5600000000002s)]
*  I think ultimately where I kind of defer, and again, it just goes back to a lot of what [[00:19:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1177.44s)]
*  we're talking about, my views on technology as an extension of us. [[00:19:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1181.44s)]
*  It's like, we just don't want technology to be a thing that's just merely replacing us. [[00:19:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1185.44s)]
*  We want it to be an empowerment. [[00:19:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1191.44s)]
*  We want it to be a way to be able to do things that are just a little bit more efficient. [[00:19:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1193.44s)]
*  So I think that's a really good point. [[00:19:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1197.44s)]
*  I think that's a really good point. [[00:19:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1199.44s)]
*  And a thing that's just merely replacing us. [[00:20:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1202.3200000000002s)]
*  We want it to be an empowering thing. [[00:20:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1204.3200000000002s)]
*  And what we do at Replet is we empower people to learn the code, to build startups, to build [[00:20:06](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1206.3200000000002s)]
*  companies, to become entrepreneurs. [[00:20:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1212.3200000000002s)]
*  And I think you can, in this world, you have to create the power process. [[00:20:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1214.3200000000002s)]
*  You have to struggle. [[00:20:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1222.3200000000002s)]
*  And yes, you can. [[00:20:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1225.2s)]
*  This is why I'm also, you know, a lot of technologists talk about UBI and universal [[00:20:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1227.2s)]
*  basic income. [[00:20:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1232.56s)]
*  Oh, I know. [[00:20:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1233.04s)]
*  I think it's all wrong because it just goes against human nature. [[00:20:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1233.52s)]
*  Thank you. [[00:20:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1236.8s)]
*  So I think- [[00:20:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1238.24s)]
*  You want to kill everybody, put them on the dole. [[00:20:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1238.8799999999999s)]
*  Yes. [[00:20:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1241.2s)]
*  Yes. [[00:20:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1242.24s)]
*  So I don't think technology is inherently at odds with the power process. [[00:20:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1243.28s)]
*  I'll leave it at that. [[00:20:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1250.0s)]
*  We can go to existential threat. [[00:20:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1250.8s)]
*  Yeah, of course. [[00:20:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1253.84s)]
*  Sorry. [[00:20:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1254.8799999999999s)]
*  Boy, am I just aggressive. [[00:20:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1255.28s)]
*  I can't believe I interview people for a living. [[00:20:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1257.4399999999998s)]
*  We had dinner last night. [[00:21:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1261.36s)]
*  Yeah, it was awesome. [[00:21:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1262.3999999999999s)]
*  It was one of the best dinners. [[00:21:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1263.4399999999998s)]
*  Oh, it was the best. [[00:21:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1264.3999999999999s)]
*  But we hit about 400 different threats. [[00:21:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1264.9599999999998s)]
*  Yes. [[00:21:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1268.0s)]
*  That was amazing. [[00:21:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1268.6399999999999s)]
*  So that's what's out there. [[00:21:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1269.6s)]
*  I know I'm sort of convinced of it. [[00:21:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1272.56s)]
*  It makes sense to me. [[00:21:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1275.36s)]
*  And I'm kind of threat oriented anyway. [[00:21:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1278.1599999999999s)]
*  So people with my kind of personality are always looking for the big bad thing that's coming, [[00:21:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1280.48s)]
*  the asteroid or the nuclear war, the AI, slavery. [[00:21:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1286.48s)]
*  But I know some pretty smart people who, very smart people, [[00:21:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1290.8s)]
*  who are much closer to the heart of AI development who also have these concerns. [[00:21:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1294.4s)]
*  And I think a lot of the public shares these concerns. [[00:21:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1300.56s)]
*  And the last thing I'll say before soliciting your view of it, [[00:21:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1304.0s)]
*  much better informed view of it, [[00:21:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1307.84s)]
*  is that there's been surprisingly and tellingly little conversation about the upside of AI. [[00:21:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1309.6799999999998s)]
*  So instead it's like, this is happening. [[00:21:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1317.4399999999998s)]
*  And if we don't do it, China will. [[00:21:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1319.36s)]
*  That may, I think that's probably true. [[00:22:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1321.28s)]
*  But like, why should I be psyched about it? [[00:22:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1323.84s)]
*  Like, what's the upside for me? [[00:22:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1325.76s)]
*  You know what I mean? [[00:22:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1328.08s)]
*  Normally when some new technology or huge change comes, [[00:22:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1328.6399999999999s)]
*  the people who are profiting from like, you know what? [[00:22:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1333.04s)]
*  It's going to be great. [[00:22:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1334.6399999999999s)]
*  It's going to be great. [[00:22:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1335.76s)]
*  You're not going to ever have to do X again. [[00:22:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1336.56s)]
*  You know, you just throw your clothes in a machine and press a button and they'll be clean. [[00:22:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1338.96s)]
*  Yes. [[00:22:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1341.6s)]
*  I'm not hearing any of that about AI. [[00:22:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1342.3999999999999s)]
*  That's a very astute observation. [[00:22:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1344.32s)]
*  And I'll exactly tell you why. [[00:22:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1345.84s)]
*  And to tell you why, it's like a little bit of a long story [[00:22:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1349.2s)]
*  because I think there is a organized effort to scare people about AI. [[00:22:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1351.6799999999998s)]
*  Organized? [[00:22:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1357.6799999999998s)]
*  Organized, yes. [[00:22:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1358.3999999999999s)]
*  And so this starts with a mailing list in the 90s. [[00:22:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1359.36s)]
*  Is a transhumanist mailing list called the Extropians. [[00:22:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1364.1599999999999s)]
*  And these Extropians, they, I might have got them wrong, [[00:22:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1369.9199999999998s)]
*  Extropia or something like that, but they believe in the singularity. [[00:22:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1373.12s)]
*  So the singularity is a moment of time where, you know, AI is progressing so fast [[00:22:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1378.8799999999999s)]
*  or technology in general progressing so fast that you can't predict what happens. [[00:23:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1385.28s)]
*  It's self-evolving and it just all bets are off. [[00:23:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1388.8799999999999s)]
*  We're entering a new world where you just can't predict it. [[00:23:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1393.9199999999998s)]
*  Where technology can't be controlled. [[00:23:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1396.9599999999998s)]
*  Technology can't be controlled. [[00:23:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1398.56s)]
*  It's going to remake everything. [[00:23:19](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1399.6799999999998s)]
*  And those people believe that's a good thing because the world now sucks so much [[00:23:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1402.8s)]
*  and we are imperfect and unethical and all sorts of irrational or whatever. [[00:23:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1407.04s)]
*  And so they really wanted for the singularity to happen. [[00:23:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1412.6399999999999s)]
*  And there's this young guy on this list, his name is Ilya Yatsur-Yatskowski. [[00:23:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1417.12s)]
*  And he claims he can write this AI and he would write like really long assays [[00:23:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1421.84s)]
*  about how to build this AI. [[00:23:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1426.96s)]
*  Suspiciously, he never really publishes code and it's all just prose [[00:23:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1429.68s)]
*  about how he's going to be able to build AI. [[00:23:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1433.92s)]
*  Anyways, he's able to fundraise. [[00:23:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1436.48s)]
*  They started this thing called the Singularity Institute. [[00:23:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1439.36s)]
*  A lot of people were excited about the future, kind of invested in him. [[00:24:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1442.08s)]
*  Peter Thiel most famously. [[00:24:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1445.2s)]
*  And he spent a few years trying to build an AI. [[00:24:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1447.36s)]
*  Again, never published code, never published any real progress. [[00:24:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1450.32s)]
*  And then came out of it saying that not only you can't build AI, [[00:24:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1456.72s)]
*  but if you build it, it will kill everyone. [[00:24:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1460.32s)]
*  So he kind of switched from being this optimist, singularity is great, [[00:24:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1462.6399999999999s)]
*  to like actually AI will for sure kill everyone. [[00:24:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1466.8799999999999s)]
*  And then he was like, okay, the reason I made this mistake is because I was irrational. [[00:24:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1471.12s)]
*  And the way to get people to understand that AI is going to kill everyone [[00:24:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1477.76s)]
*  is to make them rational. [[00:24:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1481.04s)]
*  So he started this blog called Less Wrong. [[00:24:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1482.8799999999999s)]
*  And Less Wrong is like walks you through steps to becoming more rational. [[00:24:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1485.2s)]
*  Look at your biases, examine yourself, sit down, [[00:24:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1489.04s)]
*  meditate on all the irrational decisions you've made and try to correct them. [[00:24:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1492.1599999999999s)]
*  And then they start this thing called Center for Advanced Rationality [[00:24:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1497.12s)]
*  or something like that, CFAR. [[00:25:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1501.52s)]
*  And they're giving seminars about rationality. [[00:25:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1503.52s)]
*  What's the seminar about rationality? [[00:25:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1507.76s)]
*  What's that like? [[00:25:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1509.36s)]
*  I've never been to one, but my guess would be they will talk about the biases or whatever. [[00:25:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1511.28s)]
*  But they have also like weird things where [[00:25:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1515.68s)]
*  they have this almost struggle session like thing called debugging. [[00:25:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1518.6399999999999s)]
*  A lot of people wrote blog posts about how that was demeaning [[00:25:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1522.0s)]
*  and it caused psychosis in some people. [[00:25:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1526.56s)]
*  2017, that community, there was like collective psychosis. [[00:25:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1528.72s)]
*  A lot of people were kind of going crazy. [[00:25:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1531.68s)]
*  And it was all written about it on the internet. [[00:25:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1533.6s)]
*  Debugging, so that would be like kind of your classic cult technique [[00:25:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1535.68s)]
*  where you have to strip yourself bare like auditing and Scientology. [[00:25:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1540.3200000000002s)]
*  Yes. [[00:25:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1543.52s)]
*  Or it's very common. [[00:25:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1543.68s)]
*  It's a constant in cults. [[00:25:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1547.52s)]
*  Yes. [[00:25:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1548.88s)]
*  Is that what you're describing? [[00:25:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1549.6000000000001s)]
*  Yeah, I mean, that's what I read on these accounts. [[00:25:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1550.72s)]
*  They will sit down and they will audit your mind [[00:25:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1553.76s)]
*  and tell you where you're wrong and all of that. [[00:25:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1556.16s)]
*  And it caused people huge distress. [[00:26:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1560.8s)]
*  Young guys all the time talk about how going into that community [[00:26:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1563.2s)]
*  has caused them huge distress. [[00:26:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1567.04s)]
*  And there were like offshoots of this community [[00:26:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1568.32s)]
*  where there were suicides, there were murders, [[00:26:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1571.1200000000001s)]
*  there were a lot of really dark and deep shit. [[00:26:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1573.6000000000001s)]
*  And the other thing is they kind of teach you about rationality. [[00:26:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1576.16s)]
*  They recruit you to AI risk because if you're rational, [[00:26:19](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1579.68s)]
*  you're a group, we're all rational now. [[00:26:23](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1583.92s)]
*  We learned the art of rationality. [[00:26:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1585.44s)]
*  And we agree that AI is going to kill everyone. [[00:26:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1587.76s)]
*  Therefore, everyone outside of this group is wrong [[00:26:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1591.12s)]
*  and we have to protect them. [[00:26:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1594.3999999999999s)]
*  AI is going to kill everyone. [[00:26:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1595.76s)]
*  But also they believe other things. [[00:26:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1597.9199999999998s)]
*  Like they believe that polyamory is rational. [[00:26:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1599.84s)]
*  Polyamory? [[00:26:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1605.36s)]
*  Yeah, like you can have sex with multiple partners essentially. [[00:26:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1606.32s)]
*  I think it's certainly a natural desire if you're a man [[00:26:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1612.3999999999999s)]
*  to sleep with more indifferent women for sure. [[00:26:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1617.04s)]
*  But it's rational in the sense how? [[00:27:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1620.08s)]
*  Like you've never met a happy polyamorous long term. [[00:27:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1622.6399999999999s)]
*  I've done a lot of them, not a single one. [[00:27:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1627.1999999999998s)]
*  So how is it? [[00:27:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1629.52s)]
*  It might be self-serving. [[00:27:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1630.0s)]
*  You think? [[00:27:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1631.4399999999998s)]
*  To recruit more impressionable people into... [[00:27:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1633.04s)]
*  And their hot girlfriends. [[00:27:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1637.04s)]
*  Yes. [[00:27:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1638.1599999999999s)]
*  Right. [[00:27:19](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1639.1999999999998s)]
*  So that's rational? [[00:27:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1641.36s)]
*  Yeah, supposedly. [[00:27:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1642.8s)]
*  And so they convince each other of all these cult-like behavior. [[00:27:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1644.4799999999998s)]
*  And the crazy thing is like this group ends up being super influential [[00:27:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1648.72s)]
*  because they recruit a lot of people that are interested in AI. [[00:27:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1654.72s)]
*  And the AI labs and the people who are starting these companies [[00:27:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1660.0s)]
*  were reading all this stuff. [[00:27:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1665.1200000000001s)]
*  So Elon famously read a lot of... [[00:27:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1666.72s)]
*  Nick Bostrom is kind of an adjacent figure to the rationality community. [[00:27:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1670.4s)]
*  He was part of the original mailing list. [[00:27:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1675.0400000000002s)]
*  I think he would call himself a rational part of the rational community. [[00:27:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1677.2s)]
*  But he wrote a book about AI and how AI is going to kill everyone essentially. [[00:28:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1680.8s)]
*  I think he moderated his views more recently, [[00:28:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1685.04s)]
*  but originally he was one of the people that are kind of banging the alarm. [[00:28:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1687.2s)]
*  And the foundation of OpenAI was based on a lot of these fears. [[00:28:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1691.76s)]
*  Like Elon had fears of AI killing everyone. [[00:28:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1698.48s)]
*  He was afraid that Google was going to do that. [[00:28:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1702.16s)]
*  And so they group of people. [[00:28:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1704.8s)]
*  I don't think everyone at OpenAI really believed that, [[00:28:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1706.48s)]
*  but some of the original founding story was that. [[00:28:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1709.2s)]
*  And they were recruiting from that community. [[00:28:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1712.24s)]
*  So much so when Sam Altman got fired recently, [[00:28:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1714.88s)]
*  he was fired by someone from that community. [[00:28:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1719.68s)]
*  Someone who started with effective altruism, [[00:28:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1725.3600000000001s)]
*  which is another offshoot from that community. [[00:28:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1727.04s)]
*  Really? [[00:28:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1730.4s)]
*  And so the AI labs are intermarried in a lot of ways with this community. [[00:28:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1731.1200000000001s)]
*  So they kind of borrowed a lot of their talking points. [[00:28:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1736.6399999999999s)]
*  By the way, a lot of these companies are great companies now, [[00:29:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1742.1599999999999s)]
*  and I think they're cleaning up house. [[00:29:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1744.24s)]
*  I'll just use the term. [[00:29:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1747.76s)]
*  It sounds like a cult to me. [[00:29:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1748.6399999999999s)]
*  It has the hallmarks of it in your description. [[00:29:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1751.2s)]
*  And can we just push a little deeper on what they believe? [[00:29:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1753.9199999999998s)]
*  You say they are transhumanists. [[00:29:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1757.28s)]
*  What is that? [[00:29:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1760.1599999999999s)]
*  Well, I think they're just unsatisfied with human nature, [[00:29:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1760.8799999999999s)]
*  unsatisfied with the current ways we're constructed, [[00:29:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1764.56s)]
*  and that we're irrational, we're unethical. [[00:29:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1770.3999999999999s)]
*  And so they start, they long for the world where we can become more rational, [[00:29:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1774.1599999999999s)]
*  more ethical by transforming ourselves, [[00:29:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1781.2s)]
*  either by merging with AI via chips or what have you, [[00:29:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1784.24s)]
*  changing our bodies and fixing fundamental issues [[00:29:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1789.52s)]
*  that they perceive with humans via modifications and merging with machines. [[00:29:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1794.96s)]
*  It's just so interesting and so shallow and silly, [[00:29:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1799.44s)]
*  like a lot of those people I have known are, [[00:30:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1805.12s)]
*  not that smart actually, [[00:30:06](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1806.96s)]
*  because the best things, I mean, reason is important, [[00:30:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1808.56s)]
*  and we should, in my view, given us by God, [[00:30:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1810.8799999999999s)]
*  and it's really important, and being irrational is bad. [[00:30:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1813.68s)]
*  On the other hand, the best things about people, [[00:30:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1818.0s)]
*  their best impulses are not rational. [[00:30:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1820.6399999999999s)]
*  I believe so too. [[00:30:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1824.1599999999999s)]
*  There is no rational justification for giving something you need to another person, [[00:30:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1824.8799999999999s)]
*  for spending an inordinate amount of time helping someone, [[00:30:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1830.6399999999999s)]
*  for loving someone, those are all irrational. [[00:30:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1833.04s)]
*  Now, banging someone's hot girlfriend, I guess that's rational, [[00:30:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1835.2s)]
*  but that's kind of the lowest impulse that we have, actually. [[00:30:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1838.48s)]
*  We'll wait to hear about effective altruism. [[00:30:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1841.36s)]
*  They think our natural impulses that you just talked about are indeed irrational, [[00:30:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1844.4s)]
*  and there's a guy, his name is Peter Singer, a philosopher from Australia. [[00:30:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1849.76s)]
*  The infanticide guy. [[00:30:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1854.24s)]
*  Yes. [[00:30:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1855.44s)]
*  He's so ethical, he's for killing children. [[00:30:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1855.76s)]
*  Yeah, I mean, their philosophy is utilitarianism, [[00:30:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1858.24s)]
*  is that you can calculate ethics, and you can start applying it, [[00:31:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1862.24s)]
*  and you get into really weird territory. [[00:31:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1865.52s)]
*  There's all these problems, all these thought experiments, [[00:31:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1870.0s)]
*  like you have two people at the hospital requiring some organs of another third person [[00:31:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1872.4s)]
*  that came in for a regular checkup, or they will die, you're ethically, [[00:31:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1880.3200000000002s)]
*  you're supposed to kill that guy, get his organ, and put it into the other two. [[00:31:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1888.0800000000002s)]
*  I don't think people believe that, per se, [[00:31:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1895.44s)]
*  but there are so many problems with that. [[00:31:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1897.92s)]
*  There's another belief that they have. [[00:31:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1904.72s)]
*  But can I say that belief or that conclusion grows out of the core belief, [[00:31:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1906.0s)]
*  which is that you're God. [[00:31:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1911.6000000000001s)]
*  It's like a normal person realizes, sure, it would help more people [[00:31:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1912.96s)]
*  if I killed that person and gave his organs to a number of people, [[00:31:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1917.28s)]
*  like that's just a math question, true, [[00:32:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1920.88s)]
*  but I'm not allowed to do that because I didn't create life. [[00:32:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1923.68s)]
*  I don't have the power. [[00:32:06](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1926.8s)]
*  I'm not allowed to make decisions like that [[00:32:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1928.1599999999999s)]
*  because I'm just a silly human being who can't see the future [[00:32:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1931.04s)]
*  and is not omnipotent because I'm not God. [[00:32:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1933.76s)]
*  I feel like all of these conclusions stem from the misconception that people are gods. [[00:32:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1935.68s)]
*  Does that sound right? [[00:32:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1942.48s)]
*  No, I agree. [[00:32:23](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1943.2s)]
*  I mean, a lot of the... [[00:32:23](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1943.84s)]
*  I think at root, they're just fundamentally unsatisfied with humans, [[00:32:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1948.96s)]
*  and maybe perhaps hate the humans. [[00:32:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1955.92s)]
*  Well, they're deeply disappointed. [[00:32:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1959.2s)]
*  I think that's such a... [[00:32:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1962.0s)]
*  I've never heard anyone say that as well, [[00:32:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1963.04s)]
*  that they're disappointed with human nature, [[00:32:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1965.76s)]
*  they're disappointed with human condition, [[00:32:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1968.16s)]
*  they're disappointed with people's flaws. [[00:32:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1969.3600000000001s)]
*  And I feel like that's the... [[00:32:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1971.6000000000001s)]
*  I mean, on one level, of course, I mean, we should be better. [[00:32:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1972.88s)]
*  But we used to call that judgment, which we're not allowed to do, by the way. [[00:32:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1977.04s)]
*  That's just super judgy, actually. [[00:33:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1982.24s)]
*  What they're saying is, you suck. [[00:33:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1983.84s)]
*  And it's just a short hop from there to you should be killed, I think. [[00:33:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1987.6799999999998s)]
*  I mean, that's a total lack of love. [[00:33:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1992.48s)]
*  Whereas a normal person, a loving person says, [[00:33:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1994.56s)]
*  you kind of suck, I kind of suck too. [[00:33:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=1997.52s)]
*  But I love you anyway, and you love me anyway, [[00:33:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2000.6399999999999s)]
*  and I'm grateful for your love, right? [[00:33:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2002.48s)]
*  That's right. [[00:33:23](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2003.84s)]
*  Well, they'll say, you suck, join our rationality community, have sex with us. [[00:33:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2004.72s)]
*  Can I just clarify, these aren't just like, [[00:33:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2008.4s)]
*  support staff at these companies? [[00:33:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2016.0800000000002s)]
*  You've heard about SPF and FDX. [[00:33:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2018.88s)]
*  They had what's called a polycule. [[00:33:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2021.1200000000001s)]
*  They were all having sex with each other. [[00:33:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2024.16s)]
*  Just given now, I just want to be super catty and shallow, [[00:33:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2026.64s)]
*  but given some of the people they were having sex with, that was not rational. [[00:33:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2029.2s)]
*  Come on now. [[00:33:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2036.3200000000002s)]
*  Yeah, that's true. [[00:33:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2037.1200000000001s)]
*  Well, so, what's even more disturbing, [[00:33:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2038.88s)]
*  there's another ethical component to their philosophy called long-termism. [[00:34:06](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2046.8000000000002s)]
*  And this comes from the effective altruist sort of branch of rationality. [[00:34:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2053.04s)]
*  Long-termism? [[00:34:19](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2059.12s)]
*  Long-termism. [[00:34:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2060.32s)]
*  And so what they think is, in the future, if we made the right steps, [[00:34:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2061.04s)]
*  there's going to be a trillion humans, trillion minds. [[00:34:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2066.64s)]
*  They might not be humans, that might be AI, [[00:34:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2069.52s)]
*  but they're going to be trillion minds who can experience utility, [[00:34:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2071.2799999999997s)]
*  can experience good things, fun things, whatever. [[00:34:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2074.08s)]
*  If you're utilitarian, you have to put a lot of weight on it. [[00:34:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2077.2s)]
*  And maybe you discount that, sort of like discounted cash flows, [[00:34:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2081.04s)]
*  but you still have to posit that if there are trillions, perhaps many more people in the future, [[00:34:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2085.44s)]
*  you need to value that very highly. [[00:34:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2095.04s)]
*  Even if you discount it a lot, it ends up being valued very highly. [[00:34:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2096.72s)]
*  So a lot of these communities end up all focusing on AI safety, [[00:35:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2100.32s)]
*  because they think that AI, because they're rational, [[00:35:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2104.64s)]
*  they arrived, and we can talk about their arguments in a second, [[00:35:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2107.44s)]
*  they arrived at the conclusion that AI is going to kill everyone. [[00:35:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2110.16s)]
*  Therefore, effective altruists and rational community, all these branches, [[00:35:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2113.2799999999997s)]
*  they're all kind of focused on AI safety, because that's the most important thing, [[00:35:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2116.88s)]
*  because we want a trillion people in the future to be great. [[00:35:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2122.56s)]
*  But when you're assigning value that high, it's sort of a form of Pascal's wager. [[00:35:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2126.32s)]
*  You can justify anything, including terrorism, including doing really bad things. [[00:35:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2137.2s)]
*  If you're really convinced that AI is going to kill everyone, [[00:35:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2144.72s)]
*  and the future holds so much value, more value than any living human today has value, [[00:35:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2148.3999999999996s)]
*  you might justify really doing anything. [[00:35:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2157.2799999999997s)]
*  And so built into that, it's a dangerous framework. [[00:36:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2160.08s)]
*  But it's the same framework of every genocidal movement from, you know, [[00:36:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2164.0s)]
*  at least the French Revolution to present. [[00:36:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2170.3999999999996s)]
*  Yes. [[00:36:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2172.0s)]
*  A glorious future justifies a bloody present. [[00:36:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2173.3599999999997s)]
*  Yes. [[00:36:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2176.0s)]
*  And look, I'm not accusing them of genocidal intent, by the way. [[00:36:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2177.46s)]
*  I don't know them, but those ideas lead very quickly to the camps. [[00:36:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2181.3s)]
*  I feel kind of weird just talking about people who just generally, [[00:36:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2185.62s)]
*  I like to talk about ideas about things. [[00:36:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2187.86s)]
*  But if they were just like a silly Berkeley cult or whatever, [[00:36:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2190.26s)]
*  and they didn't have any real impact on the world, I wouldn't care about them. [[00:36:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2194.42s)]
*  But what's happening is that they were able to convince a lot of billionaires [[00:36:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2199.7s)]
*  of these ideas. [[00:36:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2204.26s)]
*  I think Elon maybe changed his mind, but at some point he was convinced of these ideas. [[00:36:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2206.02s)]
*  I don't know if he gave them money. [[00:36:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2210.26s)]
*  There was a story at some point, and while he was thinking about it, [[00:36:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2211.5400000000004s)]
*  but a lot of other billionaires gave them money, and now they're organized, [[00:36:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2214.6600000000003s)]
*  and they're in DC lobbying for AI regulation. [[00:36:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2218.7400000000002s)]
*  They're behind the AI regulation in California, and actually profiting from it. [[00:37:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2222.6600000000003s)]
*  There was a story in Piratewares where the main sponsor, Dan Hendricks, behind SB 1047, [[00:37:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2229.7s)]
*  started a company at the same time that certifies the safety of AI. [[00:37:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2240.18s)]
*  And as part of the bill, it says that you have to get certified by a third party. [[00:37:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2244.3399999999997s)]
*  So there's aspects of it that are kind of less profit from it. [[00:37:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2248.5s)]
*  By the way, this is all allegedly based on this article. [[00:37:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2253.46s)]
*  I don't know for sure. [[00:37:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2256.2599999999998s)]
*  I think Senator Scott Wiener was trying to do the right thing with the bill, [[00:37:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2258.1s)]
*  but he was listening to a lot of these cult members, let's call them. [[00:37:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2262.74s)]
*  And they're very well organized, and also a lot of them still have connections to the big AI labs. [[00:37:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2270.9s)]
*  Some of them work there, and they would want to create a situation where there's no competition [[00:37:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2278.1s)]
*  in AI, regulatory capture per se. [[00:38:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2284.26s)]
*  I'm not saying that these are the direct motivations. [[00:38:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2288.0200000000004s)]
*  All of them are true believers, but you might infiltrate this group [[00:38:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2290.6600000000003s)]
*  and direct it in a way that benefits these corporations. [[00:38:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2297.0600000000004s)]
*  Yeah. [[00:38:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2300.42s)]
*  Well, I'm from DC, so I've seen a lot of instances where my bank account aligns with my beliefs. [[00:38:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2300.7400000000002s)]
*  Thank heaven. [[00:38:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2307.2200000000003s)]
*  It just kind of happens. [[00:38:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2308.98s)]
*  It winds up that way. [[00:38:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2309.86s)]
*  It's funny. [[00:38:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2310.5s)]
*  Climate is the perfect example. [[00:38:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2311.78s)]
*  There's never one climate solution that makes the person who proposes it poor or less powerful. [[00:38:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2313.78s)]
*  Exactly. [[00:38:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2319.46s)]
*  Ever. [[00:38:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2319.94s)]
*  Not one. [[00:38:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2320.26s)]
*  We've told you before about Halo. [[00:38:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2320.98s)]
*  It is a great app that I am proud to say I use, my whole family uses. [[00:38:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2323.3s)]
*  It's for daily prayer and Christian meditation, and it's transformative. [[00:38:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2328.1000000000004s)]
*  As we head into the start of school and the height of election season, you need it. [[00:38:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2332.5s)]
*  Trust me, we all do. [[00:38:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2336.5800000000004s)]
*  Things are going to get crazier and crazier and crazier. [[00:38:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2337.78s)]
*  Sometimes it's hard to imagine even what is coming next. [[00:39:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2340.5800000000004s)]
*  So with everything happening in the world right now, it is essential to ground yourself. [[00:39:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2344.1800000000003s)]
*  This is not some quack cure. [[00:39:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2350.1800000000003s)]
*  This is the oldest and most reliable cure in history. [[00:39:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2353.38s)]
*  It's prayer. [[00:39:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2356.9s)]
*  Ground yourself in prayer and scripture every single day. [[00:39:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2358.02s)]
*  That is a prerequisite for staying sane and healthy and maybe for doing better eternally. [[00:39:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2360.82s)]
*  So if you're busy on the road, headed to kids sports, there's always time to pray and reflect alone or as a family. [[00:39:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2366.98s)]
*  But it's hard to be organized about it. [[00:39:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2372.82s)]
*  Building a foundation of prayer is going to be absolutely critical as we head into November, [[00:39:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2375.6200000000003s)]
*  praying that God's will is done in this country and that peace and healing come to us here in the United States and around the world. [[00:39:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2379.54s)]
*  Christianity obviously is under attack everywhere. [[00:39:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2386.9s)]
*  That's not an accident. [[00:39:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2391.38s)]
*  Why is Christianity, the most peaceful of all religions, under attack globally? [[00:39:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2392.5800000000004s)]
*  Did you see the opening of the Paris Olympics? [[00:39:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2396.6600000000003s)]
*  There's a reason. [[00:39:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2398.7400000000002s)]
*  Because the battle is not temporal. [[00:40:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2400.02s)]
*  It's taking place in the unseen world. [[00:40:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2401.78s)]
*  It's a spiritual battle, obviously. [[00:40:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2404.42s)]
*  So try Hallow. [[00:40:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2407.3s)]
*  Get three months completely free at Hallow. [[00:40:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2408.1000000000004s)]
*  That's Hallow.com slash Tucker. [[00:40:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2410.5s)]
*  If there is ever a time to get spiritually in tune and ground yourself in prayer, it's now. [[00:40:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2413.7000000000003s)]
*  Hallow will help personally and strongly and totally sincerely recommend it. [[00:40:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2418.7400000000002s)]
*  Hallow.com slash Tucker. [[00:40:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2424.02s)]
*  I wonder about the core assumption, which I've had up until right now, [[00:40:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2426.42s)]
*  that these machines are capable of thinking. [[00:40:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2432.7400000000002s)]
*  Is that true? [[00:40:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2439.1400000000003s)]
*  So let's go through their chain of reasoning. [[00:40:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2440.1800000000003s)]
*  I think the fact that it's a stupid cult-like thing, [[00:40:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2442.1800000000003s)]
*  or perhaps actually a cult, does not automatically mean that their arguments are wrong. [[00:40:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2447.1400000000003s)]
*  That's right. That's exactly right. [[00:40:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2452.98s)]
*  I think you do have to kind of discount some of the arguments, [[00:40:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2454.74s)]
*  because it comes from crazy people. [[00:40:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2458.74s)]
*  But the chain of reasoning is that humans are general intelligence. [[00:41:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2460.42s)]
*  We have these things called brains. [[00:41:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2467.7799999999997s)]
*  Brains are computers. [[00:41:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2470.18s)]
*  They're based on purely physical phenomena that we know they're computing. [[00:41:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2472.2599999999998s)]
*  And if you agree that humans are computing, [[00:41:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2477.2999999999997s)]
*  and therefore we can build a general intelligence in the machine, [[00:41:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2481.78s)]
*  and if you agree up to this point, [[00:41:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2487.6200000000003s)]
*  if you're able to build a general intelligence in the machine, [[00:41:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2490.6600000000003s)]
*  even if only at human level, then you can create a billion copies of it. [[00:41:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2493.1400000000003s)]
*  And then it becomes a lot more powerful than any one of us. [[00:41:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2498.9s)]
*  And because it's a lot more powerful than any one of us, [[00:41:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2502.82s)]
*  it would want to control us, [[00:41:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2506.02s)]
*  or it would not care about us because it's more powerful, [[00:41:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2508.18s)]
*  we don't care about ants, we'll step on ants, no problem, [[00:41:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2513.62s)]
*  because these machines are so powerful, they're not going to care about us. [[00:41:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2516.74s)]
*  And I sort of get off the train at the first chain of reasoning. [[00:42:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2520.98s)]
*  But every one of those steps I have problems with. [[00:42:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2525.46s)]
*  The first step is the mind is a computer. [[00:42:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2529.22s)]
*  And based on what? [[00:42:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2534.4199999999996s)]
*  And the idea is, oh, well, if you don't believe that the mind is a computer, [[00:42:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2536.98s)]
*  then you believe in some kind of spiritual thing. [[00:42:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2540.7400000000002s)]
*  Well, you have to convince me, you haven't presented an argument. [[00:42:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2544.18s)]
*  Speaking of rational, this is what reason looks like. [[00:42:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2552.5s)]
*  Right. [[00:42:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2555.78s)]
*  The idea that we have a complete description of the universe anyways is wrong. [[00:42:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2558.58s)]
*  We don't have a universal physics. [[00:42:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2563.86s)]
*  We have physics of the small things, we have physics of the big things. [[00:42:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2566.5s)]
*  We can't really cohere them or combine them. [[00:42:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2569.7s)]
*  So just the idea that you being a materialist is sort of incoherent [[00:42:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2572.18s)]
*  because we don't have a complete description of the world. [[00:42:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2576.02s)]
*  That's one thing, that's a slight argument. [[00:42:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2578.1s)]
*  I'm not going to dwell on it. [[00:42:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2579.46s)]
*  It's a very interesting argument. [[00:43:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2580.82s)]
*  So you're saying as someone, I mean, you're effectively a scientist. [[00:43:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2582.66s)]
*  Just state for viewers who don't follow this stuff, [[00:43:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2588.9s)]
*  like the limits of our knowledge of physics. [[00:43:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2592.02s)]
*  Yeah. So we have essentially two conflicting theories of physics. [[00:43:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2594.1s)]
*  These systems can't be kind of married. [[00:43:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2598.18s)]
*  They're not a universal system. [[00:43:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2601.46s)]
*  You can't use them both at the same time. [[00:43:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2602.8199999999997s)]
*  Well, that suggests a profound limit to our understanding [[00:43:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2607.14s)]
*  of what's happening around us in the natural world. [[00:43:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2611.46s)]
*  Does it? [[00:43:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2613.86s)]
*  Yes, it does. [[00:43:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2614.42s)]
*  And I think this is, again, another error of the rationalist types [[00:43:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2615.06s)]
*  is that just assume that we were so much more advanced [[00:43:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2619.38s)]
*  in our science than we actually are. [[00:43:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2622.58s)]
*  So it sounds like they don't know that much about science. [[00:43:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2624.74s)]
*  Yes. [[00:43:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2627.38s)]
*  Okay. Thank you. [[00:43:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2629.7s)]
*  Thank you. I'm sorry to ask you to pause. [[00:43:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2630.5s)]
*  Yeah, that's not even the main crux of my argument. [[00:43:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2631.86s)]
*  There is a philosopher slash mathematician slash scientist. [[00:43:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2633.94s)]
*  Wonderful. His name is Sir Roger Penrose. [[00:43:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2639.62s)]
*  I love how the British kind of give the Sir title. [[00:44:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2643.86s)]
*  Someone has accomplished. [[00:44:06](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2646.42s)]
*  He wrote this book called The Emperor's New Mind. [[00:44:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2647.78s)]
*  And it's based on the emperor's new clothes, [[00:44:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2654.1s)]
*  the idea that the emperor is kind of naked. [[00:44:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2658.6600000000003s)]
*  And in his opinion, the argument that the mind is a computer [[00:44:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2661.3s)]
*  is a sort of consensus argument that is wrong. [[00:44:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2667.1400000000003s)]
*  The emperor is naked. [[00:44:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2670.1s)]
*  It's not really an argument. It's an assertion. [[00:44:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2670.98s)]
*  Yes, it's an assertion that is fundamentally wrong. [[00:44:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2672.9s)]
*  And the way he proves it is very interesting. [[00:44:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2675.78s)]
*  In mathematics, there's something called [[00:44:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2680.1000000000004s)]
*  Gödel's incompleteness theorem. [[00:44:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2682.5800000000004s)]
*  And what that says is there are statements [[00:44:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2685.46s)]
*  that are true that can't be proved in mathematics. [[00:44:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2692.7400000000002s)]
*  So he constructs, Gödel constructs like a number system [[00:44:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2696.6600000000003s)]
*  where he can start to make statements about this number system. [[00:45:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2701.46s)]
*  So he creates a statement that's like, [[00:45:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2704.58s)]
*  this statement is unprovable in system F, [[00:45:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2708.02s)]
*  where the whole system is F. [[00:45:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2710.66s)]
*  Well, if you try to prove it, then that statement becomes false. [[00:45:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2713.86s)]
*  But you know it's true because it's unprovable in the system. [[00:45:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2720.42s)]
*  And Roger Penrose says, because we have this knowledge [[00:45:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2724.1s)]
*  that it is true by looking at it, despite like we can't prove it. [[00:45:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2728.18s)]
*  I mean, the whole feature of the sentence is that it is unprovable. [[00:45:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2732.74s)]
*  Therefore, our knowledge is outside of any formal system. [[00:45:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2738.5s)]
*  Therefore, the human brain is, or like our mind is understanding something [[00:45:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2743.14s)]
*  that mathematics is not able to give it to us. [[00:45:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2750.02s)]
*  To describe. [[00:45:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2753.38s)]
*  To describe. [[00:45:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2754.1s)]
*  And I thought the first time I read it, you know, [[00:45:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2756.02s)]
*  it read a lot of these things. [[00:45:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2758.18s)]
*  What's the famous, you were telling me last night, [[00:46:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2760.74s)]
*  I'd never heard it, the Bertrand Russell self-cancelling assertion? [[00:46:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2762.42s)]
*  Yeah, it's like this statement is false. [[00:46:06](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2766.5s)]
*  It's called the liar paradox. [[00:46:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2769.2200000000003s)]
*  Explain why that's just, that's going to float in my head forever. [[00:46:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2772.7400000000002s)]
*  Why is that a paradox? [[00:46:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2775.54s)]
*  So this statement is false. [[00:46:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2776.5s)]
*  If you look at a statement and agree with it, then it becomes true. [[00:46:19](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2779.86s)]
*  But if it's true, then it's not true. [[00:46:23](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2783.06s)]
*  It's false. [[00:46:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2785.14s)]
*  And you go through the circular thing and you never stop. [[00:46:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2785.78s)]
*  It broke the logic in a way. [[00:46:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2789.14s)]
*  Yes. [[00:46:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2791.78s)]
*  And Bertrand Russell spent his whole, you know, [[00:46:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2792.5s)]
*  big part of his life writing this book, Principia Mathematica. [[00:46:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2795.3s)]
*  And he wanted to really prove that mathematics is complete, consistent, [[00:46:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2800.5s)]
*  decidable, computable, all of that. [[00:46:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2807.46s)]
*  And then all these things happen, Gödel's Uncompleted Theorem, [[00:46:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2810.5800000000004s)]
*  Turing, the inventor of the computer, actually, [[00:46:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2814.5s)]
*  this is the most ironic piece of science history that nobody ever talks about. [[00:46:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2817.2200000000003s)]
*  But Turing invented the computer to show its limitation. [[00:47:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2821.38s)]
*  So he invented the Turing machine, which is the ideal representation [[00:47:06](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2826.1s)]
*  of a computer that we have today. [[00:47:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2829.86s)]
*  All computers are Turing machines. [[00:47:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2831.62s)]
*  And he showed that this machine, if you give it a set of instructions, [[00:47:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2834.1800000000003s)]
*  it can't tell whether those set of instructions will ever stop, [[00:47:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2841.54s)]
*  will run and stop, will complete to a stop, or will continue running forever. [[00:47:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2845.46s)]
*  It's called the halting problem. [[00:47:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2850.02s)]
*  And this proves that mathematics have undecidability. [[00:47:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2851.86s)]
*  It's not fully decidable or computable. [[00:47:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2856.34s)]
*  So all of these things were happening as he was writing the book. [[00:47:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2858.98s)]
*  And, you know, it was really depressing for him [[00:47:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2862.58s)]
*  because he kind of went out to prove that, you know, [[00:47:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2866.58s)]
*  mathematics is complete and all of that. [[00:47:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2869.54s)]
*  And, you know, this caused kind of a major panic [[00:47:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2873.62s)]
*  at the time between mathematicians and all of that. [[00:47:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2877.46s)]
*  It's like, oh, my God, like our systems are not complete. [[00:48:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2880.02s)]
*  So it sounds like the deeper you go into science [[00:48:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2884.02s)]
*  and the more honest you are about what you discover, [[00:48:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2887.3s)]
*  the more questions you have, [[00:48:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2890.18s)]
*  which kind of gets you back to where you should be in the first place, [[00:48:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2891.7s)]
*  which is in a posture of humility. [[00:48:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2894.7400000000002s)]
*  Yes. [[00:48:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2896.7400000000002s)]
*  And yet I see science used certainly in the political sphere. [[00:48:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2897.54s)]
*  I mean, those are all dumb people. [[00:48:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2901.06s)]
*  So it's like, who cares actually? [[00:48:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2902.26s)]
*  Kamala Harris lectured me about science. [[00:48:23](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2903.54s)]
*  I don't even hear it. [[00:48:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2904.9s)]
*  But so also some smart people like believe the science. [[00:48:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2905.78s)]
*  The assumption behind that demand is that it's complete and it's knowable. [[00:48:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2908.9s)]
*  And we know it. [[00:48:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2914.02s)]
*  And if you're ignoring it, then you're ignorant willfully or otherwise. [[00:48:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2914.82s)]
*  Right? [[00:48:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2918.5s)]
*  Well, my view of science is a method. [[00:48:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2919.0600000000004s)]
*  Ultimately, it's a method. [[00:48:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2921.46s)]
*  Anyone can apply it. [[00:48:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2922.5s)]
*  It's democratic. [[00:48:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2923.3s)]
*  It's decentralized. [[00:48:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2924.42s)]
*  Anyone can apply the scientific method, [[00:48:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2925.46s)]
*  including people who are not trained. [[00:48:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2927.5400000000004s)]
*  But in order to practice the method, [[00:48:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2929.0600000000004s)]
*  you have to come from a position of humility that I don't know. [[00:48:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2930.5800000000004s)]
*  That's right. [[00:48:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2933.94s)]
*  I'm using this method to find out. [[00:48:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2935.3s)]
*  And I cannot lie about what I observe, right? [[00:48:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2937.22s)]
*  That's right. [[00:48:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2939.46s)]
*  And today, the capital S science is used to control [[00:49:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2940.02s)]
*  and it's used to propagandize and lie. [[00:49:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2945.06s)]
*  Of course. [[00:49:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2949.7s)]
*  But in the hands of just really people who shouldn't have power, [[00:49:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2950.18s)]
*  just dumb people with pretty ugly agendas. [[00:49:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2955.22s)]
*  But we're talking about the world that you live in, [[00:49:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2958.5s)]
*  which is unusually smart people who do this stuff for a living [[00:49:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2960.98s)]
*  and are really trying to advance the ball in science. [[00:49:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2964.58s)]
*  And I think what you're saying is that some of them, knowingly or not, [[00:49:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2967.38s)]
*  just don't appreciate how little they know. [[00:49:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2972.1s)]
*  Yeah. [[00:49:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2974.9s)]
*  And they go through this chain of reasoning for this argument. [[00:49:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2975.22s)]
*  And none of those are at minimum complete. [[00:49:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2978.98s)]
*  And just take it for granted. [[00:49:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2986.98s)]
*  If you even doubt that the mind is a computer, [[00:49:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2989.86s)]
*  I'm sure a lot of people will call me heretic [[00:49:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2993.78s)]
*  and will call me all sorts of names because it's just dogma. [[00:49:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=2995.7000000000003s)]
*  That the mind is a computer? [[00:50:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3000.7400000000002s)]
*  That the mind is a computer is dogma in technology, science, all that. [[00:50:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3002.26s)]
*  That's so silly. [[00:50:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3005.78s)]
*  Yes. [[00:50:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3007.86s)]
*  Well, let me count the ways the mind is different from a computer. [[00:50:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3009.6200000000003s)]
*  First of all, you're not assured of a faithful representation of the past. [[00:50:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3012.98s)]
*  Memories change over time in a way that's misleading and who knows why, [[00:50:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3016.82s)]
*  but that is a fact. [[00:50:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3021.46s)]
*  That's not true of computers, I don't think. [[00:50:23](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3023.38s)]
*  But how are we explaining things like intuition and instinct? [[00:50:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3027.06s)]
*  Those are not... [[00:50:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3032.6600000000003s)]
*  Well, that is actually my question. [[00:50:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3033.78s)]
*  Could those ever be features of a machine? [[00:50:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3035.38s)]
*  You could argue that neural networks are sort of intuition machines, [[00:50:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3038.1000000000004s)]
*  and that's what a lot of people say. [[00:50:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3043.1400000000003s)]
*  But neural networks, and maybe I will describe them just for the audience, [[00:50:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3045.38s)]
*  neural networks are inspired by the brain. [[00:50:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3052.66s)]
*  And the idea is that you can connect a network of small little functions, [[00:50:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3056.66s)]
*  just mathematical functions, and you can train it by giving examples. [[00:51:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3062.5s)]
*  I could give it a picture of a cat. [[00:51:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3067.14s)]
*  And if it's yes, let's say this network has to say yes, [[00:51:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3070.1800000000003s)]
*  if it's a cat, no, if it's not a cat. [[00:51:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3073.78s)]
*  To give it a picture of a cat and then the answer is no, then it's wrong. [[00:51:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3077.1400000000003s)]
*  You adjust the weights based on the difference between the picture and the answer. [[00:51:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3081.2200000000003s)]
*  And you do this, I don't know, a billion times. [[00:51:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3086.26s)]
*  And then the network encodes features about the cat. [[00:51:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3089.3s)]
*  And this is literally exactly how neural networks work. [[00:51:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3095.38s)]
*  You tune all these small parameters until there is some embedded [[00:51:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3099.7000000000003s)]
*  feature detection, especially in classifiers. [[00:51:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3105.94s)]
*  This is not intuition. [[00:51:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3112.5800000000004s)]
*  This is basically automatic programming, the way I see it. [[00:51:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3113.78s)]
*  We can write code manually. [[00:52:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3120.26s)]
*  You can go to our website, write code. [[00:52:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3123.1400000000003s)]
*  But we can generate algorithms automatically via machine learning. [[00:52:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3124.66s)]
*  Machine learning essentially discovers these algorithms. [[00:52:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3131.46s)]
*  And sometimes the discoverers like very crappy algorithms. [[00:52:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3134.5s)]
*  For example, like, you know, all the pictures that we gave it of a cat had grass in them. [[00:52:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3138.02s)]
*  So it would learn that grass equals cat, the color green equals cat. [[00:52:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3145.14s)]
*  Yes. [[00:52:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3151.06s)]
*  The color green equals cat. [[00:52:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3151.78s)]
*  And then you give it one day a picture of a cat without grass and it fails. [[00:52:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3153.86s)]
*  And they're like, what happened? [[00:52:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3158.26s)]
*  All turns out it learned the wrong thing. [[00:52:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3159.5400000000004s)]
*  So because it's obscure what it's actually learning, people interpret that as intuition. [[00:52:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3162.7400000000002s)]
*  Because it's not, the algorithms are not explicated. [[00:52:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3170.1000000000004s)]
*  And there's a lot of work now on trying to explicate these algorithms, [[00:52:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3175.2200000000003s)]
*  which is great work for companies like Anthropic. [[00:52:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3178.42s)]
*  But, you know, I don't think you can call it intuition just because it's obscure. [[00:53:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3181.78s)]
*  So what is it? [[00:53:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3190.82s)]
*  How is intuition different? [[00:53:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3191.46s)]
*  Human intuition. [[00:53:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3193.38s)]
*  We don't, you know, for one, we don't require a trillion examples of cats to learn a cat. [[00:53:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3198.1000000000004s)]
*  Good point. [[00:53:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3204.34s)]
*  You know, a kid can learn language with very little examples. [[00:53:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3206.02s)]
*  Right now, when we're training these large language models like chat, [[00:53:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3212.9s)]
*  GPT, you have to give it the entire internet for it to learn language. [[00:53:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3215.62s)]
*  And that's not really how humans work. [[00:53:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3219.7799999999997s)]
*  And the way we learn is like we combine intuition and some more explicit way of learning. [[00:53:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3221.7799999999997s)]
*  And I don't think we've figured out how to do it with machines just yet. [[00:53:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3231.2999999999997s)]
*  Do you think that structurally it's possible for machines to get there? [[00:53:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3236.02s)]
*  So, you know, this chain of reasoning, [[00:54:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3242.98s)]
*  I can go through every point and present arguments to the contrary, [[00:54:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3250.82s)]
*  or at least like present doubt. [[00:54:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3254.1s)]
*  But no one is really kind of trying to deal with those doubts. [[00:54:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3255.38s)]
*  And my view is that I'm not holding these doubts, you know, very, very strongly. [[00:54:19](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3259.54s)]
*  But my view is that we just don't have a complete understanding of the mind. [[00:54:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3266.1s)]
*  And you can't, you at least can't use it to argue that kind of machine that acts like a human, [[00:54:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3269.7s)]
*  but much more powerful, it can kill us all. [[00:54:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3277.46s)]
*  But, you know, do I think that machine, you know, [[00:54:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3280.98s)]
*  I think AI can get really powerful. [[00:54:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3285.06s)]
*  Yes. [[00:54:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3287.78s)]
*  I think AI can get really powerful, can get really useful. [[00:54:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3288.98s)]
*  I think functionally can feel like it's general. [[00:54:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3291.46s)]
*  AI is ultimately a function of data. [[00:54:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3294.58s)]
*  The kind of data that we put into it, it's the functionality is based on this data. [[00:54:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3297.62s)]
*  So we can get very little functionality outside of that. [[00:55:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3302.26s)]
*  Actually, we don't get any functionality outside of that data. [[00:55:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3305.54s)]
*  It's actually been proven that these machines are just like, you know, [[00:55:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3307.7000000000003s)]
*  the function of their data. [[00:55:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3311.86s)]
*  There's some total of what you put in. [[00:55:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3315.94s)]
*  Exactly. [[00:55:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3317.62s)]
*  Garbage in, garbage out. [[00:55:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3318.34s)]
*  The cool thing about them is they can mix and match different [[00:55:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3320.7400000000002s)]
*  functionalities that they learn from the data. [[00:55:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3324.26s)]
*  So it looks a little bit more general. [[00:55:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3326.02s)]
*  But let's say we collected all data of the world, we collected everything that we care about. [[00:55:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3328.58s)]
*  And we somehow fit it into a machine. [[00:55:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3333.06s)]
*  And now everyone's building these really large data centers. [[00:55:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3334.82s)]
*  You will get a very highly capable machine that will kind of look general. [[00:55:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3338.18s)]
*  Because we collected a lot of economically useful data, [[00:55:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3345.9399999999996s)]
*  and we'll start doing economically useful tasks. [[00:55:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3350.58s)]
*  And from our perspective, it will start to look general. [[00:55:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3354.3399999999997s)]
*  So I'll call it functionally AGI. [[00:55:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3357.7s)]
*  I don't doubt we're sort of headed in some direction like that. [[00:56:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3360.1s)]
*  But we haven't figured out how these machines can actually generalize, [[00:56:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3364.02s)]
*  and can learn, and can use things like intuition for when they see something fundamentally new [[00:56:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3368.98s)]
*  outside of their data distribution, they can actually react to it [[00:56:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3374.34s)]
*  correctly and learn it efficiently. [[00:56:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3378.98s)]
*  We don't have the science for that. [[00:56:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3381.14s)]
*  Because we don't have the understanding of it on the most fundamental level. [[00:56:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3384.02s)]
*  You began that explanation by saying we don't really understand the human brain. [[00:56:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3386.98s)]
*  So how can we compare it to something because we don't even really know what it is. [[00:56:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3390.2599999999998s)]
*  And there's a machine learning scientist, Francois Choulet, [[00:56:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3392.98s)]
*  I don't know how to pronounce French names, but I think that's his name. [[00:56:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3397.94s)]
*  He took sort of an IQ like test, where you're rotating shapes and whatever. [[00:56:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3403.14s)]
*  An entrepreneur put a million dollars for anyone who's able to solve it using AI. [[00:56:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3409.54s)]
*  And all the modern AIs that we think are super powerful [[00:56:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3415.46s)]
*  couldn't do something that like a 10-year-old kid could do. [[00:56:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3419.14s)]
*  And it showed that again, those machines are just functions of their data. [[00:57:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3422.98s)]
*  The moment you throw a problem that's novel at them, they really are not able to do it. [[00:57:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3427.62s)]
*  Now again, I'm not fundamentally discounting the fact that we'll get there, [[00:57:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3432.7400000000002s)]
*  but just the reality of where we are today, you can't argue that we're just going to put [[00:57:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3437.86s)]
*  more compute and more data into this, and suddenly it becomes God and kills us all. [[00:57:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3442.9s)]
*  Because that's the argument, and they're going to DC, and they're going to all these places, [[00:57:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3447.7s)]
*  that are springing up regulation. This regulation is going to hurt American industry. [[00:57:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3451.3s)]
*  It's going to hurt startups. It's going to make it hard to compete. [[00:57:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3456.6600000000003s)]
*  It's going to give China a tremendous advantage. [[00:57:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3459.2200000000003s)]
*  It's going to really hurt us based on these flawed arguments that [[00:57:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3463.86s)]
*  they're not actually battling with these real questions. [[00:57:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3467.78s)]
*  It sounds like they're not, and what gives me pause is not so much the technology, [[00:57:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3471.2200000000003s)]
*  it's the way that the people creating the technology understand people. [[00:57:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3475.6200000000003s)]
*  So I think the wise and correct way to understand people is as not self-created beings. [[00:57:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3479.54s)]
*  People did not create themselves. People cannot create life as beings created by some higher power [[00:58:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3485.22s)]
*  who at their core have some kind of impossible to describe spark, a holy mystery. [[00:58:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3492.8199999999997s)]
*  And for that reason, they cannot be enslaved or killed by other human beings. That's wrong. [[00:58:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3498.98s)]
*  There is right and wrong. That is wrong. We know lots of gray areas. That's not a gray area. [[00:58:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3504.66s)]
*  Because they're not self-created. I think that all humane action flows from that belief [[00:58:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3508.9s)]
*  and that the most inhumane actions in history flow from the opposite belief, which is people [[00:58:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3516.8199999999997s)]
*  are just objects that can and should be improved and I have full power over them. [[00:58:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3521.86s)]
*  Like that's a totalitarian mindset and it's the one thing that connects [[00:58:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3526.98s)]
*  every genocidal movement is that belief. So it seems to me as an outsider that the people [[00:58:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3531.14s)]
*  creating this technology have that belief. [[00:58:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3535.38s)]
*  And you don't even have to be spiritual to have that belief. [[00:58:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3537.9399999999996s)]
*  You certainly don't. I think that's actually a rational conclusion based on... [[00:59:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3543.46s)]
*  I 100% agree. I'll give you one interesting anecdote again from science. [[00:59:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3547.46s)]
*  We've had brains for half a billion, if you believe in evolution, all that. [[00:59:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3552.1s)]
*  We have had brains for half a billion years. [[00:59:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3555.06s)]
*  Right. And we've had kind of a human-like species for half a million years, perhaps more, [[00:59:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3557.86s)]
*  perhaps a million years. There's a moment in time, 40,000 years ago, it's called the Great [[00:59:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3570.1800000000003s)]
*  Leap Forward, where we see culture, we see religion, we see drawings, we saw very little [[00:59:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3578.42s)]
*  of that before that, tools and whatever. And suddenly we're seeing this Cambrian explosion [[00:59:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3586.5s)]
*  of culture. Right. [[00:59:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3592.58s)]
*  And... We're going into something larger than just like daily needs or the world around them. [[00:59:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3594.98s)]
*  But yeah, and it's not... We're still not able to explain it. David Reich wrote this book, [[00:59:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3599.14s)]
*  it's called, I think, Who We Are, Where We Came From. He talks about trying to look for that [[01:00:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3605.94s)]
*  genetic mutation that happened, that potentially created this explosion. And they have some idea [[01:00:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3611.94s)]
*  of what it could be in some candidates, but they don't really have it right now. But you have to [[01:00:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3618.34s)]
*  ask the question, like, what happened 30 or 40,000 years ago? Right? Where it's clear, I mean, [[01:00:23](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3623.7000000000003s)]
*  it's indisputable that the people who lived during that period were suddenly grappling [[01:00:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3630.5s)]
*  with metaphysics. Yes. [[01:00:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3635.2200000000003s)]
*  They're worshiping things. There's a clear separation between... [[01:00:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3637.3s)]
*  Between, again, the animal brain and the human brain. And it's clearly not computation. Like, [[01:00:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3641.0600000000004s)]
*  suddenly then like grow a computer brain. It's something else happened. [[01:00:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3647.94s)]
*  But what's so interesting is like the instinct of modern man is to look for something inside [[01:00:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3653.3s)]
*  the person that caused that. Whereas I think the very natural and more correct instinct is [[01:00:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3657.86s)]
*  to look for something outside of man that caused that. I'm open to both. [[01:01:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3662.1800000000003s)]
*  Yeah. I mean, I don't know the answer. I mean, of course I do know the answer, but... [[01:01:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3665.86s)]
*  But I'll just pretend I don't. But at very least both are possible. So if like you can find [[01:01:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3670.7400000000002s)]
*  yourself to looking for a genetic mutation or change, genetic change, then you're sort of [[01:01:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3676.1000000000004s)]
*  closing out... That's not an empiricist, a scientific way of looking at things, actually. [[01:01:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3682.34s)]
*  You don't foreclose any possibility, right? Yes. [[01:01:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3686.9s)]
*  Science? You can't. Right. [[01:01:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3688.6600000000003s)]
*  Interesting. Yeah. That's very interesting. So, you know, I think that these machines, [[01:01:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3691.2200000000003s)]
*  I'm betting my business that on AI getting better and better and better, and it's gonna [[01:01:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3698.42s)]
*  make us all better. It's going to make it all more educated. [[01:01:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3704.34s)]
*  Okay. So, okay. Now, now's the time for you to tell me why I should be excited [[01:01:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3709.54s)]
*  about something I've been hearing. Sure. Yeah. So, [[01:01:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3714.98s)]
*  um, uh, uh, this technology, large language models where we kind of fed, uh, uh, a neural [[01:01:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3718.1s)]
*  network, the entire internet, and it has capabilities mostly around writing, around, [[01:02:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3727.38s)]
*  uh, information lookup, around, uh, summarization, around coding. Uh, it does a lot of really useful [[01:02:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3734.9s)]
*  thing and you can program it to kind of pick and match between these different skills. You can [[01:02:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3741.54s)]
*  program these skills using code. Um, and so the kind of products and services that you can build [[01:02:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3746.18s)]
*  with this, um, are amazing. So one of the things I'm most excited about this application of the [[01:02:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3752.3399999999997s)]
*  technology, um, there's this problem called the Bloom's two sigma problem. Uh, there's this, uh, [[01:02:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3758.74s)]
*  you know, uh, scientists that was studying education and, um, he was looking at different [[01:02:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3766.3399999999997s)]
*  interventions to try to get, you know, kids to learn better or faster or have just better [[01:02:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3773.3s)]
*  educational outcomes. And he found something kind of bad, which is there's only one thing you could [[01:02:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3779.0600000000004s)]
*  do to move kids, not in a marginal way, but in a two standard deviations from the norm, like in a [[01:03:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3787.78s)]
*  big way, like better than 98% of the other kids, um, by doing one-on-one tutoring using a type of [[01:03:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3795.86s)]
*  learning called mastery learning, uh, one-on-one touring is the, is the key formula there. [[01:03:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3805.06s)]
*  That's great. I mean, we discovered the solution to education. We can up level everyone, all humans [[01:03:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3811.38s)]
*  on earth. The problem is like, we don't have enough teachers to do one-on-one touring. It's very [[01:03:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3817.6200000000003s)]
*  expensive. You know, no country in the world can afford that. So now we have these machines that [[01:03:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3823.78s)]
*  can talk, they can teach, that can, um, they can present information, uh, that you can interact [[01:03:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3830.42s)]
*  with it in a very human way. You can talk to it. It can talk to you back, right? And we can build, [[01:03:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3838.1000000000004s)]
*  um, AI applications to teach people one-on-one and you can teach, you can have it, you can serve [[01:04:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3843.3s)]
*  7 billion people with that. And we can, everyone can get smarter. I'm totally for that. I mean, [[01:04:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3852.9s)]
*  that was the promise of the internet didn't happen. So I hope this, um, I'm was going to save this for [[01:04:19](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3859.54s)]
*  last, but I can't control myself. So I just know being from DC that when, uh, the people in charge [[01:04:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3866.6600000000003s)]
*  see new technology, the first thing they think of is like, how can I use this to kill people? [[01:04:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3872.6600000000003s)]
*  Um, so what are the military applications potentially of this technology? You know, [[01:04:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3877.78s)]
*  that's one of the other things that I, I, I'm sort of very skeptical of this lobbying effort [[01:04:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3882.34s)]
*  to get government to, um, to regulate it because like, I think the biggest offender would be of [[01:04:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3887.06s)]
*  abuse of this technology, probably government. You think, you know, I watched your interview with, [[01:04:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3892.7400000000002s)]
*  uh, uh, Jeffrey Sachs, um, who's like a Columbia professor, very, very mainstream. Um, and, uh, [[01:04:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3898.02s)]
*  I think he got assigned to like a, uh, uh, Lancet, um, uh, sort of study of COVID origins or whatever. [[01:05:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3907.06s)]
*  And he, uh, he arrived at very, at the time, heterodox view that it was created in a lab and [[01:05:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3913.38s)]
*  it was created by the U S government. Um, and, and, and, and so, you know, the government is [[01:05:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3918.02s)]
*  supposed to protect us from these things. And now they're talking about pandemic readiness and [[01:05:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3925.2999999999997s)]
*  whatever. Well, well, let's talk about, let's talk about, uh, how do we watch what the government's [[01:05:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3929.46s)]
*  doing? How do we actually have democratic processes to ensure that you're, you're not [[01:05:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3935.22s)]
*  the one abusing these technologies because they're going to regulate it. They're going to make it so [[01:05:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3941.54s)]
*  that everyday people are not going to be able to use these things. And then they're going to have [[01:05:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3945.2999999999997s)]
*  free reign on how to, you know, how to abuse these things. Just like with encryption, right? [[01:05:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3949.06s)]
*  Concription is another one. That's right. But they've been doing that for decades. [[01:05:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3953.62s)]
*  Yes. Like we get privacy, but you're not allowed it because we don't trust you. [[01:05:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3957.62s)]
*  But by using your money and the moral authority that you gave us to lead you, [[01:06:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3961.46s)]
*  we're going to hide from you everything we're doing. And there's nothing you can do about it. [[01:06:06](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3966.9s)]
*  I mean, that's the state of America right now. So how would they use AI to further repress us? [[01:06:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3970.34s)]
*  I mean, you can use it in all sorts of ways, like, uh, autonomous drones. We already have [[01:06:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3976.98s)]
*  autonomous drones. They get, it got a lot, a lot worse. You can, uh, you know, there's a video on [[01:06:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3981.3s)]
*  the internet where like the, you know, uh, Chinese guard or whatever was walking with a dog, with a [[01:06:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3986.98s)]
*  robotic dog and the robotic dog had a gun mounted to it. Uh, and so you can have robotic sort of [[01:06:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3992.98s)]
*  dogs with shooting guns, a little sci-fi, like you can be. It's a dog lover. That's so offensive [[01:06:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=3999.54s)]
*  to me. It is kind of offensive. Yeah. In a world increasingly defined by deception and the total [[01:06:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4006.1s)]
*  rejection of human dignity, we decided to found the Tucker Carlson network and we did it with [[01:06:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4012.42s)]
*  one principle in mind. Tell the truth. You have a God given right to think for yourself. [[01:06:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4017.2200000000003s)]
*  Our work is made possible by our members. So if you want to enjoy an ad free experience and keep [[01:07:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4024.26s)]
*  this going, join TCN at tuckercarlson.com slash podcast, tuckercarlson.com slash podcast. [[01:07:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4030.1s)]
*  There was this huge expose in this magazine called nine seven two about how Israel was using, [[01:07:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4037.7000000000003s)]
*  um, AI to target, uh, suspects, but ended up killing huge numbers of civilians. It's called [[01:07:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4042.18s)]
*  the lavender, a very interesting piece. Um, so the, the technology wound up killing people who were [[01:07:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4049.94s)]
*  not even targeted. Yes. It's pretty dark. Um, what about surveillance? Um, [[01:07:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4058.5s)]
*  I think this recent, uh, AI, um, boom, I think it could be used for surveillance. I, I, I'm not sure [[01:07:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4066.34s)]
*  if it gives a special advantage. I think the, they can get the advantage by again, if these lobbying [[01:07:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4073.2200000000003s)]
*  groups are successful, part of their, you know, their, their ideal outcome is to, is to make sure [[01:08:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4080.9s)]
*  that no one is training, uh, large language models. And to do that, you would need to have a [[01:08:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4088.7400000000002s)]
*  large language models and to do that, you would need to insert, uh, surveillance apparatus at the [[01:08:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4094.34s)]
*  compute level. And, um, and so perhaps that's very dangerous. Our computers would like spy on us to [[01:08:19](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4099.62s)]
*  make sure we're not training AIs. Um, I think, you know, the kind of AI that's really good at [[01:08:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4105.9400000000005s)]
*  surveillance is kind of the vision AI with China sort of perfected. So that's been around for a [[01:08:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4112.74s)]
*  while now. I, you know, I'm sure there's ways to abuse language models for, for surveillance, [[01:08:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4117.7s)]
*  but I can't think of it right now. What about manufacturing? Um, it would help with manufacturing [[01:08:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4124.26s)]
*  right now. People are figuring out how to do, um, uh, I invested in a couple of companies, they, uh, [[01:08:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4130.82s)]
*  how to apply this technology foundation models to robotics. Um, it's still early science, but you [[01:08:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4136.82s)]
*  might have a huge advancement and robotics, uh, if we're able to apply this technology to it. [[01:09:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4144.18s)]
*  So the whole point of technology is to replace human labor, either physical or mental, I think. [[01:09:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4150.900000000001s)]
*  I mean, historically that's what, you know, the steam engine replaced the arm, et cetera, et cetera. [[01:09:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4156.1s)]
*  So if this is as transformative as it appears to be, you're going to have a lot of idle people. [[01:09:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4161.780000000001s)]
*  And that's, I think the concern that led a lot of your friends and colleagues to support UBI. [[01:09:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4169.54s)]
*  Universal basic income. Like there's nothing for these people to do. So we just got to pay them to [[01:09:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4174.18s)]
*  exist. You said you're opposed to that. I'm adamantly opposed to that. On the other hand, [[01:09:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4178.42s)]
*  like, what's the answer? Yeah. So, you know, uh, there's, there's two ways to look at it. [[01:09:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4184.42s)]
*  We can look at the individuals that are losing the jobs, which is tough and hard. I don't really [[01:09:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4189.860000000001s)]
*  have a good answer, but we can look at it from a macro perspective. And when you look at it from [[01:09:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4194.34s)]
*  that perspective, for the most part, technology created more jobs over time. You know, uh, you [[01:09:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4199.86s)]
*  know, before alarm clocks, we had this job called the knocker oper, which goes to your room. You [[01:10:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4205.78s)]
*  kind of pay him. It was like, come every day at like five a.m. They knock on your, or ring the [[01:10:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4212.42s)]
*  village bell. Right. Yeah. And you know, that job disappeared, but like we, we had, you know, 10, [[01:10:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4216.58s)]
*  10 times more jobs in manufacturing or perhaps, you know, a hundred or a thousand more jobs in [[01:10:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4222.66s)]
*  manufacturing. And so overall, I think the general trend is technology just creates more jobs. And [[01:10:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4228.5s)]
*  so like, I'll give you a few examples how AI can create more jobs. Actually can create more [[01:10:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4236.5s)]
*  interesting jobs. Um, entrepreneurship, it's like a very American thing, right? It's, it's like [[01:10:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4241.78s)]
*  America is the entrepreneurship country, but actually new firm creation has been going down [[01:10:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4248.42s)]
*  for a long time, at least a hundred years. It's just like been going down. Although we have all [[01:10:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4253.62s)]
*  this excitement around startups or whatever, uh, Silicon Valley is the only place that's still [[01:10:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4257.54s)]
*  producing startups. Like the rest of the country, there isn't as much startup or new firm creation, [[01:11:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4262.74s)]
*  which is kind of sad because again, the internet was supposed to be this, you know, great wealth [[01:11:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4269.3s)]
*  creation engine that anyone has access to. But the way it turned out is like it was concentrated [[01:11:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4273.38s)]
*  in this one geographic area. Well, it looks, I mean, in retrospect, it looks like a monopoly [[01:11:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4278.18s)]
*  generator actually. Yeah. But again, it doesn't have to be that way. And the way I think AI would [[01:11:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4282.5s)]
*  help is that it will give people the tools to start businesses because you have this easily [[01:11:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4289.14s)]
*  programmable machine that can help you with programming. I'll give you a few examples. We, [[01:11:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4295.46s)]
*  there's a teacher in Denver that, you know, during COVID was, was, was a little bored, [[01:11:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4300.42s)]
*  went to our website. We have a free course to learn how to code. And, uh, he learned a bit of coding [[01:11:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4305.06s)]
*  and, uh, he used his knowledge as a teacher to build an application that helps teachers use AI [[01:11:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4310.74s)]
*  to teach. And within a year, he built a business that's worth tens of millions of dollars. That's [[01:11:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4317.860000000001s)]
*  bringing in a huge amount of money. I think he raised $20 million. Uh, and that that's a teacher [[01:12:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4324.58s)]
*  who learned how to code and created this massive business really quickly. We have, we have stories [[01:12:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4330.9800000000005s)]
*  of photographers doing millions of dollars in revenue. Uh, so it just, it's a, it's a, you know, [[01:12:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4336.5s)]
*  AI will decentralize access to this technology. So there's a lot of ways in which you're right, [[01:12:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4344.02s)]
*  technology tend to centralize, but there's a lot of ways that people kind of don't really look at [[01:12:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4349.46s)]
*  in which technology can decentralize. Well, that was, I mean, that promise makes sense to me. I, [[01:12:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4354.26s)]
*  I would just, I firmly want it to become a reality. I have a, we have a mutual friend who [[01:12:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4358.66s)]
*  showed me Nameless, so smart and a good humane person, um, who was very way up into the subject [[01:12:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4362.5s)]
*  and participates in the subject. And he said to me, well, one of the promises of AI is that it will [[01:12:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4370.5s)]
*  allow people to have virtual friends or mates that it will fill, you know, it will solve the loneliness [[01:12:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4377.62s)]
*  problem. That is clearly a massive problem in the United States. And I felt like I don't want to say [[01:13:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4385.7s)]
*  it because I like him so much, but that seemed really bad to me. Yeah. I'm not interested in [[01:13:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4390.42s)]
*  those. I think we have the same intuition about, about, you know, what's, what's dark and dystopian [[01:13:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4394.9800000000005s)]
*  versus what's cool. He's a wonderful person, but I may, I just don't think he's thought about it or [[01:13:23](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4403.06s)]
*  I don't know what, but we disagree. But why just, I don't even disagree. I don't have an argument, [[01:13:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4407.38s)]
*  just an instinct, but like people should be having sex with people, not machines. Right. That's right. [[01:13:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4411.38s)]
*  Um, like I would go so far as to say some of these applications are like a little unethical, [[01:13:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4417.22s)]
*  like the, you know, praying on sort of lonely, lonely men with no, uh, with no, [[01:13:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4423.22s)]
*  with no opportunities for, for a mate. And, uh, like, you know, it will make it so that they were [[01:13:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4429.3s)]
*  actually not motivated to go out and date and get an extra girlfriend. Like porn 10 X. Yes. Yes. And [[01:13:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4434.9s)]
*  I think that's really bad. That's really bad for society. Uh, and so I think the application, look, [[01:14:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4441.54s)]
*  you can apply, apply this technology in a positive way, or you can apply them negative way. [[01:14:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4445.86s)]
*  You know, I would love for this, you know, doom cult if instead they were like trying to, you [[01:14:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4450.179999999999s)]
*  know, make it so that AI is applied in a positive way. If we had a cult that was like, oh, we're [[01:14:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4456.9s)]
*  going to lobby, we're going to, uh, sort of go out and, uh, you know, uh, make it, make it so that, [[01:14:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4461.54s)]
*  um, you know, AI is a positive technology. I'd be all for that. And by the way, there are in [[01:14:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4469.94s)]
*  history, there are times where the culture self corrects, right? I think there's some [[01:14:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4475.38s)]
*  self correction on porn that's happening right now. Um, you know, uh, fast food, right? I mean, [[01:14:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4480.9800000000005s)]
*  if, uh, you know, just generally junk, uh, you know, everyone is like, whole foods is like high [[01:14:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4486.82s)]
*  status now. Like you, you, you, you know, there's a place called, oh, you can go to, and people are [[01:14:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4492.42s)]
*  interested in needing healthy and chemicals in the air and water. Another thing that was a very [[01:14:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4498.02s)]
*  esoteric concern even 10 years ago was only the wackos. It was Bobby Kennedy cared about that. [[01:15:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4502.42s)]
*  No one else did. And now that's like a feature of normal conversation. Yes. Everyone's worried about [[01:15:06](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4506.42s)]
*  microplastics and the testicles. That's right. Yeah. Which is, I think a legitimate concern. [[01:15:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4511.9400000000005s)]
*  Absolutely. So what I'm not surprised that there are cults in Silicon Valley. I don't think you [[01:15:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4515.860000000001s)]
*  named the only one. I think there are others. That's my sense. And I'm not surprised because [[01:15:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4520.18s)]
*  of course, every person is born with the intuitive knowledge that there's a power beyond himself. [[01:15:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4524.900000000001s)]
*  That's why every single civilization has worshiped something. And if you don't acknowledge that, [[01:15:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4529.9400000000005s)]
*  you just, it doesn't change. You just worship something even dumber. Yeah. But so my question [[01:15:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4534.66s)]
*  to you as someone who lives and works there is what percentage of the people were making decisions [[01:15:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4538.66s)]
*  in Silicon Valley will say out loud, you know, not, I'm a Christian Jew or Muslim, but that like, [[01:15:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4542.5s)]
*  I'm not, you know, there is a power bigger than me in the universe. Do people think that, [[01:15:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4549.06s)]
*  do they acknowledge that? For the most part, no. I don't want to say most people, but like, [[01:15:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4552.74s)]
*  the vast majority of the discussions tend to be like more intellectual. [[01:16:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4563.86s)]
*  I think people just take for granted that everyone has like a secular, mostly secular point of view. [[01:16:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4567.86s)]
*  Well, I think that, you know, the truly brilliant conclusion is that we don't know a lot and we [[01:16:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4573.219999999999s)]
*  don't have a ton of power. That's my view. Right. Right. So like the actual intellectual [[01:16:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4578.82s)]
*  will over time, if he's honest, will read it. This is the view of like many scientists and [[01:16:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4584.259999999999s)]
*  many people who really went deep. I mean, I don't know who said it. I'm trying to remember, [[01:16:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4588.179999999999s)]
*  but someone said like the first gulp of science, making an atheist, but at the bottom of the cup, [[01:16:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4591.78s)]
*  you'll find God waiting for you. That's Matthias Desmet wrote a book about this, [[01:16:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4599.219999999999s)]
*  supposedly about COVID. It was not about COVID. I just cannot recommend it more strongly. [[01:16:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4603.86s)]
*  The book is about the point you just made, which is the deeper you go into science, [[01:16:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4609.3s)]
*  the more you see some sort of order reflected that is not random at all. Yes. And a beauty [[01:16:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4614.74s)]
*  exhibited in math even. And the less you know, and the more you're certain that [[01:17:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4621.54s)]
*  this is, that there's a design here and that's not human or quote natural, it's supernatural. [[01:17:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4629.78s)]
*  That's his conclusion and I affirm it, but how many people do you know in your science world [[01:17:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4636.18s)]
*  who think that? Yeah. I can count them on one hand, basically. How interesting. Yeah. [[01:17:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4641.62s)]
*  That concerns me because I feel like without that knowledge, hubris is inevitable. Yeah. And, [[01:17:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4650.74s)]
*  you know, a lot of these conclusions are from hubris. Like the fact that, you know, there's so [[01:17:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4656.1s)]
*  many people that believe that AI is an eminent existential threat. A lot of people believe that [[01:17:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4661.46s)]
*  we're going to die. We're all going to die in the next five years. Comes from that hubris. How [[01:17:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4666.1s)]
*  interesting. I've never, until I met you, I've never thought of that, that actually, [[01:17:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4671.54s)]
*  that is itself an expression of hubris. I never thought of that. Yeah. You can go negative with [[01:17:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4676.9800000000005s)]
*  hubris. You can go positive and I think the positive thing is good. Like I think Elon is an [[01:18:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4683.06s)]
*  embodiment of that as like just a self belief that you can like fly rockets and build electric [[01:18:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4689.7s)]
*  cars is good. And maybe in some cases it's delusional, but like net net will kind of put [[01:18:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4694.82s)]
*  you in a, on a, on a good path for, for creation. I think it can go pathological. If you, um, [[01:18:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4700.26s)]
*  if you, you know, if you're, for example, SBF, and again, he's, he's kind of part of those groups, [[01:18:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4707.139999999999s)]
*  um, just sort of believed that he can do anything in service of his ethics, [[01:18:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4712.1s)]
*  including seal and cheat and all of that. Yeah, I don't, I never really understood. Well, of course, [[01:18:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4720.42s)]
*  I understood too well, I think, but the, the obvious observable fact that effective altruism [[01:18:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4725.86s)]
*  led people to become shittier toward each other, not better. [[01:18:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4733.62s)]
*  Yeah. I mean, it's such an irony, but I feel like it's in the name. If you call yourself such [[01:18:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4738.1s)]
*  grandiose thing, you typically, yeah, horrible. Like the Islamic state is neither Islamic or state. [[01:19:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4744.26s)]
*  Effective altruists are neither altruists. The United Nations is not United. [[01:19:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4754.5s)]
*  No, that's boy is that wise. So I don't think to your earlier point that any large language model [[01:19:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4758.9800000000005s)]
*  or machine could ever arrive at what you just said the iron, because like the deepest level of truth [[01:19:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4765.86s)]
*  is wrapped in irony always. And I don't have machines don't get irony, right? Not yet. [[01:19:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4773.38s)]
*  Could they? Um, maybe I mean, I, I don't think I don't take as strong of a stance as, as you are [[01:19:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4780.179999999999s)]
*  at like the, you know, the capabilities of the machines. I do believe that, you know, if you [[01:19:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4786.82s)]
*  represent it, I don't know. I mean, I'm asking, I really don't know what they're capable of. [[01:19:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4792.099999999999s)]
*  Well, I think maybe they can't come up with really novel irony that is like really insightful for us. [[01:19:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4797.0599999999995s)]
*  But if you put a lot of irony in the data, they'll understand. [[01:20:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4804.099999999999s)]
*  Right. They can ape human iron. [[01:20:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4807.7s)]
*  They can ape. I mean, they're ape machines. They're imitation machines. They're literally imitating [[01:20:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4809.0599999999995s)]
*  like, you know, the way large language models are trained is that you give them a corpus of [[01:20:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4813.38s)]
*  text and they hide different words and they try to guess them. And then they adjust the weights of [[01:20:19](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4819.0599999999995s)]
*  those neural networks. And then eventually they get really good at guessing what humans would say. [[01:20:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4824.259999999999s)]
*  Well, then, okay. So you're just kind of making the point unavoidable. Like if, if the machines, [[01:20:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4829.54s)]
*  as you have said, that makes sense or the sum total of what's put into them, then, and that [[01:20:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4834.74s)]
*  would include the personalities and biases of the people putting the data in. Then you want like the [[01:20:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4840.58s)]
*  best people, the morally best people, which is say the most humble people to be doing that. [[01:20:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4846.42s)]
*  But it sounds like we have the least humble people doing that. [[01:20:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4851.62s)]
*  Yeah. I think some of them are humble. Like I wouldn't like, I think some people working in AI [[01:20:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4854.1s)]
*  are really upstanding and good and want to do the right thing. But there are a lot of people with [[01:20:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4858.26s)]
*  the wrong motivations coming at it from fear and things like that. And look, this is the other [[01:21:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4863.78s)]
*  point I will make is that, you know, free markets are good because you're going to get all sorts of [[01:21:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4868.34s)]
*  entrepreneurs with different motivations. And, and I think what's, what, what determines the, [[01:21:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4874.82s)]
*  the winner is not always the ethics of whatever, but it's the larger culture. Like what, what is [[01:21:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4882.66s)]
*  the, what kind of product is pulling out of you? If they're pulling the porn and the, you know, [[01:21:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4887.139999999999s)]
*  companion chat bots, whatever, versus they're pulling the education and the healthcare. And [[01:21:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4894.0199999999995s)]
*  I think all the positive things that will make our life better. I think that's really on the, [[01:21:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4900.98s)]
*  on the larger culture. I don't think we can regulate that with government or whatever, [[01:21:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4906.419999999999s)]
*  but if the culture creates demand for things, just makes us worse as humans, then there are [[01:21:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4911.0599999999995s)]
*  entrepreneurs that will spring up and serve this. That's totally right. And it is a snake eating its [[01:21:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4919.54s)]
*  tail at some point, because of course, you know, you, you serve the baser human desires and you [[01:22:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4925.86s)]
*  create a culture that inspires those desires and a greater number of people. In other words, the more [[01:22:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4933.219999999999s)]
*  porn you have, the more porn people want like actually. I wonder about the pushback from [[01:22:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4938.099999999999s)]
*  existing industry, from the guilds. So like if you're the AMA, for, for example, you mentioned [[01:22:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4945.62s)]
*  medical advances, that's something that makes sense to me for, for diagnoses, which really is just a [[01:22:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4950.82s)]
*  matter of sorting the data, like what's most likely. And a machine can always do that more [[01:22:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4957.139999999999s)]
*  efficiently and more quickly than any hospital or individual doctor. So like, and diagnosis is like [[01:22:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4961.38s)]
*  the biggest hurdle. That's going to like, that's going to actually put people in a business, [[01:22:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4968.26s)]
*  right? If I can just type my symptoms into a machine and I'm getting a much higher likelihood [[01:22:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4975.06s)]
*  of a correct diagnosis than I would be after three days, the Mayo clinic, like who needs the [[01:23:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4981.06s)]
*  mail? I actually have a concrete story about that. I've, I've dealt with like a chronic issue for [[01:23:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4985.14s)]
*  a couple of years. I spent hundreds of thousands of dollars on, on doctors out of pocket, get like [[01:23:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4990.26s)]
*  world's experts and all that. Hundreds of thousands of dollars. Yes. And they couldn't come up with a [[01:23:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=4996.26s)]
*  right diagnosis. And eventually it took me like writing a little bit of software to collect the [[01:23:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5001.86s)]
*  data or whatever, but I ran it around AI. I used AI, I ran the AI ones and it gave me a diagnosis. [[01:23:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5006.5s)]
*  They haven't looked at, and I went to them. They were very skeptical of it. And then we ran the [[01:23:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5012.259999999999s)]
*  test. Turns out it was the right diagnosis. That's incredible. Yeah. It's amazing. It changed my life. [[01:23:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5016.099999999999s)]
*  That's incredible. You had to write the software to get there. Yeah. A little bit of software. [[01:23:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5021.86s)]
*  So that's just, we're not that far from like having publicly available. [[01:23:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5025.94s)]
*  Right. And by the way, I think that anyone can write a little bit of software right now at [[01:23:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5030.1s)]
*  Radplet. We are working on a way to generate most of the code for you. We have this program called [[01:23:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5034.34s)]
*  a hundred days of code. If you give it 20 minutes, you know, do a little bit of coding every day. [[01:24:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5040.18s)]
*  In like three months, you'll be good enough coder to build a startup. I mean, eventually you will [[01:24:06](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5046.74s)]
*  get people working for you and you'll upscale and all of that, but you'll have enough skills. And in [[01:24:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5052.74s)]
*  fact, you know, I'll put up a challenge out there. People listening to this, if they go through this [[01:24:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5057.46s)]
*  and they build something that they think could be a business, whatever I'm willing to help them [[01:24:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5062.82s)]
*  get it out there, promote it. We'll give them some credits and cloud services, whatever, [[01:24:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5066.9s)]
*  just, you know, tweet at me or something and mention this, this podcast. I'll help. [[01:24:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5071.3s)]
*  What's your Twitter? Amassad. A-M-A-S-A-D. [[01:24:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5075.06s)]
*  So, but there are a lot of entrenched interests. I mean, I don't want to get into the whole COVID [[01:24:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5080.9s)]
*  poison thing, but I'm revealing my biases. But I mean, you saw it in action during COVID where, [[01:24:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5085.860000000001s)]
*  you know, it's always a mixture of motives. Like I do think there were high motives mixed with low [[01:24:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5093.62s)]
*  motives, cause that's how people are, you know, it's always a, a broad base of good and bad, but, [[01:24:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5097.860000000001s)]
*  but to some extent, the profit motive prevailed over public health. That is I think fair to say. [[01:25:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5103.14s)]
*  And so they, if they're willing to hurt people to keep the stock price up, [[01:25:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5109.38s)]
*  they, I mean, what's the resistance you're going to get to allowing people to come to a more, [[01:25:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5112.58s)]
*  uh, accurate diagnosis with a machine for free. [[01:25:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5120.74s)]
*  Yeah. So in, in some sense, that's why I think open source AI, people learning how to do some of the [[01:25:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5124.1s)]
*  stuff themselves is, is, is probably good enough. Of course, if there's a company that's building [[01:25:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5132.58s)]
*  these services, it's going to do better, but just the fact that this AI exists and a lot of it is [[01:25:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5140.5s)]
*  open source, you can download it on your machine and use it is enough to potentially help a lot [[01:25:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5146.02s)]
*  of people. By the way, you should always talk to your doctor. I talked to my doctor. I'm not giving [[01:25:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5151.86s)]
*  people advice, kind of figure out all this themselves, but I do think that, uh, it's, [[01:25:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5155.62s)]
*  it's already empowering. So that, that's sort of step one. [[01:26:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5160.9s)]
*  But for someone like me, I'm not going to talk to a doctor until he apologizes to my face for lying [[01:26:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5163.54s)]
*  for four years, because I have no respect for doctors at all. I have no respect for anybody who [[01:26:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5167.78s)]
*  lies period. And I'm not taking life advice and particularly important life advice, like about my [[01:26:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5172.58s)]
*  health from someone who's a liar. I'm just not doing that because I'm not insane. I don't take [[01:26:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5178.9s)]
*  real estate advice from homeless people. I don't take financial advice from people who are going [[01:26:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5181.94s)]
*  to jail for fraud. So like, um, I'm sure there's a doctor out there who would apologize, uh, but I [[01:26:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5185.46s)]
*  haven't met one yet. So for someone like me, who's just, I'm not going to a doctor until they [[01:26:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5192.98s)]
*  apologize. Uh, this could be like literally life saving. Right. So, um, to the question of whether [[01:26:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5197.22s)]
*  there's going to be a regulatory capture, I think that the, the, the, that's, I mean, that's, that's [[01:26:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5204.34s)]
*  why you see Silicon Valley getting into politics. Hmm. You know, Silicon Valley, uh, you know, [[01:26:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5211.9400000000005s)]
*  what was always sort of into politics, you know, when I was, I remember I came in 2012, it was, uh, [[01:26:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5218.740000000001s)]
*  uh, early on in my time, there was, uh, it was the Romney Obama debate. And I was, can I just [[01:27:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5225.38s)]
*  pause? Imagine a debate between Romney and Obama who agree on everything. Yes. Uh, I, I didn't see [[01:27:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5232.740000000001s)]
*  a lot of daylight and, and people were just like making fun of Romney. It was like, it was like, [[01:27:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5240.5s)]
*  he said something like a binder's full of women and kind of that stuck with them or whatever. [[01:27:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5245.38s)]
*  And I remember asking everyone around me, like, who, who are you with? I was like, of course, [[01:27:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5250.820000000001s)]
*  Democrats, of course. Uh, it was like, why, why isn't anyone here for Republicans? And they're [[01:27:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5254.74s)]
*  like, oh, because they're dumb. You know, only dumb people are going to have Republicans. Uh, [[01:27:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5261.62s)]
*  and you know, Silicon Valley was this like one state town in a way. Uh, actually look, you know, [[01:27:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5267.54s)]
*  there's like, um, data on like donations by company for, for, for state. There's like Netflix is 99 [[01:27:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5275.94s)]
*  percent to Democrats and like 1% to, uh, Republicans. If you look up the, you know, [[01:28:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5282.58s)]
*  diversity of parties in North Korea is actually a little better. Oh, of course it is. They have more [[01:28:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5289.299999999999s)]
*  honest media too. But anyways, I mean, you see now a lot of people are surprised that a lot of, um, [[01:28:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5295.139999999999s)]
*  you know, people in tech are going for Republicans, uh, going for Trump. Um, and, uh, particularly [[01:28:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5300.259999999999s)]
*  Mark Andreessen and Ben Horowitz, uh, put out a two hour podcast talking about, [[01:28:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5306.74s)]
*  they are the biggest venture capitalists in the United States, I think. Uh, I don't know [[01:28:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5311.78s)]
*  on what metric you would, you would judge, but they're certainly on their way to be the biggest. [[01:28:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5315.78s)]
*  They're the most, I think the best for sure. Um, and, uh, and they put out a, what was their, [[01:28:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5320.26s)]
*  I didn't, I should have watched, I didn't. Yeah. So the reasoning for why they would vote for Trump, [[01:28:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5329.0599999999995s)]
*  by the way, you know, they would have never done that in like 2018 or 19, whatever. Uh, and, uh, [[01:28:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5335.62s)]
*  so this, this vibe shift that's happening and how was it received? Uh, it's still, it's still mixed, [[01:29:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5345.22s)]
*  but, but I think, you know, way better than what would have happened 10 years ago. They would have [[01:29:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5351.38s)]
*  been canceled and they would have, no one would ever like, no founder would take their money. [[01:29:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5355.54s)]
*  But it's like, and I mean, again, I'm an outsider just watching, but Andreessen Horowitz is so big [[01:29:19](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5359.62s)]
*  and so influential and they're considered smart and not at all crazy. Yeah. That like, [[01:29:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5364.58s)]
*  that's gotta change minds. If Andreessen Horowitz is doing it. Yeah. It will have certainly changed [[01:29:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5370.74s)]
*  minds. Um, I think a lot, I think, you know, give people some courage to say I'm, I have for Trump [[01:29:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5375.62s)]
*  as well at minimum, but I think it does change my, and they put out the arguments is, um, you know, [[01:29:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5381.86s)]
*  they put out this agenda called little tack, you know, there's big tack and they have the lobbying [[01:29:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5388.5s)]
*  and whatever who's lobbying for little tack, like smaller companies, companies like ours, [[01:29:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5392.18s)]
*  but much smaller too, like, you know, one, two person companies. And actually no one is, [[01:29:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5397.06s)]
*  your company would be considered little in Silicon Valley. Uh, but, uh, I want a little company. [[01:30:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5402.66s)]
*  Right. Um, so, uh, but you know, Scott, like really your startups that just started, right? [[01:30:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5409.780000000001s)]
*  Like, you know, typically no one is, um, protecting them sort of politically. No one's [[01:30:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5416.74s)]
*  really thinking about it. And it's very easy to disadvantage startups. Like you just talked about [[01:30:23](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5423.3s)]
*  with the healthcare regulation, what are very easy to create regulators or capture such that [[01:30:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5427.46s)]
*  companies can't even get off the ground doing their thing. Um, and so they came up with this [[01:30:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5433.0599999999995s)]
*  agenda that like, we're going to be the, you know, the firm that's, that's going to be looking out [[01:30:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5438.5s)]
*  for, for that little guy, the little tack, right. Which I think is brilliant. And, you know, part [[01:30:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5443.38s)]
*  of their argument, uh, for Trump is that, you know, uh, the, um, you know, AI, for example, [[01:30:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5448.5s)]
*  like the, the Democrats are really excited about regulating AI. Um, one of the most hilarious [[01:30:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5456.5s)]
*  things that happened, I think, uh, um, Kamala Harris was, uh, invited to AI safety conference [[01:31:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5463.46s)]
*  and they were talking about existential risk. And she was like, well, someone being denied [[01:31:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5471.700000000001s)]
*  healthcare, that's essential for them. Someone, uh, whatever that's existential. So she interpreted [[01:31:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5477.06s)]
*  a central risk as like any risk is, is essential. And so, you know, that's just one anecdote, but [[01:31:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5482.820000000001s)]
*  like, there was this anecdote where she was like, AI is a two letter word and you clearly don't [[01:31:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5488.18s)]
*  understand it very well. And they're, they're moving very fast at, at regulating it. They put [[01:31:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5493.3s)]
*  out an executive order that a lot of people think they kind of, I mean, the, the tweaks they've done [[01:31:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5498.1s)]
*  so far from a user perspective to keep it safe or really like just making sure it hates white people. [[01:31:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5503.700000000001s)]
*  Like it's about pushing a dystopian, totalitarian social agenda, racist social agenda on the country. [[01:31:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5509.46s)]
*  Like can, is that going to be embedded in it permanently? Um, I think it's a function of the [[01:31:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5516.9800000000005s)]
*  culture rather than the regulation. So I think, uh, the culture was sort of this woke culture [[01:32:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5522.18s)]
*  broadly in America, but certainly in Silicon Valley. And now that the vibe shift is happening, [[01:32:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5528.740000000001s)]
*  I think, I think Microsoft just fired their DAI team, Microsoft. Yeah. Uh, I mean, it's a huge [[01:32:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5535.14s)]
*  vibe. Are they going to learn to code? Do you think? Microsoft, perhaps. [[01:32:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5542.26s)]
*  So, uh, you know, the, uh, you know, I wouldn't pin this on the government just yet, but it's very [[01:32:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5548.900000000001s)]
*  easy. No, no, no, no. I just been democratic members of Congress. I know for a fact, applied [[01:32:34](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5554.9s)]
*  pressure to the labs. Like, no, you can't, it has to reflect our values. Okay. Yeah. Yeah. So maybe [[01:32:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5559.139999999999s)]
*  that's where it's permanent. Am I always going to get, when I type in who is George Washington, [[01:32:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5566.0199999999995s)]
*  you know, a picture of Denzel Washington. You know, it's already changing is what I'm saying. [[01:32:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5570.66s)]
*  It's already, a lot of these things are being reversed. It's not perfect, but it's already [[01:32:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5574.74s)]
*  changing. And that's, I think it's just a function of the larger culture change. I think Elon buying [[01:32:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5579.139999999999s)]
*  Twitter, uh, is, uh, and letting people talk and debate moved the culture to like, I think a more [[01:33:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5584.66s)]
*  moderate place. I think he's gone a little more, uh, you know, a little further, but like, I think [[01:33:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5593.62s)]
*  it was net positive on the culture because it was so far left. Um, it was so far left inside these [[01:33:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5601.3s)]
*  companies, the way they were designing their products such that, you know, George Washington [[01:33:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5608.26s)]
*  will look like there's like a black George Washington. Yeah. Have you, uh, that's just [[01:33:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5612.26s)]
*  insane, right? It was like, it was verging on insanity. Well, it's lying. And that's what [[01:33:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5616.74s)]
*  freaked me out. I mean, it's like, I don't know, just tell the truth. There are lots of truths. [[01:33:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5621.22s)]
*  I don't want to hear it. Don't comport with my, you know, desires, but I don't want to be lied to. [[01:33:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5624.42s)]
*  George Washington was not black. No framers were. They were all white Protestant men. Sorry. All [[01:33:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5629.780000000001s)]
*  of them. Yeah. So like, that's a fact deal with it. So if you're going to lie to me about that, [[01:33:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5634.34s)]
*  um, you're my enemy, right? I think so. I mean, you're, uh, and I would say it's a small element [[01:33:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5639.46s)]
*  of these companies that are doing that, but they tend to be the control. They were the controlling [[01:34:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5644.5s)]
*  element. Those like sort of activist folks that were, and I was at Facebook in 2015, uh, and [[01:34:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5649.54s)]
*  worked at Facebook. I worked at Facebook. Yeah. I worked on open source. Mostly I worked on react [[01:34:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5656.98s)]
*  and react native. One of the most powerful kind of wave programming user interfaces. So I, I [[01:34:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5662.66s)]
*  mostly worked on that. I didn't really work on the kind of blue app and all of that. Um, but I saw [[01:34:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5668.26s)]
*  the sort of cultural change where like a small minority of activists were just like shaming [[01:34:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5675.06s)]
*  anyone who is thinking independently. And it's send Silicon Valley and it's like sheep like [[01:34:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5679.780000000001s)]
*  direction where everyone is afraid of this activist class, uh, because they can cancel you. They can, [[01:34:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5687.14s)]
*  you know, um, I think one of the early shots fired there was like Brandon Ike, [[01:34:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5695.219999999999s)]
*  the inventor of JavaScript, the inventor of the language that like runs the browser, [[01:35:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5700.5s)]
*  uh, because of the way he votes or donates, whatever, get, get fired from his position as CTO [[01:35:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5705.219999999999s)]
*  of Mozilla browser. And that was like seen as a win or something. And I was like, again, [[01:35:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5711.94s)]
*  I was like very politically, you know, I was not really interested in politics in like 2012, 13, [[01:35:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5718.9s)]
*  when I first came to this country, but I just accepted it. It's like, oh, you know, all these [[01:35:23](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5723.86s)]
*  people are Democrats, liberal is what you are, whatever. But I just looked at that. I was like, [[01:35:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5728.339999999999s)]
*  that's awful. Like, you know, no matter what his political opinion is, like, you know, you're, [[01:35:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5733.54s)]
*  you're, you're taking from a man, his ability to earn a living. Eventually he started another [[01:35:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5739.219999999999s)]
*  browser company and it's good. Right. But, uh, this like sort of cancel culture created such a [[01:35:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5745.299999999999s)]
*  bubble of conformism and the leadership class at these companies were actually afraid of the [[01:35:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5752.26s)]
*  employees. So that is the fact that bothers me most Silicon Valley is defining our future. [[01:35:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5758.18s)]
*  It, that is technology. We don't have kind of technology in the United States anymore. [[01:36:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5764.02s)]
*  Manufacturing creativity has obviously been extinguished everywhere in the visual arts, [[01:36:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5768.1s)]
*  you know, everywhere Silicon Valley is the last place. Yes. It's important. [[01:36:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5772.02s)]
*  What's the most important. Yes. And so the number one requirement for leadership is courage. [[01:36:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5776.1s)]
*  Number one, nothing even comes close to bravery as a requirement for wise and effective leadership. So [[01:36:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5782.740000000001s)]
*  if the leaders of these companies were afraid of like 26 year old, unmarried screechy girls [[01:36:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5788.740000000001s)]
*  in the HR department, like, Whoa, that's really cowardly. Like, shut up. You're not leading this [[01:36:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5793.860000000001s)]
*  company. I am like, that's super easy. I don't know why that's so hard. Like what is the reason? [[01:36:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5802.1s)]
*  I think it was hard. It was because these companies were competing for talent hand over fist. [[01:36:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5808.74s)]
*  And it was the sort of zero interest era and, and sort of a U S economy and everyone was throwing [[01:36:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5816.58s)]
*  cash at like talents. And therefore, if you offend the sensibilities of the employees, even to the [[01:37:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5824.0199999999995s)]
*  slightest bit, you're afraid that they're going to leave or something like that. I'm trying to make [[01:37:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5830.26s)]
*  up an excuse for them. Well, you would, you could answer this question because you are the talent [[01:37:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5836.74s)]
*  that you came all the way from Jordan to work in the Bay area, to be at the center of creativity [[01:37:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5841.219999999999s)]
*  in science. So, um, the people who do what you do, who can write code, which is the basis of all of [[01:37:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5847.78s)]
*  this, are they, I don't like, they seem much more like you or James DeMore. They just, they don't [[01:37:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5855.38s)]
*  seem like political activists to me for the most part. Yeah. They're, they're still a segment of [[01:37:42](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5862.42s)]
*  those of program or population. Well, they have to be rational because code is about reason, right? [[01:37:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5867.9400000000005s)]
*  Nah, I mean, this is the whole thing. You know, it's like, I don't think, I mean, a lot of these [[01:37:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5873.86s)]
*  people that we talked about are into code and things like that. They're not rational. Really? [[01:37:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5877.3s)]
*  Yeah. Like, look, I think coding could help you become more rational, but he could very easily [[01:38:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5881.3s)]
*  override that. I think that's like the basis of it. I thought, well, if this is true and that is [[01:38:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5885.86s)]
*  true, then that must be true. I thought that was the point. Yeah. But you know, people are very easy, [[01:38:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5890.66s)]
*  very, it's very easy for people to just, uh, you know, compartmentalize, right? I was like, [[01:38:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5894.5s)]
*  now I'm doing coding, now I'm doing emotions. Oh, so the brain is not a computer. The brain is not. [[01:38:19](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5899.38s)]
*  Exactly. Exactly. That's my point. I know. You know, when it, when it's so, you know, I, I'm [[01:38:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5904.42s)]
*  probably, you know, responsible for the most amount of people learning to code in America because I, [[01:38:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5910.34s)]
*  it was like, uh, um, I like built the reason I came to the US is I built this piece of software [[01:38:35](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5915.54s)]
*  that was the first to make it easy to, uh, code in the browser. And it went super viral. [[01:38:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5920.58s)]
*  And a bunch of us companies started using him, including, uh, code academy. Uh, and I joined [[01:38:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5926.74s)]
*  them as like a founding engineer. They had just started two guys, amazing guys that just started [[01:38:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5933.94s)]
*  and I joined them and we taught like 50 million people how to code. You know, many of them, [[01:38:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5938.66s)]
*  many millions of them are American. Um, and you know, the, uh, sort of rhetoric at a time, [[01:39:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5942.5s)]
*  what you would say is like coding is important because, you know, it'll teach you how to think [[01:39:08](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5948.18s)]
*  computational thinking and, and all of that. I sort of like not, maybe I've said it at some [[01:39:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5953.14s)]
*  point, but I've never really believed it. I think coding is a tool you can use to build things, [[01:39:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5957.3s)]
*  to automate things, to, it's a fun tool. You can do art with it. You can do a lot of things with it. [[01:39:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5962.74s)]
*  But, uh, ultimately I don't think, you know, you can, you know, sit people down and sort of make [[01:39:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5968.02s)]
*  them more rational. Um, and you get into all these weird things if you try to do that, [[01:39:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5973.62s)]
*  you know, people can become more rational by virtue of education, by virtue of seeing that, [[01:39:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5979.62s)]
*  you know, taking a more rational approach to, um, to their life yields results, [[01:39:46](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5986.18s)]
*  but you can't, you can't like really teach it that way. [[01:39:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5992.900000000001s)]
*  Well, I agree with that completely. That's interesting. I just thought it was a certain, [[01:39:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=5996.42s)]
*  because I have to say without getting into controversial territory, um, every person I've [[01:40:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6001.06s)]
*  ever met who writes code, like is kind of similar in some ways to every other person I've ever met [[01:40:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6005.06s)]
*  who writes code. Like it's not a broad cross section of any population. No. At all. Well, [[01:40:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6009.22s)]
*  people who make it a career, but I think anyone sort of can write a lot of code. I'm sure. I mean, [[01:40:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6017.3s)]
*  people get paid to do it. Right. Right. Yeah. Um, interesting. So bottom line, do you see, [[01:40:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6021.139999999999s)]
*  and then we didn't even mention Elon Musk, David Sachs. Um, I've also come out for Trump. So do you [[01:40:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6026.339999999999s)]
*  think the vibe shift in Silicon Valley is real? Yes, actually I would credit Sachs originally, [[01:40:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6033.62s)]
*  like perhaps more than Elon, because look, it's one party state. Yeah. No one watches you, [[01:40:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6041.219999999999s)]
*  for example, no one ever watched anything sort of, you know, I was, I don't want to over [[01:40:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6048.58s)]
*  generalize, but most people didn't get any right wing or center right opinions for, uh, uh, for the [[01:40:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6053.78s)]
*  most part. They didn't seek it. It wasn't there. You're swimming in just, you know, liberal, [[01:41:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6061.0599999999995s)]
*  democratic, uh, sort of talking points. I say Sachs, uh, in the All In Podcast was sort of the [[01:41:06](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6066.42s)]
*  first time a lot of people started on a weekly basis, hearing a conservative doc being David [[01:41:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6074.66s)]
*  Sachs. Amazing. And I would start to hear at parties and things like that, people describe [[01:41:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6080.82s)]
*  their politics as Sachs, Sachsism. I just started calling it. They were like, you know, I agree with [[01:41:26](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6086.58s)]
*  it. You know, most of the time I agree with it. Uh, you know, Sachs is, uh, point of view on all, [[01:41:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6092.26s)]
*  All In Podcast. Like, yeah, you're kind of maybe moderate or center right at this point. [[01:41:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6097.62s)]
*  And he's so reasonable. He's first of all, he's a wonderful person, I, in my opinion, but, um, [[01:41:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6103.7s)]
*  like I didn't have any sense of the reach of that podcast until I did him. I had no sense at all. [[01:41:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6109.459999999999s)]
*  And he's like, we do my podcast. Sure. Cause I love David Sachs. I do the podcast. Like everyone [[01:41:55](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6115.299999999999s)]
*  I've ever met texts me, Oh, you're on All In Podcast. Like it's, it's not my world. But I [[01:41:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6119.78s)]
*  didn't realize that is the vector. If you want to reach sort of business minded people were not [[01:42:06](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6126.0199999999995s)]
*  very political, but are probably going to like send money to a buddy who's bundling for common [[01:42:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6131.94s)]
*  law because like she's our candidate. Yes. I mean, that's the way to reach people like that. [[01:42:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6136.259999999999s)]
*  That's right. And by the way, this is my point about technology can have a centralizing effect, [[01:42:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6141.139999999999s)]
*  but also decentralizing. Yes. So YouTube, uh, you can argue YouTube is the centralized thing. [[01:42:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6145.94s)]
*  They're pushing opinions on us, whatever. But you know, now you have a platform on YouTube after [[01:42:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6152.259999999999s)]
*  you got fired from Fox, right? Uh, you know, Sachs, uh, can have a platform, uh, and put these [[01:42:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6157.14s)]
*  opinions out. And I think, uh, you know, there was a moment during COVID that I felt like they're [[01:42:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6163.9400000000005s)]
*  going to close everything down. Yeah. Uh, for good reason, you felt that way. Yes. Uh, and maybe [[01:42:50](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6170.42s)]
*  they were, maybe there's going to be some other event that they'll like allow them to close it [[01:42:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6176.42s)]
*  down. But one of the things I really love about America is the first amendment. It's just, it's [[01:43:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6181.06s)]
*  just the most important, uh, institutional innovation in the history of humanity. I agree [[01:43:06](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6186.26s)]
*  with that completely. And we should, we should really, you grew up without it too. I mean, it [[01:43:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6191.62s)]
*  must be, we should really protect it. Like we should, like we should be so coveting of it. Like, [[01:43:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6195.46s)]
*  you know, we should, you know, like your wife or something. Can you, I totally agree hands off. [[01:43:21](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6201.860000000001s)]
*  Can you just repeat your description of its importance historically? I'm sorry. You put it [[01:43:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6209.14s)]
*  so well. Uh, it's the most important institutional innovation in human history. [[01:43:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6212.98s)]
*  The first amendment is the most important institutional innovation in human history. [[01:43:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6219.139999999999s)]
*  Yes. I love that. I think it's absolutely, absolutely right. And as someone who grew up [[01:43:44](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6224.0199999999995s)]
*  with it in a country that had had it for, you know, 200 years when I was born, um, you know, [[01:43:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6228.58s)]
*  you don't, you don't feel that way. It's just like, well, it's the first amendment. It's like, [[01:43:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6234.099999999999s)]
*  just part of nature, right? It's like gravity. It just exists. But as someone who, you know, [[01:43:56](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6236.66s)]
*  grew up in a country that does not have it, which is true of every other country on this planet. [[01:44:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6241.22s)]
*  It's the only country that has it. Um, you see it that way. You see it as the thing that makes [[01:44:06](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6246.02s)]
*  America America. Well, the thing that makes it so that we can change course. Yes. Right. And, [[01:44:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6250.58s)]
*  and the reason why, you know, we had this, you know, conformist, um, uh, mob rule mentality that [[01:44:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6256.02s)]
*  people call woke, uh, uh, you know, um, the reason that we're now past that almost, you know, [[01:44:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6267.78s)]
*  still kind of there, but we'll like, we're on our way past that is because of, of the first amendment [[01:44:37](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6277.0599999999995s)]
*  and free speech. And again, I would credit Elon a lot for buying Twitter and letting us talk and [[01:44:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6283.38s)]
*  can debate and push back on the craziness, right? It's kind of, it's, well, it's beautiful. I've [[01:44:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6288.66s)]
*  been a direct beneficiary of it is I think everyone in the country has been. So I'm not [[01:44:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6294.66s)]
*  critical and I love Elon, but I'm, I mean, it's a little weird that like a foreigner has to do that. [[01:44:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6298.9s)]
*  A foreigner foreign born person, you, Elon appreciates it in this way. It's like, [[01:45:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6304.98s)]
*  it's a little depressing. Like why didn't some American born person do that? I guess, [[01:45:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6311.3s)]
*  cause they don't, we don't take it. Yeah. You know, I wrote a thread. It's like 10 things. I [[01:45:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6314.66s)]
*  like about America. I expected it to do well, but you know, it was like three, four years ago. [[01:45:19](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6319.86s)]
*  It went super viral. Did Wall Street journal covered it. Peggy Newton, you know, called me [[01:45:24](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6324.42s)]
*  and it was like, I want to write a story about it. I was like, okay, it's like a Twitter thread. You [[01:45:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6329.62s)]
*  can read it. But, you know, I, I, and I just like talk about normal things, you know, free speech, [[01:45:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6333.46s)]
*  one of them, but also like hard work, appreciation for talent and, and all of that. And it was [[01:45:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6340.82s)]
*  starting to close up, right? I started to see, you know, meritocracy kind of like being less valued [[01:45:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6347.54s)]
*  and think that that's part of the reason why I wrote the thread. And what I, what I realized is [[01:45:52](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6352.18s)]
*  like, you know, yeah, most Americans just, just don't think about that and don't really value it [[01:45:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6358.900000000001s)]
*  as much. I agree. And so maybe you do need, you do need. Oh, I think that's absolutely right. I, [[01:46:03](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6363.62s)]
*  but why do you, I mean, I have seen, I hate to say this because I've always thought my whole life [[01:46:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6370.18s)]
*  that foreigners are great. You know, I like traveling to foreign countries. I like my best [[01:46:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6374.26s)]
*  friend is foreign born actually, as opposed to mass immigration as I am, which I am. Arabs really [[01:46:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6380.34s)]
*  like you by the way. Oh, well, I really like Arabs. I'm throwing off the brainwashing. [[01:46:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6385.62s)]
*  Just a sidebar, I feel like we had a bad experience with Arabs 23 years ago and what a lot of Americans [[01:46:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6392.9800000000005s)]
*  didn't realize, but I knew from traveling a lot of the middle East. Yeah, it was bad. It was bad. [[01:46:39](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6399.3s)]
*  However, like that's not representative of the people that I have dinner with in the middle East [[01:46:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6405.54s)]
*  at all. Like someone once said to me, like, those are the worst people in our country and right. And [[01:46:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6409.86s)]
*  no, I totally agree with that strongly. I always defend the Arabs in a heartfelt way, but no, I, [[01:46:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6417.299999999999s)]
*  I wonder if some of the, particularly the higher income immigrants recently I've noticed are like [[01:47:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6424.98s)]
*  parroting the same kind of anti-American crap that they're learning from the institute. You know, [[01:47:15](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6435.22s)]
*  you come from Punjab and go to Stanford and all of a sudden, like you've got that same rotten [[01:47:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6440.82s)]
*  decadent attitudes of your native born professors from Stanford. Do you see that? No, I'm not sure [[01:47:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6447.06s)]
*  what's the distribution like. I mean, speaking of Indians, I mean, on the right side of the spectrum, [[01:47:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6453.14s)]
*  we have Vivek and who's the best. Yeah. He's a perfect example of what I'm saying. Like [[01:47:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6458.900000000001s)]
*  Vivek is thought through, not just like first amendment good, but why it's good. [[01:47:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6463.38s)]
*  Yeah. Well, I, you know, I'm not sure, you know, I'm not sure it good. Yeah. I think it's, yeah, [[01:47:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6467.22s)]
*  I think foreigners for the most part do appreciate it more, but it's easy. You know, [[01:47:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6473.9400000000005s)]
*  it talked about how I just, you know, try not to be, you know, this conformist kind of really [[01:47:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6479.9400000000005s)]
*  absorb everything that I mean and act on it, but it's very easy for people to go in these [[01:48:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6484.74s)]
*  one party state places and really get, you know, become part of this like mob mentality where [[01:48:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6489.38s)]
*  everyone believes the same thing and any deviation from that is considered cancelable offense. And [[01:48:18](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6498.66s)]
*  you know, you asked about the shift in Silicon Valley. I mean, part of the shift is like, [[01:48:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6507.7s)]
*  yeah, Silicon Valley still has a lot of people who are independent minded and they see this sort of [[01:48:30](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6510.58s)]
*  conformist type of thinking and the democratic party. And that's really repulsive for them where, [[01:48:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6516.66s)]
*  you know, there's like a party line. It's like Biden's sharpest attack, sharpest attack. Everyone [[01:48:43](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6523.700000000001s)]
*  says that. And then the debates happen. Oh, unfit, unfit, unfit. And then, oh, he's out. Oh, [[01:48:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6528.42s)]
*  Kamala, Kamala. It's like, right. It's like, you know, lockstep and there's like no range. There's [[01:48:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6534.1s)]
*  no, there's very little dissent within, within that party. And maybe Republicans, I think at [[01:48:59](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6539.38s)]
*  some point where we're the same, maybe now it's, it's sort of a little different, but this is what, [[01:49:04](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6544.74s)]
*  why people are attracted to the other side in Silicon Valley. By the way, this is advice for [[01:49:10](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6550.58s)]
*  the Democrats. Like if you want sort of Silicon Valley back, you know, maybe don't be so controlling [[01:49:14](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6554.099999999999s)]
*  of opinions and like be okay with more dissent. Well, you have to relinquish a little bit of [[01:49:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6562.179999999999s)]
*  power to do that. I mean, it's the same as raising teenagers. There's always a moment in [[01:49:27](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6567.62s)]
*  the life of every parent of teenagers where a child is going in a direction you don't want. [[01:49:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6571.94s)]
*  You know, it's a, you know, it's a shooting heroin direction. You have to intervene with [[01:49:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6576.5s)]
*  maximum force, but there are a lot of directions a kid can go that are deeply annoying to you. [[01:49:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6580.339999999999s)]
*  And you have to restrain yourself a little bit. If you want to preserve the relationship, [[01:49:45](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6585.379999999999s)]
*  actually, if you want to preserve your power over the child, you have to pull back and be like, [[01:49:51](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6591.379999999999s)]
*  I'm not going to say anything. This child will come back to my, my gravitational pull is [[01:49:57](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6597.0599999999995s)]
*  strong enough. I'm not going to lose this child because she does something that offends me today. [[01:50:01](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6601.7s)]
*  You know what I mean? You can't hold too tightly. And I feel like they don't [[01:50:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6607.62s)]
*  understand. I feel like the Democratic party, I'm not an intimate, of course I'm not in the meetings, [[01:50:12](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6612.58s)]
*  but I feel by their behavior that they feel very threatened. That's what I see. These are people [[01:50:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6617.78s)]
*  who feel like they're losing their power. And so they have to control what you say on Facebook. [[01:50:23](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6623.62s)]
*  I mean, what? If you're worried about people say on Facebook, you know, you've lost confidence in [[01:50:28](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6628.82s)]
*  yourself. Do you feel that? Yeah. And I mean, you know, there's Matt Taibbi and Michael Schellenberger [[01:50:33](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6633.139999999999s)]
*  and a lot of folks, you know, did a lot of great work on censorship and the government's kind of [[01:50:41](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6641.46s)]
*  involvement in that and how they push social media companies. I don't know if you can put it [[01:50:49](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6649.139999999999s)]
*  just on the Democrats, because I think part of it happened during the Trump administration as well. [[01:50:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6654.82s)]
*  But I think they're more excitable about it. They really love misinformation as a term, [[01:50:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6658.98s)]
*  which I think is kind of a BS term. It's a meaningless term. It's a meaningless term. [[01:51:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6665.219999999999s)]
*  All that matters is whether it's true or not. Yeah. And the term miss and disinformation [[01:51:09](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6669.219999999999s)]
*  doesn't even address the veracity of the claim. That's right. It's like irrelevant to them, [[01:51:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6673.54s)]
*  whether it's true or not. In fact, if it's true, it's more upsetting. Yeah. It's like everything [[01:51:17](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6677.139999999999s)]
*  what we talked about earlier is just making people stupid by taking their faculty of, [[01:51:20](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6680.58s)]
*  you know, trying to discern truth. I think that's, you know, that's how you actually become [[01:51:25](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6685.78s)]
*  rational by like trying to figure out whether something's true or not. And then being right [[01:51:31](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6691.86s)]
*  or wrong. And then that really kind of trains you for having a better judgment. You talked about [[01:51:38](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6698.58s)]
*  judgment. That's how people build good judgment. You can't outsource your judgment to the group. [[01:51:47](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6707.86s)]
*  Which again, it feels like what's asked from us, especially in liberal circles, is that, [[01:51:54](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6714.66s)]
*  no, Fauci knows better. Two weeks to stop the spread. You know, take the job, stay home, [[01:52:00](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6720.34s)]
*  wear the mask. You know, it's just like talking down to us as children. You can't discuss certain [[01:52:05](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6725.78s)]
*  things on YouTube. You'll get banned. At some point you couldn't say the Laplique theory, [[01:52:11](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6731.86s)]
*  right? Which is now the mainstream theory. Yes. And again, a lot of this self-corrected because [[01:52:16](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6736.5s)]
*  of the first amendment. Yeah. And Elon. Wow. That was as interesting as dinner was last night. [[01:52:22](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6742.5s)]
*  A little less profanity, but I'm really grateful that you took the time to do this. [[01:52:29](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6749.46s)]
*  Thank you. It's absolutely my pleasure. It was mine. Thank you. [[01:52:32](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6752.74s)]
*  Thanks. [[01:52:36](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6756.18s)]
*  So it turns out that YouTube is suppressing our show. I know. Shocking that in an election year, [[01:52:40](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6760.9s)]
*  with everything at stake, Google would be putting its thumb on the scale and preventing you from [[01:52:48](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6768.5s)]
*  hearing anything that the people in charge don't want you to hear. But it turns out it's happening. [[01:52:53](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6773.78s)]
*  So what can you do about it? Well, we could whine about it, but that's a waste of time. [[01:52:58](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6778.9s)]
*  We're not in charge of Google, or we could find a way around it. A way that you could actually get [[01:53:02](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6782.66s)]
*  information that's true. It's not intentionally deceptive. And the way to do that on YouTube, [[01:53:07](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6787.7s)]
*  we think, is to subscribe to our channel. Subscribe, and you'll have a much higher chance [[01:53:13](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6793.38s)]
*  of hearing what we say. And we hope you will. [[01:53:19](https://www.youtube.com/watch?v=cNqTu_58sZ8&t=6799.7s)]
