---
Date Generated: April 06, 2024
Transcription Model: whisper medium 20231117
Length: 1623s
Video Keywords: ['Democracy Now', 'Amy Goodman', 'News', 'Politics', 'democracynow', 'Independent Media', 'Breaking News', 'World News']
Video Views: 62681
Video Rating: None
---

# Lavender & Where's Daddy: How Israel Used AI to Form Kill Lists & Bomb Palestinians in Their Homes
**Democracy Now!:** [April 05, 2024](https://www.youtube.com/watch?v=4RmNJH4UN3s)
*  This is Democracy Now!, democracynow.org, The War and Peace Report.
*  I'm Amy Goodman.
*  The Israeli publications Plus 972 magazine and Local Call have exposed how the Israeli
*  military used artificial intelligence, known as Lavender, to develop a kill list in Gaza
*  that includes as many as 37,000 Palestinians who are targeted for assassination with little
*  human oversight.
*  The report is based in part on interviews with six Israeli intelligence officers who'd
*  with the AI system.
*  972 reports, quote,
*  Lavender has played a central role in the unprecedented bombing of Palestinians, especially
*  during the early stages of the war.
*  In fact, according to the sources, its influence on the military's operations was such that
*  they essentially treated the outputs of the AI machine as if it were a human decision.
*  A second AI system, known as Where's Daddy, tracked Palestinian men on the kill list.
*  It was purposely designed to help Israel target individuals when they were at home
*  at night with their families.
*  One intelligence officer told the publications, quote,
*  "...we were not interested in killing operatives only when they were in a military building
*  or engaged in military activity.
*  On the contrary, the IDF bombed them in homes without hesitation as a first option.
*  It's much easier to bomb a family's home.
*  The system is built to look for them in these situations," they said.
*  Today we spend the hour with the Israeli investigative journalist Yuval Abraham, who broke the story
*  for 972 and Local Call.
*  It's headlined, Lavender, the AI machine directing Israel's bombing spree in Gaza.
*  I spoke with Yuval Abraham yesterday and began by asking him to lay out what he found.
*  Yeah, thank you for having me again, Amy.
*  It is a very long piece.
*  It's 8,000 words.
*  We divided it into six different steps.
*  Each step represents a process in the highly automated way in which the military marked
*  targets since October.
*  The first finding is Lavender.
*  Lavender was designed by the military.
*  Its purpose was, when it was being designed, to mark the low-ranking operatives in the
*  Hamas and Islamic Jihad military wings.
*  That was the intention.
*  Because Israel estimates that there are between 30,000 to 40,000 Hamas operatives, and it's
*  a very, very large number.
*  They understood that the only way for them to mark these people is by relying on artificial
*  intelligence.
*  That was the intention.
*  What sources told me is that after October 7, the military basically made a decision
*  that all of these tens of thousands of people are now people that could potentially be bombed
*  inside their houses, meaning not only killing them, but everybody who was in the building,
*  the children, the families.
*  They understood that in order to try to attempt to do that, they are going to have to rely
*  on this AI machine called Lavender with very minimal human supervision.
*  One source said that he felt he was acting as a rubber stamp on the machine's decisions.
*  What Lavender does is it scans information on probably 90% of the population of Gaza.
*  We're talking about more than a million people.
*  It gives each individual a rating between 1 to 100.
*  A rating that is an expression of the likelihood that the machine thinks, based on a list of
*  small features, and we can get to that later, that that individual is a member of the Hamas
*  or Islamic Jihad military wings.
*  Sources told me that the military knew, because they checked, they took a random sampling
*  and checked one by one, the military knew that approximately 10% of the people that
*  the machine was marking to be killed were not Hamas militants.
*  Some of them had a loose connection to Hamas.
*  Others had completely no connection to Hamas.
*  One source said how the machine would bring people who had the exact same name and nickname
*  as a Hamas operative, or people who had similar communication profiles, like it could be civil
*  defense workers, police officers in Gaza.
*  They implemented, again, minimal supervision on the machine.
*  One source said that he spent 20 seconds per target before authorizing the bombing of the
*  alleged low-ranking Hamas militant.
*  Often it was also a civilian killing those people inside their houses.
*  And I think the reliance on artificial intelligence here to mark those targets and basically the
*  deadly way in which the officers spoke about how they were using the machine could very well be
*  part of the reason why in the first six weeks after October 7th, one of the main characteristics
*  of the policies that were in place were entire Palestinian families being wiped out inside
*  their houses.
*  If you look at UN statistics, more than 50% of the casualties, more than 6,000 people at that time
*  came from a smaller group of families.
*  It's an expression of the family unit being destroyed.
*  And I think that machine and the way it was used led to that.
*  AMY GOODMAN You talk about the choosing of targets, and you talk about the so-called
*  high-value targets, Hamas commanders, and then the lower-level fighters.
*  And as you said, many of them, in the end, it wasn't either.
*  But explain the buildings that were targeted and the bombs that were used to target them.
*  ADIM ZEID-MOSTAFA Yeah, yeah, it's a good question.
*  So, what sources told me is that during those first weeks after October, for the low-ranking
*  militants in Hamas, many of whom were marked by Lavender, so we can say alleged militants that
*  were marked by the machine, they had predetermined what they call collateral damage degree.
*  And this means that the military's international law departments told these intelligence officers
*  that for each low-ranking target that Lavender marks, when bombing that target, they are allowed
*  to kill—one source said the number was up to 20 civilians.
*  Again, for any Hamas operator, regardless of rank, regardless of importance, regardless of age,
*  one source said that there were also minors being marked. Not many of them, but he said that was a
*  possibility, that there was no age limit. Another source said that the limit was up to 15 civilians
*  for the low-ranking militants. The sources said that for senior commanders in Hamas—so,
*  it could be commanders of brigades or divisions or battalions—the numbers were, for the first time
*  in the IDF's history, in the triple digits, according to sources. So, for example, Ayman Naufel,
*  who was the Hamas commander of the Central Brigade, a source that took part in the strike against that
*  person, said that the military authorized to kill alongside that person 300 Palestinian civilians.
*  And we've spoken at Plus 972 on the local call with Palestinians who were witnesses of that strike,
*  and they speak about four quite large residential buildings being bombed on that day,
*  entire apartments filled with families being bombed and killed.
*  And that source told me that this is not a—it's not some mistake, like the amount of civilians,
*  of this 300 civilians, it was known beforehand to the Israeli military.
*  And sources described that to me, and they said that—I mean, one source said that during those
*  weeks at the beginning, effectively, the principle of proportionality, as they call it under
*  international law, quote, did not exist. So, there's two programs. There's
*  Lavender and there's Where's Daddy. How did they even know where these men were, innocent or not?
*  Yeah. So, the way the system was designed is—there is this concept in—generally,
*  in systems of mass surveillance called linking. When you want to automate these systems,
*  you want to be able to very quickly—you know, you get, for example, an ID of a person,
*  and you want to have a computer be very quickly able to link that ID to other stuff. And what's
*  told me is that since everybody in Gaza has a home as a house, or at least that was the case in the
*  past, the system was designed to be able to automatically link between individuals and houses.
*  And in the majority of cases, these households that are linked to the individuals that Lavender
*  is marking as low-ranking militants are not places where there is active military action,
*  active military action taking place, according to sources.
*  Yet the way the system was designed—and programs like Where's Daddy, which were designed to search
*  for these low-ranking militants when they enter houses, specifically, it sends an alert to the
*  intelligence officers when these AI-marked suspects enter their houses. The system was
*  designed in a way that allowed the Israeli military to carry out massive strikes against
*  Palestinians, sometimes militants, sometimes alleged militants, who we don't know, when they
*  were in these spaces, in these houses. And the sources said—you know, CNN reported
*  in December that 45 percent of the munitions, according to U.S. intelligence assessments that
*  Israel dropped on Gaza, were unguided, so-called dumb bonds that have a larger damage to civilians.
*  They destroyed the entire structure. And sources said that for these low-ranking
*  operatives in Hamas, they were only using the dumb munitions, meaning they were collapsing the houses
*  on everybody inside. And when you ask intelligence officers why, one explanation they give is that
*  these people were, quote, unimportant. They were not important enough from a military perspective
*  that the Israeli army would, one source said, waste expensive munitions, meaning more guided
*  floor bombs that could have maybe taken just a particular floor in the building. And to me,
*  that was very striking, because, you know, you're dropping a bomb on a house and killing
*  entire families, yet the targets that you are aiming to assassinate by doing so
*  is not considered important enough to, quote, waste an expensive bomb on.
*  And I think it's a very rare reflection of sort of the way, you know, the way the Israeli military
*  measures the value of Palestinian lives in relation to expected military gain,
*  which is the principle of proportionality. And I think one thing that was very, very clear from
*  all the sources that I spoke with is that, you know, this was, they said it was psychologically
*  shocking even for them. You know, like, it was, yeah. So that's the combination
*  between Lavender and Where is Daddy? The Lavender lists are fed into Where is Daddy?
*  And these systems track the suspects and wait for the moments that they enter houses, usually
*  family houses or households where no military action takes place, according to several sources
*  who did this, who spoke to me about this. And these houses are bombed using unguided missiles.
*  This was a main characteristic of Israeli policy in Gaza, at least for the first weeks.
*  AMY GOODMAN You're right that they said they didn't have as many smart bombs. They were more
*  expensive, so they didn't want to waste them. So they used the dumb bombs, which kill so many more.
*  Yeah, exactly. Exactly. That's what they said. But then I say, if the person is, you know,
*  is not important enough for you to waste ammunition on, but you're willing to kill
*  15 civilians, a family— AMY GOODMAN
*  Yuval Abraham, I wanted to read from the Israeli military statement, the IDF statement,
*  in response to your report. They say, quote,
*  The process of identifying military targets in the IDF consists of various types of tools and
*  methods, including information management tools, which are used in order to help the intelligence
*  analysts to gather and optimally analyze the intelligence obtained from a variety of sources.
*  Contrary to claims, the IDF does not use an artificial intelligence system that identifies
*  terrorist operatives or tries to predict whether a person is a terrorist. Information systems
*  are merely tools for analysts in the target identification process. Again, that's the
*  IDF response, Yuval Abraham, to your report. Your response?
*  YVAL ABRAHAM I read this response to some of the sources,
*  and they said that they're lying, that it's not true. And I was surprised that they were—you know,
*  usually they're not so blatant in saying something that is false. I think this can very easily be
*  disproven, because a senior-ranking Israeli military official, the head of the 8th to 100
*  unit AI center, gave a public lecture last year in 2023 in Tel Aviv University—you can Google it,
*  anybody who's listening to us—where he spoke about, quote, I'm quoting him in that lecture,
*  an AI system that the Israeli military used in 2021 to find terrorists. That's what he said.
*  So to have that on record, to have—I have the presentation slides showing how the system is
*  rating the people, and then to get a comment from the IDF spokesperson saying, we do not have a
*  system that uses AI. I really don't know—like, I almost thought, do I put this in the piece or
*  not? Because, you know, like, I know it—in the end, I gave them the space in the piece to make
*  those claims. Like, I think I tried to be as dry as possible in the way that I was reporting.
*  But really, like, I am very, very confident in those findings. They are verified from numerous
*  sources that I've spoken with. And I think that people who read the full investigation,
*  read the depth of it—the commander of the 8200 unit wrote a book in 2021 titled Human Machine
*  Teams, How Synergy Between AI and Human Beings Can Revolutionize the World. And in the book,
*  he's talking about how militaries should rely on artificial intelligence to, quote, solve the problem
*  of the human bottleneck in creating new targets and in the decision-making to approve new targets.
*  And he wrote in that book, he said—and this is another quote from him—he said that no matter how
*  many intelligence officers you have tasked with producing targets during the war, they still will
*  not be able to produce enough targets per day. And he gives a guide in that book as to how to build
*  these AI systems. Now, I want to emphasize, you know, he writes in the book very, very clearly
*  that these systems are not supposed to replace human judgment. He calls it, you know,
*  mutual learning between humans and artificial intelligence. And he says—and the IDF still
*  maintains this—they say it is intelligence officers who look at the results and make a decision.
*  From what I heard from numerous sources, after October 7, that stopped being the cases, at least
*  some parts of the IDF, where again, Amy, as I said before, sources were told that if they check that
*  the target is a male, they can accept Lavender's recommendations without thoroughly looking at them,
*  without checking why the machine made the decision that it made. And I think, you know,
*  when speaking with sources, like, just to describe, like, many of these sources, you know,
*  they were drafted to the military after October 7. Many of them were shocked by atrocities that
*  happened on October 7. Their families, their friends, some of them did not think they would
*  be drafted to the military again. They said, okay, we have to go now. There was this sense that—and
*  gradually, when they realized what they were being asked to do, the things that they are involved in,
*  I wouldn't say that all six are like this, but at least some of them felt, again, shocked by
*  committing atrocities and by being involved in things and killing families, and they felt it's
*  unjustifiable. And they felt a responsibility, I think. And I felt this also in the previous piece
*  that I wrote, the mass assassination factory, which spoke about another AI machine called the
*  Gospel. They felt a need to share this information with the world out of a sense that people are not
*  getting it. They're hearing the military spokesperson and all of these narratives that
*  we've been hearing for the past six months, and they do not reflect the reality on the ground.
*  And I really believe—you know, there's a looming attack now on Rafah. These systems could be used
*  there again to kill Palestinians in massive numbers. These attacks—it's placing the Israeli
*  hostages in danger, who are still unjustifiably held in Gaza and need to be released. There needs
*  to be a ceasefire. It cannot go on. And I hope that this investigation that exposes things so
*  clearly will help more people all around the world call for a ceasefire, call to release the hostages
*  and end the occupation and move towards a political solution. For me, there is no other way forward.
*  AMY GOODMAN I wanted to ask if U.S. military—if
*  U.S. technology is playing a role in Israeli AI, artificial intelligence.
*  ADIM ZEDIQI So, I don't know, and there is some information that I cannot fully share. Like,
*  at this moment, I'm investigating, like, you know, who is involved in developing these systems.
*  What I can tell you is, based on previous experience of the 2014 war and the 2021 war,
*  when—when, you know, when the war ends, these systems are then sold to militaries all over the
*  world. And I think, regardless of, you know, the horrific results and consequences of these systems
*  in Gaza, alongside that, I really think there is a danger to humanity. Like, this AI-based warfare
*  allows people to escape accountability. It allows to generate targets really on a massive,
*  you know, thousands—37,000 people marked for potential assassination. And it allows to
*  do that and maintain a sort of aesthetic of international law, because you have a machine
*  that makes you a target file with, you know, commander or, like, target, collateral damage,
*  but it loses all meaning. I mean, take the principle of distinction under international law.
*  When you design a system that marks 37,000 people and you check and you know that 10 percent of them
*  are actually not militants, right, they're loosely related to Hamas or they're not related at all,
*  and you still authorize to use that system without any meaningful supervision for weeks.
*  I mean, isn't that a breach of that principle? When you authorize to kill, you know, up to 15
*  or up to 20 civilians for targets that you consider, from a military point of view,
*  not especially important. Isn't that a clear breach of the principle of proportionality?
*  You know, and I don't know, like, I think international law really is in a crisis right now,
*  and I think these AI-based systems are making that crisis even worse. They are draining
*  all of these terms from meaning. Let me play for you a clip of National
*  Security Council spokesperson John Kirby being questioned on Tuesday about Israel's killing
*  of seven aid workers in three cars from Chef André's World Central Kitchen. This is Kirby.
*  Is firing a missile at people delivering food and killing them not a violation of international
*  humanitarian law? Israelis have already admitted that this was a mistake that they made. They're
*  doing an investigation. They'll get to the bottom of this. Let's not get ahead of that.
*  The State Department has a process in place, and to date, as you and I are speaking, they have not
*  found any incidents where the Israelis have violated international humanitarian law.
*  So that's the U.S. top spokesperson, John Kirby, saying Israel's never broken international law so
*  far since October 7th. And again, this is in response to a question about the killing of the
*  seven aid workers, one Palestinian and six international aid workers. Can you talk about
*  your response to this attack, three different missiles hitting all three cars, and then what
*  Kirby said? Yeah. Wow. It's quite shocking in my mind, I mean, what he said, based on the evidence
*  that exists. The first thought that popped up to my mind when he was talking about Israel is
*  investigating it. Since I know the statistics, if you take the 2014 bombing and war in Gaza,
*  512 Palestinian children were killed. Israel said that it will investigate. There were hundreds of
*  claims for war crimes. Only one fire the Israeli military actually persecuted a soldier about,
*  and it was about looting of 1,000 shekels. Everything was closed. This happened
*  2018, 2019, 230 Palestinians are shot dead at the border. Again, one file prosecuted. To claim that
*  because Israel is having an investigation, it somehow means that they are getting to the bottom
*  of this and changing something. It's just mocking our intelligence, I think. The second thing that
*  I would say is that it's true that the state of Israel has apologized for it, but if you actually
*  look at the track record of people being killed around aid trucks, this has happened over and over
*  again for Palestinians. I mean, in the beginning of March, 112 Palestinians were killed around the
*  Flower Aid truck. The Guardian reported at the time that 14 such cases happened in February and
*  January. It's clear to me that the Israeli military is apologizing not because of the crime,
*  but because of the identity of the people who were killed in the crime. I think that's really
*  hypocrisy. To answer the question about my findings, I mean, I don't know if artificial
*  intelligence was involved in that strike. I don't want to say something that I'm not 100%
*  sure of, but what I have learned from Israeli intelligence officers makes me not be surprised
*  that this strike took place, because the firing policy is completely permissive,
*  and we're seeing it. I mean, we're seeing unarmed civilians being bombed to death. We saw that video
*  of four people walking and being bombed to death. We have doctors talking about how in hospitals,
*  they're seeing young children with bullet holes, like the Guardian investigation, who spoke to nine
*  doctors that spoke about that. So this extreme permissiveness is not surprising to me.
*  Your piece doesn't talk about drones, but Yuval, can you talk about how the AI systems
*  interact with unmanned attack drones? Yeah. So, you know, I said this last time,
*  like, I can't speak about everything, also, because we are sort of always have to think
*  of the military sensor in Israel. Israeli journalists were very much, you know,
*  blinded by that. But the systems interact. And, you know, if somebody is marked to be killed
*  by Lavender, then that person could be killed by a warplane. They could be killed by a drone,
*  and they could be killed by a tank that's on the ground. There is this sort of policy of
*  sharing intelligence between different units and different weapon operators.
*  I wouldn't be surprised if—because Israel said there was a target, somebody that we suspected.
*  Of course, the aid workers, they completely rejected that. But I wouldn't be surprised if
*  the flagging that the Israeli system received was somehow related to a faulty automated mechanism
*  that is mass surveilling the area and picked up on something and had not the highest precision rate.
*  Again, from what I'm hearing from sources, this is the atmosphere. This is the case.
