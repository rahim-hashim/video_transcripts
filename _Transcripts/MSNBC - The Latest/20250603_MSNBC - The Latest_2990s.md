---
Date Generated: June 03, 2025
Transcription Model: whisper medium 20231117
Length: 2990s
Video Keywords: ['MSNBC', 'MSNBC latest', 'Politics', 'News']
Video Views: 2138
Video Rating: None
Video Description: There’s a good chance that before November of 2022, you hadn’t heard of tech nonprofit OpenAI or cofounder Sam Altman. But over the last few years, they’ve become household names with the explosive growth of thee generative AI tool called ChatGPT. What’s been going on behind the scenes at one of the most influential companies in history and what effect has this had on so many facets of our lives? Karen Hao is an award-winning journalist and the author of “Empire of AI: Dreams and Nightmares in Sam Altman's OpenAI” and has covered the impacts of artificial intelligence on society. She joins “Why Is This Happening?” The Chris Hayes podcast to discuss the trajectory AI has been on, economic effects, whether or not she thinks the AI bubble will pop and more. 
“Why Is This Happening?” is produced by Doni Holloway and Brendan O’Melia, engineered by Bob Mallory and features music by Eddie Cooper. Aisha Turner is the Executive Producer of MSNBC Audio. New episodes come out every Tuesday. Share your thoughts by using #WITHpod. You can also email us at withpod@gmail.com You can see more of our work including links to things we mentioned here by going to MSNBC.com/whyisthishappening.
For more context and news coverage of the most important stories of our day click here: https://www.msnbc.com/
» Subscribe to MSNBC: https://www.youtube.com/msnbc
» Subscribe to MSNBC on TikTok https://www.tiktok.com/@msnbc 
» Subscribe to MSNBC on Instagram https://www.instagram.com/msnbc 
Download our new MSNBC app for the latest breaking news and daily headlines at a glance: https://www.msnbc.com/information/download-msnbc-app-n1241692
Follow MSNBC Show Blogs 
MaddowBlog: https://www.msnbc.com/maddowblog
MSNBC delivers breaking news, in-depth analysis of political headlines, commentary and informed perspectives. Find video clips and segments from The Rachel Maddow Show, The Briefing with Jen Psaki, Morning Joe, The Beat, Deadline: White House, The Weeknight, All In, The Last Word, The 11th Hour, and more.
Connect with MSNBC Online 
Visit msnbc.com: https://www.msnbc.com/ 
Subscribe to the MSNBC Daily Newsletter: https://link.msnbc.com/join/5ck/msnbc-daily-signup
#ElonMusk #SamAltman #Tech
---

# Can Musk, Altman, and the Tech Bros Capture Human Intelligence?
**MSNBC - The Latest:** [June 03, 2025](https://www.youtube.com/watch?v=5eU4t7Euxv4)
*  Hello and welcome. Why is this happening with me? Your host Chris Hayes. I'm willing to [[00:00:00](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=0.0s)]
*  bet that before November of 2022, you had never heard of a tech nonprofit called OpenAI [[00:00:13](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=13.200000000000001s)]
*  and you'd probably never heard of an individual named Sam Altman. Now, maybe you work in tech [[00:00:20](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=20.900000000000002s)]
*  and in AI and you had, but I think the vast majority of people hadn't. I had never heard [[00:00:25](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=25.3s)]
*  of OpenAI. I had never heard of Sam Altman. And then they introduced this new generative AI chat [[00:00:29](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=29.66s)]
*  feature called ChatGPT. And, you know, there's only a few times where there's been a kind of [[00:00:35](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=35.76s)]
*  tech sensation like the introduction of that. It became the fastest growing app in history. [[00:00:41](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=41.5s)]
*  It catapulted them into fame, both OpenAI and Sam Altman. Sam Altman is now one of the most [[00:00:46](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=46.379999999999995s)]
*  famous people probably in America and in the world. He was in the White House, I think the second day [[00:00:55](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=55.5s)]
*  of the Trump administration. He was recently in Saudi Arabia during Trump's trip there. [[00:01:00](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=60.38s)]
*  OpenAI is at the vanguard of what has been this kind of revolution in tech in which hundreds of [[00:01:04](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=64.82s)]
*  billions of, not trillions of dollars have poured into the development of artificial [[00:01:13](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=73.18s)]
*  intelligence based on large language models. And, you know, depending on who you ask, [[00:01:20](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=80.02s)]
*  it's either at the cusp of the most transformative technology in all of human history that will [[00:01:24](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=84.69999999999999s)]
*  inalterably catapult us into a strange new future in which humans enjoy like the leisure [[00:01:30](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=90.61999999999999s)]
*  they've been promised since the dawn of the industrial age. When Marx was writing about how [[00:01:37](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=97.89999999999999s)]
*  if we capture the surplus, everyone would be fishing and critiquing art and Keynes was writing [[00:01:43](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=103.14s)]
*  about the economic prospects of our grandchildren. What will people do when they don't have to work [[00:01:47](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=107.66s)]
*  all day? That like finally this will happen. We finally got the technology. The ones in the [[00:01:51](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=111.5s)]
*  previous eras were wrong, but now we finally got the technology that will allow that. That's one [[00:01:56](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=116.22s)]
*  way of looking at it, right? The total transformation of human activity through the portal of a [[00:02:00](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=120.38s)]
*  generalized intelligence. Or one person described to me and said, imagine the nuclear arms race was [[00:02:06](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=126.86s)]
*  happening in the 1940s and 50s, but it was just a bunch of Silicon Valley. It was just a bunch of [[00:02:14](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=134.5s)]
*  CEOs who were developing the nuclear weapons and they were competing with each other. How would have [[00:02:19](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=139.34s)]
*  that gone? Like, so one is endless surplus and bounty. The other is the destruction and end of [[00:02:25](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=145.02s)]
*  the world in which a kind of like, you know, Frankenstein's monster 2001 space Odyssey, [[00:02:32](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=152.18s)]
*  how figure takes over the world and destroys humanity as we know it, enslaves us, etc. Or the [[00:02:39](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=159.02s)]
*  third thing that I've heard as a possibility for this is like the, remember when there were the [[00:02:46](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=166.70000000000002s)]
*  like non-fungible tokens, the NFTs and like that was going to be the future and there's like this [[00:02:53](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=173.58s)]
*  web 3.0 and then it was just like, no one, like remember when Zuckerberg's whole thing was we're [[00:02:58](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=178.78s)]
*  rebranding the company and remaking company around this new technology, which is the metaverse. [[00:03:04](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=184.22s)]
*  Everything's gonna be the metaverse and that lasted for like maybe 12 months and now we shall [[00:03:08](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=188.54s)]
*  never speak of it again. There were people who were buying like, I remember that I'm reading [[00:03:13](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=193.5s)]
*  an article about like Snoop Dogg sells waterfront metaverse property for seven figures. I was like, [[00:03:17](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=197.66s)]
*  I don't even understand that sentence, but like all of that is now just a dream. It's gone. So maybe [[00:03:24](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=204.85999999999999s)]
*  it'll be that maybe it's just all absolute hype. Those are kind of the three different ways that [[00:03:30](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=210.14s)]
*  people sketch this future. And I have to say, I truly don't know which it is. Like I really feel, [[00:03:35](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=215.74s)]
*  I'm trying very hard these days to get my arms around this. And I don't know, but depending on [[00:03:41](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=221.66s)]
*  my mood or what article I read, I can be convinced of any of those three. So I thought it'd be a good [[00:03:47](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=227.66s)]
*  time to spend some time on today's program, speaking with someone who spent years reporting [[00:03:51](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=231.66s)]
*  out a book about open AI. The book is called Empire of AI. It's out now, Dreams and Nightmares [[00:03:56](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=236.46s)]
*  in Sam Altman's Open AI. And the author is Karen Howe, who joins me now. Karen, great to [[00:04:02](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=242.62s)]
*  have you in the program. Thank you so much for having me, Chris. Can we, so there's so much to [[00:04:07](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=247.26s)]
*  get to here, like in the generalized AI discussion, the specifics of open AI. So I just want to start [[00:04:11](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=251.9s)]
*  with filling in some of which I've learned from your book, but for the listeners who have not [[00:04:17](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=257.1s)]
*  encountered your book, just filling in like the open AI backstory, because it really was one of [[00:04:21](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=261.42s)]
*  those things that like from day one to two, like no one knew what open AI was or what they were [[00:04:27](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=267.26s)]
*  working on or who Sam Altman was. And then all of a sudden it was like, they were going to control [[00:04:31](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=271.26s)]
*  the world. And it really, I've never really encountered anything quite like it. I feel like [[00:04:35](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=275.9s)]
*  Google maybe is maybe the closest to it, but even Google felt like a slower burn, to be honest. [[00:04:40](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=280.46s)]
*  I was there when people were looking for search and people started word of mouth being like Google, [[00:04:47](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=287.34s)]
*  whereas chat, chat GPT really felt like from one day to the next, no one knew about chat GPT, [[00:04:53](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=293.65999999999997s)]
*  and then everyone knew about chat GPT. And what I would love for you to start with is, [[00:04:59](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=299.58s)]
*  who started open AI? How did it start? What was its mission to the beginning? What was, [[00:05:03](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=303.26s)]
*  how did this thing come into being? Yeah, totally. So opening, I started as a non-profit and it was [[00:05:08](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=308.46s)]
*  co-founded by Sam Altman, who we now all know as CEO and Elon Musk, which is kind of an interesting [[00:05:14](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=314.3s)]
*  piece of the history that people don't, aren't as familiar with. And they originally started [[00:05:20](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=320.62s)]
*  as a non-profit because they were trying to position themselves as an anti-Google. [[00:05:27](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=327.58s)]
*  So Musk at the time had become really deeply obsessed with this idea that AI was going to [[00:05:34](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=334.29999999999995s)]
*  destroy humanity if the development of the technology fell into the wrong hands. And he [[00:05:39](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=339.97999999999996s)]
*  specifically believed that Google was accumulating a rather large, outsized influence on AI talent [[00:05:45](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=345.34s)]
*  at the time, particularly with the acquisition of DeepMind and that that was going to lead AI [[00:05:55](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=355.9s)]
*  development to be almost purely done with profit incentives, profit driven incentives. [[00:06:01](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=361.65999999999997s)]
*  And that was, that was what was going to lead to the demise of humanity. So Altman and Musk [[00:06:08](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=368.29999999999995s)]
*  joined forces and say, well, the best way to make an anti-Google is to just have a non-profit that's [[00:06:13](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=373.26s)]
*  going to be a fundamental AI research lab doing the development of this technology purely without [[00:06:17](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=377.82s)]
*  commercial interest, purely for the benefit of the public. We're going to open source everything, [[00:06:23](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=383.5s)]
*  make everything transparent. And that is how it originally begins. [[00:06:29](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=389.18s)]
*  How genuine do you think that was on Musk's part? I mean, it's just, it's a little hard to square [[00:06:34](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=394.62s)]
*  the current version of Musk and the behavior I've observed from Musk, which does not seem [[00:06:41](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=401.34s)]
*  um, particularly altruistic with this sort of like, he just had this genuine belief that this [[00:06:47](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=407.65999999999997s)]
*  was a scourge to humanity and we still need to start a non-profit. You know, I think every major [[00:06:54](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=414.94s)]
*  Silicon Valley Titan always thinks of themselves as being altruistic and also is deeply competitive [[00:07:03](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=423.18s)]
*  and also has a deep seated desire to have a lot of influence and control in the world. [[00:07:12](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=432.94s)]
*  You know, so, um, so I think it kind of all blurs and blends together. And so [[00:07:18](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=438.86s)]
*  do I think Musk believed that he believed that he was doing this for altruism? Yes. [[00:07:23](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=443.98s)]
*  But of course there were other, yeah, but of course there were other, other things at play. [[00:07:29](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=449.34s)]
*  And it was the same with Altman. I would say the same exact thing about him. He also believed that [[00:07:34](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=454.38s)]
*  he believed that this was for the benefit of humanity. But there were other strategic [[00:07:39](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=459.34s)]
*  factors kind of in the air. Right. So that the Google concerns, you've got the sort of, [[00:07:47](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=467.17999999999995s)]
*  if this is done by a private firm solely pursuing profit motive, it might lead to something that's [[00:07:51](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=471.73999999999995s)]
*  genuinely catastrophic for humanity. Yes. And also maybe we don't want Google to get [[00:07:57](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=477.34s)]
*  to grab the brass ring. Exactly. Sort of side by side. Now, Musk, we all went around, when was this [[00:08:02](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=482.78s)]
*  that they had these conversations and they started it? This was in 2015. Yeah. They started having [[00:08:08](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=488.46s)]
*  those conversations. And then at the end of 2015 was when they announced open AI. And what's [[00:08:13](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=493.09999999999997s)]
*  Altman's background at that point. And how does he even know Musk? Altman was the president of [[00:08:18](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=498.62s)]
*  Y Combinator, which was at that point already the most prestigious startup accelerator that you could [[00:08:25](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=505.58s)]
*  join in Silicon Valley. If your startup got into YC, you were kind of set. You got, you didn't get [[00:08:31](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=511.66s)]
*  a lot of capital. The YC promise was interesting. They only gave you a little bit of capital, [[00:08:38](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=518.94s)]
*  but you tapped into a huge network, very influential network that would then rapidly accrue you [[00:08:43](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=523.26s)]
*  more headlines, more capital, more talent, so on and so forth. So he was an investor. He was a [[00:08:50](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=530.86s)]
*  startup guy. He came up through Silicon Valley. He'd been, he was in a Y Combinator class. He [[00:08:56](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=536.38s)]
*  was in the inaugural Y Combinator class. With a friend of mine, actually. [[00:09:02](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=542.0600000000001s)]
*  That's wild. And yeah, and that's where he met one of his most important mentors, Paul Graham, [[00:09:07](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=547.58s)]
*  who was the original founder of YC. And then after he launched his startup and did it for seven years, [[00:09:14](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=554.3000000000001s)]
*  it failed. But then Graham thought, you are one of the most unique people I've ever met in my life. [[00:09:21](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=561.26s)]
*  I want you to take over YC. And so Altman, at a very young age, in his early 30s, ends up taking [[00:09:27](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=567.26s)]
*  over, either late 20s or early 30s, ends up taking over Y Combinator. And at that point, [[00:09:32](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=572.86s)]
*  when he took over YC, he started, you know, the startup that he was really working on, it was [[00:09:41](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=581.1s)]
*  kind of a four square competitor. It wasn't so ambitious and big picture vision. [[00:09:48](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=588.14s)]
*  I've seen video of him giving his pitch deck, which is like, we never know, you might be right [[00:09:55](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=595.26s)]
*  near your friend who's a block away and then you find out and then you're both there. You can go [[00:09:59](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=599.5s)]
*  get a drink. And it's like, it's so funny because it's like, it captures a certain era of like, [[00:10:03](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=603.5s)]
*  innocence intact. Like, hey, it's like, it's not like we're replacing all of you disgusting humans. [[00:10:09](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=609.1s)]
*  Hey, maybe you meet your friend for a drink. Yeah. Amazing serendipity. Yeah. And basically, [[00:10:15](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=615.66s)]
*  when he becomes president of YC, he starts realizing, he really starts kind of stepping [[00:10:23](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=623.42s)]
*  into the shoes of the typical Silicon Valley titans of, let me paint you a sweeping vision [[00:10:31](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=631.9799999999999s)]
*  of the future. And this is why you need to put a lot of money in X, Y, Z, whatever thing that I'm [[00:10:39](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=639.26s)]
*  investing in. I think it's worth noting here, and this is something that does come through in your [[00:10:45](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=645.42s)]
*  book about the culture of Silicon Valley. When you talk about painting the sweeping picture, [[00:10:51](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=651.02s)]
*  that in some ways, because of the models of venture capital, it rewards grandiose storytellers. [[00:10:55](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=655.98s)]
*  Absolutely. More than it rewards anything else in some ways. Like, there's lots of people who [[00:11:04](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=664.9399999999999s)]
*  are great engineers. I mean, they're brilliant engineers and you've never heard their names. [[00:11:09](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=669.74s)]
*  They're not the people that who are household names. The household names are people who have [[00:11:14](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=674.38s)]
*  a particular talent for sketching a vision of the future that people hear and they're like, [[00:11:18](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=678.94s)]
*  I want to put billions of dollars towards that. Yes. And I wouldn't say that Steve Jobs invented [[00:11:25](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=685.34s)]
*  or was the origin of this culture, but certainly Jobs was the most iconic storyteller that everyone [[00:11:33](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=693.1800000000001s)]
*  in the Valley looked up to. So I worked in Silicon Valley myself, and I remember the startup that I [[00:11:40](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=700.0600000000001s)]
*  worked at, everyone always talked about how storytelling was so important, how our CEO was [[00:11:45](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=705.98s)]
*  a really great storyteller and that's what made him a really good CEO. That's how people discussed [[00:11:52](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=712.38s)]
*  it. And it was because Jobs was just so successful at executing that storytelling talent and showing [[00:11:57](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=717.26s)]
*  all of these young founders in that generation what it could be like, how much transformative [[00:12:05](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=725.9s)]
*  power you could kind of accrue by painting those sweeping visions. Now in the case of Jobs, [[00:12:12](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=732.78s)]
*  I would say that he also had a talent for product development that was unparalleled. And in some [[00:12:19](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=739.8199999999999s)]
*  ways, the story of Apple was the pairing of his ability to do both. It truly is the case [[00:12:24](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=744.86s)]
*  that the products they developed were genuinely transformative, unlike anything else, totally [[00:12:31](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=751.74s)]
*  changed the world. I think the iPhone more than anything is one of the most transformative devices [[00:12:35](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=755.9s)]
*  created in my lifetime. So he had kind of both. I think one of the questions we're facing now is [[00:12:41](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=761.1s)]
*  like, I feel like with a lot of Silicon Valley, it's like, is this all just like, am I watching [[00:12:45](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=765.1800000000001s)]
*  Music Man? And the guy who's like, doing a song and dance, can they produce? And on that back end, [[00:12:49](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=769.98s)]
*  I want you to talk a little bit about ChatGPT, because it is transformative. And how did that [[00:12:57](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=777.1800000000001s)]
*  happen? And how did it, what made it happen? Who developed it? And how did it kind of come out of [[00:13:06](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=786.78s)]
*  nowhere? It's so interesting, because as someone who was covering it, it did not feel like it came [[00:13:12](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=792.22s)]
*  out of nowhere. It felt like there was a really clear kind of slow boil up until the release of [[00:13:17](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=797.26s)]
*  ChatGPT, to the point where, like, I have to admit regularly now that when people ask me, [[00:13:23](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=803.8199999999999s)]
*  what were you doing during the ChatGPT moment? I don't remember, because I didn't consider it [[00:13:30](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=810.62s)]
*  to be a moment. When ChatGPT first came out, I was like, oh, seen that, like, whatever. [[00:13:33](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=813.82s)]
*  And that was a huge miss on my part, in that I didn't understand that, even though the technology [[00:13:40](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=820.1400000000001s)]
*  had previously existed, the fact that you now slap a super easy and free user interface on it [[00:13:46](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=826.62s)]
*  completely changes the game. But I guess to go to your question of where did that come from? I mean, [[00:13:51](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=831.6600000000001s)]
*  what I write about in the book is how a lot of people now associate all of AI with ChatGPT, [[00:13:58](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=838.46s)]
*  but it's actually a very specific pathway of AI development that OpenAI decided to take. [[00:14:04](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=844.46s)]
*  So early in its nonprofit heady days, it was trying to figure out how do we become a leader [[00:14:10](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=850.54s)]
*  in AI, because we want to beat Google? Like, how do we surpass Google as a scrappy nonprofit? [[00:14:18](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=858.22s)]
*  And what they hit upon was this thesis, oh, we need to scale existing AI technologies [[00:14:23](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=863.7399999999999s)]
*  to an unprecedented degree as fast as possible and continue scaling it faster than anyone else. [[00:14:31](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=871.5s)]
*  And in order to do that, we are going to need an extraordinary amount of data to the point that we [[00:14:38](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=878.4599999999999s)]
*  will be scraping the whole internet and an extraordinary amount of supercomputers. Like, [[00:14:43](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=883.66s)]
*  we need the largest supercomputers that have ever been built in human history. [[00:14:50](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=890.78s)]
*  And when they hit upon that, they then transitioned into a nonprofit with a for-profit arm [[00:14:55](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=895.42s)]
*  underneath because they realized there's no way we can raise the amount of capital that we need [[00:15:03](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=903.02s)]
*  under this nonprofit, so let's create this other fundraising vehicle where we can promise investors [[00:15:07](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=907.5799999999999s)]
*  returns. And then they set on this trajectory where they then partnered with Microsoft, [[00:15:12](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=912.4599999999999s)]
*  who is going to build them the supercomputers, and then they started scraping all of that data and [[00:15:16](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=916.4599999999999s)]
*  just glomming on more and more pools of data on their servers for training these models. [[00:15:20](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=920.46s)]
*  And in that time, in the AI research world, because ultimately OpenAI at its core, [[00:15:26](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=926.14s)]
*  it was still a research project at that point, in the AI research world, there had been a debate [[00:15:35](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=935.1s)]
*  about how to actually make AI progress. What are the things, the shapes of the technology that you [[00:15:44](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=944.62s)]
*  should be building? What's the path forward to get us to the next level? Yeah. What is the thing that [[00:15:52](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=952.38s)]
*  we should be scaling? That was the question at OpenAI at the time. And within the field, there was this [[00:15:57](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=957.66s)]
*  hypothesis that maybe language models could be a fast way to get rapid progress because language, [[00:16:04](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=964.3s)]
*  the theory argues that people communicate all the knowledge that they ever have through language. [[00:16:13](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=973.34s)]
*  So if you can take all of the language from the internet, it should be able to give you a pretty [[00:16:19](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=979.02s)]
*  good approximation of all the knowledge in the world. There's a lot of debate about that. [[00:16:24](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=984.62s)]
*  But this was the theory, right? But this was the theory. [[00:16:30](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=990.54s)]
*  Large language models, because you have a base, you have a data set that exists that you can go [[00:16:33](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=993.5s)]
*  and scrape. Exactly. [[00:16:39](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=999.34s)]
*  We can talk about the copyright stuff in a second. And we have a technology that [[00:16:40](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1000.7s)]
*  can suck in a lot of data, use the connections between those data points to create neural [[00:16:47](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1007.1s)]
*  networks that can predictively find patterns. Yes. [[00:16:53](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1013.1800000000001s)]
*  I'm oversimplifying how a large language model works. [[00:16:57](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1017.5s)]
*  But no, that is exactly what it is. You pour all this data in and then the large language model [[00:16:59](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1019.9000000000001s)]
*  starts to learn the patterns of the human language and then it spits it back out at you. [[00:17:05](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1025.82s)]
*  And you can use those generations to test whether or not it's actually starting to learn [[00:17:09](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1029.8999999999999s)]
*  sophisticated concepts about the world because it's spitting it out in a way that is human [[00:17:18](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1038.06s)]
*  legible. And so that's the research case. But also there's a great commercial case for it [[00:17:22](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1042.78s)]
*  in that when you create systems that can talk to you, I mean, what a compelling product. [[00:17:31](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1051.66s)]
*  Well, this to me is... So the thing about chat GPT, there's two breakthroughs happening. [[00:17:38](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1058.38s)]
*  There's the fact that they bet on this scaling issue. We're just going to like, [[00:17:44](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1064.22s)]
*  we're going to get more compute than anyone. And this is just fundamentally, this is just [[00:17:48](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1068.5400000000002s)]
*  a numbers problem. Like basically, if you get enough data and you run enough GPUs, [[00:17:54](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1074.22s)]
*  you can get to something like the human mind. Yes. [[00:17:59](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1079.26s)]
*  That the human mind is just like a massively parallel processor. It just, because of its [[00:18:02](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1082.06s)]
*  cellular structure is beating our computers. But if we just do the electronic version that [[00:18:06](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1086.3799999999999s)]
*  gets to the scale, you'll produce stuff that's like what humans do. That's basically the theory, [[00:18:10](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1090.62s)]
*  right? That is the theory. Exactly. [[00:18:15](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1095.34s)]
*  And then they started going about... Doing that. [[00:18:17](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1097.58s)]
*  Doing it, right? The biggest data set, the most amount of GPUs, they're training the data, [[00:18:20](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1100.4599999999998s)]
*  they're scraping it. But to me, part of the genius is the interface. [[00:18:26](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1106.14s)]
*  Yeah. Because if you go back to this very famous [[00:18:31](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1111.18s)]
*  logician, mathematician, computer scientist and philosopher named Alan Turing, Alan Turing came [[00:18:36](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1116.3000000000002s)]
*  up with this thing when he was thinking about artificial intelligence called the Turing test. [[00:18:41](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1121.8200000000002s)]
*  And the idea of the Turing test was like, you can call a machine intelligent if you can chat [[00:18:46](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1126.7s)]
*  with it and not know it's a machine. Yeah. [[00:18:51](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1131.98s)]
*  And basically they took the Turing test and they were like, [[00:18:54](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1134.7s)]
*  we're going to use that as our main interface. [[00:18:57](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1137.82s)]
*  Right? I mean... Yeah. Well, you know, like there were a couple [[00:19:01](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1141.9s)]
*  of reasons that they did that. First of all, science fiction was a really big reason. [[00:19:05](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1145.82s)]
*  Altman had always been really obsessed with the movie Her and this idea that you can chat with [[00:19:12](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1152.22s)]
*  a model through a language interface. I think in part because he felt like this is the clearest [[00:19:18](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1158.7s)]
*  articulation of what AGI could possibly look like and avoid articulations about what this technology [[00:19:27](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1167.98s)]
*  is and what it should do. Also, you know, it's not a coincidence that ChatGBT was not the first chat [[00:19:34](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1174.78s)]
*  bot that started creating waves of hype around AI development. The very first chat bot to do that [[00:19:42](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1182.38s)]
*  was Eliza and it was built in the 1960s, 50s, sorry, maybe around like late 50s, early 60s. [[00:19:47](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1187.5s)]
*  And that was built by an MIT professor named Joseph Weissenbaum, who was actually doing it [[00:19:57](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1197.98s)]
*  as an experiment to see how easily can humans be duped when a computer starts talking to you [[00:20:03](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1203.1s)]
*  into believing that there might be intelligence hidden beneath the surface. And he was really [[00:20:09](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1209.66s)]
*  alarmed that actually people, it was a much simpler system. It wasn't- [[00:20:13](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1213.5s)]
*  Much, much, much, much. [[00:20:18](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1218.38s)]
*  Yeah. It wasn't statistical calculation, scraping the internet. It was rule-based. It literally [[00:20:20](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1220.3s)]
*  followed a method of Rogerian psychotherapy where you say, hey, I'm feeling bad today. [[00:20:26](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1226.54s)]
*  And then there were these rules behind Eliza that would say, why are you feeling bad today? Like it [[00:20:34](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1234.54s)]
*  would just copy and paste what you were saying and then flip the U to the I and your friend to my [[00:20:41](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1241.98s)]
*  friend and your whatever dad to my dad. It was following all these rules. And people- [[00:20:46](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1246.6200000000001s)]
*  It's like a role that you're hitting a tennis ball off of basically. [[00:20:53](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1253.58s)]
*  Yes. And people back then were declaring that AGI is already solved. I mean, they weren't using the [[00:20:57](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1257.42s)]
*  term AGI, but they were effectively saying that AGI is already solved. We will not need doctors [[00:21:03](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1263.66s)]
*  anymore. Psychotherapy is going to be automated. Teachers will be replaced. All jobs will go away. [[00:21:08](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1268.3s)]
*  And that obviously sounds incredibly familiar in the moment that we're in, but what it really [[00:21:15](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1275.4199999999998s)]
*  hits upon, it's not a coincidence that Eliza and Chachi B.T. function similarly. And when you think [[00:21:20](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1280.22s)]
*  about, I was watching a lecture from a computer science professor who was saying at Notre Dame, [[00:21:28](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1288.06s)]
*  who was saying, humans have anthropomorphized anything and everything for thousands of years. [[00:21:35](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1295.8200000000002s)]
*  And there is something deeply, it taps into our psychology to have things talk to us. [[00:21:44](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1304.22s)]
*  So I think that's why opening I ultimately chose an interface that was language based. [[00:21:54](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1314.46s)]
*  And what's amazing is so much of the discourse around it recalls the Oracle Adelphi, [[00:22:00](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1320.78s)]
*  like this irracular consultation where you go to the Oracle and you say, what should I do with my [[00:22:08](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1328.7s)]
*  life? People asking, even the other day where we were doing a story about the Elon Musk's AI model, [[00:22:13](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1333.98s)]
*  Grok, which is embedded in what used to be called Twitter. And it had started spouting all this [[00:22:23](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1343.82s)]
*  craziness about white genocide and kill the board because someone had clearly mucked with the code. [[00:22:30](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1350.14s)]
*  And at some point in this, someone asked Grok, like, well, what happened to you? [[00:22:35](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1355.5s)]
*  And Grok said, gave some answer. And we were talking about this in our editorial meeting. I [[00:22:40](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1360.3000000000002s)]
*  was like, you know, people were saying, well, I was like, well, we can't credit the answer. Like, [[00:22:45](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1365.74s)]
*  it doesn't actually know. Exactly. It's not like, oh, now it's telling the truth. There is no it [[00:22:50](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1370.38s)]
*  there to tell the truth. Like, it could be right or it could be wrong. But this the impulse we have [[00:22:58](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1378.3s)]
*  to view it as sentient is so overwhelming. It is so overwhelming. I mean, the fact that we can [[00:23:05](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1385.18s)]
*  anthropomorphize a rock in cartoons, rock is not talking to us. So the moment that you have [[00:23:11](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1391.18s)]
*  a chat interface that seems to be empathetic, that seems to understand your emotion, I mean, [[00:23:22](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1402.22s)]
*  that's even more powerful. So part of the story of open AI, among many stories is a story of like, [[00:23:28](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1408.3000000000002s)]
*  a kind of classic story of, oh, we're going to do this to save the world. We're going to we don't [[00:23:36](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1416.14s)]
*  want we don't want a profit driven AI model, we're going to be a nonprofit. And now like, [[00:23:41](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1421.26s)]
*  that's basically all fallen apart. Right? Yeah, completely. I mean, opening is potentially the [[00:23:46](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1426.06s)]
*  most capitalistic tech company ever. They just finished raising $40 billion at a $300 billion [[00:23:55](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1435.34s)]
*  valuation. And that's the largest fundraise of private tech investment money in the history of [[00:24:02](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1442.62s)]
*  Silicon Valley. So they changed the structure. They nested the for profit arm under the [[00:24:08](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1448.94s)]
*  nonprofit and for a while they called it capped profit arm because they were capping the amount of [[00:24:16](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1456.6200000000001s)]
*  returns that investors could get from their initial investment. They've now changed it to a PBC, [[00:24:22](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1462.78s)]
*  which is a for profit arm that is supposed to also have a bit of a social mission driven [[00:24:30](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1470.7s)]
*  like around it. But the but the core difference between what previously existed and what exists [[00:24:39](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1479.58s)]
*  now is that there's no cap to the returns anymore. Who owns it? [[00:24:44](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1484.22s)]
*  You know, that's an honestly, it's a really great question. And one of the things that [[00:24:52](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1492.54s)]
*  I realized in my reporting is, whereas Elon Musk is someone who plays legal offense, and he just [[00:24:57](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1497.9s)]
*  tries to sue everyone and everything in his way, Sam Altman plays legal defense where he creates [[00:25:04](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1504.78s)]
*  these such deeply convoluted structures. OpenAI is actually just one example of the different [[00:25:10](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1510.7s)]
*  types of structures that he's spun up throughout his career, that it becomes impossible to fully [[00:25:17](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1517.26s)]
*  understand who owns what, and where's the money flowing and what are the tax, what is the tax [[00:25:21](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1521.98s)]
*  situation. And this is something that even employees have mentioned to me, like, it takes an [[00:25:29](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1529.18s)]
*  army of lawyers to understand what is going on. And there have been moments when employees have [[00:25:36](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1536.8600000000001s)]
*  felt really in the dark themselves about like what even their rights are as employees, and they have [[00:25:44](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1544.46s)]
*  to spend a lot of money on legal services to try and comb through all the documentation that they [[00:25:50](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1550.86s)]
*  get because there's just so many nested entities. Given that Sam Altman is now one of the most [[00:25:57](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1557.1s)]
*  powerful people in the country, which I think it's fair to say, and maybe the world, they're [[00:26:01](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1561.5s)]
*  starting to do development with, they just announced some new, they're going to do development with [[00:26:05](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1565.74s)]
*  countries, they just announced a partnership with the former Apple designer, Johnny Ive today. [[00:26:10](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1570.3s)]
*  You know, they say, I mean, again, one of the craziest things, if you listen to the people [[00:26:16](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1576.2199999999998s)]
*  generating this technology, they're like, yeah, it's definitely gonna be the most dangerous thing [[00:26:20](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1580.78s)]
*  that's ever hit human beings. Like all of them say that. There's a line in your book, and that was [[00:26:24](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1584.3799999999999s)]
*  in the excerpt in the Atlantic about, you know, we'll have to get into a bunker before we release, [[00:26:30](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1590.22s)]
*  you know, artificial general intelligence. We could talk about what that means. Can you just [[00:26:35](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1595.74s)]
*  give me a little character sketch of Sam Altman? I find him a very difficult person to get a read on. [[00:26:39](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1599.74s)]
*  You're not the only one. Yeah, I mean, I think that's his shtick. That is, you know, [[00:26:47](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1607.3400000000001s)]
*  the thing that was weirdest to me when I was reporting, and just everyone that I interviewed, [[00:26:52](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1612.3s)]
*  I would ask them, so what do you think about Sam? And no matter how long they had worked with him, [[00:26:58](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1618.06s)]
*  or how closely they had worked with him, no one could actually articulate what this man believes [[00:27:03](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1623.18s)]
*  and what his deal is. And I got to a point where I learned in my reporting to stop asking people, [[00:27:08](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1628.46s)]
*  what do you think about Sam? I would be like, what did Sam tell you in such and such meeting? [[00:27:14](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1634.62s)]
*  Or like, what did Sam tell you about what he believed in this particular situation? What did [[00:27:19](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1639.34s)]
*  Sam tell you to motivate you to do X? And what they would say, what the sources would say, [[00:27:24](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1644.2199999999998s)]
*  Sam would tell them exactly what that source believed. And it would, what he said changed [[00:27:32](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1652.6999999999998s)]
*  from person to person based on that specific individual. And so no one really, yeah, [[00:27:39](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1659.82s)]
*  like has a grout. But like, he is a once in a generation storytelling talent, who has a loose [[00:27:46](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1666.46s)]
*  relationship with the truth. And he's incredibly persuasive. So he can paint the vision of the [[00:27:51](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1671.82s)]
*  future. And he has this really profound ability to understand what people need to hear and what [[00:27:58](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1678.1399999999999s)]
*  they want and to frame everything. You join me because I can get you what you want. And he can [[00:28:05](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1685.1s)]
*  sort of kind of say whatever he wants in that meeting, because it doesn't necessarily need to [[00:28:14](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1694.78s)]
*  be grounded in a reality. There's also been a series of kind of, and this is chronicled in the [[00:28:20](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1700.3799999999999s)]
*  book of kind of whistleblowers and dissidents and people who have been really alarmed by the [[00:28:26](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1706.2199999999998s)]
*  direction he's taken it. Tell me a little bit about who those folks are and what are the concerns [[00:28:31](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1711.58s)]
*  they've raised about the trajectory of open AI under Sam Altman. Yeah, so you were saying in [[00:28:36](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1716.54s)]
*  the beginning that there's sort of these really dramatic, different, dramatically different [[00:28:42](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1722.14s)]
*  narratives about AI and dominating discourse today, which is AGI will bring us utopia or AGI [[00:28:47](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1727.18s)]
*  will kill us all. And colloquially, they're called the boomers and the doomers, with the boomers [[00:28:53](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1733.66s)]
*  being like the pauses and the doomers being the negatives. To me, those are actually two sides [[00:29:02](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1742.7s)]
*  of the same coin because they both camps believe in the AGI religion, as I call it. That AGI [[00:29:08](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1748.38s)]
*  is almost here, that it's achievable, it's just around the corner, that it's going to be profoundly [[00:29:16](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1756.8600000000001s)]
*  affecting a society. Can we just stop there and can you just explain what AGI means? [[00:29:22](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1762.3s)]
*  Okay, AGI refers to artificial general intelligence, and this is an incredibly [[00:29:28](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1768.6999999999998s)]
*  poorly defined term. The very hand wavy summary of what people typically describe it as is [[00:29:34](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1774.06s)]
*  an AI system that can ultimately do anything that humans can do. But this is a really challenging [[00:29:41](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1781.4199999999998s)]
*  measure because what makes humans intelligent doesn't have any scientific consensus. [[00:29:49](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1789.82s)]
*  So when you're trying to capture in software something that we don't understand [[00:29:57](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1797.4199999999998s)]
*  about humans, you end up with lots of different opinions about how to do it, what it should look [[00:30:01](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1801.34s)]
*  like, who it should serve, all those things. So throughout the decades of AI research and [[00:30:07](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1807.1799999999998s)]
*  development, all the way from the 1950s until present day, there have been just tons and tons [[00:30:12](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1812.9399999999998s)]
*  of debates, egos clashing, opinions clashing about these kind of core questions of what AI [[00:30:18](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1818.38s)]
*  and AGI ultimately is. The way that OpenAI has specifically defined it is highly autonomous [[00:30:26](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1826.7800000000002s)]
*  systems that outperform humans in most economically valuable work. And so they've specifically [[00:30:34](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1834.6200000000001s)]
*  defined it as a labor automating machine. That is also a really key important dimension to [[00:30:41](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1841.5s)]
*  understanding the truly deeply capitalistic nature of OpenAI. And also to understanding the [[00:30:48](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1848.46s)]
*  trajectory that they're taking as a company. Ultimately, they are trying to build systems [[00:30:56](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1856.38s)]
*  that they can sell to CEOs for a lot of money. To replace humans. To automate human work. [[00:30:59](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1859.66s)]
*  Automate away. Yeah. If they're trying to build systems that outperform humans at the thing that [[00:31:04](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1864.7s)]
*  makes people want to pay you, you're no longer going to be paid. They're just going to opt for the AI. [[00:31:11](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1871.98s)]
*  And to your point about the boomers and doomers, the kind of utopians and dystopians being two [[00:31:19](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1879.34s)]
*  sides of the same coin, that coin being that this level of generalized intelligence that [[00:31:23](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1883.98s)]
*  outperforms human and economic relevant task is achievable and around the corner. [[00:31:29](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1889.02s)]
*  Is the core faith that unites them? Yes. And that in and of itself, whether or not artificial [[00:31:34](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1894.46s)]
*  general intelligence is achievable and whether it even has a well-defined definition, that is [[00:31:40](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1900.06s)]
*  a scientific debate. There was a really great story in the New York Times that was just highlighting [[00:31:47](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1907.02s)]
*  this by Cade Metz. And the headline was, why we're unlikely to get artificial general intelligence [[00:31:50](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1910.78s)]
*  anytime soon. And there was a specific line, in a recent survey of the Association for the [[00:31:57](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1917.58s)]
*  Advancement of Artificial Intelligence, a 40-year-old academic society, more than three quarters of [[00:32:03](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1923.4199999999998s)]
*  respondents said the methods used to build today's technology were unlikely to lead to AGI. [[00:32:09](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1929.26s)]
*  That's why I call it an AGI religion. There is a faith that people have that's not grounded in [[00:32:15](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1935.8999999999999s)]
*  scientific evidence that AGI is coming, it's coming soon, it's imminent, and that it's going [[00:32:22](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1942.54s)]
*  to be dramatically transformative, i.e. utopia or dystopia. And ultimately, the conclusion that both [[00:32:29](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1949.6599999999999s)]
*  camps have, both the boomers and the do-mers, is because it will have such high consequences [[00:32:38](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1958.06s)]
*  for humanity, it has the potential to bring humanity to heaven or to hell, they're the ones [[00:32:43](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1963.58s)]
*  that have to control it. The boomers are the ones that have to control it, the do-mers are [[00:32:48](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1968.62s)]
*  the ones that have to control it. Their adherents of their sub-sex of this religion need to be the [[00:32:52](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1972.54s)]
*  ones that usher humanity to the next era. I want to take a step back from one of the things that's [[00:33:00](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1980.54s)]
*  hard to sort of disentangle is the myth-making and the storytelling and the projections of the future [[00:33:06](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1986.54s)]
*  from the actual technology itself. And I really find this difficult because I do find myself, [[00:33:11](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1991.7399999999998s)]
*  I feel a little bit like one of those lame centrists who's like, well both sides get it wrong, [[00:33:18](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=1998.22s)]
*  but I do feel that way a little bit about the discourse here where there's a certain group [[00:33:21](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2001.98s)]
*  who are like, this is all slop and schlock and BS and garbage and it doesn't do anything useful. [[00:33:26](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2006.22s)]
*  And then there's people who are like, you're going to marry an AI. And neither of those sort [[00:33:33](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2013.02s)]
*  of seem right to me. It very clearly seems profoundly useful and incredibly impressive [[00:33:39](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2019.26s)]
*  and sophisticated technology. It already does stuff that was not conceivable to me three years [[00:33:44](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2024.46s)]
*  ago. It is capable of doing things I did not think computers could do three years ago. [[00:33:49](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2029.98s)]
*  It has a million different use cases, very obviously, many of which I think are really [[00:33:57](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2037.02s)]
*  worrisome for the human welfare labor market implications. But I guess, how do you think about [[00:34:02](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2042.3s)]
*  the technology itself independent of these, the religion and the story and the projections of the [[00:34:10](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2050.14s)]
*  future? Yeah, totally. I mean, there's, there's a, to me, there's a clear explanation for why [[00:34:16](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2056.06s)]
*  there's such a fragmented set of opinions about whether this technology is useful or not. It's [[00:34:22](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2062.94s)]
*  because the technology can be highly useful for very specific things and it can fall apart very [[00:34:28](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2068.94s)]
*  quickly for other things. For example, if you switch out of the English language, it suddenly [[00:34:34](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2074.7s)]
*  becomes much more difficult to use and much less effective. Yeah. And that's because of the, just [[00:34:39](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2079.42s)]
*  the data set of English is so enormous that it's trained on. Yes. And also because most of these [[00:34:44](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2084.7000000000003s)]
*  companies only do their testing in English. So they only do their content moderation in English. [[00:34:50](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2090.86s)]
*  They only do their refining and tuning and all of these things in English. And they also, you know, [[00:34:56](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2096.46s)]
*  like open AI will use test cases internally where they will test the model on its ability to talk [[00:35:02](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2102.54s)]
*  about AI research topics. So even the topic selection for like the stress testing of their [[00:35:11](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2111.02s)]
*  technology becomes hyper-specific. And so people within the AI world are like, this technology is [[00:35:17](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2117.58s)]
*  incredible because everything that it does is stress tested on the things that they need done. [[00:35:24](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2124.86s)]
*  It's so funny because it's funny you say this because the thing that I have most used AI for [[00:35:32](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2132.54s)]
*  is to teach me about AI and it's been kind, it's actually been pretty amazing at it. [[00:35:37](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2137.1800000000003s)]
*  It's actually the one thing like I have had a long running sort of self tutorial about AI [[00:35:42](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2142.1400000000003s)]
*  with chat GPT and deep seek. And it's been very good. Like, you know, basically I started with [[00:35:50](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2150.06s)]
*  like, what's a neural network? I don't understand this. How does this work? And walking me through [[00:35:56](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2156.86s)]
*  and then building up from that and predict. And then, you know, I want you to explain there's a [[00:36:00](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2160.3s)]
*  famous paper in AI that sort of transformed things called attention is all you need. And explain this [[00:36:05](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2165.5s)]
*  paper to me. Like I don't have a computer science degree, like explain the paper to me. And then [[00:36:13](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2173.1800000000003s)]
*  asking more questions. And it's been amazing at that. I have to say, [[00:36:17](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2177.9s)]
*  you have hit upon the exact use case that is the most like, if you had to use these technologies [[00:36:21](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2181.9s)]
*  for anything, this is like the one use case that it is going to knock out of the park [[00:36:30](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2190.7000000000003s)]
*  time and time again, because that is exactly what, what these companies stress test the [[00:36:36](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2196.14s)]
*  technologies on. But if you move towards, you know, like a particularly esoteric [[00:36:40](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2200.3s)]
*  piece of cultural analysis, I don't know, or like a historical fact of a particular tribe in a [[00:36:45](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2205.8999999999996s)]
*  certain region of the world, as far from Silicon Valley, like that's when you're going to get some [[00:36:51](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2211.5s)]
*  like really wild stuff coming out of the models that has no bearing on reality. [[00:36:56](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2216.06s)]
*  I mean, part of what at a philosophical level, here's part of what I think we're wrestling with, [[00:36:59](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2219.8199999999997s)]
*  because it really does stoke profound questions about what human intelligence is to your point [[00:37:05](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2225.8999999999996s)]
*  about like, the question of what, whether we have artificial generalized intelligence is in some [[00:37:10](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2230.22s)]
*  ways dependent on how we understand what human intelligence is, and that's an unsolved problem. [[00:37:14](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2234.54s)]
*  But I want to, I want to give an example of an early version of AI that we've all been living [[00:37:20](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2240.46s)]
*  with for a long time that I thought about many times as a sort of philosophical experiment. [[00:37:25](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2245.1s)]
*  And that is the algorithm banks use to detect fraud. So what that's doing is it's just scanning [[00:37:30](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2250.38s)]
*  a ton of data and it's making predictions. And then when you do something that deviates so much [[00:37:36](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2256.94s)]
*  from that prediction, they're like, ah, red flag, right? So it's just a predictive, it's scraping [[00:37:43](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2263.34s)]
*  a bunch of data, it's building a bunch of models using neural networks that say, oh, this is usually [[00:37:47](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2267.98s)]
*  how these people operate, this is usually what their behavior. And what's always amazed me was [[00:37:54](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2274.14s)]
*  how good they were, because sometimes I would make a purchase where I was like, that was kind of an [[00:37:58](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2278.86s)]
*  impulse purchase, and the bank wouldn't flag it. And I'd be like, huh, I thought I had human [[00:38:04](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2284.62s)]
*  autonomy and human agency. When at a whim, I walked into this place and bought this thing. [[00:38:11](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2291.18s)]
*  But it turns out that wasn't enough of an autonomous decision to so wildly deviate from [[00:38:17](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2297.3399999999997s)]
*  what some algorithmic prediction of my behavior would be that it would flag a fraud thing. [[00:38:23](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2303.8199999999997s)]
*  And then at the same time, when someone once got my credit card and bought like 30 pairs of shoes [[00:38:27](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2307.98s)]
*  at a place, you know, in I forget where, like, yeah, they did flag it, they flagged it. They knew. [[00:38:33](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2313.5s)]
*  And so there's something about that that's kind of profound, right? Because I know it's so funny. [[00:38:40](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2320.62s)]
*  Okay. Because this, like, I get flagged for fraudulent activity all the time when it's, [[00:38:45](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2325.42s)]
*  like, incorrectly and this is like, this means that you're a more fully formed, [[00:38:51](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2331.98s)]
*  unique subject and agent. No, no, a lame, predictable, walking automaton. [[00:38:56](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2336.54s)]
*  No, I'm serious. There's a profound philosophical point here. [[00:39:04](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2344.14s)]
*  No, what I know what this means is that there is a specific profile. I mean, this is this has been [[00:39:07](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2347.9s)]
*  studied for a long time. There's bias embedded in all kinds of AI systems. And they're, they train [[00:39:14](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2354.2200000000003s)]
*  on certain types of data. And it will work really well when you match that training data. But it [[00:39:20](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2360.06s)]
*  will not like it will not work once you drift out of that distribution. And it's called drift. [[00:39:27](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2367.1000000000004s)]
*  When you're applying an AI system to something that it wasn't actually intended to be used on. [[00:39:33](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2373.98s)]
*  And this is one of the challenges with I mean, this is this is a challenge with all types of AI [[00:39:41](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2381.58s)]
*  systems, but predictive AI systems, as you were talking about with the bank fraud, [[00:39:45](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2385.98s)]
*  it is an easier problem to solve, because it is a very specific use case, you can stress test it [[00:39:51](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2391.5s)]
*  with kind of as much imagination as possible of all the ways that it could, it could go wrong. [[00:39:57](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2397.58s)]
*  And you get where you get definite concrete feedback, like in every case, you confirm this [[00:40:02](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2402.8599999999997s)]
*  was fraud, this wasn't fraud. Yeah, so you go back to the training model, and you say you got this [[00:40:08](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2408.7s)]
*  right, or you got this wrong. And there's always this definitive as opposed to like, write a paper, [[00:40:12](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2412.9399999999996s)]
*  right? Like, yes, is it good? Is it bad? Like, this is just a it's a binary thing, you got it right or [[00:40:18](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2418.7799999999997s)]
*  wrong. Yeah, exactly. And the thing the challenge with generative AI, or what people now increasingly [[00:40:23](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2423.66s)]
*  call general AI systems is that they're meant to be everything machines, they're meant to do [[00:40:29](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2429.26s)]
*  everything for anyone, but actually, they only do some things for some people. And there are people [[00:40:35](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2435.82s)]
*  that exist. That's a great point, right? Yeah, there are people that exist out of the target [[00:40:42](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2442.7000000000003s)]
*  audience that are doing things that are out of the target tasks, where everything starts to fall apart. [[00:40:46](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2446.78s)]
*  So if you're a boring Brooklyn dad, who just does just like, lamely goes through his life, [[00:40:52](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2452.38s)]
*  he brings his kids to sports, he buys the stuff that he buys is like, like, [[00:40:57](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2457.5s)]
*  trade on the target data, then it works. And then you're specifically asking about AI, the exact [[00:41:02](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2462.78s)]
*  topic that they stress test. Yeah, but it's seamless. That's right. But if you're, you know, [[00:41:09](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2469.26s)]
*  if you're, you're a more unpredictable person, or you're or you're not, you know, you're not in this [[00:41:15](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2475.98s)]
*  incredibly nailed target demo, right? Yeah, you're not in the societal norm of the society that [[00:41:21](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2481.34s)]
*  Silicon Valley defines to be the norm. Right. Then things start to fall apart. And it doesn't, [[00:41:27](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2487.98s)]
*  you know, like, that's, yeah, that's what that's, this is actually a perfect illustration of that, [[00:41:34](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2494.1400000000003s)]
*  right? For you to say, I get flagged with this all the time. Like, I need to be like, I never have, [[00:41:37](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2497.5800000000004s)]
*  is it gives these guys knows me backwards and forwards, like, it does actually kind of perfectly [[00:41:42](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2502.78s)]
*  illustrate the point. Yeah, this is actually one of my greatest frustrations when I'm traveling, [[00:41:47](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2507.34s)]
*  because like, you're you, it's so automated now that you can't tell your bank No, that was not [[00:41:52](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2512.1400000000003s)]
*  fraud, like make me make allow me to make this damn purchase, like, I want this thing. And now I just [[00:41:57](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2517.6600000000003s)]
*  have to carry like three credit cards with me. Okay, so this to me is actually a place where [[00:42:03](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2523.58s)]
*  we're zeroing in on what I think is sort of an important, again, like middle place, which is that [[00:42:11](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2531.1800000000003s)]
*  like, this idea that it's just gonna it's gonna, it's this sort of panacea that's every it's a [[00:42:15](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2535.5s)]
*  universal toolkit and all things for all people. Yeah. For the reasons you enunciated a how [[00:42:20](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2540.94s)]
*  difficult it is to achieve that and be the fact that like the training data really matters and who [[00:42:26](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2546.3s)]
*  what data it's training on and for whom based on what we're in the distribution, right? Yeah. [[00:42:30](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2550.86s)]
*  That said, it also just seems clearly the case that from this, like, labor replacement [[00:42:36](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2556.14s)]
*  project, right? Let's say, you know, that's the project economically, which we can talk about the [[00:42:45](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2565.1s)]
*  implications of, you know, like, okay, fine, brief writing, there's a lot of first year [[00:42:49](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2569.66s)]
*  associates in this country, they make a fair amount of money. I mean, you know, not as much [[00:42:55](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2575.42s)]
*  as the people above them. But it's a well, it's a relatively remunerative job. Let's just train at [[00:42:59](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2579.98s)]
*  how to do first year associate work. And that seems like, yeah, that that seems like it's, [[00:43:06](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2586.46s)]
*  it's probably up to that task. Yeah, 100. So there's a part in the book, I got access to a trove of [[00:43:12](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2592.22s)]
*  documents that opening I specifically used to detail how the model should be training on what [[00:43:19](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2599.7400000000002s)]
*  tasks it should be trained on. And so there's a part of the book that writes about this, the thing [[00:43:25](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2605.98s)]
*  and there was there's literally a line that I quote in one of the documents that says, think about [[00:43:31](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2611.42s)]
*  all of the things that people might want to do that are economically valuable, like creating art, [[00:43:35](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2615.98s)]
*  writing film scripts, writing emails, summarizing briefs, like and it talks about all the and like [[00:43:41](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2621.02s)]
*  think about the industries. And it basically starts listing all of the lucrative industries, [[00:43:47](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2627.98s)]
*  entertainment, media, finance. And those are they, even as they're trying to paint this as [[00:43:52](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2632.14s)]
*  it can do anything internally, what they're actually doing is enumerating a list of the most [[00:44:01](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2641.98s)]
*  economically valuable tasks that they want to focus on. And so that is yeah, you're exactly right, [[00:44:08](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2648.3799999999997s)]
*  like that that is ultimately their stated goal. That is ultimately what they're doing internally. [[00:44:14](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2654.8599999999997s)]
*  And that is the impact that it will have. I mean, and that's that's where we end up to me where [[00:44:19](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2659.34s)]
*  the thing that seems most likely and in some ways the most unnerving is actually building a quite [[00:44:24](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2664.78s)]
*  effective white collar labor automation machine. And what I find pretty unnerving about this, [[00:44:30](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2670.6200000000003s)]
*  and I've been thinking about this a lot, and I'm actually kind of working maybe on writing something [[00:44:37](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2677.1000000000004s)]
*  about this. But as we sit here, and we look at the last 30 or 40 years of automation and neoliberal [[00:44:41](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2681.1000000000004s)]
*  trade policy that basically kind of took the country's manufacturing base, the possibility of [[00:44:46](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2686.14s)]
*  like a unionized stable job with a high school degree that you could, you know, support a family [[00:44:51](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2691.3399999999997s)]
*  and buy a house and maybe go on vacation, right, and kind of destroyed that. And it dislocated [[00:44:57](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2697.74s)]
*  people geographically and it annihilated entire like towns and areas. And as we look back on that [[00:45:02](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2702.2999999999997s)]
*  40 years later, and we think, maybe that didn't go so well, right? That's like the now the new [[00:45:08](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2708.7799999999997s)]
*  consensus. You got yelled at if you said that in 1999, but now new consensus. At the same time [[00:45:13](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2713.82s)]
*  we're doing that, it's like, hey, what if we do that to all the white collar workers too? [[00:45:19](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2719.5s)]
*  Yeah. Yeah. Which is the explicit project they're trying to do. [[00:45:23](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2723.82s)]
*  Yeah. And the thing that I'll add, I guess to go back to your question of like, what do I actually [[00:45:28](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2728.38s)]
*  think about this technology is I don't think it requires this technology to be that highly [[00:45:32](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2732.7000000000003s)]
*  performative in order for this reality to come to fruition. You're saying the substitution [[00:45:39](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2739.02s)]
*  effects, like the labor automation. Yes. Yes. Because if you are a worker going to the negotiating [[00:45:43](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2743.74s)]
*  table and the CEO on the other side and you on this side believe that AI could replace you, [[00:45:49](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2749.1s)]
*  even if it can't actually effectively do so, you have no more bargaining power and you're going to [[00:45:56](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2756.7799999999997s)]
*  get fired and replaced. And we're seeing that already literally with companies announcing, [[00:46:01](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2761.8999999999996s)]
*  I mean, Microsoft just announced that they want to make 50% of their code base AI generated, [[00:46:07](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2767.02s)]
*  and then they laid off 6,000 of their workers. They're going to do all the rhetorical maneuvering [[00:46:12](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2772.38s)]
*  possible to say those are not correlated, but of course they are. And there are other companies [[00:46:19](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2779.5s)]
*  that have been much more explicit saying, we are now entering an AI era. We need a strength head [[00:46:23](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2783.34s)]
*  count. We need every single team to start increasing their output by using AI tools. And they're doing [[00:46:28](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2788.38s)]
*  layoffs at the same time. And there was recently a story where one of these companies fired all the [[00:46:33](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2793.58s)]
*  workers and then went, whoops, AI turns out is not that good. Can you guys all come back? [[00:46:39](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2799.18s)]
*  Okay. But see, this is where actually I do think that it's not all just storytelling, right? Like [[00:46:44](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2804.54s)]
*  in the end, it is going to matter whether it can do the job or not. Sometimes, but you know, [[00:46:49](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2809.5s)]
*  I mean, like if you're a law firm and you're the one that gets rid of all your first year associates, [[00:46:57](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2817.3399999999997s)]
*  and then you start producing briefs that like there was a brief famously that was circulating [[00:47:01](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2821.4199999999996s)]
*  recently that like had a bunch of citations of cases that don't exist. Like that's a big risk. [[00:47:05](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2825.5s)]
*  Like, you know, someone's paying you a thousand or $2,000 an hour to produce work. They're not [[00:47:10](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2830.7s)]
*  going to be happy if you do that. If it's sloppy. Right. So in the labor case, I think there is [[00:47:16](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2836.14s)]
*  possibly a ability to reverse. Like if you make too early of a call and you try to replace all [[00:47:21](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2841.58s)]
*  your workers with AI and then it doesn't work out, you hire back the workers. But there are [[00:47:28](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2848.14s)]
*  lots of other things that this hype around AI is driving forward that is not reversible. [[00:47:32](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2852.3s)]
*  So for example, the amount of data centers and supercomputers that are being laid all around the [[00:47:39](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2859.82s)]
*  world to power these astronomically large models. Once the brick is laid, you can't just suddenly, [[00:47:45](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2865.26s)]
*  you know, oops, we didn't actually need that. Let's just delete the data center. It's already [[00:47:54](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2874.1400000000003s)]
*  there. And it's already changed the way that a country or a town has designed their utility grid. [[00:47:58](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2878.94s)]
*  It's changed the positioning of power plants and how they're distributed around the world. [[00:48:05](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2885.66s)]
*  It's changed people's access to fresh water resources because these data centers actually [[00:48:10](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2890.06s)]
*  need to be cooled with fresh water resources. So that to me is actually the most concerning [[00:48:14](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2894.46s)]
*  impacts that we're seeing with the current Silicon Valley quest to AGI is that it's actively [[00:48:21](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2901.66s)]
*  terraforming the earth, reforming political alliances, reforming economic structures in [[00:48:28](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2908.86s)]
*  ways that will be extremely hard to reverse even when the bubble pops, if the bubble pops. [[00:48:34](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2914.86s)]
*  RG It sounds like you think it will. [[00:48:40](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2920.0600000000004s)]
*  MS I think the most likely scenario is that the bubble will pop, but I do not want to estimate [[00:48:41](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2921.98s)]
*  how good Silicon Valley is at continuing to perpetuate the bubble with even more intense [[00:48:52](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2932.38s)]
*  storytelling. And there is, I think, a very, very narrow path in which the bubble won't pop [[00:48:59](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2939.7400000000002s)]
*  because of the rhetorical footwork that is done. But that specific path, I think, will be [[00:49:07](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2947.6600000000003s)]
*  very dangerous. RG Karen Howe is the author of Empire of AI, [[00:49:14](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2954.46s)]
*  Dreams and Nightmares and Sam Altman's Open AI. It's a great book. It is out now. [[00:49:20](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2960.06s)]
*  I learned a lot from it. And that was really, really, really enlightening. Thank you so much. [[00:49:24](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2964.38s)]
*  MS Thank you so much, Chris. [[00:49:28](https://www.youtube.com/watch?v=5eU4t7Euxv4&t=2968.38s)]
