---
Date Generated: December 06, 2024
Transcription Model: whisper medium 20231117
Length: 8492s
Video Keywords: ['Joe Rogan Experience', 'JRE', 'Joe', 'Rogan', 'podcast', 'MMA', 'comedy', 'stand', 'up', 'funny', 'Freak', 'Party', 'Joe Rogan', 'Tristan Harris', 'The Social Dilemma', 'JRE #1558', 'comedian', 'Facebook', 'Twitter', 'Instagram', 'Google', 'Apple', 'Android', 'iPhone']
Video Views: 9850310
Video Rating: None
Video Description: Called the “closest thing Silicon Valley has to a conscience,” by The Atlantic magazine, Tristan Harris spent three years as a Google Design Ethicist developing a framework for how technology should “ethically” steer the thoughts and actions of billions of people from screens. He is now co-founder & president of the Center for Humane Technology, whose mission is to reverse ‘human downgrading’ and re-align technology with humanity. Additionally, he is co-host of the Center for Humane Technology’s Your Undivided Attention podcast with co-founder Aza Raskin.
---

# Joe Rogan Experience #1558 - Tristan Harris
**Joe Rogan Experience:** [October 30, 2020](https://www.youtube.com/watch?v=OaTKaHKCAFg)
*  Joe Rogan podcast, checking in.
*  The Joe Rogan Experience.
*  Train by day, Joe Rogan podcast by night, all day.
*  Tristan, how are you?
*  Good.
*  Good to be here.
*  Good to have you here, man.
*  You were just telling me before we went on air the numbers of the social dilemma and
*  their bonkers.
*  So just say that.
*  Yeah.
*  The social dilemma was seen by 38 million households in the first 28 days on Netflix,
*  which I think is broken records.
*  And if you assume, you know, a lot of people are seeing it with their family because parents
*  seeing it with their kids, the issues that are on teen mental health.
*  So if you assume one out of 10 families saw it with a few family members, we're in the
*  40 to 50 million people range, which is just broken records, I think, for Netflix.
*  I think it was the second most popular documentary throughout the month of September.
*  I think.
*  Of course, or film throughout the month of September.
*  Is really well done documentary.
*  But I think it's one of those documentaries that affirmed a lot of people's worst suspicions
*  about the dangers of social media.
*  And then on top of that, it sort of alerted them to what they were already experiencing
*  in their own personal life and like highlighted it.
*  Yeah, I think that's right.
*  I mean, most people were aware.
*  I think it's a thing everyone's been feeling that the feeling you have when you use social
*  media isn't that this thing is just a tool or it's on my side.
*  It is an environment based on manipulation, as we say in the film.
*  And that's really what's changed that, you know, I remember, you know, I was working
*  on these issues for something like eight or eight years or something now.
*  You please tell people who didn't see the documentary what your background is and what
*  how you got into it.
*  Yes.
*  So I.
*  You know, the film goes back as a set of technology insiders.
*  My background was as a design ethicist at Google.
*  So I first had a startup company that we sold to Google and I landed there through a talent
*  acquisition and then started work about a year into being at Google, made a presentation
*  that was about how essentially technology was holding the human collective psyche in
*  its hands, that we were really controlling the world's psychology, because every single
*  time people look at their phone, they are basically experiencing thoughts and scrolling
*  through feeds and believing things about the world.
*  This has become the primary meaning making machine for the world and that we as Google
*  had a moral responsibility to, you know, hold the collective psyche in a thoughtful, ethical
*  way and not create this sort of race to the bottom of the brain stem attention economy
*  that we now have.
*  So my background was as a kid, I was a magician.
*  We can get into that.
*  I studied at a lab at Stanford called or studied in a class called the Stanford persuasive technology
*  class that taught a lot of the engineers at Silicon Valley, kind of how the mind works.
*  And the co-founders of Instagram were there and then later studied behavioral economics
*  and how the mind is sort of influenced.
*  I went into cults and started studying how cults work and then arrived at Google through
*  this lens of, you know, technology isn't really just this thing that's in our hands.
*  It's more like this manipulative environment that is tapping into our weaknesses, everything
*  from the slot machine rewards to the way you get tagged in a photo and it sort of manipulates
*  your social validation and approval, these kinds of things.
*  When you were at Google, did they still have the don't be evil sign up?
*  I don't know if there's actually a physical sign.
*  Was there was never a physical sign?
*  I thought there was something that they actually had.
*  I think it was there is this guy.
*  Was it Paul?
*  Not Paul.
*  What was his last name?
*  He was the inventor, one of the inventors of Gmail and they had a meeting and they came
*  up with this mantra because they realized the power that they had and they realized
*  that there is going to be a conflict of interest between advertising on the search results
*  and regular search results.
*  And so we know that they knew that they could have used that power.
*  And they came up with this mantra, I think, in that meeting in the early days to don't
*  be don't be evil.
*  There was a time where they took that mantra down and I remember reading about it online
*  and they took it off their page, I think.
*  That's what it was.
*  Yeah.
*  And when I read that, I was like, that should be big news.
*  Like, there's no reason to take that down.
*  Why would you take that down?
*  Yeah.
*  Why would you why would you say, well, maybe give you a little evil?
*  Let's not get crazy.
*  It's a good question.
*  I mean, I wonder what logic would have you remove a statement like that.
*  That seems like a standard state.
*  Like, it's a great statement.
*  OK, here it is.
*  Google removes don't be evil clause from its code of conduct in 2018.
*  Yeah.
*  Yeah.
*  I wonder why.
*  Did they have an explanation?
*  Did it say anything?
*  Don't be evil has been a part of the company's corporate code of conduct since 2000 when
*  Google was reorganized under a new patent parent company, Alphabet, in 2015.
*  Alphabet assumed a slightly adjusted version of the model.
*  Do the right thing.
*  Yeah.
*  Do the right thing.
*  Oh, that's a Spike Lee movie, bitch.
*  However, Google retained its original don't be evil language until the past several weeks.
*  The phrase has been deeply incorporated into Google's company culture, so much so that
*  a version of the phrase has served as the Wi-Fi password on the shuttles that Google
*  uses to ferry its employees to its Mountain View headquarters.
*  I think I remember that.
*  Yeah.
*  Wow.
*  You get on the bus and you type in don't be evil.
*  I wonder why they decided.
*  Well, I mean, they did change it to do the right thing.
*  I mean, we always used to say that just to friends, not within Google, but just, you
*  know, instead of saying don't be evil, just say let's do some good here, right?
*  That's nice.
*  Let's do some good here.
*  Yeah.
*  Think positive.
*  And then you're doing good instead of don't do bad.
*  Yeah.
*  But the problem is when you say do good, the question is who's good because you live in
*  a morally plural society.
*  And there's this question of who are you to say what's good for people.
*  And it's much easier to say let's reduce harms than it is to say let's actually do good like
*  this.
*  It says the updated version of Google's code of conduct still retains one reference to
*  the company's unofficial motto.
*  The final line of the document is still and remember dot dot dot don't be evil.
*  And if you see something that you think isn't right, speak up.
*  Hmm.
*  Okay.
*  Well, they still have don't be evil.
*  So maybe it's much ado about nothing.
*  But having that kind of power, we were just before the podcast, we were watching Jack
*  Dorsey speak to members of the Senate in regards to Twitter censoring the Hunter Biden story
*  and censorship of conservatives, but allowing dictators to spread propaganda, dictators
*  from other countries and why and what this is all about.
*  One of the things that Jack Dorsey has been pretty adamant about is that they really never
*  saw this coming when they started Twitter.
*  And they didn't think that they were ever going to be in this position where they were
*  going to be really the arbiters of free speech for the world, which is essentially in some
*  ways what they are.
*  I think it's important to roll back the clock for people because it's easy to think, you
*  know, that we just sort of landed here and that they would know that they're going to
*  be influencing the global psychology.
*  But I think we should really reverse engineer for the audience.
*  How did these products work the way that they did?
*  So let's go back to the beginning days of Twitter.
*  I think his first tweet was something like checking out the buffaloes in Golden Gate
*  Park in San Francisco.
*  You know, Jack was fascinated by the taxicab dispatch system that you could send a message
*  and then all the taxis get it.
*  And the idea is, could we create a dispatch system so that I post a tweet and then suddenly
*  all these other people can see it?
*  And the real genius of these things was that they weren't just offering this thing you
*  could do.
*  They found ways of keeping people engaged.
*  I think this is important for people to get that they're not competing for your data or
*  for money.
*  They're competing to keep people using the product.
*  And so when Twitter, for example, invented this persuasive feature of the number of followers
*  that you have, remember, like that was a new thing at the time, right?
*  You log in and you see your profile.
*  Here's the people who you can follow.
*  And then here's the number of followers you have.
*  That created a reason for you to come back every day to see how many followers do I have.
*  So that was part of this race to keep people engaged.
*  As we talk about in the film, like these things are competing for your attention, that if
*  you're not paying for the product, you are the product.
*  But the thing that is the product is your predictable behavior.
*  You're using the product in predictable ways.
*  I remember a conversation I had with someone at Facebook who was a friend of mine who said
*  in a coffee shop one day, people think that we Facebook are competing with something like
*  Twitter, that one social network is competing with another social network.
*  But really, he said, our biggest competitor is YouTube because they're not competing for
*  social networks.
*  They're competing for attention.
*  And YouTube is the biggest competitor in the digital space for attention.
*  And that was a real light bulb moment for me because you realize that as they're designing
*  these products, they're finding new clever ways to get your attention.
*  That's the real thing that I think is different in the film, The Social Dilemma, rather than
*  talking about censorship and data and privacy and these themes.
*  It's really what is the core influence or impact that the shape of these products have
*  on how we're making meaning of the world when they're just hearing our psychology?
*  Do you think that it was inevitable that someone manipulates the way people use these
*  things to gather more attention?
*  And do you think that any of this could have been avoided if there was laws against that?
*  If instead of having these algorithms that specifically target things that you're interested
*  in or things that you click on or things that are going to make you engage more, if they
*  just allowed these things to...
*  If someone said, listen, you can have these things, you can allow people to communicate
*  with each other, but you can't manipulate their attention span.
*  Yeah.
*  I mean, I think the...
*  So we've always had an attention economy, right?
*  And you're competing for it right now.
*  And politicians compete for it.
*  Can you vote for someone you've never paid attention to, never heard about, never heard
*  them say something outrageous?
*  No.
*  So there's always been an attention economy.
*  And so it's hard to say we should regulate who gets attention or how.
*  But it's organic in some ways.
*  Like this podcast is an organic...
*  If we're in competition, it's organic.
*  I just put it out there and if you watch it, you don't, or you don't, I don't have any
*  say over it and I'm not manipulating it in any way.
*  Sort of.
*  So let's imagine that the podcast apps were different and they actually...
*  While you're watching, they had the hearts and the stars and the voting up in numbers
*  and you could send messages back and forth.
*  Apple podcasts worked in a way that didn't just reward the things that you clicked follow
*  on, it actually sort of promoted the stuff that someone said the most outrageous thing.
*  Then you as a podcast creator have an incentive to say the most outrageous thing and then
*  you arrive at the top of the Apple podcast or Spotify app.
*  And that's the thing is that we actually are competing for attention.
*  It felt like it was neutral and it was relatively neutral.
*  And to progress that story back in time with Twitter competing for attention, let's look
*  at some other things that they did.
*  So they also added this retweet, this instant resharing feature, right?
*  And that made it more addictive because suddenly we're all playing the fame lottery, right?
*  Like I could retweet your stuff and then you get a bunch of hits and then you could go
*  viral and you could get a lot of attention.
*  So then instead of the companies competing for attention, now each of us suddenly win
*  the fame lottery over and over and over again and we're getting attention.
*  And then...
*  Oh, I had another example I was going to think about.
*  I forgot it.
*  What was it?
*  You can jump in if you want.
*  Apple has an interesting way of handling sort of the way they have their algorithm for their
*  podcast app is it's secret.
*  It's kind of it's weird.
*  But one of the things that it favors is it favors new shows and it favors engagement
*  in new subscribers.
*  So comments, engagement and new shows.
*  There you go.
*  And that's the same as competing for attention because engagement must mean people like it.
*  And there's going to be a fallacy as we go down that road.
*  But go on.
*  Well, it's interesting because you could say if you have a podcast and your podcast gets
*  like let's say 100,000 downloads, a new podcast can come along and it can get 10,000 downloads
*  and it'll be ahead of you in the rankings.
*  And so you could be number three and it could be number two and you're like, well, how is
*  that number two?
*  And it's got 10 times less, but they don't do it that way.
*  Their logic is they don't want the podcast world to be dominated by, you know, New York
*  Times and the big ones.
*  Yeah.
*  And whatever, whatever's number one and number two and number three forever.
*  We actually just experienced this.
*  We have a podcast called You're Undivided Attention.
*  And since the film came out in that first month, we went from being, you know, in the
*  lower 100 or something like that to we shot to the top five.
*  I think we were the number one tech podcast for a while.
*  And so we just experienced this through the fact not that we had the most listeners, but
*  the trend was so rapid that we sort of jumped to the top.
*  I think it's wise that they do that because eventually it evens out over time.
*  You know, you see some people rocket to the top like, oh my God, we're number three.
*  And you're like, hang on there, fella.
*  Just give it a couple of weeks.
*  And then three weeks later, four weeks later, now they're number 48.
*  Right.
*  They get depressed.
*  Right.
*  So that was really where you should have been.
*  The thing that Apple does that I really like in that is it gives an opportunity for these
*  new shows to be seen where they might have gotten just stuck because these rankings and
*  the ratings for a lot of these shows, these shows are so consistent and they have such
*  a following already.
*  Yeah.
*  It's very difficult for these new shows to gather attention.
*  Right.
*  And the problem was that there were some people that gamed the system and there was companies
*  that could literally move.
*  Earl Skakel.
*  Remember Earl became the number one podcast and no one was listening to it?
*  Earl has money and he hired some people to game the system and he was kind of open about
*  it and laughing about it.
*  Now isn't he banned from iTunes now or something?
*  I think he got banned because of that because it was so obvious he gamed the system.
*  He had like a thousand downloads and he was number one.
*  I mean the thing is that Apple podcasts you can think of as like the Federal Reserve or
*  the government of the attention economy because they're setting the rules by which you win.
*  They could have set the rules as you said to be who has the most listeners and then
*  you just keep rewarding the kings that already exist versus who is the most trending.
*  There's actually a story a friend of mine told me, I don't know if it's true although
*  it was a fairly credible source, who said he was in a meeting with Steve Jobs when they
*  were making the first podcast app and that they had made a demo of something where you
*  could see all the things your friends were listening to.
*  So just like making a news feed like we do with Facebook and Twitter, right?
*  And then he said was, well, why would we do that?
*  If something is important enough, your friend will actually just send you a link and say
*  you should listen to this.
*  Like why would we automatically just promote random things that your friends are listening
*  to?
*  Again, this is kind of how you get back to social media.
*  How is social media so successful?
*  Because it's much more addictive to see what your friends are doing in a feed, but it doesn't
*  reward what's true or what's meaningful.
*  And this is the thing that people need to get about social media is it's really just
*  rewarding the things that tend to keep people back addictively.
*  The business model is addiction in this race to the bottom of the brain stem for attention.
*  Well, it seems like if we in hindsight, if hindsight is 2020, what should have been done
*  or what could have been done had we known where this would pile out is that they could
*  have said you can't do that.
*  You can't manipulate these algorithms to make sure that people pay more attention and manipulate
*  them to ensure that people become deeply addicted to these platforms.
*  What you can do is just let them openly communicate, but it has to be organic.
*  And then the problem is, so this is the thing I was going to say about Twitter, is when
*  one company does the, call it the engagement feed, meaning showing you the things that
*  the most people are clicking on and retweeting, trending, things like that.
*  Let's imagine there's two feeds.
*  So there's the feed that's called the reverse chronological feed, meaning showing in order
*  in time, you know, Joe Rogan posted this two hours ago, but that's, you know, after that
*  you have the thing that people posted an hour and a half ago, all the way up to 10 seconds
*  ago.
*  So it's chronological.
*  They have a mode like that on Twitter.
*  If you click the sparkle icon, I don't know if you know this, it'll show you just in time,
*  here's what people said, you know, sorted by recency.
*  But then they have this other feed called what people click on, retweet, et cetera, the
*  most people you follow, and it sorts it by what it thinks you'll click on and want the
*  most.
*  Which one of those is more successful at getting your attention, the sort of recency, what
*  they posted recently versus what they know people are clicking on retweeting on the most?
*  Correct.
*  And so once Twitter does that, let's say Facebook was sitting there with the recency feed, like
*  just showing you here's the people who posted in this time order sequence.
*  They have to also switch to who is like the most relevant stuff, right?
*  The most clicked, retweeted the most.
*  So this is part of this race for attention that once one actor does something like that
*  and they algorithmically, you know, figure out what people, what's most popular, the
*  other companies have to follow because otherwise they won't get the attention they need.
*  Because otherwise they won't get the attention.
*  So it's the same thing if Netflix adds the autoplay five, four, three, two, one countdown
*  to get people to watch the next episode.
*  That if that works at, say, increasing Netflix's watch time by five percent, YouTube sits
*  there says we just shrunk how much time people were watching YouTube because now they're
*  watching more Netflix.
*  So we're going to add five, four, three, two, one autoplay countdown.
*  And it becomes again, this game theoretic race of who's going to do more.
*  Now, you feel going up, Tick Tock, Tick Tock doesn't even wait.
*  I don't know if you know or your kids use Tick Tock, but when you open up the app, it
*  doesn't even wait for you to click on something.
*  It just actually plays the first video the second you open it, which none of the other
*  apps do. Right.
*  And the point of that is that causes you to enter into this engagement stream even faster.
*  So this is this again, this race for attention produces things that are not good for society.
*  And even if you took the whack-a-mole sticker, you took the antitrust case and you whack
*  Facebook and you got rid of Facebook or you whack Google or you whack YouTube, you're
*  just going to have more actors flooding in doing the same thing.
*  And one other example of this is the time it takes to reach, let's say, 10 million
*  followers. So if you remember back in the ashton, it was an Ashton Kutcher who raced for
*  the first million followers.
*  Raced with CNN.
*  Raced with CNN. Right.
*  Yeah.
*  So now if you think of it, the companies are competing for our attention if they find out
*  that each of us becoming a celebrity and having a million people we get to reach, if
*  that's the currency of the thing that gets us to come back to get more attention, then
*  they're competing at who can give us that bigger fame lottery hit faster.
*  So let's say 2009 or 2010 when Ashton Kutcher did that, it took him, I don't know how long
*  it took months for him to get.
*  I don't remember.
*  It was it was a little bit, though.
*  Right.
*  And then TikTok comes along and says, hey, we want to give kids the ability to hit the
*  fame lottery and make it big, hit the jackpot even faster.
*  We want you to be able to go from zero to a million followers in 10 days.
*  Right. And so they're competing to make that shorter and shorter and shorter.
*  And I know about this because, you know, speaking from a Silicon Valley perspective,
*  venture capitalists fund these new social platforms based on how fast they can get to
*  like 100 million users.
*  There is this famous line that like I forgot what it was, but I think Facebook took like
*  10 years to get to 100 million users.
*  Instagram took, you know, I don't know, four years, three years or something like that.
*  TikTok can get there even faster.
*  And so it's shortening, shortening, shortening.
*  And that's what people are.
*  That's what we're competing for.
*  It's like who can win the fame lottery faster?
*  But is a world where everyone broadcast to millions of people without the responsibilities
*  of publishers, journalists, et cetera, does that produce an information environment
*  that's helped that that's that's healthy?
*  And obviously the film, The Social Dilemma is really about how it makes the worst of
*  us rise to the top.
*  Right. So our hate, our outrage, our polarization, what we disagree about, black and
*  white thinking, more conspiracy oriented views of the world, QAnon, you know, Facebook
*  groups, things like that.
*  And I can we can definitely go into it.
*  There's a lot of legitimate conspiracy theories.
*  I don't want to make sure I'm not categorically dismissing stuff.
*  But that's really the point is that we have landed in a world where the things that we
*  are paying attention to are not necessarily the agenda of topics that we would say in
*  a reflective world.
*  What we would say is the most important.
*  So there's a lot of there's a lot of conversation about free will and about letting
*  people choose whatever they choose, whatever they enjoy viewing and watching and
*  paying attention to.
*  But when you're talking about these incredibly potent algorithms and incredibly potent
*  addictions that people.
*  The people develop to these these things and we're pretending that people should have
*  the ability to just ignore it and put it away.
*  Right. And use your willpower.
*  Yeah, that seems I have a folder.
*  I have a folder on my phone called Addict and it's all caps.
*  And it's at the end of my all you have to scroll through all my other apps to get to
*  it. And so if I want to get to Twitter or Instagram, the problem is that the app switcher
*  will put it in the most recent.
*  So once you switch apps and you have Twitter in a recent it'll be right there.
*  So that's if I want to go left.
*  And yeah, if I want to see that, you can do that.
*  Yet some it's insanely addictive.
*  And if you can control yourself, it's not that big a deal.
*  But how many people can control themselves?
*  Well, I think the the thing we have to hone in on is the asymmetry of power.
*  You know, as I say in the film, it's like we're bringing this ancient brain hardware,
*  the prefrontal cortex, which is like what you use to do goal directed action, self
*  control, willpower, holding back, you know, marshmallow test.
*  Don't do the don't get the marshmallow now.
*  Wait later for the two marshmallows later.
*  All of that is through our prefrontal cortex.
*  And when you're sitting there and you think, OK, I'm going to go watch, I'm going to
*  look at this one thing on Facebook because my friend invited me to this event or it's
*  this one post I have to look at.
*  And the next thing you know, you find yourself scrolling through the thing for like an
*  hour. And you say, man, that was on me.
*  I should have had more self-control.
*  But there behind the screen, behind that glass lab is like a supercomputer pointed at
*  your brain that is predicting the perfect thing to show you next.
*  And you can feel it like it's this is really important.
*  So like if I'm Facebook and when you flick your finger, you think when you're using
*  Facebook, it's just going to show me the next thing that my friend said.
*  But it's not doing that. When you flick your finger, it actually literally wakes up
*  this sort of supercomputer avatar voodoo doll version of Joe and the voodoo doll of
*  Joe is, you know, the more clicks you ever made on Facebook is like adding a little
*  hair to the voodoo doll and the more likes you've ever made adds little clothing to
*  the voodoo doll and the more, you know, watch time on videos you've ever had adds
*  little shoes to the voodoo doll.
*  So the voodoo doll is getting more and more accurate the more things you click on.
*  This is in the film, The Social Dilemma.
*  Like if you notice, like the character, you know, as he's using this thing, it builds
*  a more and more accurate model that the AIs, the three AIs behind the screen are kind
*  of manipulating. And the idea is it can actually predict and prick the voodoo doll
*  with this video or that post from your friends or this other thing.
*  And it'll figure out the right thing to show you that it knows will keep you there
*  because it's already seen how that same video or that same post has kept 200 million
*  other voodoo dolls there because you just look like another voodoo doll.
*  So here's an example. And this works the same on all the platforms.
*  If you are a teen girl and you opened a dieting video on YouTube, 70 percent of
*  YouTube's watch time comes from the recommendations on the right hand side.
*  Right. So the things that are showing recommended videos next.
*  And it will show you it'll show what did it show that the girls who watch the teen
*  dieting video, it showed anorexia videos because those were better at keeping the
*  teen girls attention, not because it said these are good for them.
*  These are helpful for them.
*  It just says these tend to work at keeping their attention.
*  So, again, these tend to work if you are already watching diet videos.
*  Yeah. So if you're a 13 year old girl and you watch the diet video, YouTube wakes up
*  its voodoo doll version of that girl and says, hey, I've got like 100 million other
*  voodoo dolls of 13 year old girls.
*  Right. And they all tend to watch these these other videos.
*  I don't know what I just know that they have this word thin spoh.
*  Thinspiration is the name for it to be inspired for anorexia.
*  Yeah, it's a real thing. YouTube addressed this problem a couple of years ago.
*  But when you let the machine run blind, all it's doing is picking stuff that's
*  engaging. Why did they choose to not let the machine run blind with one
*  thing like anorexia?
*  Well, so now we're getting into the Twitter censorship conversation, the moderation
*  conversation. So the real this is why I don't focus on censorship and moderation,
*  because the real issue is if you blur your eyes and zoom way out and say, how does
*  the whole machine tend to operate?
*  Like, no matter what I start with, what is it going to recommend next?
*  So, you know, if you started with, you know, a World War Two video, YouTube would
*  recommend a bunch of Holocaust denial videos.
*  Right. If you started teen girls with a dieting video, it would recommend these
*  anorexia videos. In Facebook's case, if you joined, there's so many different
*  examples here because Facebook recommends groups to people based on what it thinks
*  is most engaging for you.
*  So if you were a new mom, you had Rene DiResta, my friend, on this podcast.
*  We've done a bunch of work together and she has this great example of as a new
*  mom, she joined one Facebook group for mothers who do do it yourself baby food,
*  like organic baby food.
*  And then Facebook has this sidebar.
*  It says, here's some other groups you might recommend you might want to join.
*  And what do you think was the most engaging of those?
*  Because Facebook, again, is picking on which group, if I got you to join it,
*  would cause you to spend the most time here.
*  Right.
*  So for some do it yourself baby food groups, which group do you think it selected?
*  Probably something about vaccines.
*  Exactly.
*  So anti-vaccines for moms.
*  Yeah.
*  Okay.
*  So then if you join that group, now it does the same run the process again.
*  So then, so now look at Facebook.
*  So it says, Hey, I've got these voodoo dolls.
*  I've got like a hundred million voodoo dolls and they're all, they just join
*  this anti-vaccine moms group.
*  And then what do they tend to engage with for very long time?
*  If I get them to join these other groups, which of those other groups would show up?
*  Chemtrails, the Pizzagate, flat earth, absolutely.
*  Yep.
*  And YouTube recommended, so I'm interchangeably going from YouTube to
*  Facebook because it's the same dynamic.
*  They're competing for attention.
*  And YouTube recommended flat earth conspiracy theories, hundreds of millions of times.
*  And so when you, when you're a parent during COVID and you sit your kids in
*  front of YouTube, cause you're like, I'm, I've got to, this is the digital pacifier.
*  Got to let them do the thing.
*  I got to do work.
*  And then you come back to the dinner table and your kid says, you know, the
*  Holocaust didn't happen and the earth is flat and people are wondering why.
*  It's because of this.
*  And now to your point about this sort of moderation thing, we can take the whack
*  a mole stick after the public yells and Renee and I, you know, make a bunch of
*  noise or something and large community, by the way, of people making noise about
*  this and they'll say, okay, shoot, you're right.
*  Flat earth.
*  We got to deal with that.
*  And so they'll, they'll tweak the algorithm and then people make a
*  much a noise about the inspiration videos for the anorexia for kids.
*  And they'll deal with that problem.
*  But then they start doing it based reactively.
*  But again, if you zoom out, it's just still recommending stuff.
*  That's kind of from the crazy town section of the problem, the
*  recommendation.
*  Because I don't mind that people have ridiculous ideas about hollow earth
*  because I think it's humorous, but I'm also a 53 year old man, right?
*  Right.
*  I'm not, I'm not a 12 year old boy with a limited education that is like,
*  Oh my God, the government's lying to us.
*  There's lizard people that live under the earth.
*  Right.
*  But if that's the real argument about these conspiracy theories is that they
*  can influence young people or the easily impressionable or, or people that maybe
*  don't have a sophisticated sense of vetting out bullshit.
*  Right.
*  Well, and the algorithms aren't making a distinction between who is just laughing
*  at it and who is deeply vulnerable to it.
*  And generally it's just, it's, it just says who's vulnerable to it.
*  Cause another example, the way I think about this is if you're driving down the
*  highway and, and you know, there's Facebook and Google trying to figure out like,
*  what should I give you based on what tends to keep your attention?
*  If you look at a car crash and everybody driving in the highway, they look at the
*  car crash according to Facebook and Google is like the whole world wants car crashes.
*  We just, just feed them car crashes after car crashes, after car crashes.
*  And what the algorithms do as Guillaume Chasleau in the film says, who's the
*  YouTube whistleblower from the YouTube recommendation system is they find the
*  perfect little rabbit hole for you that it knows will keep you there for five
*  hours and the conspiracy theory, like dark corners of YouTube were the dark
*  corners that tends to keep people there for five hours.
*  And so you have to realize that we're now something like 10 years in to this
*  vast psychology experiment where it's been, you know, in every language and
*  hundreds of countries, right.
*  And every, in hundreds of languages, it's been steering people towards the crazy
*  town.
*  And when I say crazy town, I think of, you know, imagine there's a spectrum on
*  YouTube and there's on one side, you have like the calm Walter Cronkite, Carl
*  Sagan, you know, slow, you know, kind of boring, but like educational material or
*  something.
*  And then the other side of the spectrum, you have, you know, the craziest stuff
*  you can find.
*  Um, crazy town, no matter where you start, you could start in Walter Cronkite
*  or you could start in crazy town.
*  But if I'm YouTube and I want you to watch more, am I going to steer you
*  towards the calm stuff or am I going to steer you more towards crazy town?
*  Crazy town, always more towards crazy town.
*  So then you imagine just tilting the floor of humanity just by like three
*  degrees, right.
*  And then you just step back and you let society run its course.
*  As Jaren Lanier says in the film, if you just tilt society by one degree, two
*  degrees, that's the whole world.
*  That's the, that's what everyone is thinking and believing.
*  And so if you look at the, at the degree to which people are deep into rabbit
*  hole conspiracy thinking right now, and again, I want to acknowledge Cointel pro
*  operation, walking bird, like there's a lot of real stuff, right?
*  So I'm not categorically dismissing it, but we're asking what is the basis upon
*  which we're believing the things we are about the world and increasingly that's,
*  that's based on technology and we can get into, you know, what's going on in
*  Portland.
*  Well, the only way I know that is I'm looking at my social media feed and
*  according to that, it looks like the entire city's on fire and it's a war zone.
*  But if you, I called a friend there the other day and he said, that's a beautiful
*  day. There's, there's actually no violence anywhere near where I am.
*  It's just like these two blocks or something like that.
*  And this is the thing is warping our view of reality.
*  And I think that's what really, for me, the social dilemmas was really trying to
*  accomplish as a film.
*  And the director, Jeff Orlovsky was trying to accomplish is, is how did this
*  society get, go crazy everywhere all at once?
*  You know, seemingly, you know, this, this didn't happen by accident happened by
*  design of this business model.
*  When did the business model get implemented?
*  Like when did they start using these algorithms to recommend things?
*  Cause initially YouTube was just a series of videos and it didn't have that
*  recommended correct section.
*  When was that?
*  You know, it's a good question.
*  I mean, I, um, you know, they originally YouTube was just post a video and you
*  can get people to, you know, go to that URL and send it around.
*  Uh, they needed to figure out once the competition for attention got more
*  intense, they needed to figure out how am I going to keep you there?
*  And so recommending those videos on the right hand side, I think that was there
*  pretty early, if I remember actually, uh, because that's, that was sort of the
*  innovation is like keeping people within this YouTube wormhole.
*  And once people were in the YouTube wormhole, constantly seeing videos, that
*  was what the, they could, they could offer the promise to a new video uploader.
*  Hey, if you post it here, you're going to get way more views than if you
*  post it on Vimeo, right?
*  And that's, that's the thing.
*  If I open up Tik TOK right now on my phone, you have to talk on your phone.
*  Um, well, I'm not supposed to, obviously, but more for research purposes.
*  Research.
*  Do you know how to take talk at all?
*  No.
*  My 12 year olds obsessed.
*  Oh really?
*  Oh yeah.
*  She can't even sit around if she's standing still for five minutes.
*  She just starts like, she starts to talk and that's the thing.
*  I mean, 2012, 2012.
*  So the Mayans were right.
*  Right.
*  2012, the platform announced an update to the discovery system, uh, designed to
*  identify the videos people actually want to watch by prioritizing videos that
*  hold attention throughout, as well as increasing the amount of time a user
*  spends on the platform overall.
*  YouTube, YouTube could assure advertisers that it was providing a valuable,
*  high quality experience for people.
*  Yep.
*  So, um, that that's beginning of the end.
*  Yep.
*  So 2012 on YouTube's timeline, I mean, um, you know, the Twitter and Facebook
*  world, I think introduces the retweet and reshare buttons in the 2009 to 2010
*  kind of time period.
*  So you end up with this world where the things that we're most paying attention
*  to are based on algorithms choosing for us.
*  And so the sort of deeper argument that's in the film that I'm not sure everyone
*  picks up on is these technology systems have taken control of human choice.
*  They're taking control of humanity because they're controlling the information
*  that all of us are getting.
*  Think about every election.
*  Like, um, I think of Facebook as kind of a voting machine, but it's a sort of
*  indirect voting machine because it controls the information for four years
*  that you're in charge of the entire society is getting, and then everyone
*  votes based on that information.
*  Now you could say, well, hold on radio and television were there and
*  were partisan before that.
*  But actually TV, um, radio and TV are often getting their news stories from
*  Twitter and Twitter is rec is recommending things based on these algorithms.
*  So when you control the information that an entire population is getting, you're
*  controlling their choices.
*  I mean, literally in military theory, if I want to screw up your military, I want
*  to control the information that it's getting, I want to confuse the enemy.
*  And that information funnel is the very thing that's been corrupted.
*  And it's like the Flint water supply for our minds.
*  I was talking to a friend yesterday and she was saying that there were articles
*  that, uh, she was laughing that there's articles that are written about negative
*  tweets that random people make about, uh, a celebrity doing this or that.
*  And she was like, and she was quoting this article.
*  She's like, look how crazy this is.
*  This is a whole article that's written about someone who decided to say something
*  negative about some, something some celebrity had done.
*  And then it becomes this huge art.
*  And then the tweets are prominently featured and the response to those, I mean,
*  like really like arbitrary, like weird.
*  Because it's a values blind system that just cares about what we'll get attention.
*  Exactly.
*  And that's what the article was.
*  It was just an intention grab.
*  It's interesting because, um, Prince Harry and Megan have, have become very
*  interested in these issues and are actively working on these issues and, um,
*  getting to know them just a little bit.
*  Are they really?
*  Yeah.
*  Well, they're because it affects them personally.
*  Well, it's, it's actually interesting.
*  I mean, I don't want to speak for them, but, um, I think Megan has been the
*  target of the most vitriol hate oriented stuff on the planet, right.
*  From just the amount of sort of criticism that they, that they get and scrutiny.
*  Yeah.
*  I mean, she's just like news feeds filled with hate about just what she looks
*  like, what she says just constantly.
*  And I, boy, I'm out of the loop.
*  I've never seen anything.
*  She's pretty.
*  What are these things she looks like?
*  I honestly, I don't follow it myself because I don't fall into these attention
*  traps. I try not to, but people just face the worst vitriol.
*  I mean, this is the thing with teen bullying, right?
*  So I think they work on these issues because teenagers are now getting a
*  micro version of this thing where each of us are scrutinized, you know,
*  and I think that's what's not, I mean, think about what celebrity status does
*  and how it screws up humans in general, right?
*  Like take an average celebrity, like it warps your mind, it warps your psychology
*  and you get scrutiny, right?
*  When you suddenly are followed, each person gets thousands or project forward
*  into the future a few years, each of us have, you know, tens of thousands to
*  hundreds of thousands of people that are following what we say.
*  That's a lot of feedback.
*  And, you know, as Jonathan Haidt says in the film, I know you've had him here,
*  you know, it's made kids much more cautious and less risk taking and more
*  bullied overall. And there's just huge problems in mental health around this.
*  Yeah, it's really bad for young girls, right?
*  Especially for celebrities.
*  And I've had quite a few celebrities in here and we've discussed it.
*  I just tell them that you can't read that stuff. Just don't read it.
*  Yeah.
*  Like there's no good in it.
*  Like I had a friend, she did a show, she's a comedian, she did a show and she
*  was talking about this one negative comment that was inaccurate.
*  You know, it said she only did a half an hour and her show sucked.
*  She's like, fuck her. And it's not like, I go, why are you reading that?
*  She's like, cause it's mostly positive.
*  I go, but how come you're not talking about most of it then talking about this
*  one person, this one negative person. We're both laughing about it.
*  Like she's, she's healthy. You know, she's not, she's not completely fucked up by
*  it, but this one person got into her head.
*  I'm like, I'm telling you, it's not, the juice is not worth the squeeze, but
*  don't read those things.
*  But this is, this is exactly right.
*  And this is based on how our minds work.
*  I mean, our minds literally have something called negativity bias.
*  So if you have a hundred comments and 99 are positive and one is negative,
*  just where does the average human's mind go?
*  Right. They go to the negative.
*  And it also goes to the negative.
*  Even when you shut down the screen, your mind is sitting there looping on that
*  negative comment. And why?
*  Because evolutionarily it's really important that we look at social approval,
*  negative social approval, because our reputation is at stake in the tribe.
*  Yes.
*  So it matters.
*  Yes.
*  But it's never been easier now for not just that, that one comment to sort of
*  gain more airtime, but then for that to build a hate mob and then to see the
*  interconnected clicks.
*  And I can go in and see 10 other people that responded to that, that are
*  negative.
*  And so, especially when you have teenagers that are exposed to this and you can
*  keep going down the tree and see all of the hate fest on you.
*  This is the psychological environment that is the default way that kids are
*  growing up now.
*  I actually faced this recently with, with the film itself, because actually the
*  film has gotten just crazy positive acclaim for the most part.
*  And there's just a few, you know, negative comments.
*  And for myself even, right?
*  Like comes a conjunction, but I was glued to a few negative comments and I,
*  and then you could click and you would see other people that you know, who
*  positively like, or respond to those comments.
*  Like, why did that person say that negative thing?
*  I thought we were friends, that whole kind of psychology and we're all vulnerable
*  to it.
*  Unless you learn as you said, to tell your celebrity friends, just don't pay
*  attention.
*  Even mild stuff.
*  I see people fix it on even a mild disagreement or mild criticism people
*  fix it on.
*  And it's, it's, it's also a problem because you realize that someone's saying
*  this and you're not there and you can't defend yourself.
*  So you have this feeling of helplessness, like, Hey, that's not true.
*  I didn't.
*  And then you, you don't get it out of your system.
*  You never, you never get to express it.
*  And people can share that false negative stuff.
*  I mean, I didn't, not all negative stuff is false, but you can assert things and
*  build on the hate fest, right?
*  And start going crazy and saying this person's a white supremacist or this
*  person's even worse.
*  And that'll spread to thousands and thousands of people.
*  And next thing you know, you check into your feed again at, you know, 8pm that
*  night and you, your whole reputation has been destroyed and you didn't even know
*  what happened to you.
*  Well, and it's happened to teenagers too.
*  I mean, they're anxious, like I'll post, you know, teenager will post a photo,
*  uh, their high school, they make a dumb comment without thinking about it.
*  And then next thing they know, you know, at the end of the day, the parents are
*  all calling because like 300 parents saw it and are calling up the parent of that
*  kid.
*  And it's, it's, you know, we talk to teachers a lot in our work at the center
*  for humane technology and they, um, we'll say that on Monday morning, this is
*  before COVID, but on Monday morning, they spend the first like hour of class
*  having to clear all the drama that happened on social media from the weekend
*  for the kids.
*  Jesus.
*  And again, like this.
*  And these kids are in what age group?
*  This was like eighth, ninth, ninth, tenth grade, that kind of thing.
*  And the other problem with these kids is there's not like, uh, a long history of
*  people growing up through this kind of influence and successfully navigating it.
*  These are the, these are the pioneers.
*  Yeah.
*  And they won't know anything different, which is why we talk about in the film,
*  like this, they're growing up in this environment.
*  And, you know, one of the simplest principles of ethics, um, uh, is the
*  ethics of symmetry, doing onto others as you would do to yourself.
*  And as we say at the end of the film, like one of the easiest ways, you know,
*  that there's a problem here is that many of the executives at the social media
*  tech companies don't let their own kids use social media.
*  Right.
*  They literally say at the end of the film, like it's a, we have a rule about it.
*  We're religious about it.
*  We don't do it.
*  The CEO of Lunchables foods didn't let his own kids eat Lunchables.
*  That's when you know, if you talk to a doctor or a lawyer, a doctor, and you say,
*  you know, would you get this surgery for your own kid?
*  They say, oh no, I would never do that.
*  Like, would you trust that doctor?
*  Right.
*  And it's the same for a lawyer.
*  So this is the relationship where we have a relationship of asymmetry and
*  technology is influencing all of us.
*  And we need a system by which, you know, when I was growing up, uh, you know, I
*  grew up on the Macintosh and technology and I was creatively doing programming
*  projects and whatever else, the people who built the technology I was using
*  would have their own kids use the things that I was using because they were
*  creative and they were about tools and empowerment and that's what's changed.
*  We don't have that anymore because the business model took over.
*  And so instead of having just tools sitting there, like hammers waiting to be
*  used to build, you know, creative projects or programming to invent things or paint
*  brushes or whatever, we now have a manipulation based technology environment
*  where everything you use has this incentive to not only addict you, but to
*  have you play the fame lottery, get social feedback, because those are all
*  the things that keep people's attention.
*  Isn't this also a problem with these information technologies being attached
*  corporations that have this philosophy of unlimited growth?
*  Yes.
*  So they're there no matter how much they make.
*  I, I, I applaud Apple because I think they're the only company that takes
*  steps to protect privacy, to block advertisements, to make sure that at
*  least like when you, when you use their maps application, they're not saving
*  your data and sending it to everybody.
*  And it's one of the reasons why Apple maps is really not as good as Google
*  maps, but I use it and that's one of the reasons why I use it.
*  And when Apple came out recently and there was, um, they were doing something
*  to, uh, to, to block your, uh, information being, uh, sent to other places.
*  And they forget, what was the exact thing that it was in the new iOS, they
*  released a thing that blocks the tracking identifiers.
*  That's right.
*  And it's not actually out yet.
*  It's going to be out in January or February.
*  I think someone told me and what that's doing, that's a good example of
*  they're putting a tax on the advertising industry because just by saying you
*  can't track people individually, that, you know, takes down the value of an
*  advertisement by like 30% or something.
*  Here it is.
*  And you, when I do safari, I get this whole privacy report thing.
*  Right.
*  It says it's like in the last seven days, it's prevented 125
*  trackers from profiling me.
*  Right.
*  Yeah.
*  And you can opt out of that if you'd like, if you're like, no, fuck that.
*  Track me.
*  Yep.
*  Yeah.
*  You can do that.
*  You can let them send your data, but that, that seems to me a much more
*  ethical approach to be able to decide whether or not these companies get
*  your information.
*  I mean, those things are great.
*  Um, the challenge is imagine you get the privacy equation perfectly right.
*  Look at this Apple working on its own search engine as Google ties.
*  Oh yeah.
*  It could be cut soon.
*  I started using duck duck go.
*  Yep.
*  For that very reason, just cause it's, they don't do anything with it.
*  You know, they give you the information, but they don't, they don't take your
*  data and do anything with it.
*  The challenge is let's say we get all the privacy stuff, perfectly, perfectly
*  right and data production and data controls and all that stuff in a system
*  that's still based on attention and grabbing attention and harvesting and
*  strip mining our brains.
*  Uh, you still get maximum polarization, addiction, mental health problems,
*  isolation, teen depression and suicide, um, polarization breakdown of truth.
*  Right.
*  Right.
*  So that's, we really focus in our work, uh, on those topics because that's the
*  direct influence of the business model on warping society.
*  Like we need to name this mind warp.
*  We think of it like the climate change of culture that, you know, they seem
*  like, they seem like different disconnected topics, much like with
*  climate change, you'd say like, okay, we've got species loss in the Amazon.
*  We've got, we're losing insects.
*  We've got melting glaciers.
*  We've got, uh, ocean acidification.
*  We've got the coral reefs, you know, getting dying.
*  These can feel like disconnected things until you have a unified model of how
*  emissions change all those different phenomena, right in the social fabric.
*  We have shortening of attention spans.
*  We have more outrage driven news media.
*  We have more polarization.
*  Um, we have more breakdown of truth.
*  We have more conspiracy minded thinking.
*  These seem like separate events, uh, and separate phenomena, but they're
*  actually all part of this attention extraction paradigm that the company's
*  growth, as you said, depends on extracting more of our attention, which
*  means more polarization, more extreme material, more conspiracy thinking and
*  shortening attention spans, because we also say like, you know, if we want to
*  double the size of the attention economy, I want your attention Joe to be
*  split into two separate streams.
*  Like I want you watching the TV, uh, the tablet and the phone at the same time.
*  Cause now I've tripled the size of the amount of extractable attention that I
*  can get for advertisers, which means that by fracking for attention and splitting
*  you into more junk, you know, attention, that's like thinner, we can sell that as
*  if it's real attention, like the financial crisis where you're selling
*  thinner and thinner financial assets as if it's real, but it's really just a
*  junk asset and that's kind of where we are now where it's sort of the junk
*  attention economy because we, we're, we can shorten attention spans and we're
*  debasing the substrate of that makes up our society because everything in a
*  democracy depends on individual sense making and meaningful choice, meaningful
*  free will, meaningful independent views.
*  But if that's all basically sold to the highest bidder that debases the soil
*  from which independent views grow, because all of us are jacked into this sort of
*  matrix of social media manipulation, that's, that's ruining integrating our
*  democracy and that's really, there's many other things that are ruining
*  integrating our democracy, but that's, that's this sort of invisible force.
*  It's upstream that affects every other thing downstream, because if we can't
*  agree on what's true, for example, you can't solve any problem.
*  I think that's what you talked about in your 10 minute thing on the social
*  dilemma.
*  I think I saw on YouTube.
*  Yeah.
*  Your organization highlights all these issues and you know, in an amazing way
*  and it's very important, but do you have any solutions?
*  It's hard, right?
*  So I just want to say that this is, is a complex of problem as climate change.
*  Um, in the sense that you need to change the business model.
*  I think of it like we're on the fossil fuel economy and we have to switch to
*  some kind of beyond that thing, right?
*  Because so long as the business models of these companies depend on
*  extracting attention, can you expect them to do something different?
*  Like you can't, but how could you, is it, I mean, there's so much money involved
*  and now they've accumulated so much wealth that they have an amazing amount
*  of influence.
*  Yep.
*  You know, and.
*  And the asymmetric influence can buy lobbyists, can influence Congress and
*  prevent things from happening.
*  So this is why it's kind of the last moment.
*  That's right.
*  But you know, I think we're seeing signs of real change.
*  We have the antitrust case that was just filed against Google.
*  Um, in Congress, we're seeing more hearings.
*  What was the basis of that case?
*  You know, to be honest, I was actually in the middle of the social
*  dilemma launch when I think that happened and our, our, my home burned
*  down in the recent fires in Santa Rosa.
*  So I actually missed that happening.
*  It's hard to hear that.
*  Yeah.
*  Sorry.
*  That was a big thing to drop, but yeah, no, it's awful.
*  There's so much that's been happening in the last six weeks.
*  I've been, uh, I was evacuated three times where I lived in California.
*  Oh, really?
*  Yeah.
*  So real close to our house justice departments, who's monopolist Google
*  for violating antitrust laws, department files, complaining against
*  Google to restore competition in search and search advertising markets.
*  Okay.
*  So it's all about search.
*  Yeah, this is right.
*  This was a case that's about Google using its dominant position to
*  privilege its own search engine, um, in its own products and beyond, which
*  is similar to sort of Microsoft bundling in the, uh, internet explorer browser.
*  But I, you know, this is all good progress, but really it misses the kind
*  of fundamental harm of like, these things are warping our society.
*  They're warping how our minds are working and there's no, you know,
*  congressional action against that.
*  Cause it's a really hard problem to solve.
*  I think that the reason the film for me is so important is that if I look at
*  the growth rate of how fast, uh, Facebook has been recommending people into
*  conspiracy groups and, um, kind of polarizing us into separate echo
*  chambers, which we should really break down, I think as well for people, like
*  exactly the mechanics of how that happens.
*  But if you look at the growth rate of all of those harms compared to, you
*  know, how fast has Congress passed anything to deal with it?
*  Like basically not at all.
*  They seem a little bit unsophisticated in that regard.
*  Like the understatement.
*  Yeah.
*  They try to be charitable.
*  I want to be charitable too.
*  And I want to make sure I call out and there's Senator Mark Warner,
*  Blumenthal, um, uh, several other senators we've talked to have been
*  really on top of these issues and led.
*  I think Senator Warner's white paper, um, on how to regulate the tech
*  platforms is one of the best.
*  It's from two years ago in 2018.
*  Uh, and Rafi Martina, his staffer is an amazing human being.
*  His works very hard on these issues.
*  So there, there are some good folks, but when you look at the
*  broad, like the hearing yesterday, it's mostly grandstanding
*  to politicize the issue.
*  Right.
*  Cause you, you turn it into on the right, um, Hey, you're
*  censoring conservatives and on the left it's, Hey, you're not taking
*  down enough misinformation and dealing with the hate speech and all
*  these kinds of things, right.
*  And they're not actually dealing with how would we solve this problem?
*  They're just trying to make a political point to win over their base.
*  Now, Facebook recently banned the Q and on pages, which, uh, I thought
*  it was kind of fascinating because I'm like, well, this is a weird sort
*  of slippery slope, isn't it?
*  Like if you decide that you, I mean, it's, it almost seemed to me like,
*  well, we'll throw them a bone.
*  We'll get rid of Q and on it.
*  Cause it's so preposterous.
*  Let's just get rid of that.
*  But what else, like, if you keep going down that rabbit hole, where do you
*  draw the line, like where are you allowed to have JFK conspiracy theories?
*  Are you allowed to have flat earth?
*  Are you allowed?
*  I mean, I guess flat earth is not dangerous.
*  Is that where they make the distinction?
*  So I think their policy is evolving in the direction of when things are
*  causing offline harm, when online content is known to proceed offline harm.
*  That's when the platform that's the standard by which platforms are acting.
*  What, um, what offline harm has been caused by the Q and on stuff?
*  Do you know?
*  Um, there's several incidents.
*  We interviewed a guy on our podcast about it.
*  Um, there's some armed at gunpoint type thing.
*  I can't remember.
*  Um, uh, and there's, there's things that are priming people to be violent.
*  You know, um, uh, these are, I just want to say these are really tricky topics.
*  Right?
*  I think what I want to make sure we get to though, is that there are many people
*  manipulating the group think that can happen in these echo chambers, because
*  once you're in one of these things, like I studied Colts earlier in my career.
*  And the power of Colts is like, they're a vertically integrated persuasion stack
*  because they control your social relationships.
*  They control who you're hearing from and who you're not hearing from.
*  They give you meaning purpose and belonging.
*  They, um, uh, they have a custom language.
*  They have an internal way of referring to things and social media allows you to
*  create this sort of decentralized Colts factory where, um, it's easier to grab
*  people into an echo chamber where they only hear from other people's views.
*  And Facebook, you can even just recently announced that they're going to be
*  promoting more of the Facebook group content into feeds, which means that
*  they're actually going to make it easier for that kind of manipulation to happen.
*  But did they make the distinction between group content and conspiracy groups?
*  Like, how do you, how do you, when, when does group content,
*  when does it cross a line?
*  I don't know.
*  I mean, the, the policy teams that work on this are coming up with their own
*  standards, so I'm not familiar with it.
*  If you think about, you know, think about how hard it is to come up with a law at
*  the federal level that all States will agree to, then you imagine Facebook
*  trying to come up with a policy that will be universal to all the countries
*  that are running Facebook, right?
*  Well, then you imagine how you take a company that never thought they were
*  going to be in the position to do that.
*  And then within a decade, they become the most prominent source of news and
*  information on the planet earth.
*  And now they have to regulate it.
*  And, and, you know, I actually believe Zuckerberg when he says, I don't
*  want to make these decisions.
*  I shouldn't be in this role where my beliefs decide the whole world's views.
*  He genuinely believes that.
*  Yeah.
*  Um, and, and to be sure of that, but the problem is he created a situation
*  where he is now in that position.
*  I mean, he got there very quickly and they did it aggressively when they
*  went into countries like Myanmar, Ethiopia, uh, all throughout the African
*  continent where they gave, do you know about free basics?
*  No.
*  So this is the program that I think has gotten something like 700 million, um,
*  accounts onto Facebook where they do a deal with like a telecommunications
*  provider, like at their version of AT&T in Myanmar or something.
*  So when you get your smartphone, it comes, Facebook's built in
*  and there's a, uh, asymmetry of, uh, access where it's free to access Facebook,
*  but it costs money to do the other things for the data plan.
*  So you get a free Facebook account.
*  Facebook is the internet basically, cause it's the free thing you can do on your
*  phone and then there's, we know that there's fake information that's being spread.
*  So the data doesn't apply to Facebook use.
*  Yeah.
*  I think like the costs, like, you know, how we pay for data here, like that.
*  I think you don't pay for Facebook, but you do pay for all the other things,
*  which creates an ACP.
*  Symmetry where your course, you're going to use Facebook for most things.
*  Right.
*  So you've Facebook messenger.
*  Yeah.
*  And what's that?
*  Yeah.
*  Yeah.
*  Um, I don't know exactly with video cause different
*  Facebook has video calls as well.
*  In general, they do.
*  Yeah.
*  I just don't know how that works in the developing world, but there's a
*  joke within Facebook where I mean, this has caused genocides, right?
*  So in Myanmar, which is in the film, um, the Rohingya Muslim minority group,
*  um, many Rohingya were persecuted and murdered because of fake information
*  spread by the government, um, on Facebook using their asymmetric
*  knowledge with fake accounts.
*  I mean, even just a couple of weeks ago, Facebook took down a network of,
*  I think, several hundred thousand fake accounts in Myanmar.
*  And they didn't even have at the time more than something like four or five
*  people in their extended Facebook network who even spoke the
*  language of that country.
*  Oh God.
*  So when you realize that this is like the, I think of like the Iraq war,
*  Colin Powell, pottery barn rule where like, you know, if you go in and you
*  break it, then you are responsible for fixing it.
*  This is Facebook actively doing deals to go into Ethiopia, to go into
*  Myanmar, to go into the Philippines or whatever, and providing these solutions.
*  And then it breaks the society and they're, they're now in a position
*  where they have to fix it.
*  There's actually a joke within Facebook that if you want to know which countries
*  will be quote unquote at risk in two years from now, look at which ones
*  have Facebook free basics.
*  Jesus.
*  And then it's terrifying that they do that and they don't have very many
*  people that even speak the language.
*  So there's no way they're going to be able to filter it.
*  That's right.
*  And so now if you take it back, I know we were talking outside about the
*  congressional hearing and Jack Dorsey and the questions from the Senator about,
*  are you taking down the content from the Ayatollahs or from the Chinese
*  Jingjing province about the Uyghurs?
*  Uh, you know, when there's sort of speech that leads to offline violence
*  in these other countries, the issue is that these platforms are managing
*  the information commons for countries they don't even speak the language of.
*  And if you think that conspiracy theory sort of dark corners, crazy town of the
*  English internet are bad and we've, we've already taken out like hundreds of
*  whack-a-mole sticks and they've hired hundreds of policy people and hundreds
*  of engineers to deal with that problem.
*  You go to a country like Ethiopia where, um, there's something like 90 major,
*  there's 90 something dialects, I think in the country and six major languages
*  where one of them is the dominant Facebook sort of language.
*  And then the others get persecuted because they actually don't have, um, uh,
*  they don't have a voice on the platform.
*  This is really important that, um, the people in Myanmar who got persecuted and
*  murdered didn't have to be on Facebook for the fake information spread about
*  them to impact them for people to go after them, right?
*  So this is the whole, I can assert something about this minority group.
*  That minority group isn't on Facebook, but if it manipulates the dominant culture
*  to go, we have to go kill them, then they can go do it.
*  And the same thing has happened, um, you know, in, in India, uh, where there's
*  videos uploaded about, Hey, those Muslims, I think they're called flesh killings
*  where they'll say that this, these Muslims killed this cow and in Hindu, um,
*  is it Hinduism?
*  The cows are sacred.
*  Um, the, uh, did I get that right?
*  I'm anyway, I believe you did.
*  Yeah.
*  Um, the, uh, they will post those that go viral on WhatsApp and say, we have to
*  go lynch those Muslims because they killed our sacred, the sacred cows.
*  And they went from something like five of those prepping per year to now
*  hundreds of those happening per year because of fake news being spread again,
*  on Facebook, Facebook about them on WhatsApp about them.
*  And again, they don't have to be on the platform for this to happen to them.
*  Right.
*  So this is critical that, you know, imagine you and I are in all, let's
*  imagine all of your listeners, you know, I don't even know how many you have,
*  like tens of millions, right?
*  And we all listen to this conversation.
*  We say, we don't want to even use Facebook and Twitter or YouTube.
*  We all still, if you live in the U S still live in a country that everyone
*  else will vote based on everything that they're seeing on these platforms.
*  If you zoom out to the global context, all of us don't, we don't use Facebook
*  in Brazil, but if Brazil, which, uh, was heavily the last election was skewed,
*  uh, by Facebook and WhatsApp where something like 87% of people saw at least
*  one of the major fake news stories about Bolsonaro and he got elected and you have
*  people in Brazil chanting Facebook, Facebook, when he wins, he wins and then
*  he sets a new policy to wipe out the Amazon.
*  All of us don't have to be on Facebook to be affected by a leader that wipes out
*  the Amazon and accelerates climate change timelines because of those interconnected
*  effects.
*  So, you know, we at the center for remain technology are looking at this from a
*  global perspective where it's not just the U S election, Facebook manages
*  something like 80 elections per year.
*  And if you think that they're doing all the monitoring that they are for, you
*  know, English speaking American election, most privileged society now look at the
*  hundreds of other countries that they're operating in.
*  Do you think that they're devoting the same resources to, to the other countries?
*  This is so crazy.
*  It's like, is that you Jamie?
*  So weird noise.
*  You hear like a squeaky.
*  Yeah.
*  Maybe it was me.
*  I don't think it is.
*  Might be feedback.
*  There it is.
*  It might be me breathing.
*  I don't know.
*  You have a, you have asthma?
*  I think I had an allergy coming.
*  Oh, I was like, sorry.
*  What's terrifying is that we're talking about from 2012 to 2020 YouTube implementing
*  this program.
*  And then what is the, even the birth of Facebook?
*  What is that like 2002 or three or 2004?
*  So this is such a short timeline and having these massive worldwide implications from
*  the use of these things.
*  When you look at the future, do you look at this like a runaway train that's headed
*  towards a cliff?
*  Yeah.
*  And I think right now this thing is a Frankenstein that it's not like, even if
*  Facebook was aware of all these problems, they don't have the staff unless they
*  hired like hundreds of tens, hundreds of thousands of people, definitely minimum
*  to try to address all these problems.
*  But the paradox we're in is that the very premise of these services is to rely on
*  automation.
*  Like it used to be we had editors and journalists or at least editors or, you
*  know, people who edited even what went on television saying what is credible, what
*  is true.
*  Like, you know, you sat here with, you know, with Alex Jones, even yesterday and
*  you're trying to check him on everything he's saying, right?
*  You're researching and trying to look that stuff up.
*  You're trying to be doing some more responsible communication.
*  The premise of these systems is that you don't do that.
*  Like the reason venture capitalists find social media so profitable and such a good
*  investment is because we generate the content for free.
*  We are the useful idiots, right?
*  Instead of paying a journalist seventy thousand dollars a year to write something
*  credible, we can each be convinced to share our political views and we'll do it
*  knowingly for free.
*  Actually, we don't really know that we're the useful idiots.
*  That's the kind of the point.
*  And then instead of paying an editor one hundred thousand dollars a year to figure
*  out which of those things is true that we want to promote and give exponential
*  reach to, you have an algorithm says, hey, what do people click on the most?
*  What people like the most?
*  And then you realize the quality of the signals that are going into the
*  information environment that we're all sharing is a totally different process.
*  We went from a high quality gated process that cost a lot of money to this really
*  crappy process that costs no money, which makes the company so profitable.
*  And then we fight back for territory, for values when we raise our hands and say,
*  hey, there's a thin spiration video problem for teenagers and anorexia.
*  Hey, there's a mass conspiracy sort of echo chamber problem over here.
*  Hey, there's flat earth sort of issues.
*  And again, these get into tricky topics because we want to know.
*  I know we both believe in free speech and we have this feeling that the solution to
*  bad speech is better, you know, more speech that counters the things that are said.
*  But in a finite attention economy, we don't have the capacity for everyone who
*  gets bad speech to just have a counter response.
*  In fact, what happens right now is that that bad speech rabbit holes into not,
*  I don't want to call worse and worse speech, but more extreme versions of that view
*  that confirms it, because once Facebook knows that that flat earth rabbit hole
*  is good for you at getting your attention back, it wants to give you just more and
*  more of that.
*  It doesn't want to say here's 20 people who disagree with that thing.
*  Right.
*  Right.
*  So I think if you were to imagine a different system, we would ask who are the
*  thinkers that are most open minded and synthesis oriented, where they can actually
*  steel man the other side.
*  Actually, they can do, you know, for this speech, here is the opposite counter
*  argument.
*  They can show that they understand that and imagine those people get lifted up.
*  But notice that none of those people that you and I know, I mean, we're both friends
*  with Eric Weinstein and, you know, I think he's one of these guys who's really good
*  at sort of offering the steel manning.
*  Here's the other side of this.
*  Here's the other side of that.
*  But the people who generally do that aren't the ones who get the tens of millions of
*  followers on these surfaces.
*  It's the black and white extreme outraged oriented thinkers and speakers that get
*  rewarded in these attention economy.
*  And so if you look at how, if I zoom way out and say, how is the entire system
*  behaving?
*  Just like if I zoom out and say climate, you know, the climate system, like how is
*  the entire overall system behaving?
*  It's not producing the kind of information environment on which democracy can survive.
*  Jesus.
*  The thing that troubles me the most that I clearly see you're thinking and I agree
*  with you.
*  Like, I don't see any holes in what you're saying.
*  Like, I don't know how this plays out, but it doesn't look good.
*  And I don't see a solution.
*  It's like, if there are a thousand bison running full steam towards a cliff and they
*  don't realize the cliff is there, I don't see how you pull them back.
*  So I think of it like we're trapped in a body and that's eating itself.
*  So like, it's kind of a cannibalism economy because our economic growth right now with
*  these tech companies is based on eating our own organs.
*  So we're eating our own mental health organs.
*  We're eating the health of our children.
*  We're eating the sorry for being so gnarly about it, but it's it's a cannibalistic
*  system in a system that's hurting itself or eating itself or punching itself.
*  If one of the neurons wakes up in the body, it's not enough to change that.
*  It's going to keep punching itself.
*  But if enough of the neurons wake up and say, this is stupid.
*  Why would we build our system this way?
*  And the reason I'm so excited about the film is that if you have 40 to 50 million people
*  who now recognize that we're living in this sort of cannibalist system in which the
*  economic incentive is to debase the life support systems of your democracy, we can all
*  wake up and say, that's stupid.
*  Let's do something differently.
*  Let's actually change the system.
*  Let's use different platforms.
*  Let's fund different platforms.
*  Let's regulate and tame the existing Frankensteins.
*  And I don't mean regulating speech.
*  I mean, really thoughtfully, how do we change the incentives so it doesn't go to the
*  same race to the bottom?
*  And we have to all recognize that we're now 10 years into this hypnosis experiment of
*  warping of the mind.
*  And like, you know, as friends with some hypnotists, it's like, how do we snap our
*  fingers and get people to say that that artifice there's an inflated level of
*  polarization and hatred right now that especially going into this election, I think
*  we all need to be much more cautious about what's running in our brains right now.
*  Yeah, I don't think most people are generally aware of what's causing this
*  polarization. I think they think it's the climate of society because the president
*  and because of Black Lives Matter and the George Floyd protests and all this jazz.
*  But I don't think they understand that that's exacerbated in a fantastic way by
*  social media and the last 10 years of our addictions to social media and these echo
*  chambers that we all exist in.
*  Yes. So I want to make sure that we're both clear.
*  And I know you agree with this, that these things were already in society to some
*  degree. Right. So we want to make sure we're not saying social media is blamed for
*  all of it. Absolutely not.
*  No, no, no. In fact, gasoline is gasoline.
*  Right. Exactly. It's it's lighter fluid for sparks of polarization.
*  It's lighter fluid for sparks of, you know, more paranoid, which is ironically what
*  everybody was the opposite of everybody, what everybody hoped the Internet was going
*  to be. Right. Everybody hoped the Internet was going to be this bottomless resource of
*  information where everyone was going to be educated in a way they had never
*  experienced before in the history of the human race, where you'd have access to all
*  the answers to all your questions.
*  You know, Eric Weinstein describes as the library of Alexandria in your pocket.
*  Yeah. But no. Well, and I want to be clear so that I'm not against technology or
*  giving people access. In fact, I think a world where everyone had a smartphone and
*  a Google search box and Wikipedia and like a search oriented of YouTube so you can
*  look up health issues and how to do it yourself, fix anything.
*  Sure. Would be awesome. That would be great.
*  I would love that. Just want to be really clear because this is not an anti-technology
*  conversation. It's about, again, this business model that depends on recommending
*  stuff to people, which just to be clear on the polarization front, it social
*  media is more profitable when it gives you your own Truman show that affirms your
*  view of reality every time you flick your finger.
*  Right. Like it.
*  That's going to be more profitable than every time you flick your finger.
*  I actually show you here's a more complex, nuanced picture that disagrees with that.
*  Here's a different way to see it.
*  That won't be nearly as successful.
*  And the best way for people to test this, we actually recommend even after seeing the
*  film to do this, is open up Facebook on two phones, especially like, you know, two
*  partners or people who have the same friends.
*  So you have the same friends on Facebook.
*  You would think if you scroll your feed, you'd see the same thing.
*  The same people you're following.
*  So why wouldn't you see the same thing?
*  But if you swap phones and you actually scroll through their feed for 10 minutes
*  and you scroll through mine for 10 minutes, you'll find that you'll see
*  completely different information.
*  And it won't, you'll also notice that it won't feel very compelling.
*  Like if you asked yourself, my friend, Emily just did this with, with her husband
*  after seeing the film and she literally has the same friends as her husband.
*  And she scrolled through the feed.
*  She's like, this isn't interesting.
*  I wouldn't come back to this.
*  Right.
*  And so we have to, again, realize how subtle and, and, and yeah, just how
*  subtle this has been.
*  I wonder what would happen if I scrolled through my feed because I
*  literally don't use Facebook.
*  What do you use it at all?
*  I only use Instagram.
*  Use Instagram.
*  I stopped using Twitter because it's like a bunch of mental patients throwing shit
*  at each other.
*  Um, and I, uh, very rarely use it.
*  I should say occasionally I'll check some things to see like what the climate is.
*  But, uh, of the cultural climate, but I use Instagram and I
*  Facebook, I used to use Instagram to post a Facebook, but I kind of stopped
*  even doing that because it just, it just seems gross.
*  It's just, and it's these people in these verbose arguments about politics and the
*  economy and world events and just,
*  we have to ask ourselves is, is that medium constructive to solving these
*  problems?
*  No, just not at all.
*  And it's an attention casino, right?
*  The house always wins and we're, you know, Eric, you might, I might see
*  Eric Wednesday in a thread, you know, battling it out or sort of duking it out
*  with someone and maybe even reaching some convergence on something, but it just
*  whizzes by your feet and then it's gone.
*  And all the effort that we're putting in to make these systems work, but then
*  it's just all gone.
*  What do you do?
*  I mean, I try to very minimally use social media overall.
*  Um, luckily the work is so busy that that's easier.
*  Um, I want to say first that, um, you know, on the addiction fronts of these things,
*  I, you know, myself am very sensitive and, you know, easily addicted by these things
*  myself.
*  And that's why I think I notice that you were saying in the social dilemma, it's
*  email for you, huh?
*  Yeah.
*  I, well, I, you know, for me, I, if I refresh my email and pull to refresh like
*  a slot machine, sometimes I'll get invited to meet the president of such and such to
*  advise on regulation.
*  And sometimes I get a stupid newsletter from the politician I don't care about or
*  something, right?
*  Um, so I email is very addictive.
*  Um, it's funny.
*  I talked to, uh, Daniel Kahneman, who wrote the, he's like the founder of
*  behavioral economics.
*  He wrote the book, uh, thinking fast and slow, if you know that one.
*  And he said as well that email was, uh, the most addictive for him.
*  And he, you know, the one thing you'll find is that the people who know most
*  about these sort of persuasive manipulative tricks, they'll say, we're not immune to
*  them just because we know about them.
*  Dan Ariely, who's another famous persuasion, behavioral economics guy talks
*  about flattery and how flattery still feels good.
*  Even if I tell you, I don't mean it.
*  Like, I love that, that sweatshirt.
*  That's an awesome sweatshirt.
*  Where'd you get it?
*  You're just going to bullshit me, but that's, that's the, um, it feels good to get
*  flattery, even if you know that it's not real.
*  Right.
*  And the point being that like, can we have so much evolutionary wiring to care about
*  what other people think of us that just because you know that they're manipulating
*  you and the likes or whatever, it still feels good to get those hundred extra
*  likes on that thing that you posted.
*  Yeah.
*  When did the likes come about?
*  Um, well, let's see.
*  Well, actually, you know, in the film, you know, Justin Rosenstein, uh, who's the
*  inventor of the like button talks about, I think the first version was something
*  called beacon and it arrived in 2006, I think, but then the simple, like one click
*  like button was like a little bit later, like 2008, 2009.
*  Are you worried that it's going to be more and more invasive?
*  I mean, you think about the problems we're dealing with now with Facebook and Twitter
*  and Instagram, all these within the last decade or so, what, what do we have to
*  look forward to?
*  I mean, is there something on the horizon that's going to be even more invasive?
*  Well, we have to change this system because as you said, technology is only going to
*  get it is only going to get more immersed into our lives and infused into our lives.
*  Not less.
*  Is technology going to get more persuasive or less persuasive?
*  More, more sure.
*  Is AI going to get better at predicting our next move or less good at predicting
*  our next move?
*  It's almost like we have to eliminate that.
*  And I mean, it would be really hard to tell them you can't use algorithms anymore that
*  depend on people's attention spans.
*  Right.
*  It would be really hard, but it seems like the only way for the internet to be pure.
*  Correct.
*  I think, I think of this like the environmental movement.
*  I mean, some people have compared the film, the social dilemma to Rachel Carson's
*  silent spring, right?
*  Where that was the birth.
*  That was the book that birthed the environmental movement.
*  And that was in a Republican administration, the Nixon administration.
*  We actually passed, we created the EPA, the environmental protection agency.
*  We went from a world where we said the environment is something we don't pay
*  attention to, to we passed a bunch of, I forgot the laws we passed between 1963 and
*  1972, over a decade, we started caring about the environment.
*  We created things that protected the national parks.
*  We, and I think that's kind of what's going on here that, you know, imagine, for
*  example, it is illegal to show advertising on youth oriented social media apps
*  between 12 a.m.
*  and 6 a.m.
*  because you're basically monetizing loneliness and lack of sleep.
*  Right.
*  Like imagine that you cannot advertise during those hours because we say that
*  like a national park, our children's attention between this is a very minimal
*  example, but it's to be like, you know, taking the most obvious piece of low
*  hanging fruit and land and say, let's quarantine this off and say, this is sacred.
*  But isn't the problem like the environmental protection agency?
*  It resonates with most people.
*  The idea of, oh, let's protect the world for our children.
*  Right.
*  There's not a lot of people profiting off of polluting the rivers.
*  Right.
*  But when you look over hunting, you know, certain lands or overfishing certain
*  fisheries and collapsing them.
*  I mean, there are, if you have big enough corporations that are based on an
*  infinite growth profit model, you know, operating with less and less resources
*  to get, this is a problem we faced before.
*  For sure.
*  For sure.
*  But it's not the same sort of scale as 300 X amount of millions of people.
*  And a vast majority of them are using some form of social media.
*  And also this is not something that really resonates in a very clear, like
*  one plus one equals two way, like the environmental protection agency, it's,
*  it makes sense.
*  Like if you ask people, should you be able to throw garbage into the ocean?
*  Everyone's going to say, no, that's a terrible idea.
*  Should you be able to make an algorithm that shows people what they're interested
*  in on YouTube?
*  Like, yeah, what's wrong with that?
*  Well, it's more like sugar, right?
*  Because it's sugar is always going to taste way better than something else.
*  Cause our evolutionary heritage says like that's rare.
*  And so we should pay more attention to it.
*  This is like sugar for the fame lottery for attention, for social approval.
*  And so it's always going to feel good and we need to have consciousness about it.
*  And we haven't banned sugar, but we have created a new conversation about what
*  healthy, you know, eating is, right?
*  I mean, there's a whole new fitness movement and sort of yoga and all these
*  other things that people care more about their bodies and health than they
*  probably ever have.
*  I think many of us wouldn't have thought we'd ever reach it through, you know,
*  get through the period of soda being at the sort of pinnacle popularity that is.
*  I think in 2013 or 14 was the year that water crossed over as being more of a
*  successful drinking product than, than soda.
*  I think.
*  Really?
*  I think that's true.
*  You might want to look that up, but.
*  So I think we could have something like that here.
*  We have to, I think of it this way.
*  If you want to even get kind of weirdly, I don't know, spiritual or something
*  about it, which is we are the only species that could even know that we've,
*  we've, we're doing this to ourselves.
*  Right.
*  Like we're the only species with a capacity for self-awareness to know that
*  we have actually like roped ourselves into this matrix of like literally the
*  matrix of, of sort of undermining our own psychological weaknesses, like a, a
*  lion that somehow manipulated its environment so that there's gazelles
*  everywhere and is like overeating on gazelles doesn't have the self-awareness
*  to know, wait a second, if we keep doing this, this is going to cause all these
*  other problems.
*  It can't do that because it's brain doesn't have that capacity.
*  Our brain, we do have the capacity for self-awareness.
*  We can name negativity bias, which is that if I have a hundred comments
*  and 99 are positive, my brain goes to the negative.
*  We can name that.
*  And once we're aware of it, we get some agency back.
*  We can name that.
*  We have a draw towards social approval.
*  So when I see I've been tagged in a photo, I know that they're just
*  manipulating my social approval.
*  We can name social reciprocity, which is when I get all those text
*  messages and I feel, oh, I have to get back to all these people.
*  Well, that's just an inbuilt bias that we have to get back.
*  Reciprocity.
*  We have to get back to people who do give it, give stuff to us.
*  The more we name our own biases, like confirmation bias, we can name that
*  my brain is more likely to feel good getting information that I already
*  agree with than information that disagrees with me.
*  Once I know that about myself, I can get more agency back.
*  And we're the only like species that we know of that has the capacity to
*  realize that we're in a self terminating sort of system.
*  And we have to change that by understanding our own weaknesses and that
*  we've created the system that is undermining ourselves.
*  And I think the film is doing that for a lot of people.
*  It certainly is, but I think it needs more.
*  It's like inspiration.
*  It needs a refresher on a regular basis.
*  Right. Do you feel this massive obligation to be that guy that is out
*  there sort of as the Paul Revere of the technology influence
*  invasion? I just see these problems and I want them to go away.
*  You know, I didn't I, you know, didn't desire and wake up to run a social
*  movement, but honestly, right now, that's what we're trying to do
*  with the Center for Human Technologies.
*  We realize that before the success of the film, we were actually more focused
*  on working with technologists inside the industry.
*  You know, I come from Silicon Valley.
*  Many of my friends are executives at the companies and we have these inside
*  relationships. We focused at that level.
*  We also worked with policymakers and we were trying to speak to policymakers.
*  We weren't trying to mobilize the whole world against this problem.
*  But with the film, suddenly we as an organization have had to do that.
*  And we're frankly, I wish we had I'm just being really honestly, we
*  I really wish we'd had those funnels so that people who saw the film
*  could have landed into, you know, a carefully designed funnel where we
*  actually started mobilizing people to deal with this issue, because there are
*  ways we can do it. We can pass certain laws.
*  We have to have a new cultural sort of set of norms about how do we want to
*  show up and use the system?
*  You know, families and schools can have whole new protocols of how do we want
*  to do group migrations?
*  Because one of the problems is that if a teenager says by themselves, well, I
*  saw the film, I'm going to delete my Instagram account by myself or TikTok
*  account by myself.
*  That's not enough because all their friends are still using Instagram and
*  TikTok and they're still going to talk about who's dating who or gossip about
*  this or homework or whatever on those services.
*  And so the services, Instagram and TikTok prey on social exclusion that you will
*  feel excluded if you don't participate.
*  And the way to solve that is to get whole schools or families together, like
*  different parent groups, whatever together and do a group migration from
*  Instagram to Signal or iMessage or some kind of group thread that way.
*  Because notice that when you, as you said, Apple's a pretty good actor in this
*  space. If I make a FaceTime call to you, FaceTime isn't trying to monetize my
*  attention. It's just sitting there being like, yeah, how can I help you have a
*  good face? It's close to face to face conversation that's possible.
*  Jamie pulled up an article earlier that was saying that Apple was creating its
*  own search engine.
*  Yeah.
*  I hope that is the case.
*  And I hope that if it is the case, they apply the same sort of ethics that they
*  have towards sharing your information that they do with other things to their
*  search engine.
*  But I wonder if there would be some sort of value in them creating a social media
*  platform that doesn't rely on that sort of algorithm.
*  Yeah. Well, I think in general, one of the exciting trends that has happened since
*  the film is there's actually many more people trying to build alternatives,
*  social media products that are not based on these business models.
*  Yeah. I could name a few, but I don't want to be endorsing it.
*  I mean, there's people building Marco Polo, Clubhouse, Wikipedia is trying to
*  build a sort of for a nonprofit version.
*  I always forget the names of these things.
*  But the interesting thing is that for the first time, people are trying to build
*  something else because now there's enough people who feel disgusted by the present
*  state of affairs. And that wouldn't be possible unless we created a kind of a
*  cultural movement based on something like the film that reaches a lot of people.
*  It's interesting that you made this comparison to the Environmental Protection
*  Agency because there's kind of a parallel in the way other countries handle the
*  environment versus the way we do and how it makes them competitive.
*  And that's always been the Republican argument for not getting rid of certain
*  fossil fuels and coal and all sorts of things that have a negative consequence
*  that we we need to be competitive with China.
*  We need to be competitive with these other countries that don't have these
*  regulations in effect. The concern would be, well, first of all,
*  the problem is these companies are global, right?
*  Like Facebook is global.
*  If they put these regulations on America but didn't put these regulations worldwide,
*  then wouldn't they use the income and the algorithm in other countries unchecked
*  and have this tremendous negative consequence and gather up all this money?
*  Which is why just like sugar, it's like everyone around the world has to understand
*  and be more antagonistic.
*  And not like sugar is evil, but just you have to have a common awareness about the
*  problem. But how could you educate people that like if you're talking about some
*  country like Myanmar or these other countries that have had these serious
*  consequences because of Facebook, how could you possibly get our ideas across to
*  them if we don't even know their language?
*  And it's this this system that's already set up in this very advantageous way for
*  them where Facebook comes on their phone.
*  Yeah. How could you hit the brakes on that?
*  Well, I mean, first, I just want to say this is an incredibly hard and depressing
*  problem. It's the scale of it, right?
*  Right.
*  You need something like a global language, independent global self-awareness about this
*  problem. Now, again, I don't want to be tooting the horn about the film, but the thing
*  I'm excited about is it launched on Netflix in 190 countries and in 30 languages.
*  So you should toot the horn.
*  Well, yeah.
*  Toot it.
*  Yeah. Well, I think, you know, the film was seen in 30 languages.
*  So, you know, the cool thing is I wish I could show the world my inbox.
*  I think people see the film and they feel like, oh, my God, this is huge.
*  And I'm a huge problem and I'm all alone.
*  How are we ever going to fix this?
*  But I get emails every day from Indonesia, Chile, Argentina, Brazil, people saying, oh,
*  my God, this is exactly what's going on in my country.
*  I mean, I've never felt more optimistic and I've felt really pessimistic for the
*  last eight years working on this because there really hasn't been enough movement.
*  But I think for the first time, there's a global awareness now that we could then start
*  to mobilize. I know the EU is mobilizing, Canada is mobilizing, Australia is mobilizing,
*  California State is mobilizing with Prop 24.
*  There's a whole bunch of movement now in the space and they have a new rhetorical
*  arsenal of why we have to make this bigger transition.
*  Now, you know, are we going to get all the countries that are the six different major
*  dialects in Ethiopia where they're going to know about this?
*  I don't think the film was translated into all those dialects.
*  I think we need to do more.
*  It's a really, really hard, messy problem.
*  But on the topic of if we don't do it, someone else will.
*  You know, one interesting thing in the environmental movement was there's a great
*  WNYC radio piece about the history of lead and when we regulated lead.
*  I don't know. Do you know anything about this?
*  Yeah, I do. Yeah.
*  The accuracy matches up with your experience.
*  My understanding is that obviously lead was this sort of miracle thing.
*  We put it in paint, we put it in gas.
*  It was great. And then the way we figured out that we should regulate lead
*  out of our sort of infused product supply is by proving there is this guy
*  who proved that it dropped kids IQ by four points for every, I think,
*  microgram per deciliter, I think.
*  So in other words, for the amount of if you had a microgram of lead per deciliter
*  of either I'm guessing air, it would drop the IQ of kids by four points.
*  And they measured this by actually doing a sample on their teeth or something,
*  because lead shows up in your bones, I think.
*  And they proved that if the IQ points dropped by four points,
*  it would lower future age-earning, age-earning, excuse me,
*  wage-earning potential of those kids, which would then lower the GDP of the country,
*  because it would be shifting the IQ of the entire country down by four points,
*  if not more, based on how much lead is in the environment.
*  If you zoom out and say, is social media now, let's replace the word IQ,
*  which is also a rot term because there's like a whole bunch of views about how
*  that's designed in certain ways and not others and measuring intelligence.
*  Let's replace IQ with problem solving capacity.
*  What is your problem solving capacity, which is actually how they talk about it
*  in this radio episode and imagine that we have a societal IQ
*  or a societal problem solving capacity.
*  The US has a societal IQ.
*  Russia has a societal IQ.
*  Germany has a societal IQ.
*  How good is a country at solving its problems?
*  Now imagine that what does social media do to our societal IQ?
*  What distorts our ideas, gives us a bunch of false narratives.
*  It fills us with misinformation and it makes it impossible to agree with each other.
*  And in a democracy, if you don't agree with each other and you can't even do
*  compromise, you have to recognize that politics is invented to avoid warfare.
*  So we have compromise and understanding so that we don't
*  like physically are violent with each other.
*  We have compromise and conversation.
*  If social media makes compromise conversation and under shared
*  understanding and shared truth impossible.
*  It doesn't drop our societal IQ by four points.
*  It drops it to zero because you can't solve any problem, whether it's human
*  trafficking or poverty or climate issues or, um, you know, racial injustice,
*  whatever it is that you care about.
*  It depends on us having some shared view about what we agree on.
*  And by the way, and on the optimistic side, there are countries like Taiwan
*  that have actually built a digital democratic sort of social media type thing.
*  Uh, Audrey Tang, you should have Audrey Tang on your show.
*  She's amazing.
*  She's the digital minister of Taiwan and they've actually built a system
*  that rewards unlikely consensus.
*  So when two people who would traditionally disagree post something online,
*  um, and when, when they act, when two people traditionally disagree,
*  actually agree on something, that's what gets boosted to the top of the way
*  that we look at our information feeds.
*  Really?
*  Yeah.
*  So it's about finding consensus where there'd be unlikely and saying, Hey,
*  actually, you know, you, Joe and Tristan, you typically, you agree, you
*  disagree on these six things.
*  But you agree on these three things and of things that we're going to encourage
*  you to talk about on a menu, we hand you a menu of the things you agree on.
*  And how did they manipulate that?
*  Um, honestly, we, we did a great interview with her on our podcast, um,
*  that people can listen to.
*  Uh, I think you should have her on.
*  She honestly, I would love to, but what is your podcast again?
*  Tell people it's called a urine divided attention.
*  Um, and with the interview is with Audrey Tang is her name.
*  Uh, and I think that's, this is one model of how do you have, you know, sort of
*  digital media bolted onto the top of a democracy and have it work better as
*  opposed to how do you, it just degrades into kind of nonsense and polarization
*  and inability to agree.
*  And that's what we need.
*  Such a unique situation too, right?
*  Because China doesn't recognize them and there's a real threat that they're
*  going to be invaded by China.
*  Correct.
*  And so what's interesting about Taiwan is there's, we didn't, we haven't talked
*  about the disinformation issues, but it's under, like you said, not just physical
*  threat from China, but massive propaganda, disinformation campaigns are
*  trying to run there, right?
*  I'm sure.
*  And so what's amazing is that their digital media system is good at, um,
*  dealing with these disinformation campaigns and conspiracy theories and
*  other things, even in the face of a huge threat like China, but there's more
*  binding energy in the country because they all know that there's a tiny island
*  and there's a looming threat of this big country.
*  Whereas the United States, we're not this tiny island with a looming threat
*  elsewhere.
*  In fact, many people don't know or don't think that there's actually
*  information warfare going on.
*  Um, I actually think it's really important to point out to people that,
*  um, the social media is one of our biggest national security risks because
*  while we're obsessed with protecting our physical borders and building walls and,
*  you know, spending a trillion dollars, we doing the nuclear fleet.
*  Um, we left the digital border wide open.
*  Like if Russia or China try to fly a plane into the United States, our
*  Pentagon and billions of dollars of defense infrastructure from Raytheon
*  and Boeing or whatever, we'll shoot that thing down and it doesn't get in.
*  If they try to come into the country, they'll get stopped by the
*  passport control system.
*  Ideally.
*  If they try to fly, if Russia or China try to fly an information bomb into the
*  country, instead of being met by the department of defense, they're met by a
*  Facebook algorithm with a white glove that says exactly which zip code you want
*  to target.
*  Like it's the opposite of protection.
*  So social media makes us more vulnerable.
*  I think of it like, if you imagine like a bank that spent billions of dollars,
*  um, you know, surrounding the bank with physical bodyguards, right?
*  Like just the buffest guys and every single quarter, you just totally
*  secured the bank.
*  But then you installed on the bank, a computer system that everyone interacts
*  with and no one changes the default password from like lowercase password.
*  Anyone can hack in.
*  That's what we do when we install Facebook in our society, or you install
*  Facebook in Ethiopia, because if you think Russia or China, you know, or Iran
*  or South Korea, excuse me, North Korea, um, influencing our election is bad.
*  Just keep in mind the like dozens of countries throughout Africa where we
*  actually know, um, recently there was a huge campaign that the Stanford, um,
*  cyber policy center did a report on of Russia targeting, I think something like
*  seven or eight major countries and disinformation campaigns running in those
*  countries.
*  Um, or the Facebook whistleblower who came out about a month ago, uh, Sophie
*  Zhang, I think is her name, uh, saying that she personally had to step in to
*  deal with disinformation campaigns in Honduras, as urgent by John, um, I think
*  Greece or some other countries like that.
*  So the scale of what these technology companies are managing, they're managing
*  the information environments for all these, these countries, but they don't
*  have the resources to do it.
*  So they, not only that they're not trained to do it, they're not qualified
*  to do it, they're making up as they go along 20 to 30 to four and they're way
*  behind the curve.
*  When, when I had Renee to rest on and she detailed all the issues with the, uh,
*  internet research agency in Russia and what they did during the 2016 campaign
*  for both sides.
*  I mean, the ideas that had just promoted Trump, but they were basically sowing
*  the seeds of, uh, just the decline of the democracy.
*  They were trying to figure out how to create turmoil and they were doing it in
*  this very bizarre calculated way that it didn't see it.
*  It was hard to see like, what's the end game here?
*  Well, the end game is to have everybody fight.
*  I mean, it's really what the end game was.
*  And if I'm, you know, one of our major adversaries, you know, after World War
*  two, there was no ability to use kinetic like nukes or something on the bigger
*  countries, right?
*  Like that's all done.
*  So the, what's the best way to take down the biggest, you know, country, you know,
*  on the planet, on the block, you use its own internal tensions against itself.
*  This is what SunZoo would tell you to do.
*  And that's never been easier because of Facebook and because of these
*  platforms being open to do this manipulation.
*  And if I'm looking now, we're four days away from the U S elections or something
*  like that, when this goes out, Jesus Christ, there is never, we have never been
*  more destabilized as a country until now.
*  I mean, this is the most destabilized you've probably ever been, I would say.
*  Um, and polarized, um, maybe people would argue the civil war was the worst, but in
*  recent history, um, there is maximum incentive for foreign actors to drive up
*  again, not one side or the other, but to drive us into conflict.
*  So I would really, you know, I think what we all need to do is recognize how much
*  incentive there is to plant stories, to actually have so physical violence on the
*  streets.
*  I think there was just a story, wasn't we talking about this morning that, um,
*  there was some kind of truck, I think in Philadelphia or DC loaded with
*  explosives or something like this.
*  There's, there's such an incentive to try to, you know, throw the agent
*  provocateur, like throw the first stone, throw the first, um, you know, Molotov
*  cocktail, throw the first, uh, you know, make the first shot fired, uh, to drive
*  up that conflict.
*  And I think we have to realize how much that may be artificially motivated.
*  Very much so.
*  And the Renee de Resta podcast that I did where she went into depth about all the
*  different ways that they did it.
*  The most curious one being funny memes.
*  Yep.
*  That there's so many of the memes that you read that you laugh at.
*  Yeah.
*  Well, there's, it was just so weird.
*  That's there were humorous and she said she looked at probably a hundred thousand
*  memes.
*  And the funny thing is you actually can agree with them, right?
*  Like you would laugh at them.
*  It's like, oh, you know,
*  And they're being constructed by foreign agents that are doing this to try to mock
*  certain aspects of our society and pit people against each other and create a
*  mockery.
*  And, you know, back in 2016, there was no, there was very little collaboration
*  between our defense industry and CIA and DOD and people like that, uh, and the
*  tech platforms and the tech platform said it's government's job to deal with
*  if foreign actors are doing these things.
*  How do you stop something like the IRA?
*  Like say if they're creating memes in particular and they're funny memes.
*  Well, so one of the issues that Renee brings up and I'm just a huge fan of her
*  and her work, uh, is, as am I, yeah.
*  Uh, is that if I'm, you know, China, I don't need to invent some fake news
*  story.
*  I just find someone in your society who's already saying what I want you to be
*  talking about.
*  And I just like amplify them up.
*  I take that dial and I just turn it up to 10, right?
*  So I find your Texas secessionists and like, Oh, Texas, that would be a good
*  thing if I'm trying to rip the country apart.
*  So I'm going to take those tests as the session is and the California
*  secessionists, and I'm just going to dial them up to 10.
*  So those are the ones we hear from.
*  Now, if you're trying to stop me in your Facebook and you're the integrity team
*  or something on what grounds are you trying to stop me because it's your own
*  people, your own free speech.
*  I'm just the one amplifying the one I want to be out there.
*  Right.
*  And so that's what gets tricky about this is I think our moral concepts that we
*  hold so dear of free speech are inadequate in an attention economy that is
*  hackable.
*  And it's really more about what's getting the attention rather than what are
*  individuals saying or can't say.
*  And, you know, again, they've created this Frankenstein where they're making
*  mostly automated decisions about who's looking like what pattern behavior or
*  coordinated and authentic behavior here or that, and they're shutting down.
*  I don't know if people know this people, Facebook shut down 2 billion fake
*  accounts.
*  I think this is a stat from a year ago.
*  They shut down 2 billion fake accounts.
*  They have 3 billion active real users.
*  Do you think that those 2 billion were the perfect, like, real, you know, real
*  fake accounts and they didn't miss any or they didn't overwhelm and took some
*  real accounts down with it?
*  You know, our friend, Brett Weinstein, he just got taken down by Facebook.
*  I think you saw that.
*  That seemed calculated though.
*  Facebook has shut down 5.4 billion fake accounts this year.
*  And that was in November 2019.
*  Oh my God.
*  Oh my God.
*  That is insane.
*  That's so many.
*  And so again, it's the scale that these things are operating at.
*  And that's why, you know, when Brett got his thing taken down, I didn't like that.
*  But I, it's not like there's this vendetta against Brett, right?
*  It's, I don't know about that.
*  That seemed to me to be a calculated thing because, uh, you know, Eric, uh,
*  actually tweeted about it saying that, you know, you could probably find the
*  tweet cause I retweeted it.
*  Like basically it was reviewed by a person.
*  So you're lying.
*  He's like, this is not something that was taken down by an algorithm.
*  He believes that it was because it was unity 2020 platform where they were
*  trying to bring together conservatives and liberals and try to find some common
*  ground and create like a third party candidate that combines the best of both worlds.
*  I don't understand what policy is unique.
*  Unity 2020 thing was going up against.
*  Like I have no idea.
*  It's going against the two party system.
*  The idea is that it's taking away votes from Biden and then it may help Trump win.
*  Right.
*  Banned him off Twitter as well.
*  You know that too.
*  They, they blocked the account or something from the band.
*  The entire account.
*  They banned the 20 unity 2020 account.
*  Yeah.
*  Unity.
*  Yeah.
*  I mean, literally unity.
*  They're like, Nope, no unity.
*  Fuck you.
*  We want Biden.
*  Yeah.
*  The political bias on social media is undeniable and that's maybe the least of
*  our concerns in the long run, but it's a tremendous issue.
*  And it also, it, it for sure sows the seeds of discontent and it creates more
*  animosity and it creates more conflict.
*  The interesting thing is that if I'm one of our adversaries, I see that there is
*  this view that people don't like the social media platforms that I want them to be more
*  like, let's say I'm Russia or China.
*  Right.
*  And I'm currently using Facebook and Twitter successfully to run information
*  campaigns.
*  And then I want them, I can actually plant a story so that they end up shutting it
*  down and shutting down conservatives or shutting down one side, which then forces
*  the platforms to open up more so that I then Russia or China can keep manipulating
*  even more.
*  Right.
*  I understand.
*  Yeah.
*  So right now they, they want it to be a free for all where there's no moderation
*  at all, because that allows them to get in and they can weaponize the
*  conversation against itself.
*  Right.
*  I don't see a way out of this Tristan.
*  We have to all be aware of it.
*  I mean, I, I,
*  even if we are all aware of it, it seems so pervasive.
*  Yeah.
*  Well, it's not just pervasive.
*  It's like I said, it's weird.
*  10 years into this hypnosis experiment, this is the largest psychological
*  experiment we've ever run on humanity.
*  It's insane.
*  It is insane.
*  And it, and it's also with tools that never existed before evolutionarily.
*  So like we, we, we really are not designed just the way these brightly lit metal
*  devices and glass devices interact with your brain.
*  They're so enthralling.
*  Right.
*  And we, we've never had to resist anything like this before.
*  We, the things we've had to resist is don't go to the bar.
*  You know, you have an alcohol problem, stop smoking cigarettes.
*  It'll give you cancer.
*  Right.
*  We've never had a thing that does so much.
*  Right.
*  You can call your mom, you can text a good friend.
*  You can, you can receive your news.
*  You can get amazing email about this project you're working at, and it could
*  suck up your time staring at butts.
*  And the, and the infusion of the things that you, that are necessary for life,
*  like text messaging or like looking something up are infused and right next
*  to all of the sort of corrupt stuff.
*  Right.
*  And if you're using it to order food and if you're using it to get an Uber and.
*  Right.
*  But imagine if we all wiped our phones of all the extractive business model stuff
*  and we only had the tools.
*  Like you thought about using a light phone.
*  Yeah.
*  It's funny.
*  I, those guys used to be brought up in my awareness more often.
*  Um, uh, for those who don't know, it's like, it's like a, a mini one of the
*  guys on the documentary is one of the creators of it, right?
*  No, I think you're thinking of Tim Kendall who started and he's the guy who
*  invented who brought in Facebook's business model of advertising and he runs
*  a company now called moment that shows you, uh, the number of hours you spend
*  in different apps and helps you use it.
*  I thought someone involved in the documentary was also a part of the
*  light phone team.
*  No, no, not, not, not officially.
*  No, I don't think so.
*  Um, but the light phone is like a basically a black and white, black and
*  white phone thing text.
*  And I think it does, it plays music now, which I was like, well, that's a mistake.
*  Right.
*  Like that's a slippery slope.
*  That's the thing.
*  And we have to all be comfortable with losing access to things that we might love.
*  I like, oh, maybe you do want to take notes this time, but you don't have your
*  full keyboard to do that.
*  And are you willing to that?
*  I think the thing is one thing people can do is to take like a digital Sabbath one
*  day a week off completely because the very imagine if, if you got several hundred
*  million people to do that, that drops the revenue of these companies by like 15%.
*  Cause that's one out of seven days that you're not on the system.
*  So long as you don't rebalance and use it more on the other days.
*  I'm inclined to think that Apple's their solution is really the way out of this,
*  that the two opt out of all sharing of your information.
*  And, uh, if, if they could come up with some sort of a social media platform that
*  kept that as an ethic, I mean, it might allow us to communicate with each other,
*  but stop all this algorithm nonsense.
*  And look, if anybody has the power to do it, they have so much goddamn money.
*  Totally.
*  Well, and also they're like the, you know, people talk about, you know, the
*  government regulating these platforms, but Apple is kind of the government that
*  can regulate the attention economy.
*  When they do this thing we talked about earlier of saying, do you want to be
*  tracked?
*  Right.
*  And they give you this option when like 99% of people are going to say, no, I
*  don't want to be tracked.
*  When they do that, they just put a 30% tax on all the advertising based
*  businesses, cause now you don't get as personalized an ad, which means they make
*  less money, which means that business model is less attractive to venture
*  capitalists to fund the next thing, which means so they're actually enacting a
*  kind of a carbon tax, but it's like a, you know, on the polluting stuff, right?
*  They're enacting a kind of social media polluting stuff.
*  They're taxing by 30%, but they could do more than that.
*  Like imagine, you know, they have this 30 70 split on app developers get 70% of
*  the revenue when you buy stuff and Apple keeps 30%.
*  They could modify that percentage based on how much sort of social value that
*  those things are delivering to society.
*  So this gets a little bit weird and people may not like this, but if you think
*  about who's the real customer that we want to be, like how do we want things
*  oriented?
*  How should we, if I'm an app developer, I want to make money the more I'm helping
*  society and helping individuals, not how much I'm extracting and stealing their
*  time and attention.
*  Um, and imagine that governments in the future actually paid, um, like some kind
*  of budget into, let's say the app store, there's, there's antitrust issues with
*  this, but you pay money into the app store.
*  And then as apps started helping people with more social outcomes, like let's
*  say learning programs or schools or things like Khan Academy, things like
*  this, that more money flows in the direction of where people got that value.
*  And it was that, that revenue split between Apple and the app developers, um, ends
*  up going more to things that end up helping people as opposed to things that
*  were just good at capturing attention and monetizing a zombie behavior.
*  One of my favorite lines in the film is Justin Rosenstein from the like button,
*  um, saying that, you know, so long as a whale is worth more dead than alive and a
*  tree is worth more as lumber and two by fours than a living tree.
*  Now we're the whale, we're the tree.
*  We're worth more when we have predictable zombie-like behaviors, when we're more
*  addicted, distracted, outraged, polarized, and disinformed than if we're a living,
*  thriving citizen or a growing child that's like playing with their friends.
*  And I think that that kind of distinction that just like we protect national parks
*  or we protect, you know, certain fisheries and we don't kill the whales in those
*  areas or something, we need to really protect, like we have to call out what's
*  sacred to us now.
*  Yeah, it's, um, it's an excellent message.
*  My problem that I see is that I just don't know how well that message is going to be
*  absorbed on the people that are already in the trance.
*  I mean, I think it's so difficult for people to put things down.
*  I mean, how, like I was telling you how difficult it is for me to tell my friends,
*  don't read the comments.
*  Right.
*  You know, right.
*  It's, it's hard to have that kind of discipline and it's hard to have that kind
*  of, because people do get bored.
*  And when they get bored, like if you're waiting in line for somewhere, you pull out
*  your phone, you're at the doctor's office, you pull out your phone, like.
*  Totally.
*  I mean, and that's why, you know, and I do that, right?
*  I mean, this is incredibly, right?
*  This is incredibly hard.
*  Um, back in the day, uh, when I was at Google trying to change, I tried to change
*  Google from the inside for two years before leaving.
*  What was it like there?
*  Please share your experiences because when you said you try to change it from
*  the inside, what kind of resistance were you met with and what was their reaction
*  to these thoughts that you had about the unbelievable negative consequences of.
*  Well, this is in 2013.
*  So we didn't know about all the negative consequences, but you saw the writing on
*  the wall, at least some of it.
*  Some of it.
*  Yeah.
*  I mean, the, the notion that things were competing for attention, which would mean
*  that they would need to compete to get more and more persuasive and hack more and
*  more of our vulnerabilities and that that would grow.
*  That was the core insight.
*  I didn't know that it would lead to polarization or conspiracy
*  theory, like recommendations, but I would, I did know, you know, more addiction,
*  kids having less weaker relationships.
*  When did it occur to you?
*  Like, what were your initial feelings?
*  Um, I was on a hiking trip in the Santa Cruz mountains with our
*  co-founder now, um, Aza Raskin.
*  Um, it's funny enough, our co-founder Aza, he, his dad was Jeff Raskin, who
*  invented the Macintosh project at Apple.
*  I don't know if you know the history there, but he started the Macintosh
*  project and actually came up with the word, um, humane to describe the humane
*  interface and that's where our, our name and our work comes from is from his
*  father's work.
*  He and I were in the mountains in Santa Cruz and just experiencing nature and
*  just came back and realized like this.
*  All of this stuff that we've built is just distracting us from the
*  stuff that's really important.
*  And that's when coming back from that trip, um, I made the first
*  Google deck that then spread virally throughout the company saying never
*  before in history have, you know, 50 designers, uh, you know, white 20 to 35
*  year old engineers who look like me to hold the collective psyche of humanity.
*  And then that presentation was released and about, you know, 10,000
*  people at Google saw it.
*  It was actually the number one, um, meme within the company.
*  They have this internal thing inside of Google called MoMA that has like,
*  people can post like gifts and memes about various topics.
*  And it was the number one meme that, Hey, we need to talk about this at this
*  week's TGIF, which is the like weekly, thank God it's Friday type, uh, company
*  meeting.
*  Um, it didn't get talked about, but I got emails from across the company saying,
*  we definitely need to do something about this.
*  Um, it was just very hard to get momentum on it.
*  And really the key interfaces to change within Google are Chrome and Android,
*  cause those are the neutral portals into which you're then using apps and
*  notifications and websites and all of that.
*  Like those are the kinds of governments of the attention economy that Google
*  runs.
*  And when you worked there, did they, um, did you have to use Android?
*  Was it part of the requirement to work there?
*  No, I mean, a lot of people had Android phones.
*  I still used an iPhone.
*  Was it an issue?
*  No, no.
*  I mean, people, because they realized that they needed products to work on, on
*  all the phones.
*  I mean, if you work directly on Android, then you would have to use an Android
*  phone.
*  But we tried to get, you know, some of those things like, um, the screen time
*  features that are now launched, you know, so everyone now has on their phone, like
*  it shows you the number of hours or whatever you spend on Android as well.
*  It is.
*  Yeah.
*  And actually that came, I think, as a result of this advocacy and that's
*  shipping on a billion phones, which shows you, you can, you can change this
*  stuff, right?
*  Like that goes against their financial interest.
*  People spending less time in their phones, getting less notifications.
*  But it doesn't work.
*  Well, correct.
*  So it doesn't actually work is the thing.
*  Yeah.
*  And let's separate the intention and the fact that they did it.
*  It's like labels on cigarettes that tell you it's going to give you cancer.
*  Like by the time you're buying them, you're already hooked.
*  Correct.
*  I mean, it's even worse than imagine.
*  Like, um, every cigarette cigarette box had like, um, a little pencil inside.
*  So you can mark, there was like little streaks that said the number of days in a
*  row you haven't smoked and you could like mark each day.
*  It's like, it's too late, right?
*  Like, um, it's just the wrong paradigm.
*  Um, the, the fundamental thing we have to change is the incentives and how money
*  flows because we want money flowing in the direction of the more of these things
*  help us like, let me give you a concrete example, like, let's say, um, you want to
*  learn a musical instrument and you go to YouTube to pick up ukulele or whatever.
*  Um, and you're seeing how to play the ukulele.
*  Like from that point in a system that was designed in a humane and sort of time
*  well spent kind of way, it would really ask you instead of saying here's 20 more
*  videos that are going to just like suck you down a rabbit hole, it would sort of
*  be more oriented towards what do you really need help with?
*  Like, do you need to buy ukulele?
*  Here's a link to Amazon to get the ukulele.
*  Do you looking for a ukulele teacher?
*  Let me do a quick scan on your Facebook or Twitter search to find out which of
*  those people are ukulele teachers.
*  Do you need instant like a tutoring?
*  Because there's actually the service you never heard of called Skillshare or
*  something like that, where you can get instant ukulele tutoring.
*  And if we're really designing these things to be about what would most help you
*  next, you know, we're only as good as the menu of choices on life's menu.
*  And right now the menu is here's something else to addict you and keep you hooked
*  instead of here's a next step that would actually be on the trajectory of helping
*  people live their lives better.
*  But you'd have to incentivize the companies because like there's so much
*  incentive on getting you addicted because there's so much financial reward.
*  What would be the financial reward that they could have to get you to something
*  that would be helpful for you, like lessons or this?
*  I mean, so one way that that could work is like, let's say people pay a monthly
*  subscription of like, I don't know, 20 bucks a month or something.
*  So it's never going to work.
*  I get you.
*  But like, let's say you pay some, you put money into a pot where the pot.
*  But then we have the problem.
*  The problem is like the cost of money versus free.
*  Like there was a, there's a company that still exists for now that was trying to
*  do the Netflix of podcasting and they approached us and they're like, we're
*  just going to get all these people together and they're going to make them.
*  People are going to pay to use your podcast.
*  I'm like, why would they do that?
*  When podcasts are free?
*  Yeah.
*  Like that's one of the reasons why podcasts work is because they're free.
*  Right.
*  When things are free, they're, they're attractive.
*  It's easy.
*  When things cost money, you have to have something that's extraordinary.
*  Like Netflix.
*  Yeah.
*  Like when you say the Netflix of podcasting, we'll Netflix makes their own shows.
*  Right.
*  They spend millions of dollars on special effects and all these different things.
*  And they're really like enormous projects.
*  Like you're, you're just talking about people talking shit and you want money.
*  Right.
*  Well, that's the things we have to actually deliver something that's
*  totally qualitatively better.
*  And it would also have to be like someone like you or someone who's really aware
*  of the issues that we're dealing with with addictions to social media, should
*  have to say this is, this is the best possible alternative.
*  Like in this environment, you are, you, yes, you are paying a certain amount of
*  money per month, but maybe that could get factored into your cell phone bill.
*  And maybe with this sort of an ecosystem, you're no longer being drawn in by your
*  addictions and you know, it's not playing for your attention span.
*  It's rewarding you in a very productive way.
*  And imagine Joe, if like 15% more of your time was just way better spent.
*  Like he was actually spent on, you were actually doing the things you cared about.
*  Like it actually helped improve your life.
*  Yeah.
*  Like imagine when you use email, if it was truly designed, I mean, forget email.
*  I mean, I know people don't relate to that because email isn't that popular, but
*  whatever it is, that's a huge time sync for you.
*  For me, email is a huge one for me, you know, web browsing or whatever is a big
*  one, imagine that those things were so much better designed that I actually wrote
*  back to the right emails and I mostly didn't think about the rest that when I
*  was spending time on, you know, whatever I was spending time on, that it was
*  really my, my, my more and more of my life was a life well lived and time well spent.
*  That's like the retrospective view.
*  I keep going to Apple, but because I think that the only social media, excuse me,
*  the only technology company that does have these ethics to sort of protect
*  privacy, have you thought about coming to them?
*  Yep.
*  Have you?
*  Well, I mean, I think that they've made great first steps and they were the
*  first along with Google to do those, the screen time management stuff, but that
*  was just this barely scratching the surface, like baby, baby, baby steps.
*  Like what we really need them to do is radically, re-imagine how those
*  incentives and how the phone fundamentally works.
*  So it's not just all these colorful icons.
*  And one of the problems they do have a disincentive, which is a lot of
*  their revenue comes from gaming.
*  And as they move more into Apple TV, competing with HBO and Hulu and Netflix
*  and that whole thing where they need subscriptions, so that Apple's revenue
*  on devices and hardware is sort of maxing out and where they're going to get their
*  next bout of revenue to keep their stock price up is on these subscriptions.
*  I am less concerned with those addictions.
*  I'm less concerned with gaming addictions that I have information
*  addictions because at least it's not fundamentally altering your view of the world.
*  Right.
*  And screwing up democracy and make it impossible to agree.
*  And this is coming from a person that's had like legitimate
*  video game addictions in the past.
*  Yeah.
*  But like my wife is addicted to Subway Surfer.
*  Like, I don't know what is it.
*  That's a crazy game.
*  It's like you're riding on the top of subways and you're jumping around.
*  And it's like, it's really ridiculous, but it's fun.
*  Like you watch like, whoa, but I don't fuck with video games, but I watch it.
*  And it's those games at least are enjoyable.
*  There's something silly about it.
*  Like, ah, fuck.
*  And then you start doing it again.
*  When I, when I see people getting angry about things on social media, I don't see
*  the upside, right?
*  I don't mind them making a profit off games.
*  There is an issue though, with games that addict children.
*  And then these children, there's like, you could spend money on like roadblocks
*  and you can have all these different things you spend money on.
*  You wind up, you know, having these enormous bills on bills.
*  Yeah.
*  You leave your kid with an iPad and you come back, you have a $500 bill.
*  Like, what did you do?
*  Yeah.
*  This is, this is an issue for sure, but at least it's not an issue in that it's
*  changing their view of, of the world.
*  Right.
*  And I, I feel like there's a way for, I keep going back to Apple, but a company
*  like Apple to rethink the way that, you know, they already have a walled garden,
*  right?
*  With iMessage and FaceTime and all this different.
*  Come up and totally build those things out.
*  I mean, iMessage and iCloud could be the basis for some new neutral.
*  Social media is not based on instant social approval and rewards, right?
*  Yes.
*  They can make it easier to share information with small groups of friends
*  and have that all synced.
*  And even, you know, in the pre-COVID days, I was thinking about Apple a lot.
*  I think you're right, by the way, to really poke on them.
*  I think they're the one company that's in a position to lead on this.
*  And they also have a history of thinking along those lines.
*  You know, they had this feature that's kind of hidden now, but the
*  find my friends, right?
*  They call it find my now it's all buried together.
*  So you can find your devices and find your friends.
*  But in a pre-COVID world, imagine they really built out the, you know,
*  where are my friends right now and making it easier to know when you're nearby
*  someone so you can easily more easily get together in person.
*  Because right now all the bit like to the extent Facebook wants to
*  bring people closer together, they don't want to.
*  And again, this is pre-COVID, but they don't want to incentivize
*  lots and lots of Facebook events.
*  They really care about groups that keep people posting it online and looking at
*  ads because of the category of bringing people closer together, they want to do
*  the online screen time based version of that, as opposed to the offline.
*  Apple, by contrast, if you had little iMessage groups of friends, you could say,
*  Hey, does everyone in this little group want to opt into being able to see where
*  each other are, where we all are on say weekdays between five and eight PM or
*  something like that.
*  So you could like time bound it and make it easier for serendipitous connection
*  and availability to happen.
*  That's hard to do.
*  It's hard to design that.
*  But there's things like that, that Apple's in a position to do if it
*  really took on that mantle.
*  And I think as people get more and more skeptical of these other products,
*  they're in a better and better position to do that.
*  One of the antitrust issues is do we want a world where our entire well being
*  as a society depends on what one massive corporation worth over a trillion dollars
*  does or doesn't do?
*  Like we need more openness to try different things.
*  And we're really at the behest of whether one or two companies, Apple or Google,
*  does something more radical.
*  And there has to be some massive incentive for them to do something that's
*  really going to change the way we interface with these devices and the way we
*  interface with social media.
*  And I don't know what incentive exists more potent than financial incentives.
*  Well, and this is where the government in the same way that we want to transition
*  long term from a fossil fuels oriented economy to something that changes
*  the kind of pollution levels.
*  We have a hugely emitting society ruining kind of business model of this
*  attention extractive paradigm.
*  And we could long term sort of just like a progressive tax on that transition to
*  some other thing. The government could do that, right?
*  That's not like who do we censor.
*  It's how do we disincentivize these businesses to pay for the sort of life
*  support systems of society that they've ruined.
*  A good example of this, I think in Australia, is there, I think it's Australia
*  that's regulated that Google and Facebook have to pay the publishers who they're
*  basically hollowing out.
*  Because one of the effects we've not talked about is the way that Google and
*  Facebook have hollowed out the fourth estate in journalism.
*  I mean, because journalism has turned into and local news websites can't make any
*  money except by basically producing click bait.
*  So even to the extent that local newspapers exist, they only exist by
*  basically click baitification of even lower and lower paid workers who are just
*  generating content farms.
*  Right. So anyway, so that's an example of if you force those companies to pay to
*  to revitalize the fourth estate and to make sure we have a very sustainably
*  funded fourth estate that doesn't have to produce this click bait stuff.
*  That's that's another direction.
*  Yeah, that that's interesting that they have to pay.
*  I mean, these are the wealthiest companies in the history of humanity.
*  So that's the thing. So we shouldn't be cautious about how much they should have
*  to pay, because we also don't want it to happen on the other end.
*  Right. You don't want to have a world where we have roundup making a crazy amount
*  of money from giving everybody cancer and lymphoma from, you know, the chemicals.
*  Glyphosate. Right. Glyphosates.
*  And then they pay everybody on the other end after a lawsuit of a billion dollars.
*  But now everyone's got cancer. Let's actually do it in a way so we don't want a
*  world where Facebook and Google profit off of the erosion of our social fabric.
*  And then they pay us back.
*  How do you quantify how how much money they have to pay to journalism?
*  Yeah, it seems like it's almost a form of socialism.
*  Or yeah, I mean, this is where like that the IQ lead lead example is interesting
*  because they were able to disincentivize and tax the lead producers because they
*  were able to produce some result on how much this lowered the wage earning
*  potentials of the entire population.
*  I mean, like, how much does this cost our society?
*  We used to say free is the most expensive business model we've ever created
*  because we get the free downgrading of our attention spans, our mental health,
*  our kids, like our ability to agree with each other, our capacity to do
*  anything as a democracy.
*  Like, yeah, we got all that for free. Wonderful.
*  Obviously, we get lots of benefits and I want to acknowledge that.
*  But that's just not sustainable.
*  The real question, I mean, right now we're.
*  We have huge existential problems.
*  We have a global competition and power competition going on.
*  I think China just passed the GDP of the US, I believe.
*  There is, you know, if we care about the US having a future
*  in which it can lead the world in some meaningful and enlightened way,
*  we have to deal with this problem.
*  And we have to have a world where digital democracy outcompetes
*  digital authoritarianism, which is the China model.
*  And right now that builds more coherence and is more efficient
*  and doesn't devolve the way that our current system does.
*  I think Taiwan, Estonia and countries like that, where they are doing
*  digital democracies, are good examples that we can learn from.
*  But we are behind right now.
*  Well, China also has a really fascinating situation with Huawei
*  where Google is banned Huawei.
*  So you can't have Google applications on Huawei.
*  So now Huawei is creating their own operating system
*  and they have their own ecosystem now that they're building up.
*  And that's you know, it's weird that there's only a few
*  different operating systems now.
*  I mean, there's a very small amount of people that using Linux phones.
*  Yeah. Then you have a large amount of people using Android and iPhones.
*  And if China becomes.
*  The first to adopt their own operating system,
*  and then they have even more unchecked rules and regulations
*  in regards to like the influence that they have over their people
*  with an operating system that they've developed and they control.
*  And who knows what kind of back doors and spying.
*  Tons. Yeah, it's.
*  It's weird. Yeah.
*  Well, when you see this,
*  do you like it feels so futile for me on the outside looking in looking.
*  But you you're working on this.
*  How long do you anticipate this is going to be a part of your life?
*  I mean, what does it feel like to you?
*  I mean, it's not easy, right?
*  In the film ends with this question, do you think we're going to get there?
*  Yeah, I just say we have to.
*  Like, I mean, if you care about this going well, I wake up every day
*  and I ask, what will it take for this whole thing to go well?
*  Like, and how do we just orient each of our choices
*  as much as possible towards this going well?
*  And we have a whole bunch of problems.
*  I do look a lot at the environmental issues, the permafrost methane bombs,
*  like the timelines that we have to deal with certain problems are crunching.
*  And we also have certain dangerous exponential technologies that are emerging.
*  Decentralization of, you know, CRISPR and like there's a lot of existential threats.
*  I hang out a lot with the sort of existential threats community.
*  It's going to take must be a lot of fun.
*  There's a lot of psychological problems in that community.
*  Actually, a lot of depression.
*  There's certainly a man of suicide as well.
*  It's it's, you know, it's
*  it's hard, but I think we each have a responsibility when you see this stuff
*  to say, what will it take for this to go well?
*  And I will say that really seeing the film
*  impact people the way that it has.
*  I used to feel like, oh, my God, how are we ever going to do this?
*  No one cares. Like, not a people know.
*  At the very least, we now have about 50 to 40 to 50 million people
*  who are at least introduced to the problem.
*  The question is, how do we harness them into a collective movement?
*  And that's what we're trying to do next.
*  I mean, I I'll say also these issues get more and more weird over time.
*  My co-founder, Aza Raskin, will say that it's making reality
*  more and more virtual over time because we haven't talked about how
*  as technology advances at hacking our weaknesses,
*  we start to prefer it over the real thing.
*  We start, for example, there's a recent company
*  VC funded raised like I think it's worth like over one hundred
*  twenty five million dollars.
*  And what they make are virtual influencers.
*  So these are like virtual people, virtual video
*  that is more entertaining, more interesting and that fans like more
*  than real people. Oh, boy.
*  And it's kind of related to the kind of deep fake world, right?
*  We're like people prefer this to the real thing.
*  And Sherry Turkle, you know, has been working at MIT,
*  wrote the book Reclaiming Conversation and Alone Together.
*  She's been talking about this forever, that over time,
*  humans will prefer connection to robots and bots
*  and the computer generated thing more than the real thing.
*  Think about AI generated music being more.
*  It'll start to sweeten our taste buds and give us exactly that thing
*  we're looking for better than we will know ourselves.
*  Just like YouTube can give us the perfect next video that actually
*  every bone in our body will say, actually, I kind of do want to watch that,
*  even though it's a machine pointed at my brain, calculating the next thing.
*  There's an example from Microsoft writing this chatbot called
*  Xiaoice, I can pronounce it, that after nine weeks,
*  people preferred that chatbot to their real friends.
*  And 25 or 10, 10 to 25 percent of their users actually said,
*  I love you to the chatbot. Oh, boy.
*  And that there are several who actually said that it convinced them
*  not to commit suicide to have this relationship with this chatbot.
*  So it's her. It's her.
*  It's the movie. Exactly.
*  Which is what so all these things are the same.
*  We're we're veering into a direction where technology,
*  if it's so good at meeting these underlying paleolithic emotions
*  that we have, the way out of it is we have to see that this is what's going on.
*  We have to see and reckon with ourselves saying, this is how I work.
*  I have this negativity bias.
*  If I get those 99 comments and one's one's positive comments
*  and one's negative, my mind is going to go to the negative.
*  I don't see that. I see you in the future wearing an overcoat.
*  You're you are literally Laurence Fishburne in The Matrix
*  trying to tell people to wake up.
*  Well, that's there's a line in the social dilemma where I say,
*  how do you wake up from The Matrix if you don't know you're in The Matrix?
*  Well, that is the issue, right?
*  And I even in The Matrix, we at least had a shared matrix.
*  The problem now is that in The Matrix, each of us have our own matrix.
*  That's the real kicker.
*  I struggle with the idea that this is all inevitable
*  because this is a natural course of progression with technology
*  and that it's sort of figuring out the best way to.
*  To have us with as little resistance, embed ourselves into its system
*  and that our ideas are what we are with emotions
*  and with our biological issues.
*  This is just how life is and this is how life always should be.
*  But this is just all we've ever known.
*  It's all we've ever known.
*  Einstein didn't write into the laws of physics that social media has to exist
*  for humanity, right? We've gotten rid.
*  Again, the environmental movement is a really interesting example
*  because we passed all sorts of laws.
*  We got rid of lead.
*  We've changed from some of our pesticides.
*  You know, we're slow on some of these things and corporate interests
*  and asymmetric power of large corporations.
*  You know, which I want to say markets and capitalism are great
*  because when you have asymmetric power for predatory systems that cause harm,
*  they're not going to terminate themselves.
*  They have to be bound in by the public, by culture, by the by the state.
*  And we just have to point to the examples where we've done that.
*  And in this case, I think the problem is that how much of our stock market
*  is built on the back of like five companies generating a huge amount of wealth.
*  So this is similar.
*  I don't mean to make this example, but there's a great book by
*  Adam Hokeshield called Bury the Chains, which is about
*  the British abolition of slavery, in which he talks about how for the British
*  empire, like if you think about it, when when we collectively wake up and say,
*  this is an abhorrent practice that has to end.
*  But then at that time, in the 17, 1800s in Britain,
*  slavery was what powered the entire economy.
*  It was free labor for, you know, huge percentage of the economy.
*  So if you say we can't do this anymore, we have to stop this.
*  How do you decouple when your entire economy is based on slavery?
*  Right. And the book is actually inspiring because it tracks
*  a collective movement that was through networked all these different groups,
*  the Quakers in the US, the people testifying before parliament,
*  the former slaves who did firsthand accounts, the graphics and art of all the
*  people had never seen what it looked like on a slave ship.
*  And so by making the invisible visceral and showing just how abhorrent this stuff
*  was through a period of about 60 to 70 years, the British Empire
*  had to drop their GDP by 2 percent every year for 60 years and willing
*  to do that to get off of slavery.
*  Now, I'm not making a moral equivalence.
*  I want to be really clear for taking things out of context.
*  But just that it's possible for us to do something that isn't just in the interest
*  of economic growth. And I think that's the real challenge.
*  That's actually something that should be on the agenda, which is how do we
*  one of the major tensions is economic growth, you know, being in conflict
*  with dealing with some with many of our problems, whether it's some of the
*  environmental issues or, you know, with some of the technology issues
*  we're talking about right now.
*  Artificial intelligence is something that people are terrified of
*  as an existential threat.
*  They think of it as one day you're going to turn something on
*  and it's going to be sentient and it's going to be able to create
*  other forms of artificial intelligence that are exponentially more powerful
*  than the one that we created and that will have unleashed this beast
*  that we cannot control.
*  What my concern is with all of this.
*  Yeah, that's my concern.
*  My concern is that this this is a a slow acceptance of drowning.
*  It's like a slow.
*  Oh, we're OK. I'm only up to my knees.
*  It's fine. It's just my waist high.
*  I can still walk around in boiling water.
*  Exactly. Exactly. It seems like.
*  This is like humans have to fight back to reclaim our autonomy
*  and free will from the machines.
*  I mean, one one clear. OK, Neo.
*  It's very much the matrix.
*  It's me.
*  And one of my favorite lines is actually when the Oracle says to Neo,
*  and don't worry about the vase and he says, what face?
*  And he knocks it over. It's that face.
*  And so it's like she's the AI who sees so many moves ahead on the chessboard.
*  She can say something which will cause him to do the thing
*  that verifies the thing that she predicted would happen. Yeah.
*  That's what AI is doing now, except it's pointed at our nervous system
*  and figuring out the perfect thing to dangle in front of our dopamine system
*  and get the thing to happen, which instead of knocking off the vases
*  to be outraged at the other political side and be fully certain that you're right,
*  even though it's just a machine that's calculating shit
*  that's going to make you, you know, do the thing.
*  When you're concerned about this, how much time do you spend
*  thinking about simulation theory?
*  The simulation? Yeah.
*  The idea that if not currently, one day there will be a simulation
*  that's indiscernible from regular reality.
*  And it seems we're on that path.
*  I don't know if you mess around with VR at all, but.
*  Well, this is the point about, you know, the virtual chat bots out competing.
*  Yes, exactly. The technology, you know.
*  I mean, that's what's happening is that reality is getting more and more virtual.
*  Right. As we interact with a virtual news system,
*  that's all this sort of click bait economy outrage machine.
*  That's already a virtual political environment that then translates
*  into real world action, then becomes real.
*  And that's the weird feedback.
*  Go back to 1990, whatever it was when the Internet became mainstream
*  or at least started becoming mainstream.
*  And the small amount of time that it took, the 20 plus years
*  to get to where we are now and then think what?
*  What about the virtual world?
*  And once this becomes something that's
*  has the same sort of rate of growth that the Internet has experienced
*  or that we have experienced through the Internet.
*  I mean, we're looking at like 20 years from now being unrecognizable.
*  We're looking at I mean, it almost seems like that is what life does.
*  The same way bees create beehives, you know, a caterpillar
*  doesn't know what the fuck's going on when it gets into that cocoon,
*  but it's becoming a butterfly.
*  Yeah, we seem to be a thing that creates newer and better objects.
*  Correct. More effective.
*  But we have to realize
*  AI is not conscious and won't be conscious the way we are.
*  And so many people think that.
*  But is consciousness essential?
*  I think so. To us.
*  I don't know.
*  Essential in the sense of we're the only ones who have it.
*  No, I don't know that.
*  In fact, there's a theory with the might be more.
*  Yeah, things that have consciousness.
*  But is it essential?
*  I mean, it's the to the extent that choice exists.
*  It would exist through some kind of consciousness.
*  And this choice is choice essential.
*  It's essential to us as we know it, like his life as we know it.
*  But my worry is that we're in essential.
*  They would like we're thinking now like single celled organisms.
*  But I'm like, hey, I don't want to gang up with a bunch of other people
*  and become an object that can walk.
*  I like being a single celled organism.
*  This is a lot of fun.
*  I mean, I hear you saying, you know, are we a bootloader?
*  Yeah, the AI that then runs the world.
*  That's Eliens perspective.
*  I mean, I think this is a really dangerous way to think.
*  I mean, we have to.
*  Yeah. So are we then dangerous for us?
*  Yeah. I mean, are.
*  But what if the next version of the next is the version being run by machines
*  that have no values, that don't care, that don't have choice
*  and are just maximizing for things that were programmed in
*  by our little miniature brains anyway.
*  But they don't cry.
*  They don't commit suicide.
*  But then consciousness and life dies.
*  That could be the future.
*  I think this is the last chance to try to snap out of that.
*  And is it important in the eyes of the universe that we do that?
*  I don't know. It feels important.
*  How does it feel to you?
*  Feels important. But I'm a monkey.
*  You know, the monkey's like, I'm staying in this tree, man.
*  You guys are out of your fucking mind.
*  I mean, this is the weird paradox of being human is that again,
*  we have these lower level emotions.
*  We care about social approval.
*  We can't not care. At the same time, like I said,
*  there's this weird proposition here.
*  We're the only species that if this were to happen to us,
*  we would have the self-awareness to even know that it was happening.
*  Right. Like we can concept like this to our interview,
*  we can conceptualize that this this thing has happened to us.
*  Right. That we have built this matrix,
*  this external object, which has like AI and supercomputers
*  and voodoo doll versions of each of us.
*  And it has perfectly figured out how to predictably move each of us in this matrix.
*  Let me propose this to you.
*  We are what we are now.
*  Human beings, homo sapiens in 2020.
*  We are this thing that if you believe in evolution, I'm pretty sure you do.
*  We've evolved over the course of millions of years to become who we are right now.
*  Should we stop right here? Are we done?
*  No. Right. We should keep evolving.
*  What does that look like?
*  What does it look like if we go ahead?
*  Just forget about social media.
*  What would you like us to be in a thousand years or a hundred
*  thousand years or five hundred thousand years?
*  You certainly wouldn't want us to be what we are right now, right?
*  No one would.
*  No, I mean, I think this is what visions of Star Trek and things like that
*  were trying to ask, right?
*  Like, hey, let's imagine humans do make it and we become the most enlightened
*  we can be and we actually somehow make peace with these other alien tribes
*  and we figure out, you know, space travel and all of that.
*  I mean, actually, a good heuristic that I think people can ask is
*  on an enlightened planet where we did figure this out,
*  what would that have looked like?
*  Isn't it always weird that those movies, it's people are just people,
*  but they're in some weird future, but they haven't really changed that much.
*  Right. I mean, which is to say that
*  the fundamental way that we work is just unchanging.
*  But there are such things as more wise societies, more sustainable societies,
*  more peaceful or harmonious societies.
*  I mean, the Janes ultimately biologically we have to evolve as well.
*  But our version of the best version is probably the gray aliens.
*  Right. Maybe so.
*  I mean, the ultimate future.
*  I mean, we're going to get into gene editing and becoming more perfect,
*  perfect on the sense of that.
*  But we are going to start optimizing
*  for what are the outcomes that we value.
*  I think the question is, how do we actually come up with brand new values
*  that are wiser than we've ever thought of before that actually are able
*  to transcend the win lose games that lead to omni lose lose that everyone loses
*  if we keep playing the win lose game at greater and greater scales?
*  I, like you, have a vested interest in the biological existence of human beings.
*  I think people are pretty cool.
*  I love being around them. I enjoy talking to you today.
*  My fear is that we are.
*  We're we're a model T.
*  Right. You know, and there's there's no sense in making those fucking things anymore.
*  The brakes are terrible.
*  They smell like shit when you drive them.
*  They don't go very fast.
*  We need a better version.
*  You know, the funny thing is, God, there's some quote by someone, I think, like,
*  I wish I could remember it.
*  It's something about how much would be solved if we were at peace with ourselves.
*  Like if we were able to just be OK with nothing,
*  like just being OK with living and breathing.
*  I don't mean to be, you know, playing the new age card.
*  I just genuinely mean how much of our lives is just running away from,
*  you know, anxiety and discomfort and aversion.
*  It is. But, you know, in that sense, some of the most satisfied
*  and happy people are people that live a subsistence living,
*  have these subsistence existences in the middle of nowhere,
*  just chopping trees and catching fish.
*  Right. And more connection, probably.
*  Yeah. Authentic than something else.
*  And I think that's probably resonates biologically, too,
*  because of the history of human beings living like that is just
*  so much longer and greater.
*  Totally. And I think that those are more sustainable societies.
*  We can never obtain peace in the outer world until we make peace with ourselves.
*  Dalai Lama. Yeah, but I don't buy that guy.
*  You know, that guy, he's he's an interesting case.
*  I was thinking there was a different slightly different quote, but actually
*  there's one quote that I would love to if it's possible.
*  One of the reasons why I don't buy him is just chosen.
*  They just chose that guy. Yeah.
*  Also, he doesn't have sex.
*  Wait, how how?
*  Yeah. How much can you be enjoying life if that's not not a part of it?
*  Come on, bro.
*  You wear the same outfit every day.
*  The fuck out of here with your orange robes.
*  Can I? There's a there's a really
*  important quote that I think would really be good to share.
*  It's from the book.
*  Have you read Amusing Ourselves to Death by Neil Postman? No.
*  From 1982? No.
*  So especially when we get into big tech and we talk about censorship a lot
*  and we talk about Orwell, he has this really wonderful opening to this book.
*  It was written in 1982.
*  It literally predicts everything that's going on now.
*  I frankly think that I'm adding nothing and it's really just
*  Neil Postman called it all in 1982.
*  He had this great opening. It says.
*  Let's see, we're all looking out for, you know, 1984,
*  when the year came and the prophecy didn't, thoughtful Americans sang softly
*  in praise of themselves.
*  The roots of liberal democracy had held.
*  This is like we made it through the 1984 gap.
*  Wherever else the terror had happened, we at least had not been visited
*  by Orwellian nightmares.
*  But we had forgotten that alongside Orwell's dark vision,
*  there was another slightly older, slightly less well known, equally
*  chilling vision of Aldous Huxley's brave new world.
*  Contrary to common belief, even among the educated,
*  Huxley and Orwell did not prophecy the same thing.
*  Orwell warns that we will become overwhelmed, overcome by an externally
*  imposed oppression.
*  But in Huxley's vision, no big brother is required to deprive people
*  of their autonomy, maturity or history.
*  As he saw it, people will come to love their oppression,
*  to adore the technologies that undo their capacities to think.
*  What Orwell feared were those who would ban books.
*  What Huxley feared was that there would be no reason to ban a book,
*  for there would be no one who wanted to read one.
*  Orwell feared those who would deprive us of information.
*  Huxley feared those who would give us so much that we would be reduced
*  to passivity and egoism.
*  Orwell feared the truth would be concealed from us.
*  Huxley feared the truth would be drowned in a sea of irrelevance.
*  Orwell feared we would become a captive culture.
*  But Huxley feared we would become a trivial culture, preoccupied
*  with some equivalent of the feelys and the orgy porgy and the centrifugal
*  bumble puppy. Don't know what that means.
*  As Huxley remarked in Brave New World Revisited, the civil libertarians
*  and rationalists who are ever on the alert to oppose tyranny
*  failed to take into account man's almost infinite appetite for distractions.
*  Lastly, in 1984, Orwell added, people are controlled by inflicting pain.
*  In Brave New World, they are controlled by inflicting pleasure.
*  In short, Orwell feared that what we fear will ruin us.
*  Huxley feared that what we desire will ruin us.
*  Holy shit.
*  Isn't that good?
*  That's that's the best way to end this.
*  God damn.
*  But again, if we can become aware that this is what's happened, we're the only
*  species with the capacity to see that our own psychology, our own emotions, our own
*  paleolithic evolutionary system has been hijacked.
*  I like that you're optimistic.
*  We have to be.
*  If we want to remain people, we have to be.
*  Optimism is probably the only way to live in a meat suit body and keep going.
*  Otherwise.
*  It certainly helps.
*  Yeah, it certainly helps.
*  Thank you very much for being here, man.
*  I really enjoyed this, even though I'm really depressed now.
*  I really don't want you to be depressed.
*  I really hope people, you know, I'm kidding.
*  We really want to build a movement and, you know, we're just, I wish I could give
*  people more resources.
*  We do have a podcast called Your Undivided Attention, and we're trying to
*  build a movement at humainetech.com.
*  But.
*  Well, listen to any new revelations or new developments that you have.
*  I'd be more than happy to have you on again.
*  We'll talk about them and send them to me and I'll put them on social media and
*  whatever you need.
*  Awesome.
*  I'm here to help.
*  Awesome.
*  That's great.
*  Great to be here.
*  Resist.
*  Yeah, resist.
*  Together.
*  Humanity.
*  Resist.
*  Humanity.
*  We're in this together.
*  Thank you, Tristana.
*  I really, really appreciate it.
*  Goodbye, everybody.
