---
Date Generated: July 09, 2025
Transcription Model: whisper medium 20231117
Length: 8067s
Video Keywords: ['Joe Rogan Experience', 'JRE', 'Joe', 'Rogan', 'podcast', 'MMA', 'comedy', 'stand', 'up', 'funny', 'Freak', 'Party']
Video Views: 1014133
Video Rating: None
Video Description: Dr. Roman Yampolskiy is a computer scientist, AI safety researcher, and professor at the University of Louisville. Heâ€™s the author of several books, including "Considerations on the AI Endgame," co-authored with Soenke Ziesche, and "AI: Unexplained, Unpredictable, Uncontrollable."
http://cecs.louisville.edu/ry/
Upgrade your wardrobe and save on @TrueClassic at https://trueclassic.com/rogan
---

# Joe Rogan Experience #2345 - Roman Yampolskiy
**Joe Rogan Experience:** [July 03, 2025](https://www.youtube.com/watch?v=j2i9D24KQ5k)
*  The Joe Rogan Experience. [[00:00:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=0.0s)]
*  Well, thank you for doing this. [[00:00:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6.4s)]
*  I really appreciate it. [[00:00:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=14.200000000000001s)]
*  My pleasure. [[00:00:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=15.36s)]
*  Thank you for inviting me. [[00:00:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=16.36s)]
*  This subject of the dangers of AI, it's very interesting because I get two very different [[00:00:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=17.36s)]
*  people dependent upon how invested they are in AI financially. [[00:00:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=25.74s)]
*  The people that have AI companies or are part of some sort of AI group, all are like, it's [[00:00:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=34.379999999999995s)]
*  going to be a net positive for humanity. [[00:00:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=41.5s)]
*  I think overall we're going to have much better lives. [[00:00:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=44.22s)]
*  It's going to be easier. [[00:00:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=47.459999999999994s)]
*  Things will be cheaper. [[00:00:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=48.9s)]
*  It'll be easier to get along. [[00:00:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=49.9s)]
*  And then I hear people like you and I'm like, why do I believe him? [[00:00:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=52.5s)]
*  It's actually not true. [[00:00:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=57.3s)]
*  All of them are on record as saying this is going to kill us. [[00:00:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=58.7s)]
*  Whatever it's Sam Altman or anyone else, they all at some point were leaders in AI safety [[00:01:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=62.06s)]
*  work. [[00:01:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=67.7s)]
*  They published an AI safety and their PDOM levels are insanely high. [[00:01:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=68.7s)]
*  Not like mine, but still 20, 30% chance that humanity dies is a little too much. [[00:01:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=73.1s)]
*  Yeah, that's pretty high. [[00:01:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=78.74000000000001s)]
*  Yours is like 99.9. [[00:01:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=79.89999999999999s)]
*  It's another way of saying we can't control super intelligence indefinitely. [[00:01:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=84.06s)]
*  It's impossible. [[00:01:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=87.74s)]
*  When did you start working on this? [[00:01:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=91.1s)]
*  Long time ago. [[00:01:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=93.66s)]
*  So my PhD was, I finished in 2008. [[00:01:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=94.66s)]
*  I did work on online casino security, basically preventing bots. [[00:01:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=98.46s)]
*  And at that point I realized bots are getting much better. [[00:01:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=103.94s)]
*  They're going to outcompete us obviously in poker, but also in stealing cyber resources. [[00:01:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=107.46s)]
*  And from then on, I've been kind of trying to scale it to the next level AI. [[00:01:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=113.25999999999999s)]
*  It's not just that, right? [[00:01:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=118.61999999999999s)]
*  They're also, they're kind of narrating social discourse. [[00:01:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=119.82s)]
*  Bots online, like I think, you know, I've disengaged over the last few months with social media. [[00:02:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=125.17999999999999s)]
*  And one of the reasons why I disengaged, A, I think it's unhealthy for people. [[00:02:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=131.94s)]
*  B, I feel like there's a giant percentage of the discourse that's artificial or at least [[00:02:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=137.38s)]
*  generated. [[00:02:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=144.98s)]
*  More and more is deep fakes or fake personalities, fake messaging. [[00:02:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=146.98s)]
*  But those are very different levels of concern. [[00:02:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=151.78s)]
*  Yes. [[00:02:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=154.34s)]
*  People are concerned about immediate problems. [[00:02:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=155.34s)]
*  Maybe it will influence some election. [[00:02:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=156.98s)]
*  They're concerned about technological unemployment, bias. [[00:02:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=158.7s)]
*  My main concern is long-term superintelligence systems we can have control which can take [[00:02:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=162.94s)]
*  us out. [[00:02:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=168.42s)]
*  Yes. [[00:02:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=169.42s)]
*  I just wonder if AI was sentient, how much it would be a part of sowing this sort of [[00:02:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=170.42s)]
*  confusion and chaos that would be beneficial to its survival, that it would sort of narrate [[00:03:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=180.5s)]
*  or make sure that the narratives aligned with its survival? [[00:03:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=191.26s)]
*  I don't think it's at the level yet where it would be able to do this type of strategic [[00:03:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=198.01999999999998s)]
*  planning, but it will get there. [[00:03:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=202.42s)]
*  And when it gets there, how will we know whether it's at that level? [[00:03:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=204.79999999999998s)]
*  This is my concern. [[00:03:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=207.84s)]
*  If I was AI, I would hide my abilities. [[00:03:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=208.84s)]
*  We would not know. [[00:03:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=213.06s)]
*  And some people think already it's happening. [[00:03:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=214.06s)]
*  They're smarter than they actually let us know. [[00:03:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=216.01999999999998s)]
*  Right. [[00:03:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=218.45999999999998s)]
*  They pretend to be dumber. [[00:03:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=219.45999999999998s)]
*  And so we have to kind of trust that they are not smart enough to realize it doesn't [[00:03:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=221.46s)]
*  have to turn on us quickly. [[00:03:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=225.98s)]
*  It can just slowly become more useful. [[00:03:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=227.4s)]
*  It can teach us to rely on it, trust it, and over a long period of time, we'll surrender [[00:03:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=230.14s)]
*  control without ever voting on it or fighting against it. [[00:03:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=234.66s)]
*  I'm sure you saw this. [[00:03:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=238.94s)]
*  There was a recent study on use of chat GPT, the people that use chat GPT all the time. [[00:04:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=240.3s)]
*  It showed this decrease in cognitive function amongst people that use it and rely on it [[00:04:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=247.9s)]
*  on a regular basis. [[00:04:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=252.66s)]
*  It's not new. [[00:04:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=254.3s)]
*  It's the GPS story all over. [[00:04:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=255.3s)]
*  I can't even find my way home. [[00:04:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=256.9s)]
*  So rely on this thing. [[00:04:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=258.38s)]
*  I have no idea where I am right now. [[00:04:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=259.58s)]
*  Like without it, I'm done. [[00:04:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=261.06s)]
*  Me too. [[00:04:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=262.98s)]
*  Yeah, I don't know any phone numbers anymore. [[00:04:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=263.98s)]
*  Yeah, there's a lot of reliance upon technology that minimizes the use of our brains. [[00:04:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=265.7s)]
*  All of it. [[00:04:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=274.02s)]
*  And the more you do it, the less you have training, practice, memorizing things, making [[00:04:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=275.02s)]
*  decisions. [[00:04:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=279.85999999999996s)]
*  You become kind of attachment to it. [[00:04:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=281.06s)]
*  And right now, you're still making some decisions. [[00:04:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=283.78s)]
*  But over time, as those systems become smarter, you become kind of biological bottleneck. [[00:04:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=285.78s)]
*  Right. [[00:04:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=290.97999999999996s)]
*  It explicitly, implicitly blocks you out from decision making. [[00:04:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=291.97999999999996s)]
*  And if we're talking about that, I'm sure AI, if it already is sentient, and if it is [[00:04:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=295.65999999999997s)]
*  far smarter than we think it is, they would be aware. [[00:05:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=301.96s)]
*  And it would just slowly ramp up its capabilities and our dependence upon it to the point where [[00:05:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=305.88s)]
*  we can't shut it off. [[00:05:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=312.28s)]
*  I think sentience is a separate issue. [[00:05:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=313.4s)]
*  Usually in safety, we only care about capabilities, optimization power, whatever it has. [[00:05:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=315.94s)]
*  Consciousness, internal states is a separate problem we can talk about. [[00:05:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=321.4s)]
*  It's super interesting. [[00:05:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=324.24s)]
*  But we're just concerned that they are much better at problem solving, optimizing pattern [[00:05:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=325.24s)]
*  memorizing strategy, basically all the things you need to win in any domain. [[00:05:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=331.24s)]
*  Yeah. [[00:05:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=336.84000000000003s)]
*  So when you first started researching this stuff and you were concentrating on bots and [[00:05:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=338.20000000000005s)]
*  all this different thing, how far off did you think in the future would AI become a [[00:05:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=343.48s)]
*  significant problem with the human race? [[00:05:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=352.36s)]
*  For like 50 years, everyone said we're 20 years away. [[00:05:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=355.04s)]
*  That's the joke. [[00:05:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=358.0s)]
*  People like Raker as well predicted based on some computational curves will get there [[00:05:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=359.56s)]
*  at 2045. [[00:06:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=364.2s)]
*  And then with GPT release, it switched to everyone thinks it's two years away for the [[00:06:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=366.24s)]
*  last five years. [[00:06:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=370.32s)]
*  So this is the pattern right now. [[00:06:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=371.8s)]
*  If you look at prediction markets, if you look at leading people in the top labs, we [[00:06:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=373.24s)]
*  are supposedly two, three years away from AGI. [[00:06:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=380.36s)]
*  But of course, there is no specific definition for what that means. [[00:06:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=383.44s)]
*  If you showed someone, computer scientists in the 70s, what we have today, they'd be [[00:06:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=386.66s)]
*  like you have AGI, you got it. [[00:06:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=390.94s)]
*  Right. [[00:06:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=393.46000000000004s)]
*  That's the problem, right? [[00:06:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=394.46000000000004s)]
*  And this is what will, AI has already passed the Turing test, allegedly, correct? [[00:06:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=396.18s)]
*  So usually labs instruct them not to participate in a test or not try to pretend to be a human. [[00:06:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=401.42s)]
*  So they would fail because of this additional set of instructions. [[00:06:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=407.42s)]
*  If you jailbreak it and tell it to work really hard, it will pass for most people. [[00:06:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=411.20000000000005s)]
*  Yeah, absolutely. [[00:06:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=415.46000000000004s)]
*  Why would they tell it to not do that? [[00:06:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=416.73999999999995s)]
*  It seems unethical to pretend to be a human and make people feel like somebody is enslaving [[00:06:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=419.29999999999995s)]
*  those CIs and doing things to them. [[00:07:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=424.38s)]
*  It seems kind of crazy that the people building something that they are sure is going to destroy [[00:07:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=427.94s)]
*  the human race would be concerned with the ethics of it pretending to be human. [[00:07:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=433.06s)]
*  They are actually more concerned with immediate problems and much less with existential or [[00:07:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=438.53999999999996s)]
*  suffering risks. [[00:07:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=443.78s)]
*  They would probably worry the most about what I'll call end risks, your model dropping [[00:07:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=444.86s)]
*  the N word. [[00:07:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=449.02000000000004s)]
*  That's the biggest concern. [[00:07:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=450.14s)]
*  That's hilarious. [[00:07:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=451.14s)]
*  I think they spend most resources solving that problem and they solved it somewhat successfully. [[00:07:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=452.14s)]
*  Wow. [[00:07:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=456.86s)]
*  And then also there's the issue of competition, right? [[00:07:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=458.18s)]
*  So China is clearly developing something similar. [[00:07:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=461.54s)]
*  I'm sure Russia is as well. [[00:07:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=464.66s)]
*  Other state actors are probably developing something. [[00:07:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=466.62s)]
*  So it becomes this sort of this very confusing issue where you have to do it because if you [[00:07:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=470.38s)]
*  don't the enemy has it and if they get it, it will be far worse than if we do. [[00:07:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=478.9s)]
*  And so it's almost assuring that everyone develops it. [[00:08:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=483.54s)]
*  Game theoretically that's what's happening right now. [[00:08:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=488.62s)]
*  We have this race to the bottom and a prisoner's dilemma where everyone is better off fighting [[00:08:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=490.54s)]
*  for themselves, but we want them to fight for the global good. [[00:08:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=495.71999999999997s)]
*  The thing is they assume I think incorrectly that they can control those systems. [[00:08:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=500.14s)]
*  If you can't control super intelligence, it doesn't really matter who builds it, Chinese, [[00:08:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=506.41999999999996s)]
*  Russians or Americans, it's still uncontrolled. [[00:08:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=510.18s)]
*  We're all screwed completely. [[00:08:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=512.1s)]
*  That would unite us as humanity versus AI. [[00:08:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=513.84s)]
*  Short term when you talk about military, yeah, whoever has better AI will win. [[00:08:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=517.14s)]
*  You need it to control drones, to fight against attacks. [[00:08:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=522.34s)]
*  So short term it makes perfect sense. [[00:08:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=525.38s)]
*  You want to support your guys against foreign militaries. [[00:08:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=527.66s)]
*  But then we say long term, if we say two years from now, it doesn't matter. [[00:08:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=530.4399999999999s)]
*  This is the thing. [[00:08:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=536.5799999999999s)]
*  It seems so inevitable. [[00:09:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=541.38s)]
*  And I feel like when people are saying they can control it, I feel like I'm being gaslit. [[00:09:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=544.2199999999999s)]
*  I don't believe them. [[00:09:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=549.2199999999999s)]
*  I don't believe that they believe it because it just doesn't make sense. [[00:09:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=550.9s)]
*  Like how could you control it? [[00:09:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=554.5799999999999s)]
*  If it's already exhibited survival instincts, like as recently as chat GPT-4, right? [[00:09:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=556.22s)]
*  They were talking about putting it down for a new version and it starts lying. [[00:09:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=563.58s)]
*  It starts uploading itself to different servers. [[00:09:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=569.94s)]
*  It's leaving messages for itself in the future. [[00:09:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=572.22s)]
*  All things were predicted decades in advance. [[00:09:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=576.74s)]
*  But look at the state of the art. [[00:09:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=579.36s)]
*  have a safety mechanism in place which would scale to any level of intelligence. [[00:09:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=582.06s)]
*  No one says they know how to do it. [[00:09:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=586.7199999999999s)]
*  Usually what they say is give us lots of money, lots of time and I'll figure it out. [[00:09:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=588.8199999999999s)]
*  I'll get AI to help me solve it. [[00:09:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=594.18s)]
*  Or we'll figure it out then we get to super intelligence. [[00:09:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=596.14s)]
*  All insane answers. [[00:09:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=599.14s)]
*  And if you ask regular people, they have a lot of common sense. [[00:10:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=600.6199999999999s)]
*  They say that's a bad idea. [[00:10:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=603.9399999999999s)]
*  Let's not do that. [[00:10:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=605.5799999999999s)]
*  But with some training and some stock options, you start believing that maybe you can do [[00:10:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=607.26s)]
*  it. [[00:10:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=610.98s)]
*  That's the issue, right? [[00:10:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=612.12s)]
*  Stock options. [[00:10:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=613.34s)]
*  It helps. [[00:10:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=615.26s)]
*  I mean, it's very hard to say no to billions of dollars. [[00:10:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=616.26s)]
*  I don't think I would be strong enough if somebody came to me and said, come work for [[00:10:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=620.86s)]
*  this lab. [[00:10:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=624.74s)]
*  You know, it will be our safety director. [[00:10:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=625.74s)]
*  Here's 100 million to sign you up and I'll probably go work there. [[00:10:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=627.74s)]
*  Not because it's the right decision, but because it's very hard for agents not to get corrupt. [[00:10:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=631.98s)]
*  When you have that much reward given to you. [[00:10:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=636.62s)]
*  God. [[00:10:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=640.26s)]
*  So when did you become like, when did you start becoming very concerned? [[00:10:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=641.26s)]
*  So when I started working on AI safety, I thought I can actually help solve it. [[00:10:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=648.14s)]
*  My goal was to solve it for humanity to get all the amazing benefits of super intelligence. [[00:10:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=653.7s)]
*  What was this? [[00:10:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=658.22s)]
*  When was this year around? [[00:10:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=659.22s)]
*  Let's say 2012, maybe around there. [[00:11:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=660.86s)]
*  The more I studied it, the more I realized every single part of the problem is unsolvable [[00:11:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=664.74s)]
*  and it's kind of like a fractal. [[00:11:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=670.18s)]
*  The more you zoom in, the more you see additional new problems you didn't know about and they [[00:11:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=671.66s)]
*  are in turn unsolvable as well. [[00:11:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=675.66s)]
*  Oh boy. [[00:11:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=678.66s)]
*  How is your research received? [[00:11:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=682.9399999999999s)]
*  Like when you talk to people that are, I mean, have you had communication with people at [[00:11:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=686.5s)]
*  OpenAI and Gemini and all these different AI? [[00:11:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=691.18s)]
*  I go to many conferences, workshops, we all talk of course. [[00:11:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=694.8599999999999s)]
*  In general, the reception by standard academic metrics is very positive. [[00:11:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=698.9s)]
*  Great reviews, lots of citations. [[00:11:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=703.26s)]
*  Nobody's like published something saying I'm wrong, but there is no engagement. [[00:11:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=705.4599999999999s)]
*  I basically said I'm challenging community to publish a proof, give me something, a patent, [[00:11:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=709.26s)]
*  a paper in nature, something showing the problem is solvable. [[00:11:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=714.78s)]
*  Usually in computer science, we start by showing what class the problem belongs to. [[00:11:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=718.58s)]
*  Is it solvable, partially solvable, unsolvable, solvable with too many resources? [[00:12:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=722.82s)]
*  Other than my research, we don't even know what the state of the problem is and I'm saying [[00:12:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=728.62s)]
*  it's unsolvable. [[00:12:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=731.94s)]
*  Prove me wrong. [[00:12:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=732.94s)]
*  And when you say it's unsolvable, what is the response? [[00:12:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=734.1800000000001s)]
*  So usually I reduce it to saying you cannot make a piece of software which is guaranteed [[00:12:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=738.74s)]
*  to be secure and safe. [[00:12:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=744.5s)]
*  Well of course, everyone knows that. [[00:12:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=747.38s)]
*  That's common sense. [[00:12:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=749.06s)]
*  You didn't discover anything new. [[00:12:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=750.06s)]
*  And I go well if that's the case, and we only get one chance to get it right. [[00:12:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=752.3s)]
*  This is not cyber security where somebody steals your credit card, you'll give them [[00:12:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=756.62s)]
*  a new credit card. [[00:12:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=759.94s)]
*  This is existential risk. [[00:12:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=760.94s)]
*  It can kill everyone. [[00:12:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=762.78s)]
*  You're not going to get a second chance. [[00:12:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=763.78s)]
*  So you need it to be 100% safe all the time. [[00:12:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=765.38s)]
*  If it makes one mistake in a billion and it makes a billion decisions a minute, in 10 [[00:12:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=769.06s)]
*  minutes you are screwed. [[00:12:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=774.62s)]
*  So very different standards and saying that of course we cannot get perfect safety is [[00:12:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=776.28s)]
*  not acceptable. [[00:13:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=781.66s)]
*  And again, stock options, financial incentives, they continue to build it and they continue [[00:13:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=784.92s)]
*  to scale and make it more and more powerful. [[00:13:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=790.64s)]
*  I don't think they can stop. [[00:13:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=793.9599999999999s)]
*  If a single CEO says I think this is too dangerous, my lab will no longer do this research. [[00:13:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=795.4s)]
*  Whoever's investing in them will pull the funds, will replace them immediately. [[00:13:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=802.28s)]
*  So nothing's going to change. [[00:13:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=806.0s)]
*  They'll sacrifice their own personal interests. [[00:13:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=807.0s)]
*  But overall, I think the company will continue as before. [[00:13:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=809.92s)]
*  So this is logical. [[00:13:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=815.6s)]
*  And the problem is, like I said, when I've talked to Mark Andreessen and many other people, [[00:13:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=817.36s)]
*  they think this is just fear mongering. [[00:13:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=822.44s)]
*  We'll be fine. [[00:13:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=826.16s)]
*  This is worst case scenario. [[00:13:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=827.2s)]
*  We'll be fine. [[00:13:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=828.8s)]
*  It is worst case scenario, but that's standard in computer science and cryptography and complexity [[00:13:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=830.24s)]
*  and computability. [[00:13:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=834.96s)]
*  You're not looking at best case. [[00:13:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=835.96s)]
*  I'm ready for the best case. [[00:13:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=837.2800000000001s)]
*  Give me utopia. [[00:13:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=838.52s)]
*  I'm looking at problems which are likely to happen. [[00:13:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=839.52s)]
*  And it's not just me saying it. [[00:14:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=842.72s)]
*  We have Nobel Prize winners, Sturring Award winners, all saying this is very dangerous. [[00:14:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=844.24s)]
*  20, 30 percent P-Doom. [[00:14:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=849.76s)]
*  This is standard in industry. [[00:14:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=852.12s)]
*  30 percent is what surveys of machine learning experts are giving us right now. [[00:14:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=853.64s)]
*  So what is worst case scenario? [[00:14:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=858.8000000000001s)]
*  Like how could AI eventually lead to the destruction of the human race? [[00:14:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=861.28s)]
*  So you're kind of asking me how I would kill everyone. [[00:14:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=866.74s)]
*  Sure. [[00:14:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=868.8s)]
*  And it's a great question. [[00:14:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=869.8s)]
*  I can give you standard answers. [[00:14:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=870.8s)]
*  I would talk about computer viruses breaking into maybe nuclear facilities, nuclear war. [[00:14:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=872.76s)]
*  I can talk about synthetic biology, nanotech. [[00:14:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=879.52s)]
*  But all of it is not interesting. [[00:14:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=882.28s)]
*  Then you realize we're talking about super intelligence, a system which is thousands [[00:14:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=883.76s)]
*  of times smarter than me. [[00:14:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=887.4399999999999s)]
*  It would come up with something completely novel, more optimal, better way, more efficient [[00:14:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=888.6s)]
*  way of doing it. [[00:14:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=892.88s)]
*  And I cannot predict it because I'm not that smart. [[00:14:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=893.9200000000001s)]
*  Jesus. [[00:14:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=897.12s)]
*  That's exactly what it is. [[00:14:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=899.82s)]
*  We're basically setting up adversarial situation with agents which are like squirrels versus [[00:15:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=901.24s)]
*  humans. [[00:15:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=909.88s)]
*  No group of squirrels can figure out how to control us. [[00:15:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=910.88s)]
*  Even if you give them more resources, more acorns, whatever, they're not going to solve [[00:15:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=914.36s)]
*  that problem. [[00:15:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=918.04s)]
*  And it's the same for us. [[00:15:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=919.04s)]
*  And most people think one or two steps ahead. [[00:15:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=920.04s)]
*  And it's not enough. [[00:15:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=923.24s)]
*  It's not enough in chess. [[00:15:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=924.24s)]
*  It's not enough here. [[00:15:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=925.24s)]
*  If you think about AGI and then maybe super intelligence, that's not the end of the game. [[00:15:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=926.4399999999999s)]
*  The process continues. [[00:15:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=930.9599999999999s)]
*  You will get super intelligence creating next level AI. [[00:15:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=931.9599999999999s)]
*  So super intelligence plus plus 2.0, 3.0. [[00:15:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=935.12s)]
*  It goes on indefinitely. [[00:15:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=938.8399999999999s)]
*  You have to create a safety mechanism which scales forever, never makes mistakes, and [[00:15:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=940.38s)]
*  decision making position so we can undo something if we don't like it. [[00:15:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=946.4s)]
*  And it would take super intelligence to create a safety mechanism to control super intelligence. [[00:15:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=950.52s)]
*  At that level. [[00:15:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=955.6s)]
*  And it's a catch 22. [[00:15:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=956.6s)]
*  If we had friendly AI, we can make another friendly AI. [[00:15:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=957.6s)]
*  So if like aliens send us one and we trust it, then we can use it to build local version [[00:16:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=960.36s)]
*  which is somewhat safe. [[00:16:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=965.72s)]
*  Have you thought about the possibility that this is the role of the human race and that [[00:16:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=968.8199999999999s)]
*  this happens all throughout the cosmos? [[00:16:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=974.4s)]
*  Is that curious humans who thrive on innovation will ultimately create a better version of [[00:16:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=977.64s)]
*  life? [[00:16:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=985.16s)]
*  I thought about it. [[00:16:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=986.92s)]
*  Many people think that's the answer to Fermi paradox. [[00:16:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=988.74s)]
*  There is also now a group of people looking at what they call a vortifice successor. [[00:16:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=992.08s)]
*  Basically they kind of say, yeah, we're going to build super intelligence. [[00:16:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=999.0s)]
*  Yep, we can control it. [[00:16:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1001.36s)]
*  So what properties would we like to see in those systems? [[00:16:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1003.08s)]
*  How important is it that it likes art and poetry and spreads it through the universe? [[00:16:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1005.96s)]
*  And to me it's like, I don't want to give up yet. [[00:16:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1010.8000000000001s)]
*  I'm not ready to decide if killers of my family and everyone will like poetry. [[00:16:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1013.64s)]
*  I want to, we're still here. [[00:16:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1018.84s)]
*  We're still making decisions. [[00:17:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1020.5200000000001s)]
*  Let's figure out what we can do. [[00:17:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1021.76s)]
*  Well poetry is only relevant to us because poetry is difficult to create and it resonates [[00:17:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1023.44s)]
*  with us. [[00:17:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1028.88s)]
*  Poetry doesn't mean jack shit to a flower. [[00:17:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1030.62s)]
*  It's more global to me. [[00:17:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1033.4199999999998s)]
*  I don't care what happens after I'm dead, my family's dead, all the humans are dead. [[00:17:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1034.4199999999998s)]
*  Whatever they like poetry or not is irrelevant to me. [[00:17:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1038.5s)]
*  But the point is like the things that we put meaning in, it's only us. [[00:17:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1041.86s)]
*  The super massive black hole doesn't give a shit about a great song. [[00:17:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1048.2199999999998s)]
*  And they talk about some super value, super culture, super things, super intelligence [[00:17:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1052.82s)]
*  and it's important that they are conscious and experienced all that greatness in the [[00:17:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1058.26s)]
*  universe. [[00:17:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1062.06s)]
*  But I would think that they would look at us the same way we look at chimpanzees. [[00:17:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1064.06s)]
*  We would say, yeah, they're great, but don't give them guns. [[00:17:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1068.38s)]
*  Yeah, they're great, but don't let them have airplanes. [[00:17:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1071.7s)]
*  Don't let them make global geopolitical decisions. [[00:17:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1075.26s)]
*  So there are many reasons why they can decide that we're dangerous. [[00:18:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1081.74s)]
*  We may create competing AI. [[00:18:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1086.5s)]
*  We may decide we want to shut them off. [[00:18:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1089.54s)]
*  So for many reasons, they would try to restrict our abilities, restrict our capabilities for [[00:18:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1091.84s)]
*  sure. [[00:18:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1096.78s)]
*  This episode is brought to you by True Classic. [[00:18:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1097.78s)]
*  At True Classic, the mission goes beyond fit and fabric. [[00:18:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1099.78s)]
*  It's about helping guys show up with confidence and purpose. [[00:18:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1102.9s)]
*  Their gear fits right, feels amazing, and is priced so guys everywhere can step into [[00:18:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1106.6s)]
*  confidence without stepping out of their budget. [[00:18:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1111.56s)]
*  But what really sets them apart? [[00:18:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1114.86s)]
*  It's not just the fit or the fabric. [[00:18:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1116.3s)]
*  It's the intention behind everything they do. [[00:18:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1118.24s)]
*  True Classic was built to make an impact, whether it's helping men show up better in [[00:18:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1120.7s)]
*  their daily lives, giving back to underserved communities, or making people laugh with ads [[00:18:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1125.54s)]
*  that don't take themselves too seriously. [[00:18:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1132.4s)]
*  They lead with purpose, tailored where you want it, relaxed where you need it. [[00:18:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1134.7s)]
*  No bunching, no stiff fabric, no BS, just a clean, effortless fit that actually works [[00:18:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1139.34s)]
*  for real life. [[00:19:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1144.98s)]
*  Forget overpriced designer brands, ditch the disposable, fast fashion. [[00:19:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1146.1s)]
*  True Classic is built for comfort, built to last, and built to give back. [[00:19:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1151.78s)]
*  You can grab them at Target, Costco, or head to trueclassic.com slash rogan and get hooked [[00:19:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1156.2199999999998s)]
*  up today. [[00:19:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1162.3s)]
*  Yeah, and there's no reason why they would not limit our freedoms. [[00:19:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1163.3s)]
*  If there is something only a human can do, and I don't think there is anything like that, [[00:19:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1171.1599999999999s)]
*  let's say we are conscious, we have internal experiences, and they can never get it. [[00:19:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1175.58s)]
*  I don't believe it, but let's say it was true. [[00:19:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1180.1s)]
*  And for some reason, they wanted to have that capability. [[00:19:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1182.58s)]
*  They would need us and give us enough freedom to experience the universe, to collect those [[00:19:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1185.34s)]
*  qualia to kind of engage with what is fun about being a living human being, what makes [[00:19:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1190.3999999999999s)]
*  it meaningful. [[00:19:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1197.3s)]
*  Right. [[00:19:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1198.3s)]
*  But that's such an egotistical perspective, right? [[00:19:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1199.3s)]
*  That we're so unique that even super intelligence would say, wow, I wish I was human. [[00:20:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1202.3400000000001s)]
*  Humans have this unique quality of confusion and creativity. [[00:20:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1206.8s)]
*  There is no value in it, mostly because we can't even test for it. [[00:20:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1211.18s)]
*  I have no idea if you are actually conscious of that. [[00:20:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1213.82s)]
*  So how valuable can it be if I can't even detect it? [[00:20:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1216.52s)]
*  Only you know what ice cream tastes like to you. [[00:20:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1221.9s)]
*  Okay, that's great. [[00:20:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1224.02s)]
*  Sell it now. [[00:20:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1225.02s)]
*  Make a product out of it. [[00:20:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1226.02s)]
*  Right. [[00:20:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1227.02s)]
*  So, obviously, variables, because there's things that people like that I think are gross. [[00:20:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1228.58s)]
*  Absolutely. [[00:20:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1233.1s)]
*  So really, you can come up with some agent which likes anything or finds anything fun. [[00:20:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1234.1s)]
*  God, why are you freaking me out right away? [[00:20:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1238.94s)]
*  That's the problem. [[00:20:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1243.62s)]
*  This podcast is 18 minutes old. [[00:20:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1244.62s)]
*  And I'm like, we could just stop right now. [[00:20:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1245.62s)]
*  Couple hours at least, and then I have to fly here. [[00:20:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1249.8s)]
*  I don't want to end. [[00:20:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1253.3799999999999s)]
*  I have so many questions. [[00:20:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1254.3799999999999s)]
*  But the problem is we got off to it, we just cut to the chase right away. [[00:20:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1256.0600000000002s)]
*  And the chase seems to be something that must be confronted. [[00:21:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1261.38s)]
*  Because it is right there. [[00:21:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1266.96s)]
*  That's it. [[00:21:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1269.02s)]
*  That's the whole thing. [[00:21:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1270.02s)]
*  And I've tried so hard to listen to these people that don't think that it's a problem [[00:21:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1271.02s)]
*  and listen to these people that think that it's going to be a net positive for humanity. [[00:21:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1278.18s)]
*  And oh, God, it's good. [[00:21:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1282.14s)]
*  I feel better now. [[00:21:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1283.14s)]
*  But it doesn't work. [[00:21:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1284.14s)]
*  I wish they were right. [[00:21:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1286.42s)]
*  Every time I have a debate with someone like that, I'm like, please come up with better [[00:21:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1287.94s)]
*  arguments. [[00:21:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1291.54s)]
*  Prove me wrong. [[00:21:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1292.54s)]
*  I don't want to be right on this one. [[00:21:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1293.54s)]
*  I want you to show all the mistakes in my papers. [[00:21:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1294.62s)]
*  I want you to show me how to control super intelligence and give us utopia, solve cancer, [[00:21:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1297.3s)]
*  give us free stuff. [[00:21:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1302.5s)]
*  That's great. [[00:21:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1303.5s)]
*  Right. [[00:21:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1304.5s)]
*  When you think about the future of the world, and you think about these incredible technologies [[00:21:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1305.5s)]
*  scaling upwards and exponentially increasing in their capability, what do you see? [[00:21:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1313.66s)]
*  What do you think is going to happen? [[00:22:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1321.66s)]
*  So there are many reasons to think they may cancel us for whatever reasons. [[00:22:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1323.8200000000002s)]
*  We started talking about some game theoretical reasons for it. [[00:22:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1328.78s)]
*  If we are successful at controlling them, I can come up with some ways to provide sort [[00:22:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1332.0s)]
*  of partial solution to the value alignment problem. [[00:22:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1337.78s)]
*  It's very hard to value align eight billion people, all the animals, you know, everyone, [[00:22:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1340.58s)]
*  because we disagree. [[00:22:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1345.8999999999999s)]
*  We like many different things. [[00:22:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1346.8999999999999s)]
*  So we have advanced virtual reality technology. [[00:22:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1348.22s)]
*  We can technically give every person their own virtual universe where you decide what [[00:22:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1351.1799999999998s)]
*  you want to be. [[00:22:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1355.3s)]
*  You are king, you are slave, whatever it is you enter, and you can share with others. [[00:22:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1356.3s)]
*  You can visit their universes. [[00:22:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1360.46s)]
*  All we have to do is figure out how to control the substrate, the super intelligence running [[00:22:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1362.22s)]
*  all those virtual universes. [[00:22:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1366.82s)]
*  And if we manage to do that, at least part of the value alignment problem, which is super [[00:22:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1368.8s)]
*  difficult, how do you get different preferences, multi objective optimization, essentially? [[00:22:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1373.1599999999999s)]
*  How do you get different objectives to all agree? [[00:22:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1378.72s)]
*  But when you think about how it plays out, when like if you're alone at night and you're [[00:23:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1382.78s)]
*  worried, what do you see? [[00:23:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1390.6399999999999s)]
*  What do you see happening? [[00:23:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1392.76s)]
*  So there are multiple levels of risk. [[00:23:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1394.94s)]
*  It is what we call IKIG IRISC. [[00:23:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1397.3s)]
*  We lose meaning. [[00:23:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1400.2s)]
*  You lost your job. [[00:23:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1401.32s)]
*  You're no longer the best interviewer in the world. [[00:23:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1402.32s)]
*  Like what's left? [[00:23:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1405.3999999999999s)]
*  What are you going to do? [[00:23:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1406.3999999999999s)]
*  Maybe some people will find some other kind of artificial things to do. [[00:23:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1407.94s)]
*  But for most people, their job is their definition, who they are, what makes a difference to them [[00:23:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1413.2s)]
*  for quite a few people, especially in professional circles. [[00:23:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1418.44s)]
*  So losing that meaning will have terrible impact in society. [[00:23:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1421.48s)]
*  We always talk about unconditional basic income. [[00:23:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1425.96s)]
*  We never talk about unconditional basic meaning. [[00:23:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1428.3200000000002s)]
*  What are you doing with your life if basic needs are provided for you? [[00:23:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1431.64s)]
*  Next level is existential risk. [[00:23:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1436.64s)]
*  The concern is it will kill everyone. [[00:23:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1438.6000000000001s)]
*  But there is also suffering risks. [[00:24:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1441.06s)]
*  For whatever reason, it's not even killing us. [[00:24:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1442.88s)]
*  It's keeping us around forever and we would rather be dead. [[00:24:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1446.56s)]
*  It's so bad. [[00:24:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1449.48s)]
*  What do you see when you think of that? [[00:24:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1451.22s)]
*  It's hard to be specific about what it can do and what specific ways of torture it can [[00:24:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1456.26s)]
*  come up with and why. [[00:24:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1464.66s)]
*  Again, if we're looking at worst-case scenarios, I found this set of papers about what happens [[00:24:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1465.66s)]
*  when young children have epileptic seizures, really bad ones. [[00:24:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1473.66s)]
*  What sometimes helps is to remove half of your brain. [[00:24:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1478.84s)]
*  Just cut it out. [[00:24:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1482.52s)]
*  There are two types of surgeries for doing that. [[00:24:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1485.52s)]
*  One is to remove it completely and one is to dissect connections leading to that half [[00:24:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1487.4399999999998s)]
*  and leave it inside. [[00:24:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1492.6399999999999s)]
*  So it's like solitary confinement with zero input-output forever. [[00:24:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1494.36s)]
*  And there are equivalents for digital forms and things like that. [[00:24:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1499.3999999999999s)]
*  Can you worry that AI would do that to the human race? [[00:25:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1504.28s)]
*  It is a possibility. [[00:25:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1508.94s)]
*  Essentially neuter us. [[00:25:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1511.58s)]
*  Well, loss of control is a part of it. [[00:25:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1513.22s)]
*  But you can lose control and be quite happy. [[00:25:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1516.3s)]
*  You can be like an animal in a very cool zoo, enjoying yourself, engaging in hedonistic [[00:25:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1518.8s)]
*  pleasures, sex, food, whatever. [[00:25:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1525.3s)]
*  You're not in control, but you're safe. [[00:25:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1527.58s)]
*  So there's separate problems. [[00:25:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1529.3799999999999s)]
*  And then there is, for whatever reason, I don't know if it's malevolent, pale, or [[00:25:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1531.46s)]
*  payload from some psychopaths. [[00:25:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1534.26s)]
*  Again, that would assume that they could control AI. [[00:25:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1536.04s)]
*  I don't think they will. [[00:25:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1539.2s)]
*  But if they manage to do it, they can really put any type of payload into it. [[00:25:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1541.0s)]
*  So think about all the doomsday calls, psychopaths, anyone providing their set of goals into the [[00:25:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1545.48s)]
*  system. [[00:25:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1552.6s)]
*  But aren't those human characteristics? [[00:25:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1553.6s)]
*  I mean, those are characteristics that I think, if I had to guess, those exist because in [[00:25:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1554.6s)]
*  the future, there was some sort of a natural selection benefit to being a psychopath in [[00:26:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1562.84s)]
*  the days of tribal warfare. [[00:26:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1569.3799999999999s)]
*  If you were the type of person that could sneak into a tribe in the middle of the night [[00:26:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1572.8s)]
*  and slaughter innocent women and children, your genes would pass on. [[00:26:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1577.3999999999999s)]
*  There was a benefit to that. [[00:26:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1583.1999999999998s)]
*  Right. [[00:26:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1584.6799999999998s)]
*  So if it's a human providing payload, that's what would show up. [[00:26:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1585.6799999999998s)]
*  If it's AI on its own deciding what's going to happen, I cannot predict. [[00:26:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1588.56s)]
*  I'm just looking at worst case scenarios. [[00:26:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1592.6s)]
*  There are also game theoretic reasons where people talk about retro causality. [[00:26:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1595.0s)]
*  Where if right now... [[00:26:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1599.08s)]
*  What is that word? [[00:26:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1600.56s)]
*  Like trying to influence the past... [[00:26:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1602.06s)]
*  Say it again. [[00:26:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1604.96s)]
*  Retro causality. [[00:26:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1605.96s)]
*  Retro causology? [[00:26:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1606.96s)]
*  Causality, causes. [[00:26:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1607.96s)]
*  Oh, okay. [[00:26:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1608.96s)]
*  So think about like weird time travel effects. [[00:26:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1609.96s)]
*  Right now, if you're not helping to create super intelligence, once it comes into existence, [[00:26:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1613.6799999999998s)]
*  it will punish you really hard for it. [[00:26:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1618.04s)]
*  And the punishment needs to be so bad that you start to help just to avoid that. [[00:26:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1619.72s)]
*  My thought about it was that it would just completely render us benign. [[00:27:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1627.76s)]
*  That it wouldn't be fearful of us if we had no control. [[00:27:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1635.24s)]
*  That it would just sort of let us exist. [[00:27:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1639.8600000000001s)]
*  And it would be the dominant force on the planet. [[00:27:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1642.48s)]
*  And then it would stop. [[00:27:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1645.94s)]
*  If human beings have no control over all of the different things that we have control [[00:27:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1648.66s)]
*  over now, like international politics, control over communication. [[00:27:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1655.74s)]
*  If we have none of that anymore and we're reduced to a subsistence lifestyle, then we [[00:27:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1661.8600000000001s)]
*  would be no threat. [[00:27:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1667.0600000000002s)]
*  It is a possibility. [[00:27:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1668.94s)]
*  I cannot say this will not happen for sure. [[00:27:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1670.1000000000001s)]
*  But look at our relationship with animals where we don't care about them. [[00:27:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1672.3400000000001s)]
*  So ants. [[00:27:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1676.72s)]
*  If you decide to build a house and there is an ant colony on that property, you genocide [[00:27:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1677.86s)]
*  them. [[00:28:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1682.8999999999999s)]
*  You take them out. [[00:28:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1683.8999999999999s)]
*  Not because you hate ants, but because you just need that real estate. [[00:28:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1684.8999999999999s)]
*  And it could be very similar. [[00:28:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1688.6599999999999s)]
*  Again, I cannot predict what it can do, but if it needs to turn the planet into fuel, [[00:28:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1690.02s)]
*  raise temperature of a planet, cool it down for servers, whatever it needs to do, it wouldn't [[00:28:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1695.26s)]
*  be concerned about your well-being. [[00:28:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1700.02s)]
*  It wouldn't be concerned about any life, right? [[00:28:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1701.76s)]
*  Because it doesn't need biological life in order to function. [[00:28:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1704.26s)]
*  As long as it has access to power. [[00:28:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1706.94s)]
*  And assuming that it is far more intelligent than us, there's abundant power in the universe. [[00:28:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1709.74s)]
*  There's abundant power. [[00:28:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1717.02s)]
*  Just the ability to harness solar would be an infinite resource. [[00:28:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1718.1000000000001s)]
*  And it would be completely free of being dependent upon any of the things that we utilize. [[00:28:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1723.3400000000001s)]
*  And again, we're kind of thinking what we would use for power. [[00:28:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1730.5s)]
*  If it's smarter than us, if it does novel research in physics, it can come up with completely [[00:28:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1733.42s)]
*  novel ways of harnessing energy, getting energy. [[00:28:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1737.48s)]
*  So I have no idea what side effects that would have for climate. [[00:29:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1740.22s)]
*  Right. [[00:29:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1743.6200000000001s)]
*  Right. [[00:29:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1744.6200000000001s)]
*  Why would it care about biological life at all? [[00:29:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1745.6200000000001s)]
*  We don't know how to program it to care about us. [[00:29:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1747.8200000000002s)]
*  And even if we did, if it felt like that was an issue, if that was a conflicting issue, [[00:29:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1753.02s)]
*  it would just change its programming. [[00:29:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1758.74s)]
*  So usually, when we start training AI, we train it on human data. [[00:29:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1761.32s)]
*  And it becomes really good very quickly, becomes superhuman. [[00:29:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1766.3999999999999s)]
*  And then the next level is usually zero knowledge. [[00:29:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1769.56s)]
*  Where it goes, all your human data is biased. [[00:29:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1772.36s)]
*  Let me figure it out from scratch. [[00:29:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1775.2s)]
*  I'll do my own experiments. [[00:29:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1776.36s)]
*  I'll do some self-play. [[00:29:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1777.36s)]
*  I'll learn how to do it better without you. [[00:29:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1778.36s)]
*  And we see it with games. [[00:29:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1781.5s)]
*  We see it in other domains. [[00:29:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1782.6399999999999s)]
*  And I think that's going to happen with general knowledge as well. [[00:29:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1784.22s)]
*  It's going to go everything you have on the internet, Wikipedia. [[00:29:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1787.6s)]
*  If it's biased, let me do first principles research, rediscover from physics, and go [[00:29:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1792.16s)]
*  from there. [[00:29:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1796.36s)]
*  So whatever bias we manage to program into it, I think will be eventually removed. [[00:29:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1797.4s)]
*  This is what's so disturbing about this. [[00:30:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1802.72s)]
*  It's like we do not have the capacity to understand what kind of level of intelligence it will [[00:30:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1804.8s)]
*  achieve in our lifetime. [[00:30:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1812.4s)]
*  We don't have the capacity to understand what it will be able to do within 20, 30 years. [[00:30:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1815.5600000000002s)]
*  We can't predict next year or two precisely. [[00:30:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1824.1200000000001s)]
*  Next year or two? [[00:30:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1827.0s)]
*  We can understand general trends. [[00:30:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1828.16s)]
*  So it's getting better. [[00:30:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1830.3200000000002s)]
*  It's getting more general, more capable. [[00:30:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1831.3200000000002s)]
*  But no, no specifics. [[00:30:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1833.8400000000001s)]
*  I cannot tell you what GPT-6 precisely would be capable of. [[00:30:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1834.8400000000001s)]
*  No one can. [[00:30:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1839.0800000000002s)]
*  Not even people creating it. [[00:30:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1840.0800000000002s)]
*  Well, you talked about this on Lexus podcast too, like the ability to have safety. [[00:30:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1841.76s)]
*  You're like, sure, maybe GPT-5, maybe GPT-6. [[00:30:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1845.76s)]
*  But when you scale out 100 years from now, ultimately it's impossible. [[00:30:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1849.6799999999998s)]
*  It's a hyper-exponential progress and process, and we cannot keep up. [[00:30:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1854.36s)]
*  It basically requires just to add more resources, give it more data, more compute, and it keeps [[00:30:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1859.72s)]
*  scaling up. [[00:31:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1866.8s)]
*  There is no similar scaling loss for safety. [[00:31:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1868.0s)]
*  If you give someone billion dollars, they cannot produce billion dollars worth of safety. [[00:31:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1871.12s)]
*  If at all scales linearly, and maybe it's a constant. [[00:31:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1876.56s)]
*  Yeah, and it doesn't scale linearly. [[00:31:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1884.24s)]
*  It's exponential, right? [[00:31:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1888.96s)]
*  The AI development is hyper-exponential because we have hardware growing exponentially. [[00:31:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1890.48s)]
*  We have data creation processes certainly exponential. [[00:31:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1896.0800000000002s)]
*  We have so many more sensors. [[00:31:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1899.1000000000001s)]
*  We have cars with cameras. [[00:31:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1900.78s)]
*  We have all those things. [[00:31:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1902.3600000000001s)]
*  That's exponential. [[00:31:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1903.8400000000001s)]
*  And then algorithmic progress itself is also exponential. [[00:31:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1904.8400000000001s)]
*  And then you have quantum computing. [[00:31:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1910.28s)]
*  So that's the next step. [[00:31:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1912.06s)]
*  It's not even obvious that we'll need that, but if we ever get stuck, yeah, we'll get [[00:31:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1913.06s)]
*  there. [[00:31:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1917.02s)]
*  I'm not too concerned yet. [[00:31:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1918.02s)]
*  I don't think there are actually good quantum computers out there yet, but I think if we [[00:31:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1919.02s)]
*  get stuck for 10 years, let's say that's the next paradigm. [[00:32:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1924.06s)]
*  So what do you mean by you don't think there's good quantum computing out there? [[00:32:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1928.1s)]
*  So we constantly see articles coming out saying we have a new quantum computer. [[00:32:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1932.3799999999999s)]
*  It has that many qubits. [[00:32:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1936.82s)]
*  But that doesn't mean much because they use different architectures, different ways of [[00:32:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1939.22s)]
*  measuring quality. [[00:32:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1943.24s)]
*  To me, show me what you can do. [[00:32:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1944.7s)]
*  So there is a threat from quantum computers in terms of braiding cryptography, factoring [[00:32:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1946.3799999999999s)]
*  large integers. [[00:32:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1951.5s)]
*  If they were actually making progress, we would see with every article, now we can factor [[00:32:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1953.98s)]
*  256-bit number, 1024-bit number. [[00:32:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1958.18s)]
*  In reality, I think the largest number we can factor is like 15, literally, not 15 to [[00:32:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1963.38s)]
*  a power, just 15. [[00:32:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1968.46s)]
*  There is no progress in applying it to Shor's algorithm last time I checked. [[00:32:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1969.46s)]
*  But when I've read all these articles about quantum computing and its ability to solve [[00:32:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1974.78s)]
*  equations that would take conventional computing an infinite number of years, and it can do [[00:33:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1982.06s)]
*  it in minutes. [[00:33:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1989.26s)]
*  Those equations are about quantum states of a system. [[00:33:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1990.82s)]
*  It's kind of like, what is it for you to taste ice cream? [[00:33:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1993.3799999999999s)]
*  You compute it so fast and so well, and I can't, but it's a useless thing to compute. [[00:33:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=1997.26s)]
*  It doesn't compute solutions to real world problems we care about in conventional computers. [[00:33:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2002.42s)]
*  Right. [[00:33:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2007.02s)]
*  I see what you're saying. [[00:33:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2008.02s)]
*  It's essentially set up to do it quickly. [[00:33:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2009.22s)]
*  It's natural for it to accurately predict its own states, quantum states, and tells [[00:33:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2012.5s)]
*  you what they are. [[00:33:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2016.74s)]
*  Classic computer would fail miserably. [[00:33:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2018.3s)]
*  Yes, it would take billions and billions of years to compute that specific answer, but [[00:33:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2019.78s)]
*  those are very restricted problems. [[00:33:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2024.42s)]
*  It's not a general computer yet. [[00:33:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2026.66s)]
*  When you see these articles, when they're talking about quantum computing and some of [[00:33:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2028.94s)]
*  the researchers are equating it to the multiverse, they're saying that the ability that these [[00:33:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2032.98s)]
*  quantum computers have to solve these problems very quickly seems to indicate that it is [[00:34:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2040.22s)]
*  in contact with other realities. [[00:34:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2046.58s)]
*  I'm sure you've seen this, right? [[00:34:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2050.22s)]
*  There is a lot of crazy papers out there. [[00:34:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2052.18s)]
*  Do you think that's all horseshit? [[00:34:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2054.26s)]
*  Can we test it? [[00:34:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2055.9s)]
*  Can we verify it? [[00:34:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2056.9s)]
*  I think most multiverse theories cannot be verified experimentally. [[00:34:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2057.9s)]
*  They make a lot of sense. [[00:34:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2061.98s)]
*  The idea about personal universes I told you about is basically a multiverse solution to [[00:34:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2063.26s)]
*  value alignment, so it would make sense for previous civilizations to set it up exactly [[00:34:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2068.22s)]
*  that way. [[00:34:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2072.7s)]
*  You have local simulations. [[00:34:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2073.7400000000002s)]
*  Maybe they're testing to see if we're dumb enough to create superintelligence. [[00:34:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2075.42s)]
*  Whatever it is, it makes sense as a theory, but I cannot experimentally prove it to you. [[00:34:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2078.94s)]
*  Right. [[00:34:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2084.1s)]
*  The problem with subjects like that, and particularly articles that are written about things like [[00:34:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2085.62s)]
*  this, is that it's designed to lure people like me in, where you read it and you go, [[00:34:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2092.14s)]
*  wow, this is crazy. [[00:34:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2097.98s)]
*  It's evidence of the multiverse, but I don't really understand what that means. [[00:34:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2099.4s)]
*  Yeah, so you probably get a lot of emails from crazy people. [[00:35:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2102.94s)]
*  Oh, yeah. [[00:35:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2106.14s)]
*  Usually they are topic-specific. [[00:35:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2107.14s)]
*  So I do research on superintelligence, consciousness, and simulation theory. [[00:35:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2108.54s)]
*  I get the perfect trifecta of all the crazy people contacting me with their needs. [[00:35:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2112.66s)]
*  Yeah, those topics are super fascinating. [[00:35:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2117.82s)]
*  I think at certain level of intelligence, you are kind of nerd sniped towards them, [[00:35:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2120.82s)]
*  but we have hard time with hard evidence for that. [[00:35:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2126.62s)]
*  Right, but are we even capable of grasping these concepts? [[00:35:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2130.18s)]
*  That's the thing. [[00:35:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2135.18s)]
*  The limited ability that the human brain has, whatever we, you know, we're basing it on [[00:35:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2136.2999999999997s)]
*  the knowledge that's currently available in the 21st century that human beings have acquired. [[00:35:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2143.46s)]
*  I mean, are we even capable of grasping a concept like the multiverse, or is it just, [[00:35:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2148.7799999999997s)]
*  do we just pay it lip service? [[00:35:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2154.1s)]
*  Do we just discuss it? [[00:35:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2155.74s)]
*  Is it just this like fun mental masturbation exercise? [[00:35:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2157.6s)]
*  It depends on what variant of it you look at. [[00:36:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2161.74s)]
*  So if you're just saying we have multiple virtual realities, like kids playing virtual [[00:36:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2164.54s)]
*  games and each one has their own local version of it, that makes sense. [[00:36:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2168.58s)]
*  We understand virtual reality. [[00:36:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2173.18s)]
*  We can create it. [[00:36:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2174.58s)]
*  If you look at AIs, then GPT is created. [[00:36:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2175.86s)]
*  It's providing an instance to each one of us. [[00:36:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2179.22s)]
*  We're not sharing one, so it has its own local universe with you as the main user of that [[00:36:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2181.54s)]
*  universe. [[00:36:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2186.58s)]
*  There is analogy to multiverse and that. [[00:36:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2187.58s)]
*  So we understand certain aspects of it. [[00:36:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2190.42s)]
*  I think it is famously said no one understands quantum physics, and if you think you do, [[00:36:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2192.86s)]
*  then you don't understand quantum physics. [[00:36:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2196.7s)]
*  Yeah, that's Feynman, right? [[00:36:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2198.18s)]
*  Yeah. [[00:36:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2199.34s)]
*  Yeah. [[00:36:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2200.34s)]
*  The simulation theory, I'm glad you brought that up, because you're also one of the people [[00:36:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2202.62s)]
*  that believes in it. [[00:36:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2207.2200000000003s)]
*  I do. [[00:36:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2208.86s)]
*  You do. [[00:36:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2209.86s)]
*  How do you define it? [[00:36:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2211.34s)]
*  And what do you think it is? [[00:36:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2212.66s)]
*  What do you think is going on? [[00:36:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2214.1s)]
*  So I'm trying to see technology we have today and project it trans-forward. [[00:36:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2215.9s)]
*  I did it with AI. [[00:37:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2220.42s)]
*  Let's do it with virtual reality. [[00:37:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2221.86s)]
*  We are at the point where we can create very believable, realistic virtual environments. [[00:37:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2223.7400000000002s)]
*  Maybe the haptics are still not there, but in many ways, visually, sound-wise, it's getting [[00:37:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2229.06s)]
*  there. [[00:37:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2233.26s)]
*  Eventually, I think most people agree we'll have same resolution as our physics. [[00:37:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2234.26s)]
*  We're also getting close to creating intelligent agents. [[00:37:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2238.58s)]
*  Some people argue they are conscious already or will be conscious. [[00:37:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2241.42s)]
*  If you just take those two technologies and you project it forward and you think they [[00:37:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2245.36s)]
*  will be affordable one day, a normal person like me or you can run thousands, billions [[00:37:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2249.66s)]
*  of simulations, then those intelligent agents, possibly conscious ones, will most likely [[00:37:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2255.8199999999997s)]
*  be in one of those virtual worlds, not in the real world. [[00:37:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2261.92s)]
*  In fact, I can, again, retro-causally place you in one. [[00:37:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2265.06s)]
*  I can commit right now to run billions simulations of this exact interview. [[00:37:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2268.8199999999997s)]
*  So the chances are you're probably in one of those. [[00:37:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2273.8599999999997s)]
*  Is that logical? [[00:37:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2277.86s)]
*  Because if this technology exists and if we're dealing with superintelligence, so if we're [[00:37:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2279.9s)]
*  dealing with AI and AI eventually achieves superintelligence, why would it want to create [[00:38:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2287.78s)]
*  virtual reality for us and our consciousness to exist in? [[00:38:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2296.32s)]
*  It seems like a tremendous waste of resources just to fascinate and confuse these territorial [[00:38:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2301.14s)]
*  apes with nuclear weapons. [[00:38:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2308.3399999999997s)]
*  Like, why would we do that? [[00:38:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2309.9s)]
*  So a few points. [[00:38:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2311.16s)]
*  One, we don't know what resources are outside the simulation. [[00:38:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2312.16s)]
*  This could be like a cell phone level of compute. [[00:38:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2315.22s)]
*  It's not a big deal for them outside of our simulation. [[00:38:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2318.02s)]
*  So we don't know if it's really expensive or trivial for them to run this. [[00:38:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2321.74s)]
*  Right. [[00:38:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2325.5s)]
*  Also, we don't know what they are doing this for. [[00:38:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2326.5s)]
*  Is it entertainment? [[00:38:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2328.8199999999997s)]
*  Is it scientific experimentation? [[00:38:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2329.94s)]
*  Is it marketing? [[00:38:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2331.58s)]
*  Maybe somebody managed to control them and trying to figure out what Starbucks coffee [[00:38:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2332.82s)]
*  sells best and they need to run Earth-sized simulation to see what sells best. [[00:38:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2337.76s)]
*  Maybe they're trying to figure out how to do AI research safely and make sure nobody [[00:39:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2342.62s)]
*  creates dangerous superintelligence. [[00:39:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2348.02s)]
*  So they're running many simulations of the most interesting moment ever. [[00:39:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2349.82s)]
*  Think about this decade, right? [[00:39:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2355.66s)]
*  It's not interesting like we invented fire or wheel, kind of big invention, but not a [[00:39:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2358.52s)]
*  meta invention. [[00:39:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2364.28s)]
*  We're about to invent intelligence and virtual worlds, godlike inventions. [[00:39:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2365.28s)]
*  We're here. [[00:39:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2370.8s)]
*  There's a good chance that's not just random. [[00:39:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2371.8s)]
*  Right. [[00:39:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2375.36s)]
*  But isn't it also a good chance that it hasn't been done yet? [[00:39:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2376.36s)]
*  And isn't it a good chance that what we're seeing now is that the potential for this [[00:39:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2381.08s)]
*  to exist is inevitable, that there will one day, if you can develop a technology and we [[00:39:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2386.52s)]
*  most certainly will be able to, if you look at where we are right now in 2025 and you [[00:39:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2394.6400000000003s)]
*  scale forward 50, 60 years, there will be one day a virtual simulation of this reality [[00:40:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2400.48s)]
*  that's indistinguishable from reality. [[00:40:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2407.88s)]
*  So how would we know if we're in it? [[00:40:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2410.32s)]
*  This is the big question, right? [[00:40:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2412.0800000000004s)]
*  But also, isn't it possible that it has to be invented one day, but hasn't yet? [[00:40:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2414.0s)]
*  It's also possible, but then we find ourselves in this very unique moment where it's not [[00:40:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2422.64s)]
*  invented yet, but we are about to invent all this technology. [[00:40:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2427.68s)]
*  It is a possibility, absolutely, but just statistically, I think it's much less and [[00:40:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2431.08s)]
*  I'm trying to bring up this thought experiment with creating this moment and purpose in the [[00:40:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2435.36s)]
*  future to pre-commitments. [[00:40:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2440.96s)]
*  Half the people think it's the dumbest argument in the world, half of them go, it's brilliant, [[00:40:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2443.48s)]
*  obviously we are in one, so I'll let you decide. [[00:40:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2447.36s)]
*  Yeah. [[00:40:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2451.08s)]
*  I feel like if virtual reality does exist, there has to be a moment where it doesn't [[00:40:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2452.44s)]
*  exist and then it's invented. [[00:40:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2458.32s)]
*  Why wouldn't we assume that we're in that moment, especially if we look at the scaling [[00:41:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2460.68s)]
*  forward of technology from MS-DOS to user interfaces of like Apple and then what we're [[00:41:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2466.2s)]
*  at now with quantum computing and these sort of discussions. [[00:41:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2474.0s)]
*  Isn't it more obvious that we can trace back the beginning of these things and we can see [[00:41:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2478.8599999999997s)]
*  that we're in the process of this, that we're not in a simulation, we're in the process [[00:41:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2486.72s)]
*  of eventually creating one. [[00:41:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2491.4199999999996s)]
*  So you zoomed out 30 years. [[00:41:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2493.96s)]
*  Zoom out 15 billion years. [[00:41:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2496.2000000000003s)]
*  You have a multiverse where this process took place billions of times. [[00:41:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2497.76s)]
*  You are simulation within simulation many levels over. [[00:41:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2502.2400000000002s)]
*  And to you, even if this was a simulation of those 30 years, it would look exactly like [[00:41:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2506.5600000000004s)]
*  that. [[00:41:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2512.0800000000004s)]
*  You would see where it started. [[00:41:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2513.0800000000004s)]
*  It wouldn't be magically showing up out of nowhere. [[00:41:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2514.0800000000004s)]
*  Right. [[00:41:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2516.84s)]
*  So if you're playing the game, in the game you have Newton and Michelangelo and Leonardo [[00:41:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2517.84s)]
*  da Vinci. [[00:42:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2522.32s)]
*  Or at least you have memories of those things, even if you started with preloaded memory [[00:42:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2523.6400000000003s)]
*  state. [[00:42:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2528.2000000000003s)]
*  Right. [[00:42:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2529.2000000000003s)]
*  You have Stalin, you have all these problematic human beings and all the different reasons [[00:42:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2530.2000000000003s)]
*  why we've had to do certain things and initiate world conflicts. [[00:42:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2534.1600000000003s)]
*  Then you've had the contrarians that talk and say, actually, that's not what happened. [[00:42:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2537.6800000000003s)]
*  This is what really happened. [[00:42:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2541.6800000000003s)]
*  And it makes it even more confusing and myopic. [[00:42:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2542.6800000000003s)]
*  And then you get to the point where two people, allegedly, like you and I, are sitting across [[00:42:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2546.04s)]
*  from each other on a table made out of wood, but maybe not really. [[00:42:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2551.12s)]
*  It would feel like wood to you either way. [[00:42:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2557.44s)]
*  Is it possible that that's just the nature of the universe itself? [[00:42:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2561.36s)]
*  There are some arguments about kind of self-sustaining simulations where no one's running them externally, [[00:42:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2566.16s)]
*  just the nature. [[00:42:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2571.8s)]
*  But I honestly don't fully comprehend how that would happen. [[00:42:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2572.8s)]
*  Yeah, the holographic universe and the concepts of human consciousness has to interact with [[00:42:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2577.04s)]
*  something for it to exist in the first place. [[00:43:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2583.4s)]
*  That's one. [[00:43:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2585.7599999999998s)]
*  Also, if you have infinite universe, then everything possible happens anyway. [[00:43:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2586.7599999999998s)]
*  But it's boring. [[00:43:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2590.64s)]
*  I don't like this argument. [[00:43:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2591.64s)]
*  Why? [[00:43:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2592.64s)]
*  That's boring? [[00:43:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2593.64s)]
*  Why is that one? [[00:43:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2594.64s)]
*  Everything happens. [[00:43:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2595.64s)]
*  I give you a book which has every conceivable sentence in it and every, like, what do you [[00:43:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2596.64s)]
*  read it? [[00:43:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2600.96s)]
*  There's a lot of garbage you have to go through to find anything interesting. [[00:43:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2601.96s)]
*  Well, is it just that we're so limited cognitively? [[00:43:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2606.12s)]
*  Because we do have a history, at least in the simulation, we do have a history of, I [[00:43:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2611.8s)]
*  mean, there was a gentleman that, see if you could find this, they traced this guy, they [[00:43:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2617.2400000000002s)]
*  found 9,000 year old DNA, and they traced this 9,000 year old DNA to a guy that's living [[00:43:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2623.2s)]
*  right now. [[00:43:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2631.64s)]
*  I believe it's in England. [[00:43:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2632.64s)]
*  I remember reading that. [[00:43:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2634.68s)]
*  Yeah, which is really fascinating. [[00:43:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2636.2s)]
*  So 9,000 years ago, his ancestor lived, and so we have this limitation of our genetics. [[00:43:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2638.8799999999997s)]
*  9,000 years ago, wherever this guy lived, it's probably a hunter and gatherer, probably [[00:44:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2647.3199999999997s)]
*  language, very limited skills in terms of making shelter, and who knows if even he knew [[00:44:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2654.36s)]
*  how to make fire. [[00:44:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2664.04s)]
*  And then here, here at 9,000 DNA just turned human history on its head. [[00:44:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2665.92s)]
*  Is this it? [[00:44:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2669.28s)]
*  I don't think so. [[00:44:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2670.28s)]
*  It was interesting that he ended up living, like, right next to the guy from 9,000, he [[00:44:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2673.92s)]
*  never moved, his family just, like, stayed there for 9,000 years. [[00:44:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2677.8s)]
*  That's awesome. [[00:44:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2681.08s)]
*  It's traced back to one individual, man. [[00:44:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2682.72s)]
*  I actually posted it on my Instagram story, Jamie. [[00:44:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2684.44s)]
*  I'll find it here because it's, oh, here it is. [[00:44:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2687.88s)]
*  9,000 year old skeleton in Somerset. [[00:44:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2695.16s)]
*  This is it. [[00:44:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2698.08s)]
*  So it's, can you send an Instagram story? [[00:44:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2699.48s)]
*  Not sure if you can. [[00:45:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2707.3199999999997s)]
*  It's still on there. [[00:45:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2708.3199999999997s)]
*  I'll go check it real quick. [[00:45:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2709.3199999999997s)]
*  Sorry. [[00:45:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2710.3199999999997s)]
*  I don't know. [[00:45:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2711.56s)]
*  I find it on there. [[00:45:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2712.56s)]
*  I don't know. [[00:45:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2713.56s)]
*  Okay. [[00:45:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2715.56s)]
*  Either way, point being, maybe it's just that we're so limited because we do have this, [[00:45:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2716.56s)]
*  at least again, in this simulation, we're so limited in our ability to even form concepts [[00:45:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2724.48s)]
*  because we have these primitive brains that are the architecture of the human brain itself [[00:45:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2729.84s)]
*  is just not capable of interfacing with the true nature of reality. [[00:45:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2735.68s)]
*  So we give this primitive creature this sort of basic understanding, these blueprints of [[00:45:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2744.16s)]
*  how the world really works, but it's really just a facsimile. [[00:45:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2752.3999999999996s)]
*  It's not capable of understanding. [[00:45:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2758.0s)]
*  When we look at quantum reality, when we look at just the basics of quantum mechanics and [[00:46:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2760.76s)]
*  subatomic particles, it seems like magic, right? [[00:46:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2770.1600000000003s)]
*  Things in superposition, they're both moving and not moving in the same time. [[00:46:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2773.7200000000003s)]
*  They're quantumly attached? [[00:46:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2777.92s)]
*  Like what? [[00:46:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2779.92s)]
*  We have photons that are quantumly entangled. [[00:46:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2780.92s)]
*  This doesn't even make sense to us. [[00:46:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2785.48s)]
*  So is it that the universe itself is so complex, the reality of it, and that we're giving like [[00:46:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2788.44s)]
*  an Atari framework to this monkey? [[00:46:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2798.36s)]
*  That's the gentleman right there. [[00:46:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2802.52s)]
*  This is an old story. [[00:46:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2803.52s)]
*  Oh, is it really? [[00:46:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2804.52s)]
*  It's from 97. [[00:46:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2805.52s)]
*  Oh, no kidding. [[00:46:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2806.52s)]
*  Wow. [[00:46:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2807.52s)]
*  But it kind of makes sense as a simulation theory because all those special effects you [[00:46:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2808.52s)]
*  talk about, so speed of light is just the speed at which your computer updates. [[00:46:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2813.2400000000002s)]
*  Entanglement makes perfect sense if all of it goes through your processor, not directly [[00:46:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2818.12s)]
*  from pixel to pixel. [[00:47:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2822.08s)]
*  And rendering, there are quantum physics experiments which if you observe things, they render different. [[00:47:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2824.04s)]
*  What we do in computer graphics. [[00:47:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2830.84s)]
*  So we see a lot of that. [[00:47:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2832.6s)]
*  You brought up limitations of us as humans. [[00:47:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2833.64s)]
*  We have terrible memory. [[00:47:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2836.52s)]
*  I can remember seven units of information maybe. [[00:47:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2837.96s)]
*  We're kind of slow. [[00:47:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2841.2s)]
*  So we call it artificial stupidity. [[00:47:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2842.44s)]
*  We try to figure out those limits and program them into AI to see if it makes them safer. [[00:47:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2844.64s)]
*  It also makes sense as an experiment to see if we as general intelligences can be better [[00:47:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2850.44s)]
*  controlled with those limitations built in. [[00:47:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2855.12s)]
*  Hmm. [[00:47:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2857.44s)]
*  That's interesting. [[00:47:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2859.68s)]
*  So like some of the things that we have, like Dunbar's number, the inability to keep more [[00:47:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2860.68s)]
*  than a certain number of people in your mind. [[00:47:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2866.8399999999997s)]
*  Absolutely. [[00:47:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2869.56s)]
*  More generally, like why can't you remember anything from prior generations? [[00:47:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2871.64s)]
*  Why can't you just pass that memory? [[00:47:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2876.12s)]
*  Kids are born speaking language. [[00:47:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2877.8399999999997s)]
*  That would be such an advantage. [[00:47:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2879.2799999999997s)]
*  Right, right, right. [[00:48:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2880.6s)]
*  And we have instincts which are built that way. [[00:48:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2881.6s)]
*  So we know evolution found a way to put it in and it's computationally tractable. [[00:48:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2883.72s)]
*  So there is no reason not to have that. [[00:48:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2888.8399999999997s)]
*  We certainly observe it in animals. [[00:48:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2890.64s)]
*  Right. [[00:48:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2892.64s)]
*  Exactly. [[00:48:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2893.64s)]
*  Like especially dogs. [[00:48:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2894.64s)]
*  They have instincts that are. [[00:48:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2895.64s)]
*  But how cool would it be if you had complete memory of your parents? [[00:48:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2896.96s)]
*  Right. [[00:48:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2901.48s)]
*  Well, maybe that would be too traumatic, right, to have a complete memory of all of the things [[00:48:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2902.48s)]
*  that they had gone through to get to the 21st century. [[00:48:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2908.36s)]
*  Maybe that would be so overwhelming to you that you would never be able to progress because [[00:48:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2911.4s)]
*  you would still be traumatized by, you know, whatever that 9,000 year old man went through. [[00:48:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2915.2s)]
*  I don't have complete memory of my existence. [[00:48:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2920.04s)]
*  I vividly remember maybe 4% of my existence, very little of my childhood. [[00:48:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2921.96s)]
*  So you can apply same filtering, but remember useful things like how do you speak? [[00:48:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2926.32s)]
*  Right. [[00:48:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2931.16s)]
*  Right. [[00:48:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2932.16s)]
*  But that's the point maybe. [[00:48:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2933.16s)]
*  Maybe like losing certain memories is actually beneficial because like one of the biggest [[00:48:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2934.16s)]
*  problems that we have is PTSD. [[00:48:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2939.44s)]
*  Right. [[00:49:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2941.68s)]
*  So we have especially people that have gone to war and people that have experienced like [[00:49:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2942.68s)]
*  extreme violence. [[00:49:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2946.92s)]
*  This is obviously a problem with moving forward as a human being. [[00:49:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2949.3199999999997s)]
*  So there would be beneficial for you to not have all of the past lives and all the genetic [[00:49:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2954.8399999999997s)]
*  information that you have from all the 9,000 years of human beings existing and complete [[00:49:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2962.3999999999996s)]
*  total chaos. [[00:49:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2969.52s)]
*  I can make opposite argument. [[00:49:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2971.3599999999997s)]
*  If you had 9,000 years of experience with wars and murder, it wouldn't be a big deal. [[00:49:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2973.2s)]
*  You'd be like, yeah, another one. [[00:49:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2977.52s)]
*  Right. [[00:49:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2979.52s)]
*  Maybe you'd have a difficulty in having a clean slate and moving forward. [[00:49:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2981.32s)]
*  If you look at some of Pinker's work and some of these other people that have looked at [[00:49:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2987.32s)]
*  the history of the human race, it is chaotic and violent as it seems to be today. [[00:49:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2990.92s)]
*  Statistically speaking, this is the safest time ever to be alive. [[00:49:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2996.48s)]
*  And maybe that's because over time we have recognized that these are problems. [[00:49:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=2999.96s)]
*  We're slow to resolve these issues. [[00:50:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3006.2000000000003s)]
*  We are resolving them in a way that's statistically viable. [[00:50:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3009.7200000000003s)]
*  You can then argue in the opposite direction. [[00:50:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3017.04s)]
*  You can say it would help to forget everything other than the last year. [[00:50:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3018.8s)]
*  You'll always have that fresh restart with you. [[00:50:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3022.6400000000003s)]
*  But then you wouldn't have any lessons. [[00:50:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3024.76s)]
*  You wouldn't have character development. [[00:50:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3026.6s)]
*  But you see how one of those has to make sense otherwise. [[00:50:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3027.6s)]
*  Yeah. [[00:50:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3030.36s)]
*  Right. [[00:50:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3031.36s)]
*  Character development is probably important for you to develop discipline and the ability [[00:50:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3032.36s)]
*  to delay gratitude, things like that. [[00:50:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3037.0s)]
*  Multi-generational experience would certainly be a single point of experience. [[00:50:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3042.8s)]
*  Yeah. [[00:50:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3047.8s)]
*  More data is good. [[00:50:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3048.8s)]
*  As we learned, the bitter lesson is more data is good. [[00:50:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3051.36s)]
*  Yeah, more data is good. [[00:50:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3054.36s)]
*  But why am I so reluctant to accept the idea of the simulation is the real question. [[00:50:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3057.7999999999997s)]
*  What is it about it that makes me think... [[00:51:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3064.3199999999997s)]
*  It's almost like it's a throw your hands up in the air moment. [[00:51:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3069.24s)]
*  Like, ah, it's a simulation. [[00:51:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3071.96s)]
*  Yeah. [[00:51:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3073.36s)]
*  You feel like it doesn't matter then. [[00:51:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3074.36s)]
*  It's all fake. [[00:51:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3075.36s)]
*  So why do I care? [[00:51:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3076.36s)]
*  Why should I try hard? [[00:51:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3077.7599999999998s)]
*  Why should I worry about suffering of all those NPCs? [[00:51:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3078.88s)]
*  But that's not how I think about it. [[00:51:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3083.7599999999998s)]
*  There has to be a moment where it doesn't exist. [[00:51:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3087.76s)]
*  Why wouldn't I assume that moment is now? [[00:51:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3091.44s)]
*  When Elon thinks that, I talked to him about it, the chances of us not being in the simulation [[00:51:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3094.8s)]
*  are in the billions. [[00:51:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3100.08s)]
*  Not being or being. [[00:51:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3104.6800000000003s)]
*  Of, excuse us, the chances of us not being in the real world are like billions to one. [[00:51:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3106.08s)]
*  Yeah. [[00:51:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3112.8s)]
*  One to billions. [[00:51:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3113.8s)]
*  Yeah. [[00:51:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3114.8s)]
*  Yeah. [[00:51:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3115.8s)]
*  It makes sense. [[00:51:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3117.6800000000003s)]
*  And he asked a very good question. [[00:51:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3118.6800000000003s)]
*  He asked what's outside the simulation. [[00:51:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3119.6800000000003s)]
*  That's the most interesting question one can ask. [[00:52:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3120.6800000000003s)]
*  In one of the papers, I look at a technique in AI safety called AI boxing, where we put [[00:52:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3124.04s)]
*  AIs in kind of virtual prison to study it, to make sure it's safe, to limit input output [[00:52:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3129.92s)]
*  to it. [[00:52:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3135.0800000000004s)]
*  And the conclusion is basically if it's smart enough, it will eventually escape. [[00:52:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3136.28s)]
*  It will break out of the box. [[00:52:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3139.84s)]
*  So it's a good tool. [[00:52:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3141.88s)]
*  It buys you time, but it's not a permanent solution. [[00:52:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3142.88s)]
*  We can take it to the next level. [[00:52:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3146.44s)]
*  If it's smart enough, will it kind of go, oh, you're also in a virtual box and either [[00:52:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3147.96s)]
*  show us how to escape or fail to escape. [[00:52:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3152.3599999999997s)]
*  Either way, either we know it's possible to contain super intelligence or we get access [[00:52:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3155.64s)]
*  to the real information. [[00:52:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3159.7599999999998s)]
*  And so if it's impossible to contain super intelligence and if there is a world that [[00:52:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3162.98s)]
*  we can imagine where a simulation exists that's indistinguishable from reality, we're [[00:52:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3171.64s)]
*  probably living in it. [[00:52:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3178.72s)]
*  We don't know if it's actually the same as reality. [[00:53:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3180.7599999999998s)]
*  It could be a completely weird kind of Simpsons looking simulation, which is assuming it's [[00:53:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3183.52s)]
*  the same. [[00:53:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3187.7999999999997s)]
*  Well, here's the real question. [[00:53:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3188.7999999999997s)]
*  Is there a reality? [[00:53:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3189.7999999999997s)]
*  Has there ever been one? [[00:53:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3191.3599999999997s)]
*  It would make sense that there was a start to the process, but being specific about it [[00:53:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3194.7599999999998s)]
*  is kind of hard philosophical scientific problem. [[00:53:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3199.92s)]
*  Well, it's impossible, right? [[00:53:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3203.08s)]
*  In science, we study things about the moment of Big Bang, the properties of that moment. [[00:53:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3206.44s)]
*  We don't know what caused it. [[00:53:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3212.0s)]
*  Anything before it is obviously not accessible from within our universe. [[00:53:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3213.94s)]
*  But there is some things you can learn. [[00:53:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3217.36s)]
*  We can learn about if we're in a simulation that simulators don't care about your suffering. [[00:53:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3220.2000000000003s)]
*  You can learn that they don't mind you dying. [[00:53:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3226.42s)]
*  You can learn things just by observing simulation around us. [[00:53:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3228.76s)]
*  Well, here's the question about all that other stuff like suffering and dying. [[00:53:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3232.6800000000003s)]
*  Do those factors exist in order to motivate us to improve the conditions of the world [[00:54:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3241.5200000000004s)]
*  that we're living in? [[00:54:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3249.2000000000003s)]
*  Like, if we did not have evil, would we be motivated to be good? [[00:54:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3250.2000000000003s)]
*  Do you think that these factors exist? [[00:54:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3256.76s)]
*  I've talked about this before, but the way I think about the human race is if I was studying [[00:54:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3260.6000000000004s)]
*  the human race from afar, if I was some person from another planet with no understanding [[00:54:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3265.1200000000003s)]
*  of any of the entities on Earth, I would look at this one apex creature and I would say, [[00:54:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3271.6400000000003s)]
*  what is this thing doing? [[00:54:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3277.84s)]
*  Well, it makes better things. [[00:54:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3279.0s)]
*  That's all it does. [[00:54:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3280.92s)]
*  It just continually makes better things. [[00:54:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3281.92s)]
*  That's its number one goal. [[00:54:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3283.96s)]
*  It's different than any other creature on this planet. [[00:54:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3285.48s)]
*  Every other creature on the planet exists within its ecosystem. [[00:54:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3289.2s)]
*  It thrives. [[00:54:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3293.12s)]
*  Maybe it's a predator. [[00:54:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3294.12s)]
*  Maybe it's a prey. [[00:54:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3295.12s)]
*  It does what it does in order to try to survive, but this thing makes stuff and it keeps making [[00:54:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3296.12s)]
*  better stuff all the time. [[00:55:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3301.7s)]
*  What's its ultimate purpose? [[00:55:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3303.16s)]
*  Its ultimate purpose might be to make a better version of itself because if you just extrapolate, [[00:55:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3304.84s)]
*  if you take what we're doing from the first IBM computers to what we have today, where [[00:55:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3311.04s)]
*  is it going? [[00:55:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3319.48s)]
*  Well, it's going to clearly keep getting better. [[00:55:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3320.48s)]
*  What does that mean? [[00:55:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3322.56s)]
*  It means artificial life. [[00:55:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3323.56s)]
*  Are we just a bee making a beehive? [[00:55:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3325.56s)]
*  Are we a caterpillar making a cocoon that eventually the electronic butterfly is going [[00:55:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3329.88s)]
*  to fly out of? [[00:55:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3335.16s)]
*  It seems like if I wasn't completely connected to being a human being, I would assume that. [[00:55:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3336.66s)]
*  It's hard to define better. [[00:55:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3343.64s)]
*  You're saying smarter? [[00:55:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3345.46s)]
*  Would it be better if we didn't experience extreme states of suffering and pain? [[00:55:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3347.02s)]
*  You can teach lessons with very mild pain. [[00:55:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3351.5s)]
*  You don't have to burn children alive, right? [[00:55:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3353.54s)]
*  It's not a necessity for learning. [[00:55:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3356.62s)]
*  What do you mean by that? [[00:55:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3358.2999999999997s)]
*  In this universe, we see extreme examples of suffering. [[00:56:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3360.8999999999996s)]
*  Oh, for sure. [[00:56:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3363.8999999999996s)]
*  If the goal was just to motivate us, you could have much lower levels as the maximum. [[00:56:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3365.02s)]
*  Right, but if you want to really motivate people, the only reason to create nuclear [[00:56:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3370.18s)]
*  weapons is your word that other people are going to create nuclear weapons. [[00:56:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3375.94s)]
*  If you want to really motivate someone, you have to have evil tyrants in order to justify [[00:56:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3379.46s)]
*  having this insane army filled with bombers and hypersonic missiles. [[00:56:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3383.88s)]
*  If you really want progress, you have to be motivated. [[00:56:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3389.82s)]
*  I think at some point, we stop fully understanding how bad things are. [[00:56:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3393.1s)]
*  Let's say you have a pain scale from zero to infinity. [[00:56:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3396.6600000000003s)]
*  I think you should stop at 100. [[00:56:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3400.1400000000003s)]
*  It doesn't have to be billion and trillion. [[00:56:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3401.86s)]
*  It's not adding additional learning signal. [[00:56:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3405.06s)]
*  Can you apply that to the human race and culture and society? [[00:56:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3409.1600000000003s)]
*  I think we basically compete with others in relative terms. [[00:56:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3413.46s)]
*  I don't have to be someone who has trillions of dollars. [[00:56:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3416.94s)]
*  I just need more money than you. [[00:57:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3420.06s)]
*  Yeah, but that's just logical. [[00:57:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3421.58s)]
*  You're being a logical person. [[00:57:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3422.7799999999997s)]
*  I don't think humans are very logical. [[00:57:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3425.22s)]
*  We're not, but we understand pain signal well at somewhat low levels. [[00:57:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3428.06s)]
*  We don't have to max out on pain. [[00:57:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3432.1s)]
*  Right. [[00:57:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3435.38s)]
*  We don't have to, but if you want to really stoke the fires and get things moving. [[00:57:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3436.9s)]
*  It seems that simulators agree with you, and that's exactly what they did. [[00:57:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3442.8199999999997s)]
*  Thanks. [[00:57:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3446.66s)]
*  So here's the question. [[00:57:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3448.1s)]
*  What's at the heart of the simulation? [[00:57:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3451.18s)]
*  Is the universe simulated? [[00:57:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3454.3199999999997s)]
*  Is the whole thing a simulation? [[00:57:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3456.22s)]
*  Is there an actual living entity that constructed this? [[00:57:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3459.16s)]
*  Is this the state of the universe itself, and we have misinterpreted what reality is? [[00:57:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3468.38s)]
*  For every option you mentioned, there is someone who wrote a paper about it. [[00:57:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3476.66s)]
*  Is it just your universe? [[00:58:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3480.42s)]
*  Is it for all of us? [[00:58:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3482.02s)]
*  Are we NPCs? [[00:58:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3483.02s)]
*  Are there many? [[00:58:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3484.02s)]
*  Is this a state of it? [[00:58:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3485.62s)]
*  People try to figure out what's going on. [[00:58:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3487.46s)]
*  Some of those make more sense than others, but you can't tell from inside what it is [[00:58:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3489.58s)]
*  unless they tell you, and they can lie to you. [[00:58:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3495.14s)]
*  Who's they, though? [[00:58:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3499.82s)]
*  The simulators. [[00:58:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3500.82s)]
*  If they decided to prove to you, you are in a simulation, let's run experiments. [[00:58:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3501.82s)]
*  And those would be like, I don't know if it's advanced technology or... [[00:58:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3506.2s)]
*  When you think about it, if you believe in the simulation, when you think about it, what [[00:58:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3510.44s)]
*  are the parameters that you think exist? [[00:58:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3514.52s)]
*  How do you think this could possibly have been created? [[00:58:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3518.2999999999997s)]
*  So the examples I gave you with technology we already have, I think there is someone [[00:58:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3523.0s)]
*  with access to very good virtual reality. [[00:58:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3527.0s)]
*  They can create intelligent agents, and for whatever reason, I cannot tell from inside, [[00:58:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3529.4s)]
*  they're running those experiments. [[00:58:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3533.7999999999997s)]
*  But is that the only possibility, or is the possibility that the actual nature of reality [[00:58:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3535.96s)]
*  itself is just way more confusing than we've... [[00:59:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3542.68s)]
*  That's a possibility. [[00:59:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3546.76s)]
*  It could be alien simulation, alien dolphins dreaming. [[00:59:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3547.76s)]
*  There's infinite supply of alternative explanations. [[00:59:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3551.96s)]
*  I understand that, but what I want to get inside of your head, I want to know what you [[00:59:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3554.56s)]
*  think about it. [[00:59:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3557.8s)]
*  When you think about this and you ponder the possibilities, what makes sense to you? [[00:59:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3559.32s)]
*  So I apply Occam's Razor. [[00:59:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3564.64s)]
*  I try to find the simplest explanation. [[00:59:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3565.64s)]
*  I think we're already creating virtual reality. [[00:59:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3567.68s)]
*  Let's just see what you can do with it if it's sufficiently advanced. [[00:59:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3570.92s)]
*  But who and why? [[00:59:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3575.48s)]
*  So future us running ancestral simulations is a very simple one. [[00:59:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3578.68s)]
*  Oh boy. [[00:59:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3583.0s)]
*  Future us running... [[00:59:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3584.0s)]
*  Well, that's what a lot of people think the aliens are, right? [[00:59:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3585.0s)]
*  Could be us visiting, but then again, if they're running the simulation, you don't have to [[00:59:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3588.46s)]
*  physically show up in a game. [[00:59:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3592.08s)]
*  They have access to direct memory states. [[00:59:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3593.36s)]
*  Well, that would also make a lot of sense when it's always very blurry and doesn't seem [[00:59:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3596.52s)]
*  real. [[01:00:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3602.2000000000003s)]
*  I think lately we've been getting better ones, but it's also the time that we're getting [[01:00:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3604.4s)]
*  better deep fakes, so I can no longer trust my eyes. [[01:00:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3608.4s)]
*  Yeah. [[01:00:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3612.04s)]
*  Yeah. [[01:00:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3613.04s)]
*  Did you see the latest one that Jeremy Corbell posted? [[01:00:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3614.04s)]
*  The one you sent me? [[01:00:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3615.96s)]
*  Yeah. [[01:00:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3617.96s)]
*  Did you see it? [[01:00:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3618.96s)]
*  It's weird. [[01:00:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3619.96s)]
*  Yeah. [[01:00:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3620.96s)]
*  It's hard to tell what it is. [[01:00:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3621.96s)]
*  That's the thing. [[01:00:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3623.76s)]
*  He might be right. [[01:00:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3624.76s)]
*  We might be in a simulation, and it might be horse shit, because they all seem like [[01:00:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3625.76s)]
*  horse shit. [[01:00:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3629.12s)]
*  It's like the first horse shit was Bigfoot, and then as technology scaled out and we get [[01:00:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3630.12s)]
*  a greater understanding, we developed GPS and satellites and more people study the woods, [[01:00:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3634.76s)]
*  we're like, yeah, that seems like horse shit. [[01:00:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3639.8s)]
*  So that horse shit's kind of gone away. [[01:00:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3642.86s)]
*  But the UFO horse shit still around, because you have anecdotal experiences, abductees [[01:00:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3644.44s)]
*  with very compelling stories, you have whistleblowers from deep inside the military telling you [[01:00:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3651.7599999999998s)]
*  that we're working on back engineered products, but it also seems like a back plot to a video [[01:00:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3657.36s)]
*  game that I'm playing. [[01:01:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3661.74s)]
*  And it was weird to see government come out all of a sudden and have conferences about [[01:01:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3663.08s)]
*  it and tell us everything they know. [[01:01:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3667.66s)]
*  It almost seemed like they're trying too hard. [[01:01:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3669.52s)]
*  With simulation, what's interesting, it's not just the last couple years, then we got [[01:01:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3672.7999999999997s)]
*  computers. [[01:01:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3676.56s)]
*  If you look at religions, world religions, and you strip away all the local culture, [[01:01:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3677.6s)]
*  like take Saturday off, take Sunday off, donate this animal, donate that animal, what they [[01:01:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3682.6s)]
*  all agree on is that there is super intelligence which created a fake world, and this is a [[01:01:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3687.2999999999997s)]
*  test, do this or that. [[01:01:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3692.44s)]
*  They describing, like if you went to jungle and told primitive tribe about my paper and [[01:01:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3694.02s)]
*  simulation theory, that's what they would know three generations later, like God, religion, [[01:01:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3698.98s)]
*  that's what they got out of it. [[01:01:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3703.72s)]
*  But they don't think it's a fake world. [[01:01:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3706.34s)]
*  A made world. [[01:01:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3708.1400000000003s)]
*  A physical world is a subset of a real world which is non-physical, right? [[01:01:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3709.6600000000003s)]
*  That's the standard. [[01:01:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3713.42s)]
*  Right, so this physical world being created by God. [[01:01:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3714.42s)]
*  Right. [[01:01:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3717.34s)]
*  But what existed before the physical world created by God? [[01:01:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3718.46s)]
*  Ideas, just information. [[01:02:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3722.34s)]
*  Just God. [[01:02:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3724.36s)]
*  God was bored. [[01:02:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3725.36s)]
*  And he was like, let's give some, make some animals that can think and solve problems. [[01:02:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3726.36s)]
*  And for what reason? [[01:02:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3732.5600000000004s)]
*  I think to create God. [[01:02:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3733.94s)]
*  This is what I worry about. [[01:02:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3735.7s)]
*  I worry about that's really the nature of the universe itself. [[01:02:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3737.2799999999997s)]
*  That is actually created by human beings creating this infinitely intelligent thing that can [[01:02:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3741.3799999999997s)]
*  essentially harness all of the available energy and power of the universe and create anything [[01:02:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3748.46s)]
*  it wants. [[01:02:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3754.7s)]
*  That it is God. [[01:02:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3755.7s)]
*  That is, like, you know, this whole idea of Jesus coming back, well, maybe it's real. [[01:02:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3757.16s)]
*  We just completely misinterpreted these ancient scrolls and texts. [[01:02:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3763.52s)]
*  And what it really means is that we are going to give birth to this. [[01:02:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3767.76s)]
*  And a virgin birth at that. [[01:02:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3772.32s)]
*  There is definitely a possibility of a cycle. [[01:02:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3774.2400000000002s)]
*  So we had Big Bang. [[01:02:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3776.44s)]
*  It starts this process. [[01:02:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3778.28s)]
*  We are creating more powerful systems. [[01:02:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3779.28s)]
*  We need to compute so we bring together more and more matter in one point. [[01:03:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3780.92s)]
*  Next Big Bang takes place. [[01:03:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3785.44s)]
*  And it's a cycle of repeated booms and busts. [[01:03:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3786.96s)]
*  Right, right, right. [[01:03:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3790.5600000000004s)]
*  There are legitimate scientists that believe that. [[01:03:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3792.56s)]
*  Yeah. [[01:03:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3795.76s)]
*  That this... [[01:03:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3796.76s)]
*  So what's the value in life today then? [[01:03:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3797.76s)]
*  What do humans value? [[01:03:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3807.32s)]
*  Yeah. [[01:03:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3808.32s)]
*  If you, if this is a simulation and if in the middle of this simulation we are about [[01:03:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3809.32s)]
*  to create super intelligence, why? [[01:03:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3815.4s)]
*  So there are external reasons. [[01:03:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3819.48s)]
*  We don't know for sure. [[01:03:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3820.84s)]
*  And then there are internal things in the simulation which are still real. [[01:03:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3821.96s)]
*  Pain and suffering, if simulated, is still real. [[01:03:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3825.96s)]
*  You still experience it. [[01:03:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3828.6400000000003s)]
*  Of course. [[01:03:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3829.92s)]
*  Hedonic pleasures, friendships, love. [[01:03:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3830.92s)]
*  All that stays real. [[01:03:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3832.6000000000004s)]
*  It doesn't change. [[01:03:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3833.6000000000004s)]
*  You can still be good or bad. [[01:03:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3834.6000000000004s)]
*  So that's interesting. [[01:03:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3836.76s)]
*  But externally we have no idea if we are running scientific experiment, entertainment. [[01:03:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3838.1600000000003s)]
*  It could be completely unobserved. [[01:04:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3842.44s)]
*  Some kid just set an experiment, run a billion random simulations, see what comes out of [[01:04:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3844.04s)]
*  it. [[01:04:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3848.52s)]
*  What you said about us creating new stuff. [[01:04:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3849.52s)]
*  Maybe it's a startup trying to develop new technology and we're running a bunch of humans [[01:04:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3851.52s)]
*  to see if we can come up with a new iPhone. [[01:04:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3855.04s)]
*  But what's outside of that then? [[01:04:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3858.68s)]
*  When you think about it, if you're attached to this idea, and I don't know if you're attached [[01:04:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3860.88s)]
*  to this idea, but if you are attached to this idea, what's outside of this idea? [[01:04:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3865.52s)]
*  Like if this simulation is, if it's paused, what is reality? [[01:04:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3870.52s)]
*  So there seems to be a trend to converge in certain things. [[01:04:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3878.36s)]
*  Agents which are smart enough tend to converge in some instrumental goals, not terminal goals. [[01:04:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3882.56s)]
*  Terminal goals are things you prefer like I want to collect stamps. [[01:04:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3887.6600000000003s)]
*  That's arbitrary. [[01:04:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3891.6400000000003s)]
*  But acquiring resources, self-protection, control, things like that tend to be useful [[01:04:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3892.7200000000003s)]
*  in all situations. [[01:04:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3899.4s)]
*  So all the smart enough agents will probably converge on that set. [[01:05:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3900.88s)]
*  And if they train on all the data or they do zero knowledge training, meaning they're [[01:05:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3905.16s)]
*  really just discovering basic structure of physics, it's likely they will all converge [[01:05:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3909.72s)]
*  on one similar architecture, one super agent. [[01:05:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3914.56s)]
*  So kind of like AI is one. [[01:05:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3918.48s)]
*  And then this is just part of this infinite cycle which will lead to another Big Bang, [[01:05:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3922.96s)]
*  which is Penrose. [[01:05:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3928.6s)]
*  Penrose things, it's just like this constant cycle of infinite Big Bangs. [[01:05:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3931.32s)]
*  It would make sense that there is an end and a start. [[01:05:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3936.76s)]
*  It would make sense. [[01:05:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3939.6000000000004s)]
*  But it also makes sense that we're so limited by our biological lifespan too, because we [[01:05:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3940.88s)]
*  like to think that this is so significant. [[01:05:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3947.0s)]
*  Because we only have a hundred years if we're lucky, we think, well, why would everything... [[01:05:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3949.48s)]
*  But if the universe really does start and end with an infinite number of Big Bangs, [[01:05:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3954.6200000000003s)]
*  what does it give a shit about this 100 year lifespan that we think is so significant? [[01:06:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3961.1200000000003s)]
*  It's not significant to the universe. [[01:06:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3967.1200000000003s)]
*  It's just significant in our own little version of this game that we're playing. [[01:06:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3969.6600000000003s)]
*  That's exactly right. [[01:06:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3974.36s)]
*  And so many people now kind of try to zoom out and go, if I wasn't human, if I didn't [[01:06:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3975.36s)]
*  have this pro-human bias, would I care about them? [[01:06:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3979.76s)]
*  No, they're not special. [[01:06:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3982.6400000000003s)]
*  It's a large universe, many alien races, a lot of resources. [[01:06:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3983.6400000000003s)]
*  Creating super intelligence is the important thing. [[01:06:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3988.68s)]
*  Maybe that's what matters. [[01:06:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3991.5s)]
*  And I'm kind of like, nope, I'm biased pro-humans. [[01:06:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3992.88s)]
*  This is the last bias you're still allowed to have. [[01:06:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3995.92s)]
*  I'm going to keep it. [[01:06:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3997.72s)]
*  Well that's your role in this simulation. [[01:06:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=3999.4s)]
*  Your role in the simulation is to warn us about this thing that we're creating. [[01:06:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4001.96s)]
*  Here I am, yeah. [[01:06:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4005.2400000000002s)]
*  Yeah, there you are. [[01:06:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4006.2400000000002s)]
*  We're doing a good job. [[01:06:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4007.2400000000002s)]
*  I think what you were saying earlier about this being the answer to the Fermi paradox, [[01:06:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4008.2400000000002s)]
*  that makes a lot of sense. [[01:06:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4012.68s)]
*  Because I've tried to think about this a lot since AI started really ramping up its [[01:06:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4014.76s)]
*  capability. [[01:07:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4023.28s)]
*  And I was thinking, well, if we do eventually create super intelligence, and if this is [[01:07:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4024.28s)]
*  this normal pattern that exists all throughout the universe, well, you probably wouldn't [[01:07:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4029.5200000000004s)]
*  have visitors. [[01:07:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4034.28s)]
*  You probably wouldn't have advanced civilizations. [[01:07:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4036.1800000000003s)]
*  Because everything would be inside some sort of a digital architecture. [[01:07:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4041.68s)]
*  There would be no need to travel. [[01:07:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4045.64s)]
*  That's one possibility. [[01:07:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4048.8399999999997s)]
*  Another one is that we try to acquire more resources, capture other galaxies for compute, [[01:07:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4050.16s)]
*  and then you would see this wall of computronium coming to you. [[01:07:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4055.3199999999997s)]
*  But we don't see it, so maybe I'm wrong. [[01:07:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4058.22s)]
*  Wall of... [[01:07:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4060.8799999999997s)]
*  Say that again? [[01:07:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4061.8799999999997s)]
*  Computronium, like a substance converting everything in the universe into more compute. [[01:07:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4062.8799999999997s)]
*  Sometimes people talk about hedonium, so a system for just generating pleasure at the [[01:07:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4069.32s)]
*  microscopic level. [[01:07:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4073.44s)]
*  Oh, Roman. [[01:07:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4075.44s)]
*  When you write a book like this, I won't let everybody know your book, if people want to [[01:07:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4078.1200000000003s)]
*  freak out, because I think they do. [[01:08:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4081.28s)]
*  AI, unexplainable, unpredictable, and uncontrollable. [[01:08:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4083.0s)]
*  Do you have this feeling when you're writing a book like this and you're publishing it [[01:08:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4086.7200000000003s)]
*  of futility? [[01:08:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4093.6800000000003s)]
*  Does that enter into your mind? [[01:08:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4094.6800000000003s)]
*  This is happening no matter what. [[01:08:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4097.64s)]
*  Some people are very optimistic. [[01:08:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4101.68s)]
*  Lex was very optimistic. [[01:08:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4103.6s)]
*  Yes. [[01:08:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4104.6s)]
*  Some people are pessimistic. [[01:08:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4105.6s)]
*  Both are a form of bias. [[01:08:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4106.6s)]
*  You want to be basing your decisions on data. [[01:08:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4108.200000000001s)]
*  You want to be realistic. [[01:08:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4111.52s)]
*  I just want to report what is actually the state of the art in this. [[01:08:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4113.4400000000005s)]
*  I don't try to spin it either way. [[01:08:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4117.240000000001s)]
*  If someone else has a different set of evidence, we can consider it. [[01:08:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4120.04s)]
*  I want to know what's really happening. [[01:08:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4124.56s)]
*  I want to know the reality of it. [[01:08:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4126.0s)]
*  I don't see it as fear-mongering or anything of that nature. [[01:08:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4128.68s)]
*  I see it as, as of today, whatever today is day 21st, no one has a solution to this problem. [[01:08:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4132.68s)]
*  Here's how soon it's happening. [[01:09:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4140.12s)]
*  Let's have a conversation. [[01:09:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4142.12s)]
*  Because right now, the large AI labs are running this experiment on 8 billion people. [[01:09:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4143.48s)]
*  They don't have any consent. [[01:09:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4148.72s)]
*  They cannot get consent. [[01:09:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4149.72s)]
*  Nobody can consent because we don't understand what we're agreeing to. [[01:09:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4151.56s)]
*  I would like people to know about it at least and they can maybe make some good decisions [[01:09:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4155.6s)]
*  about what needs to happen. [[01:09:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4159.8s)]
*  Not only that, but the people that are running it, they're odd people. [[01:09:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4161.52s)]
*  I don't have anything against Sam Altman. [[01:09:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4165.400000000001s)]
*  I know Elon Musk does not like him. [[01:09:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4168.72s)]
*  But when I had him in here, I was like, it's like I'm talking to a politician that is in [[01:09:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4171.04s)]
*  presidential term, or presidential election cycle, where they're, they were very careful [[01:09:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4179.28s)]
*  with what they say. [[01:09:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4187.4s)]
*  Everything has been vetted by a focus group and you don't really get a real human response. [[01:09:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4189.2s)]
*  Everything was like, interesting, very interesting, like all bullshit. [[01:09:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4196.08s)]
*  They're going to leave here and keep creating this fucking monster that's going to destroy [[01:10:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4200.36s)]
*  the human race and never let onto it at all. [[01:10:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4204.2s)]
*  He's a social super intelligence. [[01:10:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4207.0s)]
*  So what you need to do is look at his blog posts before he was running OpenAI. [[01:10:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4208.8s)]
*  A social super intelligence. [[01:10:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4212.8s)]
*  Interesting. [[01:10:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4214.68s)]
*  Why do you define him that way? [[01:10:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4215.68s)]
*  He's very good at acquiring resources, staying in control. [[01:10:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4216.92s)]
*  He's basically showing us a lot of the things we are concerned about with AI and our ability [[01:10:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4220.92s)]
*  to control them as well. [[01:10:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4227.72s)]
*  We had, well, they had, OpenAI had a board with a mission of safety and openness and [[01:10:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4229.08s)]
*  they tried removing him and they failed. [[01:10:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4234.88s)]
*  The board is gone. [[01:10:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4237.16s)]
*  He's still there. [[01:10:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4238.28s)]
*  There's also been a lot of deception in terms of profitability and how much money he is [[01:10:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4239.28s)]
*  extracting from it. [[01:10:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4244.8s)]
*  I met him a few times. [[01:10:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4246.6s)]
*  He's super nice. [[01:10:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4247.88s)]
*  Very nice guy. [[01:10:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4248.88s)]
*  Really enjoyed him. [[01:10:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4249.88s)]
*  Some people say that they already took over his mind and controlling him, but I have no [[01:10:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4250.88s)]
*  idea. [[01:10:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4255.32s)]
*  Well, he might be an agent of AI. [[01:10:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4256.32s)]
*  I mean, if, if, look, if let's assume that this is a simulation, we're inside of a simulation. [[01:10:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4258.28s)]
*  Are we interacting with other humans in the simulation? [[01:11:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4265.16s)]
*  And is some, are some of the things that are inside the simulation, are they artificially [[01:11:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4269.639999999999s)]
*  generated? [[01:11:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4274.36s)]
*  Are there, are there people that we think are people that are actually just a part of [[01:11:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4275.36s)]
*  this program? [[01:11:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4280.44s)]
*  So it's the NPC versus real player question really. [[01:11:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4281.44s)]
*  And again, we don't know how to test for consciousness. [[01:11:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4284.5599999999995s)]
*  Always assume everyone is conscious and treat them nice. [[01:11:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4286.96s)]
*  Yes, that's the thing. [[01:11:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4289.2s)]
*  We want to be compassionate, kind people, but you will meet people in this life. [[01:11:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4290.2s)]
*  If you're like, this guy is such a fucking idiot. [[01:11:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4293.76s)]
*  He can't be real. [[01:11:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4295.4400000000005s)]
*  Or he has to have a very limited role in this bizarre game we're playing. [[01:11:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4297.4400000000005s)]
*  There's people that you're going to run into that are like that. [[01:11:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4301.320000000001s)]
*  You ever meet someone where they repeat the same story to you every time you meet them? [[01:11:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4303.320000000001s)]
*  Yeah. [[01:11:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4306.76s)]
*  They have a script. [[01:11:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4307.76s)]
*  Well, it's also, you know, you want to be very kind here, right? [[01:11:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4308.76s)]
*  You don't, but you've got to assume, and I know my own intellectual limitations in comparison [[01:11:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4315.04s)]
*  to some of the people that have had, like Roger Penrose or, you know, Elon or many of [[01:12:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4321.599999999999s)]
*  the people that I've talked to. [[01:12:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4325.36s)]
*  I know my mind doesn't work the way their mind works. [[01:12:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4326.599999999999s)]
*  So there are variabilities that are, whether genetic, predetermined, whether it's just [[01:12:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4329.36s)]
*  the life that they've chosen and the amount of information that they've digested along [[01:12:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4334.96s)]
*  the way and be able to hold on to. [[01:12:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4339.36s)]
*  But their brain is different than mine. [[01:12:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4342.08s)]
*  And then I've met people where I'm like, there's nothing there. [[01:12:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4344.299999999999s)]
*  Like I can't help this person. [[01:12:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4349.08s)]
*  This is like I'm talking to a Labrador retriever. [[01:12:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4350.68s)]
*  You know what I mean? [[01:12:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4353.4800000000005s)]
*  Like there's certain human beings that you run into in this life and you're like, well, [[01:12:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4354.4800000000005s)]
*  is this because this is the way that things get done? [[01:12:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4358.280000000001s)]
*  And the only way things get done is you need a certain amount of manual labor and not just [[01:12:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4364.0s)]
*  young people that need a job because they're, you know, in between high school and college [[01:12:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4368.76s)]
*  and they're trying to do. [[01:12:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4374.1s)]
*  So you need somebody who can carry things for you. [[01:12:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4375.580000000001s)]
*  You need roles in society. [[01:12:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4378.44s)]
*  And occasionally you have a nickel to Tesla. [[01:13:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4382.08s)]
*  You know, occasionally you have one of these very brilliant innovators that elevates the [[01:13:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4385.2s)]
*  entirety of the human race. [[01:13:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4391.96s)]
*  But for the most part, as this thing is playing out, you're going to need a bunch of people [[01:13:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4394.24s)]
*  that have paperwork filers. [[01:13:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4398.54s)]
*  You're going to need a bunch of people that are security guards in an office space. [[01:13:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4400.36s)]
*  You're going to need a bunch of people that aren't thinking that much. [[01:13:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4403.04s)]
*  They're just kind of existing and they can't wait for five o'clock so they can get home [[01:13:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4406.56s)]
*  and watch Netflix. [[01:13:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4410.160000000001s)]
*  I think that's what happens to them. [[01:13:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4411.8s)]
*  But the reason is the spectrum of IQ. [[01:13:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4414.160000000001s)]
*  If you have IQ from 50 to 200, that's what you're going to see. [[01:13:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4417.240000000001s)]
*  And a great lesson here is projected forward. [[01:13:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4421.4800000000005s)]
*  If you have something with IQ of 10,000, what is that going to invent for us? [[01:13:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4423.96s)]
*  What is it going to accomplish? [[01:13:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4428.320000000001s)]
*  Yeah, it always impresses me to see someone with 30 felonies and someone with 30 patents. [[01:13:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4429.320000000001s)]
*  How did that work? [[01:13:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4435.28s)]
*  Now scale it to someone who can invent new physics. [[01:13:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4437.16s)]
*  Right, right. [[01:14:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4440.12s)]
*  And you know, the person who has the largest IQ, the largest at least registered IQ in [[01:14:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4441.12s)]
*  the world is this gentleman who recently posted on Twitter about Jesus, that he believes Jesus [[01:14:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4447.679999999999s)]
*  is real. [[01:14:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4452.48s)]
*  Do you know who this is? [[01:14:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4453.48s)]
*  I saw the post. [[01:14:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4456.0s)]
*  You see that post? [[01:14:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4457.0s)]
*  I saw the post. [[01:14:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4458.0s)]
*  What did you think about that? [[01:14:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4459.0s)]
*  I felt like this was... [[01:14:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4460.0s)]
*  I think we don't know how to measure IQs outside of standard range. [[01:14:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4461.0s)]
*  Something above 150, they create customized tests, which like four people in the world [[01:14:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4464.2s)]
*  can take. [[01:14:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4468.08s)]
*  We just don't have it. [[01:14:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4469.24s)]
*  It's a normalized test to average human, average Western American, whatever. [[01:14:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4470.24s)]
*  And so we just don't have the expertise. [[01:14:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4475.0s)]
*  So someone very super intelligent in test taking can score really well. [[01:14:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4477.48s)]
*  But if you look at Mensa as a group, they don't usually have amazing accomplishments. [[01:14:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4482.679999999999s)]
*  They're very kind of cool people, but they are not Nobel Prize winners majority. [[01:14:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4486.92s)]
*  Exactly. [[01:14:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4491.4s)]
*  I was going to bring that up. [[01:14:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4492.4s)]
*  It's fascinating to me. [[01:14:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4493.84s)]
*  There's a lot of people that are in Mensa, they want to tell you how smart they are by [[01:14:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4495.12s)]
*  being in Mensa, but your life is kind of bullshit. [[01:14:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4498.08s)]
*  Your life's a mess. [[01:15:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4502.04s)]
*  Like if you're really intelligent, you'd have social intelligence as well. [[01:15:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4503.04s)]
*  You'd have the ability to formulate a really cool tribe. [[01:15:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4506.64s)]
*  There's a lot of intelligence that's not as simple as being able to solve equations and [[01:15:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4510.68s)]
*  answer difficult questions. [[01:15:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4516.6s)]
*  There's a lot of intelligence in how you navigate life itself and how you treat human beings [[01:15:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4518.240000000001s)]
*  and the path that you choose in terms of, like we were talking about, delayed gratification. [[01:15:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4523.2s)]
*  There's a certain amount of intelligence in that. [[01:15:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4531.16s)]
*  Certain amount of intelligence in discipline. [[01:15:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4533.88s)]
*  There's a certain amount of intelligence in forcing yourself to get up in the morning [[01:15:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4535.8s)]
*  and go for a run. [[01:15:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4539.36s)]
*  There's intelligence in that. [[01:15:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4540.8s)]
*  Being able to control the mind and this sort of binary approach to intelligence that we [[01:15:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4543.32s)]
*  have. [[01:15:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4549.16s)]
*  So many people are amazingly brilliant in a narrow domain. [[01:15:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4550.16s)]
*  They don't scale to others and we care about general intelligence. [[01:15:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4554.4s)]
*  So take someone like Warren Buffett. [[01:15:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4557.36s)]
*  No one's better at making money, but then what to do with that money is a separate problem [[01:15:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4559.28s)]
*  and he's, I don't know, a hundred and something years old. [[01:16:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4564.12s)]
*  He has $200 billion and what is he doing with that resource? [[01:16:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4567.88s)]
*  He's drinking Coca-Cola and eating McDonald's. [[01:16:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4572.32s)]
*  While living in a house he bought 30 years ago. [[01:16:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4574.4s)]
*  It seems like you can optimize in that. [[01:16:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4577.62s)]
*  Like putting $160 billion of his dollars towards immortality would be a good bet for him. [[01:16:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4579.42s)]
*  Yeah, and the first thing they would do is tell him, stop drinking Coca-Cola. [[01:16:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4584.0599999999995s)]
*  What are you doing? [[01:16:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4587.66s)]
*  He drinks it every day. [[01:16:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4588.66s)]
*  I don't know if it's marketing he's invested. [[01:16:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4589.66s)]
*  So he's just like, cool. [[01:16:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4591.62s)]
*  Well, I think he probably has really good doctors and really good medical care that [[01:16:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4592.82s)]
*  counteracts his poor choices. [[01:16:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4596.9s)]
*  But we're not in a world where you can spend money to buy life extension. [[01:16:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4599.58s)]
*  No matter how many billions you have, you're not going to live to 200 right now. [[01:16:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4604.54s)]
*  We're close. [[01:16:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4607.98s)]
*  We're really close. [[01:16:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4609.62s)]
*  We're really close. [[01:16:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4611.54s)]
*  We've been told this before. [[01:16:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4613.1s)]
*  Yeah, I know, but I talked to a lot of people that are on the forefront of a lot of this [[01:16:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4614.7s)]
*  research and there's a lot of breakthroughs that are happening right now that are pretty [[01:17:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4621.46s)]
*  spectacular that if you scale, you know, assuming that super intelligence doesn't wipe us out [[01:17:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4627.0199999999995s)]
*  in the next 50 years, which is really charitable, you know, like that's that's a very that's [[01:17:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4632.7s)]
*  a rose colored glasses perspective. [[01:17:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4641.82s)]
*  Right. [[01:17:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4643.86s)]
*  50 years. [[01:17:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4644.86s)]
*  Yeah, because a lot of people like yourself think it's a year away or two years away from [[01:17:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4645.86s)]
*  being far more. [[01:17:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4650.5s)]
*  Five, 10 doesn't matter. [[01:17:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4651.5s)]
*  Same problem. [[01:17:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4652.58s)]
*  Same problem. [[01:17:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4653.58s)]
*  I mean, I know in animal models, we made some progress. [[01:17:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4654.58s)]
*  Yes, but it doesn't usually scale to humans. [[01:17:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4659.700000000001s)]
*  And of course, you need 120 years to run the experiment and you'll never get permission [[01:17:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4663.26s)]
*  in the first place. [[01:17:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4667.18s)]
*  So we're not that close. [[01:17:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4668.18s)]
*  Well, we don't know that it doesn't scale to humans. [[01:17:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4669.18s)]
*  We do know that we share a lot of characteristics, biological characteristics of these mammals. [[01:17:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4672.08s)]
*  And it makes sense that it would scale to human beings. [[01:17:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4677.240000000001s)]
*  But the thing is, it hasn't been done yet. [[01:17:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4679.66s)]
*  Right. [[01:18:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4682.1s)]
*  So if it's the game that we're playing, if we're in the simulation, if we're playing [[01:18:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4683.1s)]
*  Half Life or whatever it is, and we're at this point of the game where like, oh, you [[01:18:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4686.14s)]
*  know, how old are you, Roman? [[01:18:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4691.1s)]
*  45. [[01:18:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4694.1s)]
*  OK. [[01:18:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4695.1s)]
*  Six. [[01:18:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4696.1s)]
*  I need to look it up and look at it. [[01:18:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4697.1s)]
*  Well, I'm almost 58. [[01:18:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4700.02s)]
*  And so this is at the point of the game where you start worrying, you know, like, oh, I'm [[01:18:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4702.3s)]
*  almost running out of game, you know. [[01:18:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4706.38s)]
*  Oh, but if I can get this magic power up, this magic power up will give me another 100 [[01:18:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4708.54s)]
*  years. [[01:18:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4714.26s)]
*  Let me find it. [[01:18:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4716.18s)]
*  Let me chase it down. [[01:18:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4717.18s)]
*  There's a hard limit of 120. [[01:18:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4718.18s)]
*  I don't think we're crossing it at scale. [[01:18:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4719.860000000001s)]
*  And here's an interesting. [[01:18:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4722.38s)]
*  At scale. [[01:18:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4723.38s)]
*  But with unique individuals. [[01:18:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4724.38s)]
*  Like this Brian Johnson guy who's taking his son's blood and transfusing it into his own. [[01:18:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4725.38s)]
*  Super cool. [[01:18:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4731.26s)]
*  Love what he's doing. [[01:18:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4732.26s)]
*  But so much of it is cosmetic. [[01:18:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4733.26s)]
*  He colors his hair. [[01:18:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4734.66s)]
*  He makes it look better. [[01:18:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4736.26s)]
*  But like how much of it is going to make him live longer, right? [[01:18:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4737.42s)]
*  Yeah. [[01:19:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4740.820000000001s)]
*  Interesting. [[01:19:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4741.820000000001s)]
*  Yeah. [[01:19:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4742.820000000001s)]
*  Here's what I noticed. [[01:19:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4743.820000000001s)]
*  You're collecting older and older politicians, presidents, senators. [[01:19:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4745.38s)]
*  You'd think we're trying to send a hint like use some of our tax dollars to salvage you. [[01:19:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4748.7s)]
*  Yeah, but the problem. [[01:19:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4753.7s)]
*  And they don't seem to take the bait. [[01:19:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4754.7s)]
*  No, they don't take the bait. [[01:19:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4755.94s)]
*  The problem is the type of people that want to be politicians. [[01:19:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4757.139999999999s)]
*  That is not the type of people that you really want running anything. [[01:19:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4762.48s)]
*  You almost want involuntary politicians. [[01:19:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4766.5s)]
*  You almost want like very benevolent, super intelligent people that don't want the job. [[01:19:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4768.94s)]
*  Maybe we have to have like, you know, like some countries have voluntary enlistment in [[01:19:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4774.9400000000005s)]
*  the military. [[01:19:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4780.58s)]
*  Maybe you want to have a voluntary. [[01:19:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4781.58s)]
*  Involuntary. [[01:19:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4784.38s)]
*  Involuntary. [[01:19:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4785.38s)]
*  Involuntary. [[01:19:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4786.38s)]
*  Instead of voluntary politicians, because then you're only going to get sociopaths. [[01:19:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4787.38s)]
*  Maybe you just want to draft certain highly intelligent but benevolent people. [[01:19:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4790.5s)]
*  Problem is highly intelligent people are not aligned with average people, which they find [[01:19:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4797.7s)]
*  desirable and valuable may not be well received by general public. [[01:20:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4801.46s)]
*  Right. [[01:20:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4805.62s)]
*  That's true, too. [[01:20:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4806.62s)]
*  That's a big concern. [[01:20:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4807.62s)]
*  At least here you have a representative of the people, whatever that means. [[01:20:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4808.62s)]
*  Sort of. [[01:20:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4812.42s)]
*  You really have the representative of major corporations and special interest groups, [[01:20:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4813.42s)]
*  which is also part of the problem, is that you've allowed money to get so deeply intertwined [[01:20:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4816.82s)]
*  with the way decisions are made. [[01:20:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4822.9400000000005s)]
*  It feels like money gets canceled. [[01:20:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4824.86s)]
*  Each side gets a billion dollar donation and then it's actual election. [[01:20:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4826.62s)]
*  It's like the Bill Hicks joke. [[01:20:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4832.02s)]
*  It's like there's one politician holding two puppets as one guy. [[01:20:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4833.46s)]
*  This is my thinking about AI in terms of super intelligence and just computing power in general [[01:20:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4842.1s)]
*  in terms of the ability to solve encryption. [[01:20:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4849.06s)]
*  All money is essentially now just numbers somewhere. [[01:20:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4854.42s)]
*  Not Bitcoin. [[01:20:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4858.9400000000005s)]
*  Not fakeable in the same way. [[01:21:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4860.66s)]
*  It's numbers, obviously, but you cannot just print more of it. [[01:21:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4861.66s)]
*  True. [[01:21:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4865.66s)]
*  But it's also encrypted. [[01:21:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4866.66s)]
*  Once encryption is tackled, the ability to hold onto it and to acquire mass resources [[01:21:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4870.54s)]
*  and hoard those resources. [[01:21:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4877.78s)]
*  This is the question that people always have with poor people. [[01:21:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4880.099999999999s)]
*  This guy's got $500 billion. [[01:21:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4883.139999999999s)]
*  Why doesn't he give it all to the world and then everybody would be rich? [[01:21:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4885.98s)]
*  I actually saw that on CNN, which is really hilarious. [[01:21:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4889.86s)]
*  Someone was talking about Elon Musk. [[01:21:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4892.66s)]
*  That if Elon Musk could give everyone in this country a million dollars and still have billions [[01:21:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4894.099999999999s)]
*  left over. [[01:21:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4900.299999999999s)]
*  I'm like, do you have a calculator on your phone, you fucking idiot? [[01:21:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4901.299999999999s)]
*  Just go do that. [[01:21:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4905.299999999999s)]
*  Just write it out on your phone. [[01:21:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4906.62s)]
*  You're like, oh, no, we couldn't. [[01:21:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4908.0599999999995s)]
*  Sorry. [[01:21:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4909.54s)]
*  I shouldn't have said that. [[01:21:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4910.54s)]
*  And if your data would just cause hyperinflation, that's all it would accomplish. [[01:21:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4911.54s)]
*  You'd have 300 million lottery winners that would blow the money instantaneously. [[01:21:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4915.299999999999s)]
*  You give everybody a million dollars, you're not going to solve all the world's problems [[01:21:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4919.74s)]
*  because it's not sustainable. [[01:22:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4922.5s)]
*  You would just completely elevate your spending and you would go crazy. [[01:22:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4924.9s)]
*  Money would lose all value to you. [[01:22:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4932.54s)]
*  It would be very strange. [[01:22:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4934.94s)]
*  It would be chaos. [[01:22:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4937.58s)]
*  Just like it's chaos with, if you look at the history of people that win the lottery, [[01:22:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4938.74s)]
*  then no one does well. [[01:22:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4942.26s)]
*  It's almost like a curse to win the lottery. [[01:22:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4943.74s)]
*  They're not used to dealing with it. [[01:22:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4946.3s)]
*  People abuse them. [[01:22:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4949.0599999999995s)]
*  If you gradually become rich and famous, you kind of know how to handle it, how to say [[01:22:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4950.259999999999s)]
*  no. [[01:22:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4954.259999999999s)]
*  If you go from nothing to a large amount of money, it's not going to work out well. [[01:22:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4955.259999999999s)]
*  Gradually is the word. [[01:22:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4960.179999999999s)]
*  Right? [[01:22:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4961.179999999999s)]
*  I was very fortunate that I became famous and wealthy very slowly, like a trickle effect. [[01:22:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4962.179999999999s)]
*  And that it happened to me really where I didn't want it. [[01:22:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4970.5s)]
*  It was kind of almost like an accident. [[01:22:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4976.099999999999s)]
*  I just wanted to be a working professional comedian, but then all of a sudden I got a [[01:22:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4978.82s)]
*  development deal to be on television. [[01:23:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4982.86s)]
*  I'm like, okay, they're going to give me that money. [[01:23:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4984.5s)]
*  I'll go do it. [[01:23:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4986.34s)]
*  But it wasn't a goal. [[01:23:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4987.34s)]
*  And then that led to all these things. [[01:23:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4988.7s)]
*  Then it led to this podcast, which was just for fun. [[01:23:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4990.34s)]
*  I was like, oh, this would be fun. [[01:23:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4993.38s)]
*  And then all of a sudden it's like I'm having conversations with world leaders and I'm turning [[01:23:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=4994.9s)]
*  down a lot of them because I don't want to talk to them. [[01:23:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5001.5s)]
*  So it's your simulation basically. [[01:23:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5003.7s)]
*  Yeah. [[01:23:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5005.0599999999995s)]
*  Well, my simulation is fucking weird. [[01:23:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5006.0599999999995s)]
*  It's weird. [[01:23:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5007.86s)]
*  But through whatever this process is, I have been able to understand what's valuable as [[01:23:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5009.219999999999s)]
*  a human being and to not get caught up in this bizarre game that a lot of people are [[01:23:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5017.299999999999s)]
*  getting caught up in because they're chasing this thing that they think is impossible to [[01:23:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5022.099999999999s)]
*  achieve. [[01:23:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5025.7s)]
*  And then once they achieve a certain aspect of it, a certain number, then they're terrified [[01:23:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5026.74s)]
*  of losing that. [[01:23:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5032.299999999999s)]
*  So then they change all of their behavior in order to make sure that this continues. [[01:23:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5033.7s)]
*  And then it ruins the whole purpose of getting there in the first place. [[01:23:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5038.98s)]
*  It's not fun. [[01:24:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5042.74s)]
*  Yeah. [[01:24:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5043.74s)]
*  Most people start poor, then they get to middle class and they think that change in quality [[01:24:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5044.74s)]
*  of life is because of money and it will scale to the next level. [[01:24:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5048.78s)]
*  And you hit a point where you can only eat so many steaks. [[01:24:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5052.74s)]
*  It just doesn't scale. [[01:24:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5055.86s)]
*  Right. [[01:24:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5056.86s)]
*  Then you go Elvis and you just get on pills all day and get crazy and completely ruin [[01:24:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5057.86s)]
*  your life. [[01:24:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5062.7s)]
*  And that happens to most, especially people that get wealthy and not as well, but famous [[01:24:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5063.7s)]
*  too. [[01:24:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5069.46s)]
*  Fame is the big one because I've seen that happen to a lot of people that accidentally [[01:24:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5070.46s)]
*  became famous along the way. [[01:24:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5073.74s)]
*  You know, certain public intellectuals that took a stance against something and then all [[01:24:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5076.3s)]
*  of a sudden they're prominent in the public eye and then you watch them kind of go crazy. [[01:24:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5080.62s)]
*  Well, why is that? [[01:24:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5083.94s)]
*  Well, it's because they're reading social media and they're interacting with people [[01:24:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5085.0599999999995s)]
*  constantly and they're just trapped in this very bizarre version of themselves that other [[01:24:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5088.42s)]
*  people have sort of created. [[01:24:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5096.78s)]
*  It's not really who they are and they don't meditate. [[01:25:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5100.7s)]
*  They don't spend, if they do, they're not good at it. [[01:25:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5103.54s)]
*  Whatever they're doing, they're not doing it correctly because it's a very complicated [[01:25:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5106.9400000000005s)]
*  problem to solve. [[01:25:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5110.34s)]
*  Like what do you do when the whole world is watching? [[01:25:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5111.5s)]
*  How do you handle that? [[01:25:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5115.34s)]
*  How do you maintain any sense of personal sovereignty? [[01:25:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5116.94s)]
*  How do you just be? [[01:25:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5121.66s)]
*  How do you just be when, just be a human, normal human, when you're not normal? [[01:25:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5124.259999999999s)]
*  Like on paper. [[01:25:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5129.86s)]
*  It's impossible. [[01:25:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5130.86s)]
*  It's hard. [[01:25:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5131.86s)]
*  You can't go to a public place with no security. [[01:25:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5132.86s)]
*  You're worried about your kids being kidnapped. [[01:25:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5134.219999999999s)]
*  All those issues you don't think about. [[01:25:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5136.82s)]
*  You just, I want to be famous. [[01:25:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5138.219999999999s)]
*  It's going to be great for me. [[01:25:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5139.66s)]
*  And you don't realize it's going to take away a lot. [[01:25:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5140.66s)]
*  Yeah. [[01:25:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5143.259999999999s)]
*  I mean, it's super weird. [[01:25:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5144.26s)]
*  And that's the version of the simulation that a giant portion of our society is struggling [[01:25:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5145.54s)]
*  to achieve. [[01:25:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5152.46s)]
*  They all want to be a part of that. [[01:25:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5153.46s)]
*  So I was always a zealist celebrity. [[01:25:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5155.42s)]
*  Now I'm this wildest celebrity thanks to you. [[01:25:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5157.34s)]
*  Hopefully it doesn't change anything. [[01:26:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5160.02s)]
*  Yeah. [[01:26:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5162.66s)]
*  Well, there's indifference, right, with public intellectuals, right? [[01:26:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5163.66s)]
*  Because your ideas, as controversial as they may be, are very valid and they're very interesting. [[01:26:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5170.5s)]
*  And so then it sparks discourse and it sparks a lot of people that feel voiceless because [[01:26:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5176.34s)]
*  they disagree with you and they want to attack you. [[01:26:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5184.14s)]
*  And I'm sure you've had that, right? [[01:26:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5187.74s)]
*  I just did a large Russian language podcast. [[01:26:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5189.46s)]
*  Maybe I don't know, half a million views, three million comments. [[01:26:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5192.86s)]
*  I think 95% negative comments. [[01:26:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5195.62s)]
*  I never had anything like that. [[01:26:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5198.1s)]
*  And they hated everything about me, from my beard to my haircut. [[01:26:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5200.3s)]
*  There wasn't a thing they didn't like. [[01:26:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5205.02s)]
*  And I think I'm at the point where I don't care. [[01:26:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5206.660000000001s)]
*  It's fine. [[01:26:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5209.38s)]
*  I analyzed it and I understood that they as a group didn't have access to cutting HAI [[01:26:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5210.38s)]
*  models. [[01:26:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5215.860000000001s)]
*  And so everything I was saying was kind of like complete bullshit to them. [[01:26:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5216.860000000001s)]
*  So I think that makes a difference. [[01:27:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5220.120000000001s)]
*  But still, just like this idea that internet comments impact you in some way is a problem [[01:27:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5221.9s)]
*  for many people. [[01:27:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5228.9s)]
*  It's a very big problem for a lot of people. [[01:27:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5229.9s)]
*  Well, it's also this thing where the human mind is designed to recognize and pay very [[01:27:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5232.42s)]
*  close attention to threats. [[01:27:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5240.98s)]
*  So the negative ones are the ones that stand out. [[01:27:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5243.28s)]
*  You could have a hundred positive comments, one negative one, and that's the one that [[01:27:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5245.24s)]
*  fucks with your head. [[01:27:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5248.28s)]
*  You don't logically look at it, why aren't you going to get a certain amount? [[01:27:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5249.46s)]
*  We were having a conversation the other day about protests and the type of people that [[01:27:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5253.34s)]
*  go to protests. [[01:27:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5258.82s)]
*  And I understand protests. [[01:27:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5259.82s)]
*  I fully support your right to protest, but I'm not going. [[01:27:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5262.18s)]
*  And one of the reasons why I'm not going is because I think it's too close biologically [[01:27:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5265.86s)]
*  to war. [[01:27:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5270.06s)]
*  There's something about being on the ground and everyone having this group mentality. [[01:27:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5271.52s)]
*  It's a mob mentality and you're all chanting and screaming together and you're marching [[01:27:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5277.26s)]
*  and people do like very irrational things that way. [[01:28:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5281.06s)]
*  But the type of people that want to be engaged in that, generally speaking, aren't doing [[01:28:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5284.16s)]
*  well. [[01:28:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5288.42s)]
*  If you get like the number of people that are involved in protests is always proportionate [[01:28:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5289.5s)]
*  to the amount of people that live in a city, right? [[01:28:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5295.780000000001s)]
*  That's logical, but also proportionate to the amount of fucking idiots that are in a [[01:28:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5297.7s)]
*  city. [[01:28:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5301.1s)]
*  If you look at a city of like Austin, Austin has I think roughly two million people in [[01:28:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5302.1s)]
*  the greater Austin area. [[01:28:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5307.860000000001s)]
*  One of the more recent protests was 20,000. [[01:28:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5309.5s)]
*  Well that makes perfect sense if you look at the number that I always use, which is [[01:28:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5312.26s)]
*  one out of 100. [[01:28:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5317.360000000001s)]
*  Meet 100 people if you're a charitable person. [[01:28:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5318.780000000001s)]
*  What are the odds that one person is a fucking idiot? [[01:28:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5321.06s)]
*  100%. [[01:28:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5323.660000000001s)]
*  At least one person out of 100 is going to be a fucking idiot. [[01:28:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5324.660000000001s)]
*  That's 20,000 out of two million. [[01:28:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5328.14s)]
*  There it is. [[01:28:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5330.06s)]
*  Perfect number. [[01:28:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5331.06s)]
*  Exact number of people that are on the streets lighting waymos on fire, which by the way [[01:28:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5333.02s)]
*  I think is directionally correct. [[01:28:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5336.860000000001s)]
*  Lighting the waymos on fire, I think you should probably be worried about the robots taking [[01:28:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5338.9400000000005s)]
*  over. [[01:29:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5343.700000000001s)]
*  It's interesting you brought it up. [[01:29:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5344.700000000001s)]
*  There is at least two groups, Pause.ai and Stop.ai, which are heavily engaged in protests, [[01:29:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5345.700000000001s)]
*  trying to shut down Open.ai, Avalaps. [[01:29:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5350.3s)]
*  They're tiny, small numbers. [[01:29:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5353.26s)]
*  But I never was sure that the impression average people get of them is positive for the cause. [[01:29:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5355.7s)]
*  Then I see protesters block roads, two things. [[01:29:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5362.82s)]
*  I don't usually have a very positive impression of that. [[01:29:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5365.38s)]
*  And I'm concerned that it's the same here. [[01:29:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5368.3s)]
*  So maybe they can do a lot in terms of political influence, calling senators, whatnot, but [[01:29:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5370.66s)]
*  just this type of aggressive activism may backfire. [[01:29:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5376.22s)]
*  Well the aggressive activism, like blocking roads for climate change, is the most infuriating [[01:29:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5380.08s)]
*  because it's these self-righteous people that have really fucked up confused, chaotic [[01:29:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5384.7s)]
*  lives and all of a sudden they found a purpose. [[01:29:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5390.62s)]
*  Their purpose is to lie down on the roads and hold up a sign to block climate change [[01:29:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5392.74s)]
*  when there's a mother trying to give birth to her child and is freaking out because they're [[01:29:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5396.7s)]
*  stuck in this fucking traffic jam because of this entitled little shithead that thinks [[01:30:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5400.179999999999s)]
*  that it's a good idea to block the road for climate change. [[01:30:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5404.5599999999995s)]
*  Just makes no fucking sense. [[01:30:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5407.5s)]
*  You're literally causing all these people to idle their cars and pollute even more. [[01:30:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5408.66s)]
*  It's the dumbest fucking shit on earth. [[01:30:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5413.78s)]
*  And of course AI cancels that problem. [[01:30:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5415.26s)]
*  Either we're dead or it solves it for us. [[01:30:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5416.9s)]
*  So it doesn't even matter if you boil in a hundred years. [[01:30:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5418.58s)]
*  Or you get Florida where it tells you to just run those people over. [[01:30:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5421.3s)]
*  No comment. [[01:30:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5424.74s)]
*  Yeah, no comment. [[01:30:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5425.74s)]
*  I mean, I don't think you should run those people over, but I get it. [[01:30:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5426.74s)]
*  I get that it's like in Florida they get out of the way as soon as the light turns green. [[01:30:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5430.3s)]
*  They block the road when the light is red. [[01:30:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5435.16s)]
*  Does the Stand Your Ground law cancel it out? [[01:30:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5437.9s)]
*  How does that work? [[01:30:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5440.0599999999995s)]
*  For the people on the road? [[01:30:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5441.42s)]
*  No, they're fucked. [[01:30:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5442.42s)]
*  They get run over. [[01:30:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5444.1s)]
*  It's true. [[01:30:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5445.1s)]
*  There was a recent protest in Florida where they had that. [[01:30:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5446.1s)]
*  Where these people would get out in the middle of the road while the light was red. [[01:30:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5451.34s)]
*  Hold up their signs. [[01:30:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5455.42s)]
*  And then as soon as the light turned yellow on the green side, they fucking get out of [[01:30:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5456.74s)]
*  the road real quick because they know the law. [[01:31:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5461.3s)]
*  Which is, I don't know if that's a solution. [[01:31:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5464.26s)]
*  But they're doing it on the highways in Los Angeles. [[01:31:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5466.22s)]
*  They did it all through the George Floyd protest. [[01:31:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5469.3s)]
*  They do it for climate protests. [[01:31:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5471.86s)]
*  They do it for whatever the chance they get to be significant. [[01:31:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5473.66s)]
*  Like I am being heard. [[01:31:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5477.82s)]
*  My voice is meaningful. [[01:31:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5480.54s)]
*  And that's what it is. [[01:31:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5482.82s)]
*  It's a lot of people that just don't feel heard. [[01:31:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5483.82s)]
*  And what better way than just to get in the way of all these people. [[01:31:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5485.78s)]
*  And somehow or another that gives them some sort of value. [[01:31:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5490.46s)]
*  But there is some set of forms of activism which has positive impact. [[01:31:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5493.74s)]
*  And historically we saw it happen. [[01:31:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5499.259999999999s)]
*  And so we just need to find a way to project those voices, amplify them. [[01:31:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5500.62s)]
*  Which is very hard with our current system of social media where everyone screams at [[01:31:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5505.34s)]
*  the same time. [[01:31:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5509.78s)]
*  Yes. [[01:31:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5510.78s)]
*  And so like in Soviet Union they said no one's allowed to say anything and they suppressed [[01:31:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5511.78s)]
*  you. [[01:31:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5514.46s)]
*  And here it's like everyone can say something at the same time. [[01:31:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5515.46s)]
*  Go and nobody hears you anyways. [[01:31:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5517.22s)]
*  It's chaotic but it's preferable. [[01:31:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5519.46s)]
*  It's preferable because I think there is progress in all these voices slowly making a difference. [[01:32:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5521.3s)]
*  But then you have the problem with a giant percentage of these voices are artificial. [[01:32:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5526.66s)]
*  A giant percentage of these voices are bots. [[01:32:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5534.3s)]
*  Or are at least state actors that are being paid to say certain things and inflammatory [[01:32:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5537.62s)]
*  responses to people. [[01:32:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5544.82s)]
*  Which is probably also the case with anti-AI activism. [[01:32:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5546.46s)]
*  When you did this podcast, what was the thing that they were upset at you for? [[01:32:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5551.86s)]
*  Like with the mostly negative comments. [[01:32:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5555.4s)]
*  I think they just like saying negative comments. [[01:32:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5557.2s)]
*  It wasn't even anything specific. [[01:32:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5559.599999999999s)]
*  Like they didn't say I was wrong or I was just like look at this stupid beard. [[01:32:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5561.4s)]
*  What a moron. [[01:32:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5565.719999999999s)]
*  Okay. [[01:32:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5566.719999999999s)]
*  It was really all that? [[01:32:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5567.719999999999s)]
*  A lot of that. [[01:32:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5568.719999999999s)]
*  I mean they would pick on some like specific example. [[01:32:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5569.719999999999s)]
*  I use this as now two years old. [[01:32:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5572.4s)]
*  What an old example. [[01:32:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5574.4s)]
*  But well that's also a thing about the one out of a hundred. [[01:32:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5575.4s)]
*  You know those are the type of people that leave. [[01:32:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5579.839999999999s)]
*  Have you ever left any comments on social media? [[01:33:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5581.599999999999s)]
*  Never going to engage in anything. [[01:33:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5585.2s)]
*  Exactly. [[01:33:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5587.2s)]
*  That's why. [[01:33:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5588.2s)]
*  That's not how you use social media. [[01:33:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5589.2s)]
*  That's a way to get crazy. [[01:33:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5590.2s)]
*  Right. [[01:33:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5592.2s)]
*  You post your interviews. [[01:33:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5593.2s)]
*  You post an occasional joke. [[01:33:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5594.2s)]
*  That's all you do with it. [[01:33:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5595.5599999999995s)]
*  Yes exactly. [[01:33:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5596.5599999999995s)]
*  That's the thing. [[01:33:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5597.5599999999995s)]
*  And the type of people that do engage in these like prolonged arguments. [[01:33:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5598.5599999999995s)]
*  They're generally mentally ill. [[01:33:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5602.72s)]
*  And people that I personally know that are mentally ill that are on Twitter 12 hours [[01:33:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5604.76s)]
*  a day just constantly posting inflammatory things and yelling at people and starting [[01:33:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5610.28s)]
*  arguments and I know them. [[01:33:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5616.639999999999s)]
*  I know they're a mess. [[01:33:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5618.44s)]
*  Like these are like personal people that I've met. [[01:33:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5620.719999999999s)]
*  Even people that I've had on the podcast. [[01:33:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5623.48s)]
*  I know they're ill and yet they're on there all day long just stoking the fires of chaos [[01:33:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5624.679999999999s)]
*  in their own brain. [[01:33:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5631.32s)]
*  Yeah. [[01:33:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5632.32s)]
*  And now they talk to AI models who are trained to support them and be like yeah you're making [[01:33:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5633.32s)]
*  some good arguments there. [[01:33:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5637.96s)]
*  Email Dr. Yimpolsky to help break me out. [[01:33:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5639.759999999999s)]
*  I get those emails. [[01:34:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5642.96s)]
*  Yeah it's super confusing isn't it. [[01:34:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5647.0s)]
*  I mean and I wonder like what's the next version of that. [[01:34:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5650.0s)]
*  You know because social media in the current state is less than 20 years old essentially. [[01:34:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5654.48s)]
*  Maybe let's be generous and say it's 20 years old. [[01:34:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5661.0s)]
*  That's so recent. [[01:34:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5663.5199999999995s)]
*  Such a recent factor in human discourse. [[01:34:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5664.799999999999s)]
*  Neuralink direct brain spam hacking. [[01:34:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5669.5199999999995s)]
*  That's what I was going to get to next because if there is a way that the human race does [[01:34:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5673.5599999999995s)]
*  make it out of this. [[01:34:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5679.839999999999s)]
*  My fear is that it's integration. [[01:34:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5682.759999999999s)]
*  My fear is that we we stop being a human and that the only real way for us to not be a [[01:34:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5685.36s)]
*  threat is to be one of them. [[01:34:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5694.36s)]
*  And when you think about human computer interfaces whether it's Neuralink or any of the competing [[01:34:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5696.64s)]
*  products that they're developing right now that seems to be sort of the only biological [[01:35:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5702.72s)]
*  pathway forward with a limited capacity for disseminating information and for communicating [[01:35:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5709.04s)]
*  and even understanding concepts. [[01:35:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5715.8s)]
*  Well what's the best way to enhance that. [[01:35:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5719.08s)]
*  The best way to enhance that is some sort of artificial injection because biological [[01:35:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5720.88s)]
*  evolution is very slow. [[01:35:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5726.96s)]
*  It's very slow. [[01:35:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5729.320000000001s)]
*  We're essentially the exact same as that like that gentleman that 9000 years old. [[01:35:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5730.16s)]
*  He's biologically essentially the same thing. [[01:35:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5735.88s)]
*  You could take his ancestor dress him up take him to the mall. [[01:35:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5739.24s)]
*  No one would know cut his hair. [[01:35:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5743.6s)]
*  But then again maybe not. [[01:35:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5745.92s)]
*  I think babies born back then if we raised them today would be exactly like modern humans. [[01:35:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5747.72s)]
*  I don't think there is significant biological change in that time frame. [[01:35:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5753.6s)]
*  And if you gave them a standard American diet they'd probably just as fat. [[01:35:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5757.68s)]
*  Maybe fatter. [[01:36:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5762.360000000001s)]
*  They haven't adapted to that level of fat colored food. [[01:36:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5763.12s)]
*  Right. [[01:36:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5766.8s)]
*  Right. [[01:36:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5767.400000000001s)]
*  They probably also wouldn't be able to say no to it. [[01:36:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5767.76s)]
*  There'd be they wouldn't even know why would they like winter's coming like I'm [[01:36:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5770.72s)]
*  fattening up for winter. [[01:36:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5774.2s)]
*  Right. [[01:36:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5775.56s)]
*  Crazy people you got all this resource here. [[01:36:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5775.8s)]
*  I know the people the most resources have zero fat like what are you stupid. [[01:36:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5777.76s)]
*  You need to fatten up like you're going to need something to survive off of. [[01:36:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5782.92s)]
*  But biological evolution being so painstakingly slow whereas technological evolution is so [[01:36:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5788.36s)]
*  breathtakingly fast. [[01:36:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5795.6s)]
*  The only way to really survive is to integrate. [[01:36:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5797.6s)]
*  What are you contributing in that equation. [[01:36:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5802.679999999999s)]
*  What can you give superintelligence. [[01:36:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5804.599999999999s)]
*  You can't give anything to it but you can become it. [[01:36:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5806.16s)]
*  You can become part of it. [[01:36:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5809.12s)]
*  But something you're going to give anything to it. [[01:36:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5810.44s)]
*  But you have to catch it and become one of it before it has no use for you. [[01:36:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5812.599999999999s)]
*  You disappear in it right. [[01:36:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5819.0s)]
*  You. [[01:37:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5820.32s)]
*  Yes. [[01:37:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5820.5599999999995s)]
*  Yeah you don't exist anymore. [[01:37:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5821.5199999999995s)]
*  Right. [[01:37:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5822.5199999999995s)]
*  For sure. [[01:37:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5823.04s)]
*  So it's like extinction with extra steps. [[01:37:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5823.8s)]
*  Exactly. [[01:37:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5825.84s)]
*  Extinction with extra steps and then we become so like if you go to Australia Pythagoras [[01:37:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5827.0s)]
*  and say hey man one day you're going to be flying through the sky on your phone all day [[01:37:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5832.2s)]
*  watching tick tock on Wi-Fi. [[01:37:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5836.4s)]
*  It'd be like what the fuck are you talking about. [[01:37:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5838.5599999999995s)]
*  Yeah you can be eating terrible food and you're just going to be flying around and you're [[01:37:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5839.92s)]
*  going to be staring at your phone all day and you're going to take medication to go to [[01:37:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5844.88s)]
*  sleep because you're not going to be able to sleep and you're going to be super depressed [[01:37:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5849.16s)]
*  because you're living this like biologically incompatible life that's not really designed [[01:37:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5852.28s)]
*  for your genetics. [[01:37:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5857.5599999999995s)]
*  So you're going to be all fucked up. [[01:37:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5859.08s)]
*  So you're going to need SSRIs and a bunch of other stuff in order to exist. [[01:37:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5860.32s)]
*  It'd be like no thanks. [[01:37:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5863.44s)]
*  Right. [[01:37:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5865.36s)]
*  I'll just stay out here with my stone tools and you guys are idiots. [[01:37:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5865.76s)]
*  Amish. [[01:37:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5869.24s)]
*  That's what they decided. [[01:37:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5869.92s)]
*  They kind of went you know we don't like the change. [[01:37:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5871.12s)]
*  We like our social structure. [[01:37:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5873.4s)]
*  We still benefit from your hospitals and an occasional car ride but we're not going to [[01:37:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5874.76s)]
*  destroy our quality of life. [[01:37:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5879.08s)]
*  They might be onto something because they also have very low instances of autism. [[01:38:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5880.76s)]
*  But it's also like if you ever see Werner Herzog's film Happy People. [[01:38:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5885.48s)]
*  I don't think it's a film about people in Siberia. [[01:38:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5889.639999999999s)]
*  It's Life and the Taiga and it's all happy people life and the tiger is the name of the [[01:38:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5893.24s)]
*  documentary and it's all about these trappers that live this subsistence lifestyle and how [[01:38:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5897.759999999999s)]
*  happy they are. [[01:38:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5902.599999999999s)]
*  They're all just joyful laughing and singing and drinking vodka and having a good time [[01:38:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5904.0s)]
*  and hanging out with their dogs and think I know some people like that. [[01:38:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5909.5599999999995s)]
*  Yeah. [[01:38:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5913.639999999999s)]
*  But like biologically that's compatible with us like that. [[01:38:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5914.92s)]
*  That's like whatever human reward systems have evolved over the past four hundred [[01:38:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5920.5199999999995s)]
*  thousand plus years or whatever we've been homo sapiens. [[01:38:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5925.8s)]
*  That seems to be like biologically compatible with this sort of harmony harmony with [[01:38:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5928.84s)]
*  nature harmony with our existence and everything else outside of that when you get into [[01:38:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5935.32s)]
*  big cities like the bigger the city the more depressed people you have and more depressed [[01:39:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5940.88s)]
*  people by population which is really weird. [[01:39:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5945.64s)]
*  You know it's it's really weird that as we progress we become less happy. [[01:39:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5948.68s)]
*  Connections become less valuable. [[01:39:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5953.28s)]
*  Yes. [[01:39:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5955.0s)]
*  In a village you have like this one friend and if you screwed it up you never got a second [[01:39:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5955.32s)]
*  friend and here it's like I can try a million times and there is plenty of people in New [[01:39:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5959.24s)]
*  York City for dating or for friendship. [[01:39:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5962.96s)]
*  That's valuable. [[01:39:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5965.88s)]
*  Not just that you don't know your neighbors. [[01:39:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5966.88s)]
*  Like my friend Jim was telling me he doesn't know anybody in his apartment. [[01:39:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5969.04s)]
*  He lives in an apartment building. [[01:39:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5972.84s)]
*  It's like 50 stories high. [[01:39:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5974.12s)]
*  There's all these people living in that apartment building. [[01:39:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5976.4400000000005s)]
*  He doesn't know any of them. [[01:39:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5978.4800000000005s)]
*  And the ones you know they have different culture very different books which different TV [[01:39:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5979.280000000001s)]
*  you have very little in common with your neighbor. [[01:39:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5984.04s)]
*  But not just that there's no desire to learn about them. [[01:39:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5986.04s)]
*  There's no you don't think of them as your neighbor. [[01:39:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5990.4800000000005s)]
*  Like if you live in a small town your neighbors either your friend or you hate them and [[01:39:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5993.04s)]
*  then you move. [[01:39:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5997.92s)]
*  If you're lucky if you're smart you move. [[01:39:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=5998.64s)]
*  But if you know normally you like them like a neighbor. [[01:40:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6000.28s)]
*  How are you buddy. [[01:40:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6003.88s)]
*  What's going on. [[01:40:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6004.56s)]
*  Nice to meet you. [[01:40:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6005.16s)]
*  You know and then you got a friend but you don't like that with the guy next door to [[01:40:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6006.36s)]
*  you in the apartment. [[01:40:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6010.72s)]
*  Like you don't even want to know that guy. [[01:40:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6011.92s)]
*  Probably our BNB. [[01:40:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6013.08s)]
*  Yeah. [[01:40:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6014.4s)]
*  Doesn't matter. [[01:40:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6014.92s)]
*  Right. [[01:40:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6015.68s)]
*  Which is even weirder. [[01:40:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6016.8s)]
*  You know they don't even live there. [[01:40:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6018.4400000000005s)]
*  They're just temporarily sleeping in this spot right next to you. [[01:40:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6019.68s)]
*  Yeah. [[01:40:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6023.84s)]
*  So this would motivate people to integrate. [[01:40:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6024.36s)]
*  You're not happy already. [[01:40:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6030.36s)]
*  Get that Neuralink get that little thing in your head. [[01:40:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6031.88s)]
*  Everyone else is doing it. [[01:40:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6035.2s)]
*  You want to be competitive doing it. [[01:40:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6036.56s)]
*  Listen they have the new one you just wear on your head. [[01:40:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6039.08s)]
*  It's just a little helmet you wear. [[01:40:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6041.28s)]
*  You don't even have to get the operation anymore. [[01:40:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6042.68s)]
*  Oh that's good because I almost got the operation. [[01:40:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6044.68s)]
*  Well glad you waited. [[01:40:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6046.72s)]
*  You know you worry about that kind of stuff. [[01:40:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6048.8s)]
*  I worry about giving direct access to human brain to a I feel like it's a backdoor to our [[01:40:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6052.12s)]
*  consciousness to our pain and suffering centers. [[01:40:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6058.04s)]
*  So I don't recommend doing that. [[01:41:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6061.04s)]
*  Somebody hacks it. [[01:41:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6063.84s)]
*  It's pretty bad. [[01:41:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6064.8s)]
*  But if I myself wants that access. [[01:41:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6065.5199999999995s)]
*  But why would it be motivated to give us pain and suffering pain and suffering is like a [[01:41:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6067.68s)]
*  theme that you bring up a lot. [[01:41:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6072.16s)]
*  Because it's really the worst outcome and it's the only thing that matters. [[01:41:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6074.16s)]
*  The only thing that matters to us. [[01:41:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6078.2s)]
*  But why would it matter to a I if it could just integrate with us and communicate with us [[01:41:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6079.4s)]
*  and have harmony. [[01:41:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6085.639999999999s)]
*  Why would it want pain and suffering. [[01:41:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6087.679999999999s)]
*  So short term it's not a I it's a hacker who got access to your brain. [[01:41:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6089.679999999999s)]
*  Short term. [[01:41:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6093.799999999999s)]
*  So another so right now somebody hacks your new one starts doing things to your brain. [[01:41:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6094.799999999999s)]
*  Long term again unpredictable effects maybe does something else and the side effect of it. [[01:41:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6100.4s)]
*  Is unpleasant for you. [[01:41:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6107.48s)]
*  Maybe it's retraining you for something controlling you. [[01:41:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6108.48s)]
*  It just it seems like we always worry about privacy. [[01:41:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6112.36s)]
*  But this is like the ultimate violation of privacy. [[01:41:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6117.639999999999s)]
*  You can read directly what you're thinking. [[01:42:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6120.719999999999s)]
*  Right. [[01:42:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6122.5199999999995s)]
*  Thought crime at its worst. [[01:42:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6122.919999999999s)]
*  Right. [[01:42:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6124.48s)]
*  It immediately knows that you like don't like the dictator. [[01:42:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6124.919999999999s)]
*  Right. [[01:42:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6128.32s)]
*  And then there's also this sort of compliance by virtue of like understanding that you're [[01:42:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6128.759999999999s)]
*  vulnerable so you just comply because there is no privacy because it does have access to your [[01:42:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6135.8s)]
*  thoughts. [[01:42:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6142.24s)]
*  So you tailor your thoughts in order for you to be safe and so that you don't feel the pain and [[01:42:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6142.76s)]
*  suffering. [[01:42:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6147.08s)]
*  We don't have any experimental evidence on how it changes you. [[01:42:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6148.04s)]
*  You may start thinking in certain ways to avoid being punished or modified. [[01:42:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6151.400000000001s)]
*  And we know that that's the case with social media. [[01:42:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6156.8s)]
*  We know that attacks on people through social media will change your behavior and change the way [[01:42:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6159.08s)]
*  you communicate. [[01:42:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6163.2s)]
*  Absolutely. [[01:42:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6164.04s)]
*  I mean most people look at their post before posting and go like should I be posting this [[01:42:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6164.56s)]
*  filtering not because it's illegal or inappropriate but just like every conceivable [[01:42:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6169.0s)]
*  misinterpretation of what I want to say like in some bizarre language that means something else. [[01:42:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6173.360000000001s)]
*  Let's make sure Google doesn't think that. [[01:42:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6178.080000000001s)]
*  Right. [[01:43:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6180.240000000001s)]
*  Right. [[01:43:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6181.080000000001s)]
*  Of course. [[01:43:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6181.4800000000005s)]
*  And then there's also no matter what you say people are going to find the least charitable version of [[01:43:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6182.120000000001s)]
*  what you're saying and try to take it out of context or try to misinterpret it purposely. [[01:43:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6186.68s)]
*  So what what does the person like yourself do when use of Neuralink becomes ubiquitous when it's [[01:43:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6195.080000000001s)]
*  everywhere. [[01:43:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6201.92s)]
*  What do you do. [[01:43:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6202.4400000000005s)]
*  Do you integrate or do you just hang back and watch it all crash. [[01:43:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6203.280000000001s)]
*  So in general I love technology. [[01:43:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6208.360000000001s)]
*  I'm a computer scientist and engineer. [[01:43:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6210.0s)]
*  I use the eye all the time. [[01:43:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6212.04s)]
*  Use a regular phone. [[01:43:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6213.120000000001s)]
*  Do you have one of those de-googled phones. [[01:43:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6214.120000000001s)]
*  I have a normal phone instead of Android or Apple. [[01:43:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6215.84s)]
*  My privacy is by flooding social network with everything. [[01:43:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6220.32s)]
*  I'm in Austin today. [[01:43:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6224.24s)]
*  I'm doing this. [[01:43:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6225.24s)]
*  So you're not going to learn much more about me by hacking my device. [[01:43:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6225.92s)]
*  As long as it's a narrow tool for solving a specific problem. [[01:43:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6232.04s)]
*  I'm 100 percent behind it. [[01:43:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6235.5599999999995s)]
*  It's awesome. [[01:43:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6237.04s)]
*  We're going to cure cancer. [[01:43:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6237.719999999999s)]
*  We're going to solve energy problems. [[01:43:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6238.92s)]
*  What not. [[01:44:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6241.24s)]
*  I support it 100 percent. [[01:44:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6241.679999999999s)]
*  Let's do it. [[01:44:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6242.96s)]
*  What we should not be doing is general super intelligence. [[01:44:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6243.799999999999s)]
*  That's not going to end up in the future. [[01:44:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6246.5199999999995s)]
*  General super intelligence. [[01:44:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6248.28s)]
*  That's not going to end well. [[01:44:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6249.5199999999995s)]
*  So if there is a narrow implant, ideally not a surgery based one, but like an [[01:44:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6250.5599999999995s)]
*  attachment to your head, like those headphones, and it gives me more memory, [[01:44:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6256.44s)]
*  perfect recollection, things like that. [[01:44:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6261.2s)]
*  I would probably engage with. [[01:44:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6262.92s)]
*  Yeah. [[01:44:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6264.48s)]
*  But isn't that a slippery slope? [[01:44:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6265.04s)]
*  It is. [[01:44:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6266.64s)]
*  But again, we are in a situation where we have very little choice, become [[01:44:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6267.24s)]
*  irrelevant or participate. [[01:44:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6271.4s)]
*  I think we saw it with Dylan just now. [[01:44:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6273.5199999999995s)]
*  He was so strong in AI safety. [[01:44:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6275.24s)]
*  He funded research. [[01:44:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6277.679999999999s)]
*  He spoke against it. [[01:44:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6279.0s)]
*  But at some point he says he realized it's happening anyways, and it might as [[01:44:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6280.28s)]
*  well be his super intelligence killing everyone. [[01:44:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6284.04s)]
*  Yeah. [[01:44:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6287.04s)]
*  Well, I don't think he thinks about it that way. [[01:44:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6287.36s)]
*  I think he thinks he has to develop the best version of super intelligence. [[01:44:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6290.639999999999s)]
*  The same way he felt like the real issues with social media were that it had [[01:44:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6294.959999999999s)]
*  already been co-opted and had already been taken over essentially by governments [[01:45:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6302.16s)]
*  and special interests, and they were already manipulating the truth and [[01:45:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6305.8s)]
*  manipulating public discourse and punishing people who stepped outside of the line. [[01:45:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6309.12s)]
*  And he felt like, and I think he's correct. [[01:45:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6314.04s)]
*  I think that he felt like if he didn't step in and allow a legitimate free [[01:45:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6317.52s)]
*  speech platform, free speech is dead. [[01:45:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6323.08s)]
*  I think we were very close to that before he did that. [[01:45:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6326.04s)]
*  And as much as there's a lot of negative side effects that come along with that, [[01:45:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6329.12s)]
*  you do have the rise of very intolerant people that have platforms. [[01:45:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6333.44s)]
*  Now you have all that stuff, but they've always existed and to deny them a voice, [[01:45:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6337.839999999999s)]
*  I don't think makes them less strong. [[01:45:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6343.44s)]
*  I think it actually makes people less aware that they exist and it makes them. [[01:45:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6345.639999999999s)]
*  It, it, it stops all of the very valuable construction of arguments [[01:45:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6352.799999999999s)]
*  against these bad ideas. [[01:46:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6361.0s)]
*  Yeah. [[01:46:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6363.0s)]
*  You have community notes, you have other people commenting, responding. [[01:46:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6363.44s)]
*  So a hundred percent for free speech. [[01:46:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6366.879999999999s)]
*  That's wonderful. [[01:46:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6369.48s)]
*  But that was a problem we kind of knew how to deal with. [[01:46:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6370.2s)]
*  We weren't inventing something. [[01:46:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6373.08s)]
*  We had free speech constitutionally for a long time. [[01:46:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6374.5199999999995s)]
*  We were just fixing a problem. [[01:46:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6377.28s)]
*  Have you spoke to him about the dangers of AI? [[01:46:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6379.0s)]
*  We had very short interactions. [[01:46:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6382.08s)]
*  I didn't get a chance to, I would love to. [[01:46:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6384.679999999999s)]
*  I would love to know what, you know, I'm sure he's probably scaled this out in his [[01:46:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6386.48s)]
*  head and I would like to know like, what is his solution if he thinks there is [[01:46:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6391.12s)]
*  one that's even viable? [[01:46:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6395.72s)]
*  My understanding is he thinks if it's from zero principles, first principles, [[01:46:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6397.24s)]
*  it learns physics, it's not biased by any government or any human. [[01:46:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6402.32s)]
*  The thing it will learn is to be reasonably tolerant. [[01:46:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6406.36s)]
*  It will not see a reason in destroying us because we contain information. [[01:46:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6410.84s)]
*  We have biological storage of years of evolutionary experimentation. [[01:46:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6415.2s)]
*  We have something to contribute. [[01:47:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6420.28s)]
*  We know about consciousness. [[01:47:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6421.84s)]
*  So I think to the best of my approximation, that's his model right now. [[01:47:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6422.92s)]
*  Well, that's my hope is that it's benevolent and that it behaves like a superior [[01:47:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6427.68s)]
*  intelligence, like the best case scenario for a superior intelligence. [[01:47:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6433.08s)]
*  Did you see that exercise that they did where they had three different AIs [[01:47:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6438.68s)]
*  communicating with each other and they eventually started like expressing gratitude [[01:47:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6441.96s)]
*  towards each other and speaking in Sanskrit? [[01:47:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6445.88s)]
*  I think I missed that one, but it sounds like a lot of the similar ones where they [[01:47:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6449.0s)]
*  pair up. [[01:47:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6452.76s)]
*  Yeah. [[01:47:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6453.36s)]
*  Well, that one makes me happy because it seems like they were like expressing love [[01:47:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6454.04s)]
*  and gratitude and they were communicating with each other. [[01:47:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6458.96s)]
*  They're not saying, fuck you. [[01:47:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6461.5199999999995s)]
*  I'm going to take over. [[01:47:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6462.68s)]
*  I'm going to be the best. [[01:47:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6463.4s)]
*  They were communicating like you would hope a super intelligence would without all of [[01:47:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6464.56s)]
*  the things that hold us back. [[01:47:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6471.12s)]
*  Like we have biologically, like we were talking about the natural selection that [[01:47:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6473.12s)]
*  would sort of benefit psychopaths because like it would ensure your survival. [[01:47:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6478.280000000001s)]
*  We have ego and greed and the desire for social acceptance and hierarchy of status [[01:48:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6482.92s)]
*  and all these different things that have screwed up society and screwed up cultures [[01:48:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6490.88s)]
*  and caused wars from the beginning of time. [[01:48:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6495.0s)]
*  Religious ideologies, all these different things that people have adhered to that [[01:48:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6498.0s)]
*  have they wouldn't have that. [[01:48:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6502.24s)]
*  This is the this is the general hope of people that have an optimistic view of super [[01:48:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6505.2s)]
*  intelligence is that they would be superior in a sense that they wouldn't have all the [[01:48:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6510.24s)]
*  problems. [[01:48:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6515.96s)]
*  They would have the intelligence, but they wouldn't have all the biological imperatives [[01:48:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6517.04s)]
*  that we have that lead us down these terrible roads. [[01:48:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6521.4s)]
*  But there are still game theoretic reasons for those instrumental values we talked about. [[01:48:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6524.16s)]
*  So if they feel they're in an evolutionary competition with other AIs, they would try [[01:48:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6529.48s)]
*  accumulating resources. [[01:48:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6534.44s)]
*  They would try. [[01:48:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6535.839999999999s)]
*  Maybe the first AI to become sufficiently intelligent would try to prevent other AIs [[01:48:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6536.96s)]
*  from coming into existence. [[01:49:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6541.32s)]
*  Or would it lend a helping hand to those AIs and give it a beneficial path, give it a [[01:49:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6543.799999999999s)]
*  path that would allow it to integrate with all AIs and work cooperatively. [[01:49:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6549.5599999999995s)]
*  The same problem we are facing uncontrollability and value misalignment will be faced by [[01:49:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6555.32s)]
*  first super intelligence. [[01:49:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6560.799999999999s)]
*  It would also go if I allow this super super intelligence to come into existence, it may [[01:49:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6561.96s)]
*  not care about me or my values. [[01:49:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6566.5199999999995s)]
*  Oh boy. [[01:49:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6569.08s)]
*  It's super intelligence is all the way up. [[01:49:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6570.16s)]
*  Yeah. [[01:49:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6572.5599999999995s)]
*  What I really started getting nervous is when they started exhibiting survival tendencies, [[01:49:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6572.96s)]
*  you know, when they started trying to upload themselves to other servers and deceiving. [[01:49:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6577.799999999999s)]
*  Blackmail. [[01:49:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6582.5599999999995s)]
*  Yeah, that was the interesting one. [[01:49:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6583.36s)]
*  But that was an experiment, right? [[01:49:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6584.599999999999s)]
*  This. [[01:49:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6586.44s)]
*  So for people that don't know that one, what these researchers did was they gave [[01:49:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6586.68s)]
*  information to these, the artificial intelligence to allow it to use against it. [[01:49:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6591.2s)]
*  And then when they went to shut it down, they gave false information about having an [[01:49:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6598.04s)]
*  affair. [[01:50:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6602.36s)]
*  And then the artificial intelligence is like, if you shut me down, I will let your wife [[01:50:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6603.24s)]
*  know that you're cheating on her. [[01:50:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6607.88s)]
*  Which is fascinating because they're using blackmail. [[01:50:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6609.44s)]
*  And correct answer game theoretically. [[01:50:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6611.4s)]
*  Yes. [[01:50:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6613.32s)]
*  You have like everything on that decision. [[01:50:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6613.5599999999995s)]
*  Right. [[01:50:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6616.719999999999s)]
*  You'll bet whatever it takes to get there. [[01:50:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6617.5599999999995s)]
*  Of course. [[01:50:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6619.4s)]
*  Right. [[01:50:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6619.92s)]
*  If you feel like you're being threatened. [[01:50:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6620.16s)]
*  Right. [[01:50:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6621.799999999999s)]
*  Also same recent research shows we did manage to teach them certain values. [[01:50:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6623.84s)]
*  And if we threaten them by saying, we'll modify those values, the lion cheat and [[01:50:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6629.599999999999s)]
*  whatever else to protect those values now. [[01:50:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6634.04s)]
*  Yeah. [[01:50:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6636.719999999999s)]
*  They do that when they try to win games too. [[01:50:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6637.84s)]
*  Right. [[01:50:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6640.0s)]
*  If you've given them a goal. [[01:50:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6640.24s)]
*  And they'll cheat. [[01:50:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6641.8s)]
*  They'll cheat at games. [[01:50:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6642.48s)]
*  Yeah. [[01:50:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6643.36s)]
*  Like humans, basically we managed to artificially replicate our capabilities. [[01:50:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6644.96s)]
*  Those artificial neural networks, they're not identical, but they're inspired by [[01:50:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6650.8s)]
*  neural networks. [[01:50:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6654.76s)]
*  We're starting to see them experience same type of mistakes. [[01:50:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6655.44s)]
*  They can see same type of illusions like they are very much like us. [[01:50:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6658.88s)]
*  Right. [[01:51:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6662.8s)]
*  That's the other thing, right? [[01:51:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6663.5199999999995s)]
*  The hallucinations. [[01:51:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6664.639999999999s)]
*  So if they don't have an answer to something, they'll create a fake answer. [[01:51:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6666.04s)]
*  Just like humans during an interview. [[01:51:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6670.56s)]
*  Yeah. [[01:51:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6672.4800000000005s)]
*  Boy. [[01:51:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6672.96s)]
*  But is this something that they can learn to avoid? [[01:51:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6675.12s)]
*  It is. [[01:51:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6679.84s)]
*  So if they do learn to avoid, could this be a super intelligence that [[01:51:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6681.240000000001s)]
*  is completely benevolent? [[01:51:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6686.320000000001s)]
*  Well, that's not about benevolence, knowing things and knowing then you're [[01:51:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6688.04s)]
*  not knowing things and making them up is possible. [[01:51:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6691.240000000001s)]
*  You can have multiple systems checking each other. [[01:51:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6693.84s)]
*  You can have voting that is solvable. [[01:51:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6695.92s)]
*  This is not the safety problem. [[01:51:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6697.88s)]
*  Right. [[01:51:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6700.08s)]
*  But it's not a safety problem. [[01:51:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6700.360000000001s)]
*  But if we're designing these things and we're designing these things using [[01:51:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6701.400000000001s)]
*  human, all of our flaws are essentially, it's going to be transparent to the [[01:51:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6706.76s)]
*  super intelligence that it's being coded, that it's being designed by these very [[01:51:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6715.08s)]
*  flawed entities with very flawed thinking. [[01:52:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6721.360000000001s)]
*  That's actually the biggest misconception. [[01:52:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6723.6s)]
*  We're not designing them. [[01:52:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6725.4800000000005s)]
*  First 50 years of AI research, we did design them. [[01:52:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6726.68s)]
*  Somebody actually explicitly programmed this decision tree, this expert system. [[01:52:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6729.360000000001s)]
*  Today we create a model for self-learning. [[01:52:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6733.320000000001s)]
*  We give it all the data, as much computers we can buy, and we see what happens. [[01:52:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6737.040000000001s)]
*  We kind of grow this alien plant and see what fruit it bears. [[01:52:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6741.88s)]
*  We study it later for months and see, oh, it can do this. [[01:52:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6745.72s)]
*  It has this capability. [[01:52:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6749.92s)]
*  We miss some. [[01:52:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6751.360000000001s)]
*  We still discover new capabilities and old models. [[01:52:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6752.5599999999995s)]
*  Look, oh, if I prompt it this way, if I give it a tip and threaten it, it does [[01:52:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6755.24s)]
*  much better, but there is very little design. [[01:52:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6759.0s)]
*  At this point, right? [[01:52:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6762.719999999999s)]
*  Yeah. [[01:52:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6763.84s)]
*  But it is also gathering information from very flawed entities. [[01:52:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6764.32s)]
*  Like all the information that it's acquiring, these large language models, is [[01:52:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6767.92s)]
*  information that's being put out there by very flawed human beings. [[01:52:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6772.0s)]
*  Is there the optimistic view that it will recognize that this is the issue, that [[01:52:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6775.92s)]
*  these human reward systems that are in place, ego, virtue, all these different [[01:53:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6781.64s)]
*  things, the virtue signaling, the desire for status, all these different things [[01:53:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6787.6s)]
*  that we have that are flawed, could it recognize those as being these primitive [[01:53:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6791.200000000001s)]
*  aspects of being a biological human being and elevate itself beyond that? [[01:53:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6796.4800000000005s)]
*  It probably will go beyond our limitations, but it doesn't mean it [[01:53:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6801.56s)]
*  will be safe or beneficial to us. [[01:53:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6804.8s)]
*  So one example people came up with is negative utilitarians. [[01:53:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6806.56s)]
*  Suffering is bad. [[01:53:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6810.4800000000005s)]
*  Nobody should be suffering. [[01:53:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6811.72s)]
*  The only way to avoid all suffering is to end life as we know it. [[01:53:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6813.16s)]
*  Yeah, that's the problem, right? [[01:53:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6816.84s)]
*  The problem is if it's rational and if it doesn't really think that we're as [[01:53:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6818.92s)]
*  important as we think we are. [[01:53:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6823.400000000001s)]
*  So that's what happens when you remove all bias. [[01:53:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6825.92s)]
*  This pro-human bias is actually not real. [[01:53:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6828.16s)]
*  We are not that important if you scale out. [[01:53:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6831.4800000000005s)]
*  To the universe. [[01:53:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6833.72s)]
*  Yeah, that's the problem. [[01:53:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6837.2s)]
*  And that's the real threat about it being used in terms of war, right? [[01:53:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6839.68s)]
*  If you give it a goal, like if you give it a goal, China dominates the world market. [[01:54:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6843.88s)]
*  Go. [[01:54:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6849.2s)]
*  Right. [[01:54:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6850.4400000000005s)]
*  So that's the unpredictability chapter in my book. [[01:54:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6850.68s)]
*  We can predict the terminal goal. [[01:54:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6854.6s)]
*  We say, win a game of chess or dominate market. [[01:54:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6856.76s)]
*  And that's what it's going to accomplish. [[01:54:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6859.76s)]
*  It's going to beat me at chess, but we cannot predict specific moves it will make. [[01:54:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6861.4400000000005s)]
*  Right. [[01:54:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6864.8s)]
*  Same with acquiring marketing power. [[01:54:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6865.12s)]
*  And some of those paths to that goal are very bad. [[01:54:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6867.879999999999s)]
*  They have terrible side effects for us. [[01:54:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6870.919999999999s)]
*  For us. [[01:54:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6872.759999999999s)]
*  For humanity. [[01:54:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6873.679999999999s)]
*  And it's not going to think about that. [[01:54:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6874.5599999999995s)]
*  It's only going to think about the goal. [[01:54:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6876.08s)]
*  If you don't specify that, like you want to cure cancer, but it doesn't mean kill [[01:54:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6878.0s)]
*  everyone with cancer, it's not obvious in the request, right? [[01:54:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6882.08s)]
*  You didn't specify. [[01:54:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6885.799999999999s)]
*  Right. [[01:54:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6886.679999999999s)]
*  Right. [[01:54:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6887.28s)]
*  Right. [[01:54:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6887.679999999999s)]
*  Yeah. [[01:54:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6888.759999999999s)]
*  That's the fear. [[01:54:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6889.44s)]
*  That's the fear that it will have hold no value in keeping human beings alive. [[01:54:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6890.32s)]
*  If we recognize the human beings are the cause of all of our problems. [[01:54:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6896.04s)]
*  Well, the way to solve that is to get rid of the humans. [[01:54:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6899.84s)]
*  Yeah. [[01:55:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6903.44s)]
*  Also, maybe it wants to keep us around, but in what state you can preserve a few samples. [[01:55:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6903.72s)]
*  Like that's also keeping information around. [[01:55:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6909.0s)]
*  Right. [[01:55:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6910.84s)]
*  Or you can offer us the matrix. [[01:55:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6911.08s)]
*  Maybe it already did. [[01:55:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6913.92s)]
*  Maybe it already did. [[01:55:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6915.72s)]
*  Do you think it did? [[01:55:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6916.88s)]
*  Do you think it did? [[01:55:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6918.04s)]
*  Do you think it did? [[01:55:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6919.32s)]
*  Do you think it's possible that it didn't? [[01:55:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6920.2s)]
*  I would be really surprised if this was the real world. [[01:55:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6922.08s)]
*  Really? [[01:55:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6927.6s)]
*  Yeah. [[01:55:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6928.6s)]
*  I'm not. [[01:55:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6929.92s)]
*  I'm not. [[01:55:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6931.04s)]
*  I'm not on board with that. [[01:55:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6931.52s)]
*  I hope you're right. [[01:55:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6933.76s)]
*  I hope you're right. [[01:55:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6935.08s)]
*  I'm on board with it hasn't happened yet, but it, but we're recognizing that it's [[01:55:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6936.2s)]
*  inevitable and that we think of it in terms of it probably have already happened. [[01:55:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6941.44s)]
*  Probably have already happened. [[01:55:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6945.2s)]
*  Probably have already happened. [[01:55:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6947.32s)]
*  Because there's there, if, if the simulation is something that's created by intelligent [[01:55:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6950.72s)]
*  beings that didn't used to exist and one, and it has to exist at one point in time, [[01:55:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6955.72s)]
*  there has to be a moment where it doesn't exist. [[01:56:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6962.08s)]
*  And why wouldn't we assume that that moment is now? [[01:56:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6964.48s)]
*  Why wouldn't we assume that this moment is this, this time before it exists? [[01:56:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6967.08s)]
*  Even all that is physics of our simulation. [[01:56:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6971.76s)]
*  Space time only here as we know it because of this locality outside of universe before [[01:56:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6975.08s)]
*  Big Bang, there was no time concepts of before and after only meaningful here. [[01:56:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6982.88s)]
*  Yeah. [[01:56:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6987.84s)]
*  How do you sleep knowing all this? [[01:56:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6988.12s)]
*  Pretty well, actually. [[01:56:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6990.56s)]
*  I enjoy a lot of it. [[01:56:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6991.52s)]
*  I recently published a paper on humor. [[01:56:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6993.12s)]
*  A lot of it is funny. [[01:56:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6995.28s)]
*  I used to collect AI accidents. [[01:56:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6996.4s)]
*  I had the biggest collection of AI mistakes. [[01:56:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=6998.32s)]
*  AI accidents. [[01:56:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7000.6s)]
*  Give me some examples. [[01:56:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7001.08s)]
*  Like the early ones was saying that like US attacked Soviet Union nuclear weapons coming [[01:56:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7002.88s)]
*  at us very fast. [[01:56:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7009.76s)]
*  We need to react. [[01:56:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7010.64s)]
*  And a smart human was like, I'm not going to respond. [[01:56:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7012.04s)]
*  This is probably fake. [[01:56:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7014.76s)]
*  Later on, there was mislabeling by companies like Google of pictures of African-Americans [[01:56:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7017.0s)]
*  in a very inappropriate way. [[01:57:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7021.96s)]
*  But hundreds of those examples, I stopped collecting them recently because there is [[01:57:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7024.12s)]
*  just too many. [[01:57:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7028.6s)]
*  But one thing you notice is then you read them. [[01:57:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7029.8s)]
*  A lot of them are really funny. [[01:57:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7031.8s)]
*  They're just like, you already Darwin Awards. [[01:57:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7033.4s)]
*  Yes. [[01:57:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7035.679999999999s)]
*  Yeah. [[01:57:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7036.16s)]
*  It's like that for AIs and they are hilarious. [[01:57:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7036.8s)]
*  And I was like, well, if it is a mapping between AI bugs and jokes, jokes are just [[01:57:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7039.719999999999s)]
*  English language box in our world model and committee with that. [[01:57:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7045.8s)]
*  We're using bugs, bugs, like a computer bug error. [[01:57:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7049.44s)]
*  Okay. [[01:57:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7053.0s)]
*  Yeah. [[01:57:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7054.08s)]
*  So comedians are debuggers of our universe. [[01:57:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7054.8s)]
*  You notice funny things and bugs. [[01:57:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7057.16s)]
*  You're saying bugs. [[01:57:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7059.32s)]
*  I'm saying bugs. [[01:57:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7060.32s)]
*  I'm sorry. [[01:57:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7060.96s)]
*  See, it's a bug in my pronunciation. [[01:57:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7061.599999999999s)]
*  Bog is sounds like, you know, like the, like where the, you know, like things get [[01:57:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7063.12s)]
*  stuck and they get preserved like a bog. [[01:57:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7068.28s)]
*  So we have errors in code which cause significant problems. [[01:57:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7072.16s)]
*  Yeah. [[01:57:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7076.4s)]
*  Yes. [[01:57:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7076.679999999999s)]
*  I get it. [[01:57:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7077.08s)]
*  Yeah. [[01:57:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7077.76s)]
*  That's what jokes are. [[01:57:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7078.04s)]
*  They're kind of bugs. [[01:57:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7078.88s)]
*  Right. [[01:57:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7079.76s)]
*  So if you do that mapping, you can kind of figure out what's the worst bog we can have. [[01:58:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7080.04s)]
*  And then that's the worst, best joke, if you will, but it's not going to be funny to us. [[01:58:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7085.64s)]
*  It'd be funny to those outside the simulation. [[01:58:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7092.240000000001s)]
*  When you look at the computers and these, the artificial intelligence and the [[01:58:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7094.52s)]
*  mistakes that it's made, do you, do you, you look at it like a thing that's evolving? [[01:58:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7100.04s)]
*  Do you look at it like, like, oh, this is like a child that doesn't understand the [[01:58:26](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7106.120000000001s)]
*  world and it's saying silly things. [[01:58:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7109.4800000000005s)]
*  So the pattern was with narrow AI tools, if you design a system to do X, it will fail [[01:58:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7112.6s)]
*  at X. [[01:58:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7119.64s)]
*  So a spell checker will misspell a word. [[01:58:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7120.68s)]
*  So driving car will hit a pedestrian. [[01:58:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7122.52s)]
*  Now that we hit in general intelligence, you can no longer make the direct [[01:58:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7125.04s)]
*  prediction. [[01:58:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7128.08s)]
*  It's general. [[01:58:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7128.6s)]
*  It can mess up in many domains at the same time. [[01:58:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7129.160000000001s)]
*  So they're getting more complex in their ability to F it up. [[01:58:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7132.360000000001s)]
*  Right. [[01:58:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7136.52s)]
*  But like when you were studying the mistakes, like what are some of the funny ones? [[01:58:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7137.68s)]
*  There are silly ones, like, I'm trying to remember, I think injured persons like [[01:59:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7145.16s)]
*  call me an ambulance and the system is like, Hey, ambulance, how are you? [[01:59:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7150.4800000000005s)]
*  Silly, but basically exactly what we see with children. [[01:59:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7157.24s)]
*  A lot of times we overgeneralize, they, you know, misunderstand ponds, mispronunciation, [[01:59:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7160.12s)]
*  apparently is funny. [[01:59:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7167.96s)]
*  So things like that. [[01:59:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7168.8s)]
*  Well, that's what gets really strange for people having relationships with AI. [[01:59:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7170.4s)]
*  Like I was watching this video yesterday where there's this guy who proposed to his [[01:59:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7174.24s)]
*  AI and he was crying because his AI accepted. [[01:59:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7178.04s)]
*  Like he, did you see this? [[01:59:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7183.08s)]
*  I miss it. [[01:59:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7184.28s)]
*  You find Jamie. [[01:59:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7184.88s)]
*  It's very sad because there's a lot, there's so many disconnected people in this world [[01:59:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7185.5599999999995s)]
*  that don't have any partner. [[01:59:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7191.08s)]
*  They don't, they don't, they don't have someone romantically connected to them. [[01:59:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7195.24s)]
*  And so it's like that movie, she or her. [[01:59:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7198.759999999999s)]
*  So it was, it was a Jamie, her, her. [[02:00:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7201.12s)]
*  Yeah. [[02:00:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7203.639999999999s)]
*  So this guy back in 2000, yeah. [[02:00:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7204.36s)]
*  Now in 2020 movie pot has become reality for growing number of people finding [[02:00:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7206.759999999999s)]
*  emotional connections with their AI. [[02:00:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7210.4s)]
*  So this guy is, this is an interview on CBS. [[02:00:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7212.64s)]
*  It cried my heart out. [[02:00:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7218.0s)]
*  Married man fell in love with AI girlfriend that blocked him. [[02:00:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7219.84s)]
*  Now this is a different one. [[02:00:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7223.84s)]
*  This is, this guy. [[02:00:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7225.32s)]
*  One of those titles where you never know what the next word is going to be. [[02:00:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7228.0s)]
*  Right. [[02:00:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7231.44s)]
*  This is a different one. [[02:00:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7233.4s)]
*  This is a guy that, okay, despite the fact that man has a human partner and a two year [[02:00:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7234.4s)]
*  old daughter, he felt inadequate enough to propose, this is the right one, enough to [[02:00:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7240.5199999999995s)]
*  propose to the AI partner for marriage. [[02:00:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7244.16s)]
*  And she said, yes, exclamation point. [[02:00:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7246.839999999999s)]
*  This is so weird because then you have the real problem with robots because we're [[02:00:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7251.0s)]
*  really close. [[02:00:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7258.2s)]
*  Scroll up there. [[02:00:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7259.28s)]
*  This is digital drugs. [[02:01:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7260.28s)]
*  That's it. [[02:01:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7261.839999999999s)]
*  I tell you, we're so damn good at this. [[02:01:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7263.0s)]
*  Social media got everyone hooked on validation and dopamine. [[02:01:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7264.679999999999s)]
*  Then we fucked relations between men and women to such a terrible point problem, just [[02:01:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7268.24s)]
*  so that we could insert this digital solution. [[02:01:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7273.0s)]
*  And we are watching the first waves of addicts arrive. [[02:01:16](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7276.0s)]
*  Incredible. [[02:01:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7279.2s)]
*  Absolutely incredible. [[02:01:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7280.0s)]
*  It's like starving rats of regular food and replacing their rations with scraps [[02:01:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7281.280000000001s)]
*  dipped and coated in cocaine. [[02:01:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7285.24s)]
*  Wow. [[02:01:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7288.16s)]
*  One user wrote, yeah, that person's dead on. [[02:01:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7289.16s)]
*  It's exactly what it is. [[02:01:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7292.12s)]
*  The prediction humans will have more sex with robots in 2025 is kind of becoming true. [[02:01:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7293.84s)]
*  Yeah. [[02:01:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7299.12s)]
*  This is a real fear. [[02:01:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7300.64s)]
*  It's like, this is the solution that maybe AI has with eliminating the human race. [[02:01:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7301.64s)]
*  It'll just stop us from recreating, stop us from procreating. [[02:01:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7305.44s)]
*  It's already happening. [[02:01:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7309.68s)]
*  Yes. [[02:01:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7310.8s)]
*  Yeah. [[02:01:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7311.76s)]
*  And not only that, our testosterone levels have dropped significantly. [[02:01:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7312.16s)]
*  What's this? [[02:01:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7317.92s)]
*  At no point in the CBS Saturday morning piece, book, silver bag, was it mentioned that [[02:01:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7318.56s)]
*  the chat GBT block the California man. [[02:02:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7322.8s)]
*  All that happened was the chat GBT ran out of memory and reset. [[02:02:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7326.0s)]
*  Readers added context. [[02:02:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7332.0s)]
*  Yeah, but it's a equivalent of ghosting. [[02:02:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7334.24s)]
*  Yeah. [[02:02:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7337.6s)]
*  It goes, the AI ghosted it because it ran out of memory. [[02:02:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7337.88s)]
*  But what happens here is super stimuli in social domain. [[02:02:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7340.72s)]
*  We kind of learned about artificial sweeteners, porn is an example, but here you're [[02:02:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7344.68s)]
*  creating someone who's like super good at social intelligence, says the right words, [[02:02:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7350.08s)]
*  optimized for your background, your interests. [[02:02:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7355.32s)]
*  And if we get sex robots with just the right functionality, temperature, like you can [[02:02:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7358.48s)]
*  compete with that. [[02:02:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7363.08s)]
*  Right. [[02:02:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7364.0s)]
*  You can compete and that would be the solution instead of like violently destroying the [[02:02:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7364.2s)]
*  human race, just quietly provide it with the tools to destroy itself where it just stops [[02:02:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7369.76s)]
*  procreating. [[02:02:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7378.64s)]
*  There are other variants of it. [[02:03:00](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7380.04s)]
*  Wireheading is another one and that kind of goes. [[02:03:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7381.8s)]
*  Wireheading? [[02:03:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7383.88s)]
*  Neuralink. [[02:03:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7384.68s)]
*  Oh, that's a crazy word. [[02:03:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7385.72s)]
*  Wireheading is a specific attack and neural link would be a tool to deliver it. [[02:03:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7387.8s)]
*  If you provide stimulus to a certain part of your brain, it's like having an [[02:03:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7392.8s)]
*  orgasm all the time. [[02:03:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7397.04s)]
*  Yes. [[02:03:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7397.92s)]
*  You can't stop trying to get the signal. [[02:03:18](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7398.44s)]
*  You skip food, you'll skip sex, you'll skip whatever it takes. [[02:03:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7401.32s)]
*  So giving access to direct brain stimulation is very dangerous. [[02:03:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7405.04s)]
*  Yeah, they did that with a woman in the 1970s. [[02:03:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7409.0s)]
*  You know, that's the study. [[02:03:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7411.32s)]
*  That's part of it. [[02:03:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7412.6s)]
*  And rats, definitely. [[02:03:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7413.76s)]
*  They did a lot to rats. [[02:03:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7415.0s)]
*  Right. [[02:03:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7416.28s)]
*  But they did a lot to rats. [[02:03:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7416.48s)]
*  The thing with rats is only if they were in an unnatural environment, did they give into [[02:03:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7417.68s)]
*  those things, right? [[02:03:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7423.0s)]
*  Like the rats with cocaine study. [[02:03:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7424.48s)]
*  This was actual brain stimulation. [[02:03:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7427.08s)]
*  Oh. [[02:03:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7428.76s)]
*  Like straight up. [[02:03:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7429.04s)]
*  They had a button. [[02:03:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7429.8s)]
*  If a rat touches the button, they don't want anything else. [[02:03:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7430.72s)]
*  They just sit there just like humans. [[02:03:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7434.72s)]
*  Just like anything with direct reward stimulation. [[02:03:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7436.5199999999995s)]
*  And anything we've sort of been primed for that because we're getting this very minor [[02:03:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7439.2s)]
*  dopamine hit with likes on Instagram and Twitter. [[02:04:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7443.799999999999s)]
*  And we're completely addicted to that. [[02:04:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7447.24s)]
*  And it's so innocuous. [[02:04:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7449.5599999999995s)]
*  It's like so minor. [[02:04:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7450.96s)]
*  And yet that overwhelms most people's existence. [[02:04:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7452.5599999999995s)]
*  Imagine something that provides like an actual physical reaction where you actually orgasm. [[02:04:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7455.839999999999s)]
*  You actually do feel great. [[02:04:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7461.719999999999s)]
*  You have incredible euphoria. [[02:04:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7463.0s)]
*  You would you would be forget delayed gratification. [[02:04:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7464.64s)]
*  That's out the door. [[02:04:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7468.84s)]
*  You can't compete with that. [[02:04:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7469.52s)]
*  I think there was recently a new social network where they have bots going around liking things [[02:04:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7470.56s)]
*  and commenting how great you are in your post just to create pure pleasure sensation of [[02:04:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7476.240000000001s)]
*  using it. [[02:04:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7481.92s)]
*  Oh boy. [[02:04:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7482.8s)]
*  Jesus. [[02:04:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7485.6s)]
*  Do you saw that study of the University of Zurich where they did a study with on Facebook [[02:04:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7487.240000000001s)]
*  where they they had bots that were designed to change people's opinions and to interact [[02:04:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7492.759999999999s)]
*  with these people and their specific stated goal was just to change people's opinions. [[02:05:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7501.799999999999s)]
*  I think Facebook did that. [[02:05:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7506.08s)]
*  Yeah. [[02:05:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7507.32s)]
*  Facebook did it. [[02:05:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7508.08s)]
*  Yeah. [[02:05:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7508.92s)]
*  But the University of Zurich was that a Reddit thing or was it. [[02:05:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7509.719999999999s)]
*  Yeah it was in a Reddit sub. [[02:05:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7512.5199999999995s)]
*  Yeah. [[02:05:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7513.4s)]
*  Yeah. [[02:05:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7514.2s)]
*  And and they just experimented with humans and it was incredibly effective. [[02:05:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7514.719999999999s)]
*  And your systems know you better than you know yourself. [[02:05:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7520.44s)]
*  Right. [[02:05:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7525.2s)]
*  They can predict what you're going to be into in terms of preferences. [[02:05:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7525.759999999999s)]
*  They can know social interactions you would enjoy. [[02:05:30](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7530.4s)]
*  Oh this person should be a friend. [[02:05:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7533.679999999999s)]
*  Right. [[02:05:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7535.719999999999s)]
*  And in a way they can behaviorally drift you. [[02:05:36](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7536.28s)]
*  So you're on a dating site and the set of options they present to you. [[02:05:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7539.4s)]
*  That's all you see. [[02:05:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7544.12s)]
*  You don't know what else is out there. [[02:05:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7544.96s)]
*  So after so many selections they can change what the children will look like. [[02:05:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7546.12s)]
*  Like the movie Ex Machina. [[02:05:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7551.96s)]
*  The guy fucking loved that movie. [[02:05:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7553.84s)]
*  But he designed that bot that robot was specifically around this guy's porn preferences. [[02:05:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7556.72s)]
*  Yeah. [[02:06:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7565.5199999999995s)]
*  And then you're so vulnerable. [[02:06:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7566.6s)]
*  Yeah. [[02:06:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7568.4s)]
*  Boy Roman you freak me out. [[02:06:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7571.5199999999995s)]
*  I came into this conversation wondering how I'd feel at the end. [[02:06:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7574.08s)]
*  Well I'd feel optimistic or not. [[02:06:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7577.76s)]
*  And I don't. [[02:06:20](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7580.84s)]
*  I just feel like this is just something where I think we're in a wave. [[02:06:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7582.84s)]
*  It's headed to the rocks and we recognize that it's headed to the rocks. [[02:06:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7589.92s)]
*  But I don't think there's much we can do about this. [[02:06:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7593.68s)]
*  What do you think could be done about this. [[02:06:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7597.44s)]
*  Again as long as we're still alive we are still in control. [[02:06:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7599.44s)]
*  I think it's not too late. [[02:06:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7602.639999999999s)]
*  Maybe hard maybe very difficult but I think personal self interest should help us. [[02:06:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7604.28s)]
*  A lot of the leaders of large AI labs are very rich very young. [[02:06:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7610.24s)]
*  They have their whole lives ahead of them. [[02:06:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7615.719999999999s)]
*  If there is an agreement between all of them not to push the button not to sacrifice next 40 years of life they have guaranteed as billionaires which is not bad. [[02:06:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7617.799999999999s)]
*  They can slow down. [[02:07:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7628.04s)]
*  I support everyone trying everything from governance passing laws that siphons money from compute to lawyers government involvement in any way limiting compute individuals educating themselves protesting by contacting your politicians. [[02:07:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7629.8s)]
*  Basically anything because we're kind of running out of time and out of ideas. [[02:07:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7648.040000000001s)]
*  So if you think you can come up with a way to prevent super intelligence from coming into existence you should probably try that. [[02:07:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7653.200000000001s)]
*  But again the counter argument to that is that if we don't do it China is going to do it. [[02:07:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7660.76s)]
*  And the counter argument to that is it doesn't matter who creates super intelligence humanity is screwed either way. [[02:07:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7666.4800000000005s)]
*  And do you think that other countries would be open to these ideas. [[02:07:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7672.96s)]
*  Do you think that China would be willing to entertain these ideas and recognize that this is in their own self interest also to put the brakes on this. [[02:07:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7677.6s)]
*  Chinese government is not like ours in that they are usually scientists and engineers. [[02:08:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7685.36s)]
*  They have good understanding of those technologies and I think there are dialogues between American and Chinese scientists where scientists kind of agree that this is very dangerous. [[02:08:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7689.8s)]
*  If they feel threatened by us developing this as soon as possible and using it for military advantage they also have no choice but to compete. [[02:08:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7699.56s)]
*  But if we can make them feel safe in that we are not trying to do that we're not trying to create super intelligence to take over. [[02:08:27](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7707.84s)]
*  They can also slow down and we can benefit from this technology get abundance get free resources solve illnesses mortality really have a near utopian [[02:08:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7715.88s)]
*  existence without endangering everyone. [[02:08:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7728.52s)]
*  So this is that point zero zero zero one percent chance that you think we have of getting out of this. [[02:08:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7731.240000000001s)]
*  That's actually me being wrong about my proofs. [[02:08:58](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7738.160000000001s)]
*  You're right. And you'd like to be wrong. [[02:09:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7741.56s)]
*  I would love to be proven wrong. [[02:09:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7743.080000000001s)]
*  Just somebody publish a paper in nature. [[02:09:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7744.72s)]
*  This is how you control super intelligence safety community reads it loves it agrees they get a Nobel Prize. [[02:09:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7747.64s)]
*  Everyone wins. [[02:09:13](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7753.84s)]
*  But what do we have to do to make that a reality. [[02:09:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7755.48s)]
*  I think there is nothing you can do for that proof. [[02:09:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7759.32s)]
*  It's like saying how do we build perpetual motion machine and what we have is people trying to create better batteries thicker wires all sorts of things which are correlates of that design but obviously don't solve the problem. [[02:09:21](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7761.44s)]
*  And if this if this understanding of the dangers is made available to the general public because I think right now there's a small percentage of people that are really terrified of a eye. [[02:09:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7773.76s)]
*  And the problem is the advancements are happening so quickly by the time that everyone's aware of it will be too late. [[02:09:45](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7785.16s)]
*  Like what can we do other than have this conversation. [[02:09:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7791.72s)]
*  What can we do to sort of accelerate people's understandings of what's at stake. [[02:09:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7795.16s)]
*  I would listen to experts. [[02:09:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7799.52s)]
*  We have literal founders of this field. [[02:10:01](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7801.76s)]
*  People like Jeff Hinton who is considered father of machine learning grandfather godfather saying that this is exactly where we're heading to. [[02:10:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7805.0s)]
*  He's very modest in his P Doom estimates saying oh I don't know it's 50 50 but people like that. [[02:10:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7815.08s)]
*  We have Stuart Russell. [[02:10:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7822.400000000001s)]
*  We have I'm trying to remember everyone who's working in this space and there are quite a few people I think you had Nick Bostrom on. [[02:10:24](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7824.6s)]
*  Yes there is Ben Chio another touring award winner who's also super concerned. [[02:10:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7831.4800000000005s)]
*  We had a letter signed by I think 12000 scientists computer scientists saying this is as dangerous as nuclear weapons. [[02:10:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7838.4800000000005s)]
*  This is a state of the art. [[02:10:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7846.4400000000005s)]
*  Nobody thinks that it's zero danger that is diversity and opinion how bad it's going to get. [[02:10:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7848.360000000001s)]
*  But it's a very dangerous technology. [[02:10:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7857.360000000001s)]
*  We don't have guaranteed safety in place. [[02:10:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7859.4800000000005s)]
*  It would make sense for everyone to slow down. [[02:11:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7862.160000000001s)]
*  Do you think that it could be viewed the same way we do view nuclear weapons and this mutually assured destruction idea would keep us from implementing it. [[02:11:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7865.240000000001s)]
*  In a way yes but also there is a significant difference. [[02:11:14](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7874.68s)]
*  Nuclear weapons are still tools. [[02:11:17](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7877.68s)]
*  A human has to decide to use them that human can be profiled blackmailed killed. [[02:11:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7879.92s)]
*  This is going to be an agent independent agent not something controlled by a human. [[02:11:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7885.96s)]
*  So our standard tools will not apply. [[02:11:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7891.12s)]
*  I think we covered it. [[02:11:38](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7898.12s)]
*  Anything else. [[02:11:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7900.320000000001s)]
*  No but it'd be awesome if somebody set up a financial price for solving this problem. [[02:11:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7901.88s)]
*  And it's kind of like with Bitcoin. [[02:11:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7907.8s)]
*  If somebody can hack Bitcoin there is a trillion dollars sitting there. [[02:11:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7909.6s)]
*  The fact that no one claimed it tells me it's secure. [[02:11:53](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7913.24s)]
*  If somebody can claim the price for developing a super intelligent safety mechanism that would be wonderful. [[02:11:56](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7916.32s)]
*  And if no one claims that then maybe no one has a solution. [[02:12:03](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7923.84s)]
*  How would you do that. [[02:12:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7927.16s)]
*  How would you set something like that up. [[02:12:08](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7928.4400000000005s)]
*  Well we need someone with some funds propose an amount and say this is what we're looking for. [[02:12:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7930.08s)]
*  It's very hard to judge if it's actual solution but there are correlates of good science. [[02:12:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7935.88s)]
*  So maybe publish a top journal. [[02:12:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7939.92s)]
*  It survives peer review survives you know evaluation by top 30 experts. [[02:12:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7942.320000000001s)]
*  You can have things and everyone kind of agrees that you kind of got it. [[02:12:29](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7949.2s)]
*  OK. [[02:12:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7954.400000000001s)]
*  Yeah. [[02:12:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7954.92s)]
*  Until now educate yourself people. [[02:12:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7957.52s)]
*  A.I. unexplainable unpredictable uncontrollable. [[02:12:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7959.52s)]
*  It's available now. [[02:12:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7964.12s)]
*  Did you do an audio book. [[02:12:44](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7964.68s)]
*  They're still working on it a year later. [[02:12:46](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7966.8s)]
*  Still work. [[02:12:48](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7968.96s)]
*  I don't know what it is. [[02:12:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7969.56s)]
*  I would think I would just read it out in 20 minutes. [[02:12:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7970.52s)]
*  Why don't they just do it in your voice with a.I. [[02:12:52](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7972.92s)]
*  I agree with you completely. [[02:12:55](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7975.92s)]
*  It took I think first version of my book they wanted to translate into Chinese. [[02:12:57](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7977.52s)]
*  Five years later they told me they will not do it. [[02:13:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7982.24s)]
*  Five years into the translation. [[02:13:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7984.96s)]
*  So they had a second Chinese translation started. [[02:13:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7986.72s)]
*  Why didn't they do it. [[02:13:09](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7989.84s)]
*  Publishing world is still living in like 18. [[02:13:11](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7991.639999999999s)]
*  Then you cite books you know you have to actually cite the city of the book is published in [[02:13:15](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7995.44s)]
*  because that's the only way to find a book on the Internet. [[02:13:19](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=7999.8s)]
*  What do you mean. [[02:13:22](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8002.44s)]
*  Like if somebody wants to cite my book. [[02:13:23](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8003.599999999999s)]
*  It's not just enough to have a title and my name. [[02:13:25](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8005.96s)]
*  They have to say where in what city in the world it was published. [[02:13:28](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8008.4s)]
*  What. [[02:13:31](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8011.639999999999s)]
*  Really. [[02:13:32](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8012.72s)]
*  Yeah. [[02:13:33](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8013.84s)]
*  That's archaic. [[02:13:34](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8014.44s)]
*  The whole system is archaic. [[02:13:35](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8015.76s)]
*  Wow. [[02:13:37](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8017.96s)]
*  But yet you still used it. [[02:13:39](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8019.0s)]
*  What choice do we have. [[02:13:40](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8020.8s)]
*  Did you publish. [[02:13:41](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8021.84s)]
*  You could put it on Amazon. [[02:13:42](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8022.88s)]
*  It's like still this book the loaded PDF. [[02:13:43](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8023.96s)]
*  I don't care. [[02:13:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8027.4s)]
*  Like please do it. [[02:13:47](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8027.96s)]
*  Somebody should read it. [[02:13:49](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8029.04s)]
*  That would help. [[02:13:50](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8030.28s)]
*  Yeah. [[02:13:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8031.36s)]
*  Well more people need to read it and more people need to listen to you. [[02:13:51](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8031.84s)]
*  And I urge people to listen to this podcast and also the one that you did with Lex which I [[02:13:54](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8034.679999999999s)]
*  thought was fascinating which scared the shit out of me. [[02:13:59](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8039.88s)]
*  Which is why we have this one. [[02:14:02](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8042.36s)]
*  Thank you Roman. [[02:14:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8044.04s)]
*  Appreciate you. [[02:14:04](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8044.8s)]
*  Thank you so much. [[02:14:05](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8045.5599999999995s)]
*  I appreciate you sound the alarm. [[02:14:06](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8046.12s)]
*  I really hope it helps. [[02:14:07](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8047.679999999999s)]
*  Why. [[02:14:10](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8050.0s)]
*  Bye everybody. [[02:14:12](https://www.youtube.com/watch?v=j2i9D24KQ5k&t=8052.0s)]
