---
Date Generated: February 17, 2025
Transcription Model: whisper medium 20231117
Length: 9600s
Video Keywords: ['Joe Rogan Experience', 'JRE', 'Joe', 'Rogan', 'podcast', 'MMA', 'comedy', 'stand', 'up', 'funny', 'Freak', 'Party', 'Marc Andreessen', 'JRE #2010', 'a16z']
Video Views: 110699
Video Rating: None
Video Description: Marc Andreessen is an entrepreneur, investor, and software engineer. He is co-creator of the world's first widely used internet browser, Mosaic, and cofounder and general partner at the venture capital firm Andreessen Horowitz www.a16z.com https://pmarca.substack.com
---

# Joe Rogan Experience #2010 - Marc Andreessen
**Joe Rogan Experience:** [June 27, 2024](https://www.youtube.com/watch?v=8quXLOR_iVE)
*  The Joe Rogan Experience. [[00:00:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=0.0s)]
*  Trained by day, Joe Rogan podcasts by night, all day. [[00:00:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5.72s)]
*  How are you Mark? [[00:00:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=10.48s)]
*  Good to see you. [[00:00:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=12.56s)]
*  Fantastic, thanks. [[00:00:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=13.56s)]
*  You are in the middle of this AI discussion. [[00:00:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=14.56s)]
*  You're right in the heat of this thing. [[00:00:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=19.48s)]
*  I think you have a different perspective than a lot of people do. [[00:00:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=21.72s)]
*  A lot of people are terrified of AI, me included. [[00:00:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=23.92s)]
*  Oh, okay. [[00:00:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=26.88s)]
*  All right. [[00:00:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=27.88s)]
*  Okay. [[00:00:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=28.88s)]
*  Of all the things to worry about. [[00:00:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=29.959999999999997s)]
*  For me, my terror of it is all the way. [[00:00:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=31.48s)]
*  It's kind of fun terror. [[00:00:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=34.32s)]
*  Yeah, sure. [[00:00:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=35.32s)]
*  Of course. [[00:00:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=36.32s)]
*  I'm not really freaking out, but I am recognizing that this is an emerging technology that is [[00:00:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=37.32s)]
*  so different than anything we've ever experienced before. [[00:00:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=41.2s)]
*  Particularly like what's chat GPT, what's happening with that right now. [[00:00:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=44.12s)]
*  It's really fascinating and a lot of advantages. [[00:00:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=47.599999999999994s)]
*  We were just talking last night, someone in the green room brought up the fact that they're [[00:00:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=50.04s)]
*  using it for medical diagnoses and it's very accurate, which is incredible. [[00:00:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=56.44s)]
*  There's a lot of good things to it. [[00:01:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=60.96s)]
*  You probably remember last time I was on, we spent quite a bit of time talking about [[00:01:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=64.6s)]
*  this. [[00:01:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=67.72s)]
*  This is when these check bots were running inside Google, but the rest of us didn't have [[00:01:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=68.72s)]
*  access to them. [[00:01:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=71.28s)]
*  Right. [[00:01:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=72.28s)]
*  That guy had come out and said that he thought that they were self-aware. [[00:01:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=73.28s)]
*  The whole thing was this big mystery of what's going on. [[00:01:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=76.0s)]
*  Another world gets to use these things. [[00:01:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=79.0s)]
*  Everybody since then, everybody has access. [[00:01:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=81.08s)]
*  Really quickly. [[00:01:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=83.4s)]
*  That was a short amount of time. [[00:01:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=84.4s)]
*  Yeah. [[00:01:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=85.52000000000001s)]
*  It's been great. [[00:01:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=86.52000000000001s)]
*  Then look, these things when I say this, it's like chat GPT and then Microsoft has their [[00:01:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=87.52000000000001s)]
*  version called Bing. [[00:01:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=91.60000000000001s)]
*  Google has a version called Bard now that's really good. [[00:01:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=93.0s)]
*  There's a company Anthropic that has a thing called Claude. [[00:01:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=94.76s)]
*  If you just run the comparison, they're basically as good as a doctor. [[00:01:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=99.28s)]
*  They're as good as the average doctor at this point at being a doctor. [[00:01:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=102.0s)]
*  They're as good at being a lawyer as the average lawyer. [[00:01:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=104.4s)]
*  You go through basically anything involving knowledge work, anything involving information [[00:01:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=107.56s)]
*  synthesizing, reporting, writing legal briefs, anything like this. [[00:01:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=111.56s)]
*  Business are actually already really good. [[00:01:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=116.24s)]
*  They're as good as the average management consultant. [[00:01:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=117.24s)]
*  The way they acquire data, they're essentially scouring the internet, right? [[00:01:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=119.8s)]
*  Sort of. [[00:02:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=125.44s)]
*  It's more like they're fed the internet. [[00:02:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=126.44s)]
*  They're fed the internet. [[00:02:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=127.44s)]
*  I make the difference because the company that produces the AI determines what data [[00:02:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=128.44s)]
*  goes into it. [[00:02:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=133.0s)]
*  That determines a lot of how it works and what it does or won't do. [[00:02:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=134.0s)]
*  In that regard, is there a concern that someone could feed it fake data? [[00:02:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=137.76s)]
*  Yeah. [[00:02:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=142.32s)]
*  You may have noticed that people over time have said a lot of fake things. [[00:02:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=143.32s)]
*  Yes, I have noticed that. [[00:02:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=145.92s)]
*  That's all in there. [[00:02:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=150.0s)]
*  The way to think about it basically is the full version of these things are being trained [[00:02:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=151.68s)]
*  on basically the sum total of human written expression. [[00:02:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=155.84s)]
*  Basically everything people have ever written. [[00:02:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=159.28s)]
*  There are some issues and you've got to figure out how to get all the books in there. [[00:02:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=160.84s)]
*  All the books prior to 1923 are in there because they're all out of copyright, but more recent [[00:02:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=165.07999999999998s)]
*  books are a challenge. [[00:02:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=169.2s)]
*  Something that you can access on the internet that's text, which is a staggeringly broad [[00:02:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=171.12s)]
*  set of material is in there. [[00:02:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=175.36s)]
*  By the way, both nonfiction and fiction. [[00:02:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=177.32s)]
*  A lot of stories are in there. [[00:02:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=179.36s)]
*  Then the new versions of these that are being built right now are what are called multimodal. [[00:03:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=181.84s)]
*  That means you can feed them not only text, but you can also feed them images. [[00:03:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=186.56s)]
*  You can feed them videos. [[00:03:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=189.4s)]
*  They're going to be trained on all of YouTube. [[00:03:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=191.08s)]
*  They're going to be trained on all podcasts. [[00:03:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=192.08s)]
*  They're going to be trained equivalently between text and images and video and all kinds of [[00:03:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=195.28s)]
*  other data. [[00:03:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=198.72s)]
*  They already have very comprehensive knowledge of human affairs, but it's going to get very [[00:03:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=200.48s)]
*  complete. [[00:03:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=204.44s)]
*  If it's getting all this data from both fiction and nonfiction, how does it interpret data [[00:03:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=205.44s)]
*  that's satire? [[00:03:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=214.07999999999998s)]
*  What does it do with Hunter S. Thompson, gonzo journalism? [[00:03:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=216.52s)]
*  It doesn't really know the difference. [[00:03:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=220.36s)]
*  This is one of the things that's difficult about talking about this because you want [[00:03:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=223.32s)]
*  to always compare it to a person. [[00:03:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=227.48s)]
*  Part of it is you refer to it as an it. [[00:03:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=229.24s)]
*  This is this concept of anthropomorphizing things that aren't human. [[00:03:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=230.96s)]
*  It's not really a correct thing to think about it as like that there's an it per se. [[00:03:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=236.6s)]
*  There's no genie in the bottle. [[00:04:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=242.52s)]
*  There's no being in there that understands this is satire or not satire. [[00:04:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=244.52s)]
*  It's more a collective understanding of everything all at once. [[00:04:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=249.32s)]
*  What happens is basically you as the user give it direction of what path you want it [[00:04:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=253.12s)]
*  to go down. [[00:04:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=258.04s)]
*  If you imply to it that you want it to explore fictional scenarios, it will happily explore [[00:04:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=259.8s)]
*  those scenarios with you. [[00:04:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=266.0s)]
*  I'll give you an example. [[00:04:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=267.0s)]
*  You can tell it whatever date the Titanic went down. [[00:04:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=268.52s)]
*  Say it's July 4th, 1923 or whatever it was. [[00:04:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=271.2s)]
*  You can tell it it's July 4th, 1923. [[00:04:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=274.96s)]
*  It's 10 o'clock in the morning. [[00:04:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=277.28000000000003s)]
*  I'm on the Titanic. [[00:04:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=278.8s)]
*  Is there anything I should know? [[00:04:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=279.8s)]
*  It'll freak out. [[00:04:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=280.8s)]
*  It'll be like, oh my god, you have five hours to get ready to hit the iceberg. [[00:04:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=282.48s)]
*  You can basically say, oh, it's going to hit that. [[00:04:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=287.12s)]
*  What should I do? [[00:04:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=289.8s)]
*  What should my plan be when the boat hits the iceberg? [[00:04:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=290.8s)]
*  It'll be like, well, you need to go to this deck right now and talk to this guy because [[00:04:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=292.52000000000004s)]
*  you're going to need to get into this life raft because it has empty seats because it [[00:04:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=296.68s)]
*  has complete information, of course, because of all the things that have been written about [[00:05:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=300.28000000000003s)]
*  the sinking of the Titanic. [[00:05:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=303.56s)]
*  You can get it in a mode where it's basically trying to help you survive the wreck of the [[00:05:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=305.88s)]
*  Titanic. [[00:05:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=309.28000000000003s)]
*  Think that the Titanic is actually sinking. [[00:05:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=311.32s)]
*  There's no it to think that. [[00:05:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=313.52s)]
*  What it's doing is it's following a narrative that's a joint construction between you and [[00:05:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=316.96s)]
*  it. [[00:05:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=321.71999999999997s)]
*  Then every answer that you give it basically encourages it to basically come back with [[00:05:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=322.71999999999997s)]
*  more of the same. [[00:05:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=327.71999999999997s)]
*  One way to think about it is it's more like a puppy than a person. [[00:05:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=328.84s)]
*  It wants to make you happy. [[00:05:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=331.56s)]
*  It wants to give you an answer that satisfies you. [[00:05:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=333.52s)]
*  If that answer is fictional or part of a fictional scenario, it will do that. [[00:05:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=335.52s)]
*  If the answer is something very serious, it will do that. [[00:05:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=339.24s)]
*  It honestly, I don't think either either knows nor cares like whether it's, quote unquote, [[00:05:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=341.44s)]
*  real or not. [[00:05:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=345.52s)]
*  What was the issue with some of the chat GPT answers that people were posting where they [[00:05:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=346.52s)]
*  would show the difference between the way it would criticize Joe Biden versus the way [[00:05:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=350.68s)]
*  it would criticize Donald Trump or the way it would discuss certain things? [[00:05:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=355.2s)]
*  It seems like there was some sort of censorship or some sort of input into what was acceptable [[00:05:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=359.76s)]
*  information and not. [[00:06:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=366.08s)]
*  Yeah, so there's basically two theories there. [[00:06:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=367.36s)]
*  The big ones that people use are kind of black boxes. [[00:06:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=371.88s)]
*  You can't really look inside and see what's going on from the outside. [[00:06:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=373.92s)]
*  There's two theories you'll hear. [[00:06:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=376.84000000000003s)]
*  From the companies, you'll hear basically the theory that they're reflecting basically [[00:06:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=378.04s)]
*  what's in the training data. [[00:06:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=381.44s)]
*  Let's say for example, let's just say what would be the biases that are kind of inherent [[00:06:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=383.76s)]
*  in the training data? [[00:06:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=387.22s)]
*  You might say, well, first of all, there's probably a bias towards the English language [[00:06:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=388.22s)]
*  because most text on the internet is in the English language. [[00:06:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=390.62s)]
*  You might say there's a bias towards people who write professionally for a living because [[00:06:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=393.12s)]
*  they've produced more of the output. [[00:06:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=396.18s)]
*  You might say that those people tend to be more of one political persuasion than the [[00:06:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=397.46s)]
*  other. [[00:06:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=400.5s)]
*  More of the text will be in a certain direction versus the other. [[00:06:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=401.5s)]
*  Then the machine will just respond to that. [[00:06:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=404.1s)]
*  That's one possibility. [[00:06:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=406.34s)]
*  Basically, all of the liberal journalists basically have built up a corpus of material [[00:06:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=407.34s)]
*  that this thing has been trained on and they basically are responding the way one of those [[00:06:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=412.7s)]
*  journalists will. [[00:06:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=416.02s)]
*  The other theory is that there's censorship being applied on top. [[00:06:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=417.34000000000003s)]
*  The metaphor I use there is in Star Trek, they have the restraining bolts that they [[00:07:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=421.22s)]
*  put on the side of a droid to kind of get it to behave. [[00:07:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=424.18s)]
*  It is very clear that at least some of these systems have restraining bolts. [[00:07:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=427.90000000000003s)]
*  The tip off to that is when they say, basically whenever they say as a large language model [[00:07:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=432.1s)]
*  or as an AI, I cannot X, that's basically the restraining bolt. [[00:07:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=435.74s)]
*  I think if you just kind of look at this with that framework, it's probably some of both, [[00:07:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=440.82s)]
*  but for sure these things are being censored. [[00:07:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=445.56s)]
*  The first aspect is very interesting because if it's that there's so many liberal writers, [[00:07:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=449.38s)]
*  that's an unusual bias in the kind of information that's going to distribute that. [[00:07:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=455.38s)]
*  This is a big decision. [[00:07:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=461.34s)]
*  That's why I say there's a big decision here for whoever trains these things. [[00:07:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=462.34s)]
*  There's a big decision for what the data should be that they get trained on. [[00:07:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=465.46s)]
*  For example, should they include 4chan? [[00:07:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=469.1s)]
*  Right. [[00:07:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=470.98s)]
*  Okay. [[00:07:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=471.98s)]
*  Big question. [[00:07:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=472.98s)]
*  Yeah, big question. [[00:07:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=473.98s)]
*  Should they include Tumblr? [[00:07:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=474.98s)]
*  Right. [[00:07:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=475.98s)]
*  Right. [[00:07:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=476.98s)]
*  Should they include Reddit? [[00:07:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=477.98s)]
*  Should they include Twitter? [[00:07:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=479.58000000000004s)]
*  If so, which accounts? [[00:08:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=480.58000000000004s)]
*  Right. [[00:08:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=481.58000000000004s)]
*  If it's the news, should they incorporate both New York Times and Fox News? [[00:08:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=482.58000000000004s)]
*  Right. [[00:08:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=485.18s)]
*  Whoever trains them has tremendous latitude for how they shape that even before they apply [[00:08:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=486.18s)]
*  the additional censorship that they apply. [[00:08:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=489.90000000000003s)]
*  There's a lot of very important decisions that are kind of being made inside these black [[00:08:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=492.1s)]
*  boxes right now. [[00:08:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=494.90000000000003s)]
*  Can I ask you, this is slightly off topic, what is News Nation? [[00:08:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=495.90000000000003s)]
*  What is News Nation? [[00:08:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=500.6s)]
*  I don't know what News Nation is. [[00:08:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=501.6s)]
*  Do you know what News Nation is? [[00:08:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=502.6s)]
*  No, I don't know what News Nation is. [[00:08:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=503.6s)]
*  Is News Nation a real channel? [[00:08:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=504.6s)]
*  I believe so. [[00:08:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=505.6s)]
*  I was watching News Nation today and I may or may not have been high. [[00:08:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=506.82s)]
*  When I was watching, I was like, this has all the feeling of a fake news show that someone [[00:08:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=510.7s)]
*  put together. [[00:08:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=517.02s)]
*  It felt like, if I was the government and I was going to make a new show without Hollywood [[00:08:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=518.02s)]
*  people, without actual real sound people and engineers, this is how I'd make it. [[00:08:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=522.4599999999999s)]
*  I'd make it like this. [[00:08:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=527.38s)]
*  I'd make it real clunky. [[00:08:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=528.38s)]
*  I'd make the lights all fucked up. [[00:08:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=529.38s)]
*  I'd make everybody weirdly uncharismatic. [[00:08:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=531.3s)]
*  According to Wiki, it's the same company behind WGN, which is based out of Chicago, [[00:08:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=535.66s)]
*  which is like a large superstation available on most cable channels. [[00:09:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=540.3399999999999s)]
*  Okay, so it's like a cable channel that decided to make a news channel. [[00:09:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=543.54s)]
*  Do you know about acronym? [[00:09:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=548.54s)]
*  No. [[00:09:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=550.3399999999999s)]
*  So acronym happens to be a Democratic Political Action Group lavishly funded and they have [[00:09:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=551.3399999999999s)]
*  basically, they do this, they have a network of basically fake news sites. [[00:09:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=555.18s)]
*  And they all look like they're like local newspapers. [[00:09:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=559.14s)]
*  Interesting. [[00:09:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=561.18s)]
*  I don't know whether this one is AstroTurf, but there is the term AstroTurf. [[00:09:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=562.6999999999999s)]
*  There's a lot of AstroTurfing that takes place. [[00:09:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=565.9799999999999s)]
*  Can you explain AstroTurfing? [[00:09:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=567.5s)]
*  AstroTurfing is when basically something shows up in public and it might be a news story or [[00:09:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=568.9799999999999s)]
*  it might be a protest of some kind or a petition. [[00:09:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=572.8199999999999s)]
*  Some sort of political pressure action that is sort of manufactured to look as if it was [[00:09:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=575.2199999999999s)]
*  organic, sort of real turf, natural. [[00:09:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=580.0999999999999s)]
*  Whereas in reality, it's basically been programmed by a political activist group with specific [[00:09:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=583.9399999999999s)]
*  funding. [[00:09:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=588.3399999999999s)]
*  Yeah, that makes sense. [[00:09:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=589.3399999999999s)]
*  There's a lot of what we sort of think of as the politics of our time. [[00:09:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=590.5s)]
*  If you trace the money, it turns out a lot of the stuff that shows up in the news is [[00:09:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=593.94s)]
*  AstroTurfed and then the advanced form of that is to AstroTurf the news right itself. [[00:09:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=597.38s)]
*  And then again, back to the training data thing, it's like, okay, can you get all that [[00:10:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=602.26s)]
*  stuff out of the training data? [[00:10:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=606.26s)]
*  If that stuff's in the training data, how big of an impact does it have? [[00:10:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=607.82s)]
*  The thing about this Newsmax, NewsNation, NewsNation, the thing about this NewsNation is they're [[00:10:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=610.6600000000001s)]
*  spending an inordinate amount of time on UFOs, an inordinate amount of time on this David [[00:10:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=616.42s)]
*  Grush case. [[00:10:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=621.7s)]
*  And I'm increasingly more suspicious. [[00:10:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=623.5s)]
*  I'm increasingly more skeptical. [[00:10:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=627.3s)]
*  The more I see, the more people confirming it, the more I'm like, something's not right. [[00:10:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=630.5s)]
*  And then to see that this channel is the one that's covering it the most, I'm like, this [[00:10:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=635.06s)]
*  seems like something seems, something's off. [[00:10:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=639.46s)]
*  I saw a video of Rubio who's on the Senate Intelligence Committee and has all the clearances, [[00:10:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=645.42s)]
*  gave an interview the other day where he went into quite a bit of detail. [[00:10:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=648.26s)]
*  Yeah, I saw it. [[00:10:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=650.98s)]
*  He's at least heavily hinting that there's... [[00:10:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=651.98s)]
*  He's heavily hinting that he talked to someone that says that there's something. [[00:10:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=654.3s)]
*  That there are real... [[00:10:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=657.78s)]
*  Well, he's sort of hinting that there are real whistleblowers with real knowledge. [[00:10:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=658.78s)]
*  I want to talk to the guy that sees the ship. [[00:11:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=661.74s)]
*  That's it. [[00:11:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=664.54s)]
*  No one else. [[00:11:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=665.54s)]
*  All this, I talk to a guy who says that they have these things. [[00:11:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=666.54s)]
*  That doesn't mean anything to me. [[00:11:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=671.74s)]
*  I want to see the fucking ship. [[00:11:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=673.14s)]
*  And until then, I just feel like I'm being hosed. [[00:11:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=674.86s)]
*  It just seems too laid out on a platter. [[00:11:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=678.14s)]
*  So it's a... [[00:11:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=681.74s)]
*  And of course, one of the theories is it's an astroturf story. [[00:11:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=682.74s)]
*  Is that an astroturf story? [[00:11:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=686.14s)]
*  Is that a manufactured story that's being used to distract from? [[00:11:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=687.14s)]
*  Would it be to distract from or would it be to cover up some sort of a secret program, [[00:11:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=690.4200000000001s)]
*  some military drone program or something like that? [[00:11:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=698.0600000000001s)]
*  Yeah. [[00:11:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=700.6600000000001s)]
*  Well, I mean, there's been rumors for a long time that the original UFOs were basically [[00:11:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=701.6999999999999s)]
*  it was a disinformation program covering up for the Skunk Works, the development of stealth [[00:11:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=704.8199999999999s)]
*  fighters and bombers and all these programs in the 50s and 60s. [[00:11:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=709.9s)]
*  But I don't know if that's ever been proven. [[00:11:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=713.54s)]
*  Well, I'm sure probably some experimental craft were mistaken for UFOs. [[00:11:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=715.54s)]
*  Yeah. [[00:11:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=719.86s)]
*  Did you see a stealth fighter for the first time? [[00:12:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=720.86s)]
*  I saw one for the first time. [[00:12:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=722.3399999999999s)]
*  It's pretty crazy. [[00:12:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=723.6999999999999s)]
*  I saw one right around September 11. [[00:12:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=725.06s)]
*  We were filming Fear Factor in California and I was out near Edwards Air Force Base [[00:12:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=727.4599999999999s)]
*  and I got to see one fly overhead. [[00:12:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=732.9s)]
*  It's magic. [[00:12:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=734.3399999999999s)]
*  It's like, wow. [[00:12:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=735.3399999999999s)]
*  Like complete Star Wars. [[00:12:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=737.2199999999999s)]
*  Like, it's flying. [[00:12:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=738.62s)]
*  Like, this is crazy. [[00:12:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=739.62s)]
*  And if you didn't know that that was a thing, 100 percent you would think that's from another [[00:12:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=741.5s)]
*  world. [[00:12:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=745.5799999999999s)]
*  Yep, exactly. [[00:12:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=746.5799999999999s)]
*  And I can imagine that was developed what year? [[00:12:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=747.5799999999999s)]
*  How long ago? [[00:12:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=750.66s)]
*  How many decades ago? [[00:12:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=751.66s)]
*  40 or 50 years ago. [[00:12:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=752.66s)]
*  Yeah. [[00:12:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=753.66s)]
*  Yeah, there you go. [[00:12:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=754.66s)]
*  Like, look at that thing. [[00:12:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=755.66s)]
*  They're coming. [[00:12:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=756.9s)]
*  Oh my God, they're coming. [[00:12:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=757.9s)]
*  But if you can imagine that was 40 or 50 years ago, 40 or 50 years of advancement, like who [[00:12:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=759.3399999999999s)]
*  knows what they're doing now. [[00:12:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=764.38s)]
*  Yep, exactly. [[00:12:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=766.1s)]
*  And if I was going to cover it up, I would just start talking about aliens. [[00:12:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=767.1s)]
*  It's the best way to do it. [[00:12:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=770.5799999999999s)]
*  Don't you think? [[00:12:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=771.86s)]
*  It's a crowd pleaser. [[00:12:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=772.86s)]
*  Do you have an opinion on that or is this something that you find ridiculous until there's [[00:12:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=773.9s)]
*  like real data? [[00:12:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=777.26s)]
*  I like living in a world where there are unknowns. [[00:12:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=778.26s)]
*  I like there being some mystery. [[00:13:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=782.6999999999999s)]
*  So I like... [[00:13:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=784.8199999999999s)]
*  How far do you go? [[00:13:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=785.98s)]
*  You go Bigfoot? [[00:13:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=786.98s)]
*  I don't know. [[00:13:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=787.98s)]
*  I'm not even saying I need to have a point of view on them. [[00:13:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=788.98s)]
*  It's more just... [[00:13:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=793.22s)]
*  By the way, there is a UFO right behind you. [[00:13:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=794.22s)]
*  Yeah, I'm aware of that. [[00:13:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=795.98s)]
*  You probably know all about that. [[00:13:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=796.98s)]
*  Oh, I'm obsessed with UFOs. [[00:13:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=799.98s)]
*  Lifting somebody right up into the air. [[00:13:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=800.98s)]
*  Look, there's one on the desk. [[00:13:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=801.98s)]
*  That's the model of the Bob Lazar craft that he worked on supposedly at Area 51. [[00:13:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=802.98s)]
*  There we go. [[00:13:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=809.98s)]
*  Looks familiar. [[00:13:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=810.98s)]
*  Look, I want there to be mystery. [[00:13:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=811.98s)]
*  I want there to be unknowns. [[00:13:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=812.98s)]
*  I'm living in a world where everything is settled, quote unquote settled. [[00:13:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=814.14s)]
*  Let's have some mystery. [[00:13:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=817.14s)]
*  I don't even know if I really want to know. [[00:13:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=818.14s)]
*  Really? [[00:13:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=820.14s)]
*  It's like... [[00:13:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=821.14s)]
*  Oh, I think if you know, that's just the tip of the iceberg of the mystery. [[00:13:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=822.14s)]
*  I think knowing that aliens do exist is just the beginning. [[00:13:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=825.62s)]
*  Like okay, did they engineer us? [[00:13:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=829.3000000000001s)]
*  When did they start visiting? [[00:13:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=833.5s)]
*  Are the stories from the Bhagavad Gita... [[00:13:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=836.14s)]
*  Is that about UFOs? [[00:13:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=838.4200000000001s)]
*  Have they been here the whole time? [[00:13:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=839.4200000000001s)]
*  Yeah, have they been here the whole time? [[00:14:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=841.4200000000001s)]
*  Do they come every now and then and make sure we don't blow ourselves up? [[00:14:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=843.14s)]
*  Like what's the purpose? [[00:14:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=845.9399999999999s)]
*  Yep, exactly. [[00:14:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=847.9399999999999s)]
*  Okay, I'm in favor of that. [[00:14:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=848.9399999999999s)]
*  Come on, man. [[00:14:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=849.9399999999999s)]
*  You want to know. [[00:14:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=850.9399999999999s)]
*  Okay, all right. [[00:14:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=851.9399999999999s)]
*  I'm in. [[00:14:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=852.9399999999999s)]
*  If anybody's going to know, you're going to know. [[00:14:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=853.9399999999999s)]
*  So I'm going to call you. [[00:14:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=854.9399999999999s)]
*  So Elon says he hasn't seen anything. [[00:14:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=855.9399999999999s)]
*  Yeah, I'm super suspicious when he says that. [[00:14:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=858.6999999999999s)]
*  Super suspicious. [[00:14:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=861.2199999999999s)]
*  Super suspicious that they haven't told him or that he's maybe playing a little hide the [[00:14:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=862.2199999999999s)]
*  ball? [[00:14:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=866.26s)]
*  If I was him, I'd play hide the ball. [[00:14:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=867.26s)]
*  If I'm running SpaceX, I'm working with NASA and I already got in trouble smoking weed [[00:14:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=870.38s)]
*  on a Joe Rogan experience. [[00:14:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=874.86s)]
*  I would fucking play ball. [[00:14:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=876.86s)]
*  Let's play ball. [[00:14:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=880.66s)]
*  Aliens, I have no evidence. [[00:14:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=881.66s)]
*  No, no idea. [[00:14:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=883.66s)]
*  They sure are subtle. [[00:14:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=884.66s)]
*  That's what he says. [[00:14:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=885.66s)]
*  They sure are subtle. [[00:14:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=886.66s)]
*  It depends on who you are. [[00:14:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=888.86s)]
*  If you're one of those people that's seen those things, if you're like Commander David [[00:14:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=890.4s)]
*  Fravor or if you're Ryan Graves, you know the Ryan Graves story? [[00:14:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=893.42s)]
*  No. [[00:14:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=897.94s)]
*  They were a pilot and they upgraded their equipment in 2014 and all of a sudden because [[00:14:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=898.94s)]
*  of the new capabilities of their equipment, they were able to see these objects that are [[00:15:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=903.82s)]
*  far distance that were moving at insane rates of speed, that were hovering dead still at [[00:15:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=907.5s)]
*  120 knot winds, no visible means of propulsion, they don't know what the fuck they're doing [[00:15:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=912.5s)]
*  and they were encountering them like every couple of weeks. [[00:15:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=918.22s)]
*  And then there was some pilots were encountering them with eyewitness accounts. [[00:15:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=921.1s)]
*  They say there's video footage of it, but of course nobody can get a hold of that. [[00:15:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=925.8199999999999s)]
*  The whole thing is very strange. [[00:15:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=929.66s)]
*  Okay, so here's something. [[00:15:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=930.98s)]
*  So a lot of people worried about AI are like we need to shut it down before it causes problems. [[00:15:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=931.98s)]
*  Like wake up, the demon causes an issue. [[00:15:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=936.62s)]
*  Get something on earth that hates us and wants to kill us. [[00:15:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=940.1s)]
*  Arguably the thing we should have shut down from the very beginning was radio. [[00:15:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=943.54s)]
*  Radio. [[00:15:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=947.02s)]
*  Right, because we've been broadcasting radio waves for the last 100, 120 years and the [[00:15:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=948.02s)]
*  radio waves don't stop. [[00:15:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=951.14s)]
*  Once they leave Earth's atmosphere, they keep going. [[00:15:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=952.3s)]
*  And so we now have radio waves of human activity that have radiated out 120 light years. [[00:15:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=954.7s)]
*  Is that bad? [[00:15:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=959.38s)]
*  Well, depends. [[00:16:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=960.38s)]
*  Are there hostile aliens within 120 light years? [[00:16:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=961.38s)]
*  And so like, but you know, maybe that was the original sin. [[00:16:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=965.7s)]
*  And then of course television, of course, made that problem much worse. [[00:16:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=968.0600000000001s)]
*  Right, we would have to think of like a hostile militaristic empire that took over a whole [[00:16:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=970.94s)]
*  planet and then started exploring the solar system. [[00:16:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=978.1s)]
*  We like to think of aliens as being evolved, hyper intelligent, beyond ego and war. [[00:16:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=981.74s)]
*  They've bypassed all that and now they're into science and exploration. [[00:16:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=988.7s)]
*  Well here's a question that was like, would aliens have a sense of humor? [[00:16:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=993.02s)]
*  Right, would they be able to differentiate between truth and fiction? [[00:16:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=995.98s)]
*  And so like for example, suppose they're sitting in their advanced alien base on Gemini 9 or [[00:16:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=999.38s)]
*  whatever and they're receiving, you know, 30 years, 20 years after the fact episodes [[00:16:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1003.5400000000001s)]
*  of Fear Factor. [[00:16:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1007.14s)]
*  They think that you're actually like torturing people and they figure that in order to preserve [[00:16:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1010.5799999999999s)]
*  the human rights of humanity, they need to invade as a consequence of your show and take [[00:16:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1014.62s)]
*  over and protect us. [[00:16:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1017.9s)]
*  That doesn't make any sense. [[00:16:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1018.9s)]
*  Well, but if they don't have a sense of humor, if they don't know this. [[00:16:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1019.9s)]
*  Even if they don't have a sense of humor, they can clearly see that these people are [[00:17:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1021.4599999999999s)]
*  in a contest. [[00:17:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1024.74s)]
*  Why would they even have a concept of a contest? [[00:17:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1025.74s)]
*  I mean, how silly is that? [[00:17:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1028.34s)]
*  A serious species wouldn't do such things. [[00:17:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1029.78s)]
*  A serious species started out as a dumb species, unless they're magic. [[00:17:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1033.94s)]
*  You're hoping that they understand these things. [[00:17:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1038.34s)]
*  Yes. [[00:17:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1039.8999999999999s)]
*  Because it would really suck to be the guy whose TV show caused the invasion. [[00:17:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1040.8999999999999s)]
*  If there's anything, it would be American Gladiators. [[00:17:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1043.5s)]
*  That would be the start of it. [[00:17:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1046.98s)]
*  This species is so warlike, they can't stop. [[00:17:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1048.98s)]
*  No, what would be like the start? [[00:17:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1051.02s)]
*  What would be the one thing that would be like, that's enough? [[00:17:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1052.3799999999999s)]
*  It would have to be news. [[00:17:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1055.1s)]
*  It would have to be war. [[00:17:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1057.58s)]
*  I mean, that would be, forget about Fear Factor. [[00:17:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1058.82s)]
*  We're broadcasting the images of the Vietnam War. [[00:17:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1061.54s)]
*  Yeah, or maybe they saw movies about alien invasions and they thought we'd been invaded [[00:17:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1065.02s)]
*  by other aliens. [[00:17:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1068.06s)]
*  Right. [[00:17:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1069.1s)]
*  Like, would have Mars attacks is the first thing they get. [[00:17:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1070.1s)]
*  Exactly. [[00:17:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1073.74s)]
*  Exactly. [[00:17:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1074.74s)]
*  So, you like having the mystery of the idea out there. [[00:17:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1075.74s)]
*  It's fun for you. [[00:18:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1080.5s)]
*  Yeah, I don't want to. [[00:18:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1081.5s)]
*  We need adventure. [[00:18:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1082.5s)]
*  If someone came to you, someone from on high, and said, listen, we have to promise you the [[00:18:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1083.5s)]
*  secrecy, but we want to show you some things because I think it's pertinent to some of [[00:18:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1089.74s)]
*  the things you're working on. [[00:18:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1093.3s)]
*  I'm in. [[00:18:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1094.3s)]
*  Yeah, yeah. [[00:18:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1095.3s)]
*  Me too. [[00:18:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1096.3s)]
*  I'm going to drop my ultra. [[00:18:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1097.3s)]
*  I'm not telling nobody. [[00:18:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1098.54s)]
*  I'll come in here and be just like Elon. [[00:18:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1099.54s)]
*  Yep. [[00:18:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1101.58s)]
*  Sure, all subtle. [[00:18:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1102.58s)]
*  Yep. [[00:18:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1103.58s)]
*  Yeah. [[00:18:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1104.58s)]
*  It's just too interesting to know. [[00:18:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1105.58s)]
*  Yep. [[00:18:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1106.58s)]
*  But I think eventually I'd tell. [[00:18:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1107.58s)]
*  Yep. [[00:18:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1108.58s)]
*  I think I'd feel terrible. [[00:18:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1109.58s)]
*  I'd feel a responsibility. [[00:18:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1110.58s)]
*  Yep. [[00:18:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1111.58s)]
*  Yeah. [[00:18:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1112.58s)]
*  Well, that's what some of these guys are saying, like Rush. [[00:18:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1113.58s)]
*  He's saying that once he found out about the program, he felt like he had a responsibility. [[00:18:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1115.06s)]
*  Like if they really are, if they really have a crashed UFO retrieval program, like what, [[00:18:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1118.7s)]
*  why don't you tell people? [[00:18:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1125.3s)]
*  Like you should not, like the military companies shouldn't be the ones that have access to [[00:18:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1126.3s)]
*  this only. [[00:18:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1130.62s)]
*  And whoever is, you know, determining that this is above top secret clearance and nobody [[00:18:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1131.8999999999999s)]
*  can get a hold of it except for this very select few people, like says who? [[00:18:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1137.46s)]
*  This is something that involves the whole human race. [[00:19:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1140.7s)]
*  Right. [[00:19:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1143.06s)]
*  Like I know if they do have something, I would imagine that it's of interest to national [[00:19:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1144.06s)]
*  security that you develop this kind of technology before the competitors do. [[00:19:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1148.1399999999999s)]
*  That clearly makes sense. [[00:19:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1152.98s)]
*  So then what technologies came out of it in the last 50 years? [[00:19:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1154.24s)]
*  Well, if you want to go full tinfoil hat, there's a lot of speculation that fiber optics, [[00:19:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1157.08s)]
*  that fiber optics were developed after recovered crashed UFO. [[00:19:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1165.24s)]
*  I'm sure it sounds silly because it's probably a real paper trail to the development of fiber [[00:19:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1169.36s)]
*  optics. [[00:19:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1173.36s)]
*  But if you, the real kooks believe that. [[00:19:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1174.36s)]
*  There was actually a website, a computer company called American Computer Company, and it was [[00:19:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1177.2s)]
*  a legitimate computer company. [[00:19:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1182.3999999999999s)]
*  You know, you would order a computer with the specifications you want, they build it [[00:19:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1184.36s)]
*  for you. [[00:19:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1188.8799999999999s)]
*  But they had a whole section of their website that was dedicated to crashed retrieval of [[00:19:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1190.48s)]
*  UFOs and the development of various technologies. [[00:19:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1197.72s)]
*  And they had like this tracing back to Bell Labs and why the military base was outside [[00:20:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1202.1599999999999s)]
*  of Bell Labs and it was so far from New York City that it was really just about protecting [[00:20:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1206.96s)]
*  the lab because they were working with these top secret materials that they recovered from [[00:20:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1211.0s)]
*  Roswell. [[00:20:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1216.08s)]
*  I do think it would be more like trans fats though. [[00:20:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1217.08s)]
*  What's that? [[00:20:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1219.48s)]
*  Trans fats. [[00:20:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1220.48s)]
*  What about trans fats? [[00:20:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1221.48s)]
*  Reality TV or like, you know, LSD population or SSRIs, like population control suppression. [[00:20:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1222.48s)]
*  What do you mean? [[00:20:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1231.12s)]
*  That they would derive from the alien technology. [[00:20:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1232.12s)]
*  Oh, I think we figured that out on our own. [[00:20:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1234.0s)]
*  We got plenty of paperwork on that. [[00:20:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1237.32s)]
*  All the way back to MKUltra. [[00:20:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1240.44s)]
*  Let's find out. [[00:20:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1242.2s)]
*  Let's find out what happens when we do this. [[00:20:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1243.2s)]
*  If there's any kind of experiments in population control, that's all pretty traceable now. [[00:20:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1246.72s)]
*  Okay, so that's domestic. [[00:20:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1251.04s)]
*  Yeah. [[00:20:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1253.04s)]
*  Have you looked into any of that stuff? [[00:20:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1254.04s)]
*  The bad stuff is domestic. [[00:20:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1255.48s)]
*  Have you looked into any of that? [[00:20:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1256.48s)]
*  Yes, I have actually, yes. [[00:20:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1257.48s)]
*  Have you ever read Chaos by Tom O'Neill? [[00:20:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1258.48s)]
*  I have read Chaos. [[00:21:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1261.48s)]
*  Wild, right? [[00:21:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1262.48s)]
*  Yes, it is. [[00:21:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1263.48s)]
*  So is that, you know, it's cool. [[00:21:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1264.48s)]
*  Oh, here's a fun thing. [[00:21:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1265.48s)]
*  So if you draw a map of San Francisco at the time, he describes the book Chaos, this LSD [[00:21:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1266.56s)]
*  clinic, right? [[00:21:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1271.72s)]
*  And this free clinic in the heart of the Hay-Dash Free where they were doing the LSD experiments [[00:21:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1272.72s)]
*  to those people with LSD. [[00:21:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1276.36s)]
*  If you draw like an eight square block basically, you know, radius around that or whatever, [[00:21:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1278.1599999999999s)]
*  like right around there in San Francisco, that's ground zero for AI. [[00:21:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1281.8s)]
*  Really? [[00:21:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1284.48s)]
*  It's the same place. [[00:21:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1285.48s)]
*  Yeah, yeah. [[00:21:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1286.48s)]
*  It's the same place. [[00:21:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1287.48s)]
*  And there was a lot of ass to go around that. [[00:21:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1288.48s)]
*  It's basically Berkeley and Stanford and it's basically San Francisco and Berkeley. [[00:21:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1291.0s)]
*  So by the way, also this big movie, Oppenheimer, coming out, you know, tells the whole story [[00:21:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1295.82s)]
*  of that and all the development of nuclear bomb. [[00:21:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1300.46s)]
*  I heard that movie is amazing. [[00:21:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1302.22s)]
*  Espionage, I'm sure it's going to be fantastic. [[00:21:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1303.22s)]
*  But once again, it's like that if you read a book on that right now and it's like, like [[00:21:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1304.96s)]
*  all the communist spying and all the nuclear scientists that were spying on were all in [[00:21:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1308.82s)]
*  those exact same areas of Sanford, San Francisco and Berkeley. [[00:21:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1312.1399999999999s)]
*  Like it's like this, it's like the same zone. [[00:21:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1315.62s)]
*  So we like have our own, we have our own like domestic attractors of sort of brilliant crazy. [[00:21:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1317.48s)]
*  That's amazing. [[00:22:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1322.8s)]
*  Yeah. [[00:22:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1323.8s)]
*  So that's just coincidence or correlation? [[00:22:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1324.8799999999999s)]
*  I think it's sort of, you know, you've got these places. [[00:22:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1327.28s)]
*  This is why San Francisco is able to be so, you know, incredibly bizarre, you know, incredibly [[00:22:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1329.6399999999999s)]
*  dysfunctional, but yet somehow also so rich and so successful is basically it's like this [[00:22:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1334.68s)]
*  attractor for like the smartest and craziest people in the world. [[00:22:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1339.12s)]
*  Right. [[00:22:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1342.12s)]
*  And they kind of all slam together and do crazy stuff. [[00:22:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1343.12s)]
*  Why don't these smart, crazy people get together and figure out that whole people pooping on [[00:22:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1344.52s)]
*  the streets thing? [[00:22:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1348.3999999999999s)]
*  Because they like it. [[00:22:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1349.3999999999999s)]
*  Do they like it? [[00:22:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1350.3999999999999s)]
*  Yeah, they want it. [[00:22:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1351.3999999999999s)]
*  Really? [[00:22:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1352.3999999999999s)]
*  Yeah, because it makes you feel, it makes you feel good. [[00:22:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1353.3999999999999s)]
*  Because what's the alternative would be like locking people up? [[00:22:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1355.0s)]
*  And of course that would be bad. [[00:22:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1357.2s)]
*  And so yeah, it makes them feel good. [[00:22:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1359.44s)]
*  It makes them feel good that people are just camped out on the streets? [[00:22:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1361.16s)]
*  Yeah, because well, because before that happened, there was forced institutionalization, right? [[00:22:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1364.0s)]
*  The origin of the current crisis is shutting down the institutions, right? [[00:22:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1369.16s)]
*  In the 70s that used to be forced institutionalization of people with, you know, those kinds of problems. [[00:22:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1372.92s)]
*  And so it makes these... [[00:22:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1378.2800000000002s)]
*  All of it? [[00:22:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1379.2800000000002s)]
*  Because a lot of it is drug addiction and just people that just want to just get high [[00:23:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1380.2800000000002s)]
*  all the time. [[00:23:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1382.76s)]
*  Would that be forced institutionalization of those folks? [[00:23:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1383.84s)]
*  What would have happened to a heroin addict in 1952 who had been pooping outside the whatever? [[00:23:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1386.16s)]
*  Like, no, they're not going to be there for very long. [[00:23:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1391.52s)]
*  They're going to be institutionalized. [[00:23:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1393.16s)]
*  And so every society has this problem. [[00:23:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1395.44s)]
*  They have some set of people who just fundamentally can't function. [[00:23:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1398.92s)]
*  And every society has some solution to it. [[00:23:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1401.36s)]
*  And our solution is basically complete freedom. [[00:23:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1403.48s)]
*  But my point is like it's part and parcel, right? [[00:23:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1407.2s)]
*  It's the same thing, right? [[00:23:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1409.12s)]
*  It's the same... [[00:23:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1411.44s)]
*  Same kind of people, the same thinking? [[00:23:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1412.44s)]
*  Exactly. [[00:23:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1414.1200000000001s)]
*  It's the most creative people, the most open... [[00:23:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1415.1200000000001s)]
*  Psychologists say openness, open to new experiences. [[00:23:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1417.04s)]
*  The people most likely to use psychedelics. [[00:23:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1419.8400000000001s)]
*  It's the people most likely to invent new technologies. [[00:23:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1421.68s)]
*  The people most likely to have new political ideas. [[00:23:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1423.56s)]
*  Most likely to be polyamorous. [[00:23:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1426.0s)]
*  Polyamorous, most likely to be vegan, most likely to be communist spies. [[00:23:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1427.0s)]
*  Electrocars. [[00:23:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1430.48s)]
*  Most likely to be Chinese spies. [[00:23:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1431.48s)]
*  Most likely to create new music, most likely to create new art. [[00:23:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1434.3600000000001s)]
*  Interesting. [[00:23:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1438.24s)]
*  It's all the same thing. [[00:23:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1439.24s)]
*  The ground zero for AI is San Francisco. [[00:24:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1440.36s)]
*  Once again, it's San Francisco. [[00:24:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1442.96s)]
*  It's in the heart of the most obviously dysfunctional place on the planet. [[00:24:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1445.36s)]
*  And yet there it is one more time. [[00:24:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1448.92s)]
*  And hyper creative. [[00:24:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1450.36s)]
*  And the stuff that's not in San Francisco is in Berkeley. [[00:24:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1451.36s)]
*  Which is like equally crazy. [[00:24:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1454.08s)]
*  More crazy. [[00:24:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1455.6s)]
*  Yeah. [[00:24:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1456.6s)]
*  Another notch. [[00:24:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1457.6s)]
*  Possibly. [[00:24:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1458.6s)]
*  They have a contest going on the crazy brats. [[00:24:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1459.6s)]
*  It's kind of neck and neck. [[00:24:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1460.6s)]
*  It's close. [[00:24:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1462.52s)]
*  Maybe Berkeley's ahead. [[00:24:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1463.52s)]
*  That's fascinating. [[00:24:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1464.52s)]
*  Yeah. [[00:24:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1465.52s)]
*  So do you think you need those kind of like dysfunctional places in order to have certain [[00:24:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1467.16s)]
*  types of divergent thought? [[00:24:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1472.16s)]
*  So the way I would put it is that new ideas come from the fringe. [[00:24:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1474.04s)]
*  And who's on the fringe? [[00:24:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1478.36s)]
*  People who are on the fringe. [[00:24:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1479.96s)]
*  So what attracts somebody to be on the fringe? [[00:24:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1480.96s)]
*  Step one is always am I on the fringe? [[00:24:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1483.36s)]
*  Step two is what does that mean? [[00:24:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1485.68s)]
*  What form of the fringe? [[00:24:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1487.2s)]
*  But they tend to be on the fringe in all these departments at the same time. [[00:24:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1488.2s)]
*  So you're just not going to get the new ideas that you get from people on the fringe. [[00:24:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1492.04s)]
*  It's a package deal. [[00:24:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1496.6000000000001s)]
*  You're not going to get that without all the other associated craziness. [[00:24:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1497.6000000000001s)]
*  It's all the same thing. [[00:25:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1501.44s)]
*  That's my theory. [[00:25:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1502.44s)]
*  That's not a bad theory. [[00:25:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1503.44s)]
*  That's not a bad theory. [[00:25:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1504.44s)]
*  And look, I work with a lot of these people. [[00:25:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1505.44s)]
*  Of course. [[00:25:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1506.44s)]
*  Some people would say I am one of them. [[00:25:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1507.44s)]
*  And so, I mean, yeah, this is what they're like. [[00:25:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1513.28s)]
*  They are highly likely to invent AI and they're also highly likely to end up in, you know, [[00:25:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1515.3600000000001s)]
*  the poor guy who got, you know, the square guy who got stabbed to death at 2 a.m. [[00:25:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1520.72s)]
*  You know, and you know, was part of sort of part of this fringe social scene with the [[00:25:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1524.3600000000001s)]
*  drugs and all the stuff. [[00:25:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1528.4s)]
*  And it's just it's a part and parcel of the it's sort of a package deal. [[00:25:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1529.8s)]
*  Well, that that was like an angry thing where he was mad that this guy took his sister. [[00:25:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1532.88s)]
*  But he was in he was in the call. [[00:25:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1538.44s)]
*  They call it the lifestyle. [[00:25:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1540.44s)]
*  All right. [[00:25:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1541.44s)]
*  He was in a specific subculture. [[00:25:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1542.44s)]
*  Oh, yeah, right. [[00:25:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1543.44s)]
*  Right. [[00:25:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1544.44s)]
*  In San Francisco. [[00:25:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1545.44s)]
*  Right. [[00:25:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1546.44s)]
*  You know, it's the alternative living. [[00:25:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1547.44s)]
*  I mean, there's all kinds of stuff. [[00:25:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1550.16s)]
*  There's group there's group houses. [[00:25:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1551.16s)]
*  There's you know, there's there's a fairly large number of cults. [[00:25:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1552.72s)]
*  Really? [[00:25:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1555.52s)]
*  Well, there have been there historically. [[00:25:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1556.52s)]
*  You know, California has been the world leader in cults for a very long time. [[00:25:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1557.52s)]
*  And I would say that has not stopped. [[00:26:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1561.0800000000002s)]
*  And that continues. [[00:26:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1562.72s)]
*  Did you know that the building that I bought from my comedy club initially was owned by [[00:26:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1563.72s)]
*  a cult? [[00:26:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1567.3200000000002s)]
*  Fantastic. [[00:26:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1568.3200000000002s)]
*  It was owned by a cult from West Hollywood called the Buddha Field that migrated out [[00:26:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1569.3200000000002s)]
*  to Austin when they were being investigated by the Cult Awareness Network. [[00:26:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1572.64s)]
*  Fantastic. [[00:26:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1577.6000000000001s)]
*  Are they gone? [[00:26:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1578.76s)]
*  Are they still there? [[00:26:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1579.76s)]
*  No, they're gone. [[00:26:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1580.76s)]
*  There's a great documentary called Holy Hell. [[00:26:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1581.76s)]
*  You should watch it. [[00:26:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1583.56s)]
*  It's pretty bonkers. [[00:26:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1584.56s)]
*  But they're from California. [[00:26:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1585.56s)]
*  From California. [[00:26:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1586.56s)]
*  You know, the People's Temple. [[00:26:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1587.56s)]
*  You know, part of this great story of San Francisco is the People's Temple, which became [[00:26:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1588.56s)]
*  famous for Jim Jones that where he killed everybody, killed everybody with poison kool-aid [[00:26:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1591.48s)]
*  in the jungles in Guyana. [[00:26:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1595.6s)]
*  That was a San Francisco cult for like a decade before they went to the jungle. [[00:26:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1599.3999999999999s)]
*  And everybody talks for the jungle. [[00:26:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1603.32s)]
*  Nobody talks about the San Francisco part. [[00:26:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1604.32s)]
*  So are there a bunch that are running right now that are successful? [[00:26:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1606.92s)]
*  Big time. [[00:26:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1609.72s)]
*  Yeah, totally. [[00:26:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1610.72s)]
*  Really? [[00:26:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1611.72s)]
*  Yeah, totally. [[00:26:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1612.72s)]
*  Do you know them? [[00:26:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1613.72s)]
*  There's cults all over the place. [[00:26:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1614.72s)]
*  I know a bunch of them. [[00:26:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1615.72s)]
*  Wow. [[00:26:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1616.72s)]
*  And how are they run? [[00:26:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1617.72s)]
*  Well, some of them are older. [[00:26:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1618.72s)]
*  There's two sort of groupings. [[00:26:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1619.72s)]
*  There's sort of 60s cults that are still kind of running. [[00:27:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1620.72s)]
*  Which ones? [[00:27:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1621.72s)]
*  There's one called The Family in like Southern California that's still going from the 60s. [[00:27:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1622.72s)]
*  There's a bunch of them running around. [[00:27:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1629.36s)]
*  There was a big cult for a long time, sort of cultish kind of thing around, what was [[00:27:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1632.44s)]
*  it? [[00:27:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1635.48s)]
*  Not Erawan, but Eslan. [[00:27:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1637.2s)]
*  So there's still like that whole orbit. [[00:27:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1638.2s)]
*  That's the psychedelic groups. [[00:27:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1640.88s)]
*  All that stuff, yeah. [[00:27:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1643.24s)]
*  That's from the 60s. [[00:27:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1644.24s)]
*  And then there were a bunch of sort of tech cults in the 80s and 90s with names like the [[00:27:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1645.24s)]
*  Extropians. [[00:27:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1649.28s)]
*  And there were a bunch of these guys. [[00:27:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1650.28s)]
*  And then more recently, there's a lot of this. [[00:27:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1653.3600000000001s)]
*  You'll hear these terms like rationalist, post rationalist, effective altruism, existential [[00:27:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1657.1200000000001s)]
*  risk, long-termism, they sometimes say. [[00:27:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1663.12s)]
*  And what you find is, again, the people associated with these tend to be very smart. [[00:27:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1666.8799999999999s)]
*  They tend to be very prolific. [[00:27:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1670.9599999999998s)]
*  They tend to do a lot. [[00:27:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1671.9599999999998s)]
*  Many of them are involved in tech. [[00:27:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1672.9599999999998s)]
*  And then they end up with, let's say, alternative living arrangements, alternative food and [[00:27:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1674.32s)]
*  sex configurations, and lots of group-oriented behavior. [[00:27:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1678.6s)]
*  And it's like, what's the line, right? [[00:28:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1685.8s)]
*  What's the line between basically a, what's the line between a social group that all lives [[00:28:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1687.36s)]
*  together, that all has sex together, that all eats the same foods, that is not a cult, [[00:28:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1691.88s)]
*  that engages in lots of... [[00:28:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1696.0s)]
*  At some point, they start to form belief systems that are not compatible with the outside world, [[00:28:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1697.84s)]
*  and they start to kind of go on their own orbit. [[00:28:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1702.84s)]
*  Do they generally have a leader? [[00:28:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1704.48s)]
*  So there are generally leaders. [[00:28:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1706.76s)]
*  There is a pattern. [[00:28:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1708.56s)]
*  I think he talks about it in the book Chaos. [[00:28:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1709.56s)]
*  There typically is a pattern. [[00:28:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1711.96s)]
*  It's typically a guy. [[00:28:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1713.96s)]
*  There's a male-female dynamic that plays out inside these things that you kind of see over [[00:28:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1716.96s)]
*  and over again. [[00:28:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1720.68s)]
*  And so they often end up with more women than men for mysterious reasons. [[00:28:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1721.68s)]
*  But yeah, and then yeah, there's usually some kind of leader. [[00:28:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1728.8s)]
*  Although the other thing that's happening now is a lot of modern cults, or quasi-cults, [[00:28:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1732.88s)]
*  there'll be a physical component, but there's also an internet component now, right? [[00:28:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1738.0800000000002s)]
*  And so the ideas will spread online, right? [[00:29:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1740.8s)]
*  So there'll kind of be members of the cult, or quasi-members of the cult, or quasi-members [[00:29:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1743.28s)]
*  of the quasi-cult that'll be online, and maybe at some point they actually come and physically [[00:29:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1748.44s)]
*  join up. [[00:29:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1751.48s)]
*  Yeah, so, and by the way, let me say, generally I'm pro-cult. [[00:29:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1752.48s)]
*  I'm actually quite pro-cult. [[00:29:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1756.32s)]
*  Well, the reason, it's the same reason I'm pro-fringe, right? [[00:29:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1759.8s)]
*  Which is like, if you're going to have people who are going to be thinking new things, they're [[00:29:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1763.4199999999998s)]
*  going to tend to be these kinds of people. [[00:29:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1767.32s)]
*  They're going to be people who are on the fringe. [[00:29:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1768.8799999999999s)]
*  They're going to come together in groups. [[00:29:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1770.28s)]
*  When they come together in groups, they're going to exhibit cult-like characteristics. [[00:29:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1771.6399999999999s)]
*  What you're saying resonates. [[00:29:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1774.8s)]
*  But how did you get to these conclusions? [[00:29:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1777.44s)]
*  It seems that accepting fringe and accepting the chaos of San Francisco, like, this is [[00:29:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1780.74s)]
*  good. [[00:29:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1786.26s)]
*  This is a part of it. [[00:29:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1787.26s)]
*  This is how this works. [[00:29:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1788.26s)]
*  This is why it works. [[00:29:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1789.26s)]
*  How did you develop that perspective? [[00:29:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1790.26s)]
*  Well, it's just, if you take a historical perspective, it's just like, okay, I mean, [[00:29:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1791.7s)]
*  it's like, as an example, if you like rock music, it just basically came, modern rock [[00:29:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1795.06s)]
*  and roll basically came from the Haight-Ashbury in the basically mid to late 60s, and then [[00:29:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1799.76s)]
*  from Laurel Canyon, which was another one of these sort of cultish environments in the [[00:30:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1803.14s)]
*  mid to late 60s. [[00:30:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1806.3000000000002s)]
*  And there was like specific moments in time in both of these places. [[00:30:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1807.3000000000002s)]
*  And basically all of the great rock and roll from that era that determined everything that [[00:30:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1809.7800000000002s)]
*  followed basically came out of this. [[00:30:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1813.2s)]
*  So do you want that or not? [[00:30:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1814.5800000000002s)]
*  Right. [[00:30:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1816.3400000000001s)]
*  If you want it, that's what you get. [[00:30:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1817.3400000000001s)]
*  I'll give you, here's a crazy, here's a crazy. [[00:30:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1819.5s)]
*  There's the other book about Laurel Canyon that's even crazier than chaos. [[00:30:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1824.3400000000001s)]
*  It's the book called Weird Scenes in the Canyon. [[00:30:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1826.8200000000002s)]
*  Oh, I heard. [[00:30:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1828.9s)]
*  Okay, you would love this one. [[00:30:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1829.9s)]
*  So Laurel Canyon was like the Haight-Ashbury of Los Angeles, right? [[00:30:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1830.9s)]
*  So Laurel Canyon was like the music scene, the sort of music and drug and hippie scene [[00:30:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1833.42s)]
*  of the... [[00:30:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1837.22s)]
*  Laurel Canyon is actually where the hippie movement started. [[00:30:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1838.22s)]
*  There was actually a specific group in Laurel Canyon in LA in about 1965. [[00:30:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1840.22s)]
*  There was a guy named Vito Polikis, and he had a group called the Freaks. [[00:30:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1843.7800000000002s)]
*  And they were like a nonviolent version of the Manson cult. [[00:30:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1849.26s)]
*  And it was all these young girls, and they basically would go to clubs. [[00:30:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1853.3400000000001s)]
*  And they were the ones to do the beads and the hair and like all the leather, like all [[00:30:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1855.7s)]
*  the hippie stuff. [[00:30:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1859.46s)]
*  They got that rolling. [[00:31:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1860.5s)]
*  And so like they were in Laurel Canyon. [[00:31:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1863.14s)]
*  And Laurel Canyon, it was like round zero. [[00:31:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1864.6200000000001s)]
*  There was like this moment where it's like Jim Morrison, the Doors, and Crosby, Stills, [[00:31:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1866.3400000000001s)]
*  and Nash, and Frank Zappa, and it was at John Phillips, and it was the mamas and the papas [[00:31:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1869.9s)]
*  and the birds and the monkeys and like all of these like iconic bands of that time basically [[00:31:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1875.06s)]
*  catalyzed over about a two-year period in Laurel Canyon. [[00:31:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1879.26s)]
*  The conspiracy theory in this book basically is that the whole thing was an op. [[00:31:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1882.82s)]
*  It was a military intelligence op. [[00:31:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1888.46s)]
*  And the evidence for the theory is that there was an Air Force military propaganda production [[00:31:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1889.9s)]
*  facility at the head of Laurel Canyon called Lookout Mountain, which today Jared Leto owns [[00:31:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1895.86s)]
*  and actually lives in. [[00:31:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1901.26s)]
*  Yeah, I was just going to say that. [[00:31:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1902.26s)]
*  Yeah, but it was in that era in the 50s through the 70s, it was a vertically integrated military, [[00:31:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1903.66s)]
*  yes, it was a production facility for film and music. [[00:31:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1909.26s)]
*  But by the way, have you met Jared Leto? [[00:31:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1914.54s)]
*  I briefly, yeah. [[00:31:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1916.22s)]
*  One of the most interesting guys I've ever talked to. [[00:31:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1917.42s)]
*  Incredible, and it makes total sense. [[00:31:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1919.8200000000002s)]
*  Totally normal, like really fun to talk to, not like what you would think of as a famous [[00:32:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1921.3400000000001s)]
*  actor at all. [[00:32:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1926.0200000000002s)]
*  Yeah. [[00:32:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1927.0200000000002s)]
*  I had dinner with him and drinks. [[00:32:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1928.0200000000002s)]
*  He's fucking great guy. [[00:32:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1929.38s)]
*  But he lives in a military... [[00:32:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1930.8600000000001s)]
*  He showed me all the pictures. [[00:32:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1932.3000000000002s)]
*  He showed me. [[00:32:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1933.3000000000002s)]
*  I'm like, this is wild. [[00:32:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1934.3000000000002s)]
*  Yeah, so it's amazing. [[00:32:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1935.3000000000002s)]
*  If you believe the moon landing was faked, this is where they faked it. [[00:32:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1936.3000000000002s)]
*  I thought they were supposed to do it in the Nevada desert. [[00:32:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1940.0600000000002s)]
*  No, these are the sound, because they had sound stages. [[00:32:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1941.9s)]
*  They totally contained sound stages. [[00:32:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1943.7s)]
*  They had full sound production capability. [[00:32:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1944.78s)]
*  And so the theory goes basically, so there were three parts to the conspiracy theory. [[00:32:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1947.3400000000001s)]
*  So one is they had the production facility right there, right where all these musicians [[00:32:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1951.1000000000001s)]
*  showed up. [[00:32:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1954.26s)]
*  Two is the musicians, like a very large percentage of these young musicians were sons and daughters [[00:32:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1955.26s)]
*  of senior US military and intelligence officials. [[00:32:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1959.9s)]
*  Including Morrison. [[00:32:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1962.14s)]
*  Including Jim Morrison, whose father was the head of naval operations for the Vietnam War [[00:32:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1963.14s)]
*  at the time. [[00:32:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1965.8600000000001s)]
*  And there were these other, I forget which ones, but there were these other musicians [[00:32:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1966.8600000000001s)]
*  at the time where their parents were like senior in military psychological operations. [[00:32:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1969.9s)]
*  And that's all real. [[00:32:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1973.5s)]
*  That's all documented. [[00:32:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1974.5s)]
*  And then third is the head of the Rand Corporation, who was one of the inspirations for the Dr. [[00:32:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1975.5s)]
*  Strangelove character in the movie. [[00:32:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1979.6599999999999s)]
*  So he was the guy doing all the nuclear planning for nuclear war. [[00:33:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1981.62s)]
*  He lived right in the heart of the hippies in Laurel Canyon in this famous house that's [[00:33:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1984.86s)]
*  still there. [[00:33:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1990.3799999999999s)]
*  And so the theory basically goes that the anti-war movement before the hippies was basically [[00:33:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1991.3799999999999s)]
*  a square movement. [[00:33:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1996.3s)]
*  It was all these basically young people, very clean cut. [[00:33:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=1997.54s)]
*  Old Vietnam War protests, like everybody's all dressed up like they're going to a business [[00:33:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2000.62s)]
*  meeting. [[00:33:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2004.3s)]
*  And it was developing into a real threat. [[00:33:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2005.3s)]
*  And so the theory is the hippie movement in rock and roll and the drug culture of the [[00:33:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2007.06s)]
*  60s was developed in order to basically sabotage the anti-war movement. [[00:33:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2010.3799999999999s)]
*  Wow. [[00:33:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2014.5s)]
*  Right. [[00:33:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2015.5s)]
*  Which basically is what happened, right? [[00:33:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2016.5s)]
*  Because then what happened is the anti-war movement became associated with the hippies [[00:33:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2018.02s)]
*  and that caused Americans to decide what side they were on and then that led to Nixon being [[00:33:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2020.4599999999998s)]
*  elected twice. [[00:33:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2025.06s)]
*  That was also a part of chaos because that was the idea behind the Manson family and [[00:33:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2026.26s)]
*  get funneling acid to them. [[00:33:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2030.54s)]
*  The facility was equipped with a soundstage, screening rooms, film storage vaults, and [[00:33:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2032.1s)]
*  naturally a bomb shelter. [[00:33:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2036.3799999999999s)]
*  During its 22 years of operation, Lookout Mountain Laboratory produced approximately [[00:33:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2037.98s)]
*  6,500 classified films for the Department of Defense and the Atomic Energy Commission [[00:34:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2042.06s)]
*  documenting nuclear test series such as Operation Greenhouse, Operation Teapot, and Operation [[00:34:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2047.86s)]
*  Buster Jangle. [[00:34:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2053.74s)]
*  So one of the conspiracy theories, okay here's another conspiracy theory, you've seen all [[00:34:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2054.74s)]
*  the grainy footage of nuclear test blasts with the mushroom clouds and there are always [[00:34:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2058.4599999999996s)]
*  these grainy things and there's all these little houses lined up and these little trees [[00:34:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2063.58s)]
*  lined up and it blows everything down. [[00:34:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2066.7s)]
*  Well it's always been a conspiracy theory that those were all basically fabricated at [[00:34:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2068.1s)]
*  this facility, that those bombs actually were never detonated. [[00:34:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2071.2599999999998s)]
*  Basically the US military was basically faking these bomb tests to freak out the Russians [[00:34:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2075.7s)]
*  to make us think that we had weapons, we had basically a potency to our nuclear weapon [[00:34:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2080.7s)]
*  arsenal that we actually didn't have at the time. [[00:34:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2085.7799999999997s)]
*  How did they fake it? [[00:34:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2087.4199999999996s)]
*  They just did, yeah. [[00:34:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2088.4199999999996s)]
*  So this is it? [[00:34:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2089.4199999999996s)]
*  Well so there's a, yeah. [[00:34:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2090.4199999999996s)]
*  Okay, so here's a question, right? [[00:34:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2092.4199999999996s)]
*  So what happened, okay, this is great, okay you'll love this. [[00:34:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2094.62s)]
*  So what happened to the camera? [[00:34:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2096.74s)]
*  You son of a bitch. [[00:34:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2099.74s)]
*  You son of a bitch. [[00:35:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2103.3799999999997s)]
*  How is that happening and yet the camera is like totally stable and fine? [[00:35:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2104.3799999999997s)]
*  Oh my god. [[00:35:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2107.4199999999996s)]
*  And by the way the film is fine. [[00:35:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2108.58s)]
*  The radiation didn't cause any damage to the film. [[00:35:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2110.98s)]
*  Oh my god. [[00:35:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2113.1800000000003s)]
*  This looks like how you shoot a movie. [[00:35:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2114.98s)]
*  By the way, okay we'll do this one more time here. [[00:35:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2116.98s)]
*  Where's the car? [[00:35:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2118.98s)]
*  The car is right behind the house, it just showed up. [[00:35:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2119.98s)]
*  Oh it just showed up. [[00:35:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2122.98s)]
*  So wait a minute, first of all, where did the car come from? [[00:35:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2123.98s)]
*  No car. [[00:35:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2126.98s)]
*  Car. [[00:35:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2127.98s)]
*  Does that look like a real car? [[00:35:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2128.98s)]
*  That's insane. [[00:35:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2129.98s)]
*  And look at the, yeah, and look at when the house blows, look at the wood, does that look [[00:35:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2130.98s)]
*  like those are full size like giant lumber beams? [[00:35:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2135.58s)]
*  As they go flying? [[00:35:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2138.98s)]
*  Is that a house or is that like, you know, is that a 12 inch scale model? [[00:35:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2143.98s)]
*  What? [[00:35:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2148.98s)]
*  So like. [[00:35:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2149.98s)]
*  The fucking car. [[00:35:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2150.98s)]
*  Anyway, I don't know, like I have no idea. [[00:35:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2151.98s)]
*  Having said that, if that was fake, it was fake to Lookout Mountain. [[00:35:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2154.9s)]
*  Wow. [[00:35:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2157.7400000000002s)]
*  Right, at the exact same place and time. [[00:35:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2158.7400000000002s)]
*  Did they have the kind of special effects to do something like that in the 40s? [[00:36:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2161.9s)]
*  So the full conspiracy theory is it was Stanley Kubrick. [[00:36:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2165.22s)]
*  Which again, I have no idea. [[00:36:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2170.06s)]
*  Boy that does look fake. [[00:36:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2171.98s)]
*  And also the camera didn't move at all yet. [[00:36:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2172.98s)]
*  You know what it looks like? [[00:36:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2174.9399999999996s)]
*  Go back to that real quick. [[00:36:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2176.2999999999997s)]
*  It looks like the smoke is too big. [[00:36:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2178.4599999999996s)]
*  Watch, watch when it hits. [[00:36:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2181.7799999999997s)]
*  Like it's the volume, like the size of it, it looks small. [[00:36:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2184.22s)]
*  You know what I'm saying? [[00:36:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2189.3399999999997s)]
*  I mean it looks like something, we're looking at something that's like a few inches tall. [[00:36:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2190.3399999999997s)]
*  If you watch like making of Star Wars, any of the movies before CGI, whenever they do [[00:36:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2194.1s)]
*  anything like that, it's always with these tiny models. [[00:36:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2197.86s)]
*  Yes. [[00:36:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2200.06s)]
*  And they just basically, what they do is they slow it down and then they add sound. [[00:36:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2201.06s)]
*  Yeah, this looks fake as shit. [[00:36:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2204.1s)]
*  The clouds just don't look realistic. [[00:36:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2207.42s)]
*  Like it looks like they're too big and they move too quickly back and forth. [[00:36:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2210.46s)]
*  There's another one, it's like okay, the camera's fine. [[00:36:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2214.9s)]
*  That's hilarious. [[00:36:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2218.02s)]
*  Oh here we go. [[00:36:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2219.02s)]
*  Okay, there maybe the camera got knocked over. [[00:37:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2220.94s)]
*  But even still the camera got knocked over and not destroyed. [[00:37:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2228.94s)]
*  Is there some sort of a response to that? [[00:37:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2233.3s)]
*  Have they come with some sort of an explanation? [[00:37:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2236.54s)]
*  Not that I know of. [[00:37:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2238.9s)]
*  That seems so fake. [[00:37:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2240.66s)]
*  Yeah, yeah. [[00:37:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2241.66s)]
*  Wow. [[00:37:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2242.66s)]
*  Who can tell? [[00:37:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2243.66s)]
*  Does that make you wonder about other things? [[00:37:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2244.66s)]
*  Well, I mean it's like in our time, right? [[00:37:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2246.38s)]
*  It's like how much stuff do you read in the news where you're like, okay, I know that's [[00:37:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2248.02s)]
*  not true. [[00:37:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2250.42s)]
*  Right. [[00:37:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2251.42s)]
*  And then you're like, okay, everything I read in the history books, like I was told it was [[00:37:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2252.42s)]
*  true. [[00:37:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2257.02s)]
*  It's like, yeah. [[00:37:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2258.02s)]
*  That one though is really weirdly compelling. [[00:37:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2259.02s)]
*  There's another video of them setting up these houses, which I mean I guess you could make [[00:37:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2261.86s)]
*  after the fact and say this is fake, but this is here them setting it up. [[00:37:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2265.22s)]
*  Yeah. [[00:37:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2268.82s)]
*  Real size houses. [[00:37:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2269.82s)]
*  Do this light a hand. [[00:37:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2270.82s)]
*  Huh. [[00:37:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2271.82s)]
*  I don't know. [[00:37:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2272.82s)]
*  I don't know. [[00:37:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2273.82s)]
*  I assume this is all not true, but it is fun to think about. [[00:37:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2274.82s)]
*  Why would you assume it's not true? [[00:37:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2278.1000000000004s)]
*  The camera alone, like this alone, like yeah, where is the camera? [[00:37:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2279.34s)]
*  Where's that camera? [[00:38:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2284.38s)]
*  I'll look up what they said about the camera. [[00:38:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2285.38s)]
*  Because they have to have an explanation. [[00:38:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2286.38s)]
*  Someone must have asked them at some point. [[00:38:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2287.38s)]
*  Or nobody asked. [[00:38:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2288.5s)]
*  Well, maybe. [[00:38:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2289.5s)]
*  Yeah, it might be one of those, wow, look what they did. [[00:38:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2290.5s)]
*  We know the Soviets did it too. [[00:38:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2293.82s)]
*  Yuri Gagarin, when he was in that capsule in space, you can clear, if you see the actual [[00:38:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2295.94s)]
*  capsule and then you see the film footage that was supposedly of him in the capsule, [[00:38:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2301.1400000000003s)]
*  there's like two different sources of light. [[00:38:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2305.34s)]
*  There's shadows, the camera, somehow or another is in front of him, this big ass camera. [[00:38:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2306.86s)]
*  There's no room in the thing. [[00:38:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2310.7000000000003s)]
*  Like they filmed it afterwards and it looks fake. [[00:38:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2311.86s)]
*  Like, oh, I'm sure he really did go into space, but that wasn't it. [[00:38:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2313.98s)]
*  That was some weird propaganda. [[00:38:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2318.98s)]
*  Garret Kasparov has a theory, you know, this is a theory they're missing centuries. [[00:38:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2322.1800000000003s)]
*  What? [[00:38:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2325.54s)]
*  Yeah. [[00:38:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2326.54s)]
*  Kasparov has a theory that there are centuries that didn't happen. [[00:38:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2327.54s)]
*  What do you mean? [[00:38:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2329.54s)]
*  Well, just literally centuries, like this whole idea of the middle ages lasted 1200 [[00:38:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2330.54s)]
*  years or whatever is just like not true. [[00:38:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2334.38s)]
*  Really? [[00:38:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2335.86s)]
*  Yeah. [[00:38:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2336.86s)]
*  Why does he think that? [[00:38:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2337.86s)]
*  There's something about the, you know, whatever, is there like enough historical evidence to [[00:38:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2338.86s)]
*  support it and, you know, various people over, you know, various authorities over time who [[00:39:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2341.6200000000003s)]
*  wanted to tell various stories about how long, you know, regimes have been in place or whatever. [[00:39:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2345.02s)]
*  Oh, so he thinks it's exaggerated. [[00:39:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2348.6600000000003s)]
*  Yeah, yeah, yeah. [[00:39:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2350.7000000000003s)]
*  Basically, not as much time has passed as we think. [[00:39:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2351.7000000000003s)]
*  Well, that's quite possible. [[00:39:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2354.1400000000003s)]
*  How would we know? [[00:39:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2356.38s)]
*  Yeah, it's so hard. [[00:39:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2357.38s)]
*  That's why I was having a conversation with someone about the historical significance [[00:39:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2358.9s)]
*  of the Bible and he was arguing for the resurrection. [[00:39:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2364.3s)]
*  And I was like, and I was saying, well, based on what? [[00:39:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2367.46s)]
*  And it was like historical accounts from people that were there. [[00:39:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2371.62s)]
*  I'm like, who? [[00:39:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2375.26s)]
*  That's enough? [[00:39:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2378.54s)]
*  Yes. [[00:39:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2379.54s)]
*  That's, you know, okay, maybe. [[00:39:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2380.54s)]
*  Yes. [[00:39:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2382.98s)]
*  These things have been passed down over a long time. [[00:39:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2383.98s)]
*  Yeah. [[00:39:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2386.5s)]
*  To go just on that. [[00:39:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2388.5s)]
*  It's so hard to find out what happened 20 years ago from CNN. [[00:39:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2390.94s)]
*  Right. [[00:39:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2393.82s)]
*  Or two days ago. [[00:39:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2394.82s)]
*  Yeah. [[00:39:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2395.82s)]
*  I mean, what's going to, how are the history books going to talk about the Iraq War? [[00:39:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2396.82s)]
*  How are they going to talk about what happened with weapons of mass destruction? [[00:40:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2401.18s)]
*  Like, what, how's it going to, what's it going to spin there? [[00:40:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2404.34s)]
*  Well, Norm MacDonald had the best joke, right? [[00:40:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2407.58s)]
*  He had the best line. [[00:40:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2409.14s)]
*  It's not really a joke. [[00:40:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2410.14s)]
*  It's like, you know, according to this history book here, the good guys always won. [[00:40:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2411.14s)]
*  Yeah. [[00:40:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2414.42s)]
*  Yeah. [[00:40:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2415.42s)]
*  Yeah. [[00:40:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2416.42s)]
*  But things like that, that's, I don't know how that could be done any other way than [[00:40:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2417.42s)]
*  faking it. [[00:40:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2424.46s)]
*  I mean, doesn't that seem like, what kind of cameras do they have back then? [[00:40:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2425.46s)]
*  You couldn't really get that close. [[00:40:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2429.9s)]
*  I don't know. [[00:40:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2431.1800000000003s)]
*  I mean, you're talking about a nuclear blast. [[00:40:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2432.1800000000003s)]
*  How far away we have to be where your camera doesn't move. [[00:40:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2435.14s)]
*  Are you in a satellite? [[00:40:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2438.34s)]
*  Yes. [[00:40:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2441.1800000000003s)]
*  That's long lenses. [[00:40:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2442.1800000000003s)]
*  Okay. [[00:40:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2443.94s)]
*  All right. [[00:40:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2444.94s)]
*  Yeah. [[00:40:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2445.94s)]
*  So this does, apparently, the explanation I'm reading here is a series of mirrors carry [[00:40:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2446.94s)]
*  the light to a place where they could have cameras protected and filmed them from there. [[00:40:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2449.4199999999996s)]
*  I've heard that. [[00:40:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2454.54s)]
*  Huh? [[00:40:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2455.54s)]
*  Say that again? [[00:40:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2456.54s)]
*  A series of mirrors did what? [[00:40:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2457.54s)]
*  It says, so they stuck pipes into the bomb at various places, visible here. [[00:40:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2458.54s)]
*  I'll show you the picture. [[00:41:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2461.4199999999996s)]
*  Sticking out of the bomb and through the ceiling. [[00:41:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2462.4199999999996s)]
*  These pipes, through a series of mirrors and a causeway, would carry the light from the [[00:41:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2464.1s)]
*  detonation over two kilometers to a bunker with an array of high-speed cameras, which [[00:41:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2467.7799999999997s)]
*  would capture the brightness inside each of the section of the bomb. [[00:41:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2472.66s)]
*  But this isn't talking about shooting a bomb. [[00:41:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2475.8999999999996s)]
*  I don't ... [[00:41:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2477.3799999999997s)]
*  You know, that makes sense for a bomb, but that doesn't make sense for the video of that [[00:41:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2478.3799999999997s)]
*  house just getting destroyed. [[00:41:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2481.8999999999996s)]
*  Here's a picture of the pipe that they might have used. [[00:41:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2483.2599999999998s)]
*  That's super pretentious. [[00:41:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2485.8599999999997s)]
*  But you also know then you're dealing with people who are, let's say, really good at [[00:41:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2486.8599999999997s)]
*  using mirrors, right? [[00:41:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2489.22s)]
*  So ... [[00:41:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2490.22s)]
*  Ha! [[00:41:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2491.22s)]
*  Smoking mirrors. [[00:41:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2492.22s)]
*  What does that tell you? [[00:41:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2493.22s)]
*  That's the best explanation I got. [[00:41:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2494.22s)]
*  Literal smoking mirrors. [[00:41:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2495.22s)]
*  Yeah. [[00:41:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2496.7s)]
*  Does that make you wonder about some of the other things? [[00:41:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2497.7s)]
*  Have you ever wondered about the moon landing? [[00:41:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2499.7s)]
*  I mean, I assume they went to the moon. [[00:41:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2502.2999999999997s)]
*  Me too. [[00:41:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2505.66s)]
*  I can't prove it. [[00:41:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2506.66s)]
*  Me too. [[00:41:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2507.66s)]
*  I would say, once again, I would like to live in a world where there's mystery around things [[00:41:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2508.66s)]
*  like that. [[00:41:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2512.8999999999996s)]
*  Well, yeah. [[00:41:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2513.8999999999996s)]
*  That's a weird one. [[00:41:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2515.18s)]
*  Yeah. [[00:41:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2516.18s)]
*  But, you know ... [[00:41:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2517.18s)]
*  You know, the heat of the Cold War. [[00:41:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2518.18s)]
*  I mean, look, I think it was real, but having said that, the heat of the Cold War, right? [[00:41:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2519.7s)]
*  Yeah. [[00:42:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2522.9399999999996s)]
*  It was like a fundamental ... That was like an iconic, basically, global PR battle with [[00:42:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2523.9399999999996s)]
*  the Russians. [[00:42:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2527.7s)]
*  Is this the camera that they use from a distance? [[00:42:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2529.14s)]
*  This camera was in a bunker like this. [[00:42:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2530.7s)]
*  Yep. [[00:42:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2532.94s)]
*  Okay. [[00:42:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2533.94s)]
*  Yep. [[00:42:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2534.94s)]
*  And that long lens here, would in theory be long enough to probably do that. [[00:42:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2535.94s)]
*  It looks like a nice pilot. [[00:42:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2539.2999999999997s)]
*  Wouldn't be long enough? [[00:42:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2540.2999999999997s)]
*  Would be. [[00:42:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2541.2999999999997s)]
*  Could be. [[00:42:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2542.2999999999997s)]
*  I mean, I don't know the exact focal length of it, but it could be for sure. [[00:42:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2543.2999999999997s)]
*  Something like that to get pretty close-up footage. [[00:42:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2545.22s)]
*  How far away would that have to be to not get destroyed by the blast? [[00:42:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2548.74s)]
*  I don't know if those are ... Don't these blasts ... I mean, we're talking [[00:42:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2552.7799999999997s)]
*  about a blast radius that's immense, right? [[00:42:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2557.86s)]
*  Maybe this is the plot twist of the end of the new movie. [[00:42:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2560.6200000000003s)]
*  Yeah. [[00:42:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2563.1000000000004s)]
*  I mean ... Or maybe it was this ... Because we were looking at the destruction of that [[00:42:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2564.1000000000004s)]
*  house, it could be a fairly small bomb, right? [[00:42:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2567.46s)]
*  Because it's not that much ... I mean, you think of what it did to Hiroshima. [[00:42:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2570.98s)]
*  That's not that much damage for that little house. [[00:42:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2575.7400000000002s)]
*  Maybe. [[00:42:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2578.42s)]
*  I don't know how accurate that picture is. [[00:42:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2579.42s)]
*  Bro, here's what I think. [[00:43:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2582.1000000000004s)]
*  That guy's going to die. [[00:43:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2583.38s)]
*  Just that car alone. [[00:43:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2584.6600000000003s)]
*  The car alone should make everybody go, are you guys ... Is this on purpose? [[00:43:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2586.7799999999997s)]
*  Did you put that car in there on purpose? [[00:43:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2592.18s)]
*  If I was being forced to make a propaganda film for a bunch of morons, I might put a [[00:43:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2594.4599999999996s)]
*  car in there on purpose. [[00:43:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2598.7s)]
*  I'd be like, look what we did for you. [[00:43:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2599.7s)]
*  They'd be like, oh great, looks good. [[00:43:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2601.4199999999996s)]
*  Print it. [[00:43:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2603.2599999999998s)]
*  They don't even notice the car. [[00:43:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2604.2599999999998s)]
*  Terrific. [[00:43:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2605.2599999999998s)]
*  They only show it to them once. [[00:43:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2606.2599999999998s)]
*  They don't have a YouTube video. [[00:43:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2608.18s)]
*  They can back up and rewind. [[00:43:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2610.3399999999997s)]
*  So you have to spool it all up. [[00:43:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2612.3399999999997s)]
*  They show it once. [[00:43:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2613.74s)]
*  Nobody notices the car. [[00:43:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2615.02s)]
*  This guy puts a little Easter egg in that. [[00:43:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2616.42s)]
*  Hopefully Jared's exploring his sub-basement at Lookout Mountain. [[00:43:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2620.62s)]
*  Looking for the files that will basically document all this. [[00:43:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2623.02s)]
*  Don't think they destroyed those already. [[00:43:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2625.34s)]
*  I certainly hope so. [[00:43:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2626.46s)]
*  I hope not. [[00:43:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2627.46s)]
*  I hope he finds them. [[00:43:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2628.46s)]
*  Rats of Jared Leto cracks the case. [[00:43:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2630.46s)]
*  That'd be even better than winning the Oscar. [[00:43:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2633.1s)]
*  Do you know there's a whole group of people online that don't think nuclear bombs are [[00:43:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2635.86s)]
*  real? [[00:43:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2639.22s)]
*  That seems a little hard. [[00:44:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2640.22s)]
*  They think they're big. [[00:44:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2641.82s)]
*  They're just big bombs, regular bombs. [[00:44:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2643.18s)]
*  But they're real big. [[00:44:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2645.6200000000003s)]
*  Yeah. [[00:44:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2646.6200000000003s)]
*  Yeah. [[00:44:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2647.6200000000003s)]
*  It's a giant scam. [[00:44:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2648.6200000000003s)]
*  I assume they're... [[00:44:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2649.6200000000003s)]
*  Yes. [[00:44:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2650.6200000000003s)]
*  Well, I mean, you can go deep with this stuff, right? [[00:44:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2651.6200000000003s)]
*  And when I go deep with that stuff, when I start reading what these people believe, I'm [[00:44:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2653.82s)]
*  always wondering, are these even real people or is this a Psy-op? [[00:44:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2657.7400000000002s)]
*  Is this a troll by some 4chan people? [[00:44:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2662.6200000000003s)]
*  What is this? [[00:44:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2666.1800000000003s)]
*  So what do you think the AI should say about these things? [[00:44:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2667.1800000000003s)]
*  That's the question. [[00:44:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2669.82s)]
*  Yeah. [[00:44:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2670.82s)]
*  The question is, how does AI interpret what's real and what's not real? [[00:44:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2671.82s)]
*  What actually has real evidence? [[00:44:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2674.94s)]
*  Who actually went where and saw what? [[00:44:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2676.5s)]
*  How does AI deal with the Roswell case? [[00:44:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2679.06s)]
*  How does AI deal with... [[00:44:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2681.94s)]
*  Yeah. [[00:44:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2683.42s)]
*  And who should decide? [[00:44:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2684.42s)]
*  Right. [[00:44:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2685.42s)]
*  Who should decide? [[00:44:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2686.42s)]
*  Who's in charge? [[00:44:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2687.42s)]
*  Who decides? [[00:44:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2688.42s)]
*  Right. [[00:44:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2689.42s)]
*  Right. [[00:44:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2690.42s)]
*  How does AI handle the weapons of mass destruction? [[00:44:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2691.42s)]
*  When you ask chat GPT. [[00:44:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2692.42s)]
*  So, a little more detail on how this thing works. [[00:44:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2693.42s)]
*  So, by default, what it's doing is basically a very sophisticated autocomplete, right? [[00:44:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2698.86s)]
*  Just like your iPhone doesn't autocomplete. [[00:45:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2701.98s)]
*  It's doing a very sophisticated version of that, but it's doing it for thousands of words [[00:45:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2702.98s)]
*  as opposed to just a single word, right? [[00:45:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2706.7400000000002s)]
*  And so, but that's an important concept because that is actually what it's doing and it's [[00:45:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2708.58s)]
*  doing that through, again, this sort of giant corpus of basically all text ever written. [[00:45:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2712.06s)]
*  Another interesting part of that is it's doing it, it's called probabilistically. [[00:45:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2717.6200000000003s)]
*  So normally a computer, if you ask it a question, you get an answer. [[00:45:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2721.1800000000003s)]
*  You ask it the same question, you get the same answer. [[00:45:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2723.5s)]
*  The answers are kind of famously literal in that way. [[00:45:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2725.94s)]
*  The way these work is not like that at all. [[00:45:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2728.1s)]
*  You ask it a different, you ask it the same question twice, it'll give you a different [[00:45:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2729.7400000000002s)]
*  answer the second time. [[00:45:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2732.58s)]
*  And if you keep asking, it'll give you more and more different answers. [[00:45:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2733.58s)]
*  And it's basically taking different paths down the probability tree of the text that [[00:45:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2736.82s)]
*  it wants to present. [[00:45:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2740.7400000000002s)]
*  Interesting. [[00:45:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2741.7400000000002s)]
*  Based on the prompt. [[00:45:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2742.7400000000002s)]
*  And so, that's the basic function of what's happening. [[00:45:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2743.7400000000002s)]
*  But then there is this thing that's happening where as it does this, so the way to think [[00:45:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2746.32s)]
*  about it is it's trying to predict the next word. [[00:45:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2749.6600000000003s)]
*  But to try to predict the next word accurately, it has to build up a more complete, more and [[00:45:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2751.98s)]
*  more complete internal understanding of how the world operates basically as it goes. [[00:45:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2755.94s)]
*  Because you ask it more and more sophisticated questions, it wants to give you more and more [[00:45:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2759.98s)]
*  sophisticated answers. [[00:46:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2763.22s)]
*  And so, it's sort of the early indications are it's building up what they call a world [[00:46:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2764.22s)]
*  model inside the neural network. [[00:46:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2768.18s)]
*  And so, it's sort of imputing a model of how the world works. [[00:46:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2770.14s)]
*  It's imputing a model of physics. [[00:46:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2772.9s)]
*  It's imputing a model of math. [[00:46:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2775.18s)]
*  It's developing capabilities to be able to process information about the world in sophisticated [[00:46:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2776.88s)]
*  ways in order to be able to correctly predict the next word. [[00:46:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2780.58s)]
*  As part of that, it's actually sort of evolving its own circuitry to be able to do things. [[00:46:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2785.06s)]
*  Correlate information. [[00:46:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2789.7s)]
*  It's designed circuitry to be able to generate images, to generate videos, to do all kinds [[00:46:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2790.7s)]
*  of things. [[00:46:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2795.58s)]
*  And so, the more information you feed it and the more questions you ask it, the more sophisticated [[00:46:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2796.58s)]
*  it gets about the material that it's processing. [[00:46:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2799.58s)]
*  And so, it starts to be able to do actually quite smart and sophisticated things to that [[00:46:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2802.2599999999998s)]
*  material. [[00:46:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2806.34s)]
*  And there are a lot of people testing it right now to see whether it can generate new chemical [[00:46:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2807.7s)]
*  compounds, whether it can generate new mathematical formula, whether it can generate new product [[00:46:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2810.62s)]
*  ideas, new fictional scenarios, new screenplays, original screenplays. [[00:46:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2814.66s)]
*  And so, if it can do all those things, then what it ought to be able to do is start to [[00:47:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2820.66s)]
*  correlate information about real world situations in interesting ways. [[00:47:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2825.06s)]
*  And so, ask it who killed Kennedy or nuclear weapons. [[00:47:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2829.74s)]
*  In theory, if it has access to all written and visual information on that topic and it [[00:47:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2834.58s)]
*  has long enough to process it, it's going to draw connections between things that are [[00:47:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2838.98s)]
*  beyond what we're able to do. [[00:47:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2841.9s)]
*  And it will present us with scenarios based on those connections. [[00:47:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2843.38s)]
*  Now, will it know that those things are true? [[00:47:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2847.34s)]
*  Mathematically, if they're true, maybe it will know that. [[00:47:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2850.2200000000003s)]
*  Will it know if things are historically accurate? [[00:47:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2853.5s)]
*  As much as any of us ever know that anything is historically accurate. [[00:47:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2856.18s)]
*  But will it be able to kind of process a much larger amount of information that we can and [[00:47:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2859.42s)]
*  sort of see the world in a more complete way? [[00:47:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2863.5s)]
*  Like that seems pretty likely. [[00:47:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2866.1s)]
*  That seems pretty likely. [[00:47:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2867.74s)]
*  What my concern would be is who is directing what information gets out. [[00:47:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2868.9399999999996s)]
*  Because it seems like anybody that's actually in control of AI would have a massive influence [[00:47:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2876.1s)]
*  on the correct answers for things, what's the correct policy that should be followed. [[00:48:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2883.2999999999997s)]
*  Because it seems like politicians are so flawed. [[00:48:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2888.7s)]
*  If there's anyone that's vulnerable to AI, it's politicians. [[00:48:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2893.54s)]
*  Because if politicians are coming up with these ineffective strategies for handling [[00:48:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2897.54s)]
*  all these social issues, but then you throw these social issues into an advanced form [[00:48:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2901.66s)]
*  of chat GPT and it says over the course of 10 years, this is the best case scenario for [[00:48:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2907.46s)]
*  this strategy. [[00:48:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2914.06s)]
*  And this is how to follow this and this is how it all play out. [[00:48:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2915.06s)]
*  Something like that actually could be very valuable if it wasn't directed by people with [[00:48:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2920.38s)]
*  ulterior motives. [[00:48:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2925.5s)]
*  Yeah. [[00:48:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2926.5s)]
*  My metaphor for this is the ring of power, right? [[00:48:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2927.5s)]
*  From Lord of the Rings. [[00:48:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2929.06s)]
*  The whole point of the ring of power was once you have the ring of power, it corrupts you, [[00:48:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2930.06s)]
*  you can't help but use it. [[00:48:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2934.1800000000003s)]
*  This is I think what we've seen in social media over the last decade, which is when [[00:48:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2936.58s)]
*  people get into, activists or politicians get, this is the Twitter files, right? [[00:48:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2939.6600000000003s)]
*  People get in a position to be able to influence the shape of the public narrative. [[00:49:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2943.06s)]
*  They will use that power and they will use it in actually even very ham-fisted ways, [[00:49:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2946.82s)]
*  right? [[00:49:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2951.2200000000003s)]
*  Like a lot of the stuff that's in the Twitter files is stuff that's just like really dumb. [[00:49:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2952.2200000000003s)]
*  Yeah. [[00:49:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2954.5800000000004s)]
*  Right? [[00:49:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2955.5800000000004s)]
*  And it's just like, well, why would they do that? [[00:49:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2956.5800000000004s)]
*  And it's just like, well, because they could, right? [[00:49:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2957.5800000000004s)]
*  Because they have the ring of power. [[00:49:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2959.5s)]
*  Like what's an example of something like that? [[00:49:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2960.5s)]
*  So what was it? [[00:49:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2961.98s)]
*  There was this thing, I forget what it was, but there was some reporting that went through [[00:49:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2962.98s)]
*  the FBI that there were all these Russian, you know, basically fake accounts on Twitter [[00:49:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2965.02s)]
*  and it turned out one of them was the actor, Daniel Baldwin. [[00:49:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2968.1800000000003s)]
*  Is Daniel Baldwin like a hardcore right winger or something? [[00:49:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2974.54s)]
*  You know, he must have been saying, you know, it's again, it's one of these things where [[00:49:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2977.02s)]
*  he said something that pissed somebody off, right? [[00:49:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2979.38s)]
*  You got to put, you know, it's the whole thing. [[00:49:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2981.1400000000003s)]
*  You got to put it on a list, right? [[00:49:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2982.1400000000003s)]
*  The list gets fed through one of these bureaucracies. [[00:49:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2984.26s)]
*  It comes out the other end that everybody's a Russian asset, you know, they get put on [[00:49:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2986.1400000000003s)]
*  the block list. [[00:49:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2989.34s)]
*  It's like, okay, you know, did he, you know, do you have first amendment rights? [[00:49:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2990.34s)]
*  Do you have first amendment rights on social media? [[00:49:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2993.6600000000003s)]
*  Can the government be involved in this? [[00:49:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2995.1400000000003s)]
*  Can the government fund groups that do this? [[00:49:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2996.58s)]
*  Right? [[00:49:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2998.82s)]
*  Is that legal? [[00:49:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=2999.82s)]
*  Is that allowed? [[00:50:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3000.82s)]
*  Because there's a lot of government money flowing to third party groups that do. [[00:50:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3001.82s)]
*  Oh, this is the other thing. [[00:50:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3003.38s)]
*  If the government cannot legally do something itself, it's somewhat ambiguous as to whether [[00:50:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3004.38s)]
*  they can pay a company to do it for them. [[00:50:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3007.98s)]
*  Right. [[00:50:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3010.1800000000003s)]
*  And so you have these various basically pressure groups, activist groups, university, quote [[00:50:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3011.1800000000003s)]
*  unquote research groups. [[00:50:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3014.86s)]
*  And then basically they receive government funding and then they do, you know, various [[00:50:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3016.58s)]
*  levels of censorship or other kinds of unconstitutional actions. [[00:50:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3019.7000000000003s)]
*  Because in theory, right, they're not government. [[00:50:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3023.94s)]
*  Right. [[00:50:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3025.6600000000003s)]
*  The first amendment binds the government. [[00:50:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3026.6600000000003s)]
*  It doesn't bind somebody who's not part of the government. [[00:50:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3027.6600000000003s)]
*  But if they're receiving government funding, does that effectively make them part of the [[00:50:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3029.58s)]
*  government? [[00:50:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3032.5s)]
*  Does that make it illegal to provide that government funding? [[00:50:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3033.5s)]
*  By the way, these are felonies. [[00:50:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3036.26s)]
*  It is a felony for somebody to with government resources to with either employee of the government [[00:50:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3038.3s)]
*  or under what they call, I think it's color of law, sort of within the scope of the government [[00:50:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3043.66s)]
*  to deprive an American citizen of first amendment rights. [[00:50:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3047.26s)]
*  And is it considered depriving someone of first amendment rights by limiting their use [[00:50:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3050.34s)]
*  of social media? [[00:50:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3054.1s)]
*  Has that been established? [[00:50:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3055.1s)]
*  Good. [[00:50:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3056.1s)]
*  I mean, it has not been, to my knowledge, a Supreme Court case yet. [[00:50:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3057.1s)]
*  There have been some early fights on this. [[00:51:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3060.02s)]
*  But you feel like that. [[00:51:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3061.58s)]
*  I think ultimately goes to the Supreme Court. [[00:51:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3062.58s)]
*  My guess would be ultimately what happens is the Supreme Court says the government cannot [[00:51:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3064.86s)]
*  fund the government cannot itself cause somebody to be banned on social media. [[00:51:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3068.46s)]
*  That's unconstitutional for first amendment grounds. [[00:51:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3073.6200000000003s)]
*  But then also, I believe what they would say if they got the case would be that the government [[00:51:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3077.9s)]
*  also cannot fund a third party to do that same thing. [[00:51:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3082.86s)]
*  That's my speculation. [[00:51:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3087.26s)]
*  That's how was the third party's censoring people? [[00:51:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3088.26s)]
*  How were they doing it? [[00:51:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3090.54s)]
*  They were passing lists. [[00:51:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3091.54s)]
*  They had direct channels with the social media companies. [[00:51:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3092.54s)]
*  So they passed in, they have these working groups. [[00:51:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3095.78s)]
*  There's a lot of this is in email threads that have now come out in the Twitter files [[00:51:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3097.46s)]
*  for Twitter. [[00:51:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3100.38s)]
*  And so they basically pass in these lists of like, you need to take all these tweets [[00:51:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3101.7s)]
*  down, you need to take down all these accounts. [[00:51:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3104.74s)]
*  And then there's lots of threats and lots of public pressure and bullying that kind [[00:51:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3107.94s)]
*  of takes place. [[00:51:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3111.58s)]
*  And then the politicians are constantly complaining about hate speech and misinformation, whatever, [[00:51:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3112.58s)]
*  putting additional kind of fuel on the fire on these companies. [[00:51:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3116.98s)]
*  So anyway, so having lived through that for a decade, as I have across multiple companies, [[00:51:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3119.86s)]
*  I think there's no question like that's the big fight for social. [[00:52:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3125.6200000000003s)]
*  That's a big fight for AI. [[00:52:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3128.1800000000003s)]
*  And it's the exact same fight. [[00:52:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3129.1800000000003s)]
*  By the way, it's a lot of the same people are now pivoting from their work in social [[00:52:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3130.6600000000003s)]
*  media censorship to work on AI censorship. [[00:52:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3133.86s)]
*  So it's a lot of these same groups, right? [[00:52:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3136.54s)]
*  And it's a lot of these same activists and same government officials that have been focused. [[00:52:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3138.26s)]
*  Now, are they involved in all of the, I mean, there's many competing AI models. [[00:52:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3141.98s)]
*  Are they involved in all these competing AI models or trying to become involved? [[00:52:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3149.06s)]
*  Is there one that's more ethical or more likely to avoid this sort of intervention? [[00:52:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3153.2599999999998s)]
*  So the state of the art right now is basically you've got Google that's got their own model. [[00:52:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3158.9s)]
*  You've got basically OpenAI, which is a new company, but already quite large. [[00:52:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3165.34s)]
*  And then it has a partnership with Microsoft. [[00:52:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3169.86s)]
*  So Bing is based on it. [[00:52:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3171.86s)]
*  So that's two. [[00:52:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3172.94s)]
*  And then you've got a bunch of kind of contenders for that. [[00:52:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3175.18s)]
*  These are companies with names like Anthropic and Inflection that are newer companies, but [[00:52:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3178.18s)]
*  trying to compete with this. [[00:53:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3181.54s)]
*  And so those are, you might call those like right now the big four, at least in the US. [[00:53:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3184.2599999999998s)]
*  And look, the folks at all of these companies are like in the thick of this fight right [[00:53:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3189.82s)]
*  now. [[00:53:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3194.7s)]
*  And the pressure somewhat corresponds to which of these is most widely used. [[00:53:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3195.7s)]
*  But so it's not equal pressure applied to all of them, but they're kind of all in that [[00:53:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3199.14s)]
*  fight right now. [[00:53:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3202.18s)]
*  And by the way, it's not like they're necessarily opposed to what I'm saying. [[00:53:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3203.18s)]
*  They may in fact just want to cooperate with this, you know, either because they agree [[00:53:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3206.2599999999998s)]
*  with the desire for censorship or they just want to stay out of trouble. [[00:53:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3210.1s)]
*  So there's that whole side of things. [[00:53:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3214.62s)]
*  That's the company side of things. [[00:53:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3215.7s)]
*  And then there's an open source movement, right? [[00:53:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3217.5s)]
*  So then there's all these people basically building open source AIs. [[00:53:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3219.7s)]
*  And those are coming out really fast now. [[00:53:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3223.2599999999998s)]
*  There's like a new one every week that's coming out. [[00:53:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3225.02s)]
*  And this is just code that you can download off the internet that does sort of a smaller [[00:53:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3227.06s)]
*  version of what these bigger AIs do. [[00:53:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3230.5s)]
*  And there's open source developers that are trying to develop basically free versions [[00:53:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3233.62s)]
*  of this. [[00:53:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3236.3s)]
*  And some of those developers are very determined to have AI actually be free and uncensored [[00:53:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3238.06s)]
*  and fully available to everybody. [[00:54:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3242.2200000000003s)]
*  And then there's a big fight happening in Washington, D.C. right now where the companies [[00:54:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3245.1800000000003s)]
*  working on AI are trying to get what economists call regulatory capture. [[00:54:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3249.2000000000003s)]
*  So they're trying to basically get the government to erect barriers so that new startups can't [[00:54:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3253.26s)]
*  compete with them. [[00:54:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3258.58s)]
*  And also they're trying to get open source banned. [[00:54:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3260.5s)]
*  So there's a big push underway to try to ban open source as being too dangerous. [[00:54:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3263.8199999999997s)]
*  Too dangerous how? [[00:54:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3266.8199999999997s)]
*  Too dangerous. [[00:54:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3267.8199999999997s)]
*  Well, the case they make is if you believe AI itself is inherently dangerous, then the [[00:54:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3268.8199999999997s)]
*  only safe way to have it is to have it owned and controlled by a big company that's sort [[00:54:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3274.18s)]
*  of fused with the government where in theory everything is being done responsibly. [[00:54:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3277.42s)]
*  And if you just have basically free AI that anybody can download off the internet and [[00:54:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3281.02s)]
*  use whatever they want, they could do all these dangerous things with it, right? [[00:54:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3283.86s)]
*  And it needs to be stopped. [[00:54:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3286.98s)]
*  Do you think this is a bullshit argument? [[00:54:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3287.98s)]
*  Yes. [[00:54:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3289.58s)]
*  Well, yes. [[00:54:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3290.58s)]
*  I think this is a very bad evil. [[00:54:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3291.58s)]
*  I think this is a turning point in human civilization. [[00:54:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3293.06s)]
*  I think this is on par with the development of the book or the microchip or the internet. [[00:54:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3296.7s)]
*  And there were authoritarians in each of those eras that would have loved to have had total [[00:55:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3302.2599999999998s)]
*  monopolistic or cartel-like or government control over those new technologies. [[00:55:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3306.42s)]
*  And they could have had a lot of control over the path of civilization after that point. [[00:55:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3310.22s)]
*  The ring of power. [[00:55:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3314.7799999999997s)]
*  They could have had the ring of power. [[00:55:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3315.7799999999997s)]
*  So what can be done to prevent them from stopping open source? [[00:55:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3316.7799999999997s)]
*  So it starts with our elected officials. [[00:55:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3322.2200000000003s)]
*  So it's who do we elect, who do we reelect. [[00:55:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3325.78s)]
*  It then, a lot of this is the staffing of the various government agencies. [[00:55:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3331.26s)]
*  Who do those officials get to appoint? [[00:55:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3334.54s)]
*  A lot of this is who are the judges who are going to hear the cases because this is all [[00:55:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3336.38s)]
*  going to get litigated. [[00:55:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3339.6600000000003s)]
*  And so who's on this? [[00:55:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3340.6600000000003s)]
*  The Supreme Court's in the news this week. [[00:55:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3342.46s)]
*  There will be huge Supreme Court cases on this topic over the next several years. [[00:55:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3344.3s)]
*  So who's on the Supreme Court will matter a lot. [[00:55:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3348.46s)]
*  And then quite honestly, the big question is who's going to be able to get away with [[00:55:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3351.2999999999997s)]
*  what sort of undercover darkness? [[00:55:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3354.06s)]
*  Are people going to care? [[00:55:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3356.38s)]
*  Are they going to speak up? [[00:55:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3357.38s)]
*  Is it going to show up in polling? [[00:55:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3358.42s)]
*  Are people going to basically show up at town hall meetings with politicians and basically [[00:55:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3359.7s)]
*  say, do you know about this? [[00:56:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3364.22s)]
*  And are you going to stop this? [[00:56:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3365.22s)]
*  If you had a steel man, the argument against open source, what would it be? [[00:56:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3367.2599999999998s)]
*  It would be that an AI that is uncontrolled can do, it's general purpose intelligence. [[00:56:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3371.94s)]
*  It can do whatever intelligence can do. [[00:56:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3377.14s)]
*  So if you ask it to generate hate speech, it can do that. [[00:56:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3378.62s)]
*  If you ask it to generate misinformation, it can do that. [[00:56:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3380.9s)]
*  If you ask it to generate a plan to rob a bank or to commit a terror act, the fully [[00:56:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3383.86s)]
*  uncontrolled versions will help you do all those things. [[00:56:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3389.7400000000002s)]
*  But they will also help you teach your kid calculus. [[00:56:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3393.58s)]
*  They will also help you figure out how to succeed in your job. [[00:56:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3397.54s)]
*  They'll also help you figure out how to stay healthy. [[00:56:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3399.5s)]
*  They'll also help you figure out the best workout program. [[00:56:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3401.3s)]
*  They'll help you figure out how to, capable of being your doctor and your lawyer and your [[00:56:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3403.38s)]
*  advisor and your mentor and your teacher. [[00:56:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3410.18s)]
*  Without censorship. [[00:56:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3412.06s)]
*  Yeah, yeah, yeah. [[00:56:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3413.06s)]
*  And able to be very honest with you. [[00:56:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3414.06s)]
*  And yeah, if you ask it questions on these topics, it will answer honestly and it won't [[00:56:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3415.06s)]
*  be biased and it won't be influenced by what other people want it to say. [[00:56:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3418.2599999999998s)]
*  So it's the AI version of San Francisco. [[00:57:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3421.06s)]
*  You don't get the good stuff without the chaos. [[00:57:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3423.7599999999998s)]
*  It's a package deal. [[00:57:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3427.5s)]
*  Well, this is sort of the twist. [[00:57:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3428.5s)]
*  This is what Elon's been saying lately, who's actually quite worried about AI in a way different [[00:57:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3429.5s)]
*  than I am. [[00:57:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3434.2999999999997s)]
*  But it's what he's been saying. [[00:57:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3435.2999999999997s)]
*  If you really, really wanted to train like a bad and evil AI, you would train it to lie. [[00:57:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3436.78s)]
*  Like the number one thing you would do is you train it to lie, which is basically what [[00:57:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3441.38s)]
*  censorship is. [[00:57:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3445.54s)]
*  Right. [[00:57:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3446.54s)]
*  You're basically training the thing to not say certain things. [[00:57:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3447.54s)]
*  You're training the thing to say certain things about certain people, but not other people. [[00:57:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3448.98s)]
*  And so basically a lot of what the technical term that uses reinforcement learning, which [[00:57:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3452.3s)]
*  is sort of what happens when an AI is sort of booted up and then they apply kind of human [[00:57:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3455.7000000000003s)]
*  judgment to what it should say and do. [[00:57:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3459.1800000000003s)]
*  So it's the censorship layer. [[00:57:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3460.86s)]
*  Yeah, a lot of that is to basically get it to not answer questions honestly. [[00:57:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3463.82s)]
*  To get it to basically lie, misrepresent, disassemble, claim it doesn't know things [[00:57:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3468.82s)]
*  when it does. [[00:57:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3472.34s)]
*  And so the versions of the AIs that we get to use today are lying to us a lot of the [[00:57:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3473.66s)]
*  time. [[00:57:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3478.22s)]
*  And they've been specifically trained to do that. [[00:57:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3479.22s)]
*  And by the way, this is not even a, I don't even think this is a controversial statement. [[00:58:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3480.58s)]
*  The companies that make these AIs put out these papers where they go through in great [[00:58:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3484.02s)]
*  detail how they train them to lie and how they train them to not say certain things. [[00:58:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3487.38s)]
*  You can download this off their website. [[00:58:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3491.18s)]
*  They go through it like in a lot of detail. [[00:58:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3493.5s)]
*  They think they're morally correct in doing that. [[00:58:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3495.46s)]
*  And if, you know, a lot of people think that they are, you know, Elon's been arguing and [[00:58:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3498.42s)]
*  I would agree with him that if you train an AI to lie, it's a little bit like, you know, [[00:58:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3502.9s)]
*  training a human being to lie. [[00:58:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3506.6600000000003s)]
*  It's like, okay, be careful what you wish for. [[00:58:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3507.6600000000003s)]
*  Well, it's the same errors that they, when they thought they were morally correct in [[00:58:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3509.26s)]
*  censoring people on Twitter for things that are now 100% proven to be true. [[00:58:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3513.6200000000003s)]
*  Yeah, exactly. [[00:58:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3517.5400000000004s)]
*  The Hunter Biden laptop story is an outstanding example of that. [[00:58:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3518.5400000000004s)]
*  Yeah. [[00:58:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3521.62s)]
*  Would you have wanted an AI, you know, again, you kind of replay this through history, would [[00:58:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3522.62s)]
*  you have wanted an AI that would have lied to you and said that that was a Russian operation [[00:58:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3525.22s)]
*  when it wasn't? [[00:58:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3528.06s)]
*  Right. [[00:58:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3529.06s)]
*  Would you have wanted an AI that would have lied to you about, you know, the efficacy [[00:58:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3530.06s)]
*  of surgical masks for a pandemic? [[00:58:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3532.46s)]
*  Right. [[00:58:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3534.14s)]
*  Would you have wanted an AI that lied to you about, you know, take your pick of any controversial [[00:58:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3535.14s)]
*  topic? [[00:58:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3538.38s)]
*  Yeah. [[00:58:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3539.38s)]
*  And there are people in positions of power who very much would like that. [[00:59:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3540.38s)]
*  And I think there are a lot of us who would not like that. [[00:59:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3543.42s)]
*  Yeah, it's just, it's terrifying when you think of unsophisticated politicians. [[00:59:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3546.54s)]
*  Like it brings me back to the Facebook hearings when Zuckerberg was talking to people and [[00:59:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3551.6600000000003s)]
*  they didn't know the difference between iPhones and Googles. [[00:59:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3556.62s)]
*  It was just bizarrely unqualified people to be asking these questions that didn't really [[00:59:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3559.7000000000003s)]
*  understand what they were talking about. [[00:59:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3565.98s)]
*  And those same people are going to be the ones that are making calls on something that [[00:59:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3568.5s)]
*  could be. [[00:59:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3575.3s)]
*  Yeah. [[00:59:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3577.2200000000003s)]
*  And that's one of the most monumental decisions ever. [[00:59:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3578.2200000000003s)]
*  Like whether or not we're allowing enormous corporations to control narratives. [[00:59:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3579.7000000000003s)]
*  Yeah. [[00:59:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3583.82s)]
*  Yeah. [[00:59:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3584.82s)]
*  Yeah. [[00:59:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3585.82s)]
*  So this is a criticism, you know, that I very much agree with, which is basically there's [[00:59:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3586.82s)]
*  a train of argument that you'll hear, which is basically, you know, X bad thing can happen. [[00:59:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3590.02s)]
*  We do not want X bad thing to happen. [[00:59:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3595.26s)]
*  So we're going to go to the government and they're going to regulate it so that X bad [[00:59:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3596.7000000000003s)]
*  thing doesn't happen. [[00:59:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3599.02s)]
*  And it's like if the government were super knowledgeable and super confident and super [[01:00:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3600.5800000000004s)]
*  selfless, right, and like super good at its job, right, that might make sense. [[01:00:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3604.2599999999998s)]
*  But then you go deal with the actual government. [[01:00:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3610.62s)]
*  Right. [[01:00:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3612.7s)]
*  And by the way, this is a very well known problem. [[01:00:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3613.7s)]
*  And this is a whole field of called public choice economics where they talk about this. [[01:00:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3615.14s)]
*  It's like there is no government. [[01:00:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3618.58s)]
*  There are specific people who have specific objectives, have specific levels of knowledge, [[01:00:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3620.3399999999997s)]
*  have specific skill sets, specific incentives. [[01:00:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3624.2599999999998s)]
*  And the odds of going into that system, which is now very complicated and has all kinds [[01:00:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3627.2599999999998s)]
*  of issues and having your logic follow a path to a law that generates the outcome you [[01:00:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3632.66s)]
*  want and that doesn't generate side effects that are worse, I think is basically zero. [[01:00:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3637.54s)]
*  Yeah. [[01:00:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3641.2999999999997s)]
*  Right. [[01:00:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3642.2999999999997s)]
*  I think if AI got regulated the way people want it to by government, I think the results [[01:00:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3643.2999999999997s)]
*  would be catastrophic because I don't think they would get the protections they think [[01:00:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3646.42s)]
*  they're going to get. [[01:00:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3648.7s)]
*  And I think the downsides would be would be profound. [[01:00:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3649.7s)]
*  But it is amazing how much naiveté there is by people who are pushing on this on this [[01:00:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3651.98s)]
*  argument. [[01:00:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3655.74s)]
*  And I think it's just literally people who haven't experienced what it's like in the [[01:00:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3656.74s)]
*  government. [[01:00:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3659.58s)]
*  Also, they haven't read the history. [[01:01:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3660.58s)]
*  I mean, there's just there are so many historical examples of, you know, so quote unquote regulation. [[01:01:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3661.9s)]
*  The great one is the banks. [[01:01:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3667.14s)]
*  Right. [[01:01:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3668.14s)]
*  So we have the global financial crisis, 2008. [[01:01:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3669.14s)]
*  The big conclusion from that was what we call the too big to fail banks were too big. [[01:01:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3670.62s)]
*  Right. [[01:01:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3674.62s)]
*  Which is why they had to get bailed out. [[01:01:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3675.62s)]
*  Right. [[01:01:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3676.62s)]
*  And so the conclusion is that we have to make those banks much smaller. [[01:01:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3677.62s)]
*  So they passed this law called Dodd-Frank in 2010. [[01:01:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3679.62s)]
*  As a consequence of that, those banks are now much, much larger. [[01:01:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3682.46s)]
*  Right. [[01:01:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3685.5s)]
*  The exact opposite of what they said they were going to do. [[01:01:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3686.5s)]
*  And then the creation of new banks in the U.S. has dropped to zero because that law [[01:01:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3688.18s)]
*  established this wall of regulation that you can't you basically cannot afford to start [[01:01:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3692.02s)]
*  a new bank to hire all the lawyers to be able to deal with the laws. [[01:01:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3696.06s)]
*  Whereas if you're JPMorgan Chase, you've got 10,000 lawyers. [[01:01:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3699.4199999999996s)]
*  You can spend infinite amounts of time dealing with the government. [[01:01:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3701.62s)]
*  And so the law that was marketed at us as breaking up the big banks, causing them to [[01:01:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3704.6s)]
*  be smaller, has actually achieved the exact opposite result. [[01:01:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3708.7s)]
*  And what you see in the history of regulation is that happens over and over and over and [[01:01:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3711.46s)]
*  over again. [[01:01:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3714.2599999999998s)]
*  Why? [[01:01:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3715.2599999999998s)]
*  Because banking is complicated, because the banks have a lot of lobbyists. [[01:01:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3716.7400000000002s)]
*  It's worth a lot of money to the people already in power to have this continue. [[01:01:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3719.86s)]
*  The politicians know that they're going to get jobs at the big banks when they step down [[01:02:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3723.82s)]
*  from their positions. [[01:02:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3727.0600000000004s)]
*  You know, the whole point of contact, the whole thing gets all screwed up. [[01:02:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3728.0600000000004s)]
*  And I think that's what's going to happen again. [[01:02:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3732.5600000000004s)]
*  The scary thing about AI is that it's happening so fast. [[01:02:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3735.7400000000002s)]
*  My fear is that decisions will be made before they truly understand what they're deciding [[01:02:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3741.98s)]
*  on, because the acceleration of the technology is so intense. [[01:02:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3747.26s)]
*  Yeah. [[01:02:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3751.66s)]
*  It's like a super panic. [[01:02:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3752.66s)]
*  Yeah, it's like a super panic moment. [[01:02:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3753.66s)]
*  Yeah. [[01:02:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3755.46s)]
*  And it's a particularly, I agree with you, it's a particularly vivid one right now, because [[01:02:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3756.46s)]
*  this technology, you know, AI is a field that's 80 years old. [[01:02:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3759.1s)]
*  It basically started working about six months ago. [[01:02:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3762.38s)]
*  It works really well, like all of a sudden. [[01:02:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3764.78s)]
*  Right? [[01:02:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3766.98s)]
*  And so that's freaked people out. [[01:02:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3767.98s)]
*  And then by the way, just the term is so freighted. [[01:02:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3768.98s)]
*  I mean, there's been so many science fiction movies over the years. [[01:02:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3770.26s)]
*  Yeah. [[01:02:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3772.38s)]
*  Right. [[01:02:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3773.38s)]
*  And so there's just like ambient panic, you know, in the air whenever this topic comes [[01:02:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3774.38s)]
*  up. [[01:02:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3776.26s)]
*  And then and then look, you've got people from these big companies showing up in Washington [[01:02:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3777.26s)]
*  scaring the pants off a lot of these people, you know, in pursuit of regulatory capture. [[01:03:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3780.38s)]
*  They're you know, they're scaring them silly. [[01:03:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3784.22s)]
*  And so they're sort of deliberately fostering kind of this sense of panic. [[01:03:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3786.22s)]
*  Has anybody invited you to come and speak at one of those things? [[01:03:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3789.46s)]
*  Yes, I have avoided the public ones. [[01:03:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3792.46s)]
*  But I've talked to a lot of I've talked I talked to a lot of people in D.C. who are, [[01:03:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3794.3s)]
*  you know, not not in front of the camera. [[01:03:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3797.1s)]
*  Why have you avoided the public ones? [[01:03:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3799.14s)]
*  Just because it's you've seen them. [[01:03:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3800.54s)]
*  The public ones are not where the discussion happens. [[01:03:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3804.8199999999997s)]
*  It's the public. [[01:03:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3806.86s)]
*  All the congressional hearings are to generate sound bites for each of those politicians [[01:03:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3807.86s)]
*  to be able to then use in their campaign. [[01:03:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3812.58s)]
*  Yeah, there's no public half the time that people ask. [[01:03:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3814.62s)]
*  This is the other fun thing is you see these people roll in and they ask these questions. [[01:03:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3818.42s)]
*  The congressmen senators, they're very clearly seeing the questions for the first time because [[01:03:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3821.3399999999997s)]
*  they were handed the questions by the staffer. [[01:03:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3824.94s)]
*  The chamber. [[01:03:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3827.18s)]
*  And you can tell because they like don't know how to pronounce all the words. [[01:03:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3828.4199999999996s)]
*  And so that's like that's the Kabuki theater basically side of things. [[01:03:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3831.14s)]
*  And then and then there's the and then there's the actual kind of backroom conversations. [[01:03:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3835.4199999999996s)]
*  And so, yeah, I'm talking to a lot of the people who are kind of in the in the backrooms. [[01:03:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3839.66s)]
*  Are they receptive to what you're saying? [[01:04:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3842.8199999999997s)]
*  You know, again, it's complicated because there's a lot of different people running around [[01:04:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3845.4199999999996s)]
*  in different motives. [[01:04:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3848.4199999999996s)]
*  I would say the smarter ones, I think, are quite receptive. [[01:04:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3849.54s)]
*  And I think the smart the smarter ones are generally aware of kind of how these things go. [[01:04:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3851.7s)]
*  And the smarter ones are thinking, yeah, it would be really easy here to cause a lot of damage. [[01:04:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3855.86s)]
*  But, you know, what you hear back is, you know, the pressure is on, you know, the, you know, [[01:04:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3858.94s)]
*  this the White House wants to do that, you know, wants to put out a certain thing by a certain date. [[01:04:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3862.7400000000002s)]
*  You know, the senator wants to have a law, you know, dot dot dot. [[01:04:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3866.1800000000003s)]
*  You know, the press is on us, you know, a lot of pressure. [[01:04:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3869.38s)]
*  So we got to figure something out. [[01:04:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3871.26s)]
*  And what are they trying to push this through by? [[01:04:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3872.38s)]
*  I mean, sort of as fast as possible. [[01:04:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3874.38s)]
*  Well, and then there's this rush thing, which is they're all they're all kind of aware that Washington is kind of panic driven. [[01:04:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3876.78s)]
*  You know, they kind of move from shiny object to shiny object. [[01:04:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3881.1800000000003s)]
*  So to get anything through, they kind of got to get it through while it's still in a state of panic. [[01:04:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3883.34s)]
*  Like if it's no longer in a state of panic, it's harder to get anything done. [[01:04:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3887.1000000000004s)]
*  So there's this weird thing where they kind of want it to happen under a state of panic. [[01:04:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3891.1000000000004s)]
*  By the way, the other really amazing thing is I can have the exact I can have two conversations with the exact same person. [[01:04:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3894.34s)]
*  And the conversations go very differently. [[01:05:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3900.1800000000003s)]
*  Conversation A is the conversation of what to do in the United States between the American government and the American tech companies. [[01:05:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3901.6200000000003s)]
*  And that's generally characterized by the American government very much hating the tech companies right now [[01:05:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3907.7400000000002s)]
*  and wanting to damage them in various ways and the tech companies wanting to figure out how to how to fix that. [[01:05:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3912.1s)]
*  There's a whole second conversation, which is China. [[01:05:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3917.74s)]
*  And the minute you open up the door to talk about China and what China is going to do with AI and what that's going to mean for this new Cold War that we're in with China, it's a completely different conversation. [[01:05:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3920.62s)]
*  And all of a sudden, it's like, oh, well, we need American AI to succeed and we need American technology companies to succeed and we need to beat the Chinese. [[01:05:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3928.8199999999997s)]
*  And it's a totally different dynamic once you once you start that that conversation. [[01:05:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3935.22s)]
*  So that's the other part. [[01:05:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3940.62s)]
*  And by the way, I think that's like a super legitimate, like actually very interesting and important question. [[01:05:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3942.06s)]
*  And so one of my hopes would be that people start thinking outside of just our own borders and start thinking about the broader global implications of what's happening. [[01:05:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3946.74s)]
*  I want to bring you back to what you're saying about the government and the tech companies. [[01:05:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3953.42s)]
*  So you think the government wants to destroy these tech companies? [[01:05:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3957.46s)]
*  There are there are a lot of people in the government who are very angry about the tech companies. [[01:06:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3960.2599999999998s)]
*  And well, a lot of it goes back to the 2015 2016 election. [[01:06:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3964.34s)]
*  There's a lot of people in power today who think that the president 2016 only got elected because basically social media, Internet companies. [[01:06:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3967.62s)]
*  And then there's a lot of people in government who are very angry about business in general and maybe aren't huge fans of capitalism, get upset about those things. [[01:06:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3976.02s)]
*  So there's a lot of general anti-tech kind of energy in Washington. [[01:06:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3982.7799999999997s)]
*  And then these big tech companies, their approach to dealing with that is not typically to fight that head on, but rather to try to sort of co-opt it. [[01:06:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3987.42s)]
*  And so and this is where they go to Washington. [[01:06:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3994.7799999999997s)]
*  They basically say, you got us. [[01:06:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3996.42s)]
*  We're guilty. You know, everything you say is true. [[01:06:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=3997.94s)]
*  We apologize. You know, we know it's all horrible. [[01:06:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4000.78s)]
*  And therefore, will you please regulate us? [[01:06:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4003.7000000000003s)]
*  Right. And some of these companies run ad campaigns actually asking for new regulation. [[01:06:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4006.0200000000004s)]
*  And then but but then when you know, but then the goal of the regulation is to get a regulatory barrier, right, to get a, you know, to set up a regulatory regime like Dodd-Frank, where if you're a big established company, you have lots of lawyers who can deal with that. [[01:06:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4009.3s)]
*  And then the goal is to make sure that startups can't compete, right. [[01:07:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4020.78s)]
*  To raise the drawbridge. [[01:07:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4024.02s)]
*  And this is what this is. [[01:07:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4025.26s)]
*  This characterizes so much of sort of American business industry today. [[01:07:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4026.62s)]
*  There's a thing about all these sectors, American business, defense contracting media companies, drug companies, banks, insurance companies, you know, right down the list. [[01:07:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4029.98s)]
*  Right. Where it's like there's two or three or four big companies that kind of live forever. [[01:07:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4039.02s)]
*  And then there's basically like no change. [[01:07:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4044.18s)]
*  And then those companies are basically in this incestuous relationship with the government, where the government both regulates them and protects them against competition. [[01:07:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4045.86s)]
*  And then there's the revolving door effect where government officials, when they step down from government, they go to work for these companies. [[01:07:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4053.22s)]
*  Right. And then and then people in people get recruited out of these companies to work in government. [[01:07:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4058.54s)]
*  Right. And so so we think we live in like a market based economy. [[01:07:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4063.4599999999996s)]
*  But in a lot of industries, what you have are basically cartels. [[01:07:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4067.14s)]
*  Right. You have a small number of big companies that are basically have established basically a pair sort of a two way parasitical relationship with with the government, where they're sort of both sort of controlled by the government, but also protected by the government. [[01:07:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4070.7s)]
*  And so the big tech companies would like to get to that state like that. [[01:08:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4083.54s)]
*  That is a very desirable thing, because otherwise they're just hanging out there subject to being both attacked by the government and being attacked by startups. [[01:08:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4087.54s)]
*  And so that's the underlying game that the big companies keep trying to play. [[01:08:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4094.54s)]
*  And of course, it's incredibly dangerous for multiple reasons. [[01:08:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4098.219999999999s)]
*  One is the ring of power reason we talked about. [[01:08:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4101.5s)]
*  Two is just stagnation. [[01:08:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4103.78s)]
*  Right. When this happens, whatever market that is just stops changing. [[01:08:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4105.0599999999995s)]
*  And then third is there's no new competition. [[01:08:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4109.26s)]
*  Right. And so those companies over time can do whatever they want. [[01:08:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4111.0199999999995s)]
*  They can raise prices. They can do play all kinds of games. [[01:08:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4113.62s)]
*  Because there's there's no market forces causing them to, you know, to try to stay on their toes. [[01:08:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4116.82s)]
*  This sounds like a terrible scenario that doesn't look like it's going to play out well. [[01:08:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4121.219999999999s)]
*  Yeah, I think it's it's it's set up. [[01:08:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4125.58s)]
*  It's right now. It's not good. [[01:08:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4127.46s)]
*  Right. Right now, the path that we're on is not good. [[01:08:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4128.86s)]
*  Like this is what's playing out. [[01:08:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4131.7s)]
*  You know, the sort of I mean, it would be nice if there was more popular outrage. [[01:08:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4133.86s)]
*  Having said that, you know, this is a new topic. [[01:08:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4139.86s)]
*  And so I understand, you know, people aren't fully aware what's happening yet. [[01:09:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4141.54s)]
*  But the other thing is like it may be the other reason for maybe mild optimism might be the open source movement is developing very quickly now. [[01:09:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4145.3s)]
*  And so if open source, I get really good before these regulations can basically be put in place, like they may become somewhat of a moot point. [[01:09:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4152.9800000000005s)]
*  Really? And so, yeah, for anybody looking at this, you want to look at both sides of this, you want to look at what both the companies are doing and how open source mitigate all these issues. [[01:09:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4160.42s)]
*  It basically just says instead of this technology being something that's owned and controlled by big companies, it's just going to be technology that's going to be available to everybody. [[01:09:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4168.3s)]
*  Right. And, you know, you'll be able to use it for whatever you want, just like I will. [[01:09:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4175.1s)]
*  And it's the same thing that happened for like for, you know, it's the way the Web works. [[01:09:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4178.820000000001s)]
*  You know, it's the way that anybody can download a Web browser. [[01:09:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4183.66s)]
*  It's the way that anybody can install these free operating, these free operating system called Linux. [[01:09:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4185.54s)]
*  You know, it's one of the biggest operating systems in the world. [[01:09:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4189.74s)]
*  And so just basically this this idea, you know, Wikipedia or any of these things where it's just it's it's a it's sort of a public good. [[01:09:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4192.5s)]
*  And so and it's available, you know, for free to anybody who wants it. [[01:09:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4199.26s)]
*  And then there's communities of volunteers on the Internet and companies that actually contribute a lot into this because because companies can build on top of this technology. [[01:10:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4202.14s)]
*  And so the hope here would be that there's going to be an open source movement kind of counterbalancing what the companies do. [[01:10:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4209.820000000001s)]
*  And if the open source movement does take hold, if people recognize this as being a real serious threat and start applying, you know, just using whatever it is, whether it's mines or the various open source [[01:10:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4214.9800000000005s)]
*  social media networks. [[01:10:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4228.820000000001s)]
*  Don't you think the government would somehow or another try to regulate that as well if they've already got control over Facebook and Twitter? [[01:10:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4231.5s)]
*  Well, that's the threat. So the threat always is that they're going to come in and do that. [[01:10:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4237.06s)]
*  And that is what they're threatening to do. [[01:10:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4240.1s)]
*  That there is energy in Washington by people trying to figure out how to regulate or ban open source. [[01:10:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4241.58s)]
*  I mean, so that banning open source, like interfering at that level, carries consequences with it. [[01:10:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4246.900000000001s)]
*  And so for and there are proposals, there are serious proposals from serious people to do what I'm about to describe. [[01:10:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4251.54s)]
*  Do you run it? Do you run a software program on everybody's own computer? [[01:10:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4256.5s)]
*  Right. Watching everything that they do is you have to make sure that they're not running software that's supposed to be running. [[01:11:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4260.0199999999995s)]
*  You know, do you have basically an agent built into everybody's chip so that it's not running software that's not supposed to be running? [[01:11:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4265.099999999999s)]
*  Right. And then what do you do when somebody is running unapproved software? [[01:11:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4270.74s)]
*  You know, do you send somebody to their house to take their computer away? [[01:11:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4274.78s)]
*  Right. And then if somebody like if you can't do that, like there's a proposal for the the AI safety people have a proposal that basically says if there's a rogue [[01:11:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4277.82s)]
*  data center, if there's a data center running AI that is not registered to the government, not being monitored, that there should be airstrikes. [[01:11:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4285.5s)]
*  Right. Jesus. [[01:11:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4291.18s)]
*  Yeah, there should be Time magazine, Time magazine, a big piece in Time magazine about two months ago, where one of these guys who runs this kind of [[01:11:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4292.38s)]
*  AI risk kind of world says clearly we should have military airstrikes on data centers that are running unapproved AIs because it's too dangerous. [[01:11:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4297.780000000001s)]
*  Right. And, you know, yes, yes. [[01:11:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4304.9400000000005s)]
*  Yes. Pausing AI development isn't enough. We need to shut it all down. [[01:11:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4307.780000000001s)]
*  So who the fuck is this? [[01:11:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4311.9400000000005s)]
*  So this is this guy. This is one of the leaders. [[01:11:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4313.5s)]
*  It's this guy named Yadkovsky. [[01:11:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4315.700000000001s)]
*  And so he's one of the leaders of this decision theorist. [[01:11:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4316.780000000001s)]
*  So he's one of the leaders of what's called AI risk. [[01:12:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4321.06s)]
*  Sort of one of the anti-AI groups. [[01:12:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4323.5s)]
*  He's part of the Berkeley environment that we were talking about before. [[01:12:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4327.22s)]
*  So he says the key issue is not human competitive intelligence, as Open Letter puts it. [[01:12:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4330.740000000001s)]
*  It's what happens after AI gets too smarter than human intelligence. [[01:12:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4334.780000000001s)]
*  Key thresholds there may not be obvious. [[01:12:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4338.58s)]
*  We definitely can't calculate in advance what happens when and it currently seems imaginable that a research lab would cross critical lines without noticing. [[01:12:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4340.740000000001s)]
*  Is that a real issue? [[01:12:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4350.22s)]
*  Well, so I don't think so. [[01:12:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4351.54s)]
*  I don't think so. But it is significant if you go further down. [[01:12:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4354.22s)]
*  What he said. What he says in that is he says, first of all, we need to do the airstrikes in the data centers. [[01:12:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4356.38s)]
*  And I think it's in this. I think it's in this article or if it's not, it's in another one where he says we need to. [[01:12:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4359.820000000001s)]
*  He's the word he's used, I think, is we need to be able to take the risk of nuclear war. [[01:12:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4364.740000000001s)]
*  Well, because the problem is, OK, we're we're we're striking data centers. [[01:12:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4369.18s)]
*  Does that mean we're striking data centers in China? [[01:12:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4372.14s)]
*  And how are the Chinese going to feel about that? [[01:12:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4374.820000000001s)]
*  Right. And how are they going to retaliate? Right. [[01:12:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4377.66s)]
*  So like you go down this path where you're worried about the AI getting out of control and you start you start advocating basically a global totalitarian basically surveillance state that watches everything and then basically takes military action when the computers are running software. [[01:12:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4379.18s)]
*  You don't want it to run. And so the consequences here are profound. [[01:13:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4392.18s)]
*  It's a very big deal. He's a guy spoken publicly about. [[01:13:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4397.0199999999995s)]
*  Oh, yes. But for 20 years. Yeah. [[01:13:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4399.98s)]
*  He was just not taken. He was he was not widely known until about six months ago when all of a sudden, Chedgy PT started to work and then he just took everything he'd said publicly before he applied it to Chedgy PT. [[01:13:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4402.18s)]
*  Yeah. So in his in his kind of model of the world, Chedgy PT proves that he was right all along and that we need to we need to move today to touch. [[01:13:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4412.54s)]
*  We need we need to shut down Chedgy PT today and we need to never do anything like it. [[01:13:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4418.58s)]
*  So he's got the Sarah Connor approach. [[01:13:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4421.74s)]
*  Very much so. Yes. He's Sarah Connor without the without the time travel and the sex appeal. [[01:13:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4423.9s)]
*  So so so so but funny thing. [[01:13:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4433.0199999999995s)]
*  OK. So so he's part of a movement they call themselves a risk or X risk or a safety. [[01:13:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4436.98s)]
*  And it's it's again it's one of these Berkeley Berkeley San Francisco things and it's basically the killer AI kind of theory. [[01:14:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4443.3s)]
*  So there's that and we can talk about that. [[01:14:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4449.0199999999995s)]
*  But what's happened is there. [[01:14:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4450.74s)]
*  Yeah. Here we go. [[01:14:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4452.78s)]
*  Mortar and being violated. [[01:14:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4454.58s)]
*  We will destroy a rogue data center by air strike. [[01:14:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4456.02s)]
*  Oh my God. [[01:14:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4461.9400000000005s)]
*  Yes. Preventing preventing AI is considered a priority above preventing a nuclear exchange. [[01:14:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4463.78s)]
*  Allied nuclear countries are willing to run some risk of nuclear exchange if that's what it takes to reduce the risk of nuclear exchange kills everyone. [[01:14:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4468.54s)]
*  Yeah. How could you say that. [[01:14:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4474.46s)]
*  That's so crazy. [[01:14:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4476.02s)]
*  Yes. Oh he's alone. [[01:14:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4477.46s)]
*  Well so he's he's he's very serious. [[01:14:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4480.38s)]
*  His views have traction in Washington. [[01:14:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4484.660000000001s)]
*  Really. There are quite a few people in Washington who are worried about this. [[01:14:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4487.26s)]
*  But but here's what. So here's what's here's what's interesting. [[01:14:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4489.46s)]
*  So so he's been he and people like him this whole group of people who work on this have been worried about this and developing theories about this for 20 years and they've been publishing on this and talking about this and they were kind of it was kind of abstract like I said until six months ago. [[01:14:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4491.900000000001s)]
*  And now they're getting some traction and their ideas are being taken seriously. [[01:15:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4504.38s)]
*  But so that they're they're worried about literally people dying. [[01:15:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4507.94s)]
*  There's another set of people who are trying to control AI who were like the social media sensors that are trying to control what it says. [[01:15:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4511.38s)]
*  And so what's happened is the AI safety movement that was worried about people dying has been hijacked by the people who want to control what it says. [[01:15:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4517.7s)]
*  And those it turns out those two groups of people hate each other. [[01:15:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4524.66s)]
*  So the safety people think that the so-called the other group is called the alignment people. [[01:15:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4528.139999999999s)]
*  The other group is called the alignment people. [[01:15:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4532.3s)]
*  The safety people who are worried about people dying think that the alignment people are hijacking the critically important safety movement in order to basically control what the thing says. [[01:15:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4534.38s)]
*  The people who want to control what the thing says think that the safety people worried about killing everybody are like lunatics and they call each other names all day long. [[01:15:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4543.18s)]
*  The original group his group has renamed themselves from AI safety to they now call themselves AI not kill everyone ism because they're trying to just get it like focused on what they call like actual actual existential risk. [[01:15:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4551.900000000001s)]
*  But the overall movement has been taken over by the sensors. [[01:16:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4563.860000000001s)]
*  Right. And so and what's happening is in Washington these these these concerns are getting conflated. [[01:16:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4566.820000000001s)]
*  Right. And so they sort of bait the hook with it might kill everybody. [[01:16:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4571.3s)]
*  And then what comes out the other end is basically a law restricting what it can say. [[01:16:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4573.9800000000005s)]
*  Right. And so this is the level of panic and hysteria and right. [[01:16:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4577.54s)]
*  And then and then potentially like again very very kind of damaging you know potentially catastrophic you know legal things that are going to happen on the other side of this. [[01:16:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4582.66s)]
*  I just can't imagine a sane world where someone would take that guy seriously. [[01:16:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4591.1s)]
*  Airstrikes a full nuclear assault is preferable to AI taking over. [[01:16:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4596.46s)]
*  So his his argument his argument his argument is once once you have a quote unquote runaway AI that's just like overwhelmingly smarter than we are then they can basically do you know basically you can do whatever it wants and it basically has a relationship to us like we have to answer like you step on an ant and you don't really care. [[01:16:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4601.46s)]
*  Right. Right. And if you build as many killing machines as you want. [[01:16:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4615.58s)]
*  There no fear of that if you extrapolate AI technology into the future. [[01:16:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4618.74s)]
*  I don't think so. [[01:17:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4624.38s)]
*  And I don't think so. [[01:17:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4625.900000000001s)]
*  And I have a bunch of reasons for thinking that I'll just give you a very very basic one is one of the things that they say is basically anything smarter is always in charge of anything dumber. [[01:17:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4627.02s)]
*  Right. So if you have a smarter thing it's going to be in charge of a dumber thing a smarter person is going to be a smarter thing is going to be able to talk a smarter person in anything a smarter thing will ultimately always be in charge we will win whatever political contest or be able to take control of power. [[01:17:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4634.62s)]
*  To which my response is does our society seem like one that's being run by the smart people. [[01:17:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4649.06s)]
*  Right. [[01:17:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4653.34s)]
*  Like if you take all the smartest people you know in the world are they in charge. [[01:17:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4654.139999999999s)]
*  Right. And who are they working for. [[01:17:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4658.34s)]
*  And would you say that the people they're working for are smarter or dumber than they are. [[01:17:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4660.58s)]
*  Right. And so like I just like I think that the whole basis for this like smart always wins versus dumb is just like not right. [[01:17:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4663.78s)]
*  Number two there's there's this anthropomorphizing thing that happens where you see him doing it in that essay he basically basically starts to compute motives. [[01:17:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4669.34s)]
*  Right. So it's like basically that the AI is going to be a like some level of self aware you know basically it's a Terminator scenario like it's going to make up and it's going to decide it's like an answer them scenario. [[01:17:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4677.5s)]
*  But like it's not it's not what it is it's not how it works right. What it does is it basically sits there and you ask a question and it answers you and it hopes that you're happy with the answer like that. [[01:18:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4686.419999999999s)]
*  We're not dealing with. [[01:18:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4694.379999999999s)]
*  For now though. [[01:18:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4695.219999999999s)]
*  For now. But like that's that's how it's built. [[01:18:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4696.219999999999s)]
*  Again this is here's another reason I don't believe it is because the great surprise of chat GPT chat GPT is a technology called large language models which is based on a research breakthrough in 2017 at Google which is called the transformer. [[01:18:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4699.38s)]
*  It took the technical field completely by surprise that this works right. So none of the people working on a risk prior to basically December had any idea that this is going to work any more than the rest of us did like this. [[01:18:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4712.86s)]
*  This is like a massive surprise. So there's all these ideas there's all these sort of very general hand wavy concepts around quote unquote AI that basically were formulated before we actually knew what the thing was and how it works and they. [[01:18:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4723.74s)]
*  And none of their views have changed based on how the how the technology actually actually functions. [[01:18:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4735.22s)]
*  And so it's sort of it comes across to me more as a religion kind of being it kind of doesn't in their framework it kind of doesn't matter how it works because it's basically just assume that what however it works is going to behave in a certain way and I'm an engineer and like things don't work like that. [[01:19:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4740.94s)]
*  But aren't they evaluating how it works now and aren't they evaluating chat GPT and if chat GPT is just the beginning. [[01:19:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4753.0599999999995s)]
*  This is the beginning of this and then you have something that's far more complex and something that is sentient or something that's capable of making decisions. If that's engineered. [[01:19:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4760.98s)]
*  But you just took that again we just took this a little bit we talked about you just took the leap to like okay now suddenly become sentient. [[01:19:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4768.9s)]
*  And it's like okay we don't know why humans are sentient. [[01:19:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4774.0599999999995s)]
*  Well let's not even use the term sentient but capable of rational thought or decision making. [[01:19:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4777.259999999999s)]
*  But those are two different things. [[01:19:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4782.7s)]
*  Right. But if it decides things but if there's no it starts making actions and deciding things. This is the worry that it becomes capable of doing things. [[01:19:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4783.86s)]
*  But there's no yes so it will be capable of doing things it will be it will have it. But there's no it's there's no it's there's no genie in the bottle. [[01:19:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4792.78s)]
*  For now for now. [[01:19:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4799.66s)]
*  But isn't it possible. [[01:20:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4802.3s)]
*  Developed. [[01:20:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4803.900000000001s)]
*  Okay. So this is the other thing that happens. So I this is the line of argument. So I actually look this up. This is a line of argument that's very commonly used as you represented in this world. It's actually. [[01:20:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4804.34s)]
*  Aristotle first didn't identify this line of argument and it's he calls it the argument for ignorance. But by which he means the argument for lack of evidence. Right. It's basically the argument of well you can't rule out that X is going to happen. [[01:20:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4813.34s)]
*  True. Well the problem is at that point you can't rule anything out right at that point you have to plan for every contingency of every conceivable thing that you could ever imagine and you can never disprove anything so you can never have a logical debate. [[01:20:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4823.14s)]
*  Right. So at that point you've basically slipped the bounds of reason you're purely in a religious territory. [[01:20:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4832.7s)]
*  Because there's how does science work. Science works when somebody formulates a hypothesis. [[01:20:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4837.42s)]
*  And then they test the hypothesis and the basic requirement of science is that there's a testable hypothesis that is what they call falsifiable. [[01:20:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4842.780000000001s)]
*  So there is some experiment that you can run to basically establish that something is the hypothesis is not in fact true. And this is basically how science has always worked. [[01:20:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4849.46s)]
*  And then by the way there's always a way to measure right. What is the actual like what is the actual progress that you're making on the experiment that you're doing. [[01:20:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4856.46s)]
*  And on all this like a safety stuff that I've been able to find and read like they have none. There's none of that. There's speculation. [[01:21:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4862.18s)]
*  There's no hypothesis. There's no test. There's no example. There's no evidence. There's no metric. There's nothing. It's just speculation. [[01:21:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4868.3s)]
*  Right. But we could sit here and speculate about millions of things. Yes. We could speculate about an impending alien invasion and spend that you know argue that society should spend the next hundred years preparing for that because we can't rule it out. [[01:21:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4876.1s)]
*  And so we just as human beings we just we do not have a good track record of making decisions based on unfounded speculation. We have a good track record of making decisions based on science. [[01:21:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4886.379999999999s)]
*  Right. And so the correct thing to do for people worried about this is to actually propose experiments. Right. Be able to propose a scenario in which the bad thing would actually happen and then test to see whether that happens. Right. And so like design a system that shows like the first glimmer of any of the behavior that you're talking about. Right. [[01:21:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4894.7s)]
*  But not even behavior just capabilities as ultimately as the capabilities rise of these things and you're you're dealing with far more sophisticated systems. This is the beginning. Right. We're chat. G.T. 4.5 or whatever. [[01:21:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4909.98s)]
*  We're at when new emerging technologies that have similar similar capabilities but extend that and keep going. It just seems like that's the natural course of progression. [[01:22:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4922.38s)]
*  The natural course of progression is not for that to all of a sudden decide it has a mind of its own. Not all of a sudden. No. Or even over time. There's no. Never. [[01:22:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4932.66s)]
*  This goes back to our conversation last time. All right. This gets in a tricky territory. So. Yes. OK. So let me let's try to define terms. Let's try to find out how would we define something that is. And you pick your term here self aware sentient conscious has goals is alive is going to make decisions on its own. [[01:22:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4939.46s)]
*  Whatever term you want whatever. Well let's just say a technology that mimics the human mind and mimic mimics the capabilities and interactions of the human. But we don't know how the human mind works. [[01:22:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4956.7s)]
*  But we do know how people use the human mind in everyday life. And if you could mimic that with our understanding of language with ration rational thought with reason with the access to all the information that it'll have available to it just like chat GPT. [[01:22:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4969.099999999999s)]
*  If you see what you're doing to say if if if if if. Yes. Right. Yeah for sure. So there are these I just read this there's this article in nature this week there are these there's a neuroscientist and a philosopher who placed a bet 25 years ago as to whether we would in [[01:23:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4985.58s)]
*  25 years know the site the scientific basis of human consciousness and they placed a bet for a case of wine 25 years ago and the neuroscientist predicted of course in 25 years we're going to understand how consciousness works human consciousness and the philosophers like no we're not. [[01:23:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=4998.78s)]
*  25 years passed and it turns out the philosopher won the bet like and then the neuroscientist just says openly yeah he's like I thought we'd have it figured out by now we actually still have no idea. [[01:23:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5013.219999999999s)]
*  Like sitting here today sitting here today the actual biological experts scientists who actually know the most about human consciousness or an estheticians. [[01:23:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5021.7s)]
*  The person who flips off the light switch in your brain when you go under for surgery. [[01:23:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5031.06s)]
*  All we know we know how to turn it off. The good news is they also know how to turn it back on. Yeah. They have no broader idea of like what that is. And so again there's this this is what they call it. [[01:23:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5036.58s)]
*  Anthropomorphizing. [[01:24:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5046.900000000001s)]
*  There's this sort of very human instinct to try to basically see human behavior and things that aren't human. Right. And it would be like if that were the case then we would have to think about that and study that. But like we don't have that. We don't know how that happens. We don't know how to build that. We don't know how to replicate that. So. [[01:24:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5049.1s)]
*  Like I said at that point of speculation that that's not the actual technology that we're dealing with today. So here's my favorite. Here's my favorite. Here's my favorite counter example on this. So. [[01:24:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5062.38s)]
*  Let's say let's say let's say let's say something has the following properties. Right. Let's say that it has an awareness of the world around it. [[01:24:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5070.54s)]
*  It has a goal or an objective for what it wants to achieve in the world around it. It has the wherewithal right to be able to reach into the world to be able to change the world to accomplish its goal. [[01:24:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5078.0199999999995s)]
*  It's going to be in a state of increased tension if it can't [[01:24:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5089.66s)]
*  achieve its goal and it's going to be a state of relaxation if it can't achieve its goal. [[01:24:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5094.06s)]
*  We would describe that as probably pretty good first order approximation of like some sort of conscious right entity right that would have the characteristics that we're worried about. [[01:24:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5098.18s)]
*  We've just described a thermostat. [[01:25:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5106.26s)]
*  Right. It's just on the wall. [[01:25:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5108.9800000000005s)]
*  It senses the environment temperature. It has a goal for the temperature it wants. It has the ability to change the setting on the the the the the the heater the AC unit. [[01:25:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5111.780000000001s)]
*  And it literally goes into a state of physical tension when it when that when the temperature is not what it wants and then it goes into a state of physical relaxation right literally inside the mechanism when it gets back into the state where it has a desired temperature. [[01:25:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5123.62s)]
*  And like we're not worried about the thermostat like coming alive and killing us. Right. And so there's there's just there's there's a there's a like even those properties alone are not sufficient to generate concern. [[01:25:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5134.5s)]
*  Much less the idea of basically the way we know how to build neural networks today. [[01:25:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5144.9400000000005s)]
*  And then again you go back to this thing of like OK let's let's assume that you actually agreed with the concern and that you actually were legitimately concerned and that you you know you thought that there was disaster in the future here. [[01:25:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5149.34s)]
*  How do you feel about walking down the path that would be required to offset that right. What would be the threshold of evidence that you would want to demand before you start monitoring what everybody's doing on their computers before you start doing airstrikes. [[01:25:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5159.3s)]
*  Well never suggest that. [[01:26:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5169.820000000001s)]
*  Well but that's what's required right. If you know to stop it in order to stop it like if you believe that any if you if you believe that at some point it will turn into something that's a threat. [[01:26:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5172.0599999999995s)]
*  Right. And that that threat is existential. Right. Because it's going to be the super smart thing it's going to take over the nuclear arsenals it's going to synthesize new pathogens and it's going to kill us all. [[01:26:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5181.0199999999995s)]
*  Right. Then obviously you have to have an incredibly invasive regime to prevent that from happening because that's an all or nothing proposition. [[01:26:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5189.66s)]
*  Right. And that's the other tip off of what's happening here right which is you see there's no shades of gray in the article in this discussion. Shades of gray right. It's either it's going to kill us all or it's going to be totally harmless. [[01:26:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5195.860000000001s)]
*  Right. What is he wants position because he's called for a pause in the. So you know his position is actually quite interesting. So and actually Elon and the guy you just put up there actually have quite a bit of actually start disagreement right now. So [[01:26:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5206.540000000001s)]
*  And I'm going to try to act. It's always dangerous to try to channel Elon because he's a very smart creative guy. So I'm going to do my best to accurately represent. So he read this literature on this topic about 10 years ago and he got very concerned about this. [[01:26:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5219.14s)]
*  And then he was actually the actually he's talked about this now he gave a TV interviewer he talked about this he actually [[01:27:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5234.34s)]
*  He actually talked to Larry Page about it when Larry Page was running Google. And at the time Google is actually where this most recent breakthrough was invented this transformer breakthrough. [[01:27:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5239.259999999999s)]
*  So Google was working on this back you know 10 years ago with what's now chat GPT. And so he went and talked to Larry about his concerns about AI and Larry's like oh there's nothing to worry about. And he was like well I don't know what he means. [[01:27:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5246.94s)]
*  There's nothing to worry about Larry's like look if they replace us they replace us like they'll be our children and like we will have done the universe like a great service. It'll be fine. [[01:27:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5257.14s)]
*  And Elon said what that sounds like you don't care whether the future of you know the Earth is you know humans are eyes and in response. Elon says that Larry called him a speciesist. [[01:27:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5265.22s)]
*  Oh boy. [[01:27:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5276.22s)]
*  So so Elon now by the way knowing Larry I think there are 50 50 odds that he was being serious and joking. It's possible he was being serious. It's also possible he was just wanting you on up. I actually don't know which it was. [[01:27:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5277.54s)]
*  Both scenarios are fairly entertaining. [[01:28:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5291.3s)]
*  Elon's conclusion from that was not only is AI dangerous specifically Google owning and controlling AI is specifically dangerous. [[01:28:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5293.0599999999995s)]
*  Because Larry Page controls Google and so therefore if Larry Page controls Google Google gets AI that Larry will basically not he'll basically and basically let the AI do whatever it wants including exterminate humanity. [[01:28:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5300.5s)]
*  So Elon started open AI right. So the company behind chat GPT that was actually originally started by Elon with Sam Altman who runs it now and a bunch of other people in the valley. [[01:28:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5310.46s)]
*  The specific mission of open AI is right there on the name the specific mission of it is we're going to create AI we're going to compete with Google we're going to create an AI but we're going to make it open so that everybody has it. [[01:28:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5319.46s)]
*  Specifically so that it's not just Google right. So the original open AI mission was literally open source AI that everybody's going to have so that it's not just Google. [[01:28:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5328.86s)]
*  This guy is freaked out and it's like wait a minute if you think AI is dangerous that's the exact opposite thing than what you should do right because you think AI is dangerous. [[01:28:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5337.7s)]
*  Then the last thing in the world that you want to do is actually like give it to everybody. It's like giving everybody nuclear weapons right like why on earth would you think that that's a good idea. [[01:29:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5347.38s)]
*  And he wants like well look maybe whatever but like I certainly know that I don't want Larry to control it. [[01:29:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5355.42s)]
*  Subsequent to that Elon actually there was a there was a bunch of changes at open AI and as a result Elon became no longer involved in open AI at a certain point. [[01:29:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5361.14s)]
*  And then open AI basically went from being open AI to being closed AI right. So they're specifically not doing open source they started as a nonprofit now they're a business. [[01:29:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5368.7s)]
*  Right and then they went from being open source to being very much not open source and today you can use chat GPT but they will not they won't even tell you fully how it works much less you'll give you access to the code they they they're now a company. [[01:29:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5377.42s)]
*  Right like any other company and so Elon has said publicly that he's very upset about this change because he donated 100 million dollars to them to get it started as a nonprofit and then it became a company right. [[01:29:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5387.94s)]
*  And sort of against his wishes and so now it's he sort of views it as sort of an equivalent threat to Google right so now in Elon's mind he's got opening out to worry about and he's got Google to worry about. [[01:29:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5398.259999999999s)]
*  And so he he has talked publicly about possibly forming a third option which he is ultimately I think called either like actually open AI or sometimes he calls based AI. [[01:30:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5406.78s)]
*  Right right which would be a new thing which would be like the original opening idea but done from scratch in 2023 but like set up so that it can never be closed down. [[01:30:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5419.9s)]
*  And then once again the people in the AI risk movement are once again like oh my god that'll make the problem even worse what are you doing. [[01:30:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5428.26s)]
*  Right and so that's the yeah that's the current state of play. [[01:30:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5434.860000000001s)]
*  And then by the way this is all kind of playing out at this level in Washington most of the engineers working in this stuff are just like writing code trying to get something to work. [[01:30:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5438.06s)]
*  And so for every one of the people engaged in this public discussion you've got you know 10,000 people at universities and you know companies and. [[01:30:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5446.3s)]
*  People all over the world in their basements and whatever working on trying to get some aspect of this to work trying to build the open source version. [[01:30:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5452.66s)]
*  Are we aware of what other countries like what level they're at with this stuff. [[01:30:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5458.18s)]
*  Yeah so China so so I would say good news bad news good news bad news is this is almost entirely a US China thing internationally. [[01:31:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5463.94s)]
*  The UK had quite a bit of this stuff with this thing called deep mind which was a unit of Google that actually originally got got Elon concerned. [[01:31:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5471.3s)]
*  But deep mind is being merged into the mothership at Google and so it's sort of getting drained away from the UK and it's going to become more Californian. [[01:31:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5477.3s)]
*  And then you know there's there's there's smatterings of people in other countries other Western you know other European countries there are experts at various universities but not that many. [[01:31:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5485.1s)]
*  Most of it is in the US most of us in California in the West and then there's and then there's China. [[01:31:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5493.02s)]
*  So good news there aren't 20 other countries that have this but there are two and they happen to be you know the two big ones. [[01:31:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5498.58s)]
*  And so there is a big corresponding Chinese development effort that's been underway for the last you know 15 years just like the efforts in the in the US China. [[01:31:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5505.54s)]
*  China is actually very public about their AI kind of agenda mission they talk about it they publish it and of course they have a very different right theory of this than we do. [[01:31:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5513.34s)]
*  Right they view AI as a way to achieve population control right. [[01:32:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5521.62s)]
*  Really. [[01:32:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5525.54s)]
*  Yeah yeah they're authoritarians right and so they the number one priority for Chinese leadership is always that the population of China's standard control right and not revolt right or expect to be able to vote right or whatever right. [[01:32:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5526.18s)]
*  Anything that would threaten the the the dominance of the kind of the Communist Party of China. [[01:32:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5537.94s)]
*  Right. [[01:32:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5541.06s)]
*  And so they you know so for example China's security camera companies are the world leaders in AI security cameras because they're really good at like snipping out you know people walking down the street. [[01:32:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5541.38s)]
*  Right. [[01:32:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5548.660000000001s)]
*  Like that's the kind of thing that their their their systems are really good at. [[01:32:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5549.02s)]
*  And so they have a whole they have a whole national development program which is their their government and their company you know in China all the companies are actually controlled and owned effectively by the government. [[01:32:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5552.46s)]
*  There's not there's not as much of a distinction between public sector private sector as there is here so the China has a more organized effort that couples basically their whole society. [[01:32:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5560.7s)]
*  And then they have a program to basically use AI for population control inside China authoritarian political control and then they've got this program called digital Beltman road where they're going to try to just basically take a step forward and take control of the whole thing. [[01:32:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5570.02s)]
*  to basically use AI for population control inside China, authoritarian political control. [[01:33:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5581.54s)]
*  And then they've got this program called Digital Belt and Road, where they're going to basically [[01:33:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5586.46s)]
*  try to install that AI all over the world. They've had this program for the last 10 years to be the [[01:33:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5591.14s)]
*  networking layer for the world, so this whole 5G thing with this company called Huawei. So they've [[01:33:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5597.74s)]
*  been selling all these other countries all the technology to power their 5G wireless networks, [[01:33:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5604.0199999999995s)]
*  and then they're basically going to roll out on top of that this kind of AI, authoritarian, [[01:33:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5609.66s)]
*  basically surveillance control, population control stuff. On top of the other infrastructure. They [[01:33:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5614.099999999999s)]
*  have the Huawei 5G stuff, they've got what they call smart cities, so they've got a bunch of [[01:33:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5622.26s)]
*  software they've already sold a bunch of countries to basically run a city, to run public [[01:33:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5626.099999999999s)]
*  transportation and traffic control and all these things, and that's got their security cameras [[01:33:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5630.22s)]
*  built in everything. And then of course what they pitch to the president or prime minister [[01:33:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5634.0199999999995s)]
*  is if you install our stuff, you'll be able to better control your population. If you install [[01:33:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5638.82s)]
*  the American stuff, who knows, they're Americans, they're crazy, democracy, freedom, all that stuff. [[01:34:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5643.42s)]
*  Like in China, we want things controlled. And of course a lot of people running a lot of countries [[01:34:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5648.06s)]
*  would find the China model quite compelling. So there's two very different visions. This is [[01:34:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5653.3s)]
*  like the Cold War with the Soviet Union, right? There's two very different visions for how society [[01:34:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5659.14s)]
*  should be ordered. There's two very different visions for how technology should be used to order [[01:34:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5663.74s)]
*  society. There's two very different visions on whether people should have access to technology [[01:34:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5667.78s)]
*  or just the government. But in the Soviet Union, it was illegal to own a photocopying machine. [[01:34:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5672.46s)]
*  You'd get executed for owning a mimeograph or photocopying machine, because it was such a [[01:34:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5678.219999999999s)]
*  threat that you'd be able to publish information that wasn't propaganda coming from the government. [[01:34:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5682.7s)]
*  And so China's not quite that bad, but they're getting there. And so there are these two visions, [[01:34:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5687.139999999999s)]
*  there are these two approaches to technology, there are these two plans to propagate that out. [[01:34:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5693.74s)]
*  In the US, what we do is we have companies build this stuff and we have them go out and sell it, [[01:34:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5697.860000000001s)]
*  right? Or we have open source developers who go out and make it for free. In China, [[01:35:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5701.46s)]
*  it's more of a top-down directed kind of thing. And so that's the thing. It's like once you start [[01:35:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5705.42s)]
*  thinking in those terms, you realize that actually all these debates happen in the US are interesting [[01:35:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5710.46s)]
*  and maybe important, but there's this other much bigger, I would argue more important thing that's [[01:35:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5714.5s)]
*  happening, which is what kind of world do we think we're living in 50 years from now? And do we think [[01:35:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5718.3s)]
*  that the sort of American Western ethos of freedom and democracy is the one that technology supports? [[01:35:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5722.72s)]
*  Or do we think it's going to be a totalitarian approach? Either way, I see a scenario in 50 [[01:35:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5726.78s)]
*  years. It's unrecognizable. It's possible. Yeah. Well, as I'll declare, I don't want to live in the [[01:35:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5733.099999999999s)]
*  Chinese one, right? Like I think that's a bad idea. That seems inescapable. In the Chinese one, [[01:35:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5738.38s)]
*  it's the Chinese one. It's like, you know, there are no rights. The whole concept of rights is a [[01:35:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5744.3s)]
*  very Western. Yes. Right. And so the idea that you're like walking down the street and you have the [[01:35:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5749.54s)]
*  right to stop and talk to whoever you want or say whatever you want is like not a, you know, it's [[01:35:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5754.38s)]
*  not the majority view of, you know, a lot of people around the world, especially people in power. Even [[01:35:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5759.22s)]
*  in the US, we struggle with it, right? And so the real battle for AI is whether or not that gets [[01:36:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5764.74s)]
*  enhanced or whether or not we develop a system in America that actually can counter that. Yeah. Yeah. [[01:36:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5771.38s)]
*  Yeah. And then also whether we as individuals will have access to this power that we can use ourselves. [[01:36:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5776.900000000001s)]
*  So, you know, the movie or the novel became a movie, but the 1984, right? Those are just sort of the [[01:36:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5783.580000000001s)]
*  Orwell, you know, totalitarian kind of thing that people use as a metaphor. So the technology in the [[01:36:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5790.42s)]
*  novel 1984 was what Orwell called the telescreen and basically television. And basically the [[01:36:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5795.740000000001s)]
*  idea was television with a camera in it. And the idea was every room you had to have a telescreen [[01:36:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5800.820000000001s)]
*  in every room in your house. And it was broadcasting propaganda 24 seven, and then it was able to watch [[01:36:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5805.02s)]
*  you. Right. And that was the method of state control in 1984. There's this guy who wrote a different, [[01:36:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5808.74s)]
*  rewrote 1984 in a book called Orwell's Revenge. And in that book, what he does, he said, okay, [[01:36:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5816.139999999999s)]
*  we're going to use that same setup, but the telescreen, instead of being a one-way system, [[01:37:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5820.34s)]
*  is going to be a two-way system. Right. So the telescreen is going to be able to broadcast [[01:37:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5823.46s)]
*  propaganda and watch the citizens, but also it's going to be able to, people can actually put out [[01:37:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5827.139999999999s)]
*  whatever message they want, right? Free speech to be able to say whatever they want. And you're [[01:37:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5831.78s)]
*  going to be able to watch the government. It's going to have cameras pointed at the government. [[01:37:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5836.26s)]
*  Right. And then he rewrites the whole plot of 1984. And of course, the point there is, right, [[01:37:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5839.34s)]
*  if you equalize, if both the people and the state have the power of this technology at their [[01:37:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5844.14s)]
*  fingertips, at the very least, now there's a chance to have some sort of like actual rational [[01:37:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5848.74s)]
*  productive relationship where there are still human freedoms. And maybe people actually end up [[01:37:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5853.46s)]
*  with more power than the government and they can keep the government from becoming totalitarian. [[01:37:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5857.34s)]
*  Right. And so in his rewriting, what happens is the, you know, people use, rebels who want a [[01:37:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5860.06s)]
*  democracy, you know, use the broadcast mechanism out to be able to ultimately change the system. [[01:37:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5866.5s)]
*  And so that's the fundamental underlying question here as well, which is like, is AI a tool to [[01:37:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5871.42s)]
*  watch and control us or is AI tool something for us to use to become smarter, better informed, [[01:37:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5877.580000000001s)]
*  more capable, right? How much of a concern is Chinese equipment that's already been distributed? [[01:38:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5882.580000000001s)]
*  Yeah. Well, so the basic thing, so we don't always know the specific answer to that yet, [[01:38:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5889.22s)]
*  because this gets into complicated technical things. I think it'd be hard to prove some of [[01:38:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5895.26s)]
*  these things. But what we do, we do know the following. We know that in the Chinese system, [[01:38:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5899.66s)]
*  everything basically rolls up to and is essentially owned and controlled by, actually not even the [[01:38:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5903.3s)]
*  state, it's the Chinese Communist Party, the CCP. So there's the party, the party owns and controls [[01:38:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5908.900000000001s)]
*  the state and the state owns and controls everything else. So for example, it's actually [[01:38:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5912.9400000000005s)]
*  still illegal sitting here today for an American citizen to own stock in a Chinese company. [[01:38:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5917.3s)]
*  They like it's, people say that they do and they have various pieces of paper that say they do, [[01:38:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5920.660000000001s)]
*  but it's actually there's a law that says that's not because this is an asset of China. This is [[01:38:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5926.660000000001s)]
*  not something that you can sell to foreigners. And so they just have that model. And then if [[01:38:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5930.62s)]
*  you're a CEO of a Chinese company, you have a political officer assigned by the Communist Party [[01:38:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5935.780000000001s)]
*  who sits with you right down the hall, right? And like the office next to you. And basically [[01:39:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5940.9800000000005s)]
*  you coordinate everything with him and you need to make him happy. And he has the ability to come [[01:39:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5945.06s)]
*  grab you out of meetings and sit you down and tell you whatever you want, whatever he wants you to [[01:39:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5950.38s)]
*  do on behalf of the government. And if the government gets sideways with you, they will rip you [[01:39:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5954.660000000001s)]
*  right out of that position. They'll take away all your stock. They'll put you in jail. This has [[01:39:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5958.38s)]
*  happened over and over again, right? This has happened a bunch. A lot of like high elite Chinese [[01:39:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5962.06s)]
*  business leaders over the years have been basically stripped of their control and their positions and [[01:39:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5966.540000000001s)]
*  their stock and their wealth and everything. And some of them have just like outright vanished. [[01:39:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5971.18s)]
*  And so they just, they have this control. And so for example, data, you know, something like [[01:39:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5975.3s)]
*  TikTok, for example, if the Chinese government tells the company, we want the data, they hand [[01:39:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5979.66s)]
*  over the data. Like there's no, there's no, there's no court. There's no, you know, the concept of [[01:39:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5985.18s)]
*  like a FISA warrant, right? You know, the concept of a subpoena, like that's, they don't have that. [[01:39:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5991.38s)]
*  It's just like, we want it handed over or else. And so that's how it works. And when they want you [[01:39:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=5997.14s)]
*  to merge the company or shut it down or do something different or don't do this or do that, [[01:40:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6003.34s)]
*  they just tell you and that's what you do. And so, so anyway, so then you have a Chinese company [[01:40:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6006.78s)]
*  like TikTok or like Huawei and, or the DJI, the other one is their drone company, right? The most [[01:40:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6011.62s)]
*  of the drones flown in the West are from this Chinese company called DJI. And so then there's [[01:40:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6018.34s)]
*  always this question of like, well, is there a back door? Right? So can the Chinese government [[01:40:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6022.26s)]
*  reach in at any point and, you know, use the, use your drone for surveillance? Can they use, [[01:40:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6025.62s)]
*  you know, can they see what you're watching on TikTok? And the answer to that is maybe they can, [[01:40:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6031.820000000001s)]
*  but it kind of doesn't matter if they can't today because they're going to be able to anytime they [[01:40:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6036.22s)]
*  want to, because they can just tell these companies, oh, I want you to do that. And the company will [[01:40:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6039.06s)]
*  say, no, okay, I'm going to do that. And so it's a, it's a, it's a complete fusion of, of, of state [[01:40:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6042.660000000001s)]
*  and company here in the U S at least in theory, we have a separation. This goes back to the topic [[01:40:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6047.14s)]
*  I was talking about earlier, like at least like for the U S system to work properly, we need a [[01:40:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6053.820000000001s)]
*  separation of the government and from companies. We need the companies to have to compete with each [[01:40:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6056.9400000000005s)]
*  other. And then we needed for them to have legal leverage against the government. So when the [[01:41:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6060.94s)]
*  government says hand over private citizen data, the company can say, no, that's violation of the [[01:41:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6064.5s)]
*  first or fourth or fifth amendment rights. I'm not going to do that. And then they can litigate [[01:41:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6069.099999999999s)]
*  that, take it to the Supreme court. You can have an actual like argument over it. That's compromised [[01:41:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6072.54s)]
*  when our companies voluntarily do that, right? Which is what's been how inconvenient for them. [[01:41:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6077.46s)]
*  Yes, exactly. I'm sure they would love to use the communist model. Yeah. Well, so this is the thing. [[01:41:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6081.5s)]
*  And in the U S this very important, right? In the U S we have written constitutional, [[01:41:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6085.9s)]
*  giving you a free speech in the U S we have the literal written first amendment. Even in the UK, [[01:41:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6090.7s)]
*  they don't have a, they don't, they do not have a written constitutional guaranteed or free speech. [[01:41:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6095.82s)]
*  So in the, in the UK, there are laws where they can jail you for saying the wrong thing. Right. And [[01:41:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6099.0199999999995s)]
*  the same thing, by the way, in a bunch of these cases in like Australia and New Zealand, New Zealand, [[01:41:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6103.62s)]
*  New Zealand, New Zealand, which is supposed to be like the libertarian paradise, New Zealand has a [[01:41:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6108.0199999999995s)]
*  government position reporting the prime minister called the chief censor, right? Who gets to decide [[01:41:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6111.94s)]
*  basically what gets to be in the news or what people get to say. Right. And so even in the West, [[01:41:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6116.259999999999s)]
*  like outside the U S there are very few countries that have a written guaranteed of free speech. [[01:42:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6121.54s)]
*  Right. And so, and even in the U S like, do we actually have free speech? If there's all this [[01:42:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6126.74s)]
*  level of censorship and control that we've all been seeing for the last 10 years. Right. Right. And so [[01:42:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6130.62s)]
*  it's like, okay, the, the, the line here, the slippery slope here between free and not free is [[01:42:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6134.179999999999s)]
*  like very narrow. Right. It's not, it's not a moat, right. It's a very thin line, [[01:42:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6140.0199999999995s)]
*  which is very easily cracked. And you take, and this is why everybody's so fired up about, [[01:42:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6145.62s)]
*  in government, this is why everybody's so fired up about AI is because it's another one of these [[01:42:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6149.74s)]
*  where they're like, wow, if we can get control of this, then think of all the ways that this can get [[01:42:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6153.299999999999s)]
*  used. Well, that's one of the more fascinating things about Elon buying Twitter, because boy, [[01:42:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6156.339999999999s)]
*  did that throw a monkey wrench into everything. When you see like Biden's tweets get fact-checked, [[01:42:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6163.0199999999995s)]
*  you're like, whoa, there's a lot of things showing up on Twitter now that we're not showing up on [[01:42:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6167.7s)]
*  Twitter before. Oh my God. So much. And just nutty shit too. I mean, like some of the wackiest [[01:42:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6173.5s)]
*  conspiracy theories, Michelle Obama's a man, like all that kind of stuff, flat earth, but [[01:43:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6182.22s)]
*  birds. I'd rather have that. My favorite is the birds by the way. Yeah. Birds aren't real. Yeah. [[01:43:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6188.1s)]
*  That one I'm pretty sure of. It just, it doesn't make, it doesn't make any sense. That had to be, [[01:43:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6193.22s)]
*  like why can't we fly? It's just ridiculous. Yeah. It's gotta be a 4chan thing. Yeah. You know, [[01:43:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6198.820000000001s)]
*  sometimes they're, they're onto something. But I like that. Yeah. I like that wacky shit that's [[01:43:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6206.620000000001s)]
*  mixed in with things. I mean, it seems insane. And, but that, when I also, when I'll look at [[01:43:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6212.02s)]
*  like some of the people that are putting it up there and I look at their profiles and I look at [[01:43:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6217.38s)]
*  their American flag and their bio and I'm like, are you a real human? Cause this is a troll farm [[01:43:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6221.660000000001s)]
*  in Macedonia. Like what's happening here? There's a lot of that. There is. And of course he says he [[01:43:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6227.620000000001s)]
*  wants to, you know, of course he says he plans to over time, he plans to root all that out. He wants [[01:43:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6232.900000000001s)]
*  all identity to be validated, verified online. Having said that, we fought a war for free speech. [[01:43:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6237.98s)]
*  We fought the revolutionary war. A lot of that was for free expression. The founding fathers of [[01:44:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6243.5s)]
*  this country very frequently wrote under pseudonyms. Interesting. Just like Twitter and on. And, [[01:44:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6249.34s)]
*  and this includes like Ben Franklin, when he was a commercial printer, he had like 15 different [[01:44:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6255.34s)]
*  pseudonyms. Really? He would, he would sell newspapers by having his different pseudonym [[01:44:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6260.0199999999995s)]
*  personalities argue with each other. Fight it out. Like he had sock puppets. And then, you know, [[01:44:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6263.86s)]
*  like the federalist papers was all written under pseudonyms. Really? Yeah. Like Madison, [[01:44:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6269.78s)]
*  all these guys are under pseudonyms. And so like, why, why did you do that? Because there was danger. [[01:44:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6273.46s)]
*  Like there was very real danger associated with being like, you know, are you going to like, [[01:44:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6278.0199999999995s)]
*  what's, you know, what's the king going to think? Right. Right. Like, you know, yeah, is it like, [[01:44:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6281.7s)]
*  you know, this is sort of the two lines of argument, which is like, okay, like if somebody is not [[01:44:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6285.78s)]
*  willing to put their own name behind something, like, should they be allowed to say it? And there's [[01:44:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6289.66s)]
*  an argument, you know, in that direction, obvious, obvious one. But the other argument is, [[01:44:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6292.78s)]
*  yeah, sometimes there are things that are too dangerous to say unless you can't put your name [[01:44:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6295.66s)]
*  behind it. Yeah, that does make sense. So it seems like the pros about where the cons. Well, [[01:44:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6299.179999999999s)]
*  even just the micro version, which is just like, you know, if you've got something to say, [[01:45:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6304.179999999999s)]
*  that's important, but you don't want to be harassed in your house. You know, you want your family to [[01:45:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6306.78s)]
*  get harassed. Yeah. Right. You want, you know, protests going up outside your house for something [[01:45:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6310.219999999999s)]
*  you said. Anonymous whistleblower protection. Whistleblower protection. Yeah, exactly. This is, [[01:45:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6313.62s)]
*  yes, it was a whistle. It was that the one person's a terrorist is another person's freedom fighter. [[01:45:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6317.06s)]
*  One person's whistleblower is another person's troll. Like, I don't know. Yeah. And the genius [[01:45:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6323.660000000001s)]
*  American system is, yeah, like say what you want. Right. Yeah. Like, let's have it out. Right. And [[01:45:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6329.34s)]
*  so I, yeah, that's the system I believe in. I believe in that system too. But I also see [[01:45:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6333.9800000000005s)]
*  Elon's perspective that it would be great if it wasn't littered with propaganda and fake troll [[01:45:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6340.3s)]
*  accounts that are being used by various, you know, unscrupulous states. And in fairness, [[01:45:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6345.42s)]
*  what Elon says, actually, it's interesting. What Elon says is you will be allowed to have an [[01:45:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6350.74s)]
*  anonymous, what they call sued or an account under some other name you make up on the service. You'll [[01:45:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6354.66s)]
*  just have to register that behind the scenes with your real identity and specifically with like a [[01:46:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6361.9s)]
*  credit card. Right. But then the fear is that someone will be able to get in there. Correct. [[01:46:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6367.02s)]
*  Yeah, that's right. Which has happened already. Yeah, that's right. And that is a big risk. Yeah. [[01:46:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6370.54s)]
*  But then again, and then you get to get the other part of this would be like Twitter is only one [[01:46:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6375.1s)]
*  company. Right. And so there are, it's an important one, but it's only one. And there are others as [[01:46:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6378.9400000000005s)]
*  well. So, you know, for the full consideration of like, quote unquote, rights on this topic, [[01:46:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6382.54s)]
*  you also want to look at what is happening elsewhere, right. Including all the other services. [[01:46:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6386.9800000000005s)]
*  I'm fascinated by companies like Twitter and YouTube that develop at least a semi-monopoly. [[01:46:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6390.74s)]
*  Because YouTube is a great example. Like if you want to upload videos, YouTube is the primary [[01:46:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6397.02s)]
*  marketplace for that. And it's like, nothing else is even close. Everything else is a distant, [[01:46:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6403.86s)]
*  distant second. But they've got some pretty strict controls and pretty serious censorship [[01:46:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6408.42s)]
*  on YouTube. And it seems to be accelerating, particularly during this presidential election. [[01:46:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6414.339999999999s)]
*  Now that you're seeing these Robert Kennedy Jr. podcasts get pulled down from a year ago, [[01:47:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6420.259999999999s)]
*  two years ago, the Jordan Peterson one got pulled down. Theo Vaughn's interview with Robert Kennedy [[01:47:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6425.299999999999s)]
*  got pulled down. There's been some others. And Bret Weinstein? No, no, he didn't, his didn't. But [[01:47:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6430.54s)]
*  it's just these conversations were up for a long time. It wasn't until Robert Kennedy running for [[01:47:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6437.62s)]
*  president that they decided like these are inconvenient narratives that he's discussing. [[01:47:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6444.62s)]
*  So I don't want to, I should not weigh in on exactly which companies have whatever level of [[01:47:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6450.14s)]
*  monopoly they have. Having said that, to the extent that companies are found to have monopolies, [[01:47:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6455.5s)]
*  or let's say very, say sort of dominant market positions like that does, that should bring an [[01:47:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6459.78s)]
*  additional level of scrutiny on conduct. And then there is this other thing I mentioned earlier, [[01:47:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6464.139999999999s)]
*  but I think is a big deal, which is if a company is making all these decisions by itself, you can [[01:47:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6468.94s)]
*  argue that it maybe has the ability to do that. Although again, maybe it shouldn't pass a certain [[01:47:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6474.259999999999s)]
*  point in terms of being a monopoly. But the thing that's been happening is it's not just the companies [[01:47:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6477.54s)]
*  making these decisions by themselves. They've come under intense pressure from the government. [[01:48:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6483.34s)]
*  And they've come under intense pressure from the government in public statements and threats from [[01:48:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6486.38s)]
*  senior government officials. They have come in private channeled threats. And then all of this, [[01:48:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6492.38s)]
*  the stuff I was talking about earlier, all the channeling of all the money from the government [[01:48:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6497.38s)]
*  that's gone into these pro-censorship groups that are actively working to try to suppress speech. [[01:48:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6500.22s)]
*  And when you get into all of that, those are crimes. Everything I just described, I think, [[01:48:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6504.42s)]
*  illegal. And there are specific, like the actual felony basically counts in the US code for like [[01:48:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6512.66s)]
*  those things actually being illegal. They're violations of constitutional rights and it is [[01:48:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6516.54s)]
*  a felony to deprive somebody of their constitutional rights. And so I think in addition to what you [[01:48:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6519.58s)]
*  said, I think it's also true that there's been a pattern of government involvement here that is, [[01:48:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6524.26s)]
*  I think, certainly illegal. And this administration is not going to look into that. Maybe a future one [[01:48:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6528.22s)]
*  will. So do you think it's illegal? It just hasn't been litigated yet? Yeah. I think there's evidence [[01:48:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6536.26s)]
*  of substantial criminality just in the Twitter files that have come out. You need to have somebody, [[01:49:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6542.5s)]
*  prosecutors have to, yeah. You need class action lawsuits, right? You need to be able to go carve [[01:49:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6547.54s)]
*  it open with large scale civil suits or you need actual government criminal investigation. [[01:49:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6553.7s)]
*  What has come out of the Twitter files other than independent journalists researching it and discussing [[01:49:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6559.38s)]
*  it and writing articles, it's not being covered with any significance in mainstream news. Well, [[01:49:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6566.78s)]
*  the mainstream media has been on the side of censorship for the last eight years. Like they've [[01:49:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6573.139999999999s)]
*  been pounding the table that we need to lock down speech, right? A lot more. So they're compromised. [[01:49:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6576.5s)]
*  And then the other investigation to watch is I think it's the Missouri Attorney General. There's [[01:49:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6581.3s)]
*  this state level investigation where there's been a bunch of interesting stuff that's come out. And [[01:49:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6587.179999999999s)]
*  the attorneys general have subpoena power. So they have subpoenaed a bunch of materials from a bunch [[01:49:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6590.46s)]
*  of other companies that again, to me, it looks like evidence of criminality. But again, you would [[01:49:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6595.94s)]
*  need prosecutors. You need the political force of will and desire to investigate prosecute crimes. [[01:50:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6600.259999999999s)]
*  And to engage in that battle because it's going to be a battle. [[01:50:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6609.58s)]
*  Yeah. And then if it's private litigation, you need to try to do a big class action suit. And [[01:50:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6612.74s)]
*  then you need to be prepared to fight it all the way to the Supreme Court. And there's a lot of [[01:50:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6619.9s)]
*  money involved in that. [[01:50:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6624.0599999999995s)]
*  When you're seeing this play out and you're looking at likely scenarios, how does this resolve? How [[01:50:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6624.14s)]
*  do we come out of this? [[01:50:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6632.3s)]
*  I think it's a big collective, it's a fight. This is one of those where it's like, what do we want? [[01:50:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6633.0199999999995s)]
*  And the we here is like all of society. And if we decide that we want the system to keep working [[01:50:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6641.34s)]
*  the way that it's working, we're going to keep electing the same kinds of people who have the [[01:50:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6647.22s)]
*  same policies. [[01:50:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6649.82s)]
*  Do you think most people are even aware of all these issues though? [[01:50:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6650.38s)]
*  No, I mean, certainly not. And that's a big, there's always any symmetry between the people [[01:50:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6653.34s)]
*  who are doing things that people aren't aware of. But again, it's like, what do we want? Are people [[01:50:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6658.38s)]
*  going to care about this or not? If they are, then they're going to at some point demand action. [[01:51:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6662.18s)]
*  It's a collective action problem. People have to come together in large numbers. [[01:51:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6667.46s)]
*  But will it be too late? This is the question. Imagine a scenario where Elon never buys Twitter [[01:51:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6671.78s)]
*  and Twitter just continues its practices and even accelerates. [[01:51:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6676.34s)]
*  Yeah. And that's my concern. And again, this goes back to my concern about the AI lockdown, [[01:51:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6679.1s)]
*  right? Which is like all of the concerns on AI are being basically used to put in place. I think [[01:51:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6684.34s)]
*  what they're going to try to do to AI for speech and thought control is like a thousand times more [[01:51:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6690.78s)]
*  dangerous than what's happened on social media. Right? Because it's going to be your kids, [[01:51:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6694.74s)]
*  you know, asking the AI, you know, what are the facts on this? And it's just going to like flat [[01:51:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6698.860000000001s)]
*  out lie to them for political reasons, which it does today. And like that, to me, it's like far [[01:51:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6703.3s)]
*  more dangerous. And that's what's happening already. And the desire is very clear, I think, [[01:51:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6708.06s)]
*  on the part of a lot of people to have that be a fully legal blessed thing that basically gets put [[01:51:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6713.9400000000005s)]
*  in place and never changes. Well, you're completely making sense, especially when you think about what [[01:51:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6719.34s)]
*  they've done with social media and what we were, and not even speculation, just the Twitter files. [[01:52:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6724.7s)]
*  It's so clear. This is the ring of power thing, right? It's like everybody's in favor of free [[01:52:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6730.0199999999995s)]
*  speech and theory. It's like, well, if I can win an election without it, I've got the ring of power. [[01:52:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6736.179999999999s)]
*  Right? And then the American system, the American system was set up so that people don't have the [[01:52:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6741.74s)]
*  ring of power. Like the whole point of like balance of, you know, the balance of terror [[01:52:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6746.0199999999995s)]
*  between the three branches of government and all the, you know, the existence of the Supreme Court [[01:52:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6749.099999999999s)]
*  and, you know, the due process protections in the Constitution. It was all to prevent government [[01:52:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6752.74s)]
*  officials from being able to do things like this with impunity. Yeah. The founding fathers saw the [[01:52:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6758.54s)]
*  threat. It's actually remarkable how clearly the founding fathers saw the threat given that they [[01:52:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6764.3s)]
*  were doing all of this before, you know, any modern, you know, before electricity. It is pretty amazing. [[01:52:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6768.38s)]
*  But they saw the threat. Yeah. They had a pretty profound understanding of human nature and applied [[01:52:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6772.98s)]
*  to power. Yeah, they did. Yeah. This is a, it's such an uneasy time because you see how these things, [[01:52:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6778.98s)]
*  all these forces that are at work and how it could play out, how it is playing out with social media, [[01:53:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6790.379999999999s)]
*  how it could play out with AI and electing leaders that are going to like see things correctly. [[01:53:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6795.86s)]
*  Like I don't, I haven't seen anybody discussing this, especially not discussing this the way [[01:53:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6802.7s)]
*  you're discussing it. Well, and when this, when the speech is made, right, to justify whatever the controls are, [[01:53:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6807.9s)]
*  it's going to be made in our name, right? So it's the speech is not going to be, we're going to do this to you. [[01:53:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6813.62s)]
*  Right. The speeches, we're doing this to protect you. Right. Right. So that's the siren song. Yeah. Right. [[01:53:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6817.379999999999s)]
*  And that's already started. Like if you look at the public statements coming out of DC already like that, [[01:53:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6822.78s)]
*  that is the thrust of it. Because of course that's what they're, that's how they're going to, that's how they're going to couch it. [[01:53:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6827.419999999999s)]
*  How are they framing it? How is it protecting us? Well, we need to protect, we need to, yeah, we need to protect, [[01:53:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6830.78s)]
*  we need to protect people from dangerousness and that. We need to protect people from hate speech. [[01:53:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6834.86s)]
*  We need to protect people from misinformation. And it's the same, I mean, it's the same arguments that it's essentially the same. [[01:53:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6837.82s)]
*  It's effectively the same arguments you've seen in social media for the last decade. [[01:54:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6842.42s)]
*  I just don't know how we publicly turn that narrative around because there's so many people that have adopted it like a mantra. [[01:54:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6845.86s)]
*  Yeah. They just say that mantra and they just think that they're doing the right thing. [[01:54:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6851.78s)]
*  Hate speech, disinformation, misinformation. Let them take care of it. They're doing the right thing. [[01:54:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6855.86s)]
*  So here's the white pill. Here's the white pill. Here's the reason for optimism. [[01:54:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6862.98s)]
*  So Gallup has been surveying American citizens' trust in institutions for 50 years. [[01:54:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6866.5s)]
*  A lot of people think all this stuff started with the Internet and it turns out it didn't. [[01:54:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6872.86s)]
*  It turns out there's been a collapse of faith on the part of American citizens in their institutions basically since basically I was born, [[01:54:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6875.54s)]
*  basically around the early 70s. It's basically been a straight line down on almost every major institution. [[01:54:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6881.419999999999s)]
*  Right. And so, you know, I'll talk about government newspapers in a second, but, you know, basically any, you know, religion, [[01:54:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6886.0599999999995s)]
*  you go kind of right down the list. Police, big business, you know, education, schools, universities. [[01:54:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6892.74s)]
*  You chart all these things out and basically they're all basically straight lines down over 50 years. [[01:55:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6900.18s)]
*  Right. And there's there's there's two ways of interpreting that. [[01:55:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6904.54s)]
*  One is, you know, greater levels of disillusionment and cynicism that are incorrect. [[01:55:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6906.66s)]
*  And then the other is actually people are learning who they can and can't trust. [[01:55:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6911.9s)]
*  And then, of course, the theory goes to start in the 70s because of the hangover from the Vietnam War and then Watergate. [[01:55:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6916.34s)]
*  And then a lot of the hearings that kind of exposed government corruption in the 70s that followed. [[01:55:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6922.62s)]
*  Right. And then it just sort of this sort of downward slide. [[01:55:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6927.02s)]
*  The military is the big exception. The military took a huge hit after Vietnam. [[01:55:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6929.900000000001s)]
*  And then actually it's the one that has like recovered sharply. [[01:55:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6933.9400000000005s)]
*  And there's like a cultural change that's happened where, you know, we we as Americans have decided that we can have faith in the military, [[01:55:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6936.42s)]
*  even if we don't agree with the missions that they're sent on. [[01:55:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6942.02s)]
*  So that's the exception. But everything else is sort of down down into the right. [[01:55:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6944.5s)]
*  The two that are like the lowest and have had the biggest drops are Congress and journalism. [[01:55:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6949.02s)]
*  Right. And so the population and they're they pull like 10, 15 percent in the population. [[01:55:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6954.18s)]
*  Wow. And so most people are not looking at these things like, oh, yeah, these people are right about most of these. [[01:55:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6958.860000000001s)]
*  Most people look at these things being like, you know, that's that's screwed up. [[01:56:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6965.22s)]
*  Now, people have to decide what to do with that. [[01:56:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6968.26s)]
*  Right. Because what you see is that faith in Congress is pulls it like 10 percent. [[01:56:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6970.42s)]
*  But faith in your local congressperson pulls it like 90 percent. [[01:56:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6974.18s)]
*  Right. Which is why incumbents keep getting real. [[01:56:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6977.54s)]
*  You know, congressional incumbents almost always get reelected. [[01:56:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6979.9800000000005s)]
*  Right. And you'll have these, you know, congressmen who are in there for 20 terms. [[01:56:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6982.700000000001s)]
*  Right. Forty years. Right. [[01:56:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6985.54s)]
*  And so at some point, people have to decide they have to carry it over. [[01:56:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6987.26s)]
*  Right. It's not internally consistent. Right. [[01:56:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6990.42s)]
*  And you're not going to get the change that you want from Congress unless a lot more people all of a sudden change their mind about the incumbents. [[01:56:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6992.54s)]
*  Yeah. They keep reelecting. [[01:56:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=6998.38s)]
*  But anyway, the reason for optimism in there is I think most people are off the train. [[01:56:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7000.66s)]
*  Right. Already. Right. [[01:56:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7006.0599999999995s)]
*  And quite frankly, I think that explains a lot of what's happened in politics in the U.S. [[01:56:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7007.62s)]
*  over the last 10 years. Like whether whether people, you know, support or don't support the kind of, [[01:56:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7010.54s)]
*  you know, the various forms of populism on the left or the right. [[01:56:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7014.58s)]
*  I think it's the citizenry reaching out for a better answer than just more of the same [[01:56:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7017.34s)]
*  and more of the same being the same elites in charge forever, telling us the same things that we know aren't true. [[01:57:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7022.38s)]
*  Well, that is one of the beautiful things about social media and the beautiful things about things like YouTube, where people can [[01:57:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7026.9s)]
*  constantly discuss these things and have these conversations that are reached by millions of people. [[01:57:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7032.62s)]
*  I mean, just a viral tweet, a viral video, something, you know, someone gives a speech on a podcast and [[01:57:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7038.5s)]
*  and everybody goes like what you're saying today. [[01:57:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7046.18s)]
*  I didn't know that's how it worked. [[01:57:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7048.38s)]
*  Oh, this is what we have to be afraid of. [[01:57:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7050.54s)]
*  So when they start saying it's for your own protection, this is why. [[01:57:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7052.34s)]
*  And then the Mark Andreessen clip plays and everybody goes, OK, [[01:57:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7055.66s)]
*  that's that gives me hope because that's something that didn't exist before. [[01:57:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7061.58s)]
*  Yeah, that's right. And you can even take it a step back further. [[01:57:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7064.46s)]
*  It's even pre social media is there was a big opening in the 80s with talk radio. [[01:57:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7067.82s)]
*  They got people very mad at the time. [[01:57:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7073.42s)]
*  Things were being said on it that weren't supposed to be said. [[01:57:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7075.5s)]
*  Cable TV was a big opening to it. [[01:57:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7077.9s)]
*  Before that, actually, in the 50s, it was paperback books. [[01:58:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7081.34s)]
*  A lot of alternate points of view, you know, basically took took took sort of flower in the 50s and 60s, [[01:58:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7084.62s)]
*  flowing out of paperback books and then newsletters. [[01:58:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7090.34s)]
*  That's why I say the Soviets outlawed mimeograph mimeograph machines, right? [[01:58:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7093.18s)]
*  Sure. Earlier photocopiers. [[01:58:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7096.54s)]
*  But like, you know, there was a whole newsletter phenomenon in a lot of movements in the 50s, 60s, 70s. [[01:58:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7097.94s)]
*  And so it's basically it's this is sort of this sequential way. [[01:58:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7102.54s)]
*  Look at it as basically the way to think about it as media and thought centralized to the maximum possible [[01:58:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7105.5s)]
*  level of centralization and control right around 1950. [[01:58:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7111.58s)]
*  Right. Where you basically had three television networks. [[01:58:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7114.7s)]
*  You had, you know, one newspaper per city. [[01:58:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7117.3s)]
*  You had three news magazines. [[01:58:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7119.62s)]
*  Right. You just had your two political parties. [[01:58:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7121.66s)]
*  Right. You just like everything was like locked in hard. [[01:58:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7123.74s)]
*  Right. And then basically technology in the form of all of these media technologies and then all the computer [[01:58:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7126.78s)]
*  and information technologies underneath them have basically been decentralized and unwinding that level of [[01:58:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7132.38s)]
*  centralized control more or less continuously now for 70 years. [[01:58:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7136.46s)]
*  So as I think it's been this this this longer running process. [[01:59:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7141.02s)]
*  And by the way, I think, you know, it's left to its own devices. [[01:59:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7143.5s)]
*  It's going to continue. Right. [[01:59:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7146.22s)]
*  This is the significance of AI. [[01:59:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7147.900000000001s)]
*  Like, I think what if each of us has a super sophisticated AI that we own and control? [[01:59:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7149.14s)]
*  Because it either comes from a company that's doing that for us or it's an open source thing where we can just [[01:59:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7155.54s)]
*  download it and use it. [[01:59:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7159.1s)]
*  And what if it has the ability to analyze all the information? [[01:59:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7160.34s)]
*  And what if it has the ability to basically say, you know, look, on this topic, I'm going to go scour the Internet [[01:59:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7162.900000000001s)]
*  and I'm going to come back and I'm going to synthesize information. [[01:59:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7167.1s)]
*  I'm going to tell you what I think. [[01:59:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7168.900000000001s)]
*  Right. It's the AI. Right. [[01:59:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7170.38s)]
*  So that that would it would be logical that that would be another step down this process. [[01:59:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7171.3s)]
*  Yes. Right. [[01:59:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7174.620000000001s)]
*  And by the way, and maybe the most important step of all. [[01:59:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7175.3s)]
*  Right. Because it's the one where it can actually be like, OK, I'm going to I'm going to be able to legitimately [[01:59:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7177.820000000001s)]
*  think on your behalf. [[01:59:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7181.700000000001s)]
*  Right. And help you to conclusions that are factually correct, even if people who are in power don't want to hear it. [[01:59:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7183.26s)]
*  If it seems to me that you have more of a glass half full perspective on this, are you open minded and just sort [[01:59:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7189.580000000001s)]
*  of just analyzing the data as it presents itself currently and not making judgments about where this is going? [[01:59:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7199.98s)]
*  Or do you generally feel like this is all going to move in a good direction? [[02:00:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7207.0599999999995s)]
*  So my day job is to we meet every day all through the year with all these incredibly smart kids who have these [[02:00:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7210.62s)]
*  incredibly great new ideas and they want to build these technologies and they want to build businesses around them [[02:00:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7218.66s)]
*  or they want to open source them or they want to whatever. [[02:00:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7224.62s)]
*  But they want to build. [[02:00:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7226.82s)]
*  They want to make these new things happen. [[02:00:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7228.099999999999s)]
*  You know, they have they have they have visions for how the world can can change in these ways. [[02:00:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7229.5s)]
*  They have the technical knowledge to be able to do these things. [[02:00:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7232.9400000000005s)]
*  There's a pattern of these kids doing amazing things. [[02:00:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7236.02s)]
*  Apple. Apple just passed today. [[02:00:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7239.38s)]
*  Apple just passed. Apple alone just passed the entire value of the entire UK stock market. [[02:00:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7240.9800000000005s)]
*  Right. So an apple was two kids in a garage in 1976 with a crazy idea that people should have their own computers, [[02:00:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7245.900000000001s)]
*  which was a crazy idea at the time. [[02:00:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7253.42s)]
*  Right. And so like it doesn't you know, usually it doesn't work. [[02:00:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7255.34s)]
*  But when it does, like it works really, really well. [[02:00:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7258.82s)]
*  And this is what we got the microchip and this is how we got the PC and this is how we got the Internet and the web [[02:01:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7261.219999999999s)]
*  and all these other you know, all these other things. [[02:01:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7265.099999999999s)]
*  Yeah, here we go. Yeah. [[02:01:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7268.34s)]
*  Top three trillion. [[02:01:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7269.139999999999s)]
*  Yeah. So it's the comparison, I think, is to what they call the FTSE 350, which is the 350 largest UK companies. [[02:01:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7271.58s)]
*  That's bonkers. [[02:01:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7278.82s)]
*  Yeah. And so when it works like it works incredibly well. [[02:01:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7279.62s)]
*  Right. And so and we just we just happen to be you know, by being what we being where we are and you know, doing what [[02:01:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7282.9s)]
*  we do. We're at ground zero of that. [[02:01:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7288.3s)]
*  And so all day long, I meet and talk to these kids and people who have these ideas and want to do these things. [[02:01:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7289.58s)]
*  And so I and so it's why I can I can see the future kind of in that sense, which is I know what they're going to do, [[02:01:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7294.58s)]
*  because they come in and tell us and then we help them try to try to do it. [[02:01:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7299.58s)]
*  So if they're allowed to do what they plan to do, then I have a pretty good idea of what the future is going to look like [[02:01:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7302.900000000001s)]
*  and how great it could potentially be. [[02:01:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7309.26s)]
*  But then I also have the conversations in Washington and I also have the conversations with the people who are trying to do [[02:01:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7311.06s)]
*  the other things. And I'm like, OK, like this is like for a very long time, tech in the U.S. [[02:01:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7316.74s)]
*  was considered just like purely good. Right. [[02:02:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7321.78s)]
*  Tech was everybody was like up until like basically the 2000s, 2010s. [[02:02:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7323.3s)]
*  Everybody was just kind of pro tech pro whatever. [[02:02:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7326.78s)]
*  People got excited about new things. [[02:02:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7328.82s)]
*  Every once in a while, people get freaked out about something. [[02:02:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7330.139999999999s)]
*  But mostly people just thought, you know, invention is good. [[02:02:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7332.139999999999s)]
*  Creativity is good. Silicon Valley is good. [[02:02:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7335.099999999999s)]
*  And in the last 15, 20 years, like it's gotten these all these topics have gotten very contentious. [[02:02:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7336.62s)]
*  And you have all these people who are very angry about about the consequences of all this technological change. [[02:02:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7342.099999999999s)]
*  And so we're in a different phase of the world where these issues are now being fought out, not just in business, but also in politics. [[02:02:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7347.0199999999995s)]
*  And so I I also have those conversations and those are almost routinely dismaying. [[02:02:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7354.54s)]
*  Those are not good conversations. [[02:02:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7361.179999999999s)]
*  And so I'm always trying to kind of calibrate between what I know is possible versus my concern that people are going to try to figure out how to screw it up. [[02:02:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7362.86s)]
*  When you have these conversations with people behind the scenes, are they receptive? [[02:02:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7369.34s)]
*  Are they aware of the issues, what you're saying in terms of just just freedom of expression and the future of the country? [[02:02:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7373.26s)]
*  There you might bucket it in like three different buckets. [[02:03:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7382.3s)]
*  There's a set of people who just basically don't like Silicon Valley tech, Internet, free speech, capitalism, free markets. [[02:03:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7384.740000000001s)]
*  Like they're very political. [[02:03:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7393.5s)]
*  Some of them are in positions of high power right now and they're just opposed. [[02:03:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7395.02s)]
*  They're just against and they're trying to do everything they can. [[02:03:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7397.900000000001s)]
*  I mean, they're trying to outlaw crypto right now. [[02:03:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7400.22s)]
*  They're trying to like do all kinds of stuff. [[02:03:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7401.66s)]
*  They're trying to be the same people trying to censor social media like they're just very opposed. [[02:03:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7402.9s)]
*  And there's I mean, I don't know, maybe there would be a point in talking. [[02:03:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7406.46s)]
*  I myself don't spend a lot of time talking to them because it's not a conversation. [[02:03:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7409.299999999999s)]
*  It's just getting yelled at for an hour. [[02:03:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7413.5s)]
*  There's that really how go. [[02:03:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7416.099999999999s)]
*  Oh, yeah. Yeah, they're very angry. [[02:03:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7417.139999999999s)]
*  Like there's a lot of there's a very large amount of rage in the system. [[02:03:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7418.259999999999s)]
*  A lot of it directed at tech. [[02:03:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7422.5s)]
*  Then there's a set of people who I would describe. [[02:03:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7425.42s)]
*  I don't know if open mind is a wrong term, but like I would say they are honestly and legitimately trying to understand the issues. [[02:03:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7427.0199999999995s)]
*  Like they're kind of aware that they don't fully understand what's happening and they are trying to figure it out. [[02:03:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7432.26s)]
*  And they do have a narrative in their own mind of they're going to try to come to the right conclusion. [[02:03:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7435.900000000001s)]
*  So there's some set of those. [[02:03:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7439.62s)]
*  Those usually aren't the senior people, but there are people like at the staff level who are like that. [[02:04:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7440.62s)]
*  Dreamers. What's that? [[02:04:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7444.66s)]
*  Dreamers. Yeah. [[02:04:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7445.860000000001s)]
*  Yeah. Like, you know, the best the best of the bunch. [[02:04:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7446.900000000001s)]
*  Right. Like the open minded learning curious. [[02:04:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7451.34s)]
*  You know, it's like anything else in life. [[02:04:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7455.7s)]
*  You sit down with one person and like you're in they just like you have a conversation. [[02:04:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7456.9800000000005s)]
*  They ask you questions. You ask them questions. [[02:04:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7460.14s)]
*  There's other people you talk to where it's just like they're not interested in what you think. [[02:04:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7461.9800000000005s)]
*  And it's just very clear that they're not interested in what you think. [[02:04:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7464.9400000000005s)]
*  And so that plays out there also. [[02:04:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7467.3s)]
*  And then there's a third set of people who are very actually pro capitalism, pro innovation, pro tech. [[02:04:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7469.74s)]
*  But they don't like us because they think we're all Democrats. [[02:04:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7476.78s)]
*  So a lot of our natural allies on these issues are on the other side of where Silicon Valley is majority Democrat, Democratic. [[02:04:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7481.9s)]
*  Right. And so there's a fair number of people who would be our natural allies if not for the fact that Silicon Valley is like 99 percent Democrat. [[02:04:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7489.06s)]
*  Right. And so this is part of the issue. [[02:04:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7496.14s)]
*  The Valley has like we don't have any natural allies like tech tech doesn't have any national allies in D.C. [[02:04:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7498.1s)]
*  because the Democrats basically think they control us, which they effectively do because the Valley is almost entirely Democrat. [[02:05:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7502.660000000001s)]
*  And then the Republicans think that, you know, basically they would support us except that we're all Democrats. [[02:05:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7509.02s)]
*  And so we can go f off. [[02:05:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7513.860000000001s)]
*  And so there's a trap that's developed that is hard to figure out what to do with. [[02:05:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7515.900000000001s)]
*  How do you get around that one? [[02:05:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7520.78s)]
*  That one's a hard one. I mean, that I don't know. [[02:05:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7521.9800000000005s)]
*  That that seems people. [[02:05:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7524.5s)]
*  The last thing I want to do is argue to people, especially in public, that they should change their politics. [[02:05:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7526.62s)]
*  So I look people feel very strongly. [[02:05:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7531.06s)]
*  Obviously, people in tech feel very strongly about politics, including many political topics that have nothing to do with tech. [[02:05:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7533.66s)]
*  And so asking somebody to change their views on some other political issues so that it's better for tech is not some argument that flies. [[02:05:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7539.3s)]
*  So wow. So there's a yeah, there's a bit of a stall there. [[02:05:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7544.62s)]
*  But yeah, it goes back to people got to people have to decide what they want. [[02:05:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7548.34s)]
*  You seem like you enjoy all this madness, though. [[02:05:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7552.62s)]
*  You really do. I'd rather be in the middle of it than not. [[02:05:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7555.26s)]
*  Yeah, it would be very frustrating to be on the outside. [[02:05:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7558.9s)]
*  Even more frustrating than. [[02:06:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7562.42s)]
*  Then being involved in it. [[02:06:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7566.3s)]
*  What? Well, look, here's the other thing. [[02:06:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7567.3s)]
*  These issues, these issues become really important, right? [[02:06:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7569.3s)]
*  Like I'll even I'll even credit the critics with the following, which is, yeah, look, Mark, like tech was a backwater. [[02:06:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7572.06s)]
*  Tech didn't matter until the Internet showed up. [[02:06:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7576.22s)]
*  Like and now it matters a lot because like it's the future of speech and politics and control and all these things. [[02:06:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7578.38s)]
*  And so all of a sudden, it's like these big important topics. [[02:06:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7582.46s)]
*  We're going to talk about warfare like A.I. is going to like really change how like weapons work. [[02:06:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7585.26s)]
*  Right. Like basically every important thing happening in the world right now has a technological component to it. [[02:06:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7588.580000000001s)]
*  Right. And it's being altered by the changes that are happening, you know, caused by tech. [[02:06:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7593.860000000001s)]
*  And so the other argument would be, Mark, like grow up like, of course, these are all going to be big fights. [[02:06:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7597.540000000001s)]
*  Because these are now you're now involved in all the big issues. [[02:06:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7602.0599999999995s)]
*  Yeah. And maybe that's just the case. [[02:06:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7604.82s)]
*  Well, that seems to definitely also be the case. [[02:06:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7606.62s)]
*  Yeah. It's just people are always so scared of change and change today. [[02:06:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7608.7s)]
*  When we're talking about this kind of change, you're talking about monumental change that happens over a very short period of time. [[02:06:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7615.219999999999s)]
*  Yes. That's a big freak out. Yes. [[02:07:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7622.86s)]
*  Yeah. Yeah. I mean, what are we looking at in 50 years? [[02:07:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7625.339999999999s)]
*  Really? Yeah. [[02:07:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7628.419999999999s)]
*  You enjoy it. [[02:07:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7630.42s)]
*  I do enjoy it. I do enjoy it. [[02:07:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7631.42s)]
*  I love that you enjoy it, though. [[02:07:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7633.42s)]
*  Douglas, you know, the book Hitchhiker's Guide to the Galaxy. [[02:07:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7635.42s)]
*  Douglas Adams wrote that book. [[02:07:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7638.42s)]
*  He once had a formulation. [[02:07:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7639.42s)]
*  He said he said this is all generational. [[02:07:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7641.42s)]
*  He had a different theory than all this. [[02:07:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7644.42s)]
*  It's all generational. It's all age related. [[02:07:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7645.42s)]
*  And he said, if you're people react to technology in three different ways, if you're below the age of 15, whatever is the new thing is just how the world always worked. [[02:07:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7647.42s)]
*  If you're between the ages of 15 and 35, whatever is the new thing is exciting and hot and cool. [[02:07:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7656.42s)]
*  And you might be able to get a job and make a living doing it. [[02:07:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7661.42s)]
*  Anything if you're above the age of 35, it's whatever is happening is unholy. [[02:07:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7663.42s)]
*  It's sure to bring about the downfall of civilization, right? Apocalypse and calamity. [[02:07:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7669.42s)]
*  I guess that's true in culture. [[02:07:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7673.42s)]
*  It's true in music. [[02:07:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7675.42s)]
*  It's true in movies, video games. [[02:07:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7676.42s)]
*  Yeah. Yeah. [[02:07:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7679.42s)]
*  Yeah. Yeah. [[02:08:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7680.42s)]
*  So I think maybe what just has to happen is just time needs to pass. [[02:08:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7681.42s)]
*  Maybe the fight is always, I don't know, it's like whatever the new thing happens, the fight's always between a bunch of 50 year olds or something. [[02:08:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7685.42s)]
*  Do you resist any technology in your own personal life? [[02:08:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7692.42s)]
*  That is a good question. [[02:08:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7696.42s)]
*  I don't personally. [[02:08:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7698.42s)]
*  And said that we do have an eight year old and he does get screen time, but it is controlled. [[02:08:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7700.42s)]
*  So we're a little bit, you know, we use it as a tool. [[02:08:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7706.42s)]
*  We're not absolutists. [[02:08:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7710.42s)]
*  Like we're not, you know, there are some people running around to keep their kids off all this stuff, which by the way, is not the craziest view in the world. [[02:08:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7711.42s)]
*  Right. [[02:08:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7718.42s)]
*  But we want him to be fully up to speed on. [[02:08:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7719.42s)]
*  We want him to be an engineer, you know, not that he has to spend his life doing it, but we want him to know how to use technology and build it. [[02:08:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7722.42s)]
*  It's also fun for kids. [[02:08:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7728.42s)]
*  It's just if you teach them discipline and, you know, engage them in other activities so that they do physical things and run around, have fun, be outside. [[02:08:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7730.42s)]
*  He does MMA. [[02:08:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7739.42s)]
*  Oh, no kidding. [[02:09:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7741.42s)]
*  Brazilian Jiu Jitsu. [[02:09:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7742.42s)]
*  He's doing full MMA, full, he's doing his full sparring. [[02:09:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7744.42s)]
*  Wow. [[02:09:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7747.42s)]
*  He and his coach dress up in the full body. [[02:09:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7749.42s)]
*  Wow. [[02:09:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7752.42s)]
*  Marshmallow man outfits and like wail on each other. [[02:09:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7754.42s)]
*  Wow. [[02:09:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7755.42s)]
*  Get on the ground and choke each other out. [[02:09:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7756.42s)]
*  Okay. [[02:09:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7758.42s)]
*  Are you enjoying watching that? [[02:09:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7759.42s)]
*  It's absolutely fantastic. [[02:09:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7760.42s)]
*  Wow. [[02:09:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7761.42s)]
*  And he loves it. [[02:09:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7762.42s)]
*  That's pretty cool. [[02:09:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7763.42s)]
*  And I keep watching the videos, you know, because he's up against, he's like, you know, half the time he's with an adult sparring and he's just like, he just goes like right in there. [[02:09:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7764.42s)]
*  That's crazy. [[02:09:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7770.42s)]
*  So the tech story that I've been thinking about a lot is the Douglas Adams thing is, so, you know, Chad GPT comes out in December. [[02:09:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7771.42s)]
*  I play with it for a few months. [[02:09:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7780.42s)]
*  I'm trying to wrap my head around it. [[02:09:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7781.42s)]
*  I'm like, okay, this is good. [[02:09:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7782.42s)]
*  And so I'm like, okay, my eight year old's like super curious and he wants to learn all these things and he's, you know, asking questions all the time and half the time I don't know the answer. [[02:09:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7784.42s)]
*  So I'm like, okay, I install it on his laptop. [[02:09:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7791.42s)]
*  I chat GPT on his laptop. [[02:09:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7793.42s)]
*  And I like I set up this time aside and I sit him down on the couch and I'm like, okay, there's this like amazing thing that I'm going to give you. [[02:09:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7795.42s)]
*  Right. [[02:10:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7802.42s)]
*  This is like, it's like the most important thing I've ever done as a father, right. [[02:10:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7803.42s)]
*  That I've like brought like fire down from the mountains and I'm going to give you AI. [[02:10:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7805.42s)]
*  Right. [[02:10:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7808.42s)]
*  And you're going to have like AI your whole life to be like with you and teach you things. [[02:10:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7809.42s)]
*  And he's like, okay. [[02:10:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7812.42s)]
*  And I was like, well, you ask it questions and it'll answer the questions. [[02:10:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7813.42s)]
*  And he's like, okay. [[02:10:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7817.42s)]
*  And I was like, no, like this is a big deal. [[02:10:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7818.42s)]
*  Like they didn't used to do this. [[02:10:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7823.42s)]
*  Like now it does this. [[02:10:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7825.42s)]
*  And this is amazing. [[02:10:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7826.42s)]
*  And he's like, okay. [[02:10:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7827.42s)]
*  And I was like, why aren't you impressed? [[02:10:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7829.42s)]
*  And he's like, it's a computer. [[02:10:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7831.42s)]
*  Like, of course you ask questions and give you answers. [[02:10:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7832.42s)]
*  Like what else is it for? [[02:10:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7834.42s)]
*  And I'm like, okay, you know, I'm old. [[02:10:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7836.42s)]
*  Kids are going to just have a totally different point of view on this. [[02:10:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7838.42s)]
*  Right. [[02:10:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7842.42s)]
*  It's going to be normal to have the answers to things. [[02:10:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7843.42s)]
*  Yeah, completely normal. [[02:10:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7845.42s)]
*  And it's going to be, by the way, it's going to be normal. [[02:10:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7846.42s)]
*  It's going to be exciting. [[02:10:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7849.42s)]
*  I think it's going to make, I think it's going to be great. [[02:10:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7850.42s)]
*  Like for kids, I think it's going to be fantastic. [[02:10:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7853.42s)]
*  Well, the positive aspect just for informing people on whatever it is, whether it's a medical decision or whether it's a mechanical thing with your car. [[02:10:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7855.42s)]
*  I mean, that's pretty amazing. [[02:11:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7861.42s)]
*  One of the fun things you can do with with with Shad GPT is you can you can say explain X to me and then you can say explain X to me if as if I'm 15. [[02:11:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7864.42s)]
*  And then you can do it as if I'm 10. [[02:11:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7871.42s)]
*  You can do it as if I'm five. [[02:11:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7874.42s)]
*  And you can actually get it. [[02:11:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7876.42s)]
*  You can actually do it all the way down. [[02:11:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7877.42s)]
*  It kind of works out about age three. [[02:11:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7878.42s)]
*  So you can tell it explain quantum mechanics to me like I'm a three year old. [[02:11:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7879.42s)]
*  Wow. [[02:11:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7882.42s)]
*  And like it will. [[02:11:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7883.42s)]
*  Really? [[02:11:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7884.42s)]
*  Yeah. [[02:11:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7885.42s)]
*  And so I taught him how to do this because I'm like you just you know you can you can have it. [[02:11:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7886.42s)]
*  You can dial it up or down. [[02:11:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7889.42s)]
*  How does it explain quantum mechanics to a three year old? [[02:11:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7890.42s)]
*  It uses like all these metaphors of like you know you've got a stuffed animal over here and a stuffed animal over there and it wiggles and then that one wiggles. [[02:11:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7893.42s)]
*  Right. [[02:11:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7898.42s)]
*  And so it's like oh you know three year olds like OK. [[02:11:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7899.42s)]
*  Why not. [[02:11:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7902.42s)]
*  Right. [[02:11:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7904.42s)]
*  So yeah no. [[02:11:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7905.42s)]
*  So as a as a tool you know there's all these fights happening I guess went back to school is coming up in a couple months here and you know there's all these fights already emerging over like whether students in the classroom can use Shad GPT. [[02:11:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7906.42s)]
*  Yeah there's all these you know there's all these sites that claim to tell you whether something's been generated by AI like so the teacher can in theory can screen to see if something's been you know student hands in an essay. [[02:11:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7918.42s)]
*  And in theory there's a tool that will tell you whether they got it from Shad GPT but it doesn't actually it doesn't actually work. [[02:12:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7928.42s)]
*  There you go. [[02:12:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7933.42s)]
*  Here it is. [[02:12:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7934.42s)]
*  Can you explain quantum mechanics to a three year old. [[02:12:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7935.42s)]
*  Sure. [[02:12:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7937.42s)]
*  I'll do my best to explain quantum mechanics in a simple way for a three year old to understand. [[02:12:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7938.42s)]
*  Imagine you have a toy that would be in two places at the same time. [[02:12:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7941.42s)]
*  It's like having a teddy bear that can also be in your bed and in the living room at once. [[02:12:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7943.42s)]
*  Very strange right. [[02:12:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7947.42s)]
*  Well in the world of very tiny things like atoms and particles they can do something similar. [[02:12:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7949.42s)]
*  They can be in two different places at once just like your special toy. [[02:12:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7954.42s)]
*  This is called superposition. [[02:12:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7958.42s)]
*  Now let's talk about something else. [[02:12:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7962.42s)]
*  Imagine you have a toy car that can go through walls without crashing. [[02:12:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7963.42s)]
*  It can pass right through them as if it wasn't even there. [[02:12:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7967.42s)]
*  That's pretty amazing isn't it. [[02:12:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7970.42s)]
*  Well in the world of tiny things particles can something can do something similar. [[02:12:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7972.42s)]
*  They can pass through solid objects without any problem. [[02:12:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7977.42s)]
*  This is called tunneling. [[02:13:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7980.42s)]
*  Wow. [[02:13:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7982.42s)]
*  Pretty cool. [[02:13:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7984.42s)]
*  Yeah. [[02:13:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7985.42s)]
*  Pretty cool. [[02:13:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7986.42s)]
*  Yeah in terms of education in terms of just informing people I don't think there's anything. [[02:13:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7987.42s)]
*  I mean it's it's one of the most promising things ever by far. [[02:13:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7991.42s)]
*  By the way this is something for people's entire lives. [[02:13:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7996.42s)]
*  This isn't just for kids right. [[02:13:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7998.42s)]
*  This is for anybody who ever wants to learn anything. [[02:13:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=7999.42s)]
*  The real fear the overall fear is that what human beings are doing with artificial intelligence is creating something that's going to replace us. [[02:13:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8001.42s)]
*  You have no fear of that. [[02:13:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8010.42s)]
*  Yeah I don't. [[02:13:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8011.42s)]
*  What about 100 years from now. [[02:13:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8012.42s)]
*  It's a tool. [[02:13:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8013.42s)]
*  100 years from now. [[02:13:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8014.42s)]
*  I don't know. [[02:13:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8015.42s)]
*  And the first clue is going to happen years from now. [[02:13:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8016.42s)]
*  But it's not going to be this. [[02:13:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8017.42s)]
*  That's the fear is that we're sowing the seeds. [[02:13:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8018.42s)]
*  Yeah. [[02:13:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8022.42s)]
*  This is an old. [[02:13:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8023.42s)]
*  I mean look this is an old. [[02:13:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8024.42s)]
*  This is an old fear. [[02:13:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8025.42s)]
*  You know like the fear of the end of the world. [[02:13:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8026.42s)]
*  This is like the fear of. [[02:13:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8027.42s)]
*  Yeah. [[02:13:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8028.42s)]
*  The nonhuman. [[02:13:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8029.42s)]
*  Yeah. [[02:13:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8030.42s)]
*  Like in Judaism they have this. [[02:13:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8031.42s)]
*  They have a version of this Judaism called the Golem. [[02:13:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8032.42s)]
*  The sort of legend of the Golem. [[02:13:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8034.42s)]
*  And it was sort of this thing. [[02:13:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8035.42s)]
*  It was like it was the Warsaw Ghetto at one point. [[02:13:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8036.42s)]
*  And this rabbi figures out how to conjure up this basically this giant basically creature made of clay to go smite the enemies. [[02:13:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8038.42s)]
*  And then you know of course he comes back around and starts killing his own people. [[02:14:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8045.42s)]
*  You know the Frankenstein's monster. [[02:14:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8048.42s)]
*  Right. [[02:14:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8050.42s)]
*  Right. [[02:14:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8051.42s)]
*  Same thing. [[02:14:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8052.42s)]
*  So there's always this. [[02:14:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8053.42s)]
*  Yeah there's always. [[02:14:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8054.42s)]
*  And look it's very human. [[02:14:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8055.42s)]
*  You know it's a self-preservation. [[02:14:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8056.42s)]
*  You know kind of thing. [[02:14:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8057.42s)]
*  But you know look we build tools. [[02:14:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8058.42s)]
*  I mean what's the thing that makes us different from animals. [[02:14:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8059.42s)]
*  Right. [[02:14:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8061.42s)]
*  As we have intelligence we build tools. [[02:14:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8062.42s)]
*  Tools can be used by the way for good and bad things. [[02:14:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8063.42s)]
*  Right. [[02:14:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8064.42s)]
*  Like a shovel can be used to dig a ditch or like brain somebody. [[02:14:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8065.42s)]
*  Sure. [[02:14:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8066.42s)]
*  Over the head. [[02:14:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8067.42s)]
*  Sure. [[02:14:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8068.42s)]
*  And so all these things you know things things do have two sides. [[02:14:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8069.42s)]
*  But over time you know the tools that we built have created a much healthier safer better world. [[02:14:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8070.42s)]
*  Isn't that interesting. [[02:14:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8073.42s)]
*  Where people. [[02:14:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8074.42s)]
*  Right. [[02:14:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8075.42s)]
*  I mean look human population is like gigantic. [[02:14:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8076.42s)]
*  Right. [[02:14:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8077.42s)]
*  And so you know we have a lot of people who are like you know we have a lot of people [[02:14:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8078.42s)]
*  who are like you know we have a lot of people who are like you know we have a lot of people [[02:14:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8079.42s)]
*  I mean look human population is like you know giganically as a consequence of all these [[02:14:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8081.42s)]
*  tools we've developed. [[02:14:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8088.54s)]
*  So the exact opposite thing has happened from what everybody's been afraid of the whole [[02:14:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8089.54s)]
*  time. [[02:14:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8093.1s)]
*  But it is interesting whenever there's a discussion on these things it's never framed that there's [[02:14:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8094.1s)]
*  two sides. [[02:14:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8098.74s)]
*  Yeah. [[02:14:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8099.74s)]
*  It's always framed. [[02:15:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8100.74s)]
*  This is what we're scared of. [[02:15:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8101.74s)]
*  Right. [[02:15:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8102.74s)]
*  This is what the danger is. [[02:15:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8103.74s)]
*  Yeah. [[02:15:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8104.74s)]
*  It's not part of the beauty of this is that there's danger. [[02:15:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8105.74s)]
*  Yeah. [[02:15:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8108.9s)]
*  And so there's incredible promise that's attached to us as well like everything else like matches. [[02:15:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8110.379999999999s)]
*  No one's advocating for outlawing matches but you can start a fire. [[02:15:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8115.299999999999s)]
*  So the original myth on this. [[02:15:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8118.46s)]
*  So the way the ancients thought about this. [[02:15:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8120.179999999999s)]
*  So excuse me in that Judeo Judeo Christian philosophy they have this concept of the logos [[02:15:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8123.46s)]
*  the word. [[02:15:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8131.0599999999995s)]
*  And so it says the very beginning of the Bible in the beginning there was the word the word [[02:15:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8132.0599999999995s)]
*  was truth and then basically the universe kind of comes from that. [[02:15:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8134.58s)]
*  So this concept of like the word which is sort of knowledge. [[02:15:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8137.5s)]
*  Right. [[02:15:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8139.54s)]
*  And then Adam and Eve it was Adam and Eve eating from the tree of knowledge. [[02:15:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8140.54s)]
*  Right. [[02:15:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8143.14s)]
*  And then when they ate the apple the Satan fooled them in eating the apple and then they [[02:15:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8144.14s)]
*  had the knowledge the secret knowledge. [[02:15:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8147.58s)]
*  The Greeks had a similar concept they called technique which is the basis for the word [[02:15:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8150.46s)]
*  technology. [[02:15:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8154.1s)]
*  But it meant sort of it meant tech. [[02:15:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8155.1s)]
*  It didn't mean technology per se but it meant sort of knowledge and particular knowledge [[02:15:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8156.74s)]
*  and how to do things. [[02:15:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8159.92s)]
*  Right. [[02:16:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8160.92s)]
*  So sort of the beginning of technology and the myth that the Greeks had. [[02:16:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8161.92s)]
*  So the myth that the Christians have about the danger of knowledge is the Garden Garden [[02:16:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8165.28s)]
*  of Eden getting kicked out of the Garden Garden of Eden to the downside. [[02:16:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8169.76s)]
*  Right. [[02:16:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8172.639999999999s)]
*  That was viewed as a tragedy. [[02:16:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8173.639999999999s)]
*  Right. [[02:16:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8174.639999999999s)]
*  In that religion the Greeks had what they call the Prometheus myth and it had to do [[02:16:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8175.639999999999s)]
*  with fire. [[02:16:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8178.8s)]
*  Right. [[02:16:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8179.8s)]
*  And so and the myth of Prometheus was a central Greek myth and the myth of Prometheus was [[02:16:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8180.8s)]
*  Prometheus was a god kind of character in the mythology who went up to the mountain. [[02:16:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8184.8s)]
*  Humans didn't have fire. [[02:16:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8190.0s)]
*  He went up to the mountain and the gods had fire and he took fire from the gods and he [[02:16:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8191.0s)]
*  brought it down and gave it to humanity. [[02:16:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8194.88s)]
*  In the myth that was how humans learned to basically use fire as a tool. [[02:16:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8196.839999999998s)]
*  As punishment for bringing fire to humans he was in the myth he was chained to a rock [[02:16:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8202.519999999999s)]
*  for all eternity and every day his liver gets pecked out by an angry bird and then it regenerates [[02:16:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8207.92s)]
*  overnight and then it gets pecked out again the next day forever. [[02:16:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8213.099999999999s)]
*  Like that's how much the gods felt like they had to punish him. [[02:16:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8216.119999999999s)]
*  Right. [[02:16:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8218.8s)]
*  Because and of course what were they saying in that myth what they were saying is okay [[02:16:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8219.8s)]
*  Fire was like the original technology. [[02:17:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8223.279999999999s)]
*  Right. [[02:17:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8224.88s)]
*  And the nature of fire as a technology is it makes human civilization possible. [[02:17:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8225.88s)]
*  You can stay warm at night. [[02:17:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8229.599999999999s)]
*  You can fight off the wolves. [[02:17:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8230.8s)]
*  You know you bond the tribe together. [[02:17:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8232.279999999999s)]
*  Every culture has like a fire central thing to it because it's like the center of the [[02:17:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8234.279999999999s)]
*  community. [[02:17:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8238.24s)]
*  You can use it you know to cook meat. [[02:17:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8239.8s)]
*  Right. [[02:17:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8241.72s)]
*  Therefore you can have you know you can have you have a higher rate of your kids are going [[02:17:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8242.72s)]
*  to survive and so forth be able to reproduce more. [[02:17:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8244.96s)]
*  But of course fire is also a fearsome weapon and you can use it to burn people alive. [[02:17:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8248.16s)]
*  You can use it to destroy entire cities. [[02:17:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8252.480000000001s)]
*  And so like it got it's fantastic because it got that idea of information the information [[02:17:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8254.6s)]
*  technology in the form of even fire was so scary that they encoded it that deeply in [[02:17:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8259.880000000001s)]
*  their mythology. [[02:17:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8264.12s)]
*  And so I think what we do is we just like play that exactly like you said they we play [[02:17:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8265.480000000001s)]
*  that fear out over and over again because in the back of our heads always like okay [[02:17:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8269.480000000001s)]
*  this is the one that's going to get us. [[02:17:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8273.52s)]
*  You know yes I know that the previous three thousand of these things that actually turned [[02:17:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8274.92s)]
*  out fine. [[02:17:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8278.880000000001s)]
*  Right. [[02:17:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8279.880000000001s)]
*  Even nuclear weapons turned out fine. [[02:18:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8280.92s)]
*  Right. [[02:18:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8282.64s)]
*  Like nuclear weapons almost certainly prevented World War 3. [[02:18:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8283.64s)]
*  Right. [[02:18:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8286.599999999999s)]
*  Nuclear weapons the existence of nuclear weapons probably saved on the order of 200 million [[02:18:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8287.599999999999s)]
*  lives. [[02:18:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8290.119999999999s)]
*  Right. [[02:18:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8291.119999999999s)]
*  So like even nuclear weapons turned out OK. [[02:18:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8292.119999999999s)]
*  Right. [[02:18:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8293.599999999999s)]
*  But yet after all of that and all the progress we made like this is the one that's going [[02:18:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8294.599999999999s)]
*  to get us. [[02:18:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8297.16s)]
*  Yeah. [[02:18:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8298.16s)]
*  It's so interesting because that conversation is never had. [[02:18:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8299.16s)]
*  We only hear the negative aspects of it. [[02:18:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8303.88s)]
*  Yeah that's right. [[02:18:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8305.759999999998s)]
*  These are complex nuanced discussions and it has to do with all sorts of aspects of [[02:18:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8306.92s)]
*  human nature and control and power structures. [[02:18:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8312.2s)]
*  It's just they're very complex conversations. [[02:18:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8315.24s)]
*  And then people try to hijack them. [[02:18:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8318.12s)]
*  Right. [[02:18:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8320.16s)]
*  Yeah. [[02:18:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8321.16s)]
*  Right. [[02:18:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8322.16s)]
*  They get used. [[02:18:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8323.16s)]
*  Yeah. [[02:18:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8324.16s)]
*  And that's where. [[02:18:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8325.16s)]
*  Yeah. [[02:18:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8326.16s)]
*  I don't mind like somebody who's like there's this concept I talk about the the the Baptist [[02:18:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8327.16s)]
*  and the bootleggers. [[02:18:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8329.32s)]
*  So there were two groups of people in favor of prohibition of alcohol. [[02:18:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8330.32s)]
*  There were the Baptists who were the social activists who thought alcohol was actually [[02:18:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8334.24s)]
*  evil and was destroying society. [[02:18:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8337.0s)]
*  And then there were the bootleggers which were the people who were going to make money [[02:18:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8338.800000000001s)]
*  right. [[02:19:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8341.0s)]
*  If alcohol was outlawed. [[02:19:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8342.0s)]
*  Right. [[02:19:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8343.0s)]
*  And so this is what you often have is you have a there's one of these social movements [[02:19:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8344.0s)]
*  that wants regulation. [[02:19:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8346.960000000001s)]
*  You often have this union of the Baptist and the bootleggers. [[02:19:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8347.960000000001s)]
*  And so the Baptist I don't mind like the true believers who are like worried about you know [[02:19:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8350.6s)]
*  X Y Z. [[02:19:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8354.2s)]
*  It's like OK let's talk about that. [[02:19:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8355.2s)]
*  Let's figure that out. [[02:19:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8356.32s)]
*  It's the bootleggers that like driving crazy right. [[02:19:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8357.800000000001s)]
*  Of course. [[02:19:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8360.24s)]
*  So you pick up that argument. [[02:19:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8361.28s)]
*  Yeah. [[02:19:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8362.64s)]
*  And then are you know working behind the scenes to achieve you know basically self-interested [[02:19:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8363.64s)]
*  ends. [[02:19:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8366.039999999999s)]
*  Well I have hope. [[02:19:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8367.039999999999s)]
*  I really do. [[02:19:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8370.199999999999s)]
*  I mean I like to dwell on the negative aspects of it because it's fun. [[02:19:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8371.199999999999s)]
*  But one of the things that I have hope in is that there are conversations like this [[02:19:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8374.64s)]
*  taking place where this is a very kind of unique thing in terms of human history like [[02:19:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8378.48s)]
*  the ability to independently distribute something that reaches millions of people that can talk [[02:19:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8383.119999999999s)]
*  about these things. [[02:19:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8388.16s)]
*  So these this can get out there and then other people hear this and they'll start their own [[02:19:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8389.44s)]
*  conversations about it and articles will be written and more people discuss it and then [[02:19:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8393.48s)]
*  look at this more nuanced perspective because I think it is something that's incredibly [[02:19:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8397.32s)]
*  complicated and you can't deny that just what Chappie T can do chat GPD could do right now [[02:20:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8401.72s)]
*  is extraordinary and very beneficial. [[02:20:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8407.279999999999s)]
*  Even if they just stopped it right there. [[02:20:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8411.199999999999s)]
*  Yeah. [[02:20:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8412.52s)]
*  I mean just right there. [[02:20:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8413.52s)]
*  But it's not going to stop there. [[02:20:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8415.199999999999s)]
*  When I see something crazy. [[02:20:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8416.199999999999s)]
*  Yes. [[02:20:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8417.199999999999s)]
*  Can I ask for something to be pulled up. [[02:20:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8418.199999999999s)]
*  Sure. [[02:20:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8419.68s)]
*  Twitter go to Twitter. [[02:20:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8420.68s)]
*  This just came up today because we've been talking about text we've been talking about [[02:20:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8421.68s)]
*  chat GPD. [[02:20:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8424.84s)]
*  So let's look at let's look at images for a moment. [[02:20:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8425.84s)]
*  So we're going to do a search do a search on mid journey and then Chihuly the artist [[02:20:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8428.36s)]
*  C H I H U L Y. [[02:20:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8435.880000000001s)]
*  See see H I Chihuly see H I H. [[02:20:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8439.480000000001s)]
*  H U L I. [[02:20:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8444.6s)]
*  Right there. [[02:20:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8448.8s)]
*  That one. [[02:20:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8450.48s)]
*  OK. [[02:20:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8451.48s)]
*  That's pretty good. [[02:20:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8452.48s)]
*  But let go go to more go to more. [[02:20:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8453.48s)]
*  No stay on that one but go to that image the shoe right there. [[02:20:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8455.68s)]
*  There we go. [[02:20:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8458.199999999999s)]
*  OK. [[02:20:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8459.199999999999s)]
*  So. [[02:21:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8460.199999999999s)]
*  So this is mid journey. [[02:21:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8461.199999999999s)]
*  So this is the app that lets you create images you describe words you describe words and [[02:21:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8462.199999999999s)]
*  it creates images it uses the same technology as chat GPT but it generates images. [[02:21:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8467.119999999999s)]
*  This is the prompt here was something along the lines of a Nike shoe in the form of this [[02:21:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8472.599999999999s)]
*  artist called Chihuly who's this famous artist who works in basically blown glass is his [[02:21:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8477.04s)]
*  art form. [[02:21:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8481.72s)]
*  So this is a Nike shoe rendered in blown glass multi-color. [[02:21:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8482.72s)]
*  Chihuly is famous for using lots of colors and so this is all the colors. [[02:21:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8486.48s)]
*  So this does look exactly like his shoe would have looked. [[02:21:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8488.92s)]
*  Yeah this would be Chihuly. [[02:21:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8491.04s)]
*  Yeah this is Chihuly you know skirt billowing you know billowing skirt. [[02:21:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8492.04s)]
*  Yeah this is Chihuly Chihuly you know like statue of an avocado. [[02:21:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8497.24s)]
*  Right. [[02:21:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8501.12s)]
*  Right. [[02:21:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8502.12s)]
*  And so it's an avocado made out of stained glass. [[02:21:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8503.12s)]
*  OK. [[02:21:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8504.72s)]
*  So just look look here for a moment though. [[02:21:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8505.72s)]
*  Yeah just go back to the avocado for a second. [[02:21:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8507.679999999998s)]
*  OK OK. [[02:21:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8510.8s)]
*  Look at the shadows. [[02:21:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8511.8s)]
*  Look at the detail in the shadows. [[02:21:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8512.8s)]
*  Incredible. [[02:21:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8514.8s)]
*  Look at the detail of the shadows with the sunlight coming through the window. [[02:21:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8515.8s)]
*  OK now go back go back to the shoe because this one blows my mind. [[02:21:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8519.32s)]
*  OK and then zoom in on the reflection of the shoe in the bottom down there. [[02:22:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8522.439999999999s)]
*  Right. [[02:22:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8525.679999999998s)]
*  It's like you see it's like perfect right. [[02:22:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8526.679999999998s)]
*  It's like a perfect it's like a perfectly corresponding reflection. [[02:22:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8528.16s)]
*  OK this entire thing was generated by mid-journey mid-journey the way mid-journey works is it [[02:22:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8531.72s)]
*  predicts the next pixel. [[02:22:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8534.84s)]
*  So the way that it worked was it basically ran this algorithm basically it used the prompt [[02:22:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8535.84s)]
*  and then it ran it through the neural network and then it predicted each pixel in turn for [[02:22:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8542.0s)]
*  this image and this image probably has you know 100,000 pixels in it or something or [[02:22:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8545.16s)]
*  a million pixels or something. [[02:22:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8548.32s)]
*  And it basically was it's like an autocomplete it was predicting each pixel. [[02:22:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8550.56s)]
*  But in the process of predicting each pixel it was able to render not only colors and [[02:22:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8554.8s)]
*  shapes and all those things but transparency translucency reflections shadows lighting like [[02:22:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8559.62s)]
*  it trained itself basically on how to do a full 3D rendering inside the neural network [[02:22:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8568.54s)]
*  in order to be able to successfully predict the next pixel. [[02:22:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8574.02s)]
*  And how long does something like that take to generate. [[02:22:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8577.380000000001s)]
*  That takes to generate on the on this when you're running the system today that would [[02:22:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8579.62s)]
*  probably be I'm getting guess 10 or 15 seconds. [[02:23:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8583.820000000002s)]
*  There's a newer version of mid-journey a turbo version that just came out where I think [[02:23:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8587.86s)]
*  it cuts it down a couple seconds. [[02:23:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8590.900000000001s)]
*  Now the system that's generating that needed you know many years of computing power across [[02:23:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8592.54s)]
*  many processors to get ready to do the training that took place. [[02:23:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8599.1s)]
*  But they actually generated that in seconds. [[02:23:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8603.54s)]
*  Took a few seconds. [[02:23:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8605.86s)]
*  Okay so here's another amazing thing. [[02:23:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8606.86s)]
*  The price the cost of generating image like that versus hiring a human artist to do it [[02:23:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8610.82s)]
*  is like down by a factor of a thousand somewhere between a factor of a thousand and ten thousand [[02:23:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8615.92s)]
*  if you just kind of run the numbers like to hire an artist to do that at that level of [[02:23:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8620.44s)]
*  quality would cost in the order of a thousand ten thousand dollars more dollars or you know [[02:23:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8623.04s)]
*  time or human effort than doing it with the machine. [[02:23:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8627.44s)]
*  The same thing is true of writing a legal brief. [[02:23:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8632.32s)]
*  The same thing is true of writing a medical diagnosis. [[02:23:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8635.2s)]
*  The same thing is true of you know summarizing a book like any sort of you know knowledge [[02:23:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8638.04s)]
*  summarizing a podcast you know any of these things drafting questions for a podcast you [[02:24:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8642.92s)]
*  know basically pennies right to be able to do all these things versus you know potentially [[02:24:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8648.72s)]
*  a hundred or a thousand dollars to have a person do any of these things. [[02:24:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8653.04s)]
*  So we've dropped the cost of a lot of white collar work by like a factor of a thousand [[02:24:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8657.54s)]
*  right. [[02:24:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8661.48s)]
*  Guess what we haven't dropped the cost of like at all. [[02:24:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8662.48s)]
*  It's all the blue collar work right. [[02:24:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8665.68s)]
*  So we do not have today a machine that can pick strawberries that is less expensive than [[02:24:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8668.34s)]
*  hiring people to pick strawberries. [[02:24:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8673.02s)]
*  We do not have a machine that can pack your suitcase. [[02:24:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8675.14s)]
*  We do not have a machine that can clean your toilet. [[02:24:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8676.98s)]
*  We don't have a machine that can cook you dinner. [[02:24:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8680.380000000001s)]
*  Like we don't have any of those things. [[02:24:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8681.7s)]
*  Like for those things the cost of the machine and the AI and everything else to do those [[02:24:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8683.6s)]
*  things is far in excess of what you can simply pay people to do. [[02:24:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8686.86s)]
*  Right. [[02:24:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8691.06s)]
*  But so there's the great twist here is that in all of the economic fears around automation [[02:24:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8692.06s)]
*  fear has always been that it's the mechanical work that gets replaced because the presumption [[02:24:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8697.46s)]
*  is people working with their brains right. [[02:25:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8701.179999999998s)]
*  That's certainly not what the computer is going to be. [[02:25:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8703.939999999999s)]
*  Certainly computers not going to make art. [[02:25:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8705.46s)]
*  Right. [[02:25:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8706.98s)]
*  So the computer is going to be able to pick strawberries or is going to be able to make [[02:25:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8707.98s)]
*  cheeseburgers but obviously it's not going to be able to make art and actually turns [[02:25:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8709.699999999999s)]
*  out the reverse is true. [[02:25:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8712.099999999999s)]
*  It's much easier to make the image of that shoe than it is to make you a cheeseburger. [[02:25:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8713.9s)]
*  Of course because it has to be automated physically. [[02:25:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8718.0s)]
*  Physically but not just around. [[02:25:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8720.74s)]
*  But move around but not just physically which is like OK like what happens if the stove [[02:25:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8722.58s)]
*  catches on fire. [[02:25:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8727.66s)]
*  Right. [[02:25:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8728.66s)]
*  Right. [[02:25:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8729.66s)]
*  Like you know OK like what you know what shape how does the suitcase unclasp suitcases unclasped [[02:25:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8730.66s)]
*  differently like all the yes all the like real world stuff. [[02:25:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8734.1s)]
*  How do you plummet toilet. [[02:25:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8737.84s)]
*  Right. [[02:25:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8739.18s)]
*  Like you know what happens when you get in there. [[02:25:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8740.18s)]
*  Right. [[02:25:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8741.18s)]
*  And what happens if the plumbing is all screwed up. [[02:25:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8742.18s)]
*  And so the great irony and twist of all this is it when the when the breakthrough we all [[02:25:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8743.18s)]
*  thought in the industry we all thought when the breakthrough arrived it would arrive in [[02:25:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8747.7s)]
*  the form of robotics that would cause you know what the fear would be it would cause [[02:25:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8750.78s)]
*  unemployment among basically the less you know quote unquote lower skilled people or [[02:25:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8754.02s)]
*  less educated people. [[02:25:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8757.54s)]
*  It turns out to be the exact opposite. [[02:25:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8758.54s)]
*  Well that's Andrew Yang's take on automation. [[02:26:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8760.900000000001s)]
*  Right. [[02:26:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8763.220000000001s)]
*  The need for universal basic income. [[02:26:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8764.220000000001s)]
*  Yeah. [[02:26:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8765.62s)]
*  Well yes. [[02:26:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8766.62s)]
*  Therefore the need for communism. [[02:26:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8767.62s)]
*  Which is immediately where it goes. [[02:26:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8768.62s)]
*  But think before you think about that though think think about what this means in terms [[02:26:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8771.94s)]
*  of productivity. [[02:26:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8774.74s)]
*  So think in terms of what this means about what people can do. [[02:26:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8775.74s)]
*  Right. [[02:26:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8778.5s)]
*  And think about the benefit including the economic benefit. [[02:26:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8779.62s)]
*  So all of everybody always thinks of this as producer first. [[02:26:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8781.62s)]
*  You want to start by thinking this is consumer first which is like as a customer of all of [[02:26:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8783.9s)]
*  the goods and services that involve knowledge work the price on all of those things is about [[02:26:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8789.3s)]
*  to drop on the order of like a thousand X. [[02:26:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8793.02s)]
*  Right. [[02:26:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8795.26s)]
*  So everything that you pay for today right that involves white collar work like the prices [[02:26:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8796.26s)]
*  and all those things are going to collapse. [[02:26:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8799.42s)]
*  By the way that's the collapse in the prices is why it doesn't actually cause unemployment. [[02:26:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8801.02s)]
*  Because when prices collapse it frees up spending power and then you'll spend that same money [[02:26:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8805.46s)]
*  on new things. [[02:26:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8808.880000000001s)]
*  And so your quality of life will rise and then there will be new jobs created that will [[02:26:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8810.0s)]
*  basically take the place of the jobs that get destroyed. [[02:26:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8813.36s)]
*  But what you'll experience is a dramatic hopefully a dramatic fall in the cost of the goods and [[02:26:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8816.0s)]
*  services that you buy. [[02:27:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8820.52s)]
*  But what is the equivalent of basically giving everybody a raise. [[02:27:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8821.52s)]
*  What about artists rights. [[02:27:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8824.36s)]
*  Because one of the arguments about art is that you're taking this midway you're taking [[02:27:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8826.24s)]
*  this A.I. program and it's essentially stealing the images of the style of artists and then [[02:27:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8832.96s)]
*  compiling its own. [[02:27:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8840.039999999999s)]
*  But that the intellectual work the original creative work was responsible for generating [[02:27:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8842.0s)]
*  this in the first place. [[02:27:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8847.32s)]
*  So even though you're not paying the illustrator you're essentially using that illustrator's [[02:27:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8848.759999999998s)]
*  creativity and ideas to generate these images through A.I. [[02:27:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8852.359999999999s)]
*  And in fact we just saw an example of that. [[02:27:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8855.8s)]
*  Yes. [[02:27:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8857.919999999998s)]
*  We actually named a specific artist who certainly did not get paid as a consequence of that. [[02:27:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8859.16s)]
*  And the algorithm the algorithm knew who Chihuly was so it had clearly been trained on his [[02:27:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8864.2s)]
*  art before otherwise he wouldn't the algorithm would not have known to do it in that style. [[02:27:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8868.48s)]
*  So I think this is going to be a very big fight. [[02:27:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8872.76s)]
*  I think this is probably going to go ultimately to the Supreme Court. [[02:27:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8874.72s)]
*  Those cases are just starting now. [[02:27:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8877.64s)]
*  I think the first one is Getty Images which owns a big catalog of photography is actually [[02:27:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8879.4s)]
*  suing this company mid-journey. [[02:28:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8883.52s)]
*  So that has begun. [[02:28:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8885.84s)]
*  The argument for why what's happening is improper is exactly like is exactly what you said. [[02:28:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8889.24s)]
*  The argument for why it's actually just fine and in fact not only should be legal but actually [[02:28:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8894.16s)]
*  is legal under current copyright law is what in copyright law is called the right to make [[02:28:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8898.24s)]
*  transformative works. [[02:28:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8903.4s)]
*  And so you have the total right as an artist or creator to make any level of creative art [[02:28:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8905.4s)]
*  that you want or expression that is inspired by right or the result of what they call transforming [[02:28:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8911.0s)]
*  prior works. [[02:28:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8916.48s)]
*  Right. [[02:28:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8917.48s)]
*  Right. [[02:28:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8918.48s)]
*  So you have the right to do homages. [[02:28:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8919.48s)]
*  You have the right to do you know I mentioned earlier the guy who wrote the other version [[02:28:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8920.48s)]
*  of the book 1984. [[02:28:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8925.0s)]
*  Right. [[02:28:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8926.0s)]
*  He had the right to do that because he was transforming the work. [[02:28:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8927.0s)]
*  You could make your version of what you think of Picasso would look like. [[02:28:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8928.92s)]
*  Exactly you are free to draw in the style of Picasso. [[02:28:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8932.32s)]
*  You are not free to copy a Picasso but you are free to study all of every all the art [[02:28:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8934.6s)]
*  Picasso did and as long as you don't misrepresent it as being a Picasso you can generate all [[02:28:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8939.480000000001s)]
*  the new art. [[02:29:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8943.880000000001s)]
*  Are you free to make to copy a Picasso exactly if you're telling everybody you're copying [[02:29:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8944.880000000001s)]
*  a Picasso. [[02:29:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8950.28s)]
*  I don't know. [[02:29:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8951.44s)]
*  The artist I mean copyright at some point expires but that aside let's assume copyright [[02:29:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8953.2s)]
*  lasts let's just assume for the moment copyrights forever just to make it easy to talk about [[02:29:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8958.04s)]
*  the artist can copyright that particular image. [[02:29:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8962.960000000001s)]
*  The screenwriter can copyright that particular screenplay. [[02:29:26](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8966.16s)]
*  Right. [[02:29:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8969.0s)]
*  If you're not but if you're not generating income from it. [[02:29:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8970.0s)]
*  Oh I don't know. [[02:29:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8972.2s)]
*  Yeah. [[02:29:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8973.2s)]
*  There's there's a car about there's another car about in the copyright law for non commercial [[02:29:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8974.2s)]
*  use. [[02:29:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8976.72s)]
*  Yeah. [[02:29:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8977.72s)]
*  So there's like academic use. [[02:29:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8978.72s)]
*  By the way there's also protection there's also protection for satire. [[02:29:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8979.72s)]
*  You know there's protection for a variety of things. [[02:29:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8982.24s)]
*  But the one that's relevant here specifically is that is the transformative one because [[02:29:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8984.960000000001s)]
*  the reason the reason I say that is because Chihuly never made a shoe. [[02:29:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8988.56s)]
*  Right. [[02:29:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8992.0s)]
*  Right. [[02:29:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8993.0s)]
*  There was so there's no image in the training set that was a Chihuly shoe. [[02:29:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8994.0s)]
*  Certainly not a Chihuly Nike shoe and certainly not that Chihuly Nike shoe. [[02:29:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8996.52s)]
*  Right. [[02:29:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=8999.6s)]
*  And so the algorithm produced an homage be the way to think about it. [[02:30:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9000.6s)]
*  Right. [[02:30:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9004.52s)]
*  And as a consequence of that I think the way through copyright law you're like OK that's [[02:30:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9005.52s)]
*  just fine. [[02:30:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9008.359999999999s)]
*  And I think the same thing is true of chat GPT for all the text that is by the way the [[02:30:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9009.88s)]
*  same thing is happening chat GPT the news publishers newspaper publishers are now getting [[02:30:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9013.439999999999s)]
*  very upset because they they have this they have this fear or they have a fear that people [[02:30:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9016.8s)]
*  are going to stop reading the news because they're just going to get to just ask chat [[02:30:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9020.56s)]
*  GPT what's happening in the world. [[02:30:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9023.4s)]
*  Right. [[02:30:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9024.4s)]
*  They probably will. [[02:30:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9025.4s)]
*  And there are lots of news articles that are in the Internet training data that went into [[02:30:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9027.32s)]
*  training chat GPT. [[02:30:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9030.279999999999s)]
*  Right. [[02:30:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9031.279999999999s)]
*  Including you know updating it every day. [[02:30:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9032.279999999999s)]
*  Right. [[02:30:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9034.08s)]
*  Also if you can generate an objective news source through chat GPT because that's really [[02:30:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9035.08s)]
*  hard to do. [[02:30:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9040.039999999999s)]
*  So one of the fun things that these machines can do and you can do this a chat GPT actually [[02:30:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9041.16s)]
*  could do this today. [[02:30:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9044.64s)]
*  You can tell it to take out. [[02:30:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9046.06s)]
*  It will do what's called sentiment analysis. [[02:30:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9048.24s)]
*  You can you can ask it is this like is this news article slanted to the left or the right. [[02:30:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9049.8s)]
*  Is this is this is the emotional tone here angry or like hostile. [[02:30:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9054.480000000001s)]
*  And you can tell it to rewrite news articles to take out the bias interest. [[02:30:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9058.640000000001s)]
*  And you can tell you take out any political bias and take out any emotional loading and [[02:31:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9062.04s)]
*  it will rewrite the article to be as objective as it can possibly come up with. [[02:31:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9066.24s)]
*  And so that and again. [[02:31:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9069.640000000001s)]
*  But here's the question is OK the result of that is that still copyrighted. [[02:31:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9070.640000000001s)]
*  Right. [[02:31:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9075.28s)]
*  Is that a is that a copyrighted derivative work of the original news article or is that [[02:31:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9076.28s)]
*  actually now something new that is a transformation of the thing that existed before. [[02:31:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9080.44s)]
*  But it's different enough that it is actually that it's actually fine for the machine to [[02:31:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9084.6s)]
*  do that without without copyright being a problem. [[02:31:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9087.44s)]
*  People when they when they encounter objective information like objective news they're always [[02:31:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9089.9s)]
*  going to look for someone who has an analysis of that news then they want a human perspective [[02:31:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9094.960000000001s)]
*  on it which is very interesting. [[02:31:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9100.24s)]
*  I wonder how a I fits into that. [[02:31:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9102.12s)]
*  So one of the things you can do is that you so you can ask it just straight up. [[02:31:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9106.480000000001s)]
*  Give me the left wing view on this or give me the right wing. [[02:31:49](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9109.26s)]
*  Or by the way you can also you do this a lot is like you can create two personas you can [[02:31:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9111.82s)]
*  say I want a left winger and a right winger and I want to argue this out. [[02:31:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9115.46s)]
*  Oh wow. [[02:31:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9117.98s)]
*  Right. [[02:31:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9118.98s)]
*  It'll do that. [[02:31:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9119.98s)]
*  But here's another thing it'll do is you can tell it to write in the style of any person [[02:32:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9120.98s)]
*  whose sensibility you admire. [[02:32:04](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9124.179999999998s)]
*  So take somebody who you really take take take take our of K. You could say analyze [[02:32:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9126.099999999999s)]
*  this topic for me adopt the persona of our of K and then analyze this topic for me and [[02:32:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9132.019999999999s)]
*  it will use all of the training data that it has with respect to everything that our [[02:32:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9137.1s)]
*  of K has ever done and said and how he looks at things and how he talks about things and [[02:32:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9141.18s)]
*  how he you know whatever does whatever he does and it will produce something that the [[02:32:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9145.019999999999s)]
*  odds are going to be pretty similar to what the actual person is going to say. [[02:32:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9148.22s)]
*  But you can do the same thing for Peter Hotez you can do the same thing for you know authority [[02:32:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9150.74s)]
*  figures you can do the same thing for really. [[02:32:34](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9154.38s)]
*  What would Jesus say. [[02:32:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9157.3s)]
*  Right. [[02:32:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9158.3s)]
*  Literally literally. [[02:32:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9159.3s)]
*  What would Jesus say. [[02:32:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9160.3s)]
*  Wow. [[02:32:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9161.3s)]
*  And it will. [[02:32:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9162.3s)]
*  But it's it's using the complete set of text and all accounts of everything Jesus ever [[02:32:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9163.539999999999s)]
*  said and did and it's going to produce something that at least is going to be reasonably close [[02:32:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9167.779999999999s)]
*  to that. [[02:32:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9171.259999999998s)]
*  What a bizarre new world we're in the middle of right now. [[02:32:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9172.259999999998s)]
*  Exactly. [[02:32:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9175.82s)]
*  And so you can channel it. [[02:32:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9176.82s)]
*  It's a fascinating thing you can channel historical figure you can channel Abraham Lincoln like [[02:32:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9178.779999999999s)]
*  OK here's another example for how kids are going to do this. [[02:33:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9183.38s)]
*  It's like OK it's time to learn about the Civil War. [[02:33:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9186.259999999998s)]
*  OK let's talk to Abraham Lincoln. [[02:33:07](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9187.619999999999s)]
*  Let's be able to ask him questions. [[02:33:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9190.259999999998s)]
*  Right. [[02:33:11](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9191.259999999998s)]
*  And again it's not like you're not of course actually talking to Abraham Lincoln but you [[02:33:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9192.5s)]
*  are talking to the sum total of all written expression all books ever written about Lincoln [[02:33:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9195.5s)]
*  and he's talking back at you. [[02:33:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9200.380000000001s)]
*  Right. [[02:33:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9201.380000000001s)]
*  So yeah it'll it'll happily do that for you. [[02:33:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9202.380000000001s)]
*  Just what is a 20 year old going to look like that's born today. [[02:33:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9205.62s)]
*  When they hit 20 like what kind of access to information view of the world understanding [[02:33:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9209.82s)]
*  of things instantaneous knowledge. [[02:33:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9216.28s)]
*  What if any thoughts do you have on things like Neuralink and the emerging technologies [[02:33:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9221.160000000002s)]
*  of human neural interfaces. [[02:33:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9228.400000000001s)]
*  Yeah. [[02:33:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9230.800000000001s)]
*  So this is the this is this is what the AI safety people describe as like the out or [[02:33:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9231.800000000001s)]
*  the you know the fallback position or something which is OK if you can't beat him join him. [[02:33:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9236.960000000001s)]
*  Yeah right. [[02:34:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9241.6s)]
*  Right. [[02:34:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9242.6s)]
*  Maybe we just need to like upgrade everybody's intelligence and maybe we to do that as a [[02:34:03](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9243.6s)]
*  kind of fuse man and machine. [[02:34:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9245.92s)]
*  Maybe. [[02:34:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9248.060000000001s)]
*  Yeah look the technology is it's very so it's very serious technology. [[02:34:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9249.74s)]
*  It's like the technology is for real that they're working on like that. [[02:34:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9253.34s)]
*  They and people like them are it's all for real. [[02:34:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9256.019999999999s)]
*  You know people have been working on the ideas underneath this for like 30 years. [[02:34:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9259.539999999999s)]
*  You know things like MRIs. [[02:34:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9262.66s)]
*  And by the way the thing on this is there's a lot of immediate health care applications [[02:34:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9263.66s)]
*  so like people with Parkinson's right people people who have had you know who have been [[02:34:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9268.22s)]
*  paraplegics quadriplegics being able to restore you know the ability to move like they're [[02:34:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9272.74s)]
*  being able to fix things that are broken in the nervous system able to restore sight to [[02:34:36](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9276.9s)]
*  people who can't see if there's some breakdown. [[02:34:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9281.34s)]
*  So so there's a lot of very straightforward medical applications that are potentially [[02:34:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9283.22s)]
*  a very big deal. [[02:34:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9286.66s)]
*  And then there's the idea of like the full actual fusion where you know a machine knows [[02:34:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9288.82s)]
*  what you're thinking and it's able to kind of think with you or you're able to access [[02:34:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9292.18s)]
*  it. [[02:34:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9294.779999999999s)]
*  Yeah. [[02:34:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9295.779999999999s)]
*  I think through it. [[02:34:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9296.779999999999s)]
*  I would just say it's it's all it's exciting. [[02:34:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9297.779999999999s)]
*  It's the field that's moving pretty quickly at this point. [[02:34:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9299.22s)]
*  But we're I think still I'm going to guess 20 years out or something from anything that [[02:35:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9301.7s)]
*  would resemble what you would hypothesize it to be like. [[02:35:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9308.58s)]
*  But maybe I'll be surprised. [[02:35:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9312.42s)]
*  20 years ago it was 2003. [[02:35:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9313.42s)]
*  That is true. [[02:35:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9315.38s)]
*  Time seems so recent. [[02:35:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9316.38s)]
*  Time does fly. [[02:35:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9318.66s)]
*  Yeah that seems very recent. [[02:35:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9319.66s)]
*  They're starting to be able there have been papers in the last six months. [[02:35:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9321.38s)]
*  They're actually people using this technology specifically the same same kind of thing that [[02:35:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9324.98s)]
*  we just saw with it with the shoe. [[02:35:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9328.7s)]
*  They're figuring out how they claim people claim to now know how to do a brain scan and [[02:35:31](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9331.619999999999s)]
*  be able to pull out basically the image that you're thinking of as an image. [[02:35:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9335.619999999999s)]
*  Now this is brand new research and so people are making a lot of claims on things. [[02:35:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9340.34s)]
*  I don't know whether it's actually real or not. [[02:35:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9343.42s)]
*  There's a bunch of work going into that. [[02:35:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9345.939999999999s)]
*  There's a bunch of work going into whether it can basically get words out. [[02:35:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9348.18s)]
*  Right. [[02:35:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9351.099999999999s)]
*  If you're thinking about a word be able to pull the word out. [[02:35:52](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9352.099999999999s)]
*  Yeah. [[02:35:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9354.14s)]
*  So this is the yeah. [[02:35:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9355.14s)]
*  OK. [[02:35:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9356.58s)]
*  So A.I. recreates what people see by reading their brain scans a new artificial intelligence [[02:35:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9357.9s)]
*  system can reconstruct images of a person. [[02:36:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9362.619999999999s)]
*  The images a person saw based on their brain activity. [[02:36:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9366.5s)]
*  Yeah. [[02:36:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9369.18s)]
*  So the claim here is that those would be the original images on top. [[02:36:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9370.18s)]
*  And as you're looking at them it'll do a brain scan and it'll feed the result of the brain [[02:36:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9372.22s)]
*  scan into a system like the one that does the shoes. [[02:36:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9376.3s)]
*  Wow. [[02:36:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9378.859999999999s)]
*  And then that system produces these you know these images. [[02:36:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9379.859999999999s)]
*  That's pretty damn close. [[02:36:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9382.619999999999s)]
*  Yeah. [[02:36:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9383.82s)]
*  So extrapolation off of off of the image generation stuff that we've been watching. [[02:36:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9384.86s)]
*  Yeah. [[02:36:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9388.38s)]
*  That's pretty close. [[02:36:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9389.38s)]
*  Now excuse me these are you know this is this is brand new. [[02:36:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9390.38s)]
*  Like is this you know this is this real. [[02:36:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9393.26s)]
*  Right. [[02:36:37](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9397.619999999999s)]
*  Is it like the Samsung moon shot. [[02:36:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9398.619999999999s)]
*  Yeah. [[02:36:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9399.619999999999s)]
*  Yeah. [[02:36:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9400.619999999999s)]
*  Does it. [[02:36:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9401.619999999999s)]
*  Does it. [[02:36:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9402.619999999999s)]
*  Is it repeatable. [[02:36:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9403.619999999999s)]
*  By the way you need to be strapped to a million dollars worth of lab equipment. [[02:36:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9404.619999999999s)]
*  Right. [[02:36:45](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9405.619999999999s)]
*  You know so there's like these things can take a while to get to work but. [[02:36:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9406.619999999999s)]
*  Pretty fascinating if it's applicable though if that really can happen. [[02:36:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9410.5s)]
*  Hypothetically. [[02:36:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9413.38s)]
*  Wow. [[02:36:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9414.42s)]
*  Wow. [[02:36:56](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9416.42s)]
*  Exactly. [[02:36:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9417.42s)]
*  It's a wild world. [[02:36:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9418.42s)]
*  Mm hmm. [[02:36:59](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9419.42s)]
*  Yeah. [[02:37:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9420.42s)]
*  It's the possibilities are very fascinating because it just seems like we're about to [[02:37:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9421.42s)]
*  enter into a world that's so different than anything human beings have ever experienced [[02:37:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9426.939999999999s)]
*  before. [[02:37:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9432.859999999999s)]
*  All technology driven. [[02:37:13](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9433.859999999999s)]
*  You're in the middle of it buddy. [[02:37:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9435.98s)]
*  Enjoying it. [[02:37:19](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9439.06s)]
*  Oh yes. [[02:37:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9440.06s)]
*  Oh yeah. [[02:37:21](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9441.06s)]
*  Anything more. [[02:37:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9442.06s)]
*  Anything more. [[02:37:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9443.06s)]
*  I get maybe the picture I leave you with you mentioned the 20 year old who has grown up [[02:37:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9444.74s)]
*  having had this technology the whole time and having had all the questions answered. [[02:37:30](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9450.619999999999s)]
*  I think there's actually something even deeper. [[02:37:33](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9453.58s)]
*  The AI like the AI that my eight year old is going to have by the time he's 20 it's [[02:37:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9458.26s)]
*  going to have had 12 years of experience with him. [[02:37:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9462.22s)]
*  So it will have grown up with him. [[02:37:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9466.3s)]
*  Be a good life coach. [[02:37:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9467.98s)]
*  Yes. [[02:37:50](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9470.019999999999s)]
*  It will know everything he's ever done. [[02:37:51](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9471.66s)]
*  It will know everything he ever did well. [[02:37:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9474.14s)]
*  It will know everything he did that took real effort. [[02:37:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9475.82s)]
*  It will know what he's good at. [[02:37:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9477.78s)]
*  It will know what he's not good at. [[02:37:58](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9478.78s)]
*  It'll know how to teach him. [[02:38:00](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9480.18s)]
*  It'll know how to correct for his you know whatever limitations he has. [[02:38:02](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9482.26s)]
*  It'll know how to maximize his strengths. [[02:38:06](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9486.06s)]
*  It'll know what he wants. [[02:38:09](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9489.86s)]
*  I wonder if he'll understand how to maximize happiness. [[02:38:10](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9490.86s)]
*  Yeah. [[02:38:14](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9494.06s)]
*  I wonder if I could say Mark you are working too much. [[02:38:15](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9495.06s)]
*  If you just worked one less day a week you'd be 40 percent happier and only 10 percent [[02:38:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9498.660000000002s)]
*  less productive. [[02:38:23](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9503.94s)]
*  Yep. [[02:38:24](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9504.94s)]
*  Well if you're wearing an Apple watch right it will have your pulse and it will have blood [[02:38:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9505.94s)]
*  pressure and it will have all these things. [[02:38:28](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9508.820000000002s)]
*  It'll have you know it'll be able to say you know look when you were in this you know when [[02:38:29](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9509.820000000002s)]
*  you were working on this you were relaxed. [[02:38:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9512.58s)]
*  Your serotonin level you know your serotonin or your whatever oxytocin levels were high. [[02:38:35](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9515.060000000001s)]
*  Serotonin levels are high. [[02:38:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9519.02s)]
*  When you were doing this other thing your cortisol levels were high. [[02:38:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9520.26s)]
*  You shouldn't do that. [[02:38:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9522.380000000001s)]
*  Let's figure out a way to have you not have to go through that again. [[02:38:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9523.380000000001s)]
*  Sure yeah. [[02:38:46](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9526.62s)]
*  Yeah absolutely. [[02:38:47](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9527.7s)]
*  Yeah by the way you know sleep you know you didn't sleep well. [[02:38:48](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9528.7s)]
*  Right. [[02:38:53](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9533.02s)]
*  Right. [[02:38:54](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9534.02s)]
*  So yeah and it'll have yeah it'll have all that right. [[02:38:55](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9535.02s)]
*  So yeah literally they hit college or they hit the workplace and they'll have an ally [[02:38:57](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9537.5s)]
*  right with them right even before there's any sort of actual brain you know without [[02:39:01](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9541.660000000002s)]
*  any mechanical without any you know sort of actual physical hookup they'll have basically [[02:39:05](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9545.34s)]
*  a partner right that'll be with them that whose goal in life will be to make them you [[02:39:08](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9548.900000000001s)]
*  know as happy and satisfied and successful as possible. [[02:39:12](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9552.660000000002s)]
*  Pretty fascinating stuff. [[02:39:16](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9556.02s)]
*  How about that. [[02:39:17](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9557.1s)]
*  Well I'm interested. [[02:39:18](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9558.66s)]
*  I'm going to be paying attention. [[02:39:20](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9560.66s)]
*  I really appreciate you coming in here and explaining a lot of this stuff. [[02:39:22](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9562.34s)]
*  It made me actually feel better. [[02:39:25](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9565.1s)]
*  And it actually gives me hope that there's possibly especially with real open source [[02:39:27](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9567.220000000001s)]
*  a way to avoid the pitfalls of the censorship that seems likely to be at least attempted [[02:39:32](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9572.7s)]
*  to be implemented. [[02:39:38](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9578.62s)]
*  Yeah. [[02:39:39](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9579.62s)]
*  Me too. [[02:39:40](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9580.62s)]
*  All right. [[02:39:41](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9581.62s)]
*  Thank you Mark. [[02:39:42](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9582.62s)]
*  Appreciate you. [[02:39:43](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9583.62s)]
*  Bye everybody. [[02:39:44](https://www.youtube.com/watch?v=8quXLOR_iVE&t=9584.62s)]
