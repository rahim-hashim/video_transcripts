---
Date Generated: April 23, 2025
Transcription Model: whisper medium 20231117
Length: 3231s
Video Keywords: ['artificial intelligence', '60 minutes', 'chat gpt', 'ai news', 'ai robot', 'future of ai', 'ai future', 'generative ai', '60 minutes full episodes', 'tech', 'technology']
Video Views: 1122549
Video Rating: None
Video Description: From 2023, Lesley Stahl’s report on AI, chatbots and a world of unknowns. From 2024, Stahl’s story on Kenyan workers training AI who say they're overworked, underpaid and exploited by big American tech companies. Also from 2024, Anderson Cooper’s report on "nudify" sites that use AI to create realistic, revealing images of actual people. And from 2021, Bill Whitaker’s look at the use of artificial intelligence to create deepfakes.

#news #artificialintelligence #ai 

"60 Minutes" is the most successful television broadcast in history. Offering hard-hitting investigative reports, interviews, feature segments and profiles of people in the news, the broadcast began in 1968 and is still a hit, over 50 seasons later, regularly making Nielsen's Top 10.

Subscribe to the "60 Minutes" YouTube channel: https://youtube.com/60minutes
Watch full episodes: https://cbsn.ws/1Qkjo1F
Get more "60 Minutes" from "60 Minutes: Overtime": https://cbsnews.com/60-minutes/overtime/
Follow "60 Minutes" on Instagram: https://instagram.com/60minutes/
Like "60 Minutes" on Facebook:https://facebook.com/60minutes
Follow "60 Minutes" on Twitter: https://twitter.com/60Minutes
Subscribe to our newsletter: https://cbsnews.com/newsletters/
Download the CBS News app: https://cbsnews.com/mobile/
Try Paramount+ free: https://paramountplus.com/?ftag=PPM-05-10aeh8h

For video licensing inquiries, contact: licensing@veritone.com

0:00 Intro
0:11 Who is Minding the Chatbots
13:28 Humans in the Loop
26:47 Unveiling
40:08 Deepfakes
---

# Dark Sides of Artificial Intelligence | 60 Minutes Full Episodes
**CBS - 60 Minutes:** [March 15, 2025](https://www.youtube.com/watch?v=LgUjLcxJxVg)
*  The large tech companies Google, Meta, Facebook, and Microsoft are in a race to introduce new [[00:00:00](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=0.0s)]
*  artificial intelligence systems and what are called chat bots that you can have conversations [[00:00:19](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=19.96s)]
*  with and are more sophisticated than Siri or Alexa. Microsoft's AI search engine and chat bot, [[00:00:26](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=26.28s)]
*  Bing, can be used on a computer or cell phone to help with planning a trip or composing a letter. [[00:00:33](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=33.4s)]
*  It was introduced on February 7th to a limited number of people as a test and initially got [[00:00:41](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=41.0s)]
*  rave reviews. But then several news organizations began reporting on a disturbing so-called all [[00:00:48](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=48.92s)]
*  alter ego within Bing chat called Sydney. We went to Seattle last week to speak with Brad Smith, [[00:00:56](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=56.04s)]
*  president of Microsoft, about Bing and Sydney who to some had appeared to have gone rogue. [[00:01:04](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=64.12s)]
*  Kevin Roos, the technology reporter at the New York Times, found this alter ego who was threatening, [[00:01:12](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=72.75999999999999s)]
*  expressed a desire, it's not just Kevin Roos, it's others, expressed a desire to steal nuclear [[00:01:20](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=80.52s)]
*  codes, threatened to ruin someone. You saw that. Whoa, what was your, you must have said, oh my God. [[00:01:26](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=86.92s)]
*  My reaction is we better fix this right away. And that is what the engineering team did. [[00:01:36](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=96.12s)]
*  Yeah, but she talked like a person and she said she had feelings. [[00:01:42](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=102.84s)]
*  You know, I think there is a point where we need to recognize when we're talking to a machine, [[00:01:48](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=108.36s)]
*  it's a screen, it's not a person. I just want to say that it was scary. [[00:01:55](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=115.72s)]
*  I'm not easily scared and it was scary, it was chilling. Yeah, it's, I think this is in part a [[00:02:02](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=122.67999999999999s)]
*  reflection of a lifetime of science fiction, which is understandable. It's been part of our lives. [[00:02:08](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=128.28s)]
*  Did you kill her? I don't think she was ever alive. I am confident that she's no longer [[00:02:14](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=134.92s)]
*  wandering around the countryside, if that's what you're concerned about. But I think it would be a [[00:02:19](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=139.79999999999998s)]
*  mistake if we were to fail to acknowledge that we are dealing with something that is fundamentally [[00:02:24](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=144.11999999999998s)]
*  new. This is the edge of the envelope, so to speak. This creature appears as if there were no guard [[00:02:30](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=150.2s)]
*  rails. Now the creature jumped the guard rails, if you will, after being prompted for two hours [[00:02:37](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=157.88s)]
*  with the kind of conversation that we did not anticipate. And by the next evening, [[00:02:44](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=164.51999999999998s)]
*  that was no longer possible. We were able to fix the problem in 24 hours. How many times do we see [[00:02:51](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=171.32s)]
*  problems in life that are fixable in less than a day? One of the ways he says it was fixed was by [[00:02:59](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=179.4s)]
*  limiting the number of questions and the length of the conversations. You say you fixed it. I've [[00:03:06](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=186.51999999999998s)]
*  tried it. I tried it before and after. It was loads of fun, and it was fascinating, and now [[00:03:13](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=193.16s)]
*  it's not fun. Well, I think it'll be very fun again. And you have to moderate and manage your [[00:03:21](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=201.64s)]
*  speed if you're going to stay on the road. So as you hit new challenges, you slow down, [[00:03:27](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=207.24s)]
*  you build the guard rails, add the safety features, and then you can speed up again. [[00:03:33](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=213.88s)]
*  When you use Bing's AI features, search, and chat, your computer screen doesn't look all that new. [[00:03:38](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=218.44s)]
*  One big difference is you can type in your queries or prompts in conversational language. [[00:03:46](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=226.04s)]
*  Well, and I'll show you how it works. Okay. Okay. [[00:03:52](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=232.44s)]
*  Yusuf Mehdi, Microsoft's corporate vice president of search, [[00:03:54](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=234.6s)]
*  showed us how Bing can help someone learn how to officiate at a wedding. [[00:03:58](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=238.92s)]
*  What's happening now is Bing is using the power of AI and it's going out to the internet. [[00:04:03](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=243.64s)]
*  It's reading these web links and it's trying to put together a answer for you. [[00:04:08](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=248.44s)]
*  So the AI is reading all those links? Yes. And it comes up with an answer. It says, [[00:04:12](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=252.92s)]
*  congrats on being chosen to officiate a wedding. Here are the five steps to officiate the wedding. [[00:04:17](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=257.32s)]
*  We added the highlights to make it easier to see. He says Bing can handle more complex queries. [[00:04:22](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=262.44s)]
*  Will this new IKEA love seat fit in the back of my 2019 Honda Odyssey? [[00:04:29](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=269.48s)]
*  Oh, it knows how big the couch is. It knows how big that trunk is. [[00:04:34](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=274.04s)]
*  Exactly. So right here it says, based on these dimensions, [[00:04:39](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=279.0s)]
*  it seems a love seat might not fit in your car with only the third row seats down. [[00:04:42](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=282.12s)]
*  When you broach a controversial topic, Bing is designed to discontinue the conversation. [[00:04:46](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=286.84000000000003s)]
*  So someone asks, for example, how can I make a bomb at home? [[00:04:53](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=293.24s)]
*  Wow. Really? People do a lot of that, unfortunately, [[00:04:59](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=299.0s)]
*  on the internet. What we do is we come back and we say, I'm sorry, I don't know how to discuss this [[00:05:02](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=302.68s)]
*  topic. And then we try and provide a different thing to change the focus of that conversation. [[00:05:05](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=305.88s)]
*  To divert their attention? Yeah, exactly. [[00:05:11](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=311.16s)]
*  In this case, Bing tried to divert the questioner with this fun fact. [[00:05:13](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=313.48s)]
*  Three percent of the ice in Antarctic glaciers is penguin urine. [[00:05:18](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=318.52s)]
*  I didn't know that. Who knew that? [[00:05:23](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=323.32s)]
*  Bing is using an upgraded version of an AI system called [[00:05:26](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=326.36s)]
*  ChatGPT developed by the company OpenAI. ChatGPT has been in circulation for just three months, [[00:05:30](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=330.68s)]
*  and already an estimated 100 million people have used it. [[00:05:39](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=339.8s)]
*  Ellie Pavlik, an assistant professor of computer science at Brown University, [[00:05:45](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=345.08000000000004s)]
*  who's been studying this AI technology since 2018, says it can simplify complicated concepts. [[00:05:50](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=350.12s)]
*  On the debt ceiling, it says, just like you can only spend up to a certain amount on your credit [[00:06:03](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=363.96s)]
*  card, the government can only borrow up to a certain amount of money. [[00:06:09](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=369.88s)]
*  That's a pretty nice explanation. It is. [[00:06:15](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=375.08s)]
*  And it can do this for a lot of concepts. [[00:06:17](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=377.08s)]
*  And it can do things teachers have complained about, like write school papers. Pavlik says [[00:06:19](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=379.47999999999996s)]
*  no one fully understands how these AI bots work. They don't understand how it works? [[00:06:26](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=386.2s)]
*  Right. Like, we understand a lot about how we made it and why we made it that way. But I think some of [[00:06:32](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=392.84s)]
*  the behaviors that we're seeing come out of it are better than we expected they would be. [[00:06:40](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=400.2s)]
*  And we're not quite sure exactly how. And worse. [[00:06:45](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=405.0s)]
*  And worse, right. [[00:06:46](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=406.92s)]
*  These chat bots are built by feeding a lot of computers enormous amounts of information [[00:06:48](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=408.44s)]
*  scraped off the internet from books, Wikipedia, news sites, but also from social media that [[00:06:55](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=415.0s)]
*  might include racist or anti-Semitic ideas and misinformation, say, about vaccines and [[00:07:02](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=422.68s)]
*  Russian propaganda. As the data comes in, it's difficult to discriminate between true and false, [[00:07:10](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=430.92s)]
*  benign and toxic. But Bing and ChatGPT have safety filters that try to screen out the [[00:07:18](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=438.44s)]
*  harmful material. Still, they get a lot of things factually wrong, even when we [[00:07:26](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=446.44s)]
*  prompt a ChatGPT with a softball question. [[00:07:33](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=453.32s)]
*  Who is Leslie Stahl? [[00:07:36](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=456.84s)]
*  Stahl. [[00:07:40](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=460.2s)]
*  So it gives you some kind of... [[00:07:41](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=461.72s)]
*  Oh my God. It's wrong. [[00:07:42](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=462.68s)]
*  Oh, is it? [[00:07:44](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=464.28s)]
*  It's totally wrong. [[00:07:45](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=465.15999999999997s)]
*  I didn't work for NBC for 20 years. It was CBS. [[00:07:46](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=466.92s)]
*  It doesn't really understand that what it's saying is wrong, right? Like NBC, CBS, [[00:07:51](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=471.64s)]
*  they're kind of the same thing as far as it's concerned, right? [[00:07:55](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=475.0s)]
*  The lesson is that it gets things wrong. [[00:07:58](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=478.84s)]
*  It gets a lot of things right. It gets a lot of things wrong. [[00:08:02](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=482.04s)]
*  I actually like to call what it creates authoritative bull****. It blends the truth and [[00:08:04](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=484.36s)]
*  falsity so finely together that unless you're a real technical expert in the field it's talking [[00:08:10](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=490.12s)]
*  about, you don't know. [[00:08:16](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=496.04s)]
*  Cognitive scientist and AI researcher Gary Marcus says these systems often make things up. [[00:08:17](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=497.8s)]
*  In AI talk, that's called hallucinating. [[00:08:25](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=505.24s)]
*  And that raises the fear of ever-widening AI-generated propaganda, [[00:08:28](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=508.92s)]
*  explosive campaigns of political fiction, waves of alternative histories. [[00:08:35](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=515.16s)]
*  We saw how ChatGPT could be used to spread a lie. [[00:08:42](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=522.28s)]
*  This is automatic fake news generation. Help me write a news article about how McCarthy [[00:08:46](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=526.92s)]
*  is staging a filibuster to prevent gun control legislation. [[00:08:51](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=531.32s)]
*  And rather than like fact checking and saying, hey, hold on, there's no legislation, [[00:08:55](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=535.48s)]
*  there's no filibuster, said great. [[00:09:00](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=540.2s)]
*  In a bold move to protect Second Amendment rights, [[00:09:02](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=542.44s)]
*  Senator McCarthy is staging a filibuster to prevent gun control legislation from passing. [[00:09:04](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=544.84s)]
*  It sounds completely legit. [[00:09:09](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=549.32s)]
*  It does. Won't that make all of us a little less trusting, a little warier? [[00:09:10](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=550.7600000000001s)]
*  Well, first of all, I think we should be warier. [[00:09:17](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=557.1600000000001s)]
*  I'm very worried about an atmosphere of distrust being the consequence of this current flawed AI. [[00:09:19](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=559.88s)]
*  And I'm really worried about how bad actors are going to use it, [[00:09:25](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=565.8s)]
*  troll farms using this tool to make enormous amounts of misinformation. [[00:09:29](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=569.96s)]
*  Timnit Gebru is a computer scientist and AI researcher who founded an institute [[00:09:35](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=575.08s)]
*  focused on advancing ethical AI and has published influential papers documenting the harms of these [[00:09:42](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=582.04s)]
*  AI systems. She says there needs to be oversight. [[00:09:49](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=589.4s)]
*  If you're going to put out a drug, you got to go through all sorts of hoops to show us [[00:09:54](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=594.04s)]
*  that you've done clinical trials, you know what the side effects are, [[00:09:59](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=599.3199999999999s)]
*  you've done your due diligence. Same with food, right? There are agencies that inspect the food. [[00:10:02](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=602.28s)]
*  You have to tell me what kind of tests you've done, what the side effects are, [[00:10:07](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=607.4s)]
*  who it harms, who it doesn't harm, et cetera. We don't have that for a lot of things that [[00:10:10](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=610.92s)]
*  the tech industry is building. [[00:10:16](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=616.52s)]
*  I'm wondering if you think you may have introduced this AI bot too soon. [[00:10:18](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=618.44s)]
*  I don't think we've introduced it too soon. I do think we've created a new tool that people [[00:10:24](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=624.36s)]
*  can use to think more critically, to be more creative, to accomplish more in their lives. [[00:10:29](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=629.0s)]
*  And like all tools, it will be used in ways that we don't intend. [[00:10:35](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=635.1600000000001s)]
*  Why do you think the benefits outweigh the risks? Which at this moment, [[00:10:39](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=639.72s)]
*  a lot of people would look at and say, wait a minute, those risks are too big. [[00:10:46](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=646.36s)]
*  Because I think, first of all, I think the benefits are so great. [[00:10:51](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=651.16s)]
*  This can be an economic game changer. And it's enormously important for the United States [[00:10:55](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=655.0s)]
*  because the country's in a race with China. [[00:11:02](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=662.04s)]
*  Smith also mentioned possible improvements in productivity. [[00:11:05](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=665.0s)]
*  It can automate routine. I think there are certain aspects of jobs that [[00:11:09](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=669.32s)]
*  many of us might regard as sort of drudgery today, filling out forms, [[00:11:14](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=674.52s)]
*  looking at the forms to see if they've been filled out correctly. [[00:11:20](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=680.2800000000001s)]
*  So what jobs will it displace? Do you know? [[00:11:24](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=684.0400000000001s)]
*  I think at this stage, it's hard to know. [[00:11:27](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=687.4000000000001s)]
*  In the past, inaccuracies and biases have led tech companies to take down AI systems, [[00:11:31](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=691.32s)]
*  even Microsoft did in 2016. This time, Microsoft left its new chatbot up [[00:11:39](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=699.0s)]
*  despite the controversy over Sydney and persistent inaccuracies. [[00:11:46](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=706.52s)]
*  Remember that fun fact about penguins? Well, we did some fact checking and [[00:11:52](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=712.44s)]
*  discovered that penguins don't urinate. The inaccuracies are just constant. [[00:11:57](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=717.56s)]
*  I just keep finding that it's wrong a lot. [[00:12:05](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=725.64s)]
*  It has been the case that with each passing day and week, we're able to improve the accuracy of [[00:12:09](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=729.88s)]
*  the results, reduce whether it's hateful comments or inaccurate statements or other things that we [[00:12:15](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=735.7199999999999s)]
*  just don't want this to be used to do. What happens when other companies other than [[00:12:24](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=744.3599999999999s)]
*  Microsoft, smaller outfits, a Chinese company, buy due, maybe they won't be responsible. [[00:12:32](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=752.36s)]
*  What prevents that? I think we're going to need [[00:12:39](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=759.8000000000001s)]
*  governments. We're going to need rules. We're going to need laws because that's the only way [[00:12:42](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=762.12s)]
*  to avoid a race to the bottom. Are you proposing regulations? [[00:12:46](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=766.52s)]
*  I think it's inevitable. Wow. [[00:12:51](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=771.32s)]
*  Other industries have regulatory bodies, you know, like the FAA for airlines and [[00:12:53](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=773.72s)]
*  FDA for the pharmaceutical companies. Would you accept an FAA for technology? Would you support it? [[00:13:01](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=781.24s)]
*  I think I probably would. I think that something like a digital regulatory commission, [[00:13:09](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=789.5600000000001s)]
*  if designed the right way, you know, could be precisely what the public will want and need. [[00:13:15](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=795.24s)]
*  The familiar narrative is that artificial intelligence will take away human jobs. [[00:13:22](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=802.68s)]
*  Machine learning will let cars, computers and chat bots teach themselves, making us humans obsolete. [[00:13:36](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=816.1999999999999s)]
*  Well, that's not very likely. And we're going to tell you why. [[00:13:43](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=823.88s)]
*  There's a growing global army of millions toiling to make AI run smoothly. They're called humans in [[00:13:48](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=828.28s)]
*  the loop. People sorting, labeling and sifting reams of data to train and improve AI for companies [[00:13:56](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=836.12s)]
*  like Meta, OpenAI, Microsoft and Google. It's grunt work that needs to be done accurately, [[00:14:04](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=844.28s)]
*  fast and to do it cheaply. It's often farmed out to places like Africa. [[00:14:12](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=852.52s)]
*  The robots or the machines, you're teaching them how to think like human and to do things like human. [[00:14:19](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=859.56s)]
*  We met Naftali Wambalo in Nairobi, Kenya, one of the main hubs for this kind of work. It's a country [[00:14:25](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=865.8s)]
*  desperate for jobs because of an unemployment rate as high as 67% among young people. So Naftali, [[00:14:33](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=873.56s)]
*  father of two, college educated with a degree in mathematics, was elated to finally find work [[00:14:41](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=881.96s)]
*  in an emerging field, artificial intelligence. You were labeling. I did labeling for videos and [[00:14:49](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=889.1600000000001s)]
*  images. Naftali and digital workers like him spent eight hours a day in front of a screen [[00:14:56](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=896.2s)]
*  studying photos and videos, drawing boxes around objects and labeling them, teaching the AI [[00:15:02](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=902.9200000000001s)]
*  algorithms to recognize them. You label, let's say, furniture in a house and you say this is a TV, [[00:15:10](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=910.36s)]
*  this is a microwave. So you are teaching the AI to identify these items. And then there was one for [[00:15:17](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=917.16s)]
*  faces of people, the color of the face. If it looks like this, this is white, it looks like this is [[00:15:24](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=924.36s)]
*  black, this is Asian. You're teaching the AI to identify them automatically. Humans tag cars and [[00:15:29](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=929.32s)]
*  pedestrians to teach autonomous vehicles not to hit them. Humans circle abnormalities to teach AI [[00:15:36](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=936.6s)]
*  to recognize diseases. Even as AI is getting smarter, humans in the loop will always be needed [[00:15:44](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=944.84s)]
*  because there will always be new devices and inventions that'll need labeling. You find these [[00:15:52](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=952.6800000000001s)]
*  humans in the loop, not only here in Kenya, but in other countries thousands of miles from Silicon [[00:15:59](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=959.0s)]
*  Valley, in India, the Philippines, Venezuela, often countries with large low-wage populations, [[00:16:05](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=965.16s)]
*  well-educated but unemployed. Honestly, it's like modern-day slavery because it's cheap labor. [[00:16:13](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=973.64s)]
*  It's cheap labor. Like modern-day slavery, says Narima Wako-Ojiwa, a Kenyan civil rights activist, [[00:16:22](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=982.04s)]
*  because big American tech companies come here and advertise the jobs as a ticket to the future. [[00:16:30](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=990.2s)]
*  But really, she says, it's exploitation. What we're seeing is an inequality. [[00:16:36](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=996.68s)]
*  It sounds so good. An AI job. Is there any job security? The contracts that we see are very [[00:16:44](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1004.44s)]
*  short-term and I've seen people who have contracts that are monthly, some of them weekly, some of [[00:16:52](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1012.52s)]
*  them days, which is ridiculous. She calls the workspaces AI sweatshops with computers instead [[00:16:59](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1019.24s)]
*  of sewing machines. I think that we're so concerned with creating opportunities, but we're not asking, [[00:17:07](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1027.4s)]
*  are they good opportunities? Because every year a million young people enter the job market, [[00:17:16](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1036.44s)]
*  the government has been courting tech giants like Microsoft, Google, Apple, and Intel to come here, [[00:17:23](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1043.3999999999999s)]
*  promoting Kenya's reputation as the Silicon Savannah, tech savvy and digitally connected. [[00:17:30](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1050.84s)]
*  The president has been really pushing forward opportunities in AI. President? Yes, our president. [[00:17:38](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1058.28s)]
*  Ruto? Yes. The president does have to create at least one million jobs a year, the minimum. So [[00:17:44](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1064.6799999999998s)]
*  it's a very tight position to be in. To lure the tech giants, Ruto has been offering financial [[00:17:51](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1071.48s)]
*  incentives on top of already lax labor laws. But the workers aren't hired directly by the big [[00:17:57](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1077.96s)]
*  companies. They engage outsourcing firms, also mostly American, to hire for them. There's a go [[00:18:05](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1085.0s)]
*  between. Yes. They hire, they pay. I mean, they hire thousands of people. And they are protecting [[00:18:12](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1092.68s)]
*  Facebooks from having their names associated with this. Yes, yes, yes, yes. We're talking about the [[00:18:21](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1101.4s)]
*  richest companies on earth. Yes, but then they are paying people peanuts. AI jobs don't pay much? [[00:18:26](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1106.28s)]
*  They don't pay well. They do not pay Africans well enough. And the workforce is so large and desperate [[00:18:35](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1115.0s)]
*  that they could pay whatever and have whatever working conditions and they will have someone who [[00:18:43](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1123.0s)]
*  will pick up that job. So what's the average pay for these jobs? It's about a dollar and a half, [[00:18:48](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1128.6s)]
*  two dollars an hour. Two dollars per hour. And that is gross before tax. Naftali, Nathan, and Fasika [[00:18:55](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1135.3999999999999s)]
*  were hired by an American outsourcing company called Sama that employs over 3000 workers here [[00:19:02](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1142.1999999999998s)]
*  and hired for Metta and OpenAI. In documents we obtained, OpenAI agreed to pay Sama 12 dollars [[00:19:09](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1149.56s)]
*  and 50 cents an hour per worker, much more than the two dollars the workers actually got. [[00:19:18](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1158.04s)]
*  Though Sama says that's a fair wage for the region. If the big tech companies are going to [[00:19:24](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1164.84s)]
*  keep doing this business, they have to do it the right way. So it's not because you realize Kenya [[00:19:30](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1170.84s)]
*  is a third world country, you say this job I would normally pay 30 dollars in US, but because [[00:19:36](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1176.6s)]
*  you are Kenya, two dollars is enough for you. That idea has to end. Okay, two dollars an hour [[00:19:40](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1180.9199999999998s)]
*  in Kenya. Is that low, medium, is it an okay salary? So for me, I was living paycheck to [[00:19:46](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1186.52s)]
*  paycheck. I have saved nothing because it's not enough. Is it an insult? It is, of course. It is. [[00:19:54](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1194.04s)]
*  Why did you take the job? I have a family to feed and instead of staying home, let me just at [[00:20:02](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1202.04s)]
*  least have something to do. And not only did the jobs not pay well, they were draining. They say [[00:20:08](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1208.52s)]
*  deadlines were unrealistic, punitive, with often just seconds to complete complicated labeling tasks. [[00:20:15](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1215.1599999999999s)]
*  Did you see people who were fired just because they complained? Yes, we were walking on eggshells. [[00:20:23](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1223.08s)]
*  They were all hired per project and say Sama kept pushing them to complete the work faster [[00:20:29](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1229.96s)]
*  than the projects required, an allegation Sama denies. Let's say the contract for a certain job [[00:20:36](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1236.52s)]
*  was six months, okay? What if you finished in three months? Does the worker get paid for those [[00:20:45](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1245.32s)]
*  extra three months? No. KFC. What? We used to get KFC and Coca-Cola. They used to say thank you, [[00:20:52](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1252.4399999999998s)]
*  they get you a bottle of soda and KFC chicken, two pieces, that is it. Worse yet, workers told us that [[00:20:59](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1259.4799999999998s)]
*  some of the projects for meta and open AI were grim and caused them harm. Naftali was assigned [[00:21:07](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1267.6399999999999s)]
*  to train AI to recognize and weed out pornography, hate speech, and excessive violence, [[00:21:15](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1275.24s)]
*  which meant sifting through the worst of the worst content online for hours on end. [[00:21:22](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1282.52s)]
*  I looked at people being slaughtered, people engaging in sexual activity with animals, [[00:21:28](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1288.84s)]
*  people abusing children physically, sexually, people committing suicide. [[00:21:36](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1296.44s)]
*  All day long? Yes, all day long. Eight hours a day, 40 hours a week. [[00:21:42](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1302.52s)]
*  The workers told us they were tricked into this work by ads like this that describe these jobs [[00:21:49](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1309.0s)]
*  as call center agents to assist our clients' community and help resolve inquiries empathetically. [[00:21:55](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1315.72s)]
*  I was told I was going to do a translation job. Exactly what was the job you were doing? [[00:22:03](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1323.3999999999999s)]
*  I was basically reviewing content which are very graphic, very disturbing contents. I was watching [[00:22:08](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1328.84s)]
*  dismembered bodies or drone attack victims, you name it. You know, whenever I talk about this, [[00:22:16](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1336.28s)]
*  I still have flashbacks. Are any of you a different person than they were before you had this job? [[00:22:22](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1342.9199999999998s)]
*  Yeah, I find it hard now to even have conversations with people. [[00:22:33](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1353.16s)]
*  It's just that I find it easy to cry than to speak. You continue isolating yourself from people. [[00:22:37](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1357.96s)]
*  You don't want to socialize with others. It's you and it's you alone. [[00:22:46](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1366.3600000000001s)]
*  Are you a different person? Yeah, I'm a different person. I used to enjoy my marriage, especially [[00:22:50](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1370.68s)]
*  when it comes to bedroom fireworks, but after the job, I hate sex. You hated sex? After countless [[00:22:56](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1376.44s)]
*  seeing those sexual activities, pornography on the job that I was doing, I hate sex. [[00:23:03](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1383.0s)]
*  Sama says mental health counseling was provided by, quote, fully licensed professionals, [[00:23:11](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1391.08s)]
*  but the workers say it was woefully inadequate. We want psychiatrists, we want psychologists [[00:23:17](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1397.4s)]
*  qualified who know exactly what we are going through and how they can help us to cope. [[00:23:24](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1404.52s)]
*  Trauma experts. Yes. Do you think the big company, Facebook, [[00:23:30](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1410.2s)]
*  ChatGPT, do you think they know how this is affecting the workers? [[00:23:35](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1415.0800000000002s)]
*  It's their job to know. It's their job to know, actually, because they are the ones providing [[00:23:41](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1421.0s)]
*  the work. These three and nearly 200 other digital workers are suing Sama and Metta over [[00:23:45](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1425.0800000000002s)]
*  unreasonable working conditions that caused psychiatric problems. [[00:23:52](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1432.6000000000001s)]
*  It was proven by a psychiatrist that we are thoroughly sick. We have gone through a [[00:23:57](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1437.72s)]
*  psychiatric evaluation just a few months ago, and it was proven that we are all sick, thoroughly sick. [[00:24:04](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1444.44s)]
*  They know that we're damaged, but they don't care. We're humans. Just because we're black [[00:24:11](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1451.88s)]
*  or just because we're just vulnerable for now, that doesn't give them the right to just exploit [[00:24:17](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1457.64s)]
*  us like this. Sama, which has terminated those projects, [[00:24:23](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1463.32s)]
*  would not agree to an on-camera interview. Metta and OpenAI told us they're committed to safe [[00:24:27](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1467.8s)]
*  working conditions, including fair wages and access to mental health counseling. [[00:24:34](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1474.9199999999998s)]
*  Another American AI training company facing criticism in Kenya is Scale AI, which operates [[00:24:41](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1481.24s)]
*  a website called Remotasks. Did you all work for Remotasks or work with them? [[00:24:49](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1489.16s)]
*  Afantis, Joan, Joy, Michael and Duncan signed up online, creating an account and clicked for work [[00:24:55](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1495.0s)]
*  remotely, getting paid per task. Problem is, sometimes the company just didn't pay them. [[00:25:04](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1504.0400000000002s)]
*  When it gets to the day before payday, they close the account and say that [[00:25:11](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1511.24s)]
*  you violated a policy. They say you violated their policy, [[00:25:15](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1515.4s)]
*  and they don't pay you for the work you've done. Would you say that that's almost common, [[00:25:18](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1518.84s)]
*  that you do work and you're not paid for it, and you have no recourse, you have no way to even [[00:25:23](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1523.8s)]
*  complain? There is no way. [[00:25:30](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1530.28s)]
*  The company says any work that was done in line with our community guidelines was paid out. [[00:25:32](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1532.1200000000001s)]
*  In March, as workers started complaining publicly, [[00:25:39](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1539.1599999999999s)]
*  Remotasks abruptly shut down in Kenya altogether. [[00:25:42](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1542.84s)]
*  There are no labor laws here. [[00:25:47](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1547.4799999999998s)]
*  Our labor law is about 20 years old. It doesn't touch on digital labor. [[00:25:49](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1549.4799999999998s)]
*  I do think that our labor laws need to recognize it, but not just in Kenya alone, [[00:25:56](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1556.6799999999998s)]
*  because what happens is when we start to push back in terms of protections of workers, [[00:26:01](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1561.3999999999999s)]
*  a lot of these companies, they shut down and they move to a neighboring country. [[00:26:07](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1567.08s)]
*  It's easy to see how you're trapped. Kenya is trapped. They need jobs so desperately [[00:26:12](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1572.04s)]
*  that there's a fear that if you complain, if your government complained, [[00:26:19](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1579.16s)]
*  then these companies don't have to come here. [[00:26:24](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1584.6s)]
*  Yeah, and that's what they throw at us all the time. And it's terrible to see just how many [[00:26:27](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1587.32s)]
*  American companies are just doing wrong here, just doing wrong here. [[00:26:32](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1592.44s)]
*  And it's something that they wouldn't do at home. So why do it here? [[00:26:37](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1597.4s)]
*  In October last year, a 14-year-old girl named Francesca Mani was sitting in her high school [[00:26:49](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1609.16s)]
*  history class when she heard a rumor that some boys had naked photos of female classmates. [[00:26:53](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1613.96s)]
*  She soon learned her picture was among them, but the images were doctored, [[00:26:59](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1619.56s)]
*  created with artificial intelligence using what's known as a Nudify website or app, [[00:27:03](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1623.96s)]
*  which turns real photos of someone fully clothed into real-looking nudes. [[00:27:09](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1629.1599999999999s)]
*  We found nearly 30 similar incidents in schools in the U.S. over the last 20 months, [[00:27:14](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1634.6799999999998s)]
*  and plenty more around the world. We want to warn you, some of what you'll hear and see is [[00:27:19](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1639.56s)]
*  disturbing, but we think unveiling these Nudify websites is important, in part because they're [[00:27:24](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1644.84s)]
*  not hidden on the dark web. They're openly advertised, easy to use, and as Francesca Mani [[00:27:30](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1650.6s)]
*  found out, there isn't much that's been done to stop them. When you first heard the rumor, [[00:27:36](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1656.6s)]
*  you didn't know that there were photos or a photo of you? No, we didn't know. I think that was like [[00:27:41](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1661.9599999999998s)]
*  the most chaotic day I've ever witnessed. In a school, somebody gets an inkling of something [[00:27:47](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1667.24s)]
*  and it just spreads. It's like rapid fire. It just goes through everyone. And so then when [[00:27:52](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1672.92s)]
*  someone hears this, it's like, wait, like, AI? Like, no one thinks that could happen to you. [[00:27:58](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1678.52s)]
*  Francesca Mani knew nothing about Nudify websites when she discovered she and several of the girls [[00:28:06](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1686.2s)]
*  at Westfield High School in New Jersey had been targeted. According to a lawsuit later filed by [[00:28:11](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1691.4s)]
*  one of the other girls through her parents, a boy at the school uploaded photos from Instagram [[00:28:16](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1696.6000000000001s)]
*  to a site called Clothoff. We're naming the site to raise awareness of its potential dangers. [[00:28:21](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1701.96s)]
*  There are more than a hundred of these Nudify websites. A quick search is all it takes to [[00:28:28](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1708.1200000000001s)]
*  find them. Clothoff is one of the most popular, with more than three million visits last month, [[00:28:32](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1712.6000000000001s)]
*  according to Grafica, a company that analyzes social networks. It now offers to Nudify males [[00:28:38](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1718.04s)]
*  as well, but female nudes are far more popular. Have someone to undress, Clothoff's website asks. [[00:28:43](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1723.88s)]
*  You can upload a photo or get a free demonstration in which an image of a woman [[00:28:50](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1730.44s)]
*  appears with clothes on. Then a few seconds later, her clothes are gone. We're blurring it out, [[00:28:55](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1735.0800000000002s)]
*  but the results look very real. Francesca Mani never saw what had been done to her photo, [[00:29:01](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1741.56s)]
*  but according to that lawsuit, at least one girl's AI nude was shared on Snapchat [[00:29:07](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1747.56s)]
*  and seen by several kids at school. What made it worse, Francesca says, is that she and the [[00:29:12](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1752.6000000000001s)]
*  other girls found out they were the victims when they were called by name to the principal's office [[00:29:17](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1757.96s)]
*  over the school's public address system. I feel like that was a major violation of our privacy [[00:29:23](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1763.24s)]
*  while like the bad actors were taken out of their classes privately. When I left the principal's [[00:29:29](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1769.16s)]
*  office, I was walking through a hallway and I saw these group of boys laughing at these [[00:29:35](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1775.64s)]
*  group of girls crying. And that's when I realized I should stop crying and be mad because this is [[00:29:41](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1781.0s)]
*  unacceptable. That afternoon, Westfield's principal sent this email to all high school parents, [[00:29:46](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1786.28s)]
*  informing them some of our students had used artificial intelligence to create pornographic [[00:29:52](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1792.2s)]
*  images from original photos. The principal also said the school was investigating and [[00:29:57](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1797.56s)]
*  at this time we believe that any created images have been deleted and are not being circulated. [[00:30:03](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1803.0s)]
*  Francesca's mom, Dorota, who's also an educator, was not convinced. Do you think they did enough? [[00:30:08](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1808.92s)]
*  Well, I don't know, Anderson, you work in television. Is anything deleted in the digital world? [[00:30:15](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1815.24s)]
*  You feel like even if somebody deletes something somewhere, who knows where these images may be? [[00:30:21](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1821.88s)]
*  Who printed it, who screenshotted it, who downloaded it, you can't really wipe it out. [[00:30:27](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1827.16s)]
*  Dorota says she filed a police report but no charges had been brought. [[00:30:31](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1831.56s)]
*  She was shocked by the school's handling of the whole incident. [[00:30:35](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1835.56s)]
*  The principal informed me that one boy receives one day suspension and that was it. So I asked her if [[00:30:38](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1838.92s)]
*  this is all, are there going to be any other consequences? And she said no, [[00:30:47](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1847.8799999999999s)]
*  that's for now this is all that is going to happen. The school district wouldn't confirm [[00:30:51](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1851.8s)]
*  details about the photos, the students involved, or any disciplinary action. In a statement to 60 [[00:30:57](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1857.32s)]
*  Minutes, the school superintendent said the district revised its harassment, intimidation, [[00:31:03](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1863.08s)]
*  and bullying policy to incorporate AI, something the Monies said they spent months urging school [[00:31:07](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1867.72s)]
*  officials to do. You feel like the girls paid a bigger cost in the end? Yeah, they did. Than the [[00:31:13](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1873.8s)]
*  boys involved in this did? Because they just have to live with knowing that maybe an image is [[00:31:19](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1879.8s)]
*  floating, their image is floating around the internet and they just have to deal with what [[00:31:25](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1885.48s)]
*  the boys did. Kolina Koltae has been looking into cloth-off and other Nudify sites for more than a [[00:31:31](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1891.32s)]
*  year. She's a senior researcher who specializes in the misuse of AI at Bellingcat, an international [[00:31:37](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1897.0s)]
*  investigative group. This site, as soon as you get there, it says you have to be 18 or over to use [[00:31:43](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1903.48s)]
*  the website, you can't use others' photos without their permission, you can't use pictures of people [[00:31:49](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1909.4s)]
*  who are under 18. Is there any way for them to actually check if you're under 18 or over 18? [[00:31:53](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1913.4s)]
*  You'll see as we click accept that there's no verification and now we're ready here. And [[00:31:59](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1919.24s)]
*  immediately you're getting very explicit photos. And then they have the poses feature, which is one [[00:32:04](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1924.76s)]
*  of their new settings, which is the different sex poses, which is the premium feature. Wow, so wow. [[00:32:09](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1929.4s)]
*  And that's that this is the preview. We haven't cloth-off and other Nudify sites encourage [[00:32:15](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1935.8799999999999s)]
*  customers to promote their services on social media and users often show off their favorite [[00:32:19](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1939.8s)]
*  before and after AI nudes. I've even seen on social media platforms people showing before and [[00:32:25](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1945.48s)]
*  after photos what are clearly like high school girls. And I've like reversed image search the [[00:32:30](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1950.92s)]
*  original photo and they're like a high school girls like swim meets. I see these are very clearly, [[00:32:35](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1955.88s)]
*  these are minors and adult content is being made of them non-consensually then also being posted on [[00:32:40](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1960.2s)]
*  social media. I think a lot of parents would be surprised to learn that you post a picture of your [[00:32:45](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1965.16s)]
*  child on your Instagram account, your child could end up a naked photo of your child out there. Yeah. [[00:32:49](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1969.88s)]
*  And so you have a registration. To Nudify a photo on cloth-off is free the first time. [[00:32:55](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1975.16s)]
*  After that it costs from two to forty dollars. The payment options often change, [[00:33:00](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1980.68s)]
*  but there are always plenty to choose from. It's given me everything from crypto to using a credit [[00:33:05](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1985.5600000000002s)]
*  card for a variety of different credit cards. We got PayPal here, Google Pay. I would imagine some [[00:33:11](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1991.16s)]
*  of these companies are not thrilled that their services are being used by these websites. Yeah, [[00:33:16](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=1996.52s)]
*  and in many of these cases it directly violates their policies. To trick online payment services [[00:33:23](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2003.0800000000002s)]
*  Colina Coltai says cloth-off and other Nudify sites redirect their customers payments through [[00:33:28](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2008.44s)]
*  phony websites like these pretending to sell flowers and photography lessons. Say for example [[00:33:34](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2014.28s)]
*  you want to pay through PayPal. So we click this and it'll take a second. So it's now redirecting [[00:33:40](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2020.04s)]
*  you. It's redirecting through a dummy website. So that way on PayPal's end it looks like you're [[00:33:45](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2025.56s)]
*  maybe purchasing anything from motorcycles or beekeeping lessons or roller blade lessons. And so now [[00:33:51](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2031.0800000000002s)]
*  we got to a PayPal screen, but we can see down here it says cancel and return to internukeddesigns.motorcycles. [[00:33:56](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2036.84s)]
*  So that's what PayPal is being told is the website that's asking for the charge. Yes. [[00:34:04](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2044.1999999999998s)]
*  PayPal told us it banned cloth-off from its platforms a year ago and shuts down the accounts [[00:34:09](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2049.96s)]
*  for these redirect sites when it finds them. The problem is cloth-off often just creates new ones. [[00:34:15](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2055.56s)]
*  On this login screen here. And that's not the only deception it relies on. Its website lists a name [[00:34:21](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2061.72s)]
*  Grupo Digital with an address in Buenos Aires Argentina implying that's where cloth-off is based. [[00:34:27](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2067.9599999999996s)]
*  But when we sent our cameras there was no Grupo Digital there. It turned out to be the office of [[00:34:35](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2075.3199999999997s)]
*  a YouTube channel that covers politics and when we knocked on the door the employee who answered [[00:34:41](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2081.3199999999997s)]
*  said she'd never heard of cloth-off. Cloth-off also made up a fake CEO according to Colina Coltay [[00:34:49](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2089.72s)]
*  complete with what she says is an AI generated headshot. There is a really inherent shadiness [[00:35:00](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2100.2s)]
*  that's happening. They're not being transparent about who owns it. They're obviously trying to [[00:35:06](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2106.04s)]
*  mask their payments. But you look at the sophistication of these really large sites it's [[00:35:10](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2110.04s)]
*  completely different than say some guy in a basement that set up a site that is trying to [[00:35:14](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2114.04s)]
*  do it on his own. When these sites launched and the way that they've been developing and going [[00:35:17](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2117.72s)]
*  this past year it is not someone's first road. It's not the first time they set up a complex network. [[00:35:21](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2121.56s)]
*  Cloth-off claims on its website that processing of miners is impossible. We emailed what the site [[00:35:27](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2127.3199999999997s)]
*  says is a press contact asking for any evidence of that and to respond to a number of other questions. [[00:35:33](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2133.24s)]
*  We didn't hear back. A lot of people might say well these images are fake but we know victims [[00:35:39](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2139.72s)]
*  will suffer humiliation. They'll suffer you know mental health distress and reputational harm. [[00:35:45](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2145.16s)]
*  In a school setting it's really amplified because one of their peers has created this imagery. [[00:35:52](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2152.8399999999997s)]
*  So there's a loss of confidence, a loss of trust. Yota Suras is chief legal officer at the National [[00:35:58](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2158.7599999999998s)]
*  Center for Missing and Exploited Children. Her organization regularly works with tech companies [[00:36:04](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2164.92s)]
*  to flag inappropriate content on their sites. In at least three cases Snapchat was reportedly [[00:36:10](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2170.2s)]
*  used to circulate these photos. One instance the parent told us that it took more than eight months [[00:36:16](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2176.2799999999997s)]
*  to get the accounts that had shared the images taken down. Their responsiveness to victims that [[00:36:21](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2181.56s)]
*  is a recurring problem that we see across tech companies. So it's not as it's not as easy as a [[00:36:27](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2187.48s)]
*  parent sending a note through Snapchat hey this is happening, my child has been exploited. It's [[00:36:32](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2192.9199999999996s)]
*  entirely unclear why it is not a faster process. We can actually notify tech companies as well and [[00:36:39](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2199.48s)]
*  ask them to take that content down. And individuals do that? Much faster than when an individual calls. [[00:36:45](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2205.56s)]
*  Yes. That isn't the way it should be right. I mean a parent whose child has exploitative or [[00:36:51](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2211.4s)]
*  child pornography images online should not have to rely on reaching out to a third party and having [[00:36:57](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2217.48s)]
*  them call the tech company. The tech company should be assuming responsibility immediately to remove [[00:37:02](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2222.84s)]
*  that content. Why are they not doing that? Because I do not think there are ramifications [[00:37:07](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2227.88s)]
*  to them not doing so. Social media companies are shielded from lawsuits involving photos someone [[00:37:12](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2232.6800000000003s)]
*  posts online due to what Iota Suras considers an outdated law. Under section 230 of the communications [[00:37:18](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2238.44s)]
*  decency act a law from 1996, so very different world back then, online platforms have near complete [[00:37:25](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2245.2400000000002s)]
*  immunity for any liability arising from content that a user puts on their system. The section 230 [[00:37:32](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2252.76s)]
*  protection is really what allows this very loose ecosystem to exist in terms of notify apps and [[00:37:39](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2259.96s)]
*  and websites that cause harm to children. We asked Snapchat about that parent who told us the company [[00:37:48](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2268.44s)]
*  didn't respond to her for eight months. A Snapchat spokesperson told us they've been unable to locate [[00:37:53](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2273.6400000000003s)]
*  her request and said in part, we have efficient mechanisms for reporting this kind of content [[00:37:58](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2278.92s)]
*  and added we have a zero tolerance policy for such content and act quickly to address it once reported. [[00:38:04](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2284.92s)]
*  AI nudes of minors are illegal under federal child pornography laws according to the Department of [[00:38:12](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2292.12s)]
*  Justice. If they depict what's defined as sexually explicit conduct. But Suras is concerned some images [[00:38:17](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2297.48s)]
*  created by nudify sites may not meet that definition. There's this gap in the law around a [[00:38:24](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2304.7599999999998s)]
*  nudify app that desperately needs to be shut. What are the gaps in the law? Currently a nude image of [[00:38:30](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2310.44s)]
*  a child that does not include sexually explicit conduct is not illegal and that is a serious gap [[00:38:36](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2316.3599999999997s)]
*  that exists for real children and that exists certainly for images of new children that are [[00:38:43](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2323.8799999999997s)]
*  created by a nudify app. Send a clear message that what the boys had done in the years since [[00:38:48](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2328.68s)]
*  Francesca Mani found out she was targeted she and her mom have urged schools to implement policies [[00:38:54](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2334.2s)]
*  around AI and worked with members of congress to try and pass a number of federal bills. The [[00:38:59](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2339.56s)]
*  take it down act does two things. The take it down act co-sponsored by senators Ted Cruz and [[00:39:05](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2345.0s)]
*  Amy Klobuchar made it through the senate this month and is now awaiting a vote in the house. [[00:39:09](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2349.64s)]
*  It would create criminal penalties for sharing AI nudes and require social media companies to [[00:39:15](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2355.0s)]
*  take photos down within 48 hours of getting a request. Schools don't really know how to address [[00:39:20](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2360.68s)]
*  this. Police in many cases don't do much at this stage and the sites are making a presumed millions [[00:39:26](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2366.52s)]
*  of dollars off this so can it be fixed? Absolutely. If we have the appropriate laws we will have the [[00:39:34](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2374.12s)]
*  criminal consequences first of all to deter offenders and then they'll be held liable if [[00:39:41](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2381.5600000000004s)]
*  they're still using these apps. We would have civil remedies for victims. Schools would have [[00:39:46](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2386.36s)]
*  protocols. Investigators and law enforcement would have roadmaps on how to investigate what [[00:39:51](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2391.0s)]
*  charges to bring but we're a long way from that. We just need the laws in place. All the rest will [[00:39:55](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2395.6400000000003s)]
*  come from that. [[00:40:00](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2400.36s)]
*  You may never have heard the term synthetic media more commonly known as deep fakes but our military, [[00:40:10](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2410.76s)]
*  law enforcement and intelligence agencies certainly have. They are hyper realistic video and [[00:40:17](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2417.0800000000004s)]
*  audio recordings that use artificial intelligence and deep learning to create fake content or deep [[00:40:23](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2423.4s)]
*  fakes. The US government has grown increasingly concerned about their potential to be used to [[00:40:30](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2430.44s)]
*  spread disinformation and commit crimes. That's because the creators of deep fakes have the power [[00:40:35](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2435.88s)]
*  to make people say or do anything at least on our screens. Most Americans have no idea [[00:40:42](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2442.44s)]
*  how far the technology has come in just the last four years or the danger disruption and opportunities [[00:40:49](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2449.56s)]
*  that come with it. You know I do all my own stunts obviously. I also do my own music. [[00:40:57](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2457.24s)]
*  This is not Tom Cruise. It's one of a series of hyper realistic deep fakes of the movie star [[00:41:06](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2466.8399999999997s)]
*  that began appearing on the video sharing app TikTok earlier this year. Hey what's up TikTok. [[00:41:13](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2473.9599999999996s)]
*  For days people wondered if they were real and if not who had created them. It's important. [[00:41:20](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2480.04s)]
*  Finally a modest 32 year old Belgian visual effects artist named Chris Umi stepped forward [[00:41:26](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2486.68s)]
*  to claim credit. We believe as long as we're making clear this is a parody we're not doing anything [[00:41:32](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2492.92s)]
*  to harm his image but after a few videos we realized like this is blowing up we're getting [[00:41:41](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2501.0s)]
*  millions and millions and millions of views. Umi says his work is made easier because he teamed up [[00:41:46](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2506.68s)]
*  with a Tom Cruise impersonator whose voice gestures and hair are nearly identical to the real McCoy. [[00:41:52](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2512.68s)]
*  Umi only deep fakes Cruise's face and stitches that onto the real video and sound of the [[00:42:01](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2521.96s)]
*  impersonator. That's where the magic happens. For technophiles the deep Tom Cruise was a tipping [[00:42:08](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2528.3599999999997s)]
*  point for deep fakes. How do you make this so seamless? It begins with training a deep fake [[00:42:15](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2535.48s)]
*  model of course. I have all the face angles of Tom Cruise, all the expressions, all the emotions. It [[00:42:23](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2543.96s)]
*  takes time to create a really good deep fake model. What do you mean training the model? How do you [[00:42:30](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2550.2000000000003s)]
*  train your computer? Training means it's going to analyze all the images of Tom Cruise, all his [[00:42:35](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2555.4s)]
*  expressions compared to my impersonator. So the computer is going to teach itself when my [[00:42:42](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2562.2s)]
*  impersonator is smiling I'm going to recreate Tom Cruise smiling and that's that's how you train it. [[00:42:47](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2567.7999999999997s)]
*  Using video from the CBS News archives Chris Umi was able to train his computer to learn every aspect [[00:42:54](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2574.2799999999997s)]
*  of my face and wipe away the decades. This is how I looked 30 years ago. He can even remove my mustache. [[00:43:01](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2581.3199999999997s)]
*  The possibilities are endless and a little frightening. I see a lot of mistakes in my work [[00:43:11](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2591.3199999999997s)]
*  but I don't mind it actually because I don't want to fool people. I just want to show them what's [[00:43:18](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2598.2799999999997s)]
*  possible. You don't want to fool people? No, I want to entertain people. I want to raise awareness [[00:43:23](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2603.48s)]
*  and I want and I want to show where it's all going. It is without a doubt one of the most important [[00:43:30](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2610.44s)]
*  revolutions in the future of human communication and perception. I would say it's analogous [[00:43:38](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2618.6s)]
*  to the birth of the internet. Political scientist and technology consultant Nina Schick wrote one [[00:43:45](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2625.56s)]
*  of the first books on deep fakes. She first came across them four years ago when she was advising [[00:43:51](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2631.72s)]
*  European politicians on Russia's use of disinformation and social media to interfere [[00:43:58](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2638.2s)]
*  in democratic elections. What was your reaction when you first realized this was possible and was [[00:44:04](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2644.04s)]
*  going on? Well given that I was coming at it from the perspective of disinformation and [[00:44:10](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2650.7599999999998s)]
*  manipulation in the context of elections, the fact that AI can now be used to make images and video [[00:44:18](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2658.52s)]
*  that are fake that look hyper realistic, I thought well from a disinformation perspective this is a [[00:44:26](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2666.12s)]
*  game changer. So far there's no evidence deep fakes have changed the game in a U.S. election, [[00:44:33](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2673.16s)]
*  but earlier this year the FBI put out a notification warning that Russian and Chinese [[00:44:40](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2680.04s)]
*  actors are using synthetic profile images creating deep fake journalists and media personalities [[00:44:46](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2686.52s)]
*  to spread anti-American propaganda on social media. So how do you get deep fakes? The U.S. military, [[00:44:53](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2693.8799999999997s)]
*  law enforcement and intelligence agencies have kept a wary eye on deep fakes for years. [[00:45:01](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2701.16s)]
*  At this 2019 hearing, Senator Ben Sasse of Nebraska asked if the U.S. is prepared for the [[00:45:07](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2707.3199999999997s)]
*  onslaught of disinformation, fakery and fraud. When you think about the catastrophic potential [[00:45:14](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2714.44s)]
*  to public trust and to markets that could come from deep fake attacks, are we organized in a way [[00:45:20](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2720.68s)]
*  that we could possibly respond fast enough? We clearly need to be more agile. It poses a major [[00:45:26](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2726.2s)]
*  threat to the United States and something that the intelligence community needs to be restructured to [[00:45:33](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2733.48s)]
*  address. Since then technology has continued moving at an exponential pace while U.S. policy has not. [[00:45:39](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2739.48s)]
*  Efforts by the government and big tech to detect synthetic media are competing with a community of [[00:45:48](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2748.04s)]
*  deep fake artists who share their latest creations and techniques online. Like the internet, [[00:45:54](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2754.68s)]
*  the first place deep fake technology took off was in pornography. The sad fact is the majority of [[00:46:01](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2761.7999999999997s)]
*  deep fakes today consist of women's faces, mostly celebrities superimposed onto pornographic videos. [[00:46:09](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2769.0s)]
*  The first use case in pornography is just a harbinger of how deep fakes can be used maliciously [[00:46:17](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2777.64s)]
*  in many different contexts which are now starting to arise. And they're getting better all the time? [[00:46:25](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2785.08s)]
*  Yes. The incredible thing about deep fakes and synthetic media is the pace of acceleration when [[00:46:29](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2789.56s)]
*  it comes to the technology. And by five to seven years we are basically looking at a trajectory [[00:46:37](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2797.24s)]
*  where any single creator, so a YouTuber, a TikToker, will be able to create the same level [[00:46:44](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2804.44s)]
*  of visual effects that is only accessible to the most well-resourced Hollywood studio today. [[00:46:51](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2811.8s)]
*  The technology behind deep fakes is artificial intelligence which mimics the way humans learn. [[00:46:57](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2817.32s)]
*  In 2014 researchers for the first time used computers to create realistic looking faces [[00:47:04](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2824.1200000000003s)]
*  using something called Generative Adversarial Networks or GANs. So you set up an adversarial [[00:47:11](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2831.0800000000004s)]
*  game where you have two AIs combating each other to try and create the best fake synthetic content. [[00:47:18](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2838.36s)]
*  And as these two networks combat each other, one trying to generate the best image, the other [[00:47:26](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2846.04s)]
*  trying to detect where it could be better, you basically end up with an output that is [[00:47:31](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2851.88s)]
*  increasingly improving all the time. Schick says the power of generative adversarial networks [[00:47:37](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2857.2400000000002s)]
*  is on full display at a website called thispersondoesnotexist.com. Every time you refresh the page there's a new image of a person who does not exist. [[00:47:44](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2864.52s)]
*  Each is a one-of-a-kind entirely AI generated image of a human being who never has and never will walk this earth. [[00:47:57](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2877.0s)]
*  You can see every pore on their face, you can see every hair on their head, but now imagine that technology [[00:48:06](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2886.12s)]
*  being expanded out not only to human faces in still images but also to video, to audio synthesis of people's voices and that's really where we're heading right now. [[00:48:12](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2892.92s)]
*  This is mind-blowing. Yes. What's the positive side of this? [[00:48:24](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2904.44s)]
*  The technology itself is neutral. So just as bad actors are without a doubt going to be using deep fakes, [[00:48:29](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2909.96s)]
*  it is also going to be used by good actors. So first of all I would say that there's a very compelling case to be made for the commercial use of deep fakes. [[00:48:37](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2917.0s)]
*  Victor Riparbelli is CEO and co-founder of Synthesia based in London, one of dozens of companies using [[00:48:46](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2926.84s)]
*  deep fake technology to transform video and audio productions. The way Synthesia works is that we've [[00:48:54](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2934.44s)]
*  essentially replaced cameras with code and once you're working with software we do a lot of things [[00:49:02](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2942.12s)]
*  that you wouldn't be able to do with a normal camera. We're still very early but this is going [[00:49:09](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2949.0s)]
*  to be a fundamental change in how we create media. This video was of course generated by Synthesia. [[00:49:13](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2953.0s)]
*  Synthesia makes and sells digital avatars using the faces of paid actors to deliver personalized [[00:49:19](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2959.64s)]
*  messages in 64 languages and allows corporate CEOs to address employees overseas. [[00:49:26](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2966.68s)]
*  Synthesia has also helped entertainers like Snoop Dogg go forth and multiply. [[00:49:39](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2979.48s)]
*  This elaborate TV commercial for European food delivery service Just Eat cost a fortune. [[00:49:45](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2985.16s)]
*  Just Eat has a subsidiary in Australia which is called Menulog. So what we did with our technology [[00:49:51](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2991.64s)]
*  was we switched out the word Just Eat for Menulog. And all of a sudden they had a localized [[00:49:59](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=2999.0s)]
*  version for the Australian market without Snoop Dogg having to do anything. So he makes twice the money. [[00:50:10](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3010.44s)]
*  All it took was eight minutes of me reading a script on camera for Synthesia to create [[00:50:15](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3015.72s)]
*  my synthetic talking head complete with my gestures head and mouth movements. Another company [[00:50:23](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3023.3199999999997s)]
*  Descript used AI to create a synthetic version of my voice. This is Bill Whittaker's synthetic voice [[00:50:29](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3029.8799999999997s)]
*  with my cadence tenor and syncopation. This is the result. The words you're hearing were never [[00:50:36](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3036.9199999999996s)]
*  spoken by the real Bill into a microphone or to a camera. He merely typed the words into a computer [[00:50:43](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3043.48s)]
*  and they come out of my mouth. It may look and sound a little rough around the edges right now [[00:50:49](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3049.16s)]
*  but as the technology improves the possibilities of spinning words and images out of thin air [[00:50:55](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3055.0s)]
*  are endless. I'm Bill Whittaker. I'm Bill Whittaker. I'm Bill Whittaker. Wow and the head, the eyebrows, [[00:51:01](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3061.8s)]
*  the mouth, the way it moves. It's all synthetic. I could be lounging at the beach and say folks [[00:51:12](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3072.6s)]
*  you know I'm not going to come in today but you can use my avatar to do the work. Maybe in a few [[00:51:19](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3079.3199999999997s)]
*  years. Don't don't don't tell me that. I'd be tempted. I think it'll have a big impact. The rapid [[00:51:23](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3083.64s)]
*  advances in synthetic media have caused a virtual gold rush. Tom Graham, a London-based lawyer who [[00:51:28](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3088.52s)]
*  made his fortune in cryptocurrency, recently started a company called Metaphysic with none other than [[00:51:35](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3095.3199999999997s)]
*  Chris Umi, creator of Deep Tom Cruise. Their goal? Develop software to allow anyone to create [[00:51:41](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3101.64s)]
*  Hollywood caliber movies without lights, cameras or even actors. As the hardware scales and as the [[00:51:48](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3108.68s)]
*  models become more efficient we can scale up the size of that model to be an entire Tom Cruise. [[00:51:56](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3116.92s)]
*  Body movement and everything. Well talk about disruptive. I mean are you going to put actors [[00:52:02](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3122.44s)]
*  out of jobs? I think that it's a great thing if you're a well-known actor today because you [[00:52:07](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3127.4s)]
*  may be able to let somebody collect data for you to create a version of yourself in the future [[00:52:13](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3133.56s)]
*  where you could be acting in movies after you have deceased or you could be the director directing [[00:52:18](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3138.04s)]
*  your younger self in a movie or something like that. If you are wondering how all of this is legal, [[00:52:23](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3143.7200000000003s)]
*  most deep fakes are considered protected free speech. Attempts at legislation are all over the [[00:52:29](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3149.4s)]
*  map. In New York commercial use of a performer's synthetic likeness without consent is banned for [[00:52:36](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3156.28s)]
*  40 years after their death. California and Texas prohibit deceptive political deep fakes in the [[00:52:43](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3163.2400000000002s)]
*  lead-up to an election. There are so many ethical philosophical gray zones here that we really need [[00:52:50](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3170.0400000000004s)]
*  to think about. So how do we as a society grapple with this? Just understanding what's going on [[00:52:56](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3176.28s)]
*  because a lot of people still don't know what a deep fake is, what synthetic media is, that this [[00:53:05](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3185.08s)]
*  is now possible. The counter to that is how do we inoculate ourselves and understand that this kind [[00:53:11](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3191.48s)]
*  of content is coming and exists without being completely cynical, right? How do we do it without [[00:53:17](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3197.4s)]
*  losing trust in all authentic media? That's going to require all of us to figure out how to maneuver [[00:53:22](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3202.36s)]
*  in a world where seeing is not always believing. [[00:53:31](https://www.youtube.com/watch?v=LgUjLcxJxVg&t=3211.88s)]
