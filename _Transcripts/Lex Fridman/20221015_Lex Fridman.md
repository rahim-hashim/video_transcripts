---
Date Generated: April 10, 2024
Transcription Model: whisper medium 20231117
Length: 11013s
Video Keywords: ['agi', 'ai', 'ai podcast', 'artificial intelligence', 'artificial intelligence podcast', 'automation', 'ethics', 'jeffrey epstein', 'kate darling', 'lex ai', 'lex fridman', 'lex jre', 'lex mit', 'lex podcast', 'mit', 'mit ai', 'privacy', 'robots']
Video Views: 288538
Video Rating: None
---

# Kate Darling: Social Robots, Ethics, Privacy and the Future of MIT | Lex Fridman Podcast #329
**Lex Fridman:** [October 15, 2022](https://www.youtube.com/watch?v=ZFntEFXKDHM)
*  I think that animals are a really great thought experiment when we're thinking about AI and
*  robotics because, again, comparing them to humans, that leads us down the wrong path,
*  both because it's not accurate, but also I think for the future, we don't want that.
*  We want something that's a supplement. But I think animals, because we've used them throughout
*  history for so many different things, we domesticated them not because they do what we do,
*  but because what they do is different and that's useful. And it's just like whether we're talking
*  about companionship, whether we're talking about work integration, whether we're talking about
*  responsibility for harm, there's just so many things we can draw on in that history from these
*  entities that can sense, think, make autonomous decisions and learn that are applicable to how
*  we should be thinking about robots and AI. The following is a conversation with Kate Darling,
*  her second time on the podcast. She's a research scientist at MIT Media Lab,
*  interested in human-robot interaction and robot ethics, which she writes about in her recent book
*  called The New Breed, what our history with animals reveals about our future with robots.
*  Kate is one of my favorite people at MIT. She was a courageous voice of reason and compassion
*  to the time of the Jeffrey Epstein scandal at MIT three years ago. We reflect on this time
*  in this very conversation, including the lessons they revealed about human nature
*  and our optimistic vision for the future of MIT, a university we both love and believe in.
*  This is the Lex Friedman podcast. To support it, please check out our sponsors in the description.
*  Now, dear friends, here's Kate Darling. Last time we talked, a few years back,
*  you wore a Justin Bieber shirt for the podcast. Now, looking back, you're a respected researcher,
*  all the amazing accomplishments in robotics, you're an author. Was this one of the proudest
*  moments of your life, proudest decisions you've ever made? Definitely. You handled it really well,
*  though. It was cool because I walked in, I didn't know you were going to be filming. I walked in,
*  and you're in a fucking suit. I'm like, why are you all dressed up? Then you were so nice about
*  it, you made some excuse. You were like, oh, well, I'm interviewing some... Didn't you say you were
*  interviewing some military general afterwards to make me feel better? Oh, yeah, that was the CTO
*  of Lockheed Martin, I think. Oh, that's what it was. You didn't tell me, oh, I was dressed like this.
*  Are you an actual Bieber fan, or was that one of those T-shirts that's in the back of the closet
*  that you used for painting? I think I bought it for my husband as a joke. We were gut renovating
*  a house at the time, and I had worn it to the site. God is a joke, and now you wear it. Okay.
*  Have you worn it since? Was this the one time? No. How could I touch it again? It was on your podcast.
*  It's like a wedding dress or something like that. You only wear it once. You are the author of The
*  New Breed, what our history with animals reveals about our future with robots. You opened the book
*  with the surprisingly tricky question, what is a robot? Let me ask you, let's try to sneak up to
*  this question. What's a robot? That's not really sneaking up. It's just asking it. Yeah. All right.
*  What do you think a robot is? What I think a robot is, is something that has some level of
*  intelligence and some level of magic, that little shine in the eye that allows you to
*  navigate the uncertainty of life. That means like autonomous vehicles to me in that sense
*  are robots because they navigate the uncertainty, the complexity of life.
*  Obviously, social robots are that. I love that. I like that you mentioned magic because that also,
*  well, so first of all, I don't define robot definitively in the book because there is no
*  definition that everyone agrees on. If you look back through time, people have called things robots
*  until they lose the magic because they're more ubiquitous. A vending machine used to be called
*  a robot and now it's not. I do agree with you that there's this magic aspect, which is how people
*  understand robots. If you ask a roboticist, they have the definition of something that is, well,
*  it has to be physical. Usually, it's not an AI agent. It has to be embodied. They'll say it has
*  to be able to sense its environment in some way. It has to be able to make a decision autonomously
*  and then act on its environment again. I think that's a pretty good technical definition, even
*  though it really breaks down when you come to things like the smartphone because the smartphone
*  can do all of those things and most robotists would not call it a robot. There's really no
*  one good definition, but part of why I wrote the book is because people have a definition of robot
*  in their minds that is usually very focused on a comparison of robots to humans. If you Google
*  Image Search robot, you get a bunch of humanoid robots, robots with a torso and head and two arms
*  and two legs. That's the definition of robot that I'm trying to get us away from because I think that
*  it trips us up a lot. Why does the humanoid form trip us up a lot? Well, because this constant
*  comparison of robots to people, artificial intelligence to human intelligence, first of
*  all, it doesn't make sense from a technical perspective because the early AI researchers,
*  some of them were trying to recreate human intelligence. Some people still are and there's
*  a lot to be learned from that academically, et cetera, but that's not where we've ended up.
*  AI doesn't think like people. We wind up in this fallacy where we're comparing these two
*  and when we talk about what intelligence even is, we're often comparing to our own intelligence.
*  And then the second reason this bothers me is because it doesn't make sense.
*  I just think it's boring to recreate intelligence that we already have. I see the scientific value
*  of understanding our own intelligence, but from a practical, what could we use these technologies
*  for perspective, it's much more interesting to create something new, to create a set of
*  skills that we don't have that we can partner with and what we're trying to achieve.
*  And it should be in some deep way similar to us, but in most ways different because you still want
*  to have a connection, which is why the similarity might be necessary. That's what people argue.
*  Yes. And I think that's true. So the two arguments for humanoid robots are people need to be able to
*  communicate and relate to robots and we relate most of the things that are like ourselves.
*  And we have a world that's built for humans. So we have stairs and narrow passageways and
*  door handles. And so we need humanoid robots to be able to navigate that. And so you're speaking to
*  the first one, which is absolutely true, but what we know from social robotics and a lot of human
*  robot interaction research is that all you need is something that's enough like a person for it to
*  give off cues that someone relates to. And that, but that doesn't have to look human or even act
*  human. You can take a robot like R2D2 and it just like beeps and boops and people love R2D2, right?
*  Even though it's just like a trash can on wheels and they like R2D2 more than C3PO who's a humanoid.
*  So there's lots of ways to make robots even better than humans in some ways and make us relate more
*  to them. Yeah, it's kind of amazing the variety of cues that can be used to anthropomorphize the
*  thing like a glowing orb or something like that. Just a voice, just subtle basic interaction.
*  I think people sometimes over engineer these things. Like simplicity can go a really long way.
*  Totally. I mean, ask any animator and they'll know that. Yeah. Yeah. Those are actually,
*  so the people behind Cosmo, the robot, the right people to design those as animators,
*  like Disney type of people versus like roboticists. Roboticists, quote unquote, are mostly clueless.
*  It seems like they just have their own discipline that they're very good at and they don't have.
*  Yeah, but that don't, you know, I feel like robotics of the early 21st century
*  is not going to be the robotics of the later 21st century. Like if you call yourself a
*  roboticist, it'd be something very different. Because I think more and more you'd be like a,
*  maybe like a control engineer or something, controls engineer. Like you separate because
*  ultimately all the unsolved, all the big problems of robotics will be in the social aspect,
*  in the interacting with humans aspect, in the perception interpreting the world aspect,
*  in the brain part, not the basic control level part. You call it basic. It's actually really
*  complex. It's very, very complicated. And that's why, but like, I think you're so right and what
*  a time to be alive because for me, I just, we've had robots for so long and they've just been
*  behind the scenes. And now finally robots are getting deployed into the world. They're coming
*  out of the closet. Yeah. And, and we're seeing all these mistakes that companies are making because
*  they focused so much on the engineering and getting that right and getting the robot to be even be
*  able to function in a space that it shares with a human. See what I feel like people don't understand
*  is to solve the perception and the control problem. You shouldn't try to just solve the
*  perception control problem. You should teach the robot how to say, Oh shit, I'm sorry. I fucked up.
*  Yeah. Or ask for help or ask for help or be able to communicate the uncertainty. Yeah, exactly.
*  All of those things, because you can't solve the perception and control. We humans haven't solved
*  it. We were really damn good at it. But the magic is in the, the self-deprecating humor and
*  the self-awareness about where our flaws are, all that kind of stuff.
*  Yeah. And there's a whole body of research in human robot interaction showing like ways to do
*  this. But a lot of these companies haven't, they don't do HRI. They like the, have you seen the
*  grocery store robot in the stop and shop? Yes. Yeah. The Marty, it looks like a giant penis.
*  It's like six feet tall. It roams the aisles. I will never see Marty the same way again. Thank
*  you. You're welcome. But like they, these poor people were so hard on getting a functional robot
*  together. And then people hate Marty because they didn't at all consider how people would react to
*  Marty in their space. Does everybody, I mean, you talk about this, do people mostly hate Marty?
*  Cause I like, I like Marty. I feel like less. Yeah. And actually like, there's a, there's a
*  parallel between the two. I believe there is. So we were actually going to do a study on this
*  right before the pandemic hit. And then we canceled it because we didn't want to go to
*  the grocery store and neither did anyone else. But our theory, so this was with a student at MIT,
*  Daniella DiPaola. She noticed that everyone on Facebook in her circles was complaining about
*  Marty. They were like, what is this creepy robot? It's watching me. It's always in the way.
*  And she did this like quick and dirty sentiment analysis on Twitter where she was looking at
*  positive and negative mentions of the robot. And she found that the biggest spike of negative
*  mentions happened when stop and shop through a birthday party for the Marty robots, like with
*  free cake and balloons. Like who complains about free cake? Well, people who hate Marty apparently.
*  So, and so we were like, that's interesting. And then we did this like online poll. We used
*  Mechanical Turk and we tried to get at what people don't like about Marty. And a lot of it wasn't,
*  Oh, Marty's taking jobs. It was Marty is the surveillance robot, which it's not. It looks
*  for spills on the floor. It doesn't actually like look at any people. It's watching, it's creepy,
*  it's getting in the way. Those were the things that people complained about. And so our hypothesis
*  became is Marty a real life Clippy? Because I know Lex you love Clippy, but many people hated Clippy.
*  Well, there's a complex thing there. It could be like marriage. A lot of people seem to like to
*  complain about marriage, but they secretly love it. So it could be the relationship you might have
*  with Marty is like, Oh, there he goes again, doing a stupid surveillance thing. But you grow to love
*  the, I mean, bitching about the thing that kind of releases a kind of tension. And there's,
*  I mean, some people, a lot of people show love by sort of busting each other's chops, you know,
*  like making fun of each other. And then if I think I think people would really love it if Marty talked
*  back. And like, these are so many possible options for humor there. One, you can lean in,
*  you can be like, yes, I'm an agent of the CIA monitoring your every move, like mocking people
*  that are concerned, you know, saying like, yes, I, I'm watching you because you're so important
*  with your shopping patterns. I'm collecting all this data or, or just, you know, any kind of
*  making fun of people. I don't know. But I think you hit on what exactly it is, because when it comes
*  to robots or artificial agents, I think people hate them more than they would some other machine
*  or device or object. And it might, and it might be that thing. It might be combined with love or like,
*  whatever it is, it's a more extreme response because they view these things as social agents
*  and not objects. And that was so Clifford Nass was a big human computer interaction person. And he,
*  his theory about Clippy was that because people viewed Clippy as a social agent,
*  when Clippy was annoying and would like bother them and interrupt them and like not remember
*  what they told him. That's when people got upset because it wasn't fulfilling their social
*  expectations. And so they complained about Clippy more than they would have if it had been a different,
*  like not an, not a, you know, virtual character. So is complaining to you a sign that we're on the
*  wrong path with a particular robot or is it possible like, again, like marriage, like
*  a family that there still is a path towards that direction where we can find deep, meaningful
*  relationship. I think we absolutely can find deep, meaningful relationship with robots. And well,
*  maybe with Marty, I mean, I just would, I would have designed Marty a little differently.
*  Like how? Isn't there a charm to the clumsiness, the slowness?
*  There is if you're not trying to get through with a shopping cart and a screaming child. You know,
*  there's, I think, I think you could make it charming. I think there are lots of design tricks
*  that they could have used. And one of the things they did, I think without thinking about it at all,
*  is they slapped two big googly eyes on Marty. And I, I wonder if that contributed maybe to people
*  feeling watched because, because it's looking at them. And so like, is there a way to design the
*  robot to do the function that it's doing in a way that doesn't, that people are actually attracted
*  to rather than annoyed by? And there are many ways to do that, but companies aren't thinking
*  about it. Now they're realizing that they should have thought about it.
*  Yeah. I wonder if there's a way to, if it would help to make Marty seem like an entity of its own
*  versus the arm of a large corporation. So there's some sense where this is just the camera
*  that's monitoring people versus this is an entity that's a standalone entity. It has its own task
*  and it has its own personality. Like the more personality you give it, the more it feels like
*  it's not sharing data with anybody else. Like when we see other human beings, our basic assumption
*  is whatever I say to this human being, it's not like being immediately sent to the CIA.
*  Yeah. What I say to you, no one's going to hear that, right?
*  Yeah, that's true. That's true. No, no, I'm kidding.
*  Well, you forget it. I mean, you do forget it. I mean, I don't know if that even with microphones
*  here, you forget that that's happening, but for some reason, I think probably with Marty,
*  with Marty, I think what is done really crudely and crappily, you start to realize,
*  oh, this is like PR people trying to make a friendly version of a surveillance machine.
*  But I mean, that reminds me of the slight clumsiness or significant clumsiness on the
*  initial releases of the avatars for the metaverse. I don't know. What are your actually thoughts
*  about that? The way the avatars, the way like Mark Zuckerberg looks in that world,
*  the metaverse, the virtual reality world where you can have like virtual meetings and stuff like
*  that. How do we get that right? Do you have thoughts about that? Because it's a kind of,
*  uh, it's a, it's, it feels like a similar problem to social robotics, which is how you design
*  a digital virtual world that is compelling when you connect to others there in the same way that
*  physical connection is right. I haven't looked into, I mean, I've seen people joking about it
*  on Twitter and like posting whatever. Yeah. But I mean, have you seen it? Cause it,
*  there's something you can't quite put into words, uh, that, um, doesn't feel genuine
*  about the way it looks. And so the question is if you and I were to meet virtually,
*  what should the avatars look like for us to have similar kind of connection? Should it be really
*  simplified? Should it be a little bit more realistic? Should it be cartoonish? Should it be more,
*  um, better capturing of expressions, uh, in interesting complex ways versus like cartoonish
*  over simplified ways. But haven't video games figured this out? I'm not a gamer,
*  so I don't have any examples, but I feel like there's this whole world in video games where
*  they've thought about all of this and depending on the game, they have different like avatars. And
*  a lot of the games are about connecting with others. I just, the thing that I don't know is
*  again, I haven't looked into this at all. Um, I've been like shockingly not very interested in the
*  metaverse, but they must have poured so much investment into this, um, meta and like, why,
*  why is it so, why are people, why is it so bad? Like, there's gotta be a reason. There's gotta
*  be some thinking behind it, right? Well, I talked to Carmack about this, uh, John Carmack, who's a
*  part-time, um, Oculus CTO. I think, uh, there's several things to say. One is, as you probably
*  know that, I mean, there's bureaucracy, there's large corporations and they often large corporations
*  have a way of killing the indie kind of artistic flame that's required to create something really
*  compelling. Somehow they make everything boring because they, they run through this whole process
*  through the PR department, through all that kind of stuff. And it somehow becomes generic to that
*  process. Cause there's like- They strip out anything interesting because it could be controversial.
*  Is that, or- Yeah, right. Exactly. Like, um, like what, I mean, we, we're living through this now,
*  like with, uh, with a lot of people with cancellations, all those kinds of stuff,
*  people are nervous and nervousness results in like, uh, like usual, the assholes are
*  ruining everything. But you know, the magic of human connection is taking risks of making a risky
*  joke of, of, of like with people you like, we're not assholes, good people. Like some of the fun,
*  some of the fun in the metaverse or in video games is, you know, being edgier, being interesting,
*  revealing your personality in interesting ways, um, in the sexual tension or in, uh, like they're
*  definitely paranoid about that. Oh yeah. Like in metaverse, the possibility of sexual assault and
*  sexual harassment and all that kind of stuff, it's, it's obviously very high, but they're,
*  um, so you should be paranoid to some degree, but not too much because then you remove completely
*  per the personality of the whole thing. Then everybody's just like a vanilla bot that, uh,
*  like you have to have ability, um, to be a little bit political, to be a little bit edgy,
*  all that kind of stuff. And large companies tend to suffocate that. So I, but in general,
*  just forget all that, just the ability to come up really cool, beautiful ideas.
*  Um, if you look at, um, I think Grimes tweeted about this, she's very critical about the metaverse
*  is that, um, you know, the, uh, independent, uh, game designers have solved this problem of how
*  to create something beautiful and interesting and compelling. They, they do a really good job.
*  So you have to let those kinds of minds, the small groups of people design things and let them run
*  with it and let them run wild and do edgy stuff. Yeah. But otherwise you, because you get this kind
*  of, you get a Clippy type of situation, right? Which is like a very generic looking thing. Um,
*  but even Clippy has some, like, that's kind of wild that you would take a, a paper clip and
*  put eyes on it. And suddenly people are like, Oh, you're annoying, but you're definitely a social
*  agent. And I just feel like that wouldn't even, that Clippy thing wouldn't even survive Microsoft
*  or Facebook of today, matter of today. Cause it would be like, well, there'll be these meetings
*  about why is it a paper clip? Like, why don't we, it's not sufficiently friendly. Let's make it,
*  uh, you know, and then all of a sudden the artists that with whom it originated is killed and it's
*  all PR marketing people and all of that kind of stuff. Now they do important work to some degree,
*  but they kill the creativity. I think the killing of the creativity is in the whole, like, okay.
*  So what I know from social robotics is like, obviously if you create agents that, okay. So
*  take for an example, you'd create a robot that looks like a humanoid and it's, you know, Sophia
*  or whatever. Now suddenly you do have all of these issues where are you reinforcing an unrealistic
*  beauty standard? Are you objectifying women? Uh, why is the robot white? So you have, but the thing
*  is, I think that with creativity, you can find a solution that's even better where you're not even
*  harming anyone and you're creating a robot that looks like not, not humanoid, but like something
*  that people relate to even more. And now you don't even have any of these bias issues that
*  you're creating. And so how do we create that within companies? Because I don't think it's
*  really about, like, I, cause I, you know, maybe we disagree on that. I don't think that edginess or
*  humor or interesting things need to be things that harm or hurt people or that people are against.
*  There are ways to find things that everyone is fine with. Why aren't we doing that?
*  The problem is there's departments that look for harm and things.
*  Yeah. And so they will find harm and things that have no harm. That's the big problem because
*  their whole job is to find harm and things. So what you said is completely correct, which is
*  edginess should not hurt. Doesn't necessarily, doesn't need to be a thing that hurts people.
*  Obviously great humor, great personality doesn't have to, uh, like Clippy.
*  But yeah, I mean, but it's tricky to get right. And I'm not exactly sure. I don't know. I don't
*  know why a large corporation with a lot of funding can't get this right.
*  I do think you're right that there's a lot of aversion to risk. And so if you get lawyers
*  involved or people whose job it is, like you say, to mitigate risk, they're just going to say no to
*  most things that could even be in some way. Yeah. Yeah. You get the problem in all organizations.
*  So I think that you're right. That is a problem.
*  I think what's the way to solve that in large organizations is to have Steve Jobs type of
*  characters. Unfortunately, you do need to have, I think, um, from a designer perspective, or maybe
*  like a Johnny Ive that is almost like a dictator. Yeah. You want a benevolent dictator. Yeah. Who
*  rolls in and says, like the cuts through the lawyers, the PR, but has a benevolent aspect.
*  Like, yeah, there's a good heart and make sure like, I think all great artists and designers
*  create stuff that doesn't hurt people. Like if you have a good heart, you're going to create
*  something that's going to actually, um, make a lot of people feel good. That's what like, uh,
*  people like Johnny Ive, what they love doing is creating a thing that brings a lot of love to the
*  world. They imagine like millions of people using the thing and it instills them with, with joy.
*  You could say that about social robotics. You could say that about the metaverse.
*  It shouldn't be done by the PR people should be done by the designers. I agree. PR people ruin
*  everything. Yeah. All the fun, uh, in the, uh, in the book, you have a picture. This, I just have a
*  lot of ridiculous questions. You have a picture of two hospital delivery robots with a caption
*  that reads, by the way, see your book, I appreciate that. I'm not going to say that. I'm
*  not going to say that. By the way, see your book, I appreciate that it keeps the humor in. You
*  didn't run it by the PR department. No, no one edited the book. You got rushed through.
*  Uh, the, the caption reads two hospital delivery robots whose sexy nurse names Roxy and Lola made
*  me roll my eyes so hard. They almost fell out. Um, what aspect of it made you roll your eyes?
*  It was the naming. The form factor is fine. It's like a little box on wheels. The fact that they
*  named them also great. That'll let people enjoy interacting with them. We know that even just
*  giving a robot a name, people will, uh, it facilitates technology adoption. People will be
*  like, Oh, you know, Betsy made a mistake. Let's help her out instead of the stupid robot doesn't
*  work. But why Lola and Lola and Roxy? Like those are to you too sexy. I mean, there's research
*  showing that a lot of robots are named according to gender biases about the function that they're
*  fulfilling. So, you know, robots that are helpful and assistance and are like nurses are usually
*  female gendered robots that are powerful. All wise computers like Watson usually have like a
*  booming male, uh, coded voice and name. So like, like that's one of those things, right? You're
*  opening a can of worms for no reason, for no reason. You can avoid this whole camera. Yeah.
*  Just give it a different name. Like why Roxy it's because people aren't even thinking. So
*  to some extent, I don't, I don't like PR departments, but getting some feedback on your
*  work from a diverse set of participants, listening and taking in things that help you identify your
*  own blind spots. And then you can always make your good leadership choices and good, like you can
*  still ignore things that you don't believe are an issue, but having the openness to take in feedback
*  and making sure that you're getting the right feedback from the right people. I think that's
*  really important. So don't unnecessarily propagate the biases of society. Yeah. Why in the design?
*  But, um, if you're not careful, the, when you, when you do the research of like, you might,
*  if you ran a poll with a lot of people of all the possible names, these robots have, they might come
*  up with Roxy and Lola as, as, as names they, um, it would enjoy most. Like that could come up as, uh,
*  as the highest. As in you do marketing research and then, well, that's what they did with Alexa.
*  They did marketing research and nobody wanted the male voice. Everyone wanted it to be female.
*  Well, what do you, what do you think about that? Like what, I mean, if I, if I were to say,
*  I think the role of a great designer, again, to go back to Johnny Ive is to throw out the
*  marketing research, like take it in, do it, learn from it. But like, if everyone wants Alexa to be a
*  female voice, the role of the designers to think deeply about the future of social agents in the
*  home and think like, what does that future look like? And try to reverse engineer that future.
*  So like, in some sense, there's this weird tension, like you want to listen to a lot of people,
*  but at the same time you want to, you're creating a thing that defines the future of the world
*  and the people that you're listening to are part of the past. So like that weird tension.
*  Yeah, I think that's true. And I think some companies like Apple have historically done
*  very well at understanding a market and saying, you know what our role is? It's to understand
*  what our role is. It's not to listen to what the current market says. It's to actually shape
*  the market and shape consumer preferences. And companies have the power to do that. They can
*  be forward thinking and they can actually shift what the future of technology looks like.
*  And I agree with you that I would like to see more of that, especially when it comes to
*  existing biases that we know, or, you know, that, that I think there's the low hanging fruit of
*  companies that don't even think about it at all and aren't talking to the right people and aren't
*  getting the full information. And then there's companies that are just like doing the safe thing
*  and giving consumers what they want now, but to be really forward looking and be really successful,
*  I think you have to make some judgment calls about what the future is going to be.
*  But do you think it's still useful to gender and to name the robots?
*  Yes. I mean, gender is a minefield, but people, it's really hard to get people to not gender a
*  robot in some way. So if you don't give it a name or you give it a, like, ambiguous voice,
*  people will just choose something. And maybe that's better than just like, you know,
*  entrenching something that you've decided is best. But I do think it can be helpful on the like,
*  anthropomorphism engagement level to give it attributes that people identify with.
*  Yeah. I think a lot of roboticists I know, they don't gender the robot. They don't,
*  they even try to avoid naming the robot or naming it something that is, can be used as a name
*  in conversation kind of thing. And I think that actually that's irresponsible because
*  people are going to anthropomorphize the thing anyway. So you're just removing from yourself
*  the responsibility of how they're going to anthropomorphize it. That's a good point.
*  And so like you want to be able to like, they're going to do it. You have to start to think about
*  how they're going to do it. Even if the robot is like a Boston Dynamics robot,
*  that's not supposed to have any kind of social component. They're obviously going to project
*  a social component to it. Like that arm, I worked a lot with quadrupeds now with robot dogs. You
*  know, that arm people think is the head immediately. It's supposed to be an arm, but they start to
*  think it's a head and you have to like acknowledge that you can't. I mean, they do now.
*  They do now. Well, they've deployed the robots and people are like,
*  oh my God, the cops are using a robot dog. And so they have this PR nightmare. And so they're like,
*  oh yeah. Okay. Maybe we should hire some HRI people.
*  Well, Boston Dynamics is an interesting company. Any of the others are doing similar thing because
*  their main source of money is in industrial applications. So like surveillance of factories
*  and doing dangerous jobs. So to them, it's almost good PR for people to be scared of these things
*  because it's for some reason, as you talk about people are naturally for some reason scared.
*  We could talk about that robots. And so it becomes more viral, like playing with that little fear.
*  And so it's almost like a good PR because ultimately they're not trying to put them in
*  the home and have a good social connection. They're trying to put them in factories.
*  And so they have fun with it. If you watch Boston Dynamics videos, they're aware of it.
*  Oh yeah. The videos for sure that they put out.
*  It's almost like an unspoken tongue in cheek thing. They're aware of how people are going to feel
*  when you have a robot that does like a flip. Now, most of the people are just like excited
*  about the control problem of it, like how to make the whole thing happen. But they're aware
*  when people see. Well, I think they became aware. I think that in the beginning, they were really,
*  really focused on just the engineering. I mean, they're at the forefront of robotics,
*  like locomotion and stuff. And then when they started doing the videos, I think that was
*  kind of a labor of love. I know that the former CEO, Mark, like he oversaw a lot of the videos
*  made a lot of them himself. And like he's even really detail oriented. Like there can't be like
*  some sort of incline that would give the robot an advantage. They're very like he was very
*  held up integrity about the authenticity of them. But then when they started to go viral,
*  I think that's when they started to realize, oh, there's something interesting here that,
*  I don't know how much they took it seriously in the beginning, other than realizing that
*  they could play within the videos. I know that they take it very seriously now.
*  What I like about Boston Dynamics and similar companies, it's still mostly run by engineers.
*  But, you know, I've had my criticisms. There's a bit more PR leaking in, but those videos are
*  made by engineers because that's what they find fun. It's like testing the robustness of the system.
*  I mean, they're having a lot of fun there with the robots.
*  Totally. Have you been to visit?
*  Yeah. Yeah. Yeah. It's one of the most important. I mean, because I have eight robot dogs now.
*  Wait, you have eight robot dogs? What? They're just walking around your place?
*  Yeah, I'm working on them. That's actually one of my goals is to have at any one time
*  always a robot moving. I'm far away from that.
*  That's an ambitious goal.
*  Well, I have like more Roombas that I know what to do with. They're a program. So the
*  programmable Roombas. And I have a bunch of little like I built the, well, I'm not finished with it
*  yet, but a robot from Rick and Morty. They still have a bunch of robots everywhere. But the thing is,
*  what happens is you're working on one robot at a time and that becomes like a little project.
*  It's actually very difficult to have just a passively functioning robot always moving.
*  Yeah.
*  And that's a dream for me because I'd love to create that kind of little world. So the impressive
*  thing about Boston Dynamics to me was to see like hundreds of spots. And like there was a,
*  the most impressive thing that still sticks with me is there was a spot robot walking down the hall
*  seemingly with no supervision whatsoever. And he was wearing, he or she, I don't know,
*  was wearing a cowboy hat. It just, it was just walking down the hall and nobody paying attention.
*  And it's just like walking down this long hall and I'm like looking around this is anyone like
*  what's happening here? So presumably some kind of automation was doing the map. I mean,
*  the whole environment is probably really well mapped, but I, it was just, it gave me a picture
*  of a world where a robot is doing, doing his thing, wearing a cowboy hat, just going down the hall,
*  like getting some coffee or whatever. Like, I don't know what it's doing, what's the mission,
*  but I don't know. For some reason, it really stuck with me. You don't often see robots
*  that aren't part of a demo or that aren't, you know, like with a semi-autonomous autonomous vehicle,
*  like directly doing a task. This was just chilling. Yeah. Walking around. I don't know.
*  Well, yeah, you know, I mean, we're at MIT, like when I first got to MIT, I was like,
*  okay, where's all the, where's all the robots? And they were all like broken or like not demoing. So
*  yeah. And, and, and what really excites me is that we're about to have that. We're about to have so
*  many moving ro- about to. Well, it's coming. It's coming in our lifetime that we will just have robots
*  moving around. We're already seeing the beginnings of it. There's delivery robots in some cities,
*  on the sidewalks. And I just love seeing like the TikToks of people reacting to that. Because yeah,
*  you see a robot walking down the hall with a cowboy hat. You're like, what the fuck? What is this?
*  This is awesome and scary and kind of awesome. And people either love or hate it. That's one
*  of the things that I think companies are underestimating that people will either love a
*  robot or hate a robot and nothing in between. So it's just, again, an exciting time to be alive.
*  Yeah. I think kids almost universally, at least in my experience, love them.
*  Love legged robots. If they're not loud, my, my son hates the rumo because ours is loud.
*  Oh, that, yeah. No, the legs, the legs. Oh yeah. Cause I, your son, um,
*  did they understand rumo to be a robot? Oh yeah. My kids, that's one of the first words they
*  learned. They know how to say beep boop. And yes, they think the room is a robot.
*  Do they project intelligence out of the thing? Well, we don't really use it around them anymore
*  for the reason that my son is scared of it. Yeah. That's really interesting.
*  I think they would like even a Roomba because it's moving around on its own.
*  I think kids and animals view it as a, an agent.
*  So what do you think if we just look at the state of the art of robotics,
*  what do you think robots are actually good at today? So if we look at today,
*  you mean physical robots? Yeah. Physical robots.
*  Well, like what are you impressed by? So I think a lot of people, I mean, that's what your book is
*  about is have maybe a, not a perfectly calibrated understanding of where we are in terms of robotics,
*  what's difficult to robotics, what's easy in robotics. Yeah. We're way behind where people
*  think we are. So what's impressive to me. So, uh, let's see. Oh, one,
*  one thing that came out recently was Amazon has this new warehouse robot and it's the first
*  autonomous warehouse robot that can, is safe for people to be around. And so like it's kind of,
*  most people, I think envision that our warehouses are already fully automated and that there's just
*  like robots doing things. It's actually still really difficult to have robots and people in
*  the same space because it's dangerous for the most part. Robots, you know, because especially
*  robots that have to be strong enough to move something heavy, for example, they can really
*  hurt somebody. And so until now, a lot of the warehouse robots had to just move along like
*  preexisting lines, which really restricts what you can do. Um, and so having, I think that that's,
*  that's one of the big challenges and one of the big, like exciting things that's happening is that
*  we're starting to see more cobotics in industrial spaces like that, where people and robots can work
*  side by side and not get harmed. Yeah. That's what people don't realize.
*  Sort of the physical manipulation task with, with humans. It's not that the robots want to hurt you.
*  I think that's what people are worried about. Like this malevolent robot gets them out of its own and
*  wants to destroy all humans. No, it's, you know, it's actually very difficult to know where the
*  human is and to, to respond to the human and dynamically and collaborate with them on a task,
*  especially if you're something like an industrial robotic arm, which is extremely powerful.
*  See this, some of the, some of those arms are pretty impressive now that you can just,
*  you can, you can grab it, you can move it. So the, the collaboration between human robot in the
*  factory setting is really fascinating. Yeah. Do you think they'll take our jobs?
*  I don't think it's that simple. I think that there's a ton of disruption that's happening
*  and will continue to happen. You know, I think speaking specifically of the Amazon warehouses,
*  that might be an area where it would be good for robots to take some of the jobs that are,
*  you know, where people are put in a position where it's unsafe and they're treated horribly.
*  And, you know, probably it would be better if a robot did that. And Amazon is clearly trying to
*  automate that job away. So I think there's going to be a lot of disruption. I do think that
*  robots and humans have very different skillsets. So while a robot might take over a task,
*  it's not going to take over most jobs. I think just things will change a lot. Like, I don't know,
*  one of the examples I have in the book is mining. So there you have this job that is very unsafe
*  and that requires a bunch of workers and puts them in unsafe conditions. And now you have all these
*  different robotic machines that can help make the job safer. And as a result, now people can sit in
*  these like air conditioned remote control stations and like control these autonomous mining trucks.
*  And so that's a much better job, but also they're employing less people now. So it's just a lot of,
*  I think from a bird's eye perspective, you're not going to see job loss. You're going to see
*  more jobs created because the future is not robots just becoming like people and taking their jobs.
*  The future is really a combination of our skills and then the supplemental skillset that robots have
*  to increase productivity, to help people have better, safer jobs, to give people work that they
*  actually enjoy doing and are good at. But it's really easy to say that from a bird's eye perspective
*  and ignore kind of the rubble on the ground as we go through these transitions, because of course,
*  specific jobs are going to get lost. If you look at the history of the 20th century,
*  it seems like automation constantly increases productivity and improves the average quality of
*  life. So it's been always good. So like thinking about this time being different is that it would
*  need to go against the lessons of history. It's true. And the other thing is I think people
*  think that the automation of the physical tasks is easy. I was just in Ukraine and the interesting
*  thing is, I mean, there's a lot of difficult and dark lessons just about a war zone. But one of the
*  things that happens in war is there's a lot of mines that are placed. That's one of the big
*  problems for years after a war is even over, is the entire landscape is covered in mines.
*  And so there's a demining effort. And you would think robots would be good at this kind of thing,
*  or like your intuition would be like, well, say you have unlimited money and you want to do a good
*  job of it, unlimited money, you would get a lot of really nice robots. But no, humans are still
*  far superior. Or animals. Or animals. But humans with animals together. You can't just have a dog
*  with a hat. That's fair. But yes. But figuring out also how to disable the mine. Obviously,
*  the easy thing, the thing a robot can help with is to find the mine and blow it up.
*  But that's going to destroy the landscape. That really does a lot of damage to the land. You want
*  to disable the mine. And to do that because of all the different edge cases of the problem. It
*  requires a huge amount of human-like experience, it seems like. So it's mostly done by humans.
*  They have no use for robots. They don't want robots.
*  Yeah. I think we overestimate what we can automate.
*  Especially in the physical realm.
*  Yeah.
*  It's weird. I mean, it's continuous. The story of humans, we think we're shitty at everything in the
*  physical world, including driving. We think everybody makes fun of themselves and others for
*  being shitty drivers, but we're actually kind of incredible.
*  No, we're incredible. And that's why Tesla still says that if you're in the driver's seat, you
*  are ultimately responsible. Because the ideal for, I mean, you know more about this than I do, but
*  robot cars are great at predictable things and can react faster and more precisely than a person
*  and can do a lot of the driving. And then the reason that we still don't have autonomous
*  vehicles on all the roads yet is because of this long tail of just unexpected occurrences where
*  a human immediately understands that's a sunset and not a traffic light. That's a horse and carriage
*  ahead of me on the highway. And then the other thing is that we're not going to be able to
*  drive on the highway, but the cars never encountered that before. So like in theory,
*  combining those skill sets is what's going to really be powerful. The only problem is figuring
*  off the, figuring out the human robot interaction and the handoffs. So like in cars, that's a huge
*  problem right now, figuring out the handoffs. But in other areas, it might be easier. And that's
*  really the future is human robot interaction.
*  It's really hard to improve. It's terrible that people die in car accidents, but I mean, it's
*  like 70, 80, a hundred million miles, one death per 80 million miles. That's like really hard
*  to beat for a robot. That's like incredible. Like think about it. Like the, how many people,
*  just the number of people throughout the world that are driving every single day, all of this,
*  Steve deprived, drunk, distracted, all of that, and still very few die relative to what I would
*  imagine. If I were to guess back in the horse, see when I was like in the, in the beginning of the
*  20th century riding my horse, I would talk so much about these cars. I'd be like, this is going to,
*  this is extremely dangerous. These machines traveling at 30 miles an hour, whatever the
*  hell they're going at, this is irresponsible. It's unnatural and it's going to be destructive
*  to all of human society. But then it's extremely surprising how humans adapt to the thing. And they
*  know how to not kill each other. I mean, that ability to adapt is incredible. And to mimic
*  that in the machine is really tricky. Now that said, what Tesla is doing, I mean, I wouldn't
*  have guessed how far machine learning can go on vision alone. It's really, really incredible.
*  And people that are, at least from my perspective, people that are kind of, you know, critical of
*  Elon and those efforts, I think don't give enough credit to how much progress we've made,
*  how much incredible progress has been made in that direction. I think most of the robotics
*  community wouldn't have guessed how much you can do on vision alone. It's kind of incredible.
*  Because we would be, I think it's that approach, which is relatively unique,
*  has challenged the other competitors to step up their game. So if you're using LIDAR, if you're
*  using mapping, that challenges them to do better, to scale faster, and to use machine learning and
*  computer vision as well to integrate both LIDAR and vision. So it's kind of incredible. And I'm
*  not, I don't know if I even have a good intuition of how hard driving is anymore. Maybe it is
*  possible to solve. So all the sunset, all the edge cases you mentioned. Yeah, the question is when.
*  Yeah, I think it's not happening as quickly as people thought it would,
*  because it is more complicated. But I wouldn't have, I agree with you. My current intuition is that
*  we're going to get there. I think we're going to get there too. But I didn't before, I wasn't sure
*  we're going to get there without like with current technology. So, you know, I was kind of,
*  this is like with vision alone, my intuition was you're going to have to solve
*  common sense reasoning. You're going to have to solve some of the big problems in artificial
*  intelligence, not just perception. Yeah. Like you have to have a deep understanding of the world,
*  is my sense. But now I'm starting to like, well, this, I mean, I'm continuously surprised how well
*  the thing works. Yeah. Obviously Elon and others, others have stopped, but Elon continues,
*  you know, saying we're going to solve it in a year. Yeah, that's the thing. Bold predictions.
*  Yeah. But everyone else used to be doing that, but they kind of like, all right,
*  maybe let's not promise we're going to solve level four driving by 2020. Let's, let's chill on that.
*  But people are still trying silently. I mean, the UK just committed a hundred million pounds to
*  research and development to speed up the process of getting autonomous vehicles on the road. Like
*  everyone is, everyone can see that it is solvable and it's going to happen and it's going to change
*  everything and they're still investing in it. And like Waymo Loki has driverless cars in,
*  uh, in Arizona. Like you can get, you know, there's like robots. It's weird. Have you ever been in one?
*  No, it's so weird. It's so awesome. Cause the most awesome experience is the wheel turning
*  and you're sitting in the back. It's like, I don't know. It's, um,
*  it feels like you're a passenger with that friend who's a little crazy of a driver.
*  It feels like shit. I don't know. Are you ready to drive, bro? You know, that kind of feeling. But,
*  but then you kind of, that experience, that nervousness, um, and the excitement of trusting
*  another being. And in this case, it's a machine is really interesting. Um, just even introspecting
*  your own feelings about the thing. Yeah. They're not doing anything in terms of
*  making you feel better. Like at least Waymo, I think they went with the approach of like,
*  let's not try to put eyes on the thing. Let's it's, it's a, it's a wheel. We know what that
*  looks like. It's just a car. It's a car. Get in the back. Let's not like discuss this at all.
*  Let's not discuss the fact that this is a robot driving you and you're in the back.
*  And if the robot wants to start driving 80 miles an hour and run off a bridge, you have no recourse.
*  Let's not discuss this. You're just getting the back. There's no discussion about like how
*  shit can go wrong. Uh, there's no eyes. There's nothing. There's like a map showing what the car
*  can see. Like, you know, what happens if it's like a, a Hal 9,000 situation? Like, I'm like,
*  I'm sorry. I can't, you have a button. You can like call customer service. Oh God. Then you
*  get put on hold for two hours. Probably. Um, but you know, currently what they're doing, which I think
*  is understandable, but you know, the car just can pull over and stop and wait for help to arrive.
*  And then a driver will come and then they'll actually drive the car for you. But that's like,
*  you know, what if you're late for a meeting or all that kind of stuff? Or like the more dystopian,
*  isn't that the fifth element where it's Will Smith in that movie? Who's in that movie? No, Bruce
*  Willis. Oh yeah. And he gets into like a robotic cab or car or something. And then because he's
*  violated a traffic rule, it locks him in and he has to wait for the cops to come and he can't get
*  out. So like, we're going to see stuff like that. Maybe. Well, that's, I, I, I believe that the
*  companies that have robots, the only ones that will succeed are the ones that don't do that,
*  meaning they respect privacy. You think so? Yeah. Because people, cause cause they're going to have
*  to earn people's trust. Yeah. But like Amazon works with law enforcement and gives them the
*  data from the ring cameras. So why should it? Yeah. Oh yeah. Do you have a ring camera?
*  Uh, no. Okay. No, no. But basically any security camera, right? I have a Google's, whatever they
*  have. We have one that's not the data. We store the data on a local server because we don't
*  want it to go to law enforcement because all the companies are doing it. They're doing it. I bet
*  Apple wouldn't. Yeah. The only company I trust and I don't know how much longer.
*  I don't know. I, maybe that's true for cameras, but with robots, people are just not going to let
*  a robot inside their home where like one time where somebody gets arrested because of something
*  a robot sees that's going to be, that's going to destroy a company. You don't think people are
*  going to be like, well, that wouldn't happen to me. That happened to a bad person.
*  Yeah. Because in the modern world, people I get, have you seen Twitter? They get extremely
*  paranoid about any kind of surveillance. But the thing that I've had to learn is that Twitter is
*  not the modern world. Like when I go, you know, inland to visit my relatives, like they don't,
*  that that's a different discourse that's happening. I think like the whole tech criticism world,
*  like, you know, like, you know, like, you know, like, you know, like, you know, like, you know,
*  the whole tech criticism world. Yeah. It's loud in our ears because we're in those circles.
*  Do you think you can be a company that does social robotics and not win over Twitter?
*  That's a good question. I feel like the early adopters are all on Twitter and it feels like
*  you have to win them over. It feels like nowadays you'd have to win over TikTok. Honestly, I don't
*  TikTok. Is that, is that a website? I need to check it out.
*  And that's an interesting one because China is behind that one. Exactly.
*  So it's compelling enough. Maybe people would be able to give up privacy and that kind of stuff.
*  That's really scary. I mean, I'm worried about it. I'm worried about it. And I'm
*  there've been some developments recently that are like super exciting, like the large
*  language learning models. Like, wow, I did not anticipate those improving so quickly.
*  And those are going to change everything. And one of the things that I'm trying to be cynical about
*  is that I think they're going to have a big impact on privacy and data security and like
*  manipulating consumers and manipulating people, because suddenly you'll have these agents that
*  people will talk to and they won't care or won't know, at least on a conscious level,
*  that it's recording the conversations. So kind of like we were talking about before.
*  And at the same time, the technology is so freaking exciting that it's going to get adopted.
*  Well, it's not even just the collection of data, but the ability to manipulate
*  it.
*  At scale. So what do you think about the AI, the engineer from Google that thought lambda
*  is sentient yet actually really good post from somebody else. I forgot her name is brilliant.
*  I can't believe I didn't know about her. Thanks. Yeah. For weird AI. Oh yeah. I love her book.
*  Oh, she's great. I left a note for myself to reach out to her. She's amazing. She's hilarious
*  and brilliant and just a great summarizer of the state of AI. But she has, I think that was from
*  her where I was looking at AI explaining that it's a squirrel. Oh yeah. Because the transcripts that
*  the engineer released lambda kind of talks about the experience of human like feelings and I think
*  even consciousness. And so she was like, Oh cool. That's impressive. I wonder if an AI can also
*  describe the experience of being a squirrel. And so she interviewed, I think she did GPT-3
*  about the experience of being a squirrel. And then she did a bunch of other ones too. Like
*  what's it like being a flock of crows? What's it like being an algorithm that powers a Roomba? And
*  like, yeah, you can have a conversation about any of those things and they're very convincing.
*  Yeah. Even GPT-3, which is not like state of the art. It's convincing of being a squirrel.
*  It's like, what, what it's like, you should check it out. Cause it really is. It's like, yeah,
*  that's probably is what a squirrel would, would say. Are you excited? Like what's it like being
*  a squirrel? It's fun. I get to eat nuts and run around all day. Like how do you think people will
*  feel like when you tell them that you're a squirrel, you know, or like, I forget what it
*  was. Like a lot of people might be scared to find out that you're a squirrel or something like this.
*  And then the system answers pretty, like pretty well. Like, yeah, I hope they'll like, what do
*  you think that when they find out you're a squirrel, I hope they'll see how fun it is to be a squirrel.
*  What do you say to people who don't believe you're a squirrel? I say, come see for yourself.
*  I am a squirrel. That's great. Well, I think it's really great because it, it like,
*  the two things to note about it are first of all, just because the machine is describing
*  an experience doesn't mean it can, it actually has that experience. But then secondly, these things
*  are getting so advanced and so convincing at describing these things and talking to people.
*  That's, I mean, just the implications for health, education, communication, entertainment,
*  gaming. Like I just, I like all of the applications. It's mind boggling what we're going to be able to
*  do with this and, and that my kids are not going to remember a time before they could have
*  conversations with artificial agents. Do you think they would, because to me, this is,
*  the focus in the AI community has been, well, this engineer
*  surely is hallucinating. The thing is not sentient, but to me, I, first of all,
*  it doesn't matter if he is or not. This is coming where a large number of people would believe a
*  system of sentient, including engineers within companies. So in that sense, you start to think
*  about a world where like your kids aren't just used to having a conversation with the bot,
*  but used to believing kind of having an implied belief that the thing is sentient.
*  Yeah, I think that's true. And I think that one of the things that bothered me about all of the
*  coverage in the tech press about this incident, like obviously I don't believe the system is
*  sentient. Like I think that it can convincingly describe that it is. I don't think it's doing
*  what he thought it was doing and actually experiencing feelings, but a lot of the tech
*  press was about how he was wrong and depicting him as kind of naive. And it's not naive. Like
*  there's so much research in my field showing that people do this, even experts, they might be very
*  clinical when they're doing human robot interaction experiments with a robot that they've built.
*  And then you bring in a different robot and they're like, Oh, look at it. It's having fun.
*  It's doing this. Like that happens in our lab all the time. We are all this guy and it's gonna
*  be huge. So I think that the goal is not to discourage this kind of belief or like design
*  systems that people won't think are sentient. I don't think that's possible. I think you're right.
*  This is coming. It's something that we have to acknowledge and even embrace and be very aware of.
*  So one of the really interesting perspectives that your book takes on a system like this
*  is to see them, not to compare a system like this to humans, but to compare it to animals
*  of how we see animals. Can you kind of try to again, sneak up, try to explain why this
*  analogy is better than the human analogy, the analogy of robots as animals?
*  Yeah. And it gets trickier with the language stuff, but we'll get into that too.
*  I think that animals are a really great thought experiment when we're thinking about AI and
*  robotics, because again, this comparing them to humans that leads us down the wrong path,
*  both because it's not accurate, but also I think for the future, we don't want that.
*  We want something that's a supplement, but I think animals, because we've used them throughout
*  history for so many different things, we domesticated them not because they do what we do,
*  but because what they do is different and that's useful. And it's just like whether we're talking
*  about companionship, whether we're talking about work integration, whether we're talking about
*  responsibility for harm, there's just so many things we can draw on in that history from these
*  entities that can sense, think, make autonomous decisions and learn that are applicable to how
*  we should be thinking about robots and AI. And the point of the book is not that they're the same
*  thing, that animals and robots are the same. Obviously there are tons of differences there.
*  You can't have a conversation with a squirrel, right? But the point is-
*  I do it all the time.
*  Oh, really?
*  By the way, squirrels are the cutest. I project so much on squirrels. I wonder what their inner
*  life is. I suspect they're much bigger assholes than we imagine.
*  Really?
*  Like if it was a giant squirrel, it would fuck you over so fast. If you had the chance,
*  it would take everything you own. It would eat all your stuff because it's small. And the furry
*  tail, the furry tail is a weapon against human consciousness and cognition. It wins us over.
*  That's what cats do too. Cats outcompete squirrels.
*  And dogs.
*  Yeah. No, dogs have love. Cats have no soul. They- no, I'm just kidding. People get so angry
*  when I talk shit about cats. I love cats. Anyway, so yeah, you're describing all the
*  different kinds of animals that get domesticated. And it's a really interesting idea that it's not
*  just sort of pets. There's all kinds of domestication going on. They all have all kinds of uses.
*  Yes.
*  Like the ox that you propose might be, at least historically, one of the most useful
*  domesticated animals.
*  It was a game changer because it revolutionized what people could do economically, etc. So,
*  I mean, just like robots, they're going to change things economically. They're going to
*  change landscapes. Cities might even get rebuilt around autonomous vehicles or drones or delivery
*  robots. I think just the same ways that animals have really shifted society and society has adapted
*  also to socially accepting animals as pets. I think we're going to see very similar things
*  with robots. So I think it's a useful analogy. It's not a perfect one, but I think it helps
*  us get away from this idea that robots can, should, or will replace people.
*  If you remember, what are some interesting uses of animals? Ferrets, for example.
*  Oh yeah, the ferrets. They still do this. They use ferrets to go into narrow spaces that people
*  can't go into, like a pipe, or they'll use them to run electrical wire. I think they did that for
*  Princess Di's wedding. There's so many weird ways that we've used animals and still use animals
*  for things that robots can't do, like the dolphins that they used in the military. I think Russia
*  still has dolphins and the US still has dolphins in their navies. Mind detection, looking for lost
*  underwater equipment, some rumors about using them for weaponry, which I think Russia's like,
*  sure, believe that. And America's like, no, no, we don't do that. Who knows? But they started
*  doing that in the 60s, 70s. They started training these dolphins because they were like, oh,
*  dolphins have this amazing echolocation system that we can't replicate with machines and they're
*  trainable, so we're going to use them for all the stuff that we can't do with machines or by
*  ourselves. And they've tried to phase out the dolphins. I know the US has invested
*  a lot of money in trying to make robots do the mind detection, but like you were saying,
*  there are some things that the robots are good at and there's some things that biological creatures
*  are better at, so they still have the dolphins. So there's also pigeons, of course. Oh yeah,
*  pigeons. Oh my gosh, there's so many examples. The pigeons were the original hobby photography drone.
*  They also carried mail for thousands of years, letting people communicate with each other in new
*  ways. So the thing that I like about the animal analogy is they have all these physical abilities,
*  but also sensing abilities that we don't have. And that's just so useful. And that's robots,
*  right? Robots have physical abilities. They can help us lift things or do things that we're not
*  physically capable of. They can also sense things. I just feel like it's a really good
*  analogy and it works because people are familiar with it. What about companionship? And when we
*  start to think about like cats and dogs and pets that seem to serve no purpose whatsoever, except
*  the social connection. Yeah, I mean, it's kind of a newer thing. At least in the United States,
*  like dogs used to have, like they used to have a purpose. They used to be guard dogs or they had
*  some sort of function. And then at some point they became just part of the family. And it's
*  so interesting how there's some animals that we've treated as workers, some that we've treated as
*  objects, some that we eat and some that are parts of our families. And that's different across
*  cultures. And I'm convinced that we're going to see the same thing with robots, where people are
*  going to develop strong emotional connections to certain robots that they relate to, either
*  culturally or personally, emotionally. And then there's going to be other robots that we don't
*  treat the same way. I wonder, does that have to do more with the culture and the people or the robot
*  design? Is there interplay between the two? Like why did dogs and cats outcompete ox and, I don't
*  know, what else? Like farm animals to really get inside the home and get inside our hearts.
*  Yeah. I mean, people point to the fact that dogs are very genetically flexible and they can evolve
*  much more quickly than other animals. And so evolutionary biologists think that dogs evolved
*  to be more appealing to us. And then once we learned how to breed them, we started breeding
*  them to be more appealing to us too, which is not something that we necessarily would be able to do
*  with cows, although we've bred them to make more milk for us. But part of it is also culture. I
*  mean, there are cultures where people eat dogs still today. And then there's other cultures where
*  we're like, oh, no, that's terrible. We would never do that. And so I think there's a lot of
*  different elements that play in. I wonder if there's good, because I understand dogs, because they
*  use their eyes, they're able to communicate affection, all those kinds of things. It's
*  really interesting what dogs do. There's a whole conferences on dog consciousness and cognition and
*  all that kind of stuff. Now cats is a mystery to me because they seem to not give a shit about the
*  human, but they're warm and fluffy. But they, but they're also passive aggressive. So they're at the
*  same time, they're like, they're dismissive of, of you in some sense. I think some people like
*  that about people. Yeah, they want, they want to push and pull over a relationship. They don't want
*  loyalty or unconditional love. That does, that means they haven't earned it. Yeah. Yeah.
*  And maybe that says a lot more about the people than it does about the animal. Oh yeah. We all
*  need therapy. Yeah. So I'm judging harshly the people that have cats or the people that have
*  dogs. Maybe the people that have dogs need, are desperate for attention and unconditional love
*  and they're unable to, to, to sort of struggle to earn meaningful connections.
*  I don't know. Maybe people are talking about you and your robot pets in the same way.
*  Yeah, that's, it is kind of sad. There's just a robots everywhere, but it is, I mean,
*  I'm joking about it being sad because I think it's kind of beautiful. I think robots are
*  beautiful in the same way that pets are even children in that like they capture some kind
*  of magic of social robots. They have the capacity to have the same kind of magic of connection.
*  I don't know what that is. Like when they're brought to life and they move around,
*  the way they make me feel, I'm pretty convinced is as, as you know, they will make billions of
*  people feel like, I don't think I'm like some weird robotics guy. I'm not, I mean, you are,
*  but not in this way, not in this way. I mean, I just, I can put on my like normal human hat
*  hat and just see this. Oh, this is like, there's a lot of possibility there of something cool,
*  just like with dogs. What is it? Why are we so into dogs or cats? Like it's like that. It's
*  way different than us. It is. It's like drooling all over the place with its tongue out. It's like,
*  what it's like a weird creature that used to be a wolf. Why are we into this thing?
*  Well, dogs can either express or mimic a lot of emotions that we recognize.
*  And I think that's a big thing. Like a lot of the magic of animals and robots is
*  our own self projection. And the easier it is for us to see ourselves in something and project
*  human emotions or qualities or traits onto it, the more we'll relate to it. And then you also
*  have the movement. Of course, I think that's also really, that's why I'm so interested in
*  physical robots, because that's, I think the visceral magic of them. I think we're, I mean,
*  there's research showing that we're probably biologically hardwired to respond to autonomous
*  movement in our physical space because we've had to watch out for predators or whatever the reason
*  is. And so animals and robots are very appealing to us as these autonomously moving things that we
*  view as agents instead of objects. I love the moment, which is I've been particularly working
*  on, which is when a robot like the cowboy hat is doing its own thing and then it recognizes you.
*  I mean, the way dog does and it looks like this and the, the moment of recognition, like you're
*  walking, say you're working at an airport on the street and there's just, you know,
*  hundreds of strangers, but then you see somebody, you know, and that like,
*  where you wake up to like that excitement of seeing somebody, you know, and saying hello and
*  all that kind of stuff. That's a magical moment. Like, I think, especially with the dog,
*  it makes you feel noticed and heard and, and loved. Like that somebody looks at you and
*  recognizes you that, that it matters that you exist. Yeah. You feel seen. Yeah. And that's a
*  cool feeling. And I honestly think robots can give that feeling. Oh yeah. Totally. Currently Alexa,
*  I mean, one of the downsides of these systems is they don't, they're servants. They like,
*  part of the, you know, they're trying to maintain privacy, I suppose. But I don't feel seen with
*  Alexa. Right. I think that's going to change. I think you're right. And I think that that's,
*  that's the game changing, changing nature of things like these large language learning models
*  and the fact that these companies are investing in embodied versions that move around of Alexa,
*  like Astro. Can I just say, yeah, I haven't, is that out? I mean, it's out. You can't just like
*  buy one commercially yet, but you can apply for one. Yeah. My gut says that these companies
*  don't have the guts to do the personalization. This goes to the, because it's edgy, is dangerous.
*  It's going to make a lot of people very angry. Like in the way that, you know, just imagine,
*  okay. All right. If you do the full landscape of human civilization, just visualize the number
*  of people that are going through breakups right now, just the amount of really passionate,
*  Steven, if we just look at teenagers, the amount of deep heartbreak that's happening. And like,
*  if you're going to have Alexa have more of a personal connection with the human, you're going
*  to have humans that like have existential crises. There's a lot of people that suffer
*  from loneliness and depression. And like, you're now taking on the full responsibility
*  of being a companion to the, the, the rollercoaster of the human condition as a company.
*  And you can imagine PR and marketing people, they're going to freak out. They don't have the guts.
*  It's going to have to come from somebody from a new Apple, from those kinds of folks, like a small
*  startup. And it might. Yeah. Like they're coming. There's already virtual therapists.
*  There's that replica app. I haven't tried it, but replicas like a great whole companion, like
*  it's coming. And if big companies don't do it, someone else will.
*  Yeah. I think the next, the future, the next trillion dollar company will be those personalization.
*  Cause if you think, um, if you think about all the, the AI we have around us, all the,
*  the smart phones and so on, there's very minimal personalization.
*  You don't think that's just because they weren't able.
*  Really? I don't think they have the guts. I mean, it might be true, but I have to wonder.
*  I mean, Google is clearly going to do something with the length. I mean, they don't have the cut.
*  Are you challenging them? Uh, partially, but not really. Cause I know they're not going to do it.
*  They, they don't have to, it's bad for business in the short term. I'm going to be honest, like,
*  maybe it's not such a bad thing if they don't just like roll this out quickly, because I do
*  think there are huge issues. And, and there's not just issues with like the responsibility of like
*  unforeseen effects on people, but what's the business model? And if you are using the business
*  model that you've used in other domains, then you're going to have to collect data from people,
*  which you will anyway, to personalize the thing. And you're going to be somehow monetizing the data
*  or you're going to be doing some like ad model. It just, it seems like now we're suddenly getting
*  into the realm of like severe consumer protection issues. And I'm, I'm really worried about that.
*  I see massive potential for this technology to be used in a way that's not for the public good and
*  not, I mean, that's in an individual user's interest, maybe, but not in society's interest.
*  Yeah. See, I think, I think that kind of personalization should be
*  like redefine how we treat data. I think you should own all the data your phone knows about you
*  like, and be able to delete it with a single click and walk away. And that data cannot be monetized
*  or used or shared anywhere without your permission. I think that's the only way people will trust.
*  You should give for you to use that data. But then how are companies going to, I mean,
*  a lot of these applications rely on massive troves of data to train the AI system.
*  Right. So you have to opt in constantly and opt in not in some legal. I agree. But obvious,
*  exactly like in the way I opt in to tell you a secret, like we understand like that, like I have
*  to choose like, how well do I know you? And then I say like, don't tell this to anyone.
*  And then I have to judge how leaky that like, how good you are. I keep your secrets in that same way.
*  It's very transparent in which data you're allowed to use for which purposes.
*  That's what people are saying is the solution. And I think that works to some extent,
*  having transparency, having people consent. I think it breaks down at the point at which
*  we've seen this happen on social media too. Like people are willingly giving up their data because
*  they're getting a functionality from that. And then the harm that that causes is on a,
*  like maybe to someone else and not to them personally. So I don't think people are given
*  their data. They're not being asked. Like, but if, if you were, if you were like,
*  tell me a secret about yourself and I'll give you a hundred dollars, I'd tell you a secret.
*  No, not a hundred dollars. First of all, you wouldn't, you wouldn't trust like why you gave
*  me a hundred dollars. But like I need, I would ask for your specific like fashion
*  interest in order to give recommendations to you for shopping. And I'd be very clear for that.
*  And you could disable that. You can delete that, but then you can be, have a deep, meaningful,
*  rich connection with the system about what you think you look fat in, what you look great in,
*  what, like the full history of all the things you've worn, whether you regret the Justin Bieber or
*  enjoy the Justin Bieber shirt, all of that information that's mostly private to even you,
*  not even your loved ones that a system should have that. Cause then a system, if you trust it
*  to keep control of that data that you own, you can walk away with that system could tell you a
*  damn good thing to wear. It could. And the harm that I'm concerned about is not that the system
*  is going to then suggest a dress for me that is based on my preferences. So I, I went to this
*  conference once where I was talking to the people who do the analytics and like the big ad companies
*  and like literally a woman there was like, I can ask you three totally unrelated questions and tell
*  you what menstrual product you use. And so what they do is they aggregate the data and they map
*  out different personalities and different people and demographics. And then they have a lot of power
*  and control to market to people. So like, I might not be sharing my data with any of the systems
*  because I'm like, I'm on Twitter. I know that this is bad. Other people might be sharing data that
*  can be used against me. Like it's, I think it's, it's way more complex than just.
*  I share a piece of personal information and it gets used against me. I think that at a more
*  systemic level and then it's always, you know, vulnerable populations that are targeted by this,
*  um, you know, low income people being targeted for scamming loans or I don't know, like I could get
*  targeted, like someone, not me, because I have someone who doesn't have kids yet and is my age
*  could get targeted for like freezing their eggs. And there's all these ways that you can manipulate
*  people where it's not really clear that that came from that person's data. It came from all of us,
*  all of us opting into this, but there there's a bunch of sneaky decisions along the way that
*  could be avoided if there's transparency. So that, so one of the ways that goes wrong,
*  if you share that data with too many ad networks, don't run your own ad network.
*  Don't share with anybody. Okay. And that's data regulate you. That belongs to just you
*  and all the ways you allow the company to use it. The default is in no way at all.
*  And you are consciously constantly saying exactly how to use it. And, um, and also it has to do with
*  the recommender system itself from the company, which is, um, freezing your eggs. If that doesn't
*  make you happy, if that idea doesn't make you happy, then the system shouldn't recommend it
*  and should very, be very good at learning. So not the kind of things that the category of people
*  it thinks you belong to will do, but more you specifically, what makes you happy,
*  what is helping you grow. But you're assuming that people's preferences and like what makes them
*  happy is static. Whereas when we were talking before about how a company like Apple can tell
*  people what they want and they will start to want it. That's the thing that I'm more concerned
*  about. Yeah, that is a huge problem. It's not just listening to people, but manipulating them
*  into wanting something. And that's like, we have a long history of using technology for that purpose,
*  like the persuasive design in casinos to get people to gamble more or like, it's just, I'm,
*  the other thing that I'm worried about is as we have more social technology, suddenly you have this
*  on a new level. Like if you look at the influencer marketing that happens online now, what's the
*  influencer marketing? So like on Instagram, there will be some like person who has a bunch of
*  followers and then a brand will like hire them to promote some product and it's above board.
*  They disclose like I'm, this is an ad that I'm promoting, but they have so many young followers
*  who like deeply admire and trust them. I mean, this must work for you too. Don't you have like
*  ads on the podcast? Like people trust you. Magic spoon cereal, low carb. Yes. If you say that,
*  like I guarantee you some people will buy that just because even though they know that you're
*  being paid, they trust you. Yeah. It's different with podcasts because, well, my particular
*  situation, but it's true for a lot of pockets, especially big ones is, you know, I have
*  10 times more sponsors that want to be sponsors than, than I have. So you get to select the one
*  that you actually want to support. And so like you end up using it and then you're able to actually,
*  like there's no incentive to like, um, show for anybody. Sure. And that's why it's fine when it's
*  still human influencers. Right now, if you're a bot, you're not going to discriminate.
*  You're not going to be like, Oh, well, I think this product is good for people.
*  You think there'll be like bots essentially with millions of followers. There already are.
*  There are virtual influencers in South Korea who show products and, and like, that's just the tip
*  of the iceberg because that's still very primitive now with the new image generation
*  and the language learning models. And like, so we're starting to do some research around kids,
*  and like young adults, because a lot of the research on like, what's okay to advertise
*  to kids and what is too manipulative has to do with television ads back in the day where like
*  a kid who's 12 understands, Oh, that's an advertisement. I can distinguish that from
*  entertainment. I know it's trying to sell me something now. It's getting really, really murky
*  with influencers. And then if you have like a bot that's that a kid has developed a relationship
*  with, is it okay to market products through that or not? Like you're getting into all these consumer
*  protection issues because you're developing a trusted relationship with a social entity.
*  But it's, and so, and so now it's like personalized, it's scalable, it's automated,
*  and it has, it, it can, so some of the research showing that kids are already very confused about
*  like the incentives of the company versus what the robot is doing. Meaning they're,
*  so, okay, they're not deeply understanding the incentives of the, of the, of the system.
*  Well, yeah, so like kids who are old enough to understand this is a television advertisement,
*  it's trying to advertise to me. I might still decide I want this product, but they understand
*  what's going on. So there's some transparency there. That age child. So, um, Daniella DiPaola,
*  Anastasia Ostrovsky, and I advised on this project, they did this, um, they, they asked kids who had
*  interacted with social robots, whether they would like a policy that allows robots to
*  market to people through casual conversation or whether they would prefer that it has to be
*  transparent, that it's like an ad coming from a company. And the majority said they preferred
*  the casual conversation. And when asked why, there was a lot of confusion about, they were like,
*  well, the robot knows me better than the company does. So the robot's only going to market things
*  that I like. And so they don't really, they're not connecting the fact that the robot is an agent of
*  the company. They're viewing it as something separate. And I think that even happens
*  subconsciously with grownups when it comes to robots and artificial agents. And it will, like
*  this Blake guy at Google started going on and on, but like his main concern was that Google owned
*  this sentient agent and that it was being mistreated. His concern was not that the agent was
*  going to mistreat people. So I think we're going to see a lot of this.
*  Yeah, but shitty companies will do that. I think ultimately that confusion should be alleviated by
*  the robot should actually know you better and should not have any control from the company.
*  But what's the business model for that? If you use the robot to buy, first of all,
*  the robot should probably cost money. Should what? Cost money. Like the way windows operating
*  system does. I see it more like an operating system than like this thing is your window,
*  no pun intended, into the world. So it's helping you as like a personal assistant,
*  right? And so that should cost money. You should, you know, whatever it is, 10 bucks, 20 bucks.
*  Like that's the thing that makes your life significantly better. This idea that everything
*  should be free is like it should actually help educate you. You should talk shit about all the
*  other companies that do stuff for free. But also, yeah, in terms of if you purchase stuff based on
*  its recommendation, it gets money. So it's kind of ad driven, but it's not ads. It's like,
*  um, it's not controlled. Like no, no external entities can control it
*  to try to manipulate, to want a thing. That would be amazing. It's actually trying to
*  discover what you want. So it's not allowed to have any influence, no promoted ad, no anything.
*  So that's finding, I don't know, the, uh, the, the thing that would actually make you happy.
*  That's the only thing it cares about. I think, I think companies like this can win out.
*  Yes. I think eventually once people understand the value of the robot, even just like, I think that
*  robots would be valuable to people, even if they're not marketing something or helping with like
*  preferences or anything, like just a simple, the same thing as a pet, like a dog that has no
*  function other than being a member of your family. I think robots could really be that and people
*  would pay for that. I don't think the market realizes that yet. And so my concern is that
*  companies are not going to go in that direction, at least not yet of making like this contained
*  thing that you buy. It seems almost old fashioned, right? To have a disconnected
*  uh, object that you buy that you're not like paying a subscription for. It's not like controlled by
*  one of the big corporations. But that's the old fashioned things that people, uh, uh, yearn for
*  because I think it's very popular now and people understand the negative effects of social media,
*  the negative effects of the data being used in all these kinds of ways. I think, I think we're
*  just waking up to the realization we tried, but like we're like baby deer finding our legs in this
*  new world of social media, of ad driven companies and realizing, okay, this has to be done somehow
*  different. Like one of the most popular notions, at least in the United States of social media is
*  evil and is doing bad. It's doing bad by us. It's not like it's totally tricked us into
*  believing that is good for us. I think everybody knows is bad for us. And so like there's a hunger
*  for other ideas. All right. It's time for us to start that company. Let's do it. Let's go.
*  Hopefully no one listens to this and steals the idea. There's no, see, that's the other thing. I
*  think I'm a big person on, um, execution is what matters. I mean, it's like ideas are kind of true.
*  The social robotics is a good example that there's been so many amazing companies that
*  went out of business. I mean, to me it's obvious. Like it's obvious that there will be a robotics
*  company that puts a social robot on the home of billions of homes. Yeah. And it'll be a companion.
*  Okay. There you go. You can steal that idea. Do it. Okay. I have a question for you.
*  What about Elon Musk's humanoid? Is he going to execute on that?
*  Yeah, there might be a lot to say. So for people who are not aware, there's an optimist
*  Tesla's optimist robot. That's, um, I guess the stated reason for that robot is a humanoid robot
*  in the factory that's able to automate some of the tasks that humans are currently doing.
*  And the reason you want to do this, the second reason you mentioned the reason you want to
*  do a humanoid robots, because the factory is built for the certain tasks that are, um,
*  designed for humans. So it's hard to automate with any other form factor than a humanoid.
*  And then the other reason is because so much effort has been put into this giant data engine
*  machine of, of perception that's inside Tesla autopilot. That's seemingly at least the machine,
*  if not the data is transferable to the factory setting to any setting.
*  Yeah. He said it would do anything that's boring to us.
*  Yeah. Yeah. The interesting thing about that is there's no interest
*  and no discussion about the social aspect. Like I, I, I, I talked to him on mic and off mic about it
*  quite a bit. And there's not a discussion about like, to me, it's obvious if a thing like that
*  works at all, at all. In fact, it has to work really well in a factory. If it works kind of
*  shitty, it's much more useful in the home. Cause we're much this where I think being shitty at
*  stuff is kind of, um, what makes relationships great. Like you want to be flawed and be able
*  to communicate your flaws and be unpredictable in certain ways. Like if you fell over every
*  once in a while for no reason whatsoever, I think that's essential for, for like,
*  it's charming, but also concerning and also like, uh, like, like, are you okay? Is it,
*  I mean, it's both hilarious. It's whenever somebody you love, like falls down the stairs,
*  it was both hilarious and concerning. It's some, some dance between the two. And I think that's
*  essential for like, um, you almost want to engineer that in, uh, except you don't have to cause of
*  robotics in the physical space is really difficult. So, um,
*  I think I've learned to not discount the, the efforts that Elon does. There's a few things
*  that are really interesting there. One, because he's taken extremely seriously. What I like is
*  the humanoid form, the cost of building a robot. I talked to Jim Keller offline about this a lot,
*  and currently human robots costs a lot of money. And the way they're thinking about it now, they're
*  not talking about all the social robotics stuff that you and I care about. Uh, they are thinking,
*  how can we manufacture this thing cheaply and do it like, well, and the kind of discussions they're
*  having is really great engineering. It's like, it's the big, it's like first principles question of
*  like, why is this cost so much? Like, what's the cheap way? Why can't we build? And there's
*  not a good answer. Uh, why can't we build this humanoid form for like, for like, for like,
*  build this humanoid form for under a thousand dollars? And like I've, I've sat and had these
*  conversations. There's no reason it's, uh, I think the reason they've been so expensive is because,
*  um, they were focused on trying to, they weren't focused on doing the mass manufacturer.
*  There were, uh, people are focused on getting a thing. That's, um, I don't know, I don't know
*  exactly what the reasoning is, but it's the same like Waymo is like, let's, let's build a million
*  dollar car in the beginning or like multi-million dollar car. Let's try to solve that problem.
*  The, the way Elon, the way Jim Keller, the way some of those folks are thinking is
*  let's like at the same time, try to actually build a system that's cheap, not crappy, but cheap.
*  Unless from first principles, what is the minimum amount of degrees of freedom we need?
*  What are the joints? Where's the control set? Like how many, how do we act? Like where are the
*  activators? Uh, what's the way to power this in the lowest cost way possible, but also in a way
*  that's like actually works. How do we make the whole thing not part of the components where there's a
*  supply chain? You have to have all these different parts that have to feed us to do it all from
*  scratch and do the learning. I mean, it's like immediately certain things like become obvious,
*  do, uh, the exact same pipeline as you do for autonomous driving, just the exact, I mean,
*  the infrastructure that is incredible for the computer vision, for the manipulation task,
*  the control problem changes, the perception, uh, problem changes, but the pipeline doesn't change.
*  Do it. And so I don't, I don't, um, obviously the optimism about how long it's going to take,
*  I don't share. Um, but it's a really interesting problem and I don't want to say anything
*  because my first gut is to say that why the humanoid form, that doesn't make sense.
*  Yeah. That's my second gut too, but, but then there's a lot of people that are really excited
*  about the human form there. It's like, I don't want to get in the way like they might solve this
*  thing. And they might, it's like similar with Boston dynamics. Like why, like if I were to,
*  you can be a hater and be, and you go, go up to Mark Ribert and just like, how are you going to
*  make money with these super expensive legged robots? What's, what's your business plan?
*  This doesn't make any sense. Why are you doing these legged robots? But at the same time,
*  they're pushing forward the science, the art of robotics in the way that nobody else does.
*  Yeah. And, uh, with the Ilan, they're not just going to do that. They're going to drive down
*  the cost to where we can have humanoid bots in the home potentially. So the part I agree with is
*  a lot of people find it fascinating and it probably also attracts talent who want to work
*  on humanoid robots. I think it's a fascinating scientific problem and engineering problem,
*  and it can teach us more about human body and locomotion and all of that. I think there's a
*  lot to learn from it. Where I get tripped up is why we need them for anything other than art and
*  entertainment in the real world. Like I get that there are some areas where
*  you can't just rebuild like a spaceship. You can't just like, they've worked for so many
*  years on these spaceships. You can't just re-engineer it. You have some things that are
*  just built for human bodies, a submarine, a spaceship, but a factory, maybe I'm naive,
*  but it seems like we've already rebuilt factories to accommodate other types of robots
*  why would we want to just like make a humanoid robot to go in there? I just get really tripped
*  up on I think that people want humanoids. I think people are fascinated by them. I think it's a
*  little overhyped. Well, most of our world is still built for humanoids. I know, but it shouldn't be.
*  It should be built so that it's wheelchair accessible. Right. So the question is, do you
*  build a world that's the general form of wheelchair accessible, all robot form factor
*  accessible, or do you build humanoid robots? I mean, it doesn't have to be all. And it also
*  doesn't have to be either or. I just feel like we're thinking so little about the system in
*  general and how to create infrastructure that works for everyone, all kinds of people, all kinds
*  of robots. Like that's that. I mean, it's more of an investment, but that would pay off way more
*  in the future than just trying to cram expensive or maybe slightly less expensive humanoid technology
*  into a human. Unfortunately, one company can't do that. We have to work together. It's like
*  autonomous driving can be easily solved if you do a V2I, if you change the infrastructure of cities
*  and so on. But that requires a lot of people, a lot of them are politicians and a lot of them are
*  somewhat, if not a lot, corrupt and all those kinds of things. And the talent thing you mentioned is
*  really, really, really important. I've gotten a chance to meet a lot of folks at SpaceX and
*  Tesla, other companies too, but they're specifically the openness makes it easier to like meet everybody.
*  I think a lot of amazing things in this world happen when you get amazing people together.
*  And if you can sell an idea like us becoming a multi-planetary species,
*  you can say, why the hell would go to Mars? Like why colonize Mars? If you think from basic
*  first principles, it doesn't make any sense. It doesn't make any sense to go to the moon.
*  The only thing that makes sense to go to space is for satellites.
*  But there is something about the vision of the future, the optimism-laden that permeates this
*  vision of us becoming multi-planetary. It's thinking not just for the next 10 years, it's thinking
*  like human civilization reaching out into the stars. It makes people dream. It's really exciting.
*  And that they're going to come up with some cool shit that might not have anything to do with...
*  Here's what I... Because Elon doesn't seem to care about social robotics,
*  which is constantly surprising to me. I talk to him, he doesn't...
*  Humans are the things you avoid and don't hurt. The number one job of a robot is not to hurt a
*  human, to avoid them. The collaborative aspect, the human-robot interaction, I think is not at
*  least not something he thinks about deeply. But my sense is if somebody like that takes on the problem
*  of human robotics, we're going to get a social robot out of it.
*  Like people like... Not necessarily Elon, but people like Elon. If they take on seriously these...
*  I can just imagine with a humanoid robot, you can't help but create a social robot.
*  So if you do different form factors, if you do industrial robotics, you're likely to actually
*  not end up walking head into a social robot, human-robot interaction problem. If you create,
*  for whatever the hell reason you want to, a humanoid robot, you're going to have to
*  reinvent... Well, not reinvent, but introduce a lot of fascinating new ideas into the problem
*  of human-robot interaction, which I'm excited about. So if I was a business person, I would say,
*  this is way too risky. This doesn't make any sense. But when people are really convinced,
*  and there's a lot of amazing people working on it, it's like, all right, let's see what happens here.
*  This is really interesting. Just like with Atlas and Boston Dynamics, I apologize if I'm ignorant
*  on this, but I think they really, more than anyone else, maybe with iBoat, like Sony, pushed forward
*  humanoid robotics, like a leap with the Atlas robot. Oh yeah, with Atlas. Absolutely.
*  And without them, why the hell did they do it? Why? Well, I think for them, it is a research
*  platform. I don't think they ever... The speculation, I don't think they ever intended Atlas to be a
*  commercially successful robot. I think they were just like, can we do this? Let's try.
*  Yeah, I wonder if they... Maybe the answer they landed on is...
*  Because they eventually went to spot the earlier versions of Spot. So Quadrope has a four-legged
*  robot, but maybe they reached for... Let's try to make... I think they tried it, and they still are
*  trying it for Atlas to be picking up boxes, to moving boxes, to being... It makes sense...
*  Okay, if they were exactly the same cost, it makes sense to have a humanoid robot in the warehouse.
*  Currently. Currently. I think it's short-sighted, but yes, currently, yes, it would sell.
*  But it's not... It's short-sighted, but it's not pragmatic to think any other way.
*  To think that you're going to be able to change warehouses. You're going to have to...
*  If you're Amazon, you can totally change your warehouses.
*  Yes. Yes. But even if you're Amazon, that's very costly to change warehouses.
*  It is. It's a big investment.
*  But isn't... Shouldn't you do that investment in a way... So here's the thing. If you build the
*  humanoid robot that works in the warehouse, that humanoid robot... See, I don't know why Tessa's
*  not talking about it this way, as far as I know, but that humanoid robot is going to have all kinds
*  of other applications outside their setting. To me, it's obvious. I think it's a really hard
*  problem to solve, but whoever solves the humanoid robot problem are going to have to solve the social
*  robotics problem. Oh, for sure. I mean, they're already with the spot, needing to solve social
*  robotics problem. For like for spot to be effective at scale. I'm not sure spot is
*  currently effective at scale. It's getting better and better, but they're actually...
*  The thing they did is an interesting decision. Perhaps Tessa will end up doing the same thing,
*  which is spot is supposed to be a platform for intelligence. So spot doesn't have any
*  high level intelligence, like high level perception skills. It's supposed to be
*  controlled remotely. And it's a platform that you can attach something to. Yeah. And somebody
*  else is supposed to do the attaching. It's a platform that you can take an uneven ground
*  and it's able to maintain balance, go into dangerous situations. It's a platform. On top of that,
*  you can add a camera that does surveillance, that you can remotely monitor. You can record.
*  You can record the camera. You can remote control it, but it's not going to manipulation,
*  basic object manipulation, but not autonomous object manipulation. It's remotely controlled.
*  But the intelligence on top of it, which was what would be required for automation,
*  somebody else is supposed to do. Yeah. Perhaps that's what would do the same thing ultimately,
*  but it doesn't make sense because the goal of optimist is automation. Without that,
*  um, but then you never know. He's like, why go to Mars? Why, why, um,
*  I mean, that's true. And I, I reluctantly like, I'm very excited about space travel. Um,
*  why, why can you introspect like, why, why am I excited about it? I think what got me excited was
*  I saw a panel with some people who, um, study other planets and it became really clear
*  how little we know about ourselves and about how nature works and just how much
*  there is to learn from exploring other parts of the universe.
*  So like on a rational level, that's how I convinced myself that that's why I'm excited. In reality,
*  it's just fucking exciting. I mean, just like the idea that we can do this difficult thing
*  and that humans come together to build things that can explore space. I mean, there's just
*  something inherently thrilling about that. And I'm reluctant about it because I feel like there are
*  so many other challenges and problems that I think are more important to solve, but I also
*  think we should be doing all of it at once. And so to that extent, I'm like all for research
*  on humanoid robots, development of humanoid robots. I think that there's a lot to explore
*  and learn and it doesn't necessarily take away from other areas of science. At least it shouldn't.
*  I think unfortunately, a lot of the attention goes towards that and it does
*  take resources and attention away from other areas of robotics that we should be focused on,
*  but I don't think we shouldn't do it.
*  So you think it might be a little bit of a distraction? I'll forget the Elon particular
*  application, but if you care about social robotics, the humanoid form is a distraction.
*  It's a distraction and it's one that I find particularly boring. It's interesting from a
*  research perspective, but from what types of robots can we create to put in our world? Why
*  would we just create a humanoid robot? Even just robotic manipulation, so arms is not useful either.
*  Oh, arms can be useful, but why not have three arms? Why does it have to look like a person?
*  Well, I actually personally just think that washing the dishes is
*  harder than a robot that can be a companion. Being useful in the home is actually really tough.
*  But does your companion have to have two arms and look like you?
*  I'm making the case for zero arms.
*  Oh, okay. Zero arms. Freaky.
*  That didn't come out the way I meant it because it almost sounds like I don't want a robot
*  to defend itself. That's immediately you project. I just think that the social component doesn't
*  require arms or legs or so on, as we've talked about. I think that's probably where a lot of
*  the meaningful impact that's going to be happening. Yeah. I think we could get so
*  creative with the design. Why not have a robot on roller skates? Whatever. Why does it have to
*  look like us? Yeah. Still, it is a compelling and interesting form from a research perspective,
*  like you said. You co-authored a paper as you were talking about that for WeRobot 2022.
*  Lula, robot, consumer protection in the face of automated social marketing. I think you were
*  talking about some of the ideas in that. Yes. Oh, you got it from Twitter. I was like,
*  that's not published yet. Yeah. This is how I do my research.
*  You just go through people's Twitter feeds. Yeah. Go. Thank you. It's not stalking if it's public.
*  So there's a... You looked at me like you're offended. Like, how did you know?
*  Like, how did you know? No, I was just worried that some early... I mean...
*  Yeah, there's a PDF. There is.
*  There's a PDF. Like now?
*  Yeah. Maybe as of a few days ago.
*  Yeah. Okay. Well...
*  Yeah. Yeah.
*  Okay. You look violated. Like, how did you get that PDF?
*  It's just a draft. It's online. Nobody read it yet until we've written the final paper.
*  Well, it's really good. So I enjoyed it.
*  Oh, thank you. By the time this comes out, I'm sure it'll be out. Or no. When's WeRobot?
*  Basically, WeRobot, that's the workshop where you have an hour where people give you
*  constructive feedback on the paper and then you write the good version.
*  Right. I take it back. There's no PDF. It doesn't exist.
*  I imagine. But there is a table in there in a virtual imagined PDF that I wanted to mention,
*  which is like this kind of strategy used across various marketing platforms. And it's
*  basically looking at traditional media, person-to-person interaction, targeted ads,
*  influencers and social robots. This is the kind of idea that you've been speaking to.
*  And it's just a nice breakdown of that, that social robots have personalized recommendations,
*  social persuasion, automated, scalable data collection and embodiment. So person-to-person
*  interaction is really nice, but it doesn't have the automated and the data collection aspect.
*  But the social robots have those two elements. Yeah. We're talking about the potential for
*  social robots to just combine all of these different marketing methods to be this really
*  potent cocktail. And that table, which was Danielle's idea and a really fantastic one,
*  we put it in at the last second. So yeah, I really liked it. I'm glad you like it.
*  In a PDF that doesn't exist. Yes. That nobody can find if they look.
*  Yeah. So when you say social robots, what does that mean? Does that include virtual ones or no?
*  I think a lot of this applies to virtual ones too. Although the embodiment thing,
*  which I personally find very fascinating, is definitely a factor that research shows can
*  enhance people's engagement with a device. But can embodiment be a virtual thing also,
*  meaning like it has a body in the virtual world? Maybe.
*  It makes you feel like, because what makes a body? A body is a thing that
*  can disappear, has a permanence. I mean, there's certain characteristics that you kind of
*  associate to a physical object. So I think what I'm referring to,
*  and I think this gets messy because now we have all these new virtual worlds and AR and stuff,
*  and I think it gets messy. But there's research showing that something on a screen,
*  on a traditional screen and something that is moving in your physical space,
*  that that has a very different effect on how your brain perceives it even.
*  So I mean, I have a sense that we can do that in a virtual world.
*  Probably. Like when I've used VR, I jump around like an idiot because I think something's going
*  to hit me. And even if a video game on a 2D screen is compelling enough, like the thing that's
*  immersive about it is I kind of put myself into that world. The objects you're interacting with,
*  Call of Duty, things you're shooting, they're kind of, I mean, your imagination fills the gaps and
*  it becomes real. Like it pulls your mind in when it's well done. So it really depends what's shown
*  on the 2D screen. Yeah. Yeah. I think there's a ton of different factors and there's different
*  types of embodiment. Like you can have embodiment in a virtual world. You can have an agent that's
*  simply text-based, which has no embodiment. So I think there's a whole spectrum of factors that
*  can influence how much you engage with something. Yeah. I wonder, I always wondered if you can have
*  like an entity living in a computer. Okay. This is going to be dark. I haven't always wondered
*  about this. So it's going to make it sound like I keep thinking about this kind of thing. No, but
*  like this is almost like Black Mirror, but the entity that's convinced or is able to convince
*  you that is being tortured inside the computer and needs your help to get out. Something like this.
*  That becomes, to me, suffering is one of the things that make you empathize with. Like we're
*  not good at, as you've discussed in the physical form, like holding a robot upside down. You have
*  a really good examples about that and discussing that. I think suffering is a really good catalyst
*  for empathy. And I just feel like we can project embodiment on a virtual thing if it's capable of
*  certain things like suffering. Yeah. I always wonder. I think that's true. And I think that's
*  what happened with the Lambda thing. None of the transcript was about suffering, but it was about
*  having the capacity for suffering and human emotion that convinced the engineer that this
*  thing was sentient. And it's basically the plot of Ex Machina. True. Have you ever made a robot
*  scream in pain? Have I? No, but have you seen that? Did someone?
*  Oh yeah, no, they actually made a Roomba scream whenever it hit a wall.
*  And I programmed that myself as well. Yeah. Because I was inspired by that. Yeah.
*  Do you still have it? Oh, sorry. It hit a wall. Whenever it bumped into something, it would scream
*  in pain. Yeah. No. So the way I programmed the Roombas is when I kick it. So contact between me
*  and the robot is when it screams. Really? Okay. And you were inspired by that? Yeah. I guess I
*  misremembered the video. I saw the video a long, long time ago and, or maybe heard somebody mention
*  it and that just, it's an easy thing to program. So I did that. I haven't run those Roombas for
*  over a year now, but yeah, it was, my experience with it was that it's like, they quickly become,
*  like you remember them. You miss them. Like they're real living beings. So the capacity to suffer
*  or is a really powerful thing. Yeah. Even then that, I mean, it was kind of hilarious. It was
*  just a random recording of screaming from the internet, but it's still, it's still as weird.
*  There's a thing you have to get right based on the interaction, like the latency. Like there is,
*  there is a realistic aspect of how you should scream relative to when you get hurt. Like it
*  should correspond correctly. Like if you kick it really hard, it should scream louder. No, it's
*  just scream at the appropriate time. Not like one second later, right? Like there's a exact, like
*  there's a timing when you get like, I don't know, when you run into, when you run your foot into
*  like the side of a table or something, there's a timing there, the dynamics you have to get right
*  for the, for the actual screaming. Cause the Roomba in particular, doesn't, so I was,
*  the sensors don't, it doesn't know about pain. See, I'm sorry to say Roomba doesn't understand
*  pain. So you have to correctly map the sensors, the timing to the production of the sound.
*  But when you get that somewhat right, it starts, it's a weird, it's really weird feeling and you
*  actually feel like a bad person. Yeah. So, but it's, it's makes you think because that
*  with all the ways that we talked about, that could be used to manipulate you.
*  Oh, for sure. In a good and bad way. So the good way is like you could form a connection with a
*  thing in a bad way that you can form a connection in order to sell you products that you don't want.
*  Yeah. Or manipulate you politically or many nefarious things. You tweeted,
*  we're about to be living in the movie Her, except instead of, I'm seeing, I've researched your tweets,
*  like they're like Shakespeare. We're about to be living in the movie Her, except instead of about
*  love is going to be about what I say, the chat bot being subtly racist and the question, whether
*  it's ethical for companies to charge for software upgrades. Yeah. So, uh, can we break that down?
*  What do you mean by that? Yeah. Obviously some of it is humor. Yes. Well, kind of.
*  I am like, ah, it's so weird to be in the space where I'm so worried about this technology and
*  also so excited about it at the same time. But the, the really like I haven't, I'd gotten a
*  little bit jaded and then with GPT-3 and then the Lambda transcript, I was like re-energized,
*  but have also been thinking a lot about how, you know, what are the, what are the ethical issues
*  that are going to come up? And I think some of the things that companies are really going to have to
*  figure out is obviously algorithmic bias is a huge and known problem at this point. Like even,
*  you know, the, the new image generation tools like Dolly, uh, where they've clearly put in a lot of
*  effort to make sure that if you search for people, it gives you a diverse set of people, et cetera.
*  Like even that one, people have already found numerous like ways that it just kind of regurgitates
*  biases of things that it finds on the internet. Like how if you search for success, it gives you
*  a bunch of images of men. If you search for sadness, it gives you a bunch of images of women.
*  So I think that this is, this is like the really tricky one with these voice, voice agents that
*  companies are going to have to figure out. And that's why it's subtly racist and not overtly,
*  because I think they're going to be able to solve the overt thing. And then with the subtle stuff,
*  it's going to be really difficult. And then I think the other thing is going to be,
*  yeah, like people are going to become so emotionally attached to artificial agents
*  with this complexity of language, with a potential embodiment factor that, I mean,
*  there's already, there's a paper at WeRobot this year written by roboticists about how to deal with
*  the fact that robots die and looking at it as an ethical issue because it impacts people.
*  And I think there's going to be way more issues than just that. Like, like I think the tweet was
*  software upgrades, right? Like how much is it okay to charge for something like that? If someone is
*  deeply emotionally invested in this relationship. Oh, the ethics of that is interesting, but there's
*  also the practical funding mechanisms. Like you mentioned with the dog, in theory, there's a
*  subscription. Yeah, the new Aibo. So the old Aibo from the 90s, people got really attached to, and
*  in Japan they're still having like funerals and Buddhist temples for the Aibos that can't be
*  repaired because people really viewed them as part of their families. So we're talking about robot
*  dogs. Robot dogs, the Aibo. Yeah, the original like famous robot dog that Sony made came out in the
*  90s, got discontinued, having funerals for them in Japan. Now they have a new one. The new one
*  is great. I have one at home. It's like, it's $3,000. I think it's 3000 bucks. And then after a few
*  years, you have to start paying, I think it's like 300 a year for a subscription service for cloud
*  services and the cloud services. I mean, it's a lot, the dog is more complex than the original
*  and it has a lot of cool features and it can remember stuff and experiences and it can learn.
*  And a lot of that is outsourced to the cloud. And so you have to pay to keep that running,
*  which makes sense. People should pay and people who aren't using it shouldn't have to pay.
*  But it does raise the interesting question. Could you set that price to reflect a consumer's
*  willingness to pay for the emotional connection? So if like, you know that people are really,
*  really attached to these things, just like they would be to a real dog.
*  Could you just start charging more because there's like more demand? Yeah. And you have to be,
*  but that's true for anything that people love, right? It is. And it's also true for real dogs.
*  Like there's all these new medical services nowadays where people will shell out thousands
*  and thousands of dollars to keep their pets alive. And is that taking advantage of people
*  or is that just giving them what they want? That's the question. Well, back to marriage,
*  what about all the money that it costs to get married and then all the money that it costs
*  to get a divorce? That feels like a very, like, uh, that's like a scam. I think the society is
*  full of scams that are like, Oh, it's such a scam. And then we've created like the whole wedding
*  industrial complex has created all these quote unquote traditions that people buy into that
*  aren't even traditions. Like they're just fabricated by marketing. Like it's awful.
*  Let me ask you about racist robots. Is it up to a company that creates that? So we talk about
*  removing bias and so on. Yeah. And that's a really popular field in AI currently. Yeah.
*  And a lot of people agree that it's an important field. Um, but the question is for like social
*  robotics is should it be up to the company to remove the bias of society? Well, who else can,
*  Oh, to remove the bias of society. Like I guess because there's a lot of people that are subtly
*  racist in modern society, like why shouldn't our robots also be subtly racist? I mean, that's like,
*  why do we put so much responsibility on the robots? Because the, I'm imagining like a,
*  like a Hitler room. I mean, that, that would be funny. Uh, but the, but I guess I'm asking a
*  serious question. Right. You're allowed to make that joke. Yes.
*  And I've been nonstop reading about war war two and Hitler. I think, um, I'm glad we exist in
*  the world where we can just make those jokes. Um, that helps deal with it. Uh, anyway,
*  it is a serious question. It's sort of like, um, like it's such a difficult problem to solve. Now,
*  of course, like bias and so on, like there's low hanging fruit, which I think was that a lot of
*  people are focused on, but then it becomes like subtle stuff over time. And it's very difficult
*  to know. Now, if you, if you can also completely remove the personality, you can completely remove
*  the personalization. You can remove the language aspect, which is what I had been arguing because
*  I was like, the language is a disappointing aspect of social robots anyway. But now are
*  we introducing that because it's, it's now no longer disappointing. So I do think, well,
*  let's just start with the premise, which I think is very true, which is that racism is not a neutral
*  thing, but it is the thing that we don't want in our society. Like I, it does not conform to my
*  values. So if we agree that racism is bad, I do think that it has to be the company because the
*  pro I mean, it might not be possible and companies might have to put out products that where they're
*  taking risks and they might get slammed by consumers and they might have to adjust. I don't know like
*  how this is going to work in the market. I have opinions about how it should work, but
*  it is on the company and the danger with robots is that they can entrench this stuff. It's not like
*  your racist uncle who you can have a conversation with and put things into context. Maybe
*  with that. Yeah. Or who, who might change over time with more experience. A robot really just like
*  regurgitates things, entrenches them, could influence other people. And I mean, I think
*  that's terrible. Well, I think there's a difficult challenge here is because
*  even the premise you started with that essentially racism is bad. I think we live in a society today
*  where the definition of racism is different between different people. Some people say that
*  it's not enough not to be racist. Some people say you have to be anti-racist. So you have to,
*  you have to have a robot that constantly calls out, like calls you out on your implicit racism.
*  I would love that. I would love that robot. But like maybe it, maybe it sees, well, I don't know
*  if you love it because maybe you'll see racism in things that aren't racist. And then you're arguing
*  with your robots. I'm not exactly sure that, I mean, it's a tricky thing. I guess I'm saying that
*  the line is not obvious, especially in this heated discussion where we have a lot of identity
*  politics of what, what is harmful to different groups and so on. It feels different. It feels
*  like the broader question here is should a social robotics company be solving or being part of
*  solving the issues of society? Well, okay. I think it's the same question as should I,
*  as an individual, be responsible for knowing everything in advance and saying all the right
*  things. And the answer to that is yes, I am responsible, but I'm not going to get it perfect.
*  And then the question is how do we deal with that? And so as a person, how I aspire to deal with that
*  is when I do inevitably make a mistake because I have blind spots and people get angry, I don't
*  take that personally and I listen to what's behind the anger. And it can even happen that like,
*  maybe I'll tweet something that's well-intentioned and one group of people starts yelling at me and
*  then I change it the way that they said, and then another group of people starts yelling at me,
*  which has happened. This happened to me actually around, in my talks, I talk about robots that are
*  used in autism therapy. And so whether to say a child with autism or an autistic child is super
*  controversial. And a lot of autistic people prefer to be referred to as autistic people. And a lot of
*  parents of autistic children prefer child with autism. And then they disagree. So I've gotten
*  yelled at from both sides. And I think I'm still responsible, even if I can't get it right. I don't
*  know if that makes sense. It's a responsibility thing. And I can be as well-intentioned as I want,
*  and I'm still going to make mistakes. And that is part of the existing power structures that exist.
*  And that's something that I accept. And you accept being attacked from both sides and grow from it
*  and learn from it. But the danger is that after being attacked, assuming you don't get canceled,
*  aka completely removed from your ability to tweet, you might become jaded and not want to
*  talk about autism anymore. No, I don't. And I didn't. I mean, it's happened to me. And what I
*  did was I listened to both sides and I chose, I tried to get information. And then I decided
*  that I was going to use autistic children. And now I'm moving forward with that. Like, I don't know.
*  For now, right?
*  For now. Yeah. Until I get updated information and I'm never going to get anything perfect,
*  but I'm making choices and I'm moving forward because being a coward and just retreating from
*  that, I think- But here's the problem. You're a very smart person, an individual, a researcher,
*  thinker, an intellectual. So that's the right thing for you to do. The hard thing is when
*  as a company, imagine you had a PR team. I said, Kate, like this, you should-
*  PR teams we hate.
*  Yeah. I mean, just, well, if you hired PR people, like obviously they would see that and they'd be
*  like, well, maybe don't bring up autism. Maybe don't bring up these topics. You're getting
*  attacked is bad for your brand. They'll say the brand word. There'll be, you know, if you look at
*  different demographics that are inspired by your work, I think it's insensitive to them. Let's not
*  mention this anymore. Like there's this kind of pressure that all of a sudden you, or you do
*  suboptimal decisions. You take a kind of poll. Again, it's looking at the past versus the future,
*  all those kinds of things. And it becomes difficult in the same way that it's difficult
*  for social media companies to figure out like who to censor, who to recommend.
*  I think this is ultimately a question about leadership, honestly, like the way that I see
*  leadership, because right now the thing that bothers me about institutions and a lot of people
*  who run current institutions is that their main focus is protecting the institution or
*  protecting themselves personally. That is bad leadership because it means you cannot have
*  integrity. You cannot lead with integrity. And it makes sense because like, obviously if you're
*  the type of leader who immediately blows up the institution you're leading, then that doesn't
*  exist anymore. And maybe that's why we don't have any good leaders anymore, because they had integrity
*  and they didn't put the survival of the institution first. But I feel like you have to,
*  to be a good leader, you have to be responsible and understand that with great power comes great
*  responsibility. You have to be humble and you have to listen and you have to learn. You can't get
*  defensive and you cannot put your own protection before other things. You take risks where you
*  might lose your job. You might lose your well-being because of, because in the process
*  of standing for the principles, for the things you think are right to do, yeah, based on the things
*  you, like based on learning from, like listening to people and learning from what they feel.
*  And the same goes for the institution, yeah. Yeah, but I ultimately actually believe that
*  those kinds of companies and countries succeed that have leaders like that. You should run for
*  president. No, thank you. Yeah. That's maybe the problem, like the people who have good ideas
*  about leadership, they're like, yeah, no, this is why I don't, why I'm not running a company.
*  It's been, I think three years since the Jeffrey Epstein controversy at MIT,
*  MIT media lab, Joy Ito, the head of the media lab resigned. And I think at that time you wrote an
*  opinion article about it. So just looking back a few years have passed. What have you learned
*  about human nature from the fact that somebody like Jeffrey Epstein found his way inside MIT?
*  That's a really good question. What have I learned about human nature? I think,
*  well, there's, there's how did this problem come about? And then there's what was the reaction
*  to this problem and to it becoming public. And in the reaction,
*  the things I learned about human nature
*  were that sometimes cowards are worse than assholes. Wow. I'm really, oh,
*  I mean, that's a really powerful statement. I think because the assholes, at least
*  you know what you're dealing with. They have integrity in a way. They're just living in a
*  they have integrity in a way. They're just living out their asshole values.
*  And the cowards are the ones that you have to watch out for. And this comes back to
*  people protecting themselves over doing the right thing. They'll throw others under the bus.
*  Is there some sense that not enough people took responsibility?
*  For sure. And I mean, I don't want to sugarcoat at all what Joe Edo did. I mean,
*  I think it's gross that he took money from Jeffrey Epstein. I believe him that he didn't
*  know about the bad, bad stuff, but I've been in those circles with those like
*  public intellectual dudes that he was hanging out with. And any woman in those circles, like saw
*  10,000,000 red flags just the whole environment was so misogynist. Like,
*  and so personally, because Joey, like, was a great boss and a great friend,
*  I was really disappointed that he ignored that in favor of raising money.
*  And I think that it was right for him to resign in the face of that. But
*  one of the things that he did that, you know, many others didn't was he came forward about it
*  and he took responsibility. And all of the people who didn't, I think,
*  it's just interesting. The other thing I learned about human nature, okay, I'm going to go on
*  a tangent, but I'll come back, I promise. So I once saw this tweet from someone, or it was a
*  Twitter thread, from someone who worked at a homeless shelter. And he said that when he
*  started working there, he noticed that people would often come in and use the bathroom and they
*  would just trash the entire bathroom, like rip things out of the walls, like toilet paper on
*  the ground. And he asked someone who had been there longer, like, why do they do this? Why
*  do the homeless people come in and trash the bathroom? And he was told it's because it's
*  the only thing in their lives that they have control over. And I feel like, so I think
*  sometimes when it comes to the response, the, just the mobbing response that happens in the wake of
*  some harm that was caused, if you can't target the person who actually caused the harm, who
*  was Epstein, you will go as many circles out as you can until you find the person that you have
*  power over and you have control over, and then you will trash that. And it makes sense that people
*  do this. It's again, it's a human nature thing. Of course, you're going to focus all your energy
*  because you feel helpless and enraged and you, and it's unfair and you have no other power.
*  You're going to focus all of your energy on someone who's so far removed from the problem that
*  that's not even an efficient solution. And the problem is often the first person you find is the
*  one that has integrity, sufficient integrity to take responsibility. Yeah. And it's why my husband
*  always says he's, he's a liberal, but he's always like, when liberals form a firing squad, they stand
*  in a circle because you know that your friends are going to listen to you. So you criticize them.
*  You're not going to be able to convince someone across the aisle. But see in that situation,
*  what I had hope is the people in the farther in that situation, any situation of that sort,
*  the people that are farther out in the circles, uh, stand up and like also take some responsibility
*  for the broader picture of human nature versus like specific situation, but also take some
*  responsibility and, um, but also defend the people involved as flawed, not in a like, no, no, no,
*  nothing like, like this people fucked up. Like you said, there's a lot of red flags
*  that people just ignored for the sake of money in this particular case,
*  but also like be transparent and public about it and spread the responsibility across large
*  number of people such that you learn a lesson from it institutionally. Yeah. It was a systems
*  problem. It wasn't a one individual problem. And I feel like currently because, uh, Joey took
*  like a reside because of it or essentially fired, pressured out because of it. Uh,
*  MIT can pretend like, oh, we didn't, we didn't know anything. It wasn't part bad leadership again,
*  because when you are at the top of an institution with that much power and you were complicit in
*  what happened, which they were like, come on, there's no way that they didn't know that this
*  was happening. So I like to not stand up and take responsibility. I think it's bad leadership.
*  Do you understand why Epstein was able to, um, outside of MIT, he was able to make a lot of
*  friends with a lot of powerful people. Does that make sense to you? Why was he able to get in
*  these rooms, befriend these people, befriend people that I don't know personally, but I think a lot of
*  them indirectly, I know as being good people, smart people, why would they led Jeffrey Epstein
*  into the, into their office, have a discussion with them? What, what do you understand about
*  human nature from that? Well, so I never met Epstein or, I mean,
*  I've met some of the people who interacted with him, but I was never like, I never saw him
*  in action. I don't know how charismatic he was or what that was, but I do think that sometimes
*  the simple answer is the more likely one. And from my understanding, what he would do is he was kind
*  of a grift, a social grifter, like, you know, those people who will, you must get this because
*  you're famous. You must get people coming to you and being like, oh, I know your friends so and so
*  in order to get cred with you. I think he just convinced some people who were trusted in a network
*  that he was a great guy and that, you know, whatever, I think at that point, because at that
*  point he had had like a, what a conviction prior, but it was a one-off thing. It wasn't clear that
*  there was this other thing that was that. And most people probably don't check. Yeah. And
*  most people don't check. Like you're at an event, you meet this guy. I don't know. Maybe people do
*  check when they're that powerful and wealthy or maybe they don't. I have no idea. No, they're just
*  stupid. I mean, they're not like, all right. Does anyone check anything about me? Because I've walked
*  into some, some of the richest and most powerful people in the world and nobody like asks questions
*  like who the fuck is this guy? Like, yeah, like nobody asks those questions. It's, it's interesting.
*  I, I, I would think like there would be more security or something like there, there really
*  isn't, I think a lot of it has to do what my hope is in my case has to do with like people can sense
*  that this is a good person, but if that's the case, then they can surely then a human being can use
*  charisma to infiltrate. Yeah. Just being, just saying the right thing. People vouching for you
*  within that type of network. Like once you, yeah. Once you have someone powerful vouching for you,
*  who someone else trusts, then you know, you're in. So how do you avoid something like that?
*  If you're MIT, if you're Harvard, if you're in any of these institutions? Well, I mean, first of all,
*  you have to do your homework before you take money from someone. Like,
*  I think, I think that's required, but I think, you know, I think Joey did do his homework. I think
*  he did. And I think at the time that he took money, there was the one conviction and not like the
*  later thing. And I think that the story at that time was that he didn't know she was underage and
*  blah, blah, blah, or whatever it was a mistake. And Joey always believed in redemption for people
*  and that people can change and that they can genuinely regret and like learn and move on.
*  And he was a big believer in that. So I could totally see him being like, well, I'm not going
*  to exclude him because of this thing. And because other people are vouching for him. So, and just to
*  be clear, we're now talking about the set of people who I think Joey belonged to, who did not like go
*  to the island and have sex with underage girls, because that's a whole other set of people who
*  like were powerful and like were part of that network and who knew and participated. And so
*  like I distinguish between people who got taken in, who didn't know that that was happening and
*  people who knew. I wonder what the different circles look like. So like people that went to the island
*  and didn't do anything, didn't see anything, didn't know about anything
*  versus the people that did something. And then there's people who heard rumors maybe.
*  And what do you do with rumors? Like, isn't there, isn't there people that heard rumors about Bill
*  Cosby for the longest time? For like, for the longest, like whenever that happened, like all
*  these people came out of the woodwork, like everybody kind of knew. I mean, it's like, all
*  right, so what are you supposed to do with the rumors? Like what I think the other way to put is
*  red flags, as you were saying. Yeah. And like, I can tell you that those circles, like there were
*  red flags without me even hearing any rumors about anything ever. Like I was already like,
*  there are not a lot of women here, which is a bad sign.
*  Isn't there a lot of places where there's not a lot of women and that doesn't necessarily mean
*  it's a bad sign? There are, if it's like a pipeline problem where it's like,
*  I don't know, technology law clinic that only gets like male lawyers because there's not a lot of
*  women, you know, applicants in the pool. But there's some aspect of this situation that
*  like there should be more women here. Oh yeah. Yeah. You've, uh,
*  actually I'd love to ask you about this because, um, you have strong opinions about Richard Stallman.
*  Is that, do you still have those strong opinions? Look, all I need to say is that
*  he met my friend who's a law professor. Yeah. She shook his hand and he licked her arm from wrist
*  to elbow. And it certainly wasn't appropriate at that time. What about if you're like an incredibly
*  weird person? Okay. That's a good question because obviously there's a lot of neurodivergence at MIT
*  and everywhere. And obviously like we, we need to accept that people are different, that people
*  don't understand social conventions the same way. But one of the things that I've learned about
*  neurodivergence is that women are often expected or taught to mask their neurodivergence and kind
*  of fit in. And men are accommodated and excused. And I don't think that being neurodivergent gives
*  you a license to be an asshole. Like you can be a weird person and you can still learn that it's
*  not okay to lick someone's arm. Yeah. There is a balance. Like women should be allowed to be a
*  little weirder and men should be less weird. Cause I think, I think there was a, cause I,
*  you're one of the people I think tweeting that what made me, cause I wanted to talk to Richard
*  Stallman on the podcast about, cause I didn't have a context because I wanted to talk to him
*  cause he's, you know, free software. He's very, he's very weird in interesting, good ways in the
*  world of computer science. He's also weird in that, you know, when he gives a talk, he would be like,
*  like picking at his feet and eating the skin off his feet, right? That he's known for these extremely
*  kind of, how else do you put it? I don't know how to put it, but then there was something that
*  happened to him in conversations on this thread related to Epstein, which I was torn about because
*  I felt it's similar to Joy Ito's like, I felt he was maligned, like people were looking for somebody
*  to get angry at. So he, he was inappropriate, but the, I didn't like the cowardice more. Like I,
*  I set aside his situation and we could discuss it, but the cowardice on MIT's part, and this is me
*  saying it about the way they treated that whole situation. Oh, they're always cowards about how
*  they treat anything. They just try to make the problem go away. Yeah. So it was, it was about,
*  but yeah, exactly. Making the conversation. I think he should have left the mailing list.
*  He shouldn't have, he shouldn't have been part of the mailing list. Well, that's probably true also,
*  but I think, I think what, what bothered me, what always bothers me in these mailing list
*  situations or Twitter situations, like if you say something that's hurtful to people or makes
*  people angry and then people start yelling at you, maybe they shouldn't be yelling.
*  Maybe they are yelling because again, you're the only point of power they have. Maybe,
*  maybe it's okay that they're yelling, whatever it is, like it's your response to that that matters.
*  And I think that I just have a lot of respect for people who can say, oh, people are angry.
*  There's a reason they're angry. Let me find out what that reason is and learn more about it.
*  It doesn't mean that I'm wrong. It doesn't mean that I'm bad. It doesn't mean that I'm ill-intentioned,
*  but why are they angry? I want to understand. And then once you understand, you can respond again
*  with integrity and say, actually I stand by what I said. Here's why. Or you can say, actually I
*  listened and here are some things I learned. That's the kind of response I want to see from
*  people. And people like Stallman do not respond that way. They just like go into battle.
*  Right. Like where it's obvious you didn't listen.
*  Yeah. No interest in listening.
*  Honestly, that's to me as bad as the people who just apologize just because they are trying to
*  make the problem go away. Of course.
*  Right. So like if- That's not-
*  Both are bad. A good apology has to include understanding what you did wrong.
*  And in part standing up for the things you think you did right.
*  So yeah, if there are those things. Yeah.
*  Finding and then, but you have to give, you have to acknowledge, you have to like give that hard
*  hit to the ego that says I did something wrong. Yeah. Definitely Richard Stallman is not somebody
*  who is capable of that kind of thing or hasn't given evidence of that kind of thing. But that
*  was also even just your tweet, I had to do a lot of thinking like different people from different
*  walks of life see red flags and different things. And so things I find as a man, non-threatening
*  and hilarious are not necessarily, doesn't mean that they're aren't like deeply hurtful to others.
*  And I don't mean that in a social justice warrior way, but in a real way, like people really have
*  different experiences. So I have to like really put things into context. I have to kind of listen
*  to what people are saying, put aside the emotion of what their emotion will do, what you're saying,
*  and try to keep the facts of their experience and learn from it.
*  And because it's not just about the individual experience either. It's not like, oh, you know,
*  my friend didn't have a sense of humor about being licked. It's that she's been metaphorically
*  licked, you know, 57 times that week because she's an attractive law professor and she doesn't get
*  taken to. And so like men walk through the world and it's impossible for them to even understand
*  what it's like to have a different experience of the world. And that's why it's so important
*  to listen to people and believe people and believe that they're angry for a reason.
*  Maybe you don't like their tone. Maybe you don't like that they're angry at you. Maybe you get
*  defensive about that. Maybe you think that they should, you know, explain it to you,
*  but believe that they're angry for a reason and try to understand it.
*  Yeah, there's a deep truth there and an opportunity to become a better person.
*  Can I ask you a question?
*  Haven't you been doing that for two hours?
*  Three hours now. Let me ask you about, uh, Ghislaine Maxwell. She's been saying that she's
*  an innocent victim. Uh, is she an innocent victim or is she, uh, evil and equally responsible
*  like Jeffrey Epstein? Now I'm asking far away from any MIT things and more, just your sense of the
*  whole situation. I haven't been following it, so I don't know the facts of the situation and like,
*  what is now like known to be her role in that. If I were her, clearly I'm not, but if I were her,
*  I wouldn't be going around saying I'm an innocent victim. I would say,
*  maybe she's, I don't know what she's saying again. Like, I don't know.
*  She was controlled by Jeffrey.
*  Is she saying this as part of a legal case or is she saying this as like a PR thing?
*  Uh, well, PR, but it's not just her. It's not just her. It's not just her.
*  But it's not just her. It's our whole family believes this. There's a, there's a whole effort
*  that says like that. How should I put it? I believe they believe it. So in that sense,
*  it's not PR. I believe the family, basically the family is saying that she's a good, she's a really
*  good human being. Well, I think everyone is a good human being. I know it's a controversial
*  opinion, but I think everyone is a good human being. There's no evil people. There's people
*  who do bad things and who behave in ways that harm others. And I think we should always hold
*  people accountable for that. But holding someone accountable doesn't mean saying that they're evil.
*  Yeah. It actually, those, those people usually think they're doing good.
*  Yeah. I mean, aside from, I don't know, maybe sociopaths like are specifically trying to like
*  harm people, but I think most people are trying to do their best. And if they're not
*  doing their best, it's because there's some impediment or something in their past.
*  So I just, I genuinely don't believe in good and evil people, but I do believe in like harmful
*  and not harmful actions. And so I don't know, like, I don't care. I don't care. Yeah. She's
*  a good person, but if she contributed to harm, then she needs to be accountable for that. Like
*  that's my position. I don't know what the facts of the matter are. It seems like she was pretty
*  close to the situation. So it doesn't seem very believable that she was a victim, but I don't know.
*  I wish I have met Epstein because something tells me he would just be a regular person,
*  charismatic person like anybody else. And that's a very dark reality that we don't know which among
*  us, what each of us are hiding in the closet. That's a really tough thing to like deal with,
*  because then you can put your trust into some people and they can completely betray that trust
*  and in the process destroy you. Yeah. Which there's a lot of people that interacted with Epstein
*  that now have to, I mean, if they're not destroyed by it, then their whole, like the ground on which
*  they stand ethically has crumbled, at least in part. And I'm sure you and I have interacted
*  with people without knowing it who are bad people. As I always tell my four year old,
*  I always tell my four year old people who have done bad things.
*  People who have done bad things.
*  He's always talking about bad guys and I'm trying to move him towards,
*  they're just people who make bad choices.
*  Yeah. That's really powerful. Actually, that's really important to remember because that means
*  you have compassion towards all human beings. Do you have hope for the future of MIT, the future
*  of Media Lab in this context? So David Newman is now at the helm. I'm going to talk to her. I
*  talked to her previously. I'll talk to her again. She's great.
*  Love her. Yeah, she's great. I don't know if she knew the whole situation when she started because
*  the situation went beyond just the Epstein scandal. A bunch of other stuff happened at the same time.
*  Some of it's not public, but what I was personally going through at that time.
*  The Epstein thing happened, I think, was it August or September 2019? It was somewhere around
*  late summer in June 2019. I'm a research scientist at MIT. You are too, right?
*  And I always have had various supervisors over the years and they've just basically let me do
*  what I want, which has been great. But I had a supervisor at the time and he called me into his
*  office for a regular check-in. In June of 2019, I reported to MIT that my supervisor had grabbed me,
*  pulled me into a hug, wrapped his arms around my waist, and started massaging my hip and trying to
*  kiss me, kiss my face, kiss me near the mouth, and said literally the words,
*  don't worry, I'll take care of your career. That experience was really interesting because
*  I was very indignant. I was like, he can't do that to me. Doesn't he know who I am? I was like,
*  this is the Me Too era. I naively thought that when I reported that, it would get taken care of.
*  Then I had to go through the whole reporting process at MIT and I learned a lot about how
*  institutions really handle those things internally, particularly situations where
*  I couldn't provide evidence that it happened. I had no reason to lie about it, but I had no evidence.
*  I was going through that and that was another experience for me where there's so many people
*  in the institution who really believe in protecting the institution at all costs.
*  There's only a few people who care about doing the right thing, and one of them resigned. Now
*  there's even less of them left. What did you learn from that? If you have hope for this institution
*  that I think you love, at least in part, I love the idea of MIT.
*  I love the idea. I love the research body. I love a lot of the faculty. I love the students.
*  I love the energy. I love it all. I think the administration suffers from the same problems as
*  any leadership of an institution that is large, which is that
*  they've become risk-averse, like you mentioned. They care about PR. The only ways to
*  get their attention or change their minds about anything are to threaten the reputation of the
*  Institute or to have a lot of money. That's the only way to have power at the Institute.
*  I don't think they have a lot of integrity or believe in ideas or even have a lot of
*  connection to the research body. It's so weird. You have this amazing research body of people
*  pushing the boundaries of things who aren't afraid. There's the hacker culture,
*  and then you have the administration, and they're really
*  protecting the institution at all costs.
*  Yeah, there's a disconnect. I wonder if that was always there for just kind of
*  slowly grows over time, a disconnect between the administration and the faculty.
*  I think it grew over time is what I've heard. I've been there for 11 years now.
*  I don't know if it's gotten worse during my time, but I've heard from people who've been there
*  longer that it didn't. MIT didn't used to have a general counsel's office. They didn't used to have
*  all this corporate stuff, and then they had to come back and do it. They didn't have the
*  stuff, and then they had to create it as they got bigger in the era where such things are,
*  I guess, deemed necessary.
*  See, I believe in the power of individuals to overthrow the thing. It's just a really good
*  president of MIT or certain people in the administration can reform the whole thing.
*  The culture is still there. I think everybody remembers that MIT is about the students and
*  the faculty.
*  Do they though? I don't know. I've had a lot of conversations that have been shocking with
*  senior administration. They think the students are children. They call them kids. It's like,
*  these are the smartest people. They're way smarter than you, and you're so dismissive of them.
*  But those individuals, I'm saying the capacity, the aura of the place still values the students
*  and the faculty. I'm being awfully poetic about it, but what I mean is the administration is the
*  froth at the top of the waves, the surface. They can be removed, and new life can be brought in
*  that would keep to the spirit of the place.
*  Who decides on who to bring in? Who's higher?
*  It's bottom up. Oh, I see. I see. But I do think ultimately,
*  especially in the era of social media and so on,
*  faculty and students have more and more power. Just more of a voice, I suppose.
*  I hope so. I really do. I don't see MIT going away anytime soon. I also don't think it's a
*  terrible place at all.
*  Yeah, it's an amazing place, but there's different trajectories it can take.
*  Yeah.
*  That has to do with a lot of things, including
*  even if we talk about robotics, it could be the capital of the world in robotics.
*  But currently, if you want to be doing the best AI work in the world, you're going to go to Google
*  or Facebook or Tesla or Apple or so on. You're not going to be at MIT.
*  And so that has to do, I think that basically has to do with
*  not allowing the brilliance of the researchers to flourish.
*  Yeah, people say it's about money, but I don't think it's about that at all.
*  Sometimes you have more freedom and can work on more interesting things in companies.
*  That's really where they lose people.
*  Yeah. And the freedom in all ways, which is why it's heartbreaking to get people like Richard
*  Stallman. There's such an interesting line because Richard Stallman is a gigantic weirdo
*  that crossed lines he shouldn't have crossed, right? But we don't want to draw too many lines.
*  This is the tricky thing.
*  There are different types of lines, in my opinion.
*  But yes, your opinion. You have strong lines you hold to.
*  But then if administration listens to every line, there's also power in drawing a line.
*  It becomes like a little drug. You have to find the right balance.
*  Licking somebody's arm is never appropriate.
*  I think the biggest aspect there is not owning it, learning from it, growing from it from the
*  perspective of Stallman or people like that. Back when it happened, understanding,
*  being empathetic, seeing the fact that this was totally inappropriate.
*  Not that particular act, but everything that led up to it, too.
*  I think there are different kinds of lines.
*  Stallman crossed lines that essentially excluded a bunch of people and created an environment
*  where there are brilliant minds that we never got the benefit of because he made things feel
*  gross or even unsafe for people. There are lines that you can cross
*  where you're challenging an institution to...
*  I don't think he was intentionally trying to cross a line or maybe he didn't care.
*  There are lines that you can cross intentionally to move something forward or to do the right thing.
*  When MIT was like, you can't put an all-gender restroom in the Media Lab because something
*  permits whatever, and Joey did it anyway. That's a line you can cross to make things actually better
*  for people. The line you're crossing is some arbitrary, stupid rule that people who don't
*  want to take the risk are like... You know what I mean?
*  Ultimately, I think the thing you said is cross lines in a way that doesn't alienate others.
*  For example, I started for a while wearing a suit often at MIT, which sounds counterintuitive,
*  but people always looked at me weird for that. MIT created this culture, specifically the people
*  I was working with. Nobody wears suits. Maybe the business school does.
*  Yeah, we don't trust the suits.
*  We don't trust the suits. I was like, fuck you, I'm wearing a suit.
*  Nice. See, that I like...
*  But that's not really hurting anybody, right?
*  Exactly. It's challenging people's perceptions. It's doing something that you want to do,
*  but it's not hurting people.
*  And that particular thing was hurting people. It's a good line. It's a good line.
*  And hurting, ultimately, the people that you want to flourish.
*  You tweeted a picture of pumpkin spice Greek yogurt and asked, grounds for divorce? Yes,
*  no. So let me ask you, what's the key to a successful relationship?
*  Oh my God, a good couples therapist?
*  What went wrong with the pumpkin spice Greek yogurt? What's exactly wrong? Is it the pumpkin?
*  Is it the Greek yogurt? I didn't understand. I stared at that tweet for a while.
*  I grew up in Europe, so I don't understand the pumpkin spice in everything craze that they
*  do every autumn here. I understand that it might be good in some foods, but they just put it in
*  everything.
*  And it doesn't belong in Greek yogurt.
*  I mean, I was just being humorous. I ate one of those yogurts and actually tasted pretty good.
*  Yeah, exactly.
*  So I think part of the success of a good marriage is giving each other a hard time humorously for
*  things like that.
*  Is there a broader lesson? Because you guys seem to have a really great marriage from the external.
*  I mean, every marriage looks good from the external. Every, I think, yeah.
*  That's not true. But yeah, I get it.
*  Okay, right. That's not true. No, but relationships are hard. Relationships with anyone are hard.
*  And especially because people evolve and change and you have to make sure there's space for both
*  people to evolve and change together. And I think one of the things that I really liked about our
*  marriage vows was, I remember before we got married, Greg, at some point, got kind of nervous
*  and he was like, it's just such a big commitment to commit to something for life. And I was like,
*  we're not committing to this for life. And he was like, we're not? And I'm like, no, we're committing
*  to being part of a team and doing what's best for the team. If what's best for the team is to break
*  up, we'll break up. I don't believe in this. We have to do this for our whole lives.
*  And that really resonated with him too. So yeah.
*  Yeah, that was our vows. That we're just, we're going to be a team.
*  You're a team and do what's right for the team?
*  Yeah. Yeah.
*  That's very like Michael Jordan view.
*  Yeah.
*  Do you guys get married in the desert, like November rain style with slash plane?
*  You don't have to answer that. I'm not good at these questions. Okay.
*  You brought up marriage like eight times. Are you trying to hint something on the podcast?
*  You have an announcement to make? No. I don't know. It just seems like a good metaphor
*  for why, why, why it felt like a good metaphor for, in a bunch of cases for the marriage
*  industrial complex. I remember that and, oh, people complaining. It just seemed like marriage
*  is one of the things that always surprises me because I want to get married.
*  You do?
*  Yeah, I do. And then I listened to like friends of mine that complain, not all I like, I like guys.
*  I really like guys that don't complain about their marriage. It's such a cheap,
*  like if it's such a cheap release valve, like it doesn't, that's bitching about anything,
*  honestly. That's just like, it's too easy, but especially like bitch about the sports team or
*  the weather if you want, but like about somebody that you're dedicating your life to, like if you
*  bitch about them, you're going to see them as a lesser being also like you don't think so,
*  but you're going to like decrease the value you have. I personally believe over time,
*  you're not going to appreciate the magic of that person. I think anyway, but it's like that I just
*  noticed this a lot that people are married and they will whine about, you know, like the wife,
*  whatever, you know, this, this, this is part of the sort of the culture to kind of comment in that
*  way. I think women do the same thing about the husband. He doesn't, he never does this or he's
*  a goof. He's incompetent at this or that, whatever they, there's the kind of tropes like, Oh, you
*  know, husbands never do X and like wives. I think those do a disservice to everyone is just
*  disrespectful to everyone involved. Yeah, but it happens. So I was, I brought that up as an example
*  of something that people actually love, but they complain about, because for some reason that's
*  more fun to do is complain about stuff. Yeah. And so that's what would clip you or whatever.
*  Right? So like you complain about, but you actually love it. It's just a good metaphor that, you know,
*  what was I going to ask you? Oh, you,
*  your hamster died. When I was like eight, you miss her beige.
*  What's the closest relationship you've had with a pet? That's the one.
*  What have you loved the most in your life?
*  I think my, my first pet was a goldfish named Bob and he died immediately. And that was really sad.
*  And I think, I think it was really attached to Bob and Nancy, my goldfish. We got new Bob's
*  and then Bob kept dying and we got new Bob's. Nancy just kept living.
*  So it was very replaceable. Yeah. I was young. It was easy to.
*  Do you think there will be a time when the robot like in the movie, her be something we fall in
*  love with romantically? Oh yeah. Oh, for sure. Yeah. At scale. Like we're a lot of people.
*  Romantically, I don't know if it's going to happen at scale. I think,
*  I think we talked about this a little bit last time on the podcast too, where I think we're
*  just capable of so many different kinds of relationships. And actually part of why I think
*  marriage is so tough as a relationship is because we put so many expectations on it. Like your
*  partner has to be your best friend and you have to be sexually attracted to them and they have to
*  be a good co-parent and a good roommate. And like, it's all the relationships at once that have to
*  work. But we're like, normally with other people, we have like one type of relationship. We even
*  have, we have a different relationship to our dog than we do to our neighbor. Then we do to the,
*  you know, person out, someone, a coworker. I think that some people are going to find romantic
*  relationships with robots interesting. It might even be a widespread thing, but I don't think it's
*  going to replace like human romantic relationships. I think it's just going to be a separate type of
*  thing. It's going to be more narrow, more narrow or even like just something new that we haven't
*  really experienced before. Maybe like having a crush on a artificial agent is a different type
*  of fascination. I don't know. People would see that as cheating. I think people would. Well,
*  I mean, the things that people feel threatened by in relationships are very manyfold. So yeah,
*  that's just an interesting one because maybe, maybe it'll be good. A little jealousy for the
*  relationship. Maybe that's part of the couples therapy, you know, kind of thing or whatever.
*  I don't think jealousy. I mean, I think it's hard to avoid jealousy, but I think the objective is
*  probably to avoid. I mean, some people don't even get jealous when their partner sleeps with
*  someone else. Like there's polyamory and I think there's just such a diversity of different ways
*  that we can structure relationships or view them that this is just going to be another one that we
*  add. You dedicate your book to your dad. What did you learn about life from your dad? Oh man, my dad
*  is, he's a great listener and he is the best person I know at the type of cognitive empathy
*  that's like perspective taking. So not like emotional, like crying empathy, but trying to see
*  someone else's point of view and trying to put yourself in their shoes. And he really
*  instilled that in me from an early age. And then he made me read a ton of science fiction, which
*  probably led me down this path. Tell you how to be curious about the world and how to be open-minded.
*  Yeah. Last question. What role does love play in the human condition?
*  So this one we've been talking about love and robots. And you're fascinated by social robotics.
*  It feels like all of that operates in the landscape of something that we can call love.
*  Love? Yeah, I think there are a lot of different kinds of love. I feel like we need,
*  don't the Eskimos have all these different words for snow? We need more words to describe different
*  types and kinds of love that we experience. But I think love is so important and I also think it's
*  not zero sum. That's the really interesting thing about love is that, you know, I had one kid and
*  I loved my first kid more than anything else in the world. And I was like, how can I have a second
*  kid and then love that kid also? I'm never going to love it as much as the first, but I love them
*  both equally. It's just like my heart expanded. And so I think that people who are threatened by
*  love towards artificial agents, they don't need to be threatened for that reason.
*  Artificial agents will just, if done right, will just expand your capacity for love.
*  I think so. I agree. Beautifully put. Kate, this is awesome. I still didn't talk about half the
*  things I wanted to talk about, but we're already like way over three hours. So thank you so much.
*  I really appreciate you talking today. You're awesome. You're an amazing human being, a great
*  roboticist, great writer. Now it's an honor that you would talk with me. Thanks for doing it.
*  Right back at you. Thank you. Thanks for listening to this conversation with Kate Darling.
*  To support this podcast, please check out our sponsors in the description.
*  And now let me leave you with some words from Maya Angelou. Courage is the most important of all
*  the virtues because without courage, you can't practice any other virtue consistently.
*  Thank you for listening and hope to see you next time.
