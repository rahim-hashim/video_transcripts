---
Date Generated: April 12, 2024
Transcription Model: whisper medium 20231117
Length: 8309s
Video Keywords: ['agi', 'ai', 'ai podcast', 'artificial intelligence', 'artificial intelligence podcast', 'jeff hawkins', 'lex ai', 'lex fridman', 'lex jre', 'lex mit', 'lex podcast', 'mit ai']
Video Views: 289648
Video Rating: None
---

# Jeff Hawkins: The Thousand Brains Theory of Intelligence | Lex Fridman Podcast #208
**Lex Fridman:** [August 07, 2021](https://www.youtube.com/watch?v=Z1KwkpTUbkg)
*  The following is a conversation with Jeff Hawkins, a neuroscientist seeking to understand
*  the structure, function, and origin of intelligence in the human brain.
*  He previously wrote a seminal book on the subject titled On Intelligence, and recently
*  a new book called A Thousand Brains, which presents a new theory of intelligence that
*  Richard Dawkins, for example, has been raving about, calling the book, quote, brilliant
*  and exhilarating.
*  I can't read those two words and not think of him saying it in his British accent.
*  Quick mention of our sponsors, Codecademy, Biooptimizers, ExpressVPN, Asleep, and Blinkist.
*  Check them out in the description to support this podcast.
*  As a side note, let me say that one small but powerful idea that Jeff Hawkins mentions
*  in his new book is that if human civilization were to destroy itself, all of knowledge,
*  all our creations, would go with us.
*  He proposes that we should think about how to save that knowledge in a way that long
*  outlives us, whether that's on Earth, in orbit around Earth, or in deep space, and
*  then to send messages that advertise this backup of human knowledge to other intelligent
*  alien civilizations.
*  The main message of this advertisement is not that we are here, but that we were once
*  here.
*  This little difference somehow was deeply humbling to me, that we may, with some non-zero
*  likelihood, destroy ourselves, and that an alien civilization, thousands or millions
*  of years from now, may come across this knowledge store, and they would only with some low probability
*  even notice it, not to mention be able to interpret it.
*  And the deeper question here, for me, is what information in all of human knowledge is even
*  essential?
*  Does Wikipedia capture it, or not at all?
*  This thought experiment forces me to wonder what are the things we've accomplished and
*  are hoping to still accomplish that will outlive us?
*  Is it things like complex buildings, bridges, cars, rockets?
*  Is it ideas like science, physics, and mathematics?
*  Is it music and art?
*  Is it computers, computational systems, or even artificial intelligence systems?
*  I personally can't imagine that aliens wouldn't already have all of these things.
*  In fact, much more and much better.
*  To me, the only unique thing we may have is consciousness itself, and the actual subjective
*  experience of suffering, of happiness, of hatred, of love.
*  If we can record these experiences in the highest resolution directly from the human
*  brain, such that aliens will be able to replay them, that is what we should store and send
*  as a message.
*  Not Wikipedia, but the extremes of conscious experiences, the most important of which,
*  of course, is love.
*  This is the Lex Friedman Podcast, and here is my conversation with Jeff Hawkins.
*  We previously talked over two years ago.
*  Do you think there's still neurons in your brain that remember that conversation, that
*  remember me and got excited?
*  Like there's a Lex neuron in your brain that just like finally has a purpose?
*  I do remember our conversation.
*  I have some memories of it, and I formed additional memories of you in the meantime.
*  I wouldn't say there's a neuron or neurons in my brain that know you.
*  There are synapses in my brain that have formed that reflect my knowledge of you and the model
*  I have of you and the world.
*  Whether the exact same synapses were formed two years ago, it's hard to say because these
*  things come and go all the time.
*  One thing to note about brains is that when you think of things, you often erase the memory
*  and rewrite it again.
*  Yes, but I have a memory of you, and that's instantiated in synapses.
*  There's a simpler way to think about it.
*  We have a model of the world in your head, and that model is continually being updated.
*  I updated this morning.
*  You offered me this water, you said it was from the refrigerator.
*  I remember these things.
*  The model includes where we live, the places we know, the words, the objects in the world.
*  It's this monstrous model, and it's constantly being updated, and people are just part of
*  that model.
*  We're animals, so are the physical objects, so are events we've done.
*  There's no special in my mind, special place for the memories of humans.
*  Obviously, I know a lot about my wife and friends and so on, but it's not like a special
*  place for humans who are over here.
*  We model everything, and we model other people's behaviors too.
*  If I said you're a copy of your mind in my mind, it's just because I've learned how humans
*  behave, and I've learned some things about you, and that's part of my world model.
*  I just also mean the collective intelligence of the human species.
*  I wonder if there's something fundamental to the brain that enables that, so modeling
*  other humans with their ideas.
*  You're actually jumping into a lot of big topics.
*  Collective intelligence is a separate topic that a lot of people like to talk about.
*  We can talk about that.
*  That's interesting.
*  We're not just individuals.
*  We live in society and so on.
*  From our research point of view, again, let's just talk about it.
*  We study the neocortex.
*  It's a sheet of neural tissue.
*  It's about 75% of your brain.
*  It runs on this very repetitive algorithm.
*  It's a very repetitive circuit.
*  You can apply that algorithm to lots of different problems, but it's all underneath.
*  It's the same thing.
*  We're just building this model.
*  From our point of view, we wouldn't look for these special circuits someplace buried in
*  your brain that might be related to understanding other humans.
*  It's more like how do we build a model of anything?
*  How do we understand anything in the world?
*  Humans are just another part of the things we understand.
*  There's nothing to the brain that knows the emergent phenomena of collective intelligence.
*  I certainly know about that.
*  I've heard the terms.
*  I've read...
*  No, but that's...
*  Okay, right.
*  As an idea.
*  Well, I think we have language, which is built into our brains, and that's a key part of
*  collective intelligence.
*  There are some prior assumptions about the world we're going to live in.
*  When we're born, we're not just a blank slate.
*  We evolved to take advantage of those situations, yes.
*  But again, we study only part of the brain, the neocortex.
*  There's other parts of the brain that are very much involved in societal interactions
*  and human emotions and how we interact and even societal issues about how we interact
*  with other people, when we support them, when we're greedy, and things like that.
*  Certainly the brain is a great place where to study intelligence.
*  I wonder if it's the fundamental atom of intelligence.
*  Well, I would say it's absolutely an essential component, even if you believe in collective
*  intelligence as, hey, that's where it's all happening.
*  That's what we need to study, which I don't believe that, by the way.
*  I think it's really important, but I don't think that is the thing.
*  But even if you do believe that, then you have to understand how the brain works in
*  doing that.
*  It's more like we are intelligent individuals, and together we are much more magnified,
*  our intelligence.
*  We can do things that we couldn't do individually.
*  But even as individuals, we're pretty damn smart, and we can model things and understand
*  the world and interact with it.
*  So to me, if you're going to start someplace, you need to start with the brain.
*  Then you could say, well, how do brains interact with each other?
*  What is the nature of language?
*  And how do we share models that I've learned something about the world?
*  How do I share it with you?
*  That's really what sort of communal intelligence is.
*  I know something, you know something.
*  We've had different experiences in the world.
*  I've learned something about brains.
*  Maybe I can impart that to you.
*  You've learned something about physics, and you can impart that to me.
*  But it all comes down to even just the epistemological question of, well, what is knowledge?
*  And how do you represent it in the brain?
*  That's where it's going to reside, right, in our writings.
*  It's obvious that human collaboration, human interaction is how we build societies.
*  But some of the things you talk about and work on, some of those elements of what makes
*  up an intelligent entity is there with a single person.
*  Absolutely.
*  I mean, we can't deny that the brain is the core element here in, at least I think it's
*  obvious, the brain is the core element in all theories of intelligence.
*  It's where knowledge is represented.
*  It's where knowledge is created.
*  We interact, we share, we build upon each other's work.
*  But without a brain, you'd have nothing.
*  There would be no intelligence without brains.
*  So that's where we start.
*  I got into this field because I just was curious as to who I am.
*  How do I think?
*  What's going on in my head when I'm thinking?
*  What does it mean to know something?
*  I can ask what it means for me to know something independent of how I learned it from you or
*  from someone else or from society.
*  What does it mean for me to know that I have a model of you in my head?
*  What does it mean to know I know what this microphone does and how it works physically,
*  even when I can't see it right now?
*  How do I know that?
*  What does it mean?
*  How do the neurons do that at the fundamental level of neurons and synapses and so on?
*  Those are really fascinating questions.
*  And I'm happy to just happy to understand those if I could.
*  So in your new book, you talk about our brain, our mind as being made up of many brains.
*  So the book is called A Thousand Brains, A Thousand Brain Theory of Intelligence.
*  What is the key idea of this book?
*  The book has three sections and it has sort of maybe three big ideas.
*  So the first section is all about what we've learned about the neocortex and that's the
*  thousand brains theory.
*  Just to complete the picture, the second section is all about AI and the third section is about
*  the future of humanity.
*  So the thousand brains theory, the big idea there, if I had to summarize into one big
*  idea is that we think of the brain, the neocortex is learning this model of the world.
*  But what we learned is actually there's tens of thousands of independent modeling systems
*  going on.
*  And so each, what we call a column in the cortex, there's about 150,000 of them, is
*  a complete modeling system.
*  So it's a collective intelligence in your head in some sense.
*  So the thousand brains theory says about where do I have knowledge about this coffee cup
*  or where's the model of this cell phone?
*  It's not in one place.
*  It's in thousands of separate models that are complimentary and they communicate with
*  each other through voting.
*  So this idea that we have, we feel like we're one person.
*  That's our experience.
*  We can explain that.
*  But reality, there's lots of these, it's almost like little brains, but they're sophisticated
*  modeling systems, about 150,000 of them in each human brain.
*  And that's a total different way of thinking about how the neocortex is structured than
*  we or anyone else thought of even just five years ago.
*  So you mentioned you started this journey just looking in the mirror and trying to understand
*  who you are.
*  So if you have many brains, who are you then?
*  So it's interesting.
*  We have a singular perception, right?
*  We think, oh, I'm just here.
*  I'm looking at you.
*  But it's composed of all these things.
*  There's sounds and there's vision and there's touch and all kinds of inputs.
*  Yet we have the singular perception.
*  And what the thousand brain theory says, we have these models that are visual models,
*  we have models that are auditory models, models that are tactile models and so on.
*  But they vote.
*  And so in the cortex, you can think about these columns as like little grains of rice,
*  150,000 stacked next to each other.
*  And each one is its own little modeling system.
*  But they have these long range connections that go between them.
*  And we call those voting connections or voting neurons.
*  And so the different columns try to reach a consensus like, what am I looking at?
*  Okay, each one has some ambiguity, but they come to a consensus.
*  Oh, there's a water bottle I'm looking at.
*  We are only consciously able to perceive the voting.
*  We're not able to perceive anything that goes under the hood.
*  So the voting is what we're aware of.
*  The results of the vote.
*  Yeah, the resulting.
*  Well, you can imagine it this way.
*  We were just talking about eye movements a moment ago.
*  So as I'm looking at something, my eyes are moving about three times a second.
*  And with each movement, a completely new input is coming into the brain.
*  It's not repetitive, it's not shifting around.
*  It's completely new.
*  I'm totally unaware of it.
*  I can't perceive it.
*  But yet if I looked at the neurons in your brain, they're going on and off, on and off,
*  on and off.
*  But the voting neurons are not.
*  The voting neurons are saying, we all agree, even though I'm looking at different parts
*  of this, this is a water bottle right now.
*  That's not changing.
*  And it's in some position and pose relative to me.
*  So I have this perception of the water bottle about two feet away from me, a certain pose
*  to me.
*  That is not changing.
*  That's the only part I'm aware of.
*  I can't be aware of the fact that the inputs from the eyes are moving and changing and
*  all this other tapping.
*  So these long range connections are the part we can be conscious of.
*  The individual activity in each column doesn't go anywhere else.
*  It doesn't get shared anywhere else.
*  There's no way to extract it and talk about it or extract it and even remember it to say,
*  oh, yes, I can recall that.
*  But these long range connections are the things that are accessible to language and to our
*  you know, it's like the hippocampus, our memories, our short term memory systems and so on.
*  So we're not aware of 95% or maybe it's even 98% of what's going on in your brain.
*  We're only aware of this sort of stable, somewhat stable voting outcome of all these things
*  that are going on underneath the hood.
*  So what would you say is the basic element in the thousand brains theory of intelligence,
*  of intelligence?
*  Like, what's the atom of intelligence when you think about it?
*  Is it the individual brains?
*  And then what is a brain?
*  Well, let's let's can we just talk about what intelligence is first?
*  And then and then we can talk about the elements are.
*  So in my in my book, intelligence is the ability to learn a model of the world, to build
*  the internals to your head, a model that represents the structure of everything, you know,
*  to know what this is a table and that's a coffee cup and this is a goose neck lamp and all
*  this to know these things.
*  I have to have a model in my head.
*  I just don't look at him and go, what is that?
*  I already have internal representations of these things in my head and I had to learn them.
*  I wasn't born of any of that knowledge.
*  You were you know, we have some lights in the room here.
*  I you know, that's not part of my evolutionary heritage, right?
*  It's not in my genes.
*  So we have this incredible model and the model includes not only what things look like and
*  feel like, but where they are relative to each other and how they behave.
*  I've never picked up this water bottle before, but I know that if I took my hand on that
*  blue thing and I turn it, it'll probably make a funny little sound as the little plastic
*  things detach and then it'll rotate and it'll rotate a certain way and it'll come off.
*  How do I know that?
*  Because I have this model in my head.
*  So the essence of intelligence is our ability to learn a model and the more sophisticated
*  our model is, the smarter we are.
*  Not that there is a single intelligence because you can know about, you know, a lot about
*  things that I don't know and I know about things you don't know.
*  And we can both be very smart, but we both learned a model of the world through interacting
*  with it.
*  So that is the essence of intelligence.
*  Then we can ask ourselves, what are the mechanisms in the brain that allow us to do that?
*  And what are the mechanisms of learning?
*  Not just the neural mechanism.
*  What is the general process by how we learn a model?
*  So that was a big insight for us.
*  It's like, what are the, what is the actual things that, how do you learn this stuff?
*  It turns out you have to learn it through movement.
*  You can't learn it just by, that's how we learn.
*  We learn through movement.
*  We learn.
*  So you build up this model by observing things and touching them and moving them and
*  walking around the world and so on.
*  So either you move or the thing moves.
*  Somehow.
*  Yeah.
*  Obviously you can learn things just by reading a book, something like that.
*  But think about if I were to say, oh, here's a new house.
*  I want you to learn, you know, what do you do?
*  You have to walk.
*  You have to walk from room to room.
*  You have to open the doors, look around, see what's on the left.
*  What's on the right.
*  As you do this, you're building a model in your head.
*  It's just, that's what you're doing.
*  You can't just sit there and say, I'm going to grok the house.
*  No, you know, or you can, you don't even want to just sit down and read some
*  description of it, right?
*  You literally physically interact with the same with like a smartphone.
*  If I'm going to learn a new app, I touch it and I move things around.
*  I see what happens when I, when I do things with it.
*  So that's the basic way we learn in the world.
*  And by the way, when you say model, you mean something that can be used
*  for prediction in the future.
*  It's used for prediction and for behavior and planning.
*  Right.
*  Um, and does a pretty good job at doing so.
*  Yeah.
*  Here's the way to think about the model.
*  I, a lot of people got hung up on this.
*  So, um, you can imagine an architect making a model of a house, right?
*  So there's a physical model.
*  It's small.
*  And why do they do that?
*  Well, we do that because you can imagine what it would look
*  like from different angles.
*  You can say, okay, look at me.
*  Look at me there.
*  And you can also say, well, how, how far to get from the garage to the, to the
*  swimming pool or something like that.
*  Right.
*  You can imagine looking at this and you can say, what would
*  you be the view from this location?
*  So we build these physical models to let you imagine the future
*  and imagine that behaviors.
*  Now we can take that same model and put it in a computer.
*  So we now today they'll build models of houses and a computer and they, and
*  they do that using a set of, um, uh, we'll come back to this term in a moment,
*  reference frames, but eventually you assign a reference frame for the house
*  and you assign different things for the house in different locations.
*  And then the computer can generate an image and say, okay, this is what
*  it looks like in this direction.
*  The brain is doing something remarkably similar to this.
*  Surprising.
*  Um, it's using reference frames.
*  It's building these, it's similar to a model on a computer, which has the
*  same benefits of building a physical model.
*  It allows me to say, what would this thing look like if it was in this orientation?
*  What would likely happen if I pushed this button?
*  I've never pushed this button before, or how would I accomplish something?
*  I want to, I want to, um, uh, convey a new idea of learned.
*  Uh, how would I do that?
*  I can imagine in my head while I could talk about it, I could write a book, I
*  could do some podcasts, I could, um, you know, maybe tell my neighbor, you know,
*  and I can imagine the outcomes of all these things before I do any of them.
*  That's what the model lets you do it.
*  Let's just plan the future and imagine the consequences or actions.
*  Prediction.
*  You asked about prediction.
*  Prediction is not the goal of the model.
*  Prediction is an inherent property of it and it's how the model corrects itself.
*  So prediction is fundamental to intelligence.
*  It's fundamental to building a model and the model's intelligent.
*  And let me go back and be very precise about this prediction.
*  You can think of prediction two ways.
*  One is like, Hey, what would happen if I did this?
*  That's the type of prediction.
*  Um, that's a key part of intelligence, but isn't predictions like, Oh, what
*  does this, this is, this is what about are going to feel like when I pick it up?
*  You know, and that doesn't seem very intelligent.
*  But the way to think, one way to think about intelligent prediction is it's a
*  way for us to learn where our model is wrong.
*  So if I picked up this water bottle and it felt hot, I'd be very surprised.
*  Or if I picked up was very light, it would be very, I'd be surprised.
*  Or if I turn this top and it didn't, I had to turn it the other way.
*  I'd be surprised.
*  And so all those might have a prediction like, okay, I'm going to do it.
*  I'll drink some water.
*  I'm okay.
*  Okay.
*  I do this.
*  There it is.
*  I feel opening, right?
*  What if I had to turn it the other way?
*  Or what if it, it's split in two?
*  Then I say, oh my gosh, I, I'd misunderstood this.
*  I didn't have the right model.
*  This thing, my attention would be drawn to, I'll be looking at it going, well,
*  how the hell did that happen?
*  You know, why did it open up that way?
*  And I would update my model by doing it, just by looking at it and playing around
*  that update and say, this is a new type of water bottle.
*  But you, so you're talking about sort of, uh, complicated things like a water
*  bottle, but this also applies for just basic vision, just like seeing things.
*  That's almost like a precondition of just perceiving the world is predicting.
*  So it's just everything that you see is first passed through your prediction.
*  Everything you see and feel.
*  In fact, this is the insight I had, um, back in the late eighties, uh, no,
*  excuse me, early eighties.
*  And, um, and other people are reached the same idea is that every sensory input
*  you get, not just vision, but touch and hearing, you have an expectation about it.
*  And, um, a prediction.
*  Sometimes you can predict very accurately.
*  Sometimes you can't.
*  I can't predict what next word is going to come out of your mouth.
*  But as you start talking, I was better and better predictions.
*  And if you talked about some topics, I'd be very surprised.
*  Um, so I have this sort of background prediction that's going on all the
*  time for all of my senses.
*  Uh, I, again, the way I think about that is this is how we learn.
*  It's, it's more about how we learn.
*  It's the test of our understanding.
*  Our predictions are a test.
*  Is this really a water bottle?
*  If it is, I shouldn't see, you know, a little finger sticking out the side.
*  And if I saw a little finger stick and I was like, Oh, what the hell's going on?
*  That's not normal.
*  Um, I mean, that's fascinating that.
*  Let me linger on this for a second.
*  I, it really honestly feels that prediction is fundamental to everything, uh, to the
*  way our mind operates to intelligence.
*  So like, it's just a different way to see intelligence, which is like
*  everything starts at prediction.
*  And prediction requires a model.
*  You can't predict something unless you have a model of it.
*  Right.
*  But the action is prediction.
*  So like the thing, the model does is prediction and.
*  But it also, yeah.
*  And you, but you can then extend it to things like, Oh, what would happen if I
*  took this today, I went and did this.
*  What would be like that?
*  Or, or how you can extend prediction to like, Oh, I want to get a promotion at work.
*  Um, what action should I take?
*  And you can say, if I did this, I predict what might happen if I spoke to someone, I
*  predict what would might happen.
*  So it's not just low level predictions.
*  Yeah.
*  It's all prediction.
*  It's all predictions.
*  It's like this black box.
*  So you can ask basically any question, low level or high level.
*  So we started off with that observation.
*  It's all, it's like this, this nonstop prediction.
*  And I write about this in the book about, and then we asked how do neurons
*  actually make predictions and physically, like what does the neuron do when it
*  makes a prediction and, um, or the neural tissue does when it makes predictions.
*  And then we asked what are the mechanisms by how we build a model that
*  allows you to make predictions.
*  So we started with prediction as sort of the fundamental research agenda, if
*  in some sense, like, and say, well, we understand how the brain makes predictions.
*  We'll understand how it builds these models and how it learns.
*  And that's the core of intelligence.
*  So it was like, it was the key that got us in the door to say that is our
*  research agenda, understand predictions.
*  So in this whole process, where does intelligence originate?
*  Would you say?
*  So if we look at things that are much less intelligence to humans, and you
*  start to build up a human through the process of evolution, where's this magic
*  thing that has a prediction model or a model that's able to predict that starts
*  to look a lot more like intelligence?
*  Is there a place where Richard Dawkins wrote an introduction to your, uh, to
*  your book and excellent reduction?
*  I mean, it's, it puts a lot of things into context and it's funny just looking
*  at parallels for your book and Darwin's origin of species.
*  So Darwin wrote about the origin species.
*  So what is the origin of intelligence?
*  Well, we have a theory about it and it's just that it's a theory.
*  Theory goes as follows.
*  As soon as living things started to move, they're not just floating in sea.
*  They're not just a plant, you know, grounded someplace.
*  As soon as they started to move, there was an advantage to moving intelligently,
*  to moving in certain ways.
*  And there's some very simple things you can do, you know, bacteria or single
*  cell organisms can move towards a source of gradient of food or something like that.
*  But an animal that might know where it is and know where it's been and
*  how to get back to that place.
*  Or an animal that might say, oh, there was a source of food someplace.
*  How do I get to it?
*  Or there was a danger.
*  How do I get to it?
*  There was a mate.
*  How do I get to them?
*  Um, there was a big evolution advantage to that.
*  So early on, there was a pressure to start understanding your
*  environment, like where am I and where have I been and what happened
*  in those different places.
*  So we still have this neural mechanism in our brains.
*  Um, it's in the, in the mammals, it's in the hippocampus and entorhinal cortex.
*  These are older parts of the brain.
*  Um, and these are very well studied.
*  Um, we build a map of the, of our environment.
*  So these neurons in these parts of the brain know where I am in this room and
*  where the door was and things like that.
*  So a lot of other mammals have this.
*  All mammals have this, right?
*  And almost any, any animal that knows where it is and get around must have some
*  mapping system, must have some way of saying I've learned a map of my
*  environment, I have hummingbirds in my backyard and they, and they go to the
*  same places all the time and they have to, they must know where they are.
*  They just know where they are when they're not just randomly flying around.
*  They know, they know particular flowers they come back to.
*  So we all have this and it turns out it's very tricky to get neurons to do this,
*  to build a map of an environment.
*  It's just, and so we now know there's this, these, these famous studies that's
*  still very active about a place cells and grid cells and these other types of cells
*  in the older parts of the brain and how they build these maps of the world.
*  And it's really clever.
*  It's obviously been under a lot of evolutionary pressure over a long period
*  of time to get good at this.
*  So animals know where they are.
*  What we think has happened and there's a lot of evidence that suggests this is
*  that that mechanism we learned to map like a space is, was repackaged the same
*  type of neurons was repackaged into a more compact form and that became the
*  cortical column and it was, it was in some sense genericized, if that's a word.
*  It was turned into a very specific thing about learning maps of environments to
*  learning maps of anything, learning a model of anything, not just your space,
*  but coffee cups and so on.
*  And it got sort of repackaged into a more compact version, a more universal version
*  and then replicated.
*  So the reason we're so flexible is we have a very generic version of this mapping
*  algorithm and we have 150,000 copies of it.
*  Sounds a lot like the progress of deep learning.
*  How so?
*  So take neural networks that seem to work well for a specific task, compress them
*  and multiply it by a lot.
*  And then you just stack them on top.
*  It's like the story of transformers.
*  And yeah, but deep learning networks, they end up, you're replicating an element,
*  but you still need the entire network to do anything.
*  Right.
*  Here, what's going on each individual element is a complete learning system.
*  This is why I can take a human brain, cut it in half and it still works.
*  It's pretty amazing.
*  It's fundamentally distributed.
*  It's fundamentally distributed, complete modeling systems.
*  So, but that's, that's our story.
*  We like to tell, I, I, I would guess it's, it's likely largely right.
*  Um, but you know, it's, there's a lot of evidence supporting that story.
*  This evolutionary story.
*  Um, the thing which brought me to this idea is that the human brain, um, got big
*  very quickly, so that, that led to the proposal a long time ago that, well,
*  there's this common element just instead of creating new things, it just replicated
*  something we also are extremely flexible.
*  We can learn things that we had no history about.
*  Right.
*  And so that tells it that the learning algorithm is very generic.
*  It's very kind of universal because it's, it doesn't assume any prior
*  knowledge about what it's learning.
*  And, um, so you combine those things together, um, and you say, okay, well,
*  how did that come about?
*  Where did that universal algorithm come from?
*  It had to come from something that wasn't universal.
*  It came from something that was more specific.
*  And so anyway, this led to our hypothesis that you would find grid cells and
*  place cell equivalents in the neocortex.
*  And when we first published our first papers on this theory, we didn't
*  know of evidence for that.
*  It turns out there was some, but we didn't know about it.
*  Uh, and since then, um, so then we became aware of the evidence for grid cells
*  and parts of the neocortex.
*  And then now there's been new evidence coming out.
*  There's some, uh, interesting papers that came out just January of this year.
*  So our, one of our predictions was if this evolutionary hypothesis is correct,
*  we would see grid cell place, cell equivalents cells that work like them
*  to every column in the neocortex.
*  And that's starting to be seen.
*  What does it mean that, uh, why is it important that they're present?
*  Because it tells us, well, we're asking about the evolutionary
*  origin of intelligence, right?
*  So our theory is that these columns in the cortex are working on the same
*  principles, the modeling systems.
*  And it's hard to imagine how neurons do this.
*  And so we said, Hey, it's really hard to imagine how neurons could
*  learn these models of things.
*  I'm going to talk about the details of that if you want, but let's, um, but
*  there's this other part of the brain.
*  We know that learns models of environments.
*  So could that mechanism to learn to model this room, but you still
*  learn to model the water bottle.
*  Is it the same mechanism?
*  So we said, it's much more likely that the brain's using the same mechanism,
*  which case it would have these equivalents cell types.
*  So it's basically the whole theory is built on the idea that, um, these
*  columns have reference frames and they're learning these models and these,
*  these grid cells create these reference frames.
*  So it's, it's basically, um, the major, in some sense, the major predictive,
*  um, part of this theory is that we will find these equivalent mechanisms in
*  the, each column in the near cortex, which tells us that that, that, that,
*  that's what they're doing.
*  They're learning these sensory motor models of the world.
*  Um, so just to, we're pretty confident that would happen, but now we're
*  seeing the evidence.
*  So the evolutionary process nature does a lot of copy and paste and see what happens.
*  Yeah.
*  Yeah.
*  There's no direction to it, but, but, um, it just found out like, Hey, if I took
*  this, these elements and, and made more of them, what happens?
*  And let's hook them up to the eyes and let's look up to ears.
*  And, and, um, and that seems to work pretty well for us again, just to take
*  a quick step back to our conversation of collective intelligence, do you sometimes
*  see that as just another copy and paste aspect is a copy and pasting these brains
*  and in humans and making a lot of them and then creating a social structures
*  that then almost operate as a single brain.
*  Uh, I wouldn't have said it, but you said it sounded pretty good.
*  So to you, the brain is fundamental is, uh, is like, um, is its own thing.
*  I mean, our goal is to understand how the neurocortex works.
*  We can argue how essential that is to understand the human brain, because
*  it's not the entire human brain.
*  You can argue how essential that is to understanding human intelligence.
*  You can argue how essential this to, um, to, uh, you know, sort of
*  communal intelligence.
*  Um, I, I'm not, I didn't, our goal was to understand the neurocortex.
*  Yeah.
*  So what is in your cortex and where does it fit in, um, the various
*  aspects of what the brain does?
*  Like how important is it to you?
*  Well, obviously again, we, I mentioned again in the beginning, it's it, it's
*  about 70 to 75% of the volume of a human brain.
*  So it's, you know, it dominates our brain in terms of size, not in terms
*  of number of neurons, but in terms of size.
*  Size isn't everything, Jeff.
*  I know.
*  But it's, it's nothing, it's nothing.
*  It's not that we know that all high level vision, hearing and touch happens
*  in the neuro cortex.
*  We know that all language occurs and is understood in the neuro cortex,
*  whether that's spoken language, written language, sign language, whether
*  language of mathematics, language of physics, music, math, you know, we
*  know that all high level planning and thinking occurs in the neuro cortex.
*  If I were to say, you know, what part of your brain designed a computer and
*  understands programming and, and creates music, it's all the neuro cortex.
*  So then that's just like an undeniable fact.
*  If, but then there's other parts of our brain are important too, right?
*  Our emotional states, our body regulating our body.
*  So the way I like to look at it is, you know, could you, can you understand
*  the neuro cortex about the rest of the brain?
*  And some people say you can't.
*  And I'm, I think absolutely you can.
*  It's not that they're not interacting, but you can understand it.
*  Can you understand the neuro cortex without understanding the emotions of fear?
*  Yes, you can.
*  You can understand how the system works.
*  It's just a modeling system.
*  I make the analogy in the book that it's, it's like a map of the world and how
*  that map is used depends on who's using it.
*  So how our map of our world in our, you know, cortex, how we, how we manifest as
*  a human depends on the rest of our brain.
*  What are our motivations?
*  You know, what are my desires?
*  Am I a nice guy or not a nice guy?
*  Am I a cheater or my, you know, or not a cheater?
*  You know, how important different things are in my life.
*  So, um, so, but the, the, you know, cortex can be understood on its own.
*  Um, and, and I say that as a neuroscientist, I know there's all these interactions
*  and I want to say, I don't know them and we don't think about them, but from the
*  lay person's point of view, you can say it's a modeling system.
*  Um, I don't generally think too much about the communal aspect of intelligence,
*  which you've brought up a number of times already.
*  Um, so that's not really been my concern.
*  I just wonder if there's a continuum from the origin of the universe, like
*  this pockets of complexities that form.
*  Yeah.
*  Living organisms.
*  I wonder if, if we're just, if you look at humans, we feel like we're at the top.
*  But I wonder if there's like just where everybody probably every living type
*  pocket of complexity is probably thinks they're the, uh, pardon the French.
*  They're the shit.
*  Yeah.
*  They're, they're at the top of the parent.
*  Well, if they're thinking, um, well, then what is thinking?
*  What the.
*  In a sense, the whole point is in their sense of the world, they, their sense.
*  Is it there at the top of it?
*  I think what does the turtle, but you're, you're, you're bringing up, you know,
*  the problems of complexity and complexity theory are, you know, it's a huge,
*  interesting problem in science.
*  Um, and you know, I think we've made surprisingly little progress in
*  understanding complex systems in general.
*  Um, and so, you know, the Santa Fe Institute was founded to study this.
*  And even the scientists there will say it's really hard.
*  We haven't really been able to figure out exactly, you know, that
*  science hasn't really congealed yet.
*  We're still trying to figure out the basic elements of that science.
*  Uh, what, you know, where does complexity come from and what is it and how you
*  define it, whether it's DNA creating bodies or phenotypes or if it's individuals
*  creating societies or ants and, you know, markets and so on, it's, it's a very
*  complex thing.
*  I'm not a complexity theorist person, right?
*  Um, I, I think you need to ask, well, the brain itself is a complex system.
*  So can we understand that?
*  Um, I think we've made a lot of progress understanding how the brain works.
*  So, uh, but I haven't brought it out to like, oh, well, where are we
*  on the complexity spectrum?
*  I was like, um, it's a great question.
*  I'd prefer for that answer to be, we're not special.
*  It seems like if we're honest, most likely we're not special.
*  So if there is a spectrum or probably not in some kind of significant place,
*  I think there's one thing we could say that we are special.
*  And again, only here on earth, I'm not saying that is that if we think about
*  knowledge, what we know, um, we clearly human brains have, um, the only brains
*  that have a certain types of knowledge with the only brains on, on this earth,
*  understand what the earth is, how old it is, that the universe is a picture
*  as a whole with the only organisms understand DNA and the origins of,
*  you know, of species, uh, no other species on, on this planet has that knowledge.
*  So if we think about, I like to think about, you know, one of the endeavors of
*  humanity is to understand the universe as much as we can.
*  Um, I think our species is further along in that undeniably, um, whether our
*  theories are right or wrong, we can debate, but at least we have theories.
*  You know, we, we know that what the sun is and how it's fusion is and how, what
*  black holes are and, you know, we know general theory, relativity, and no other
*  animal has any of this knowledge.
*  So, so in that sense of we're special, uh, are we special in terms of the, the,
*  the hierarchy of complexity in the universe?
*  Probably not.
*  Can we look at a neuron?
*  Yeah.
*  You say that prediction happens in the neuron.
*  What does that mean?
*  So the neuron traditionally seen as the basic element of the brain.
*  So we, I mentioned this earlier that prediction was our research agenda.
*  Yeah.
*  We said, okay.
*  Um, how does the brain make a prediction?
*  Like I, I'm about to grab this water bottle and my brain is predicting
*  what I'm going to feel on, on all my parts of my fingers.
*  If I felt something really odd on any part here, I'd notice it.
*  So my brain is predicting what it's going to feel as I grab this thing.
*  So what is that?
*  How does that manifest itself in neural tissue?
*  Right.
*  We got brains made of neurons and there's chemicals and there's neurons and there's
*  spikes and the connect, you know, where, where is the prediction going on?
*  And one argument could be that, well, when I'm predicting something, um, a
*  neuron must be firing in advance.
*  It's like, okay, this neuron represents what you're going to feel and it's
*  firing, it's sending a spike.
*  And certainly that happens to some extent, but are you pretty sure that
*  happens to some extent, but our predictions are so ubiquitous that we're making so
*  many of them, which we're totally unaware of just the vast majority of them.
*  You have no idea that you're doing this.
*  Um, that it, there wasn't really, we were trying to figure how could this be?
*  Where, where's these, where are these happening?
*  Right.
*  And I won't walk you through the whole story unless you insist upon it, but we
*  came to the realization that most of your predictions are occurring inside
*  individual neurons, especially these, the most common are in the parameter cells.
*  And there are, there's a property of neurons.
*  We, everyone knows, or most people know that a neuron is a cell and it has this
*  spike called an action potential and it sends information, but we now know that
*  there's these spikes internal to the neuron.
*  They're called dendritic spikes.
*  They travel along the branches of the neuron and they don't leave the neuron.
*  They're just internal only.
*  There's far more dendritic spikes than there are action potentials far more.
*  They're happening all the time.
*  And what we came to understand that those dendritic spikes, the ones that are
*  occurring are actually a form of prediction.
*  They're telling the neuron, the neuron is saying, I expect that I
*  might become active shortly.
*  And that internal, so the internal spike is a way of saying, you're going to,
*  you might be generating external spikes soon.
*  I predicted you're going to become active.
*  And, and we've, we've, we've, we wrote a paper in 2016, which explained
*  how this manifests itself in neural tissue and how it is that this all works together.
*  But the vast majority, we think it's, there's a lot of evidence supporting it.
*  Um, so we, that's where we think that most of these predictions are internal.
*  That's why you can't be, they're internal in a neuron.
*  You can't perceive them.
*  Well, from understanding the prediction mechanism of a single neuron, do you
*  think there's deep insights to be gained about the prediction capabilities of the
*  mini brains within the bigger brain and the.
*  Oh yeah.
*  Yeah.
*  Yeah.
*  So having a prediction side of their individual neuron is not that useful.
*  You know what?
*  So what, um, the way it manifests itself in neural tissue is that when a neuron,
*  a neuron emits these spikes are very singular type event.
*  If a neuron is predicting that it's going to be active, it emits its spike very,
*  a little bit sooner, just a few milliseconds sooner than it would have otherwise.
*  It's like, I give the analogy of the book is like a sprinter on a, on a
*  starting blocks in a race.
*  And if someone says, get ready, set, you get up and you're ready to go.
*  And then when you're a start, you get a little bit earlier start.
*  So that it's that, that ready set is like the prediction and the
*  neurons like ready to go quicker.
*  And what happens is when you have a whole bunch of neurons together and they're
*  all getting these inputs, the ones that are in the predictive state, the ones
*  that are anticipating to become active.
*  If they do become active, they, they happen sooner, they disable everything
*  else and it leads to different representations in the brain.
*  So you have to, it's not isolated just to the neuron.
*  The prediction occurs with the neuron, but the network behavior changes.
*  So what happens under different predictions, different inputs have
*  different representations.
*  So how I, what I predict, um, it's going to be different under different contexts.
*  You know, what my input will be is different under different contexts.
*  So this is, this is a key to the whole theory, how this works.
*  So the theory of the thousand brains, if you were to count the number of brains,
*  how would you do it?
*  The thousand main theory says that basically every cortical column in the,
*  in your neuro cortex is a complete modeling system.
*  And that when I asked, where do I have a model of something like a coffee cup?
*  It's not in one of those models.
*  It's in thousands of those models.
*  There's thousands of models of coffee cups.
*  That's what the thousand brains there is a voting mechanism.
*  Then there's a voting mechanism, which you lead, which you're, which is the
*  thing you're, which you're conscious of, which leads to your singular perception.
*  Um, that's why you, you perceive something.
*  So that's the thousand brain theory.
*  The details of how we got to that theory, um, or complicated.
*  It wasn't, we just thought of it one day.
*  And one of those details that we had to ask, how does a model make predictions?
*  And we talked about just these predictive neurons.
*  That's part of this theory.
*  That's like saying, oh, it's a detail, but it was like a crack in the door.
*  It's like, how are we going to figure out how these neurons built through this?
*  You know, what is going on here?
*  So we, we just looked at prediction as like, well, we know that's ubiquitous.
*  We know that every part of the cortex is making predictions.
*  Therefore, whatever the predictive system is, it's going to be everywhere.
*  We know there's a gazillion predictions happening at once.
*  So this, here we can start teasing apart, you know, ask questions about, you know,
*  how could neurons be making these predictions?
*  And that sort of built up to now what we have the thousand brains theory, which
*  is complex, you know, it's just, I can state it simply, but we just didn't think
*  of it, we had to get there step by step.
*  Very, it took years to get there.
*  And where does a reference frames fit in?
*  So, yeah.
*  Okay.
*  So again, a reference frame, I mentioned earlier about the model of a house.
*  And I said, if you're going to build a model of a house in a computer, they
*  have a reference frame and you can think of reference, same with like a Cartesian
*  coordinates, like X, Y, and Z axes.
*  So I could say, oh, I'm going to design a house.
*  I can say, well, the, the front door is at this location, XYZ, and the roof is
*  at this location, XYZ and so on.
*  That's the type of reference frame.
*  So it turns out for you to make a prediction and then I walk you through
*  the thought experiment in the book where I was predicting what my finger was
*  going to feel when I touched the coffee cup was a ceramic coffee cup, but this
*  one will do.
*  Um, and what I realized is that to make a prediction, my finger is going to feel
*  like it's going to feel different than this, which it felt different if I touched
*  the hole or this thing on the bottom.
*  Make that prediction.
*  The cortex needs to know where the finger is, the tip of the finger
*  relative to the coffee cup.
*  And exactly relative to the coffee cup.
*  And to do that, I have to have a reference frame for the coffee cup.
*  It has to have a way of representing the location of my finger to the coffee cup.
*  And then we realized, of course, every part of your skin has to have a reference
*  frame relative to things that touch.
*  And then we did the same thing with vision, but so the idea that a reference
*  frame is necessary to make a prediction when you're touching something or when
*  you're seeing something and you're moving your eyes, you're moving your fingers.
*  It's just a requirement to know what to predict.
*  If I have a structure, I'm going to make a prediction.
*  I have to know where it is I'm looking or touching it.
*  So then we said, well, how do neurons make reference frames?
*  It's not obvious.
*  You know, XYZ coordinates don't exist in the brain.
*  It's just not the way it works.
*  So that's when we looked at the older part of the brain, the hippocampus
*  and the entorhinal cortex, where we knew that in that part of the brain, there's
*  a reference frame for a room or reference frame for environment.
*  Remember I talked earlier about how you could make a map of this room.
*  So we said, oh, that they are implementing reference frames there.
*  So we knew that a reference frames needed to exist in every quarter of a column.
*  And so that was a deductive thing.
*  We just deduced it has to go.
*  So you take the old mammalian ability to know where you are in a particular space
*  and you start applying that to higher and higher levels.
*  Yeah.
*  You first you apply it to like where your finger is.
*  So here's the way I think about it.
*  The old part of the brain says, where's my body in this room?
*  Yeah.
*  The new part of the brain says, where's my finger relative to this, this object?
*  Yeah.
*  Where is a section of my retina relative to this object?
*  Like where, where I'm looking at one little corner, where is that relative
*  to this patch of my retina?
*  Yeah.
*  Um, and then we take the same thing and apply it to concepts, mathematics,
*  physics, you know, humanity, whatever you want to think of.
*  In the ventry, you're pondering your own mortality.
*  Well, whatever.
*  But the point is when we think about the world, when we have knowledge about the
*  world, how is that knowledge organized?
*  Lex, where do you, where is it in your head?
*  The answer is it's in reference frames.
*  So the way I learned the structure of this water bottle, where the features are
*  relative to each other, when I think about history or democracy or mathematics,
*  the same basic underlying structures happening, there's reference frames for
*  where the knowledge that you're assigning things to.
*  So in the book, I go through examples like mathematics and language and
*  politics, um, but the evidence is very clear in the neuroscience, the same
*  mechanism that we use to model this coffee cup, we're going to use to model
*  high level thoughts, your, your, your demise of humanity, whatever you want
*  to think about.
*  It's interesting to think about how different are the representations of
*  those higher dimensional concepts, uh, higher level concepts, how different the
*  representation there is in terms of reference frames versus spatial.
*  But interesting thing it's, it's, it's a different application, but it's
*  the exact same mechanism.
*  But isn't there some aspect to, uh, higher level concepts that they seem to be
*  hierarchical, like they just seem to integrate a lot of information into that.
*  So is our physical objects.
*  So take this water bottle.
*  Uh, I'm not particular to this brand, but this is a Fiji water bottle and it has,
*  um, a logo.
*  I use this example in my book.
*  Our, our company's coffee cup has a logo on it, but this object is hierarchical.
*  It is, it's got like a cylinder and a cap, but then has this logo on it.
*  And the logo has a word, the word has letters, the letters have different
*  features and so I don't have to remember.
*  I don't have to think about this.
*  So I say, oh, there's a Fiji logo on this water bottle.
*  I don't have to go through and say, oh, what is the Fiji logo?
*  It's the F and I and a J and I, and there's a Hibiscus flower and, and, uh,
*  oh, it has the pest, you know, the stamen on it.
*  I don't have to do that.
*  I just incorporate all of that in some sort of hierarchical representation.
*  I say, um, you know, put this logo on this water bottle.
*  And, and, and then the logo has a word and the word has letters, all hierarchical.
*  There's all that stuff is big.
*  It's amazing that the brain instantly just does all that.
*  Yeah.
*  The idea that there's, there's water, it's liquid and the idea that you can, uh,
*  drink it when you're thirsty, the idea that there's brands and then there's like
*  all of that information is instantly like built into the whole thing.
*  When you want to proceed.
*  So I wanted to get back to your point about hierarchical representation.
*  The world itself is hierarchical, right?
*  And I can take this microphone in front of me.
*  I know inside there's going to be some electronics.
*  I know there's going to be some wires and I know there's going to be a little
*  dive from them was back and forth.
*  Um, I don't see that, but I know it.
*  Um, so everything in the world is hierarchical.
*  Just go into a room.
*  It's composed of other components of kitchen has a refrigerator.
*  You know, the refrigerator has a door.
*  The door has a hinge, a hinge has screws and pin.
*  Yeah.
*  I mean, so anyway, the modeling system that exists in every cortical column
*  learns the hierarchical structure of objects.
*  So it's a very sophisticated modeling system in this grain of rice.
*  It's hard to imagine, but this granite rice can do really sophisticated things.
*  It's got a hundred thousand neurons in it.
*  Um, it's very sophisticated.
*  So that same mechanism that can model a water bottle or coffee cup can model
*  conceptual objects as well.
*  It's if that's the beauty of this discovery that this guy, Vernon
*  Mountcastle made many, many years ago, which is that there's, there's a single
*  cortical algorithm underlying everything we're doing.
*  So common sense concepts and higher level concepts are all represented in the same
*  way.
*  They're set in the same mechanisms.
*  Yeah.
*  It's a little bit like a computers, right?
*  All computers are universal turning machines.
*  Even the little teeny one that's in my toaster and the big one that's running
*  some cloud service in place.
*  Um, they're all running on the same principle.
*  They can provide different things.
*  So the brain is all built on the same principle.
*  It's all about learning these models, structured models, using movement and
*  reference frames, and it can be applied to something as simple as a water bottle
*  and a coffee cup, and it can be like, just thinking like what's the future of
*  humanity and you know, why do you have a hedgehog on your desk?
*  I don't know.
*  Nobody knows.
*  I think it's a hedgehog.
*  That's right.
*  It's a hedgehog in the fog.
*  It's a Russian reference.
*  Does it give you any inclination or hope about how difficult that is to engineer
*  common sense reasoning?
*  So how complicated is this whole process?
*  So looking at the brain, is this a marvel of engineering or is it pretty dumb
*  stuff stack on top of each other over?
*  I can be both.
*  Can't can't be both, right?
*  I don't know if it can be both because if it's an incredible engineering job,
*  that means it's so evolution did a lot of work.
*  It, yeah, but then, but then it just copied that.
*  Right.
*  So as I said earlier, the figuring out how to model something like a space is
*  really hard and evolution had to go through a lot of trick and these, these,
*  these cells I was talking about, these grid cells and place cells, they're
*  really complicated.
*  This is not simple stuff.
*  This neural tissue works on these really unexpected, weird mechanisms.
*  Um, but it did it.
*  It figured it out.
*  But now you could just make lots of copies of it.
*  But then finding, yeah, so it's a, it's a very interesting idea.
*  That's a lot of copies of a basic mini brain.
*  But the question is how difficult it is to find that mini brain that you can
*  copy and paste effectively.
*  Today, we know enough to build this.
*  I'm sitting here with, you know, I know the steps we have to go.
*  There's still some engineering problems to solve, but we know enough and it's
*  not like, Oh, this is an interesting idea.
*  We have to go think about it for another few decades.
*  No, we actually understand it pretty well details.
*  So not all the details, but most of them.
*  So it's complicated, but it is an engineering problem.
*  So in my company, we are working on that.
*  We are basically laid out roadmap, how we do this.
*  Um, it's not going to take decades.
*  It's been a few years, um, optimistically, but I think that's possible.
*  Um, it's, you know, complex things.
*  If you understand them, you can build them.
*  So in which domain do you think it's best to build them?
*  Are we talking about robotics, like, uh, entities that operate in the physical
*  world, that are able to interact with that world, or we're talking about entities
*  that operate in the digital world.
*  Are we talking about something more like, uh, more specific, like is done in the,
*  uh, machine learning community where you look at natural language or computer
*  vision, where do you think is easiest?
*  Um,
*  it's the first, it's the first two more than the third one, I would say.
*  Um, again, again, let's just use computers as an analogy.
*  Um, the pioneers in computing people like Don by Norman, um, um, Turing,
*  they created this thing, you know, we now call the universal Turing machine,
*  which is a computer, right?
*  Did they know how it was going to be applied, where it was going to be used?
*  You know, could they envision any of the future?
*  No, they just said, this is like a really interesting computational idea
*  about algorithms and how you can implement them in, in a machine.
*  And we're doing something similar to that today.
*  Like we are, we are building this sort of universal learning principle that can be
*  applied to many, many different things.
*  But the, the robotics piece of that, the interactive,
*  all right.
*  That has to be specific.
*  You can think of this cortical column as a, what we call a sensory motor
*  learning system.
*  It has the idea that there's a sensor and then it's moving.
*  That sensor can be physical.
*  It could be like my finger and it's moving in the world.
*  It can like my eye and it's physically moving.
*  It can also be virtual.
*  So it could be, um, an example would be, I could have a system that lives in the
*  internet that, that actually samples information on the internet and moves by
*  following links.
*  That's, that's a sensory motor system.
*  So something that echoes the process of a finger moving along a copy.
*  In a very, very loose sense.
*  It's, it's like, again, learning is inherently about the subbing, the
*  structure of the world and discover the structure of the world.
*  You have to move through the world, even if it's a virtual world, even if it's a
*  conceptual world, you have to move through it.
*  You don't, it doesn't exist in one.
*  It has some structure to it.
*  So here's, here's a couple of predictions that getting what you're talking about
*  in humans, the same algorithm is does robotics, right?
*  It moves my arms, my eyes, my body.
*  Right.
*  Um, and so in my, in the future, to me, robotics and AI will merge.
*  They're not going to be separate fields because they're going to, the, the, the
*  algorithms to really controlling robots are going to be the same algorithms we
*  have in our bread, the brain at these sensory motor algorithms.
*  Today we're not there, but I think that's going to happen.
*  And, um, and then so, but not all AI systems will have be robotics.
*  Um, you can have systems that have very different types of embodiments.
*  Some will have physical movements.
*  Some will have not have physical movements.
*  It's a very generic learning system.
*  Again, it's like computers, the Turing machine is, it's like, it doesn't say
*  how it's supposed to be implemented.
*  It doesn't tell you how big it is.
*  It doesn't tell you what you can apply it to, but it's an
*  interesting, it's a computational principle.
*  Cortical column equivalent is a computational principle about learning.
*  It's about how you learn and it can be applied to a gazillion things.
*  This is what I think this is.
*  I think this impact of AI is going to be as large, if not larger than
*  computing has been in the last century by far, because it's, it's getting
*  at a fundamental thing.
*  It's not a vision system or a learning system.
*  It's a, it's not a vision system or a hearing system.
*  It is a learning system.
*  It's a fundamental principle, how you learn the structure in the world, how
*  you can gain knowledge and be intelligent.
*  And that's what the thousand brain says was going on.
*  And we have a particular implementation in our head, but it doesn't
*  have to be like that at all.
*  Do you think there's going to be some kind of impact?
*  Okay.
*  Let me ask it another way.
*  What do, uh, increasingly intelligent AI systems do with us humans in the
*  following way, like how hard is the human in the loop problem?
*  How hard is it in turn to interact the finger on the coffee cup equivalent of
*  having a conversation with a human being?
*  So how hard is it to fit into our little human world?
*  Uh, I don't, I think it's a lot of engineering problems.
*  I don't think it's a fundamental problem.
*  I could ask you the same question.
*  How hard is it if a computer is to fit into a human world?
*  Right.
*  That mean that's essentially what I'm asking.
*  Like how, um, much are we, uh, elitist are we as humans?
*  Like we tried to keep out, uh, systems.
*  I don't know.
*  I, I, I don't know.
*  So I think that I'm not sure that's the right question.
*  Let's, let's look at computers as an analogy.
*  Computers are million times faster than us.
*  They do things we can't understand.
*  Most people have no idea what's going on when they use computers.
*  How they, how we integrate them in our society?
*  Um, well, they're, we don't think of them as their own entity.
*  They're not living things.
*  Um, we don't afford them rights.
*  Um, we, uh, we rely on them.
*  Our survival as a 7 billion people or something like that is relying on computers.
*  Now.
*  Um, don't you think that's a fundamental problem that we see them as something
*  we can't, we don't give rights to computers.
*  So yeah, computers, so, uh, robots, computers, intelligence systems, it feels
*  like for them to operate successfully, they would need to have a lot of the
*  elements that we would start having to think about, like, should this entity
*  have rights?
*  I don't think so.
*  I think it's tempting to think that way.
*  First of all, I don't think anyone, hardly anyone thinks that's for computers today.
*  No one says, oh, this thing needs a right.
*  I shouldn't be able to turn it off or, you know, if I throw it in the trash can
*  and, you know, and hit it with a sledgehammer, my, my forming criminal act.
*  No, no one thinks that.
*  Um, and now we think about intelligent machines, which is where you're going.
*  Um, and, and all of a sudden like, well, no, we can't do that.
*  I think the basic problem we have here is that people think intelligent
*  machines will be like us.
*  They're going to have the same emotions as we do the same feelings as we do.
*  What if I can build an intelligent machine that have absolutely could care less
*  about whether it was on or off or destroyed or not.
*  It just doesn't care.
*  It's just like a map.
*  It's just a modeling system.
*  It has no desires to live.
*  Nothing.
*  Is it possible to create a system that can model the world deeply and not care
*  about whether it lives or dies?
*  Absolutely.
*  No question about it.
*  To me, that's not 100% obvious.
*  It's obvious to me.
*  So we can, we can debate it if you want.
*  Yeah.
*  Where does your, where does your desire to live come from?
*  It's an old evolutionary design.
*  I mean, we can argue, does it really matter if we live or not?
*  Objectively, no.
*  Right.
*  We're all going to die eventually.
*  But evolution wants us want to live.
*  Evolution makes us want to fight to live.
*  Evolutionists want to care and love one another and to care for our children
*  and our, and our relatives and our family and, and so on.
*  And those are all good things.
*  But they come about not because we're smart, because we're animals.
*  They grew up, you know, the hummingbird in my backyard cares about its offspring.
*  You know, the, every living thing in some sense cares about, you know,
*  surviving.
*  But when we talk about creating intelligent machines, we're not creating life.
*  We're not creating evolving creatures.
*  We're not creating living things.
*  We're just creating a machine that can learn really sophisticated stuff.
*  And that machine, it may even be able to talk to us, but it doesn't, it's not
*  going to have a desire to live unless somehow we put it into that system.
*  Well, there's learning, right?
*  The thing is, but you don't learn to want to live.
*  It's built into you.
*  It's hard.
*  So people like Ernest Becker argue.
*  So, okay.
*  There's the fact of finiteness of life.
*  The way we think about it is something we learn.
*  Uh, perhaps.
*  So, okay.
*  Yeah.
*  And some people decide they don't want to live.
*  And some people decide, you know, you couldn't, but the desire to live is
*  built in being a, right.
*  But I think what I'm trying to get to is, uh, in order to accomplish goals, it's
*  useful to have the urgency of mortality.
*  This is what the Stoics talked about is meditating in your mortality.
*  It might be a very useful thing to do to die and have the urgency of death and
*  to realize that to conceive yourself as an entity that operates in this world
*  that eventually will no longer be a part of this world and actually conceive of
*  yourself as a conscious entity might be very useful for you to be a system that
*  makes sense of the world.
*  Otherwise you might get lazy.
*  Well, okay.
*  We're going to build these machines.
*  Right.
*  So we're talking about building AI.
*  What, but we're, we're building the, um, uh, the, the, the equivalent of the
*  cortical columns, the, uh, the neocortex, the neocortex.
*  And the question is where do they arrive at?
*  Cause we're not hard coding everything in where, uh, well, well, in terms of, if
*  you build the neocortex equivalent, it will not have any of these desires
*  or emotional states.
*  Now you can argue that that neocortex won't be useful unless I give it some
*  agency, unless I give it some desire, unless I give it some motivation.
*  Otherwise you'll be just lazy and do nothing.
*  Right.
*  You could argue that.
*  Um, but on its own, it's not going to do those things.
*  It's just not, it's just not going to sit there and say, I understand the world.
*  Therefore I care to live.
*  No, it's not going to do that.
*  It's just gonna say, I understand the world.
*  Why is that obvious to you?
*  Why, why, why don't you think it's okay?
*  Let me ask it this way.
*  Do you think it's possible it will at least, uh, assigned to itself, um, agency
*  and, um, perceive itself in this world as being a conscious entity as a useful way
*  to operate in the world and to make sense of the world.
*  I think intelligent machine can be conscious, but that doesn't not again,
*  imply any of these, um, these desires and goals and that you're worried about.
*  It, uh, we can, I have a, we can talk about what it means for
*  any machine to be conscious.
*  And by the way, not worry about, but get excited about.
*  It's not necessarily that we should worry about it.
*  So I think there's a legitimate problem or not problem, a question asked.
*  If you build this modeling system, what's it going to model?
*  Yes.
*  Right.
*  What's it, what's its desire?
*  What is, what's its goal?
*  What are we applying it to?
*  Right.
*  So that's an interesting question.
*  Um, one thing if it, and it depends on the application, it's not something
*  that inherent to the modeling system.
*  It's something we apply to the modeling system in a particular way.
*  So if I wanted to make a really smart car, it would have to know about
*  driving and cars and what's important in driving and cars.
*  It's not going to figure that out on its own.
*  It's not going to sit there and say, you know, I've understood the world
*  and I've decided, you know, no, no, no, no, we're going to have to tell it.
*  We're going to have to say like, so I imagine I make this car really smart.
*  It learns about your driving habits.
*  It learns about the world and it's just, you know, is it one day going to wake
*  up and say, you know what, I'm tired of driving and doing what you want.
*  I think I have better ideas about how to spend my time.
*  Okay.
*  I'm not going to do that.
*  Well, part of me is playing a little bit of devil's advocate, but part of me is
*  also trying to think through this because I've studied cars quite a bit and I
*  studied pedestrians and cyclists quite a bit.
*  And there's part of me that thinks that there needs to be, um, more intelligence
*  that we realize in order to drive successfully, uh, the, that game theory
*  of human interaction seems to require some deep understanding of, um, of
*  human nature that.
*  Okay.
*  When a pedestrian crosses the street, there's some sense they, they look at a
*  car usually, and then they look away.
*  There's some sense in which they say, I believe that you're not going to murder
*  me.
*  You don't have the guts to murder me.
*  This is the little dance of pedestrian car interaction is saying, I'm going to
*  look away and I'm going to put my life in your hands because I think you're
*  human, you're not going to kill me.
*  And then the car in order to successfully operate in like Manhattan streets has to
*  say, no, no, no, I am going to kill you.
*  Like a little bit.
*  There's a little bit of this weird inkling of mutual murder and that's a
*  dance and then somehow successfully operate.
*  Do you think you were born of that?
*  Did you learn that social interaction?
*  Uh, I think it might have a lot of the same elements that you're talking about,
*  which is we're leveraging things we were born with and applying them in the
*  context that, uh, all right, I would, I would have said that that kind of
*  interaction is learned because you know, people in different cultures to have
*  different interactions like that.
*  If you cross the street in different cities and different parts of the world,
*  they have different ways of interacting.
*  I would say that's learned.
*  And I would say an intelligence system can learn that too, but that does not
*  lead and the intelligence system can understand humans.
*  It could understand that, you know, just like I can study an animal and learn
*  something about that animal, you know, I could study apes and learn something
*  about their culture and so on.
*  I don't have to be an ape to know that.
*  Um, I may not be completely, but I can understand something.
*  So until the machine can model that that's just part of the world.
*  This is part of the interactions.
*  The question we're trying to get at, will the intelligent machine have its
*  own personal agency that's beyond, you know, what we assigned to it or its, its
*  own personal, you know, goals or will evolve and create these things?
*  My confidence comes from understanding the mechanisms I'm talking about creating.
*  This is not hand wavy stuff.
*  It's down in the details.
*  We, I, I'm going to build it and I know what it's going to look like, and I
*  know what's it going to behave.
*  I know what the kind of things it could do and the kind of things it can't do.
*  Just like when I build a computer, I know it's not going to on its own, decide
*  to put another register inside of it.
*  It can't do that.
*  No way.
*  And no matter what your software does, it can't add a register to the computer.
*  Um, so in this way, when we build AI systems, we have to make choices about
*  the, the, the, the, the, how we embed them.
*  So I talked about this in the book.
*  I said, you know, it's a brain intelligence system is not just
*  the neocortex equivalent.
*  You have to have that, but it has to have some kind of embodiment, physical, virtual.
*  It has to have some sort of goals.
*  It has to have some sort of, uh, ideas about dangers, about things that shouldn't
*  do like, you know, like we, we, we build in safeguards into systems.
*  Uh, we have them in our bodies.
*  We have to put them into cars, right?
*  My car follows my directions until the day it sees I'm about to hit something
*  and it ignores my directions and puts the brakes on so we can build those things in.
*  So that's a very interesting problem.
*  Um, how to build those in.
*  I think my, my, my differing opinion about the risks of AI for most people is
*  that people assume that somehow those things will just appear automatically.
*  It'll evolve and intelligence itself begets that stuff or requires it, but it's not
*  intelligence of the neocortex equivalent doesn't require this.
*  The neocortex equivalent just says, I'm a learning system.
*  Tell me what you want me to learn.
*  And I'll tell you, ask me questions.
*  I'll tell you the answers.
*  But in that, again, it's again, like a map.
*  It doesn't, a map has no intent about things, but you can use it to solve problems.
*  Okay.
*  So the building engineering, the neocortex in itself is just creating
*  an intelligent prediction system, modeling system, sorry, modeling system.
*  Yeah.
*  Uh, you can use it to then make predictions and then, um, but you can also
*  put it inside a thing that's actually acting in this world.
*  You have to put it inside something.
*  It's again, think of the map analogy, right?
*  A map on its own doesn't do anything.
*  Right.
*  It's just inert.
*  It's just, it could learn, but it's just inert.
*  So we have to embed it somehow in something to do something.
*  So, so what's your intuition here?
*  You had, you had a conversation with Sam Harris recently that was a sort of, um,
*  you've had a bit of a disagreement and you're sticking on this point, you know,
*  Elon Musk, uh, Stuart Russell kind of have a worry, existential threats of AI.
*  What's your intuition?
*  Why, if we engineer increasingly intelligent neocortex type of system in the
*  computer, why that shouldn't be a thing that we use the word intuition.
*  And Sam Harris used the word intuition too.
*  And, and when he used that intuition, that word, I immediately stopped and
*  said, Oh, that's the cuck of the problem.
*  He's using intuition.
*  I'm not speaking about my intuition.
*  I'm speaking about something I understand, something I'm going to build,
*  something I am building, something I understand completely, or at least well
*  enough to know what it's all, I'm guessing I know what this thing's going to do.
*  And I think most people who are worried, they have trouble separating out.
*  I mean, they don't have, they don't have the knowledge or the understanding
*  about like, what is intelligence?
*  How's it manifest in the brain?
*  How's it separate from these other functions in the brain?
*  And so they imagine it's going to be human-like or animal-like.
*  It's going to have, it's going to have the same sort of drives and emotions we have,
*  but there's no reason for that.
*  That's just because there's, there's an unknown.
*  If you're, if the unknown is like, Oh my God, you know, I don't know what this is
*  going to do, we have to be careful.
*  It could be like us, but really smarter.
*  I'm saying, Oh no, it won't be like us.
*  It'll be really smart, but it won't be like us at all.
*  And, um, and, but I, I'm coming from that, not because I just guessing, I'm not
*  intuitive using intuition.
*  I'm basing it like, okay, I understand this thing works.
*  This is what it does.
*  It makes plenty to you.
*  Okay.
*  But, uh, to push back.
*  So I also disagree with the intuitions that Sam has, but, but so disagree with
*  the, what you just said, which, you know, what's a good, uh, analogy.
*  So if you look at the Twitter algorithm in the early days, just recommender
*  systems, you can understand how recommender systems work.
*  What you can't understand in the early days is when you apply that recommender
*  system at scale to thousands of millions of people, how that can change societies.
*  So the question is, yes, you're just saying this is how an engineer in your cortex
*  works, but the quote, like when you have a very useful, uh, tick tock type of
*  service that goes viral when your newer cortex goes viral and then millions of
*  people start using it, can that destroy the world?
*  No.
*  Uh, well, first of all, this is back.
*  One thing I want to say is that, um, AI is a dangerous technology.
*  I don't, I'm not denying that.
*  All technology is dangerous.
*  Well, and AI, but maybe particularly so.
*  Okay.
*  So, um, am I worried about it?
*  Yeah, I'm totally worried about it.
*  The thing where the narrow component we're talking about now is the
*  existential risk of AI, right?
*  So I want to make that distinction because I think AI can be applied poorly.
*  It can be applied in ways that, you know, people are going to understand the
*  consequences of it.
*  Um, these are all potentially very bad things, but they're not the AI system
*  creating this existential risk on its own.
*  And that's the only place that I disagree with other people.
*  Right.
*  So I think the existential risk thing is, um, humans are really damn good at
*  surviving, so to kill off the human race.
*  It'd be very, very difficult.
*  Yes, but you can even, I'll go further.
*  I don't think AI systems are ever going to try to, I don't think AI systems are
*  ever going to like say, I'm going to ignore you.
*  I'm going to do what I think is best.
*  Um, I don't think that's going to happen, at least not in the way I'm talking about
*  it.
*  So the Twitter recommendation algorithm is an interesting example.
*  Let's, let's use computers as an analogy again, right?
*  I build a computer.
*  It's a universal computing machine.
*  I can't predict what people are going to use it for.
*  They can build all kinds of things.
*  They can, they can even create computer viruses.
*  It's, you know, all kinds of stuff.
*  So there's some unknown about its utility and about where it's going to go.
*  But on the other hand, I pointed out that once I build a computer, it's not going to
*  fundamentally change how it computes.
*  It's like, I use the example of a register, which is a part, internal part of a
*  computer.
*  Um, you know, I say it can't just say, cause computers don't evolve.
*  They don't replicate.
*  They don't evolve.
*  They don't, you know, the physical manifestation of the computer itself is not
*  going to, there's certain things that can't do, right?
*  So we can break into things like things that are possible to happen.
*  We can't predict and things that are just impossible to happen.
*  Unless we go out of our way to make them happen, they're not going to happen
*  unless somebody makes them happen.
*  Yeah.
*  So there's, there's a bunch of things to say.
*  One is the physical aspect, which you're absolutely right.
*  We have to build a thing for it to operate in the physical world and you can
*  just stop building them.
*  Uh, you know, the moment they're not doing the thing you want them to do or
*  just change the design, change the design.
*  The question is, I mean, there's a, it's possible in the physical world, this is
*  probably longer term is you automate the building.
*  It makes, it makes a lot of sense to automate the building.
*  There's a lot of factories that are doing more and more and more automation to go
*  from raw resources to the final product.
*  It's possible to imagine that obviously much more efficient to keep, to create a
*  factory that's creating robots that do something, uh, you know, they do something
*  extremely useful for society.
*  It could be a personal assistance.
*  It could be, uh, it could be, it could be your toaster, but a toaster that's much
*  has deeper knowledge of your culinary preferences.
*  Yeah.
*  And that could, uh,
*  I think now you've hit on the right thing.
*  The real thing we need to be worried about Lex is self replication.
*  Right.
*  That is the thing that we're in the physical world or even the virtual world
*  self replication, because self replication is dangerous.
*  It's probably more likely to be killed by a virus, you know, or a human
*  hand engineered virus.
*  Anybody can create, you know, this, the technology is getting so almost anybody,
*  well, not anybody, but a lot of people could create a human engineered virus
*  that could wipe out humanity.
*  That is really dangerous.
*  No intelligence required.
*  Just self replication.
*  So, um, so we need to be careful about that.
*  So when I think about, you know, AI, I'm not thinking about robots, building robots.
*  Don't do that.
*  Don't build a, you know, just,
*  well, that's because you're interested in creating intelligence.
*  It seems like self replication is a good way to make a lot of money.
*  Well, fine.
*  But so is, you know, maybe editing viruses is a good way to, I don't know.
*  The point is if as a society, when we want to look at existential risks, the
*  existential risks we face that, that we can control almost all evolve around
*  self replication.
*  Yes.
*  The question is, I don't see a good way to make a lot of money by engineering
*  viruses and deploying them on the world.
*  There could be, there could be applications that are useful.
*  But let's separate out, let's separate out.
*  I mean, you don't need to, you only need some, you know, terrorists who want to do
*  it because it doesn't take a lot of money to make viruses.
*  Um, let's just separate out what's risky and what's not risky.
*  I'm arguing that the intelligence side of this equation is not risky.
*  It's not risky at all.
*  It's the self replication side of the equation that's risky.
*  And I'm not dismissing that I'm scared as hell.
*  It's like the paperclip maximizer thing.
*  Yeah.
*  The, those are often like talked about in the same conversation.
*  Um, I think you're right.
*  Like creating ultra intelligence, super intelligent systems is not necessarily
*  coupled with a self replicating arbitrarily self replicating systems.
*  Yeah.
*  And you don't get evolution unless you're self replicating.
*  Yeah.
*  And so I think that's just this argument that people have trouble separating those
*  two out, they just think, oh yeah, intelligence looks like us and look at
*  the damage we've done to this planet.
*  Like how we've, you know, destroyed all these other species.
*  Yeah.
*  Well, we replicate which 8 billion of us are 7 billion of us now.
*  So, um, I think the idea is that the, the more intelligent we're able to build
*  systems, the more tempting it becomes from a capitalist perspective of creating
*  products, the more tempting it becomes to create self reproducing systems.
*  All right.
*  So let's say that's true.
*  So does that mean we don't build intelligent systems?
*  No, that means we regulate, we, we understand the risks.
*  Uh, we regulate them.
*  Yeah.
*  Uh, you know, look, there's a lot of things we could do a society which have
*  some sort of financial benefit to someone, which could do a lot of harm.
*  And we have to learn how to regulate those things.
*  We have to learn how to deal with those things.
*  I will argue this.
*  I would say the opposite.
*  I would say having intelligent machines at our disposal will actually help us in
*  the end more because it helps us understand these risks better.
*  It helps us mitigate these risks better.
*  There might be ways of saying, oh, well, how do we solve climate change problems?
*  You know, how do we do this or how do we do that?
*  Um, that just like computers are dangerous in the hands of the wrong people, but
*  they've been so great for so many other things, we live with those dangers.
*  And I think we have to do the same with intelligent machines.
*  We just, but we have to be constantly vigilant about this idea of a bad actors
*  doing bad things with them and B, um, don't ever, ever create a self replicating system.
*  Um, uh, and by the way, I don't even know if you could create a self replicating
*  system that uses a factory that's really dangerous, you know, nature's way of
*  self-replicating is so amazing.
*  Um, you know, it doesn't require anything.
*  It doesn't mean no, the thing and resources and it goes right.
*  Yeah.
*  Um, if I said to you, you know what we have to build, uh, our goal is to build a
*  factory that can make that builds new factories and it has to end to end supply
*  chain, it has to, it has to mind the resources, get the energy.
*  I mean, that's really hard.
*  It's, you know, no one's doing that in the next, you know, a hundred years.
*  I've been extremely impressed by the efforts of Elon Musk and Tesla to try to do
*  exactly that, not, not from raw resource.
*  Well, he actually, I think states the goal is to go from raw resource to the,
*  uh, the final car in one factory.
*  Yeah.
*  And that's, that's the main goal.
*  Of course it's not currently possible, but they're taking huge leaps.
*  Well, he's not the only one to do that.
*  It is.
*  This has been a goal for many industries for a long, long time.
*  Um, it's difficult to do.
*  Well, a lot of people, what they do is instead they have like a million suppliers
*  and then they, like there's everybody's manic, co-locate them and they, and they
*  tie the systems together.
*  It's a fundamentally distributed system.
*  I think that's, that also is not getting at the issue I was just talking about,
*  um, which is self replication.
*  It's, um, I mean, self replication means there's no entity involved other than
*  the entity that's replicating.
*  Um, right.
*  And so if there are humans in this, in the loop, that's not really self replicating.
*  Right.
*  It's unless somehow we're duped into doing it.
*  But it's also, I don't necessarily agree with you because you've kind of mentioned
*  that AI will not say no to us.
*  I just think they will.
*  Yeah.
*  So like, uh, I think it's a useful feature to build in.
*  I'm just trying to like, uh, put myself in the mind of engineers to sometimes say no.
*  You know, if you, you know, I gave an example earlier, right?
*  I gave an example of my car.
*  Yeah.
*  Right.
*  My car turns the wheel and applies the accelerator and the brake, as I say, until
*  it decides there's something dangerous.
*  Yes.
*  And then it doesn't do that.
*  Now that was something it didn't decide to do.
*  It's something we programmed into the car.
*  Uh, and so good.
*  It was a good idea.
*  Right.
*  The question again, isn't like, well, if we create an intelligence system, will it
*  ever ignore our commands?
*  Of course it will.
*  Sometimes is it going to do it because it came up, came up with its own goals
*  that serve its purposes and it doesn't care about our purposes.
*  No, I don't think that's going to happen.
*  Okay.
*  So let me ask you about these, uh, super intelligent cortical systems that we
*  engineer and us humans.
*  Do you think, uh, with these entities operating out there in the world, what is
*  the future most promising future look like?
*  Is it us merging with them or is it us?
*  Like, how do we keep us humans around when you have increasingly intelligent
*  beings, is it, uh, one of the dreams is to upload our minds in the digital space.
*  So can we just give our minds to these systems so they can operate on them?
*  Is there some kind of more interesting merger or is there more, more
*  and the third part of my book, I talked about all these scenarios and let me
*  just walk through them.
*  Sure.
*  Um, the uploading the mind one.
*  Yes.
*  Extremely really difficult to do.
*  Like, like we have no idea how to do this even remotely right now.
*  Um, so it would be a very long way away, but I make the argument you
*  wouldn't like the result, um, and you wouldn't be pleased with results.
*  It's really not what you think it's going to be.
*  Um, imagine I could upload your brain into a, into a computer right now.
*  And now the computer sitting there going, Hey, I'm over here.
*  Great.
*  Get rid of that old bio person.
*  I don't need them.
*  You're still sitting here.
*  Yeah.
*  What are you going to do?
*  You just know, no, that's not me.
*  I'm here.
*  Right.
*  Yeah.
*  Are you going to feel satisfied then?
*  Then, you know, you, but people imagine, look, I'm on my deathbed and I'm about
*  to, you know, expire and I push the button and I'm uploaded, but think
*  about it a little differently.
*  And, and so I don't think it's going to be a thing because people, by the time
*  we're able to do this, if ever, cause you have to replicate the entire body, not
*  just the brain, it's, it's really, it's, I walked through the issue.
*  It's really substantial.
*  Um, do you have a sense of what makes us us?
*  Is there, if, is there a shortcut to it can only save a certain part
*  that makes us truly ours?
*  No, but I think that machine would feel like it's you too.
*  Right.
*  Right.
*  You have two people just like I have a child, I have a child, right?
*  I have two daughters.
*  They're independent people.
*  I created them.
*  Well, partly.
*  Yeah.
*  And, um, uh, I don't, just because they're somewhat like me, I don't feel on them
*  and they don't feel like on me.
*  So if you split up, you have two people, so we can come back to what, what makes
*  what consciousness do you want?
*  We can talk about that, but we don't have a remote consciousness.
*  I'm not sitting there going, oh, I'm conscious of that.
*  You know, I mean that system over there.
*  So there's this thing, let's stay on our topic.
*  Okay.
*  Sure.
*  One was uploading a brain.
*  Yep.
*  Ain't going to happen in a hundred years, maybe a thousand, but I don't
*  think people are going to want to do it.
*  The merging your mind with, uh, you know, the neural link thing, right?
*  Like again, really, really difficult.
*  It's, it's one thing to make progress, to control a prosthetic arm.
*  It's another to have like a billion or several billions, you know, things and
*  understanding what those signals mean.
*  Like it's the one thing to like, okay, I can learn to think some patterns
*  to make something happen.
*  It's quite another thing to have a system, a computer, which actually knows
*  exactly which cells it's talking to and how it's talking to them and
*  interacting in a way like that.
*  Very, very difficult.
*  We're not getting anywhere closer to that.
*  Um, interesting.
*  Can I, can I, uh, can I ask a question here?
*  What, so for me, what makes that merger very difficult practically in the next
*  10, 20, 50 years is like literally the biology side of it, which is like, it's
*  just hard to do that kind of surgery in a safe way, but your intuition is even
*  the machine learning part of it where the machine has to learn what the heck
*  it's talking to that's even hard.
*  I think it's even harder.
*  And it's not, it's, it's easy to do when you're talking about hundreds of signals.
*  It's, it's a totally different thing to say talking about billions of signals.
*  So you don't think it's the raw, it's a machine learning problem.
*  You don't think it could be learned?
*  Well, I'm just saying, no, I think you'd have to have detailed knowledge.
*  You'd have to know exactly what the types of neurons you're connecting to.
*  I mean, in the brain, there's these, there are neurons that do all
*  different types of things.
*  It's not like a neural network.
*  It's a very complex organism system up here.
*  We talked about the grid cells are in place cells, you know, you have to know
*  what kind of cells you're talking to and what they're doing and how their timing
*  works and all, all this stuff, which you can't today is no way of doing that.
*  Right.
*  But I think it's, I think it's a, I think the problem you're right that the
*  biological aspect of it, like who wants to have a surgery and have this stuff
*  you know, inserted in your brain.
*  That's a problem, but this is when we solve that problem.
*  I think the, the information coding aspect is much worse.
*  I think that's much worse.
*  It's not like what they're doing today.
*  Today it's simple machine learning stuff because you're doing simple things.
*  But if you want to merge your brain, like I'm thinking on the internet, I'm
*  merged my brain with the machine and we're both doing that's a totally different
*  issue.
*  That's interesting.
*  I tend to think if the, okay, if you have a super clean signal from a bunch of
*  neurons at the start, you don't know what those neurons are.
*  I think that's much easier than the getting of the clean signal.
*  I think if you think about today's machine learning, that's what you would
*  conclude.
*  Right.
*  I'm thinking about what's going on in the brain and I don't reach that conclusion.
*  So we'll have to see.
*  Sure.
*  But I don't think even, even then I think there's kind of a sad future.
*  Like, you know, do I, do I have to like plug my brain into a computer?
*  I'm still a biological organism.
*  I assume I'm still going to die.
*  So what, what have I achieved?
*  Right.
*  You know, what have I achieved in doing some sort of,
*  Oh, I disagree that we don't know what those are, but it seems like there could
*  be a lot of different applications.
*  It's like virtual reality is to expand your brain's capability to, to, to like,
*  to read Wikipedia.
*  Yeah, but, but fine.
*  But, but you're still a biological organ.
*  Yes.
*  Yes.
*  You know, you're still, you're still mortal.
*  You still, all right.
*  So, so what are you accomplishing?
*  You're making your life in this short period of time better, right?
*  Just like having the internet made our life better.
*  Yeah.
*  Yeah.
*  Okay.
*  So I think that's of, of, if I think about all the possible gains we can have here,
*  that's a marginal one.
*  It's an individual.
*  Hey, I'm better.
*  You know, I'm smarter.
*  Um, but you know, fine.
*  I'm not against it.
*  I just don't think it's earth changing.
*  I, but, but so this is the true of the internet.
*  When each of us individuals are smarter, we get a chance to then share our smartness.
*  We get smarter and smarter together.
*  It's like, as a collective, this is kind of like this ant colony of,
*  why don't I just create an intelligent machine that doesn't have any of this
*  biological nonsense that has all the same?
*  It's, it's everything except don't burden it with my brain.
*  Yeah.
*  Right.
*  It has a brain.
*  It is smart.
*  It's like my child, but it's much, much smarter than me.
*  So I have a choice between doing some implant, doing some hybrid, weird, you
*  know, biological thing that's bleeding and all these problems and limited by my
*  brain or creating a system, which is super smart that I can talk to, um, that
*  helps me understand the world.
*  They can read the internet, you know, read Wikipedia and talk to me.
*  I guess my, uh, the open questions there are what does the manifestation of
*  super intelligence look like?
*  So like, what are we going to, you talked about, why do I want to merge with AI?
*  Like what, what's the actual marginal benefit here?
*  If I, if we have a super intelligent system, how will it make our life better?
*  So let's, let's, that's a great question, but let's break it
*  onto little pieces.
*  All right.
*  On the one hand, it can make our life better in lots of simple ways.
*  You mentioned like a care robot or something that helps me do things.
*  It cooks.
*  I don't know what it does, right?
*  Little things like that.
*  We got super better, smarter cars.
*  We can have, you know, better agents, AIDS, helping us in our work
*  environment and things like that.
*  To me, that's like the easy stuff.
*  The simple stuff in the beginning.
*  Um, um, and so in the same way that computers made our lives better in
*  ways, many, many ways, I will have those kinds of things.
*  To me, the really exciting thing about AI is sort of its transcendent, transcendent
*  quality in terms of humanity.
*  We're still biological organisms.
*  We're still stuck here on earth.
*  It's going to be hard for us to live anywhere else.
*  Uh, I don't think you and I are going to want to live on Mars anytime soon.
*  Um, and, um, and we're flawed, you know, we may end up destroying ourselves.
*  It's totally possible.
*  Uh, we, if not completely, we could destroy our civilizations.
*  You know, it's this face the fact that we have issues here, but we can create
*  intelligent machines that can help us in various ways.
*  For example, one example I gave, and that sounds a little sci-fi, but I believe this.
*  If we really want to live on Mars, we'd have to have intelligent systems that go
*  there and build the habitat for us, not humans.
*  Humans are never going to do this.
*  It's just too hard.
*  Um, but could we have a thousand or 10,000, you know, engineer workers up there
*  doing this stuff, building things, terraforming Mars?
*  Sure.
*  Maybe we can move to Mars.
*  But then if we want to, if we want to go around the universe, should I send my
*  children around the universe or should I send some intelligent machine, which is
*  like a child that represents me and understands our needs here on earth
*  that could travel through space.
*  Um, so it's sort of, in some sense, intelligence allows us to transcend our,
*  the limitations of our biology, uh, with, and, and, and don't think of it as a
*  negative thing, it's in some sense, my children transcend the, my biology too,
*  cause they, they live beyond me.
*  Yeah.
*  Um, and we impart, they represent me and they also have their own knowledge and
*  they, I can impart knowledge to them.
*  So intelligent machines would be like that too, but not limited like us.
*  But the question is, um, th there's so many ways that transcendence can happen.
*  And the merger with AI and humans is one of those ways.
*  So you said intelligent, basically beings or systems propagating throughout
*  the universe, representing us humans.
*  They represent us humans in the sense they represent our knowledge and our
*  history, not us individually.
*  Right.
*  Right.
*  But I mean, the question is, is it just a database with, uh, with a really
*  damn good, uh, model of the world?
*  No, no, they're conscious, conscious, just like us.
*  Okay.
*  But just different.
*  They're different.
*  Uh, just like my children are different.
*  They're like me, but they're different.
*  Um, these are more different.
*  I guess maybe I've already, I kind of, I take a very broad view of our life here
*  on, on earth, I say, you know, why are we living here?
*  Are we just living because we live?
*  Is it, are we surviving because we can survive?
*  Are we fighting just because we want to just keep going?
*  What's the point of it?
*  Right.
*  So to me, the point, if I ask myself, what's the point of life is what's
*  transcends that ephemeral sort of biological experience is to me, this is
*  my answer is the acquisition of knowledge to understand more about the
*  universe, uh, and to explore.
*  And that's partly to learn more.
*  Right.
*  Um, I don't view it as a terrible thing.
*  If the ultimate outcome of humanity is we create systems that are intelligent,
*  that are our offspring, but they're not like us at all.
*  And we stay, we stay here and live on earth as long as we can, which
*  won't be forever, but as long as we can.
*  And, but that would be a great thing to do.
*  It's not a, it's not like a negative thing.
*  Well, would you be okay then if, uh, the human species vanishes, but our
*  knowledge is preserved and keeps being expanded by intelligent systems?
*  I want our knowledge to be preserved and expanded.
*  Yeah.
*  Am I okay with humans dying?
*  No, I don't want that to happen.
*  But if it, if it does happen, what if we were sitting here and this is
*  the real last few people on earth and were saying, Lex, we blew it.
*  It's all over.
*  Right.
*  Yeah.
*  Wouldn't I feel better if I knew that our knowledge was preserved and that we
*  had agents that knew about that, that were trans, you know, that left earth?
*  I would want that.
*  It's better than not having that.
*  You know, I make the analogy of like, you know, the dinosaurs, the poor
*  dinosaurs, they live for, you know, tens of millions of years, they've raised
*  their kids, they, you know, they, they fought to survive, they were hungry.
*  They, they, they did everything we do.
*  And then they're all gone.
*  Yeah.
*  Like, you know, and, and if we didn't discover their bones, nobody would ever
*  know that they ever existed.
*  Right.
*  Do we want to be like that?
*  I don't want to be like that.
*  There's a sad aspect to it.
*  And it's kind of, it's jarring to think about that.
*  It's possible that a human like intelligence civilization has
*  previously existed on earth.
*  The reason I say this is like, it is jarring to think that we would not, if
*  they weren't extinct, we wouldn't be able to find evidence of them after a
*  sufficient amount of time, after a sufficient amount of time.
*  Of course, there's like, like basically humans, like if we destroy ourselves now,
*  human civilization destroy ourselves now after a sufficient amount of time, we
*  would not be, we'd find evidence of the dinosaurs would not find evidence of
*  us humans.
*  Yeah.
*  That's kind of an odd thing to think about.
*  Although I'm not sure if we have enough knowledge about species going back for
*  billions of years, but we could, we could, we might be able to eliminate that
*  possibility, but it's an interesting question.
*  Of course, this is a similar question to, you know, there were lots of
*  intelligent species throughout our galaxy that have all disappeared.
*  That's super sad that they're exactly that there may have been much more
*  intelligent alien civilizations in our galaxy that are no longer there.
*  Yeah.
*  You actually talked about this, that humans might destroy ourselves and how
*  we might preserve our knowledge and advertise that knowledge to other
*  advertisers.
*  A funny word to use.
*  There's no PR from a PR perspective.
*  There's no financial gain in this.
*  You know, like make it like from a tourism perspective, make it interesting.
*  Can you describe how, how you think about this problem?
*  I broke it down into two parts, actually three parts.
*  One is, um, you know, there's a lot of things we know that what if, what if we
*  were, what if we ended up, what if our civilization collapsed?
*  Yeah, I'm not talking tomorrow.
*  Yeah, we could be a thousand years from now.
*  Like, you know, we don't really know, but, but historically it would be
*  likely at some point.
*  Time flies when you're having fun.
*  Yeah.
*  You know, could we, and then then intelligent life evolved again on this
*  planet, wouldn't they want to know a lot about us and what we knew when they
*  wouldn't be able to ask us questions?
*  So one very simple thing I said, how would we archive what we know?
*  That was a very simple idea.
*  I said, you know what?
*  It's not, wouldn't be that hard to put a few satellites, you know, going around
*  this in the sun and we upload Wikipedia every day and that kind of thing.
*  Uh, so, you know, if we can end up killing ourselves, well, it's up there
*  and the next intelligence piece will find it and learn something that would be,
*  they would like that.
*  They would appreciate that.
*  Um, uh, so that's one thing.
*  The next thing I said, well, what if, you know, how outside of our solar system,
*  we have the SETI program, we're looking for these intelligence signals from
*  everybody, and if you do a little bit of math, which I did in the book, uh, and
*  you say, well, what if intelligent species only live for 10,000 years before,
*  you know, technologically intelligent species, like ones are really able to do
*  the, we're just starting to be able to do, um, well, the chances are we wouldn't
*  be able to see any of them because they would have all been disappeared by now.
*  Um, they would, they've lived for 10,000 years and now they're gone.
*  And so we're not going to find these signals being sent from these people
*  because, um, but I said, what kind of signal could you create that would last
*  a million years or billion years that someone would say, damn it, someone
*  smart lived there.
*  We know that that would be a life-changing event for us to figure that out.
*  Well, what we're looking for today in the SETI program, isn't that we're looking
*  for very coded signals in some sense.
*  Um, and so I asked myself, what would be a different type of signal one could
*  create?
*  Um, I've always thought about this throughout my life and in the book, I
*  gave one, one possible suggestion, which was, um, uh, we now detect planets
*  going around other, other suns, other stars, excuse me.
*  And we do that by seeing this, the, the slight dimming of the light as the
*  planets move in front of them.
*  That's how, uh, we detect, uh, planets elsewhere in our galaxy.
*  Um, what if we created something like that, that just rotated around our, our,
*  our, around the sun and it blocked out a little bit of light in a particular
*  pattern that someone said, Hey, that's not a planet.
*  That is a sign that someone was once there.
*  You can take, what if it's beating up pie, you know, three point, whatever.
*  Um, so I did a distance, broadly broadcast takes no continue activation on our part.
*  This is the key, right?
*  You, no one has to be senior running a computer and supplying it with power.
*  It just goes on.
*  So we go, it's continuous.
*  And, and I argued that part of the SETI program should be looking for signals
*  like that and to look for signals like that, you ought to figure out what the,
*  how would we create a signal?
*  Like what would we create?
*  That would be like that.
*  That would persist for millions of years.
*  That would be broadcast broadly.
*  You could see from a distance.
*  There was on a clinical came from an, uh, uh, uh, an intelligent species.
*  And so I gave that one example, um, cause they don't know what I know of actually.
*  And then, and then finally, right.
*  If, if our ultimately our solar system will die at some point in time, you know,
*  how do we go beyond that?
*  And I think it's possible if it all possible, we'll have to create intelligent
*  machines that travel throughout the, throughout the solar system or the
*  route to the galaxy.
*  And I don't think that's going to be humans.
*  I don't think it's going to be biological organisms.
*  So these are just things to think about, you know, like what's the, you know, I
*  kind of, I don't want to be like the dinosaur.
*  I don't want to just live in, okay, that was it.
*  We're done.
*  You know, well, there is a kind of presumption that we're going to live forever,
*  which, uh, I think it is a bit sad to imagine that the message we send as, as
*  you talk about is that we were once here instead of we are here.
*  Well, it could be, we are still here.
*  Uh, but it's more of a, it's more of an insurance policy in case we're not here.
*  You know, well, I don't know, but there's something I think about.
*  We, we as humans don't often think about this, but it's like, like whenever I, um,
*  record a video, I've done this a couple of times in my life.
*  I've recorded a video for my future self, just for personal, just for fun.
*  And it's always just fascinating to think about that preserving yourself for
*  future civilizations for me, I was preserving myself for a future me, but
*  that's a little, that's a little fun example of archival.
*  Well, these podcasts are, are, are preserving you and I in a way for future.
*  Um, hopefully well after we're gone, but you don't often, we're
*  sitting here talking about this.
*  You are not thinking about the fact that you and I are going to die and there'll
*  be like 10 years after somebody watching this and we're still alive.
*  You know, in some sense I do.
*  I'm here because I want to talk about ideas and these ideas transcend me and
*  they transcend this time and in our planet.
*  Um, we're talking here about ideas that could be around a thousand years from
*  now or a million years from now.
*  I, when I wrote my book, I had an audience in mind and one of the
*  clearest audiences was aliens.
*  No, were people reading this a hundred years from now?
*  I said myself, how do I make this book relevant to someone reading
*  this a hundred years from now?
*  What would they want to know that we were thinking back then?
*  What would make it like that was an interesting, it's still an interesting book.
*  I'm not sure I can achieve that, but that was how I thought about it because these
*  ideas, like especially in the third part of the book, the ones we were just talking
*  about, you know, these crazy, it sounds like crazy ideas about, you know, storing
*  our knowledge and, and, you know, merging our brains with computers and in sending
*  you know, our machines down to space is not going to happen in my lifetime.
*  Um, and they may not even happen in the next hundred years.
*  It may not happen for a thousand years.
*  Who knows?
*  Uh, but we have the unique opportunity right now.
*  We, you, me, and other people like this, um, to sort of at least propose the
*  agenda, um, that might impact the future like that.
*  It's a fascinating way to think, uh, both like writing or creating, try to
*  make, try to create ideas, try to create things that, uh, hold up in time.
*  Yeah.
*  You know, understanding how the brain works, we're going to figure that out
*  once that's it, it's going to be figured out once and after that, that's the
*  answer and people will, people will study that thousands of years now.
*  We still, we still, you know, venerate Newton and Einstein and, um, and you
*  know, because, cause ideas are exciting even well into the future.
*  Well, the interesting thing is like big ideas, even if they're wrong, are still
*  useful, like, yeah, especially if they're not completely the one like you're
*  right, right, right.
*  Newton's laws are not wrong.
*  They're just Einstein's they're better.
*  So, um, so yeah, I mean, but we're talking with Newton and Einstein.
*  We're talking about physics.
*  I wonder if we'll ever achieve that kind of clarity, but understanding, um, like
*  complex systems and the, this particular manifestation of complex systems, which
*  is the human brain.
*  Oh, I, I'm, I'm totally optimistic.
*  We can do that.
*  I mean, we're making progress at it.
*  I don't see any reasons why we can't completely, I mean, completely understand
*  in the sense, um, you know, we don't really completely understand what all
*  the molecules in this water bottle are doing, but you know, we have laws that
*  sort of capture it pretty good.
*  Um, and, uh, so we'll have that kind of understanding.
*  I mean, it's not like you're going to have to know what every neuron in your
*  brain is doing, um, but enough to, uh, first of all, to build it.
*  And second of all, to do, you know, do what physics does, which is like have
*  concrete experiments where we can validate.
*  We're, we're, we're, this is happening right now.
*  Like, it's not, this is not some future thing.
*  Um, you know, I'm very optimistic about, I'm, I know about art, our work and what
*  we're doing, we'll have to prove it to people.
*  Um, but, um, I consider myself a rational person and, um, you know, until fairly
*  recently, I wouldn't have said that, but right now I'm where I'm sitting right now.
*  I'm saying, you know, we, we can, this is going to happen.
*  There's no big obstacles to it.
*  Um, we finally have a framework for understanding what's going on in the
*  cortex and, um, and that's liberating.
*  It's, it's like, oh, it's happening.
*  So I can't see why we wouldn't be able to understand it.
*  I just can't.
*  Okay.
*  Oh, so I mean, on that topic, let me ask you to play devil's advocate.
*  Is it possible for you to imagine look, look a hundred years from now and looking
*  at your book, uh, in which ways might your ideas be wrong?
*  Oh, I worry about this all the time.
*  Um,
*  yeah, I'm still useful.
*  Yeah.
*  Yeah.
*  I think there's, you know, um, well, I can, I can best relate it to like things
*  I'm worried about right now.
*  So we talked about this voting idea, right?
*  It's happening.
*  There's no question it's happening, but it could be far more, um, uh, there's,
*  there's enough things I don't know about it that it might be working in ways
*  differently than I'm thinking about the kind of what's voting, who's voting, you
*  know, where are representations?
*  I talked about, like, you have a thousand models of the coffee cup,
*  like that.
*  That could turn out to be wrong, um, because it may be, maybe there are a
*  thousand models that are sub models, but not really a single model of the coffee
*  cup.
*  Um, I mean, there's things, these are all sort of on the edges, things that I
*  present as like, oh, it's so simple and clean.
*  Well, that's not that it's always going to be more complex.
*  And, um, and there's parts of the theory, which I don't understand the complexity.
*  Well, so I think, I think there's a lot of things that are going to be more
*  I think the idea is brain is a distributed modeling system is not
*  controversial at all.
*  Right.
*  That's not, that's well understood by many people.
*  The question then is, are each quarter column an independent modeling system?
*  Right.
*  Um, I could be wrong about that.
*  Um, I don't think so, but I worry about it.
*  My intuition, not even thinking why you could be wrong is the same intuition I
*  have about any sort of physicist, uh, like strength theory that we as humans
*  desire for a clean explanation and a hundred years from now, uh, intelligence
*  systems might look back at us and laugh at how we try to get rid of the whole
*  mess by having simple explanation when the reality is it's, it's way messier.
*  And in fact, it's impossible to understand.
*  You can only build it.
*  It's like this idea of complex systems and cellular automata is you can only
*  launch the thing you cannot understand it.
*  Yeah.
*  I think that, you know, the history of science suggests that's not likely to
*  occur.
*  Um, the history of science suggests that as a theorist and we're theorists, you
*  look for simple explanations, right?
*  Fully knowing that whatever simple explanation you're going to come up with
*  is not going to be completely correct.
*  I mean, it can't be, I mean, it's just, it's just more complexity, but that's
*  the role of theorists play.
*  They, they sort of, they give you a framework on which you now can talk about
*  the problem and figure out, okay, now we can start digging more details.
*  The best frameworks stick around while the details change, you know, again,
*  you know, the classic example is Newton and Einstein, right?
*  You know, um, Newton's theories are still used.
*  They're still valuable.
*  They're still practical.
*  They're not like wrong.
*  It just, they've been refined.
*  Yeah, but that's in physics.
*  It's not obvious by the way, it's not obvious for physics either that the
*  universe should be such that's amenable to these simple, but it's so far it
*  appears to be as far as we can tell.
*  Um, yeah, I mean, but as far as we could tell, and, but it's also an open
*  question whether the brain is amenable to such clean theories.
*  That's the brain, but intelligence.
*  Well, I, I, I don't know.
*  I would take intelligence out of it.
*  Just say, you know, um, well, okay.
*  Um, um, the evidence we have suggested that the human brain is, is, is a, at
*  the one time extremely messy and complex, but there's some parts that are very
*  regular and structured.
*  That's why we started the neocortex.
*  It's extremely regular in its structure.
*  Yeah.
*  And unbelievably so.
*  And then I mentioned earlier, the other thing is it's, it's universal abilities.
*  It is so flexible to learn so many things.
*  We don't, we haven't figured out what it can't learn yet.
*  We don't know, but we haven't figured out yet.
*  But it learns things that never was evolved to learn.
*  So those give us hope.
*  Um, that's why I went into this field because I said, you know, this regular
*  structure, it's doing this amazing number of things.
*  There's gotta be some underlying principles that are, that are common.
*  And other, other scientists have come up with the same conclusions.
*  Um, and so it's promising and, um, and that's, and whether the theories play
*  out exactly this way or not, that is the role that theorists play.
*  And so far it's worked out well, even though, you know, maybe, you know, we
*  don't understand all the laws of physics, but so far it's been pretty damn useful.
*  The ones we have are, our theories are pretty bit useful.
*  You mentioned that, uh, we should not necessarily be at least to the degree
*  that we are worried about the existential risks of artificial intelligence relative
*  to, uh, human risks from human nature being existential risk.
*  What aspect of human nature worries you the most in terms of the
*  survival of the human species?
*  I hope they, I'm disappointed in humanity, humans.
*  I mean, all of us, I'm, I'm one, so I'm disappointed in myself too.
*  Um, it's kind of a sad state.
*  There's, there's two things that disappoint me.
*  One is how it's difficult for us to separate our rational component of
*  ourselves from our evolutionary heritage, which is, you know, not always pretty.
*  You know, um, uh, rape is a, is an evolutionary good strategy for reproduction.
*  Murder can be at times too, you know, making other people miserable at times
*  is a good strategy for reproduction.
*  It's just, and it's just, and, and so now that we know that, and yet we have this
*  sort of, you know, we, you and I can have this very rational discussion talking
*  about, you know, intelligence and brains and life and so on.
*  So it seems like it's so hard.
*  It's just a big transition to forget humans, all humans to, to, to make the
*  transition from be like, let's pay no attention to all that ugly stuff over here.
*  Let's just focus on the interesting.
*  What's unique about humanity is our knowledge and our intellects.
*  But the fact that we're striving is in itself amazing, right?
*  The fact that we're able to overcome that part.
*  And it seems like we are more and more becoming successful in overcoming that.
*  That is the optimistic view.
*  And I agree with you, but I worry about it.
*  I'm not saying I'm worrying about it.
*  I think that was your question.
*  I still worry about it.
*  Yes.
*  Um, you know, we could be end tomorrow because some terrorists could get
*  nuclear bombs and, you know, blow us all up.
*  Who knows?
*  Right.
*  Um, the other thing I think I'm disappointed is, uh, and it's just, I
*  understand it is, I guess you can't really be disappointed.
*  It's just a fact is that we're so prone to false beliefs.
*  We, you know, we have a model in our head.
*  The things we can interact with directly, physical objects, people, that
*  model is pretty good and we can test it all the time, right?
*  I touch something, I look at it, I talk to you, see it, my model is correct.
*  But so much of what we know is stuff I can't directly interact with.
*  I can't, I don't even know because someone told me about it.
*  And so, so we're prone, inherently prone to having false beliefs because
*  if I'm told something, how am I going to know it's the right or wrong?
*  Right.
*  And so then we have the scientific process, which says we are inherently flawed.
*  So the only way we can get closer to the truth is by looking for, um, country
*  evidence, um, like this, uh, conspiracy theory, this, this theory that
*  scientists keep telling me about that.
*  The earth is round.
*  Uh, as far as I can tell, when I look out, it looks pretty flat.
*  Yeah.
*  So yeah, there's, there's a tension, but it's also, um, um, I tend to believe that
*  we haven't figured out most of this thing, right?
*  Most, most of nature around us is a mystery.
*  And so it, um, but that doesn't, does that worry you?
*  I mean, it's like, oh, that's, that's like a pleasure more to figure out.
*  Right.
*  Yeah.
*  That's exciting.
*  Yeah.
*  I'm saying like, there's going to be a lot of quote unquote wrong ideas.
*  I mean, I've been thinking a lot about engineering systems like social networks
*  and so on, and I've been worried about censorship and thinking through all that
*  kind of stuff, because there's a lot of wrong ideas, there's a lot of dangerous
*  ideas, but then I also read a history, uh, read history and see when you sensor
*  ideas that are wrong.
*  Now this could be a small scale censorship, like a young grad student who comes up,
*  who like raises their hand and says some crazy idea.
*  And it's a form of censorship could be, I shouldn't use the word censorship, but
*  I think you mean, uh, like de-incentivize them from no, no, no, no, no.
*  This is the way it's been.
*  Yeah.
*  Yeah.
*  You're a foolish kid.
*  Don't think you're foolish.
*  Uh, so in some sense, uh, those wrong ideas most of the time end up being
*  wrong, but sometimes end up being foolish.
*  So I don't like the word censorship.
*  Um, at the very end of the book, I ended up with a sort of a, um, a plea or a
*  recommended force of action.
*  And the best way I could, I know how to deal with this issue that you bring up
*  is if everybody understood as part of your upbringing in life, something about
*  your brain works, that it builds a model of the world, uh, how it works, you know,
*  how basically builds that model of the world and that the model is not the real
*  world, it's just a model and it's never going to reflect the entire world and it
*  can be wrong and it's easy to be wrong.
*  And here's all the ways you can get the wrong model in your head.
*  Right.
*  It's not prescribed what's right or wrong.
*  Just understand that process.
*  If we all understood the process, then I get together and you say, I disagree
*  with you, Jeff, and I said, Lex, I disagree with you that.
*  At least we understand that we're both trying to model something.
*  We both have different information, which leads to our different models.
*  And therefore I shouldn't hold it against you and you shouldn't hold it against me.
*  And we can at least agree that, well, what can we look for in that's common
*  ground to test our, our beliefs as opposed to so much, uh, as we raise our kids on
*  dogma, which is this is a fact and this is a fact and these people are bad.
*  And, and, and, you know, where every, if everyone knew just to, to be skeptical
*  of every belief and why and how their brains do that, I think we might have a
*  better world.
*  Do you think the human mind is able to comprehend reality?
*  So you talk about this, creating models that are better and better.
*  How close do you think we get to, uh, to reality?
*  There's sort of the wildest ideas.
*  It's like Donald Hoffman saying we're very far away from reality.
*  Uh, do you think we're getting close to reality?
*  Well, it depends on what you define reality.
*  Uh, we are getting, we have a model of the world that's very useful.
*  Right.
*  Or for basic survival and our pleasure, right.
*  Right.
*  Um, so that's useful.
*  Um, I mean, it's really useful.
*  Oh, we can build planes.
*  We can build computers.
*  We can do these things.
*  Right.
*  Uh, I don't think, I don't know the answer to that question.
*  Um, I think that's part of the question we're trying to figure out.
*  Right.
*  Like, you know, obviously if you end up with a theory of everything that really
*  is a theory of everything and all of a sudden everything comes into play and
*  there's no room for something else, then you might feel like we have a good model
*  of the world.
*  Yeah.
*  But if we have a theory of everything and somehow, first of all, you'll never be
*  able to really conclusively say it's a theory of everything, but say somehow
*  we are very damn sure it's a theory of everything.
*  We understand what happened at the big bang and how just the entirety of the
*  physical process.
*  I'm still not sure that gives us an understanding of, uh, the next many
*  layers of the hierarchy of abstractions that form.
*  Well, also what if string theory turns out to be true?
*  And then you say, well, we have no reality, no modeling.
*  What's going on in those other dimensions that are wrapped into it on each other.
*  Yeah.
*  You're right.
*  Or, or the multiverse, you know, I honestly don't know how for us, for human
*  interaction, for ideas of intelligence, how it helps us to understand that we're
*  made up of vibrating strings that are like 10 to the whatever times smaller
*  than us.
*  I don't, you know, you could probably build better weapons, better rockets,
*  but you're not going to be able to understand intelligence, better computers.
*  No, you won't be able.
*  I think it's just more purely knowledge.
*  You might lead to a better understanding of the, of the beginning of the universe.
*  Right.
*  It might lead to a better understanding of, uh, I don't know.
*  I guess I think the acquisition of knowledge has always been one where you,
*  you pursue it for its own pleasure.
*  Um, and you don't always know what it's going to make a difference.
*  Yeah.
*  Uh, you're pleasantly surprised by the weird things you find.
*  Do you think, uh, for the, for the neocortex in general, do you think there's
*  a lot of innovation to be done on the machine side?
*  You know, you use the computer as a metaphor quite a bit.
*  Is there different types of computer that would help us build?
*  I mean, what are the manifestations of intelligent machines?
*  Yeah.
*  Or is it, oh, no, it's going to be totally crazy.
*  Uh, we have no idea how this is going to look out yet.
*  Uh, you can already see this.
*  Um, today we've, of course we model these things on traditional computers and
*  now, now GPUs are really popular with, with, uh, you know, neural networks and so
*  on, um, but there are companies coming up with fundamentally new physical
*  substrates, um, that are just really cool.
*  I don't know if they're going to work or not.
*  Um, but I think there'll be decades of innovation here.
*  Yeah.
*  Totally.
*  Do you think the final thing will be messy like our biology is messy?
*  Or do you think, um, it's, it's the, it's the old bird versus airplane question.
*  Or do you think we could just, um, build airplanes that fly way better than
*  birds in the same way we build, uh, uh, electrical and neural cortex?
*  Yeah.
*  You know, can I, can I, can I riff on the bird thing a bit?
*  Cause I think it's interesting.
*  People really misunderstand this.
*  The Wright brothers, um, the problem they were trying to solve was controlled
*  flight, how to turn an airplane, not how to propel an airplane.
*  They weren't worried about that.
*  They already had at that time, there was already wing shapes, which
*  they had from studying birds.
*  There was already gliders that carry people.
*  The problem was if you put a rudder on the back of a glider and you turn it,
*  the plane falls out of the sky.
*  So the problem was how do you control flight?
*  And they studied birds and they actually had birds in captivity.
*  They watched birds in wind tunnels and they observed them in the wild.
*  And they discovered the secret was the birds twist their wings when they turn.
*  And so that's what they did on the Wright brothers flyer.
*  They had these sticks that you would twist the wing and that was, that
*  was their innovation, not their propeller.
*  And today airplanes still twist their wings.
*  We don't twist the entire wing.
*  We just, just the tail end of it.
*  The flaps, which is the same thing.
*  So today's airplanes fly on the same principles as birds, which is observed
*  by, so everyone get that analogy wrong, but let's step back from that, right?
*  Once you understand the principles of flight, you can choose how to implement them.
*  No one's going to use bones and feathers and muscles, but they do have wings
*  and we don't flap them.
*  We have propellers.
*  So when we have the principles of, of computation that goes onto
*  modeling the world in a brain, we understand those principles.
*  Right?
*  Clearly we have choices on how to implement them and some of them
*  would be biological like, and some won't.
*  And, um, but I do think there's going to be a huge amount of innovation here.
*  Just think about the innovation when in the computer, they had
*  to invent the, the, the transistor.
*  It invented the silicon ship.
*  They had to invent, you know, then this software, I mean, it's
*  the things they had to do.
*  Memory systems, um, we're going to do, it's going to be similar.
*  Well, it's interesting that the deep learning, um, the effectiveness of deep
*  learning for specific tasks is driving a lot of innovation in the hardware,
*  which may have effects for, uh, actually allowing us to discover intelligence
*  systems that operate very differently or it's much bigger than deep learning.
*  So ultimately it's good to have an application that's making our life
*  better now because the, the, the capitalist process, if you can make money,
*  that works.
*  I mean, the other way, I mean, Neil, Neil deGrasse Tyson writes about this is the
*  other way we fund science, of course, is through military.
*  So like, uh, conquest.
*  So here's an interesting thing we're doing on this regard.
*  So we've decided we, we, we used to have a series of these biological principles
*  and we can see how to build these intelligent machines, but we've decided
*  to apply some of these principles to today's machine learning techniques.
*  So, uh, one of the, we didn't talk about this principle.
*  One is, uh, sparsity in the brain.
*  Um, most of the neurons are inactive at any point in time.
*  It's sparse and the connectivity is sparse and that's different
*  than deep learning networks.
*  Um, so we've already shown that we can speed up existing deep learning networks,
*  uh, anywhere from 10 to a factor of a hundred.
*  I mean, literally a hundred, um, and make it more robust at the same time.
*  So this is commercially very, very valuable.
*  Um, and so, you know, if we can prove this actually in the largest systems
*  that are commercially applied today, there's a big commercial desire to do this.
*  Well, sparsity is something that doesn't run really well on existing hardware.
*  It doesn't really run really well, um, on, um, GPUs, um, and on CPUs.
*  And so that would be a way of sort of bringing more, uh, more brain principles
*  into the existing system on a, on a commercially valuable basis.
*  Another thing we can think we can do is we're going to use these dendrites,
*  um, models of, we, uh, I talked earlier about the prediction
*  and current insight in their own, that that basic property can be applied to
*  existing neural networks and allow them to learn continuously with something
*  they don't do today.
*  And so the dendritic spikes that you were talking about.
*  Yeah.
*  Well, we wouldn't model them as spikes, but the idea that you have that
*  neuro today's neural networks have this something called the point neurons.
*  It's a very simple model of a neuron.
*  And, uh, by adding dendrites to them, it's just one more level of complexity
*  of that's in biological systems.
*  You can solve problems in continuous learning, um, and rapid learning.
*  So we're trying to take, we're trying to bring the existing field.
*  We'll see if we can do it.
*  We're trying to bring the existing field of machine learning, um,
*  commercially along with us.
*  You brought up this idea of keeping, you know, paying for it commercially
*  along with us, as we move towards the ultimate goal of a true AI system.
*  Even small innovations on your own networks are really, really exciting.
*  Yeah.
*  Is it seems like such a trivial model of the brain and applying different insights
*  that just even, like you said, continuous, uh, learning or, uh, making it more
*  asynchronous or maybe making more dynamic or like, uh, incentivizing,
*  making it even just one more robust, uh, and making it somehow much better.
*  It incentivizing sparsity, uh, somehow.
*  Yeah.
*  Uh, well, if you can make things a hundred times faster, then there's
*  plenty of incentive people, people spending millions of dollars, you know,
*  just training some of these networks.
*  Now these, uh, these transforming networks.
*  Let me ask you the big question.
*  How, for young people, uh, listening to this today in high school and college,
*  what advice would you give them in terms of, uh, which career path to take?
*  And, um, maybe just about life in general.
*  Well, in my case, um, I didn't start life with any kind of goals.
*  I was, when I was going to college, it's like, Oh, what did I say?
*  Well, maybe we'll do it's electrical engineering stuff.
*  You know?
*  Um, I wasn't like, you know, today you see some of these young kids are so
*  motivated, like I'm going to change it.
*  Well, I was like, you know, but they have her and, um, but then I did fall in
*  love with something besides my wife, but I fell in love with this, like, oh my
*  God, it would be so cool to understand how the brain works.
*  And then I said to myself, that's the most important thing I could work on.
*  I can't imagine anything more important because if you understand how brains
*  work, you build tells the machines and they could figure out all the other big
*  questions in the world.
*  Right.
*  So, and then I said, I want to understand how I work.
*  So I fell in love with this idea and I became passionate about it.
*  And this is a trope.
*  People say this, but it was, it's true because I was passionate about it.
*  I was able to put up with almost so much crap, you know, you know, I
*  was, I was in that, you know, I was like, person said, you can't do this.
*  I was, I was a graduate student at Berkeley when they said, you can't
*  study this problem, you know, no one's can solve this or you can't get funded
*  for it, you know, then I went into do, you know, mobile computing and it was
*  like, people say, you can't do that.
*  You can't build a cell phone, you know?
*  So, but all along I kept being motivated because I wanted to work on this problem.
*  I said, I want to understand the brain works.
*  And I got myself, you know, I got one lifetime.
*  I'm going to figure it out, do the best I can.
*  So by having that, because, you know, it's really, as you point out, Lex,
*  it's really hard to do these things.
*  People, it's just, there's so many downers along the way.
*  So many obstacles to get in your way.
*  Yeah.
*  I'm sitting here happy all the time, but trust me, it's not always like that.
*  That's, I guess the, the happiness, the, the, the passion is a prerequisite
*  for surviving the whole thing.
*  Yeah, I think so.
*  I think that's right.
*  Um, and so I, I don't want to sit to someone and say, you know, you need
*  to find a passion and do it.
*  No, maybe you don't.
*  But if you do find something you're passionate about, then, then you can
*  follow it as far as your passion will let you put up with it.
*  Do you remember how you found it?
*  The, how the spark happened?
*  Why specifically for me?
*  Yeah.
*  Like, cause you said it's such an interesting, so like almost like later
*  in life, by later, I mean, like not when you were five, you, you didn't really
*  know, and then all of a sudden you fell in love with that.
*  Yeah.
*  Yeah.
*  I, there was, there was, there was two separate events that compounded one
*  another one when I was probably a teenager, it might've been 17 or 18.
*  I made a list of the most interesting problems I could think of.
*  First was why does the universe exist?
*  It seems like not existing is more likely.
*  The second one was, well, given exists, why does it behave the way it does?
*  You know, it's laws of physics.
*  Why is it equal MC squared, not MC cubed?
*  You know, attention question.
*  I don't know.
*  Third one was like, what's the origin of life?
*  Um, and the fourth one was what's intelligence.
*  And I stopped there.
*  I said, well, that's probably the most interesting one.
*  And I put that aside, um, as a teenager.
*  But then when I was 22 and I was reading the, um, no, it was, I was,
*  excuse me, I was 70, it was 1979.
*  Excuse me.
*  1979.
*  I was reading, uh, so I was at that time was 22.
*  Uh, I was reading, uh, the September issue of scientific American,
*  which was all about the brain.
*  And then the final essay was by Francis Crick, who of DNA fame.
*  And he had taken his interest to studying the brain now.
*  And he said, you know, there's something wrong here.
*  He says, we got all this data, all this fact, this is 1979, all these
*  facts about the brain, tons and tons of facts about the brain.
*  Do we need more facts?
*  Or do we just need to think about a way of rearranging the facts we have?
*  Maybe we're just not thinking about the problem correctly.
*  You know, cause he says this shouldn't be, this shouldn't be like this.
*  You know?
*  So I read that and I said, wow.
*  I said, I don't have to become like an experimental neuroscientist.
*  I could just look at all those facts and try and become a theoretician
*  and try to figure it out.
*  And I said that I felt like it was something I would be good at.
*  I said, I wouldn't be a good experimentalist.
*  I don't have the patience for it, but I'm a good thinker and I love puzzles.
*  And this is like the biggest puzzle in the world.
*  It's the biggest puzzle of all time.
*  Like all the puzzle pieces in front of me.
*  Damn, that was exciting.
*  And there's something obviously you can't cover into words that just kind of.
*  Sparked this passion.
*  And I have that a few times in my life, just something.
*  Um, yeah, just, just like you, uh, it grabs you.
*  Yeah.
*  I thought it was something that was both important and that I
*  could make a contribution to.
*  Yeah.
*  And so all of a sudden it felt like, Oh, it gave me purpose in life.
*  You know, I honestly don't think it has to be as big as one of those four
*  questions.
*  Yeah.
*  I think you can find those things in the smallest.
*  Oh, absolutely.
*  I'm with the David Foster Wallace said like the key to life is to be unborrowable.
*  Um, I think, I think it's very possible to find that intensity of joy in the
*  smallest thing.
*  Absolutely.
*  I'm just, you asked me my story.
*  Yeah.
*  Yeah.
*  So I'm actually speaking to the audience.
*  Yeah.
*  It doesn't have to be those four.
*  You happen to get excited by one of the bigger questions in the universe, but,
*  but even the smallest things and watching the Olympics now, just, uh, just giving
*  yourself life, uh, giving your life over to the study and the mastery of a
*  particular sport is fascinating.
*  And, and, uh, if, if it sparks joy and passion, you're able to, in the case of
*  the Olympics, basically suffer for like a couple of decades to achieve.
*  I mean, you can find joy and passion just being a parent.
*  I mean, it.
*  Yeah.
*  Yeah.
*  The, the parenting one is funny.
*  So I, I was, uh, not always, but for a long time, wanted kids and get married
*  and stuff, and especially that has to do with the fact that I've seen a lot of
*  people that I respect get a whole nother level of joy from kids.
*  And, uh, you know, at first is like, you're thinking is, well, like, I don't
*  have enough time in the day, right?
*  If I have this passion to solve, which is true, but like, if I want to solve
*  intelligence, how's this kid situation going to help me?
*  But then you realize that, uh, you know, like you said, the things that sparks
*  joy and it's very possible that kids can provide even a greater or deeper, more
*  meaningful joy than those bigger questions.
*  When they, they enrich each other.
*  And that, that seemed like, um, obviously when I was younger, it's probably a
*  counterintuitive notion because there's only so many hours in the day, but then
*  life is finite and yet to pick the things that give, give you joy.
*  Yeah.
*  But you know, also you understand you, you can be patient too.
*  I mean, it's finite, but we do have, you know, whatever 50 years or so.
*  It's us along.
*  Yeah.
*  So in my case, you know, in my case, I had to give up on my dream of the
*  neuroscience because I was a graduate student at Berkeley and they told me I
*  couldn't do this and I couldn't get funded.
*  And, you know, and, and so I went back in and went back in the computing
*  industry for a number of years.
*  I thought it would be four, but it turned out to be more.
*  But I said, but I said, I'll come back.
*  You know, I definitely, I'm definitely going to come back.
*  I know I'm going to do this computer stuff for a while, but I'm definitely
*  coming back, everyone knows that.
*  And it's like, you know, like raising kids.
*  Well, yeah, you still, you have to spend a lot of time with your kids.
*  It's fun, enjoyable.
*  Um, but that doesn't mean you have to give up on other dreams.
*  It just means that you may have to wait a week or two to work on that next idea.
*  Well, you talked about the, the, the darker side of me, disappointing
*  sides of human nature that we're hoping to overcome so that we don't destroy
*  ourselves, I tend to put a lot of value in, um, the broad general concept of love.
*  Of, uh, the human capacity to, um, of compassion towards each other of just
*  kindness, whatever that longing of the, just the human human to human connection.
*  It connects back to our initial discussion.
*  I tend to see a lot of value in this collective intelligence aspect.
*  I think some of the magic of human civilization happens when there's, uh, a
*  party is not as fun when you're alone.
*  I totally agree with you on these issues.
*  Do you think from a newer cortex perspective, do you, uh, what role does
*  love play in the human?
*  Well, those are two separate things from a new project.
*  I don't think it, it doesn't impact our thinking about human, uh, about the
*  near cortex from a human condition point of view, I think it's core.
*  Um, I mean, we get so much pleasure out of loving people and helping people.
*  Um, so, you know, I can, I, I'll rack it up to old brain stuff and maybe we can
*  throw it under the bus of evolution if you want, um, uh, that's fine.
*  Um, uh, it doesn't impact how I think about how we model the world.
*  Um, but from a humanity point of view, I think it's essential.
*  Why I tend to give it to the new brain.
*  And also I tend to think the sum of aspects of that need to be engineered
*  into AI systems, uh, both in their ability to have compassion for the humans.
*  And their ability to maximize love in the world between humans.
*  So I'm more thinking about the social network.
*  So like whenever there's a deep integration between AI systems and
*  humans, so specific applications where it's, uh, AI and humans, I think that's
*  something that's often not talked about in terms of, um, metrics over which
*  you try to maximize, uh, like which metric to maximize in a system.
*  It seems like one of the most powerful things in societies is the capacity to.
*  It's fascinating.
*  I think it's, it's a great way of thinking about it.
*  You know, I have, I have been thinking more of these fundamental
*  mechanisms in the brain, as opposed to the social interaction between the
*  interaction between humans and AI systems in the future, which is, and I
*  think if you think about that, you're absolutely right.
*  Um, but that's, that's a complex system.
*  I can have intelligence systems that don't have that component, but they're
*  not interacting with people, you know, they're just running something or
*  building a building some place or something, I don't know.
*  Um, but if you think about interacting with humans, yeah, it's, it's gonna,
*  but it has to be engineered in there.
*  I don't think it's going to appear on its own.
*  Yeah, that's a good question.
*  I, yeah, well, we could, well, in terms of, uh, uh, from a reinforcement
*  learning perspective, whether the darker sides of human nature or the better
*  angels of our nature, uh, went out statistically speaking, I don't know.
*  I tend to be optimistic and hope that love wins out in the end.
*  Um, you've done a lot of incredible stuff.
*  Uh, and your book is, uh, driving towards this fourth question that you
*  started with, uh, on the nature of intelligence, what do you hope your
*  legacy for people reading a hundred years from now, how do you hope
*  they remember your work?
*  How do you hope they remember this book?
*  Well, I think as an entrepreneur or a scientist or any human who's trying
*  to accomplish some things, I have a view that really all you can do
*  is accelerate the inevitable.
*  Um, it's like, you know, if we didn't figure out, if we didn't study the brain,
*  if, you know, if you want us to, didn't make electric cars, someone
*  else would do it eventually.
*  And if, you know, if Thomas Edison didn't invent a light bulb, we
*  wouldn't be using candles today.
*  So what you can do as an individual is you can accelerate something that's
*  beneficial and make it happen sooner than it would have.
*  That's, that's really it.
*  That's all you can do.
*  Um, you can't create a new reality that it wasn't going to happen.
*  Um, so from that perspective, um, I would hope that our work, not just me,
*  but our work in general, um, people would look back and said, Hey, they
*  really helped make this better future happen sooner.
*  Um, they, you know, they helped us understand the nature of false beliefs
*  sooner than they made right now.
*  They made it.
*  Now we're so happy that we have these intelligent machines doing these
*  things, helping us that that maybe that solved the climate change problem.
*  And they made it happen sooner.
*  So I think that's the best I would hope for.
*  Some would say those guys just move the needle forward a little bit in time.
*  Well, I do.
*  It feels like the progress of human civilization is not, is, uh, there's a
*  lot of trajectories and if you have individuals that accelerate towards one
*  direction that helps steer human civilization.
*  So I think in those long stretch of time, all, all trajectories will be traveled,
*  but I think it's nice for this particular civilization on earth to travel down one.
*  That's yeah.
*  Well, I think you're right.
*  You look, we have the take the whole period of, you know, World War II
*  Nazism or something like that.
*  Well, that was a bad side step, right?
*  We went over there for a while, but you know, there is the optimistic view about
*  life that, um, that ultimately it does converge in a positive way.
*  It progresses ultimately, even if we have years of darkness.
*  Um, so yeah, so I think you could perhaps that's accelerating the positive.
*  It could also mean eliminating some bad missteps along the way too.
*  Um, but, but I, I'm an optimistic in that way.
*  I, I'd say, you know, despite we talked about the end of civilization, you know,
*  I, I think we're going to live for a long time.
*  I hope we are.
*  Um, I think our society and the future is going to be better.
*  We're going to have less discord.
*  We're going to have less people killing each other.
*  You know, we'll solve, you know, we'll make them live in some sort of way that's
*  compatible with the current capacity of the earth.
*  Um, I'm optimistic these things will happen.
*  Uh, and all we can do is try to get there sooner.
*  And at the very least, if we do destroy ourselves, we'll have a few satellites
*  orbiting, uh, that will, uh, that will tell alien civilization that we were once.
*  Or maybe our future, you know, future inhabitants of earth, you know, imagine
*  we, you know, the planet of the apes scenario, you know, we kill ourselves in a,
*  you know, million years from now or a billion years from now, there's another
*  species on the planet.
*  Curious creatures were once here.
*  Jeff, thank you so much for your work and, um, thank you so much for
*  talking to me once again.
*  Well, actually it's great.
*  I love what you do.
*  I love your podcast.
*  You have those interesting people me aside.
*  So, uh, it's a real service.
*  I think you do for, uh, in a very broader sense for humanity, I think.
*  Thanks Jeff.
*  All right.
*  Pleasure.
*  Thanks for listening to this conversation with Jeff Hawkins and thank you to
*  Code Academy, Biooptimizers, ExpressVPN, Aidsleep, and Blinkist.
*  Check them out in the description to support this podcast.
*  And now let me leave you with some words from Albert Camus.
*  An intellectual is someone whose mind watches itself.
*  I like this because I'm happy to be both halves, the watcher and the watched.
*  Can they be brought together?
*  This is a practical question we must try to answer.
*  Thank you for listening.
*  I hope to see you next time.
