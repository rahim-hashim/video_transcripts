---
Date Generated: February 03, 2025
Transcription Model: whisper medium 20231117
Length: 18378s
Video Keywords: ['Dylan Patel and Nathan Lambert', 'alex friedman', 'lex ai', 'lex debate', 'lex freedman', 'lex fridman', 'lex friedman', 'lex interview', 'lex lecture', 'lex mit', 'lex podcast', 'lex transcript']
Video Views: 256139
Video Rating: None
Video Description: Dylan Patel is the founder of SemiAnalysis, a research & analysis company specializing in semiconductors, GPUs, CPUs, and AI hardware. Nathan Lambert is a research scientist at the Allen Institute for AI (Ai2) and the author of a blog on AI called Interconnects.
Thank you for listening ‚ù§ Check out our sponsors: https://lexfridman.com/sponsors/ep459-sb
See below for timestamps, and to give feedback, submit questions, contact Lex, etc.

*CONTACT LEX:*
*Feedback* - give feedback to Lex: https://lexfridman.com/survey
*AMA* - submit questions, videos or call-in: https://lexfridman.com/ama
*Hiring* - join our team: https://lexfridman.com/hiring
*Other* - other ways to get in touch: https://lexfridman.com/contact

*EPISODE LINKS:*
Dylan's X: https://x.com/dylan522p
SemiAnalysis: https://semianalysis.com/
Nathan's X: https://x.com/natolambert
Nathan's Blog: https://www.interconnects.ai/
Nathan's Podcast: https://www.interconnects.ai/podcast
Nathan's Website: https://www.natolambert.com/
Nathan's YouTube: https://youtube.com/@natolambert
Nathan's Book: https://rlhfbook.com/

*SPONSORS:*
To support this podcast, check out our sponsors & get discounts:
*Invideo AI:* AI video generator.
Go to https://lexfridman.com/s/invideoai-ep459-sb
*GitHub:* Developer platform and AI code editor.
Go to https://lexfridman.com/s/github-ep459-sb
*Shopify:* Sell stuff online.
Go to https://lexfridman.com/s/shopify-ep459-sb
*NetSuite:* Business management software.
Go to https://lexfridman.com/s/netsuite-ep459-sb
*AG1:* All-in-one daily nutrition drinks.
Go to https://lexfridman.com/s/ag1-ep459-sb

*OUTLINE:*
0:00 - Introduction
3:33 - DeepSeek-R1 and DeepSeek-V3
25:07 - Low cost of training
51:25 - DeepSeek compute cluster
58:57 - Export controls on GPUs to China
1:09:16 - AGI timeline
1:18:41 - China's manufacturing capacity
1:26:36 - Cold war with China
1:31:05 - TSMC and Taiwan
1:54:44 - Best GPUs for AI
2:09:36 - Why DeepSeek is so cheap
2:22:55 - Espionage
2:31:57 - Censorship
2:44:52 - Andrej Karpathy and magic of RL
2:55:23 - OpenAI o3-mini vs DeepSeek r1
3:14:31 - NVIDIA
3:18:58 - GPU smuggling
3:25:36 - DeepSeek training on OpenAI data
3:36:04 - AI megaclusters
4:11:26 - Who wins the race to AGI?
4:21:39 - AI agents
4:30:21 - Programming and AI
4:37:49 - Open source
4:47:01 - Stargate
4:54:30 - Future of AI

*PODCAST LINKS:*
- Podcast Website: https://lexfridman.com/podcast
- Apple Podcasts: https://apple.co/2lwqZIr
- Spotify: https://spoti.fi/2nEwCF8
- RSS: https://lexfridman.com/feed/podcast/
- Podcast Playlist: https://www.youtube.com/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4
- Clips Channel: https://www.youtube.com/lexclips

*SOCIAL LINKS:*
- X: https://x.com/lexfridman
- Instagram: https://instagram.com/lexfridman
- TikTok: https://tiktok.com/@lexfridman
- LinkedIn: https://linkedin.com/in/lexfridman
- Facebook: https://facebook.com/lexfridman
- Patreon: https://patreon.com/lexfridman
- Telegram: https://t.me/lexfridman
- Reddit: https://reddit.com/r/lexfridman
---

# DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters | Lex Fridman Podcast #459
**Lex Fridman:** [February 02, 2025](https://www.youtube.com/watch?v=_1f-o0nqpEI)
*  The following is a conversation with Dylan Patel and Nathan Lampert. [[00:00:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=0.0s)]
*  Dylan runs SemiAnalysis, a well-respected research and analysis company [[00:00:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4.66s)]
*  that specializes in semiconductors, GPUs, CPUs, and AI hardware in general. [[00:00:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10.4s)]
*  Nathan is a research scientist at the Allen Institute for AI [[00:00:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16.86s)]
*  and is the author of the amazing blog on AI called Interconnects. [[00:00:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=21.36s)]
*  They are both highly respected, read, and listened to by the experts, [[00:00:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=27.16s)]
*  researchers, and engineers in the field of AI. [[00:00:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=31.72s)]
*  And personally, I'm just a fan of the two of them. [[00:00:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=34.76s)]
*  So I used the DeepSeq moment that shook the AI world a bit [[00:00:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=38.32s)]
*  as an opportunity to sit down with them and lay it all out. [[00:00:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=43.82s)]
*  From DeepSeq OpenAI, Google XAI, MetaAnthropic, to NVIDIA and TSMC, [[00:00:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=47.760000000000005s)]
*  and to US-China-Taiwan relations, [[00:00:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=53.82s)]
*  and everything else that is happening at the cutting edge of AI. [[00:00:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=57.480000000000004s)]
*  This conversation is a deep dive into many critical aspects of the AI industry. [[00:01:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=61.480000000000004s)]
*  While it does get super technical, [[00:01:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=67.92s)]
*  we try to make sure that it's still accessible to folks outside of the AI field [[00:01:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=70.88s)]
*  by defining terms, stating important concepts explicitly, [[00:01:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=75.48s)]
*  spelling out acronyms, and in general, [[00:01:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=79.32s)]
*  always moving across the several layers of abstraction and levels of detail. [[00:01:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=81.78s)]
*  There is a lot of hype in the media about what AI is and isn't. [[00:01:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=86.58s)]
*  The purpose of this podcast, in part, is to cut through the hype, [[00:01:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=92.48s)]
*  through the bullshit, and the low resolution analysis, [[00:01:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=97.02s)]
*  and to discuss in detail how stuff works and what the implications are. [[00:01:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=101.18s)]
*  Let me also, if I may, comment on the new OpenAI 03 Mini Resetting Model, [[00:01:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=106.58s)]
*  the release of which we were anticipating during the conversation, [[00:01:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=111.94s)]
*  and it did indeed come out right after. [[00:01:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=115.48s)]
*  Its capabilities and costs are on par with our expectations, as we stated. [[00:01:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=118.34s)]
*  OpenAI 03 Mini is indeed a great model, [[00:02:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=124.82000000000001s)]
*  but it should be stated that DeepSeq R1 has similar performance on benchmarks, [[00:02:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=128.08s)]
*  is still cheaper, and it reveals its chain of thought reasoning, [[00:02:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=133.12s)]
*  which 03 Mini does not. [[00:02:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=137.68s)]
*  It only shows a summary of the reasoning. [[00:02:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=139.58s)]
*  Plus, R1 is open weight, and 03 Mini is not. [[00:02:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=142.96s)]
*  By the way, I got a chance to play with 03 Mini, [[00:02:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=149.02s)]
*  and anecdotal vibe check-wise, I felt that 03 Mini, [[00:02:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=152.58s)]
*  specifically 03 Mini High, is better than R1. [[00:02:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=157.62s)]
*  Still, for me personally, I find that ClaudeSona35 is the best model for programming, [[00:02:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=162.06s)]
*  except for tricky cases where I will use 01 Pro to brainstorm. [[00:02:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=167.62s)]
*  Either way, many more better AI models will come, [[00:02:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=172.02s)]
*  including reasoning models, both from American and Chinese companies. [[00:02:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=175.5s)]
*  They will continue to shift the cost curve. [[00:03:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=180.6s)]
*  But the, quote, DeepSeq moment is indeed real. [[00:03:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=183.52s)]
*  I think it will still be remembered five years from now [[00:03:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=187.86s)]
*  as a pivotal event in tech history, [[00:03:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=190.66s)]
*  due in part to the geopolitical implications, [[00:03:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=193.1s)]
*  but for other reasons too, [[00:03:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=196.06s)]
*  as we discuss in detail from many perspectives in this conversation. [[00:03:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=198.32s)]
*  This is the Lex Friedman Podcast. [[00:03:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=202.4s)]
*  To support it, please check out our sponsors in the description. [[00:03:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=204.3s)]
*  And now, dear friends, here's Dylan Patel and Nathan Lambert. [[00:03:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=207.76s)]
*  A lot of people are curious to understand China's DeepSeq AI models, [[00:03:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=213.7s)]
*  so let's lay it out. [[00:03:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=217.42s)]
*  Nathan, can you describe what DeepSeq V3 and DeepSeq R1 are, [[00:03:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=218.9s)]
*  how they work, how they're trained? [[00:03:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=223.66s)]
*  Let's look at the big picture and then we'll zoom in on the details. [[00:03:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=225.6s)]
*  Yes, DeepSeq V3 is a new mixture of experts, [[00:03:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=229.26s)]
*  transformer language model from DeepSeq, who is based in China. [[00:03:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=234.06s)]
*  They have some new specifics in the model that we'll get into. [[00:03:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=238.96s)]
*  Largely, this is an open weight model, [[00:04:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=243.6s)]
*  and it's an instruction model like what you would use in chat GPT. [[00:04:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=246.23999999999998s)]
*  They also released what is called the base model, [[00:04:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=251.2s)]
*  which is before these techniques of post-training. [[00:04:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=253.23999999999998s)]
*  Most people use instruction models today, [[00:04:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=256.46s)]
*  and those are what's served in all sorts of applications. [[00:04:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=258.59999999999997s)]
*  This was released on, I believe, December 26th or that week. [[00:04:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=261.7s)]
*  And then weeks later on January 20th, [[00:04:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=266.7s)]
*  DeepSeq released DeepSeq R1, which is a reasoning model, [[00:04:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=270.46s)]
*  which really accelerated a lot of this discussion. [[00:04:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=274.64s)]
*  This reasoning model has a lot of overlapping training steps to DeepSeq V3, [[00:04:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=278.7s)]
*  and it's confusing that you have a base model called V3, [[00:04:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=283.24s)]
*  that you do something to get a chat model, [[00:04:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=287.03999999999996s)]
*  and then you do some different things to get a reasoning model. [[00:04:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=290.34s)]
*  I think a lot of the AI industry is going through this challenge [[00:04:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=293.84s)]
*  of communications right now, [[00:04:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=296.2s)]
*  where OpenAI makes fun of their own naming schemes. [[00:04:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=297.44s)]
*  They have GPT-40, they have OpenAI-01, [[00:05:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=300.02000000000004s)]
*  and there's a lot of types of models, [[00:05:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=304.68s)]
*  so we're going to break down what each of them are. [[00:05:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=306.02000000000004s)]
*  There's a lot of technical specifics on training, [[00:05:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=308.44s)]
*  and go from high level to specific, [[00:05:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=311.18s)]
*  and kind of go through each of them. [[00:05:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=313.12s)]
*  There's so many places we can go here, [[00:05:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=314.88s)]
*  but maybe let's go to open weights first. [[00:05:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=317.08000000000004s)]
*  What does it mean for a model to be open weights, [[00:05:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=319.28000000000003s)]
*  and what are the different flavors of open source in general? [[00:05:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=321.18s)]
*  Yeah, so this discussion has been going on for a long time in AI. [[00:05:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=323.54s)]
*  It became more important since ChatGPT, [[00:05:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=327.18s)]
*  or more focal since ChatGPT at the end of 2022. [[00:05:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=329.34000000000003s)]
*  Open weights is the accepted term for when model weights [[00:05:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=332.64s)]
*  of a language model are available on the internet [[00:05:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=336.88s)]
*  for people to download. [[00:05:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=339.22s)]
*  Those weights can have different licenses, [[00:05:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=340.68s)]
*  which is effectively the terms by which you can use the model. [[00:05:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=342.88s)]
*  There are licenses that come from history [[00:05:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=346.72s)]
*  in open source software. [[00:05:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=348.68s)]
*  There are licenses that are designed [[00:05:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=349.84000000000003s)]
*  by companies specifically. [[00:05:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=351.28000000000003s)]
*  All of Llama, DeepSeek, Quan, Mistral, [[00:05:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=354.02s)]
*  these popular names in open weight models [[00:05:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=357.91999999999996s)]
*  have some of their own licenses. [[00:06:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=361.62s)]
*  It's complicated because not all the same models [[00:06:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=363.35999999999996s)]
*  have the same terms. [[00:06:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=365.21999999999997s)]
*  The big debate is on what makes a model open weight. [[00:06:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=367.12s)]
*  Why are we saying this term? [[00:06:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=372.41999999999996s)]
*  It's kind of a mouthful. [[00:06:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=373.52s)]
*  It sounds close to open source, but it's not the same. [[00:06:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=374.35999999999996s)]
*  There's still a lot of debate on the definition [[00:06:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=377.68s)]
*  and soul of open source AI. [[00:06:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=380.32s)]
*  Open source software has a rich history [[00:06:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=382.42s)]
*  on freedom to modify, freedom to take on your own, [[00:06:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=384.26s)]
*  freedom from any restrictions on how you would use [[00:06:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=387.66s)]
*  the software and what that means for AI [[00:06:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=390.02000000000004s)]
*  is still being defined. [[00:06:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=393.02000000000004s)]
*  So for what I do, I work at the Allen Institute for AI. [[00:06:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=394.48s)]
*  We're a nonprofit. [[00:06:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=399.08000000000004s)]
*  We want to make AI open for everybody. [[00:06:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=400.56s)]
*  And we try to lead on what we think is truly open source. [[00:06:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=402.78000000000003s)]
*  There's not full agreement in the community, but for us, [[00:06:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=405.58000000000004s)]
*  that means releasing the training data, [[00:06:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=408.32s)]
*  releasing the training code, [[00:06:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=410.32s)]
*  and then also having open weights like this. [[00:06:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=411.58s)]
*  And we'll get into the details of the models. [[00:06:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=414.21999999999997s)]
*  And again and again, as we try to get deeper [[00:06:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=416.94s)]
*  into how the models were trained, [[00:06:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=419.78s)]
*  we will say things like the data processing, [[00:07:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=422.02s)]
*  data filtering, data quality is the number one determinant [[00:07:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=425.08s)]
*  of the model quality. [[00:07:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=428.38s)]
*  And then a lot of the training code is the determinant [[00:07:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=429.68s)]
*  on how long it takes to train [[00:07:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=432.08s)]
*  and how faster experimentation is. [[00:07:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=433.97999999999996s)]
*  So without fully open source models [[00:07:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=436.38s)]
*  where you have access to this data, [[00:07:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=438.74s)]
*  it is hard to know, or it's harder to replicate. [[00:07:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=440.5s)]
*  So we'll get into cost numbers for DeepSeq v3 [[00:07:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=443.94s)]
*  on mostly GPU hours and how much you could pay [[00:07:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=446.8s)]
*  to rent those yourselves. [[00:07:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=450.8s)]
*  But without the data, [[00:07:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=452.3s)]
*  the replication cost is going to be far, far higher. [[00:07:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=453.2s)]
*  And same goes for the code. [[00:07:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=456.1s)]
*  We should also say that this is probably one [[00:07:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=457.8s)]
*  of the more open models out of the frontier models. [[00:07:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=460.64s)]
*  So like in this full spectrum [[00:07:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=463.94s)]
*  where probably the fullest open source, like you said, [[00:07:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=466.58s)]
*  open code, open data, open weights, [[00:07:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=469.4s)]
*  this is not open code. [[00:07:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=473.2s)]
*  This is probably not open data. [[00:07:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=475.84s)]
*  And this is open weights. [[00:07:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=479.76s)]
*  And the licensing is MIT license or it's, [[00:08:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=481.79999999999995s)]
*  I mean, there's some nuance in the different models, [[00:08:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=486.7s)]
*  but it's towards the free, [[00:08:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=488.53999999999996s)]
*  in terms of the open source movement, [[00:08:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=490.53999999999996s)]
*  these are the kind of the good guys. [[00:08:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=491.76s)]
*  Yeah, DeepSeq is doing fantastic work [[00:08:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=493.44s)]
*  for disseminating understanding of AI. [[00:08:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=495.56s)]
*  Their papers are extremely detailed in what they do. [[00:08:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=498.76s)]
*  And for other teams around the world, [[00:08:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=502.15999999999997s)]
*  they're very actionable in terms of improving [[00:08:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=505.36s)]
*  your own training techniques. [[00:08:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=507.71999999999997s)]
*  And we'll talk about licenses more. [[00:08:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=510.42s)]
*  The DeepSeq R1 model has a very permissive license. [[00:08:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=513.5s)]
*  It's called the MIT license. [[00:08:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=517.06s)]
*  That effectively means there's no downstream restrictions [[00:08:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=518.42s)]
*  on commercial use. [[00:08:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=521.36s)]
*  There's no use case restrictions. [[00:08:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=522.2s)]
*  You can use the outputs from the models [[00:08:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=524.02s)]
*  to create synthetic data. [[00:08:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=525.66s)]
*  And this is all fantastic. [[00:08:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=527.5s)]
*  I think the closest peer is something like Llama, [[00:08:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=529.8s)]
*  where you have the weights and you have a technical report. [[00:08:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=532.7s)]
*  And the technical report is very good for Llama. [[00:08:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=535.6s)]
*  One of the most read PDFs of the year last year [[00:08:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=538.66s)]
*  is the Llama 3 paper, [[00:09:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=540.84s)]
*  but in some ways it's slightly less actionable. [[00:09:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=542.24s)]
*  It has less details on the training specifics, [[00:09:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=544.56s)]
*  like less plots and so on. [[00:09:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=546.76s)]
*  And the Llama 3 license is more restrictive than MIT. [[00:09:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=549.46s)]
*  And then between the DeepSeq custom license [[00:09:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=553.04s)]
*  and the Llama license, [[00:09:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=555.24s)]
*  we could get into this whole rabbit hole, I think. [[00:09:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=556.3s)]
*  We'll make sure we wanna go down the license rabbit hole [[00:09:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=557.8s)]
*  before we do specifics. [[00:09:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=560.3s)]
*  Yeah, and I mean, so it should be stated [[00:09:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=561.74s)]
*  that one of the implications of DeepSeq, [[00:09:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=563.66s)]
*  it puts pressure on Llama and everybody else, [[00:09:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=565.3s)]
*  on OpenAI to push towards open source. [[00:09:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=568.3s)]
*  And that's the other side of open source [[00:09:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=571.9399999999999s)]
*  that you mentioned is how much is published [[00:09:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=573.3s)]
*  in detail about it. [[00:09:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=576.16s)]
*  So how open are you with the insights behind the code? [[00:09:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=577.4s)]
*  So like how good is the technical reports? [[00:09:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=583.14s)]
*  Are they hand wavy or is there actual details in there? [[00:09:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=585.74s)]
*  And that's one of the things that DeepSeq did well [[00:09:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=588.98s)]
*  as they published a lot of the details. [[00:09:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=590.7s)]
*  Yeah, especially in the DeepSeq v3, [[00:09:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=592.58s)]
*  which is their pre-training paper. [[00:09:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=594.54s)]
*  They were very clear that they are doing interventions [[00:09:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=596.1800000000001s)]
*  on the technical stack that go at many different levels. [[00:09:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=599.86s)]
*  For example, to get highly efficient training, [[00:10:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=603.66s)]
*  they're making modifications at or below the CUDA layer [[00:10:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=606.42s)]
*  for Nvidia chips. [[00:10:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=609.74s)]
*  I have never worked there myself [[00:10:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=611.86s)]
*  and there are a few people in the world [[00:10:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=613.58s)]
*  that do that very well and some of them are at DeepSeq. [[00:10:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=615.24s)]
*  And these types of people are at DeepSeq [[00:10:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=617.6800000000001s)]
*  and leading American frontier labs, [[00:10:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=620.94s)]
*  but they're not many places. [[00:10:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=623.32s)]
*  To help people understand the other implication [[00:10:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=625.2s)]
*  of open weights, just, you know, [[00:10:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=627.5600000000001s)]
*  there's a topic we'll return to often here. [[00:10:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=629.16s)]
*  So there's a fear that China, the nation, [[00:10:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=632.44s)]
*  might have interest in stealing American data, [[00:10:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=639.2s)]
*  violating privacy of American citizens. [[00:10:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=643.24s)]
*  What can we say about open weights [[00:10:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=645.76s)]
*  to help us understand what the weights are able to do [[00:10:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=648.64s)]
*  in terms of stealing people's data? [[00:10:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=652.88s)]
*  Yeah, so these weights that you can download [[00:10:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=654.98s)]
*  from Hugging Face or other platforms [[00:10:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=656.84s)]
*  are very big matrices of numbers. [[00:10:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=658.36s)]
*  You can download them to a computer in your own house [[00:11:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=661.34s)]
*  that has no internet and you can run this model [[00:11:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=664.0s)]
*  and you're totally in control of your data. [[00:11:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=666.0s)]
*  That is something that is different [[00:11:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=669.44s)]
*  than how a lot of language model usage [[00:11:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=671.68s)]
*  is actually done today, which is mostly through APIs [[00:11:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=673.2399999999999s)]
*  where you send your prompt to GPUs run by certain companies [[00:11:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=675.52s)]
*  and these companies will have different distributions [[00:11:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=679.8s)]
*  and policies on how your data is stored, [[00:11:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=681.7199999999999s)]
*  if it is used to train future models, [[00:11:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=683.56s)]
*  where it is stored, if it is encrypted and so on. [[00:11:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=685.68s)]
*  So the open weights are, you have your fate of data [[00:11:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=688.64s)]
*  in your own hands and that is something [[00:11:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=691.0s)]
*  that is deeply connected to the soul of open source. [[00:11:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=693.2399999999999s)]
*  So it's not the model that steals your data, [[00:11:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=697.24s)]
*  it's whoever's hosting the model, [[00:11:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=699.3599999999999s)]
*  which could be China if you're using the DeepSeq app [[00:11:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=701.04s)]
*  or it could be Proplexity, [[00:11:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=704.48s)]
*  you're trusting them with your data [[00:11:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=707.76s)]
*  or OpenAI you're trusting them with your data [[00:11:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=709.48s)]
*  and some of these are American companies, [[00:11:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=711.3199999999999s)]
*  some of these are Chinese companies, [[00:11:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=712.8399999999999s)]
*  but the model itself is not doing the stealing, [[00:11:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=714.0s)]
*  it's the host. [[00:11:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=717.24s)]
*  All right, so back to the basics. [[00:11:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=719.02s)]
*  What's the difference between DeepSeq V3 and DeepSeq R1? [[00:12:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=722.12s)]
*  Can we try to like lay out the confusion, [[00:12:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=727.3199999999999s)]
*  the potential? [[00:12:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=730.66s)]
*  Yes, so for one, I have very understanding [[00:12:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=731.5s)]
*  of many people being confused by these two model names. [[00:12:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=734.14s)]
*  So I would say the best way to think about this [[00:12:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=736.62s)]
*  is that when training a language model, [[00:12:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=739.1s)]
*  you have what is called pre-training, [[00:12:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=741.14s)]
*  which is when you're predicting the large amounts [[00:12:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=742.52s)]
*  of mostly internet text, [[00:12:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=745.4399999999999s)]
*  you're trying to predict the next token [[00:12:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=747.14s)]
*  and what to know about these new DeepSeq models [[00:12:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=749.18s)]
*  is that they do this internet large scale pre-training [[00:12:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=751.86s)]
*  once to get what is called DeepSeq V3 base. [[00:12:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=755.66s)]
*  This is the base model, [[00:12:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=758.6999999999999s)]
*  it's just going to finish your sentences for you, [[00:12:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=760.16s)]
*  it's going to be harder to work with than ChatTPT, [[00:12:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=762.8399999999999s)]
*  and then what DeepSeq did is they've done [[00:12:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=765.92s)]
*  two different post-training regimes [[00:12:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=768.1999999999999s)]
*  to make the models have specific desirable behaviors. [[00:12:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=770.64s)]
*  So what is the more normal model [[00:12:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=775.0799999999999s)]
*  in terms of the last few years of AI, [[00:12:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=777.3199999999999s)]
*  an instruct model, a chat model, [[00:12:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=779.7199999999999s)]
*  a quote unquote aligned model, a helpful model, [[00:13:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=781.7199999999999s)]
*  there are many ways to describe this, [[00:13:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=783.88s)]
*  is more standard post-training. [[00:13:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=785.9s)]
*  So this is things like instruction tuning, [[00:13:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=788.14s)]
*  reinforcement learning from human feedback, [[00:13:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=790.3s)]
*  we'll get into some of these words, [[00:13:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=791.98s)]
*  and this is what they did to create the DeepSeq V3 model. [[00:13:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=793.8199999999999s)]
*  This was the first model to be released [[00:13:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=797.6999999999999s)]
*  and it is very high performance, [[00:13:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=800.08s)]
*  it's competitive with GPT-4, Lama 405B, so on. [[00:13:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=803.02s)]
*  And then when this release was happening, [[00:13:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=807.98s)]
*  we don't know their exact timeline or soon after, [[00:13:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=810.9s)]
*  they were finishing the training [[00:13:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=813.22s)]
*  of a different training process [[00:13:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=815.02s)]
*  from the same next token prediction based model [[00:13:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=816.58s)]
*  that I talked about, [[00:13:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=819.7800000000001s)]
*  which is when this new reasoning training [[00:13:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=821.1600000000001s)]
*  that people have heard about comes in, [[00:13:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=823.1600000000001s)]
*  in order to create the model that is called DeepSeq R1. [[00:13:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=825.1s)]
*  The R through this conversation [[00:13:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=828.6800000000001s)]
*  is good for grounding for reasoning, [[00:13:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=830.08s)]
*  and the name is also similar to OpenAI's O1, [[00:13:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=831.6600000000001s)]
*  which is the other reasoning model [[00:13:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=834.14s)]
*  that people have heard about. [[00:13:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=835.38s)]
*  And we'll have to break down the training for R1 [[00:13:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=837.58s)]
*  in more detail because for one, [[00:14:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=840.22s)]
*  we have a paper detailing it, [[00:14:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=842.1600000000001s)]
*  but also it is a far newer set of techniques [[00:14:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=843.82s)]
*  for the AI community, [[00:14:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=846.7800000000001s)]
*  so it is a much more rapidly evolving area of research. [[00:14:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=848.1s)]
*  Maybe we should also say the big two categories of training [[00:14:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=851.86s)]
*  of pre-training and post-training, [[00:14:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=855.9000000000001s)]
*  these umbrella terms that people use. [[00:14:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=858.7s)]
*  So what is pre-training and what is post-training, [[00:14:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=860.5s)]
*  and what are the different flavors of things [[00:14:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=864.74s)]
*  underneath post-training umbrella? [[00:14:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=866.1800000000001s)]
*  Yeah, so pre-training, [[00:14:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=867.84s)]
*  I'm using some of the same words [[00:14:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=869.1400000000001s)]
*  that really get the message across is, [[00:14:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=870.3000000000001s)]
*  you're doing what is called autoregressive prediction [[00:14:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=872.1s)]
*  to predict the next token in a series of documents. [[00:14:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=874.34s)]
*  This is done over standard practice is trillions of tokens, [[00:14:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=877.64s)]
*  so this is a ton of data that is mostly scraped from the web. [[00:14:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=881.82s)]
*  And some of DeepSeq's earlier papers, [[00:14:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=886.98s)]
*  they talk about their training data being distilled for math. [[00:14:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=889.22s)]
*  I shouldn't use this word yet, [[00:14:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=893.1800000000001s)]
*  but taken from CommonCrawl, [[00:14:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=894.34s)]
*  and that's a public access that anyone listening to this [[00:14:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=896.6600000000001s)]
*  could go download data from the CommonCrawl website, [[00:14:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=899.74s)]
*  this is a crawler that is maintained publicly. [[00:15:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=903.06s)]
*  Yes, other tech companies eventually shift [[00:15:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=905.8199999999999s)]
*  to their own crawler, [[00:15:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=908.06s)]
*  and DeepSeq likely has done this as well [[00:15:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=909.06s)]
*  as most frontier labs do. [[00:15:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=911.26s)]
*  But this sort of data is something [[00:15:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=912.9s)]
*  that people can get started with, [[00:15:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=914.7399999999999s)]
*  and you're just predicting text in a series of documents. [[00:15:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=916.3s)]
*  This can be scaled to be very efficient, [[00:15:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=919.78s)]
*  and there's a lot of numbers [[00:15:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=924.8599999999999s)]
*  that are thrown around in AI training, [[00:15:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=926.14s)]
*  like how many floating point operations or flops are used, [[00:15:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=927.4599999999999s)]
*  how many hours of these GPUs that are used. [[00:15:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=931.98s)]
*  And it's largely one loss function [[00:15:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=935.34s)]
*  taken to a very large amount of compute usage. [[00:15:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=937.9s)]
*  You set up really efficient systems. [[00:15:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=942.98s)]
*  And then at the end of that, you have the space model, [[00:15:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=945.1s)]
*  and pre-training is where there is a lot more of complexity [[00:15:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=947.98s)]
*  in terms of how the process is emerging or evolving [[00:15:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=952.58s)]
*  and the different types of training losses that you will use. [[00:15:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=956.98s)]
*  I think this is a lot of techniques grounded [[00:16:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=960.06s)]
*  in the natural language processing literature. [[00:16:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=963.26s)]
*  The oldest technique, which is still used today, [[00:16:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=966.02s)]
*  is something called instruction tuning [[00:16:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=968.02s)]
*  or also known as supervised fine tuning. [[00:16:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=969.9399999999999s)]
*  These acronyms will be IFT or SFT. [[00:16:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=972.5799999999999s)]
*  People really go back and forth throughout them, [[00:16:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=974.98s)]
*  and I will probably do the same, [[00:16:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=978.06s)]
*  which is where you add this formatting to the model [[00:16:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=979.5799999999999s)]
*  where it knows to take a question [[00:16:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=983.1s)]
*  that is like, explain the history of the Roman Empire to me. [[00:16:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=985.46s)]
*  Or something, a sort of question you'll see on Reddit [[00:16:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=990.74s)]
*  or Stack Overflow, and then the model will respond [[00:16:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=992.9s)]
*  in a information dense, but presentable manner. [[00:16:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=995.5s)]
*  The core of that formatting [[00:16:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=998.86s)]
*  is in this instruction tuning phase. [[00:16:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=999.86s)]
*  And then there's two other categories of loss functions [[00:16:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1002.1800000000001s)]
*  that are being used today. [[00:16:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1005.46s)]
*  One I will classify as preference fine tuning. [[00:16:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1007.34s)]
*  Preference fine tuning is a generalized term [[00:16:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1009.94s)]
*  for what came out of reinforcement learning [[00:16:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1012.3000000000001s)]
*  from human feedback, which is RLHF. [[00:16:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1014.3000000000001s)]
*  This reinforcement learning from human feedback [[00:16:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1017.62s)]
*  is credited as the technique [[00:17:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1020.3s)]
*  that helped a chat GPT break through. [[00:17:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1022.34s)]
*  It is a technique to make the responses [[00:17:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1025.98s)]
*  that are nicely formatted, like these Reddit answers, [[00:17:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1028.22s)]
*  more in tune with what a human would like to read. [[00:17:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1031.02s)]
*  This is done by collecting pairwise preferences [[00:17:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1034.38s)]
*  from actual humans out in the world to start, [[00:17:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1036.7s)]
*  and now AIs are also labeling this data, [[00:17:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1039.54s)]
*  and we'll get into those trade-offs. [[00:17:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1041.62s)]
*  And you have this kind of contrastive loss function [[00:17:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1043.74s)]
*  between a good answer and a bad answer. [[00:17:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1046.9s)]
*  And the model learns to pick up these trends. [[00:17:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1048.66s)]
*  There's different implementation ways. [[00:17:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1051.14s)]
*  You have things called reward models. [[00:17:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1053.3400000000001s)]
*  You could have direct alignment algorithms. [[00:17:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1055.1000000000001s)]
*  There's a lot of really specific things you can do, [[00:17:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1056.94s)]
*  but all of this is about fine tuning to human preferences. [[00:17:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1059.3400000000001s)]
*  And the final stage is much newer, [[00:17:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1062.8200000000002s)]
*  and we'll link to what is done in R1, [[00:17:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1065.02s)]
*  and these reasoning models is, I think, [[00:17:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1068.0600000000002s)]
*  OpenAI's name for this. [[00:17:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1071.02s)]
*  They had this new API in the fall, [[00:17:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1072.22s)]
*  which they called the Reinforcement Fine Tuning API. [[00:17:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1073.5s)]
*  This is the idea that you use the techniques [[00:17:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1077.42s)]
*  of reinforcement learning, which is a whole framework of AI. [[00:17:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1079.38s)]
*  There's a deep literature here. [[00:18:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1082.9s)]
*  To summarize, it's often known as trial and error learning [[00:18:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1084.5s)]
*  or the subfield of AI, [[00:18:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1088.1s)]
*  where you're trying to make sequential decisions [[00:18:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1089.74s)]
*  in a certain potentially noisy environment. [[00:18:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1092.34s)]
*  There's a lot of ways we could go down that, [[00:18:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1096.02s)]
*  but fine tuning language models, [[00:18:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1098.26s)]
*  where they can generate an answer, [[00:18:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1100.1s)]
*  and then you check to see if the answer [[00:18:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1102.06s)]
*  matches the true solution. [[00:18:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1104.06s)]
*  For math or code, you have an exactly correct answer. [[00:18:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1105.3799999999999s)]
*  For math, you can have unit tests for code, [[00:18:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1108.98s)]
*  and what we're doing is we are checking [[00:18:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1111.46s)]
*  the language models work, [[00:18:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1113.1399999999999s)]
*  and we're giving it multiple opportunities [[00:18:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1114.46s)]
*  on the same questions to see if it is right. [[00:18:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1116.1399999999999s)]
*  And if you keep doing this, [[00:18:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1118.26s)]
*  the models can learn to improve in verifiable domains [[00:18:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1119.22s)]
*  to a great extent. [[00:18:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1123.6599999999999s)]
*  It works really well. [[00:18:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1124.5s)]
*  It's a newer technique in the academic literature. [[00:18:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1125.46s)]
*  It's been used at Frontier Labs in the US [[00:18:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1127.74s)]
*  that don't share every detail. [[00:18:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1130.3s)]
*  It's been used for multiple years. [[00:18:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1132.6999999999998s)]
*  So this is the idea of using reinforcement learning [[00:18:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1134.54s)]
*  with language models, [[00:18:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1138.02s)]
*  and it has been taking off, [[00:18:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1138.86s)]
*  especially in this DeepSeq moment. [[00:19:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1140.34s)]
*  And we should say that there's a lot of exciting stuff [[00:19:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1142.2199999999998s)]
*  going on on the, again, across the stack, [[00:19:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1144.1799999999998s)]
*  but the post-training, probably this year, [[00:19:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1147.3s)]
*  there's going to be a lot of interesting developments [[00:19:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1149.3799999999999s)]
*  in the post-training. [[00:19:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1151.1s)]
*  We'll talk about it. [[00:19:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1151.9399999999998s)]
*  I almost forgot to talk about the difference [[00:19:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1153.74s)]
*  between DeepSeq V3 and R1 on the user experience side. [[00:19:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1155.9399999999998s)]
*  The technical stuff, forget all of that. [[00:19:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1159.66s)]
*  Just people that don't know anything about AI, [[00:19:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1161.3000000000002s)]
*  they show up. [[00:19:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1163.6200000000001s)]
*  What's the actual experience? [[00:19:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1164.7s)]
*  What's the use case for each one [[00:19:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1165.98s)]
*  when they actually type and talk to it? [[00:19:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1167.74s)]
*  What is each good at and that kind of thing? [[00:19:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1169.9s)]
*  So let's start with DeepSeq V3 again. [[00:19:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1171.78s)]
*  It's what more people would have tried something like it. [[00:19:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1173.8600000000001s)]
*  You ask it a question, [[00:19:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1176.1000000000001s)]
*  it'll start generating tokens very fast, [[00:19:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1177.66s)]
*  and those tokens will look like a very human-legible answer. [[00:19:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1180.14s)]
*  It'll be some sort of markdown list. [[00:19:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1184.18s)]
*  It might have formatting to help you [[00:19:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1186.9s)]
*  draw to the core details in the answer, [[00:19:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1189.1399999999999s)]
*  and it'll generate tens to hundreds of tokens. [[00:19:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1192.34s)]
*  A token is normally a word for common words [[00:19:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1195.34s)]
*  or a subword part in a longer word. [[00:19:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1198.6999999999998s)]
*  And it'll look like a very high quality Reddit [[00:20:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1202.26s)]
*  or Stack Overflow answer. [[00:20:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1205.8999999999999s)]
*  These models are really getting good at doing these [[00:20:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1206.9399999999998s)]
*  across a wide variety of domains. [[00:20:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1210.2199999999998s)]
*  Even things that, if you're an expert, [[00:20:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1212.26s)]
*  things that are close to the fringe of knowledge, [[00:20:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1215.1799999999998s)]
*  they will still be fairly good at. [[00:20:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1217.18s)]
*  Cutting edge AI topics that I do research on, [[00:20:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1219.66s)]
*  these models are capable for study aid [[00:20:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1222.6200000000001s)]
*  and they're regularly updated. [[00:20:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1226.46s)]
*  Where this changes is with the DeepSeq R1, [[00:20:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1228.46s)]
*  what is called these reasoning models, [[00:20:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1231.5800000000002s)]
*  is when you see tokens coming from these models to start, [[00:20:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1233.14s)]
*  it will be a large chain of thought process. [[00:20:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1236.6200000000001s)]
*  We'll get back to chain of thought in a second, [[00:20:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1240.66s)]
*  which looks like a lot of tokens [[00:20:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1242.5800000000002s)]
*  where the model is explaining the problem, [[00:20:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1244.8600000000001s)]
*  the model will often break down the problem [[00:20:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1247.66s)]
*  and be like, okay, they asked me for this, [[00:20:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1248.54s)]
*  let's break down the problem. [[00:20:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1250.5800000000002s)]
*  I'm going to need to do this. [[00:20:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1252.02s)]
*  And you'll see all of this generating from the model, [[00:20:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1253.6200000000001s)]
*  it'll come very fast in most user experiences. [[00:20:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1255.78s)]
*  These APIs are very fast, so you'll see a lot of tokens, [[00:20:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1258.04s)]
*  a lot of words show up really fast. [[00:21:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1260.72s)]
*  It'll keep flowing on the screen [[00:21:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1262.3400000000001s)]
*  and this is all the reasoning process. [[00:21:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1263.94s)]
*  And then eventually the model will change its tone in R1 [[00:21:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1265.94s)]
*  and it'll write the answer, [[00:21:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1269.18s)]
*  where it summarizes its reading reasoning process [[00:21:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1270.26s)]
*  and writes a similar answer to the first types of model. [[00:21:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1273.5s)]
*  But in DeepSeq's case, [[00:21:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1276.54s)]
*  which is part of why this was so popular, [[00:21:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1278.1s)]
*  even outside the AI community, [[00:21:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1281.42s)]
*  is that you can see how the language model [[00:21:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1282.86s)]
*  is breaking down problems. [[00:21:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1284.92s)]
*  And then you get this answer on a technical side, [[00:21:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1286.94s)]
*  they train the model to do this specifically [[00:21:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1289.54s)]
*  where they have a section which is reasoning [[00:21:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1291.62s)]
*  and then it generates a special token, [[00:21:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1293.5s)]
*  which is probably hidden from the user most of the time, [[00:21:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1295.02s)]
*  which says, okay, I'm starting the answer. [[00:21:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1297.06s)]
*  So the model is trained to do this two-stage process [[00:21:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1298.86s)]
*  on its own. [[00:21:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1301.82s)]
*  If you use a similar model in say OpenAI, [[00:21:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1302.8999999999999s)]
*  OpenAI's user interface is trying to summarize this process [[00:21:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1305.18s)]
*  for you nicely by kind of showing the sections [[00:21:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1310.02s)]
*  that the model is doing and it'll kind of click through, [[00:21:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1313.3400000000001s)]
*  it'll say breaking down the problem, [[00:21:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1316.02s)]
*  making X calculation, cleaning the result [[00:21:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1318.0600000000002s)]
*  and then the answer will come for something like OpenAI. [[00:22:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1321.18s)]
*  Maybe it's useful here to go through like an example [[00:22:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1323.8400000000001s)]
*  of a DeepSeq R1 reasoning. [[00:22:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1326.38s)]
*  Yeah, so if you're looking at the screen here, [[00:22:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1329.1000000000001s)]
*  what you'll see is a screenshot of the DeepSeq chat app [[00:22:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1332.1399999999999s)]
*  and at the top is thought for 151, seven seconds [[00:22:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1335.54s)]
*  with the dropdown arrow. [[00:22:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1339.62s)]
*  Underneath that, if we were in an app that we were running, [[00:22:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1340.62s)]
*  the dropdown arrow would have the reasoning. [[00:22:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1343.1399999999999s)]
*  So in this case, the specific question, [[00:22:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1344.8999999999999s)]
*  which I'm philosophically slash pothead inclined, [[00:22:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1347.82s)]
*  so this is asking DeepSeq R1 [[00:22:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1351.52s)]
*  for one truly novel insight about humans [[00:22:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1355.4599999999998s)]
*  and it reveals the reasoning [[00:22:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1359.6999999999998s)]
*  and basically the truly novel aspect [[00:22:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1361.5s)]
*  is what's pushing the reasoning to constantly sort of [[00:22:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1364.74s)]
*  the model asking itself, is this truly novel? [[00:22:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1367.58s)]
*  So it's actually challenging itself to be more novel, [[00:22:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1369.7s)]
*  more counterintuitive, less cringe, I suppose. [[00:22:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1372.68s)]
*  So some of the reasoning says, this is just snapshots. [[00:22:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1377.5s)]
*  Alternatively, humans have a unique meta emotion [[00:23:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1381.46s)]
*  where they feel emotions about their own emotions. [[00:23:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1384.1s)]
*  Did you feeling guilty about being angry? [[00:23:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1386.22s)]
*  This recursive emotional layering [[00:23:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1388.26s)]
*  creates complex motivational drives [[00:23:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1389.64s)]
*  that don't exist in other animals. [[00:23:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1391.3200000000002s)]
*  The insight is that human emotions are nested. [[00:23:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1393.4s)]
*  So it's like, it's reasoning through [[00:23:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1395.64s)]
*  how humans feel emotions. [[00:23:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1397.96s)]
*  It's reasoning about meta emotions. [[00:23:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1399.64s)]
*  It's gonna have pages and pages of this. [[00:23:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1401.24s)]
*  It's almost too much to actually read, [[00:23:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1402.88s)]
*  but it's nice to skim as it's coming. [[00:23:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1404.48s)]
*  It's a stream of, it's a James Joyce-like [[00:23:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1406.5200000000002s)]
*  stream of consciousness. [[00:23:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1408.76s)]
*  And then it goes, wait, the user wants something [[00:23:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1409.72s)]
*  that's not seen anywhere else, let me dig deeper. [[00:23:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1412.64s)]
*  And consider the human ability [[00:23:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1416.22s)]
*  to hold contradictory beliefs simultaneously. [[00:23:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1418.2s)]
*  Cognitive dissonance is known, but perhaps the function [[00:23:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1420.28s)]
*  is to allow flexible adaptation, so on and so forth. [[00:23:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1423.56s)]
*  I mean, that really captures the public imagination [[00:23:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1427.3600000000001s)]
*  that, holy shit, this isn't, I mean, [[00:23:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1430.42s)]
*  intelligence slash almost like an inkling of sentience [[00:23:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1435.42s)]
*  because you're thinking through, [[00:24:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1440.68s)]
*  you're self-reflecting, you're deliberating. [[00:24:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1442.24s)]
*  And the final result of that after 157 seconds is, [[00:24:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1444.52s)]
*  humans instinctively convert selfish desires [[00:24:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1448.48s)]
*  into cooperative systems by collectively pretending [[00:24:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1452.56s)]
*  abstract rules, money, laws, rights are real. [[00:24:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1454.96s)]
*  These shared hallucinations act as, quote, games, [[00:24:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1458.56s)]
*  where competition is secretly redirected [[00:24:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1461.86s)]
*  to benefit the group, turning conflict into society's fuel. [[00:24:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1463.8s)]
*  Pretty profound, I mean, you know. [[00:24:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1468.76s)]
*  This is a commercial digression, but a lot of people [[00:24:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1471.84s)]
*  have found that these reasoning models [[00:24:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1475.48s)]
*  can sometimes produce much more eloquent text. [[00:24:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1477.52s)]
*  That is a, at least, interesting example, I think, [[00:24:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1480.8799999999999s)]
*  depending on how open-minded you are, [[00:24:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1483.44s)]
*  you find language models interesting or not, [[00:24:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1485.04s)]
*  and there's a spectrum there. [[00:24:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1486.76s)]
*  Well, I mean, some of the, we'll talk about [[00:24:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1488.28s)]
*  different benchmarks and so on, but some is just a vibe. [[00:24:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1490.28s)]
*  Like that in itself is a, let's say, quote, fire tweet. [[00:24:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1493.3999999999999s)]
*  Yeah. [[00:24:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1497.96s)]
*  If I'm trying to produce something, [[00:24:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1498.8s)]
*  people are like, oh shit, okay. [[00:25:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1501.56s)]
*  So that's Chanathar, we'll probably return to it more. [[00:25:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1504.08s)]
*  How were they able to achieve such low cost [[00:25:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1507.96s)]
*  on the training and the inference? [[00:25:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1511.96s)]
*  Maybe you could talk the training first. [[00:25:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1513.3999999999999s)]
*  Yeah, so there's two main techniques that they implemented [[00:25:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1515.48s)]
*  that are probably the majority of their efficiency, [[00:25:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1519.4399999999998s)]
*  and then there's a lot of implementation details [[00:25:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1523.08s)]
*  that maybe we'll gloss over or get into later [[00:25:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1525.1599999999999s)]
*  that sort of contribute to it, but those two main things [[00:25:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1527.6999999999998s)]
*  are one is they went to a mixture of experts model, [[00:25:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1530.48s)]
*  which we'll define in a second, [[00:25:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1534.4s)]
*  and then the other thing is that they invented [[00:25:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1535.76s)]
*  this new technique called MLA, latent attention. [[00:25:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1537.56s)]
*  Both of these are big deals. [[00:25:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1540.56s)]
*  Mixture of experts is something that's been in the literature [[00:25:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1542.08s)]
*  for a handful of years, and OpenAI with GPT-4 [[00:25:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1544.44s)]
*  was the first one to productize a mixture of experts model, [[00:25:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1547.9s)]
*  and what this means is when you look at the common models [[00:25:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1551.96s)]
*  around that most people have been able to interact with [[00:25:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1554.92s)]
*  that are open, think llama. [[00:25:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1558.28s)]
*  Llama is a dense model, i.e. every single parameter [[00:26:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1560.8799999999999s)]
*  or neuron is activated as you're going through the model [[00:26:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1564.8799999999999s)]
*  for every single token you generate. [[00:26:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1568.16s)]
*  Now with a mixture of experts model, you don't do that. [[00:26:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1570.84s)]
*  How does the human actually work? [[00:26:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1574.08s)]
*  Oh, well, my visual cortex is active [[00:26:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1576.24s)]
*  when I'm thinking about vision tasks [[00:26:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1578.08s)]
*  or other things, my amygdala is when I'm scared. [[00:26:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1580.12s)]
*  These different aspects of your brain [[00:26:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1584.04s)]
*  are focused on different things. [[00:26:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1585.52s)]
*  A mixture of experts model attempts to approximate this [[00:26:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1586.96s)]
*  to some extent. [[00:26:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1589.6000000000001s)]
*  It's nowhere close to what a brain architecture is, [[00:26:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1590.44s)]
*  but different portions of the model activate. [[00:26:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1592.7s)]
*  You'll have a set number of experts in the model [[00:26:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1595.64s)]
*  and a set number that are activated each time, [[00:26:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1598.76s)]
*  and this dramatically reduces both your training [[00:26:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1600.8s)]
*  and inference costs, because now you're, [[00:26:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1603.1200000000001s)]
*  if you think about the parameter count [[00:26:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1606.28s)]
*  as the sort of total embedding space [[00:26:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1608.1200000000001s)]
*  for all of this knowledge that you're compressing down [[00:26:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1610.06s)]
*  during training, when you're embedding this data in [[00:26:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1612.48s)]
*  instead of having to activate every single parameter [[00:26:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1616.4s)]
*  every single time you're training or running inference, [[00:26:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1619.16s)]
*  now you can just activate a subset, [[00:27:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1621.3600000000001s)]
*  and the model will learn which expert to route to [[00:27:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1623.42s)]
*  for different tasks. [[00:27:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1626.22s)]
*  And so this is a humongous innovation in terms of, [[00:27:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1627.5400000000002s)]
*  hey, I can continue to grow the total embedding space [[00:27:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1630.88s)]
*  of parameters. [[00:27:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1633.3600000000001s)]
*  And so DeepSeq's model is 600 something billion parameters, [[00:27:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1634.24s)]
*  relative to Llama 405b, it's 405 billion parameters, [[00:27:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1637.3600000000001s)]
*  Llama relative to Llama 70b, it's 70 billion parameters. [[00:27:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1641.4s)]
*  So this model technically has more embedding space [[00:27:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1644.44s)]
*  for information, right? [[00:27:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1646.8400000000001s)]
*  To compress all of the world's knowledge [[00:27:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1648.28s)]
*  that's on the internet down, but at the same time, [[00:27:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1649.9s)]
*  it is only activating around 37 billion of the parameters. [[00:27:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1652.68s)]
*  So only 37 billion of these parameters [[00:27:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1656.76s)]
*  actually need to be computed every single time [[00:27:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1658.72s)]
*  you're training data or inferencing data out of it. [[00:27:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1660.96s)]
*  And so versus, versus again, the Llama model, [[00:27:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1664.04s)]
*  70 billion parameters must be activated, [[00:27:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1666.5s)]
*  or 405 billion parameters must be activated. [[00:27:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1668.3s)]
*  So you've dramatically reduced your compute cost [[00:27:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1670.52s)]
*  when you're doing training and inference [[00:27:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1672.76s)]
*  with this mixture of experts architecture. [[00:27:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1674.84s)]
*  So we break down where it actually applies [[00:27:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1677.44s)]
*  and go into the transformer, is that useful? [[00:27:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1679.56s)]
*  Let's go, let's go into the transformer. [[00:28:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1681.96s)]
*  So the transformer is a thing that is talked about a lot [[00:28:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1683.84s)]
*  and we will not cover every detail. [[00:28:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1686.52s)]
*  Essentially, the transformer is built on repeated blocks [[00:28:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1689.4s)]
*  of this attention mechanism, and then a traditional dense, [[00:28:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1692.96s)]
*  fully connected multilayer perception, [[00:28:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1696.86s)]
*  whatever word you want to use [[00:28:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1699.28s)]
*  for your normal neural network. [[00:28:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1700.48s)]
*  And you alternate these blocks, there's other details. [[00:28:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1701.8799999999999s)]
*  And where mixture of experts is applied [[00:28:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1704.9599999999998s)]
*  is at this dense model. [[00:28:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1706.56s)]
*  The dense model holds most of the weights [[00:28:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1708.32s)]
*  if you count them in a transformer model. [[00:28:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1711.24s)]
*  So you can get really big gains [[00:28:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1713.9599999999998s)]
*  from those mixture of experts on parameter efficiency [[00:28:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1715.6s)]
*  at training and inference, because you get this efficiency [[00:28:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1718.4399999999998s)]
*  by not activating all of these parameters. [[00:28:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1722.08s)]
*  We should also say that a transformer [[00:28:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1724.1999999999998s)]
*  is a giant neural network. [[00:28:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1726.8s)]
*  Yeah. [[00:28:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1728.78s)]
*  For 15 years now, [[00:28:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1729.86s)]
*  this was called the deep learning revolution. [[00:28:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1732.06s)]
*  Networks gotten larger and larger and at a certain point, [[00:28:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1734.54s)]
*  the scaling laws appeared where people realized. [[00:28:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1737.3799999999999s)]
*  This is a scaling law shirt by the way. [[00:29:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1740.6599999999999s)]
*  Representing scaling laws where it became more [[00:29:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1742.8999999999999s)]
*  and more formalized that bigger is better [[00:29:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1746.6999999999998s)]
*  across multiple dimensions of what bigger means. [[00:29:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1749.7399999999998s)]
*  So, but these are all sort of neural networks [[00:29:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1752.06s)]
*  we're talking about, and we're talking about [[00:29:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1755.26s)]
*  different architectures of how construct [[00:29:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1757.18s)]
*  to construct these neural networks such that the training [[00:29:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1759.18s)]
*  and the inference on them is super efficient. [[00:29:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1762.7s)]
*  Yeah, every different type of model [[00:29:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1764.38s)]
*  has a different scaling law for it, [[00:29:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1765.78s)]
*  which is effectively for how much compute you put in, [[00:29:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1768.02s)]
*  the architecture will get to different levels [[00:29:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1770.92s)]
*  of performance at test tasks. [[00:29:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1774.52s)]
*  And mixture of experts is one of the ones at training time, [[00:29:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1776.44s)]
*  even if you don't consider the inference benefits, [[00:29:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1779.3400000000001s)]
*  which are also big. [[00:29:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1781.3600000000001s)]
*  At training time, your efficiency with your GPUs [[00:29:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1782.54s)]
*  is dramatically improved by using this architecture [[00:29:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1785.02s)]
*  if it is well implemented. [[00:29:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1787.42s)]
*  So you can get effectively the same performance model [[00:29:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1788.94s)]
*  and evaluation scores with numbers like 30% less compute. [[00:29:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1793.2s)]
*  I think there's gonna be a wide variation [[00:29:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1797.54s)]
*  depending on your implementation details and stuff. [[00:29:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1799.2s)]
*  But it is just important to realize that this type [[00:30:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1801.6s)]
*  of technical innovation is something that gives huge gains. [[00:30:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1804.46s)]
*  And I expect most companies that are serving their models [[00:30:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1808.06s)]
*  to move to this mixture of experts implementation, [[00:30:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1811.3799999999999s)]
*  historically the reason why not everyone might do it [[00:30:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1814.42s)]
*  is because it's an implementation complexity, [[00:30:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1816.8600000000001s)]
*  especially when doing these big models. [[00:30:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1819.1000000000001s)]
*  So this is one of the things that DeepSeq gets credit for [[00:30:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1821.18s)]
*  is they do this extremely well. [[00:30:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1824.18s)]
*  They do mixture of experts extremely well. [[00:30:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1826.14s)]
*  This architecture for what is called DeepSeq MOE, [[00:30:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1827.8400000000001s)]
*  MOE is the shortened version of mixture of experts, [[00:30:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1831.5s)]
*  is multiple papers old. [[00:30:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1834.42s)]
*  This part of their training infrastructure [[00:30:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1835.8600000000001s)]
*  is not new to these models alone. [[00:30:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1838.0800000000002s)]
*  And same goes for what Dylan mentioned [[00:30:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1840.94s)]
*  with multi-head latent attention. [[00:30:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1843.06s)]
*  This is all about reducing memory usage during inference [[00:30:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1845.3799999999999s)]
*  and same things during training [[00:30:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1848.32s)]
*  by using some fancy low rank approximation math. [[00:30:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1850.22s)]
*  If you get into the details with this latent attention, [[00:30:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1853.7s)]
*  it's one of those things I look at and say, okay, [[00:30:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1856.8s)]
*  they're doing really complex implementations [[00:30:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1859.06s)]
*  because there's other parts of language models [[00:31:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1861.6599999999999s)]
*  such as embeddings that are used [[00:31:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1863.1599999999999s)]
*  to extend the context length. [[00:31:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1865.8999999999999s)]
*  The common one that DeepSeq uses rotary positional embeddings [[00:31:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1867.3s)]
*  which is called a rope. [[00:31:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1871.5s)]
*  And if you wanna use rope with a normal MOE, [[00:31:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1872.78s)]
*  it's kind of a sequential thing. [[00:31:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1875.3799999999999s)]
*  You take these, you take two of the attention matrices [[00:31:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1876.54s)]
*  and you rotate them by a complex value rotation [[00:31:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1879.3999999999999s)]
*  which is a matrix multiplication. [[00:31:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1882.62s)]
*  With DeepSeq's MLA with this new attention architecture, [[00:31:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1884.46s)]
*  they need to do some clever things [[00:31:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1887.84s)]
*  because they're not set up the same [[00:31:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1889.86s)]
*  and it just makes the implementation complexity much higher. [[00:31:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1891.2s)]
*  So they're managing all of these things. [[00:31:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1894.3s)]
*  And these are probably the sort of things that OpenAI, [[00:31:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1895.86s)]
*  these closed labs are doing. [[00:31:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1899.34s)]
*  We don't know if they're doing the exact same techniques [[00:31:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1900.42s)]
*  but they actually shared them with the world [[00:31:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1902.3s)]
*  which is really nice to feel like this is the cutting edge [[00:31:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1904.1s)]
*  of efficient language model training. [[00:31:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1906.62s)]
*  And some of this requires low level engineering. [[00:31:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1909.22s)]
*  Just is a giant mess and trickery. [[00:31:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1912.34s)]
*  So as I understand that one below CUDA, [[00:31:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1915.3799999999999s)]
*  so they go super low programming of GPUs. [[00:31:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1917.86s)]
*  Effectively, NVIDIA builds this library called Nickel, right? [[00:32:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1920.98s)]
*  In which, when you're training a model, [[00:32:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1925.06s)]
*  you have all these communications [[00:32:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1927.26s)]
*  between every single layer of the model [[00:32:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1928.82s)]
*  and you may have over a hundred layers. [[00:32:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1930.94s)]
*  What does Nickel stand for? [[00:32:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1932.54s)]
*  It's NCCL. [[00:32:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1933.5800000000002s)]
*  NVIDIA communications collectives library. [[00:32:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1934.7s)]
*  Nice. [[00:32:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1936.8200000000002s)]
*  And so when you're training a model, [[00:32:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1937.66s)]
*  you're gonna have all these all reduces and all gathers. [[00:32:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1943.02s)]
*  Between each layer, between the multi-layer perceptron [[00:32:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1946.14s)]
*  or feed forward network and the attention mechanism, [[00:32:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1949.66s)]
*  you'll have basically the model synchronized. [[00:32:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1951.78s)]
*  You'll have all reduce or an all gather. [[00:32:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1956.3400000000001s)]
*  And this is a communication between all the GPUs [[00:32:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1958.46s)]
*  in the network, whether it's in training or inference. [[00:32:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1961.78s)]
*  So NVIDIA has a standard library. [[00:32:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1963.1399999999999s)]
*  This is one of the reasons why it's really difficult [[00:32:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1964.6999999999998s)]
*  to use anyone else's hardware for training [[00:32:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1967.26s)]
*  is because no one's really built [[00:32:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1969.62s)]
*  a standard communications library. [[00:32:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1971.4199999999998s)]
*  And NVIDIA has done this at a sort of a higher level, right? [[00:32:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1973.54s)]
*  A deep seek because they have certain limitations [[00:32:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1976.26s)]
*  around the GPUs that they have access to. [[00:32:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1978.74s)]
*  The interconnects are limited to some extent [[00:33:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1980.6999999999998s)]
*  by the restrictions of the GPUs [[00:33:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1983.8999999999999s)]
*  that were shipped into China legally, [[00:33:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1985.82s)]
*  not the ones that are smuggled, but legally shipped in [[00:33:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1987.1399999999999s)]
*  that they used to train this model. [[00:33:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1989.74s)]
*  They had to figure out how to get efficiencies, right? [[00:33:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1991.42s)]
*  And one of those things is that instead of just calling [[00:33:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1995.3s)]
*  the NVIDIA library, nickel, right? [[00:33:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=1998.18s)]
*  They instead created their, [[00:33:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2000.6200000000001s)]
*  they scheduled their own communications, [[00:33:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2002.22s)]
*  which some of the labs do, right? [[00:33:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2004.6200000000001s)]
*  E-meta talked about in Lama 3 [[00:33:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2007.94s)]
*  how they made their own custom version of nickel. [[00:33:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2009.18s)]
*  This is, they didn't talk about the implementation details. [[00:33:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2011.22s)]
*  This is some of what they did. [[00:33:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2013.82s)]
*  Probably not as well as, maybe not as well as deep seek [[00:33:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2015.06s)]
*  because deep seek, you know, [[00:33:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2017.82s)]
*  necessity is the mother of innovation [[00:33:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2018.9399999999998s)]
*  and they had to do this. [[00:33:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2020.78s)]
*  Whereas in the case, you know, [[00:33:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2022.62s)]
*  open AI has people that do this sort of stuff, [[00:33:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2024.58s)]
*  Anthropic, et cetera, but you know, [[00:33:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2026.74s)]
*  deep seek certainly did it publicly [[00:33:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2029.02s)]
*  and they may have done it even better [[00:33:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2030.5s)]
*  because they were gimped on a certain aspect [[00:33:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2031.6599999999999s)]
*  of the chips that they have access to. [[00:33:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2034.1399999999999s)]
*  And so they scheduled communications, you know, [[00:33:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2036.1799999999998s)]
*  by scheduling specific SMs. [[00:34:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2040.8999999999999s)]
*  SMs you could think of as like the core on a GPU, right? [[00:34:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2042.6599999999999s)]
*  So there's hundreds of cores or there's, you know, [[00:34:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2046.34s)]
*  a bit over a hundred cores, SMs on a GPU [[00:34:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2049.06s)]
*  and they were specifically scheduling, [[00:34:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2051.54s)]
*  hey, which ones are running the model? [[00:34:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2053.14s)]
*  Which ones are doing all reduce? [[00:34:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2054.58s)]
*  Which one are doing all gather, right? [[00:34:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2055.9s)]
*  And they would flip back and forth between them [[00:34:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2057.46s)]
*  and this requires extremely low level programming. [[00:34:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2059.18s)]
*  This is what nickel does automatically [[00:34:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2062.18s)]
*  or other Nvidia libraries handle this automatically usually. [[00:34:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2063.74s)]
*  Yeah, exactly. [[00:34:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2066.62s)]
*  And so technically they're using, you know, [[00:34:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2067.46s)]
*  PTX, which is like sort of like, [[00:34:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2069.62s)]
*  you could think of it as like an assembly type language. [[00:34:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2071.5s)]
*  It's not exactly that or instruction set, right? [[00:34:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2073.94s)]
*  Coding directly to assembly or instruction set. [[00:34:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2076.5s)]
*  It's not exactly that, [[00:34:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2078.38s)]
*  but that's still part of technically CUDA, [[00:34:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2079.38s)]
*  but it's like, do I want to write in Python, you know, [[00:34:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2082.06s)]
*  PyTorch equivalent and call Nvidia libraries? [[00:34:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2084.38s)]
*  Do I want to go down to the C level, right? [[00:34:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2087.06s)]
*  Or, you know, in code even lower level, [[00:34:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2089.06s)]
*  or do I want to go all the way down [[00:34:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2090.7000000000003s)]
*  to the assembly or ISO level? [[00:34:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2091.62s)]
*  And there are cases where you go all the way down there [[00:34:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2093.2200000000003s)]
*  at the very big labs, [[00:34:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2095.94s)]
*  but most companies just do not do that, right? [[00:34:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2097.14s)]
*  Because it's a waste of time [[00:34:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2099.7000000000003s)]
*  and the efficiency gains you get are not worth it. [[00:35:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2101.1s)]
*  But DeepSeq's implementation is so complex, right? [[00:35:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2103.74s)]
*  Especially with their mixture of experts, right? [[00:35:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2106.9399999999996s)]
*  People have done mixture of experts, [[00:35:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2109.4599999999996s)]
*  but they're generally eight, 16 experts, right? [[00:35:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2110.74s)]
*  And they activate two. [[00:35:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2113.4199999999996s)]
*  So, you know, one of the words that we like to use [[00:35:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2114.3399999999997s)]
*  is like sparsity factor, right? [[00:35:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2117.1s)]
*  Or usage, right? [[00:35:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2118.74s)]
*  So you might have four, you know, [[00:35:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2119.8599999999997s)]
*  one fourth of your model activate, right? [[00:35:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2121.54s)]
*  And that's what Mistral's, Mistral model, right? [[00:35:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2123.8599999999997s)]
*  They're a model that really catapulted them to like, [[00:35:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2127.62s)]
*  oh my God, they're really, really good. [[00:35:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2129.8999999999996s)]
*  OpenAI has also had models that are MOE [[00:35:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2132.26s)]
*  and so have all the other labs that are major closed. [[00:35:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2134.38s)]
*  But what DeepSeq did that maybe only the leading labs [[00:35:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2137.82s)]
*  have only just started recently doing [[00:35:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2140.82s)]
*  is have such a high sparsity factor, right? [[00:35:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2142.5400000000004s)]
*  It's not one fourth of the model, right? [[00:35:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2144.3s)]
*  Two out of eight experts activating [[00:35:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2146.0600000000004s)]
*  every time you go through the model, [[00:35:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2148.0200000000004s)]
*  it's eight out of 256. [[00:35:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2149.42s)]
*  And there's different implementations [[00:35:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2151.46s)]
*  for mixture of experts where you can have [[00:35:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2153.0600000000004s)]
*  some of these experts that are always activated, [[00:35:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2155.6400000000003s)]
*  which this just looks like a small neural network [[00:35:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2158.7400000000002s)]
*  and then all the tokens go through that. [[00:36:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2162.14s)]
*  And then they also go through some that are selected [[00:36:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2163.7999999999997s)]
*  by this routing mechanism. [[00:36:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2167.2599999999998s)]
*  And one of the innovations in DeepSeq's architecture [[00:36:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2168.8199999999997s)]
*  is that they changed the routing mechanism [[00:36:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2173.04s)]
*  in mixture of expert models. [[00:36:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2175.14s)]
*  There's something called an auxiliary loss, [[00:36:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2176.58s)]
*  which effectively means during training, [[00:36:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2179.22s)]
*  you want to make sure that all of these experts [[00:36:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2181.5s)]
*  are used across the tasks that the model sees. [[00:36:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2183.3399999999997s)]
*  Why there can be failures in mixture of experts [[00:36:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2186.66s)]
*  is that when you're doing this training, [[00:36:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2188.94s)]
*  the one objective is token prediction accuracy. [[00:36:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2192.34s)]
*  And if you just let training go [[00:36:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2195.62s)]
*  with a mixture of expert model on your own, [[00:36:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2197.34s)]
*  it can be that the model learns [[00:36:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2199.46s)]
*  to only use a subset of the experts. [[00:36:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2202.1s)]
*  And in the MOE literature, [[00:36:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2204.8s)]
*  there's something called the auxiliary loss, [[00:36:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2206.7400000000002s)]
*  which helps balance them. [[00:36:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2208.66s)]
*  But if you think about the loss functions of deep learning, [[00:36:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2210.36s)]
*  this even connects to the bitter lesson [[00:36:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2214.34s)]
*  is that you want to have the minimum inductive bias [[00:36:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2215.9s)]
*  in your model to let the model learn maximally. [[00:36:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2219.58s)]
*  And this auxiliary loss, this balancing across experts [[00:37:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2222.06s)]
*  could be seen as intention [[00:37:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2225.1s)]
*  with the prediction accuracy of the tokens. [[00:37:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2227.12s)]
*  So we don't know the exact extent [[00:37:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2229.7000000000003s)]
*  that the DeepSeq MOE change, [[00:37:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2231.2200000000003s)]
*  which is instead of doing an auxiliary loss, [[00:37:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2233.2200000000003s)]
*  they have an extra parameter in their routing, [[00:37:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2235.54s)]
*  which after the batches, they update this parameter [[00:37:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2237.86s)]
*  to make sure that the next batches [[00:37:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2240.86s)]
*  all have a similar use of experts. [[00:37:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2242.46s)]
*  And this type of change can be big, [[00:37:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2244.54s)]
*  it can be small, but they add up over time. [[00:37:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2246.34s)]
*  And this is the sort of thing [[00:37:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2248.78s)]
*  that just points to them innovating. [[00:37:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2249.62s)]
*  And I'm sure all the labs that are training big MOEs [[00:37:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2251.22s)]
*  are looking at this sort of things, [[00:37:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2253.86s)]
*  which is getting away from the auxiliary loss. [[00:37:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2255.14s)]
*  Some of them might already use it, [[00:37:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2257.24s)]
*  but you keep accumulating gains. [[00:37:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2258.74s)]
*  And we'll talk about the philosophy of training [[00:37:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2260.54s)]
*  and how you organize these organizations. [[00:37:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2263.24s)]
*  And a lot of it is just compounding small improvements [[00:37:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2265.78s)]
*  over time in your data, in your architecture, [[00:37:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2268.34s)]
*  in your post-training and how they integrate with each other. [[00:37:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2270.58s)]
*  DeepSeq does the same thing and some of them are shared. [[00:37:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2274.08s)]
*  We have to take them on face value [[00:37:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2277.56s)]
*  that they share their most important details. [[00:37:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2278.7599999999998s)]
*  I mean, the architecture and the weights are out there, [[00:38:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2280.84s)]
*  so we're seeing what they're doing and it adds up. [[00:38:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2282.38s)]
*  Going back to sort of the like efficiency [[00:38:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2285.32s)]
*  and complexity point, right? [[00:38:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2287.2799999999997s)]
*  It's 32 versus four, right? [[00:38:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2288.44s)]
*  For like MixedDraw and other MOE models [[00:38:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2291.32s)]
*  that have been publicly released. [[00:38:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2293.04s)]
*  So this ratio is extremely high [[00:38:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2294.48s)]
*  and sort of what Nathan was getting at there was, [[00:38:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2296.16s)]
*  when you have such a different level of sparsity, [[00:38:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2298.72s)]
*  you can't just have every GPU have the entire model, right? [[00:38:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2302.0s)]
*  The model's too big, there's too much complexity there. [[00:38:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2305.52s)]
*  So you have to split up the model [[00:38:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2307.8s)]
*  with different types of parallelism, right? [[00:38:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2309.56s)]
*  And so you might have different experts [[00:38:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2311.4s)]
*  on different GPU nodes. [[00:38:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2312.96s)]
*  But now what happens when this set of data that you get, [[00:38:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2314.56s)]
*  hey, all of it looks like this one way [[00:38:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2318.52s)]
*  and all of it should route to one part of my model, right? [[00:38:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2320.32s)]
*  So when all of it routes to one part of the model, [[00:38:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2324.6s)]
*  then you can have this overloading [[00:38:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2327.38s)]
*  of a certain set of the GPU resources [[00:38:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2331.18s)]
*  or a certain set of the GPUs. [[00:38:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2333.66s)]
*  And then the rest of the training network sits idle [[00:38:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2335.2599999999998s)]
*  because all of the tokens are just routing to that. [[00:38:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2338.7999999999997s)]
*  So this is the biggest complexity. [[00:39:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2340.58s)]
*  One of the big complexities with running a very sparse [[00:39:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2341.7799999999997s)]
*  mixture of experts model, [[00:39:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2345.4199999999996s)]
*  IE this 32 ratio versus this four ratio [[00:39:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2347.02s)]
*  is that you end up with so many of the experts [[00:39:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2350.8799999999997s)]
*  just sitting there idle. [[00:39:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2353.18s)]
*  So how do I load balance between them? [[00:39:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2354.2599999999998s)]
*  How do I schedule the communications between them? [[00:39:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2356.2999999999997s)]
*  This is a lot of the extremely low level detailed work [[00:39:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2358.34s)]
*  that they figured out in the public first [[00:39:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2361.82s)]
*  and potentially second or third in the world [[00:39:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2365.1400000000003s)]
*  and maybe even first in some cases. [[00:39:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2367.94s)]
*  What lesson do you, [[00:39:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2369.7200000000003s)]
*  in the direction of the better lesson, [[00:39:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2372.54s)]
*  do you take from all of this? [[00:39:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2374.02s)]
*  Where is this going to be the direction [[00:39:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2375.06s)]
*  where a lot of the gain is going to be, [[00:39:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2377.54s)]
*  which is this low level optimization? [[00:39:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2379.1800000000003s)]
*  Or is this a short term thing [[00:39:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2381.7400000000002s)]
*  where the biggest gains will be more [[00:39:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2384.58s)]
*  on the algorithmic high level side of like post-training. [[00:39:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2386.62s)]
*  Is this like a short term leap [[00:39:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2390.5s)]
*  because they've figured out like a hack [[00:39:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2392.64s)]
*  because constraints, necessities, the mother of invention, [[00:39:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2394.94s)]
*  or is there still a lot of gains? [[00:39:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2399.02s)]
*  I think we should summarize [[00:40:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2401.06s)]
*  what the better lesson actually is about. [[00:40:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2402.06s)]
*  The better lesson essentially, if you paraphrase it, [[00:40:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2404.2599999999998s)]
*  is that the types of training that will win out [[00:40:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2407.68s)]
*  in deep learning as we go are those methods [[00:40:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2411.2799999999997s)]
*  that are which are scalable in learning [[00:40:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2414.86s)]
*  and search is what it calls out. [[00:40:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2416.86s)]
*  And this scale word gets a lot of attention in this. [[00:40:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2418.78s)]
*  The interpretation that I use is effectively [[00:40:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2424.78s)]
*  to avoid adding in the human priors [[00:40:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2427.6200000000003s)]
*  to your learning process. [[00:40:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2432.54s)]
*  And if you read the original essay, [[00:40:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2433.6600000000003s)]
*  this is what it talks about, [[00:40:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2435.06s)]
*  is how researchers will try to come up [[00:40:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2436.1800000000003s)]
*  with clever solutions to their specific problem [[00:40:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2439.2200000000003s)]
*  that might get them small gains in the short term [[00:40:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2443.22s)]
*  while simply enabling these deep learning systems [[00:40:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2446.4199999999996s)]
*  to work efficiently. [[00:40:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2448.9399999999996s)]
*  And for these bigger problems in the long term [[00:40:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2450.52s)]
*  might be more likely to scale and continue to drive success. [[00:40:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2453.06s)]
*  And therefore we were talking [[00:40:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2458.4599999999996s)]
*  about relatively small implementation changes [[00:40:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2459.98s)]
*  to the mixture of experts model. [[00:41:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2462.54s)]
*  And therefore it's like, okay, [[00:41:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2465.1s)]
*  like we will need a few more years to know [[00:41:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2466.4199999999996s)]
*  if one of these are actually really crucial [[00:41:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2469.18s)]
*  to the better lesson. [[00:41:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2470.98s)]
*  But the better lesson is really this long term arc [[00:41:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2472.38s)]
*  of how simplicity can often win. [[00:41:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2474.9s)]
*  And there's a lot of sayings in the industry, [[00:41:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2477.3s)]
*  like the models just wanna learn. [[00:41:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2479.1s)]
*  You have to give them the simple loss landscape [[00:41:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2480.5s)]
*  where you put compute through the model [[00:41:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2483.62s)]
*  and they will learn and getting barriers out of the way. [[00:41:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2485.7400000000002s)]
*  That's where the power, something like Nickel comes in [[00:41:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2489.7000000000003s)]
*  where standardized code that can be used by a lot of people [[00:41:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2491.88s)]
*  to create sort of simple innovations that can scale, [[00:41:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2496.1400000000003s)]
*  which is why the hacks, [[00:41:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2499.42s)]
*  I imagine that the code base for DeepSeq [[00:41:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2501.46s)]
*  is probably a giant mess. [[00:41:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2503.98s)]
*  I'm sure they have, DeepSeq definitely has code bases [[00:41:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2505.1800000000003s)]
*  that are extremely messy [[00:41:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2508.02s)]
*  where they're testing these new ideas. [[00:41:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2509.1000000000004s)]
*  Multi-headly and attention probably start, [[00:41:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2510.7400000000002s)]
*  could start in something like a Jupiter notebook [[00:41:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2513.38s)]
*  or somebody tries something on a few GPUs [[00:41:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2515.1800000000003s)]
*  and that is really messy. [[00:41:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2518.02s)]
*  But the stuff that trains the DeepSeq v3 and DeepSeq r1, [[00:42:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2520.04s)]
*  those libraries, if you were to present them to us, [[00:42:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2523.5600000000004s)]
*  I would guess are extremely high quality code. [[00:42:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2526.7000000000003s)]
*  High quality, readable code. [[00:42:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2530.3799999999997s)]
*  I think there is one aspect to note though, [[00:42:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2532.66s)]
*  is that there is the general ability for that [[00:42:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2534.7799999999997s)]
*  to transfer across different types of runs. [[00:42:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2539.02s)]
*  You may make really, really high quality code [[00:42:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2541.58s)]
*  for one specific model architecture at one size. [[00:42:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2543.98s)]
*  And then that is not transferable to, [[00:42:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2547.8999999999996s)]
*  hey, when I make this architecture tweak, [[00:42:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2550.06s)]
*  everything's broken again. [[00:42:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2552.2999999999997s)]
*  That's something that could be, [[00:42:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2553.62s)]
*  with their specific low level coding of like scheduling SMs, [[00:42:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2556.8599999999997s)]
*  is specific to this model architecture and size. [[00:42:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2561.18s)]
*  And whereas like Nvidia's collectives library [[00:42:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2563.54s)]
*  is more like, hey, it'll work for anything. [[00:42:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2566.22s)]
*  You wanna do an all reduce, great. [[00:42:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2568.94s)]
*  I don't care what your model architecture is, it'll work. [[00:42:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2570.3399999999997s)]
*  And you're giving up a lot of performance [[00:42:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2572.8599999999997s)]
*  when you do that in many cases, [[00:42:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2574.2999999999997s)]
*  but it's worthwhile for them to do the specific optimization [[00:42:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2576.2999999999997s)]
*  for the specific run, [[00:43:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2580.7999999999997s)]
*  given the constraints that they have regarding compute. [[00:43:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2581.8199999999997s)]
*  I wonder how stressful it is to like, [[00:43:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2584.2999999999997s)]
*  these frontier models, like initiate training, [[00:43:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2587.86s)]
*  like to have the code. [[00:43:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2590.82s)]
*  But to push the button, [[00:43:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2592.34s)]
*  that like you're now spending a large amount of money [[00:43:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2594.42s)]
*  and time to train this. [[00:43:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2598.5s)]
*  There must be a lot of innovation on the debugging stage [[00:43:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2601.54s)]
*  of like making sure there's no issues [[00:43:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2605.38s)]
*  that you're monitoring and visualizing [[00:43:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2607.94s)]
*  every aspect of the training, all that kind of stuff. [[00:43:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2610.02s)]
*  When people are training, [[00:43:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2613.02s)]
*  they have all these various dashboards, [[00:43:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2613.98s)]
*  but like the most simple one is your loss, right? [[00:43:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2615.34s)]
*  And it continues to go down, but in reality, [[00:43:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2618.6600000000003s)]
*  especially with more complicated stuff like MOE, [[00:43:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2621.38s)]
*  the biggest problem with it or FP8 training, [[00:43:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2624.06s)]
*  which is another innovation, [[00:43:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2626.1400000000003s)]
*  going to a lower precision number format, [[00:43:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2627.42s)]
*  i.e. less accurate, is that you end up with loss spikes. [[00:43:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2629.02s)]
*  And no one knows why the loss spike happen. [[00:43:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2632.34s)]
*  And for a long- Some of them you do. [[00:43:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2634.58s)]
*  Some of them you do. Some of them are bad data. [[00:43:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2636.02s)]
*  I give AI2's example of what blew up our earlier models [[00:43:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2637.82s)]
*  is a subreddit called Microwave Gang. [[00:44:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2641.1000000000004s)]
*  We love to shout this out. [[00:44:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2643.26s)]
*  It's a real thing. [[00:44:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2644.82s)]
*  You can pull up Microwave Gang. [[00:44:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2645.6600000000003s)]
*  Essentially, it's a subreddit where everybody makes posts [[00:44:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2647.1000000000004s)]
*  that are just the letter M, so it's like, mm. [[00:44:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2649.78s)]
*  So there's extremely long sequences of the letter M, [[00:44:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2652.46s)]
*  and then the comments are like beep beep, [[00:44:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2655.42s)]
*  because that's when the micro events. [[00:44:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2656.78s)]
*  But if you pass this into a model [[00:44:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2658.1000000000004s)]
*  that's trained to be a normal producing text, [[00:44:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2659.46s)]
*  it's extremely high loss, because normally you see an M, [[00:44:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2661.5s)]
*  you don't predict M's for a long time. [[00:44:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2664.7400000000002s)]
*  So like this is something that causes the loss spikes for us. [[00:44:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2667.1800000000003s)]
*  But when you have much like, this is old, [[00:44:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2670.02s)]
*  this is not recent. [[00:44:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2671.98s)]
*  And when you have more mature data systems, [[00:44:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2673.22s)]
*  that's not the thing that causes the loss spike. [[00:44:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2675.2999999999997s)]
*  And what Dylan is saying is true, [[00:44:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2677.1s)]
*  but it's like, it's levels to this sort of idea. [[00:44:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2678.3799999999997s)]
*  With regards to the stress, right? [[00:44:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2681.8199999999997s)]
*  These people are like, you know, [[00:44:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2684.06s)]
*  you'll go out to dinner with like a friend [[00:44:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2685.3799999999997s)]
*  that works at one of these labs, [[00:44:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2686.98s)]
*  and they'll just be like looking at their phone [[00:44:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2688.3799999999997s)]
*  every like 10 minutes, and they're not like, [[00:44:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2690.8999999999996s)]
*  you know, it's one thing if they're texting, [[00:44:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2692.8999999999996s)]
*  but they're just like, is the loss, is the loss provoking? [[00:44:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2694.22s)]
*  Yeah, it's like tokens per second, loss not blown up. [[00:44:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2696.8199999999997s)]
*  They're just walking, watching this. [[00:45:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2701.1s)]
*  And the heart rate goes up if there's a spike. [[00:45:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2703.78s)]
*  And some level of spikes is normal, right? [[00:45:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2705.5s)]
*  It'll recover and be back. [[00:45:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2707.5s)]
*  Sometimes a lot of the old strategy was like, [[00:45:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2708.78s)]
*  you just stop the run, restart from the old version, [[00:45:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2710.86s)]
*  and then like change the data mix, [[00:45:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2713.46s)]
*  and then it keeps going. [[00:45:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2715.18s)]
*  There are even different types of spikes. [[00:45:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2716.1s)]
*  So Dirk Grenneveld has a theory that I do, [[00:45:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2717.86s)]
*  that's like fast spikes and slow spikes, [[00:45:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2720.98s)]
*  where there are sometimes where you're looking at the loss [[00:45:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2722.78s)]
*  and there are other parameters, [[00:45:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2725.1s)]
*  you can see it start to creep up and then blow up, [[00:45:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2725.94s)]
*  and that's really hard to recover from. [[00:45:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2728.7000000000003s)]
*  So you have to go back much further. [[00:45:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2730.14s)]
*  So you have the stressful period [[00:45:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2731.46s)]
*  where it's like flat or might start going up, [[00:45:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2732.66s)]
*  and you're like, what do I do? [[00:45:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2734.46s)]
*  Whereas there are also loss spikes that are, it looks good. [[00:45:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2735.54s)]
*  And then there's one spiky data point. [[00:45:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2737.74s)]
*  And what you can do is you just skip those. [[00:45:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2739.62s)]
*  You see that there's a spike, you're like, okay, [[00:45:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2741.3799999999997s)]
*  I can ignore this data, don't update the model [[00:45:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2743.2999999999997s)]
*  and do the next one and it'll recover quickly. [[00:45:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2745.2599999999998s)]
*  But these like on trickier implementation [[00:45:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2747.3799999999997s)]
*  so as you get more complex in your architecture [[00:45:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2750.5s)]
*  and you scale up to more GPUs, [[00:45:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2753.54s)]
*  you have more potential for your loss blowing up. [[00:45:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2754.98s)]
*  So it's like there's a distribution. [[00:45:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2758.14s)]
*  The whole idea of grokking also comes in, right? [[00:46:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2760.5s)]
*  Just because it slowed down from improving and loss [[00:46:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2762.82s)]
*  doesn't mean it's not learning [[00:46:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2764.98s)]
*  because all of a sudden it could be like this [[00:46:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2766.26s)]
*  and it could just spike down and loss again [[00:46:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2767.94s)]
*  because it learned, truly learned something, right? [[00:46:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2769.54s)]
*  And it took some time for it to learn that. [[00:46:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2772.5s)]
*  It's not like a gradual process, right? [[00:46:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2774.46s)]
*  And that's what humans are like, [[00:46:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2776.1800000000003s)]
*  that's what models are like. [[00:46:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2777.38s)]
*  So it's really a stressful task, as you mentioned. [[00:46:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2778.2200000000003s)]
*  And the whole time the dollar count is going up. [[00:46:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2781.2200000000003s)]
*  Every company has failed runs. [[00:46:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2784.34s)]
*  You need failed runs to push the envelope [[00:46:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2786.3s)]
*  on your infrastructure. [[00:46:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2788.06s)]
*  So a lot of news cycles are made of [[00:46:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2789.02s)]
*  X company had Y failed run. [[00:46:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2791.44s)]
*  Every company that's trying to push the frontier of AI [[00:46:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2793.8s)]
*  has these. [[00:46:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2796.96s)]
*  So yes, it's noteworthy because it's a lot of money [[00:46:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2797.88s)]
*  and it can be week to month setback, [[00:46:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2800.76s)]
*  but it is part of the process. [[00:46:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2803.14s)]
*  But how do you get, if you're deep seek, [[00:46:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2804.92s)]
*  how do you get to a place where, [[00:46:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2807.7200000000003s)]
*  holy shit, there's a successful combination [[00:46:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2809.52s)]
*  of hyperparameters? [[00:46:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2811.7200000000003s)]
*  A lot of small failed runs. [[00:46:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2812.7200000000003s)]
*  And so rapid iteration through failed runs until- [[00:46:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2814.4s)]
*  And successful ones. [[00:46:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2819.52s)]
*  And then you build a small intuition like this, [[00:47:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2821.0s)]
*  this mixture of expert works. [[00:47:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2823.52s)]
*  And then this implementation of MLA works. [[00:47:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2825.64s)]
*  Key hyperparameters like learning rate and regularization [[00:47:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2829.04s)]
*  and things like this. [[00:47:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2833.64s)]
*  And you find the regime that works for your code base. [[00:47:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2834.48s)]
*  I've talking to people at Frontier Labs, [[00:47:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2836.72s)]
*  there's a story that you can tell [[00:47:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2839.4s)]
*  where training language models is kind of a path [[00:47:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2840.8s)]
*  that you need to follow. [[00:47:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2844.04s)]
*  So you need to like unlock the ability to train [[00:47:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2844.92s)]
*  a certain type of model or a certain scale. [[00:47:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2847.76s)]
*  And then your code base and your internal know-how [[00:47:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2849.68s)]
*  of which hyperparameters work for it is kind of known. [[00:47:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2851.56s)]
*  And you look at the deep seek papers and models, [[00:47:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2854.16s)]
*  they've scaled up, they've added complexity, [[00:47:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2856.12s)]
*  and it's just continuing to build the capabilities [[00:47:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2858.64s)]
*  that they have. [[00:47:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2861.44s)]
*  There's the concept of a YOLO run. [[00:47:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2862.44s)]
*  So YOLO, you only live once. [[00:47:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2865.16s)]
*  And what it is is like, [[00:47:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2867.44s)]
*  there's all this experimentation you do [[00:47:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2868.64s)]
*  at the small scale, right? [[00:47:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2871.12s)]
*  Research ablations, right? [[00:47:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2872.96s)]
*  Like you have your Jupyter Notebook [[00:47:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2874.24s)]
*  where you're experimenting with MLA [[00:47:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2875.24s)]
*  on like three GPUs or whatever. [[00:47:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2876.8399999999997s)]
*  And you're doing all these different things like, [[00:47:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2879.2s)]
*  hey, do I do four active experts, 128 experts? [[00:48:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2881.6s)]
*  Do I arrange the experts this way? [[00:48:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2884.7599999999998s)]
*  All these different model architecture things [[00:48:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2886.68s)]
*  you're testing at a very small scale, right? [[00:48:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2888.96s)]
*  Couple researchers, few GPUs, tens of GPUs, [[00:48:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2890.8399999999997s)]
*  hundreds of GPUs, whatever it is. [[00:48:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2893.7599999999998s)]
*  And then all of a sudden you're like, [[00:48:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2895.2799999999997s)]
*  okay guys, no more fucking around, right? [[00:48:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2896.56s)]
*  No more screwing around. [[00:48:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2899.04s)]
*  Everyone take all the resources we have, [[00:48:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2899.8799999999997s)]
*  let's pick what we think will work and just go for it, right? [[00:48:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2903.04s)]
*  YOLO. [[00:48:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2905.64s)]
*  And this is where that sort of stress comes in is like, [[00:48:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2906.48s)]
*  well, I know it works here, [[00:48:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2909.88s)]
*  but some things that work here don't work here. [[00:48:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2910.72s)]
*  And some things that work here don't work down here, right? [[00:48:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2912.56s)]
*  In terms of scale, right? [[00:48:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2915.04s)]
*  So it's really truly a YOLO run. [[00:48:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2916.36s)]
*  And sort of like, there's this discussion [[00:48:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2919.56s)]
*  of like certain researchers just have [[00:48:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2922.8s)]
*  this methodical nature, [[00:48:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2924.32s)]
*  they can find the whole search space [[00:48:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2925.7599999999998s)]
*  and figure out all the ablations of different research [[00:48:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2927.12s)]
*  and really see what is best. [[00:48:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2929.84s)]
*  And there's certain researchers who just kind of like, [[00:48:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2931.36s)]
*  have that innate gut instinct of like, [[00:48:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2934.2s)]
*  this is the YOLO run. [[00:48:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2936.16s)]
*  Like, you know, I'm looking at the data, [[00:48:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2937.24s)]
*  I think this is it. [[00:48:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2938.72s)]
*  This is why you wanna work in post-training [[00:49:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2940.12s)]
*  because the GPU cost for training is lower. [[00:49:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2941.68s)]
*  So you can make a higher percentage [[00:49:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2943.72s)]
*  of your training runs YOLO runs. [[00:49:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2945.0s)]
*  Yeah. [[00:49:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2946.7999999999997s)]
*  For now. [[00:49:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2947.64s)]
*  Yeah, for now. [[00:49:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2948.4799999999996s)]
*  For now. [[00:49:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2949.3199999999997s)]
*  So some of this is fundamentally luck still. [[00:49:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2950.4399999999996s)]
*  Luck is skill, right? [[00:49:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2954.8399999999997s)]
*  In many cases. [[00:49:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2955.7999999999997s)]
*  Yeah, I mean, it looks lucky, right? [[00:49:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2956.7599999999998s)]
*  When you're... [[00:49:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2958.3199999999997s)]
*  But the hill to climb, [[00:49:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2959.16s)]
*  if you're on one of these labs, [[00:49:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2960.2799999999997s)]
*  you have an evaluation, you're not crushing. [[00:49:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2961.7999999999997s)]
*  There's a repeated playbook of how you improve things. [[00:49:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2963.56s)]
*  There are localized improvements, [[00:49:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2966.68s)]
*  which might be data improvements, [[00:49:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2968.04s)]
*  and these add up into the whole model [[00:49:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2969.2s)]
*  just being much better. [[00:49:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2970.72s)]
*  And when you zoom in really close, [[00:49:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2972.0s)]
*  it can be really obvious that this model [[00:49:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2973.3599999999997s)]
*  is just really bad at this thing, [[00:49:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2975.7599999999998s)]
*  and we can fix it, and you just add these up. [[00:49:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2977.16s)]
*  So some of it feels like luck, [[00:49:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2979.3599999999997s)]
*  but on the ground, especially with these new reasoning models [[00:49:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2981.2799999999997s)]
*  we're talking to, it's just so many ways [[00:49:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2984.2s)]
*  that we can poke around, [[00:49:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2986.56s)]
*  and normally it's that some of them give big improvements. [[00:49:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2987.52s)]
*  The search space is near infinite, right? [[00:49:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2991.2799999999997s)]
*  And yet the amount of compute and time you have [[00:49:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2992.8799999999997s)]
*  is very low, and you have to hit release schedules. [[00:49:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2995.7999999999997s)]
*  You have to not get blown past by everyone. [[00:49:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=2999.8799999999997s)]
*  Otherwise, what happened with DeepSeek, [[00:50:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3002.3199999999997s)]
*  crushing Meta and Mistral and Coherent, [[00:50:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3005.4799999999996s)]
*  all these guys, they moved too slow, right? [[00:50:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3007.3599999999997s)]
*  They maybe were too methodical. [[00:50:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3009.0s)]
*  I don't know, they didn't hit the YOLO run. [[00:50:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3010.56s)]
*  Whatever the reason was, maybe they weren't a skill. [[00:50:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3011.8799999999997s)]
*  Whatever, you can call it luck if you want, [[00:50:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3014.4799999999996s)]
*  but at the end of the day, it's skill. [[00:50:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3016.56s)]
*  So 2025 is the year of the YOLO run. [[00:50:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3018.12s)]
*  It seems like all the labs are going in. [[00:50:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3021.72s)]
*  I think it's even more impressive [[00:50:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3025.44s)]
*  what OpenAI did in 2022, right? [[00:50:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3027.52s)]
*  At the time, no one believed in mixture of experts models [[00:50:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3029.96s)]
*  at Google, who had all the researchers. [[00:50:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3032.76s)]
*  OpenAI had such little compute, [[00:50:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3035.6800000000003s)]
*  and they devoted all of their compute for many months, [[00:50:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3038.0s)]
*  all of it, 100%, for many months to GPT-4 [[00:50:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3041.2400000000002s)]
*  with a brand new architecture, with no belief that, [[00:50:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3044.8s)]
*  hey, let me spend a couple hundred million dollars, [[00:50:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3047.92s)]
*  which is all of the money I have, on this model, right? [[00:50:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3050.0s)]
*  That is truly YOLO, right? [[00:50:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3052.8s)]
*  Now people are like, all these training run failures [[00:50:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3055.6400000000003s)]
*  that are in the media, right? [[00:50:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3058.96s)]
*  It's like, okay, great, but actually, [[00:51:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3060.04s)]
*  a huge chunk of my GPs are doing inference. [[00:51:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3062.04s)]
*  I still have a bunch doing research constantly, [[00:51:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3064.28s)]
*  and yes, my biggest cluster is training, [[00:51:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3066.1600000000003s)]
*  but on this YOLO run, but that YOLO run is much less risky [[00:51:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3068.2000000000003s)]
*  than what OpenAI did in 2022, [[00:51:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3072.54s)]
*  or maybe what DeepSeek did now, [[00:51:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3075.0s)]
*  or sort of like, hey, we're just gonna throw everything at it. [[00:51:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3076.6400000000003s)]
*  The big winners throughout human history [[00:51:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3079.6000000000004s)]
*  are the ones who are willing to do YOLO at some point. [[00:51:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3081.8s)]
*  Okay, what do we understand about the hardware [[00:51:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3085.88s)]
*  it's been trained on, DeepSeek? [[00:51:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3088.7200000000003s)]
*  DeepSeek is very interesting, right? [[00:51:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3090.88s)]
*  This is the second to take a zoom out [[00:51:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3093.0800000000004s)]
*  out of who they are, first of all, right? [[00:51:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3094.5600000000004s)]
*  High Flyer is a hedge fund [[00:51:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3096.1200000000003s)]
*  that has historically done quantitative trading [[00:51:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3098.1600000000003s)]
*  in China as well as elsewhere, [[00:51:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3100.76s)]
*  and they have always had a significant number of GPUs, right? [[00:51:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3102.7200000000003s)]
*  In the past, a lot of these high-frequency trading, [[00:51:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3105.6400000000003s)]
*  algorithmic quant traders used FPGAs, [[00:51:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3107.8s)]
*  but it shifted to GPUs definitely, and there's both, right? [[00:51:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3110.6s)]
*  But GPUs especially, and Deep and High Flyer, [[00:51:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3113.36s)]
*  which is the hedge fund that owns DeepSeek, [[00:51:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3116.04s)]
*  and everyone who works for DeepSeek [[00:51:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3117.72s)]
*  is part of High Flyer to some extent, right? [[00:51:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3119.3199999999997s)]
*  It's same parent company, same owner, same CEO. [[00:52:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3122.3199999999997s)]
*  They had all these resources and infrastructure for trading, [[00:52:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3125.08s)]
*  and then they devoted a humongous portion of them [[00:52:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3129.4s)]
*  to training models, both language models and otherwise, [[00:52:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3132.4s)]
*  because these techniques were heavily AI influenced. [[00:52:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3135.96s)]
*  More recently, people have realized, hey, trading with... [[00:52:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3140.8399999999997s)]
*  Even when you go back to Renaissance [[00:52:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3145.96s)]
*  and all these quantitative firms, [[00:52:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3147.56s)]
*  natural language processing is the key [[00:52:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3149.8399999999997s)]
*  to trading really fast, right? [[00:52:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3151.36s)]
*  Understanding a press release [[00:52:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3153.2799999999997s)]
*  and making the right trade, right? [[00:52:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3155.0s)]
*  And so DeepSeek has always been really good at this. [[00:52:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3156.12s)]
*  And even as far back as 2021, [[00:52:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3158.8399999999997s)]
*  they have press releases and papers saying, [[00:52:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3161.2s)]
*  like, hey, we're the first company in China [[00:52:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3163.4s)]
*  with an A100 cluster this large, those 10,000 A100 GPUs, right? [[00:52:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3166.0s)]
*  This is in 2021. [[00:52:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3170.52s)]
*  Now, this wasn't all for training large language models. [[00:52:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3171.72s)]
*  This was mostly for training models [[00:52:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3174.56s)]
*  for their quantitative aspects, their quantitative trading, [[00:52:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3176.48s)]
*  as well as a lot of that was natural language processing, [[00:52:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3179.8399999999997s)]
*  to be clear, right? [[00:53:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3181.8399999999997s)]
*  And so this is the sort of history, right? [[00:53:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3183.08s)]
*  So verifiable fact is that in 2021, [[00:53:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3185.0s)]
*  they built the largest Chinese cluster, [[00:53:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3187.16s)]
*  at least they claim it was the largest cluster in China, [[00:53:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3188.7999999999997s)]
*  10,000 GPUs. [[00:53:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3191.24s)]
*  Before expert controls started. [[00:53:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3192.3199999999997s)]
*  Yeah. [[00:53:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3194.3999999999996s)]
*  It's like they've had a huge cluster [[00:53:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3195.24s)]
*  before any conversation of expert controls. [[00:53:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3196.4399999999996s)]
*  So then you step it forward to like, [[00:53:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3198.44s)]
*  what have they done over the last four years since then, [[00:53:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3200.2400000000002s)]
*  right? [[00:53:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3202.52s)]
*  Obviously they've continued to operate the hedge fund, [[00:53:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3203.36s)]
*  probably make tons of money. [[00:53:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3205.6s)]
*  And the other thing is that they've leaned more [[00:53:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3206.68s)]
*  and more and more into AI. [[00:53:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3208.76s)]
*  The CEO, Lian Cheng Feng, Lian. [[00:53:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3210.2000000000003s)]
*  You're not putting me in spot on this, [[00:53:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3213.52s)]
*  we discussed this before. [[00:53:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3214.84s)]
*  Lian Feng, right? [[00:53:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3216.32s)]
*  The CEO, he owns maybe a little bit more [[00:53:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3217.2000000000003s)]
*  than half the company, allegedly, right? [[00:53:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3221.2000000000003s)]
*  Is an extremely like Elon Jensen kind of figure [[00:53:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3223.7200000000003s)]
*  where he's just like involved in everything, right? [[00:53:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3227.24s)]
*  And so over that time period, [[00:53:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3230.12s)]
*  he's gotten really in depth into AI. [[00:53:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3231.8399999999997s)]
*  He actually has a bit of a like, [[00:53:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3233.8399999999997s)]
*  if you see some of the statements, [[00:53:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3236.04s)]
*  a bit of an IAC vibe almost, right? [[00:53:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3237.3999999999996s)]
*  Total AGI vibes. [[00:53:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3239.64s)]
*  Like we need to do this, [[00:54:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3241.4399999999996s)]
*  we need to make a new ecosystem of open AI. [[00:54:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3242.7599999999998s)]
*  We need China to lead on this sort of ecosystem [[00:54:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3246.6s)]
*  because historically the Western countries [[00:54:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3249.16s)]
*  have led on software ecosystems [[00:54:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3251.24s)]
*  and straight up acknowledges like [[00:54:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3254.12s)]
*  in order to do this, we need to do something different. [[00:54:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3256.64s)]
*  DeepSeek is his way of doing this. [[00:54:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3259.2s)]
*  Some of the translated interviews with him are fantastic. [[00:54:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3261.3199999999997s)]
*  So he has done interviews? [[00:54:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3263.16s)]
*  Yeah. [[00:54:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3264.4s)]
*  You think he would do a Western interview or no? [[00:54:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3265.24s)]
*  Or is there controls on the channel? [[00:54:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3267.12s)]
*  There hasn't been one yet, but I would try it. [[00:54:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3268.2s)]
*  I just got a Chinese translator, so it's great. [[00:54:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3271.96s)]
*  This is L. Push. [[00:54:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3274.36s)]
*  So fascinating figure, engineer, [[00:54:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3276.4s)]
*  pushing full on into AI, [[00:54:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3278.68s)]
*  leveraging the success from the high frequency trading. [[00:54:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3280.68s)]
*  Very direct quotes, [[00:54:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3284.2799999999997s)]
*  we will not switch to closed source [[00:54:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3285.9599999999996s)]
*  when asked about this stuff. [[00:54:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3287.9199999999996s)]
*  Very long-term motivated in how the ecosystem [[00:54:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3289.68s)]
*  of AI should work. [[00:54:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3294.72s)]
*  And I think from a Chinese perspective, [[00:54:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3295.9599999999996s)]
*  he wants a Chinese company to build this vision. [[00:54:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3298.68s)]
*  And so this is sort of like the quote unquote visionary [[00:55:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3303.0s)]
*  behind the company, right? [[00:55:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3305.7999999999997s)]
*  This hedge fund still exists, right? [[00:55:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3307.08s)]
*  This quantitative firm. [[00:55:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3308.4799999999996s)]
*  And so DeepSeek is the sort of, [[00:55:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3309.4399999999996s)]
*  slowly he got turned to this full view of AI, [[00:55:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3313.5600000000004s)]
*  everything about this, right? [[00:55:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3316.76s)]
*  But at some point it slowly maneuvered [[00:55:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3317.8s)]
*  and he made DeepSeek. [[00:55:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3319.7200000000003s)]
*  And DeepSeek has done multiple models since then. [[00:55:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3320.84s)]
*  They've acquired more and more GPUs. [[00:55:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3323.48s)]
*  They share infrastructure with the fund, right? [[00:55:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3324.92s)]
*  And so there is no exact number of public GPU resources [[00:55:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3327.96s)]
*  that they have, but besides this 10,000 GPUs [[00:55:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3332.96s)]
*  that they bought in 2021, right? [[00:55:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3335.84s)]
*  And they were fantastically profitable, right? [[00:55:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3338.0400000000004s)]
*  And then this paper claims they did only 2,000 H800 GPUs, [[00:55:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3340.16s)]
*  which are a restricted GPU that was previously allowed [[00:55:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3344.12s)]
*  in China, but no longer allowed and there's a new version. [[00:55:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3347.04s)]
*  But it's basically Nvidia's H100 for China, right? [[00:55:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3349.2799999999997s)]
*  And there's some restrictions on it specifically [[00:55:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3352.52s)]
*  around the communications, sort of speed, [[00:55:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3354.7999999999997s)]
*  the interconnect speed, right? [[00:55:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3357.6s)]
*  Which is why they had to do this crazy SM, [[00:55:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3358.6s)]
*  you know, scheduling stuff, right? [[00:56:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3361.2799999999997s)]
*  So going back to that, right? [[00:56:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3362.88s)]
*  It looks like this is obviously not true [[00:56:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3363.92s)]
*  in terms of their total GPU count. [[00:56:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3367.04s)]
*  So this is a lot of obvious available GPUs, [[00:56:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3368.32s)]
*  but for this training run, [[00:56:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3371.56s)]
*  you think 2,000 is the correct number or no? [[00:56:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3373.4s)]
*  So this is where it takes, you know, [[00:56:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3375.84s)]
*  a significant amount of sort of like zoning in, right? [[00:56:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3377.96s)]
*  Like what do you call your training run, right? [[00:56:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3380.32s)]
*  Do you count all of the research and ablations [[00:56:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3383.1600000000003s)]
*  that you ran, right? [[00:56:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3385.8s)]
*  Picking all this stuff, because yes, [[00:56:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3387.1200000000003s)]
*  you can do a YOLO run, but at some level, [[00:56:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3388.6800000000003s)]
*  you have to do the test at the small scale [[00:56:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3390.6400000000003s)]
*  and then you have to do some test at medium scale [[00:56:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3392.2000000000003s)]
*  before you go to a large scale. [[00:56:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3394.0800000000004s)]
*  Accepted practice is that for any given model [[00:56:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3395.1600000000003s)]
*  that you're going to do 2 to 4X compute [[00:56:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3399.28s)]
*  of the full training run in experiments alone. [[00:56:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3401.2000000000003s)]
*  So a lot of this compute that's being scaled up [[00:56:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3404.32s)]
*  is probably used in large part at this time for research. [[00:56:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3406.8s)]
*  Yeah, and research will, you know, [[00:56:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3410.0800000000004s)]
*  research begets the new ideas [[00:56:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3411.88s)]
*  that let you get huge efficiency. [[00:56:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3413.2400000000002s)]
*  Research gets you O1. [[00:56:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3414.0800000000004s)]
*  Like research gets you breakthroughs [[00:56:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3416.0800000000004s)]
*  and you need to bet on it. [[00:56:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3417.5600000000004s)]
*  So some of the pricing strategy they will discuss [[00:56:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3418.8s)]
*  has the research baked into the price. [[00:57:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3421.0400000000004s)]
*  So the numbers that DeepSeek specifically said publicly, [[00:57:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3423.5600000000004s)]
*  10,000 GPUs in 2021 and then 2000 GPUs [[00:57:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3426.6s)]
*  for only the pre-training for V3. [[00:57:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3429.6s)]
*  They did not discuss cost on R1. [[00:57:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3432.12s)]
*  They did not discuss cost on all the other RL, right? [[00:57:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3434.2799999999997s)]
*  For the instruct model that they made, right? [[00:57:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3437.64s)]
*  They only discussed the pre-training for the base model [[00:57:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3439.92s)]
*  and they did not discuss anything on research in ablations. [[00:57:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3442.56s)]
*  And they do not talk about any of the resources [[00:57:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3445.2s)]
*  that are shared in terms of, [[00:57:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3447.0s)]
*  hey, the fund is using all these GPUs, right? [[00:57:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3448.4s)]
*  And we know that they're very profitable [[00:57:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3451.08s)]
*  and that 10,000 GPUs in 2021. [[00:57:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3453.0s)]
*  So some of the research that we've found [[00:57:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3457.48s)]
*  is that we actually believe they have closer to 50,000 GPUs. [[00:57:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3460.2s)]
*  We is semi-analysis. [[00:57:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3463.64s)]
*  So we should say that you're sort of one of the world experts [[00:57:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3464.72s)]
*  in figuring out what everybody's doing [[00:57:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3468.16s)]
*  in terms of the semiconductor, [[00:57:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3470.68s)]
*  in terms of cluster build-outs, [[00:57:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3471.84s)]
*  in terms of like who is doing what [[00:57:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3473.08s)]
*  in terms of training runs. [[00:57:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3475.68s)]
*  So yeah, so that's the we. [[00:57:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3477.68s)]
*  Okay, go ahead. [[00:57:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3479.16s)]
*  Yeah, sorry. [[00:58:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3480.0s)]
*  We believe they actually have something closer [[00:58:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3480.84s)]
*  to 50,000 GPUs, right? [[00:58:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3482.44s)]
*  Now this is split across many tasks, right? [[00:58:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3483.76s)]
*  Again, the fund, research and ablations. [[00:58:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3485.8s)]
*  For ballpark, how much would OpenAI or Anthropic have? [[00:58:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3489.4s)]
*  I think the clearest example we have [[00:58:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3491.6800000000003s)]
*  because Meta is also open, [[00:58:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3494.2400000000002s)]
*  they talk about like order of 60K to 100K, [[00:58:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3495.36s)]
*  H100 equivalent GPUs in their training clusters. [[00:58:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3498.4s)]
*  Right, so like Lama 3, [[00:58:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3501.4s)]
*  they trained on 16,000 H100s, right? [[00:58:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3502.84s)]
*  But the company of Meta last year publicly disclosed [[00:58:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3505.7200000000003s)]
*  they bought like 400 something thousand GPUs. [[00:58:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3508.32s)]
*  Yeah. [[00:58:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3510.0800000000004s)]
*  Right, so of course, tiny percentage on the training. [[00:58:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3510.92s)]
*  Again, like most of it is like serving me [[00:58:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3513.0s)]
*  the best Instagram reels, right? [[00:58:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3514.84s)]
*  Or whatever, right? [[00:58:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3516.6000000000004s)]
*  I mean, we could get into a cost of like, [[00:58:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3517.44s)]
*  what is the cost of ownership for a 2000 GPU cluster, [[00:58:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3519.0s)]
*  10,000, like there's just different sizes of companies [[00:58:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3522.56s)]
*  that can afford these things [[00:58:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3525.84s)]
*  and DeepSeek is reasonably big. [[00:58:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3526.7200000000003s)]
*  Their compute allocation compared is one of the top few [[00:58:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3529.6400000000003s)]
*  in the world, it's not OpenAI, Anthropic, et cetera, [[00:58:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3534.7200000000003s)]
*  but they have a lot of compute. [[00:58:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3536.88s)]
*  Can you in general actually just zoom out [[00:58:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3538.04s)]
*  and also talk about the Hopper architecture, [[00:58:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3539.56s)]
*  the Nvidia Hopper GPU architecture [[00:59:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3541.6800000000003s)]
*  and the difference between H100 and H800, [[00:59:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3544.48s)]
*  like you mentioned, the interconnects. [[00:59:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3548.16s)]
*  Yeah, so there's, you know, Ampere was the A100 [[00:59:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3549.72s)]
*  and then H100 Hopper, right? [[00:59:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3551.68s)]
*  People use them synonymously in the US [[00:59:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3553.64s)]
*  because really there's just H100 [[00:59:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3555.64s)]
*  and now there's H200, right? [[00:59:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3557.56s)]
*  But same thing, mostly. [[00:59:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3558.6s)]
*  In China, they've had two, [[00:59:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3560.96s)]
*  there've been different salvos of expert restrictions. [[00:59:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3562.0s)]
*  So initially the US government limited [[00:59:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3564.6s)]
*  on a two factor scale, right? [[00:59:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3566.64s)]
*  Which is chip interconnect versus a flops, right? [[00:59:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3568.0s)]
*  So any chip that had interconnects above a certain level [[00:59:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3571.12s)]
*  and flops above a certain floating point operations [[00:59:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3573.68s)]
*  above a certain level was restricted. [[00:59:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3576.3999999999996s)]
*  Later, the government realized that this was a flaw [[00:59:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3578.7599999999998s)]
*  in the restriction and they cut it down [[00:59:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3581.3199999999997s)]
*  to just floating point operations. [[00:59:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3583.52s)]
*  And so... [[00:59:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3586.2799999999997s)]
*  H800 had high flops, low communication. [[00:59:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3587.8399999999997s)]
*  Exactly, so the H800 was the same performance [[00:59:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3591.6s)]
*  as H100 on flops, right? [[00:59:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3594.6s)]
*  But it just had the interconnect bandwidth cut. [[00:59:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3596.56s)]
*  DeepSeq knew how to utilize this, you know, [[00:59:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3599.04s)]
*  hey, even though we're cut back on the interconnect, [[01:00:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3602.12s)]
*  we can do all this fancy stuff [[01:00:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3604.36s)]
*  to figure out how to use the GPU fully anyways, right? [[01:00:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3606.2s)]
*  And so that was back in October, 2022. [[01:00:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3609.3199999999997s)]
*  But later in 2023, end of 2023, implemented in 2024, [[01:00:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3612.56s)]
*  the US government banned the H800, right? [[01:00:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3617.7599999999998s)]
*  And so by the way, this H800 cluster, [[01:00:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3620.08s)]
*  these 2000 GPUs was not even purchased in 2024, right? [[01:00:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3621.92s)]
*  It was purchased in late 2023. [[01:00:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3624.88s)]
*  And they're just getting the model out now, right? [[01:00:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3627.56s)]
*  Because it takes a lot of research, et cetera. [[01:00:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3629.16s)]
*  H800 was banned and now there's a new chip called the H20. [[01:00:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3631.44s)]
*  The H20 is cut back on only flops, [[01:00:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3634.88s)]
*  but the interconnect bandwidth is the same. [[01:00:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3638.2400000000002s)]
*  And in fact, in some ways it's better than the H100 [[01:00:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3640.0s)]
*  because it has better memory bandwidth and memory capacity. [[01:00:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3642.7200000000003s)]
*  So there are, you know, [[01:00:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3645.52s)]
*  Nvidia is working within the constraints [[01:00:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3646.52s)]
*  of what the government sets [[01:00:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3648.08s)]
*  and then builds the best possible GPU for China. [[01:00:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3649.64s)]
*  Can we take this actual tangent [[01:00:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3652.4s)]
*  and we'll return back to the hardware? [[01:00:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3654.04s)]
*  Is the philosophy, the motivation, [[01:00:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3656.0s)]
*  the case for export controls? [[01:00:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3659.44s)]
*  Dari Amadei just published a blog post [[01:01:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3662.0s)]
*  about export controls. [[01:01:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3663.92s)]
*  The case he makes is that if AI becomes super powerful [[01:01:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3665.68s)]
*  and he says by 2026 we'll have AGI or super powerful AI [[01:01:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3669.04s)]
*  and that's going to give a significant, [[01:01:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3673.7599999999998s)]
*  whoever builds that will have [[01:01:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3675.32s)]
*  a significant military advantage. [[01:01:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3676.92s)]
*  And so, because the United States is a democracy [[01:01:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3679.36s)]
*  and as he says, China is authoritarian [[01:01:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3682.7599999999998s)]
*  or has authoritarian elements, [[01:01:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3687.2s)]
*  you want a unipolar world where the super powerful military [[01:01:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3689.48s)]
*  because of the AI is one that's a democracy. [[01:01:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3694.48s)]
*  It's a much more complicated world geopolitically [[01:01:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3697.44s)]
*  when you have two superpowers with super powerful AI [[01:01:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3700.44s)]
*  and one is authoritarian. [[01:01:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3704.8s)]
*  So that's the case he makes. [[01:01:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3706.76s)]
*  And so we wanna, the United States wants to use [[01:01:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3707.88s)]
*  export controls to slow down, [[01:01:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3711.16s)]
*  to make sure that China can't do [[01:01:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3713.28s)]
*  these gigantic training runs [[01:01:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3716.24s)]
*  that will be presumably required to build AGI. [[01:01:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3719.7999999999997s)]
*  This is very abstract. [[01:02:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3722.64s)]
*  I think this can be the goal [[01:02:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3723.8399999999997s)]
*  of how some people describe export controls [[01:02:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3726.04s)]
*  is this super powerful AI. [[01:02:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3728.3999999999996s)]
*  There's, and you touched on the training run idea, [[01:02:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3730.3199999999997s)]
*  there's not many worlds where China cannot train AI models. [[01:02:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3734.08s)]
*  Export controls are decapping the amount of compute [[01:02:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3739.2799999999997s)]
*  or the density of compute that China can have [[01:02:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3743.12s)]
*  and if you think about the AI ecosystem right now, [[01:02:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3746.88s)]
*  as all of these AI companies, [[01:02:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3749.36s)]
*  revenue numbers are up and to the right, [[01:02:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3751.16s)]
*  AI usage is just continuing to grow, [[01:02:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3752.84s)]
*  more GPUs are going to inference. [[01:02:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3755.2400000000002s)]
*  A large part of export controls, if they work, [[01:02:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3756.7200000000003s)]
*  is just that the amount of AI that can be run in China [[01:02:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3760.2000000000003s)]
*  is going to be much lower. [[01:02:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3764.12s)]
*  So on the training side, DeepSeq V3 is a great example [[01:02:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3765.6s)]
*  which you have a very focused team [[01:02:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3768.08s)]
*  that can still get to the frontier of AI. [[01:02:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3769.64s)]
*  This 2000 GPUs is not that hard to get [[01:02:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3772.16s)]
*  at all considering in the world. [[01:02:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3774.8s)]
*  They're still gonna have those GPUs. [[01:02:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3776.96s)]
*  They're still gonna be able to train models, [[01:02:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3778.6800000000003s)]
*  but if there's gonna be a huge market for AI, [[01:03:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3780.1600000000003s)]
*  if you have strong export controls [[01:03:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3782.2000000000003s)]
*  and you wanna have 100,000 GPUs [[01:03:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3783.6800000000003s)]
*  just serving the equivalent of Chad GPT clusters [[01:03:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3785.32s)]
*  with good export controls, it also just makes it [[01:03:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3788.2000000000003s)]
*  so that AI can be used much less. [[01:03:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3790.04s)]
*  And I think that is a much easier goal to achieve [[01:03:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3793.76s)]
*  than trying to debate on what AGI is [[01:03:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3797.96s)]
*  and if you have these extremely intelligent, [[01:03:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3800.2000000000003s)]
*  autonomous AIs and data centers, [[01:03:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3802.7200000000003s)]
*  those are the things that could be running [[01:03:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3805.52s)]
*  in these GPU clusters in the United States, [[01:03:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3806.68s)]
*  but not in China. [[01:03:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3809.2799999999997s)]
*  To some extent, training a model [[01:03:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3810.2s)]
*  does effectively nothing, right? [[01:03:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3811.96s)]
*  Like they don't have a model. [[01:03:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3813.2s)]
*  The thing that Dario is sort of speaking to [[01:03:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3814.8399999999997s)]
*  is the implementation of that model once trained [[01:03:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3817.7999999999997s)]
*  to then create huge economic growth, [[01:03:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3821.56s)]
*  huge increases in military capabilities, [[01:03:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3824.08s)]
*  huge increases in productivity of people, [[01:03:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3826.3199999999997s)]
*  betterment of lives, whatever you wanna direct [[01:03:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3829.3999999999996s)]
*  super powerful AI towards, you can't. [[01:03:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3831.8799999999997s)]
*  But that requires significant amounts of compute, right? [[01:03:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3834.04s)]
*  And so the US government has effectively said, [[01:03:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3836.92s)]
*  and forever, right? [[01:03:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3839.56s)]
*  Like training will always be a portion of the total compute. [[01:04:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3841.48s)]
*  We mentioned Meta's 400,000 GPUs, only 16,000 made Lama, [[01:04:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3845.2s)]
*  right? [[01:04:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3848.92s)]
*  So the percentage that Meta's dedicating to inference, [[01:04:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3849.7599999999998s)]
*  now this might be for recommendation systems [[01:04:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3852.08s)]
*  that are trying to hack our mind into spending more time [[01:04:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3854.4s)]
*  and watching more ads, [[01:04:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3856.48s)]
*  or if it's for a super powerful AI [[01:04:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3857.6s)]
*  that's doing productive things, [[01:04:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3860.56s)]
*  doesn't matter about the exact use [[01:04:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3861.68s)]
*  that our economic system decides, [[01:04:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3863.12s)]
*  it's that that can be delivered in whatever way we want. [[01:04:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3865.2s)]
*  Whereas with China, right? [[01:04:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3868.56s)]
*  You know, expert restrictions, great. [[01:04:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3870.72s)]
*  You're never gonna be able to cut everything off, right? [[01:04:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3872.6s)]
*  And I think that's quite well understood [[01:04:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3875.12s)]
*  by the US government is that you can't cut everything off. [[01:04:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3877.16s)]
*  You know? [[01:04:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3880.68s)]
*  And they'll make their own chips. [[01:04:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3881.52s)]
*  And they're trying to make their own chips. [[01:04:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3882.3599999999997s)]
*  They'll be worse than ours. [[01:04:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3883.48s)]
*  But the whole point is to just keep a gap, right? [[01:04:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3884.3199999999997s)]
*  And therefore at some point, [[01:04:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3887.16s)]
*  as the AI, in a world where two, 3% economic growth, [[01:04:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3888.7999999999997s)]
*  this is really dumb by the way, right? [[01:04:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3891.84s)]
*  To cut off, you know, high tech and not make money off of it. [[01:04:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3893.56s)]
*  But in a world where super powerful AI comes about [[01:04:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3896.94s)]
*  and then starts creating significant changes in society, [[01:04:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3899.8s)]
*  which is what all the AI leaders [[01:05:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3902.6800000000003s)]
*  and big tech companies believe, I think, [[01:05:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3904.1200000000003s)]
*  super powerful AI is gonna change society massively. [[01:05:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3905.84s)]
*  And therefore this compounding effect [[01:05:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3908.4s)]
*  of the difference in compute is really important. [[01:05:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3910.4s)]
*  There's some sci-fi out there where like, [[01:05:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3912.6400000000003s)]
*  AI is like measured in the power of, [[01:05:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3915.28s)]
*  in like how much power is delivered to compute, right? [[01:05:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3917.76s)]
*  Or how much is being, you know, [[01:05:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3919.7200000000003s)]
*  that's sort of a way of thinking about [[01:05:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3921.52s)]
*  what's the economic output is just, [[01:05:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3922.88s)]
*  how much power are you directing towards that AI? [[01:05:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3924.36s)]
*  Should we talk about reasoning models with this? [[01:05:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3926.72s)]
*  As a way that this might be actionable [[01:05:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3928.36s)]
*  as something that people can actually see. [[01:05:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3930.6s)]
*  So the reasoning models that are coming out [[01:05:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3932.08s)]
*  with R1 and O1, they're designed to use more compute. [[01:05:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3933.68s)]
*  There's a lot of buzzy words in the AI community about this, [[01:05:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3937.48s)]
*  test time compute, inference time compute, whatever. [[01:05:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3941.28s)]
*  But Dylan has good research on this. [[01:05:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3944.32s)]
*  You can get to the specific numbers on the ratio [[01:05:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3946.08s)]
*  of when you train a model, you can look at things [[01:05:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3948.08s)]
*  about the amount of compute use at training [[01:05:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3949.68s)]
*  and amount of compute use at inference. [[01:05:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3951.56s)]
*  These reasoning models are making inference [[01:05:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3953.3199999999997s)]
*  way more important to doing complex tasks. [[01:05:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3954.96s)]
*  In the fall in December, [[01:05:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3958.08s)]
*  their OpenAI announced this O3 model. [[01:06:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3960.3999999999996s)]
*  There's another thing in AI when things move fast, [[01:06:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3962.6s)]
*  we get both announcements and releases. [[01:06:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3964.2799999999997s)]
*  Announcements are essentially blog posts [[01:06:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3966.16s)]
*  where you pat yourself on the back [[01:06:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3967.72s)]
*  and you say you did things and releases [[01:06:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3969.12s)]
*  are on the models out there, the papers out there, et cetera. [[01:06:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3970.8399999999997s)]
*  So OpenAI has announced O3, [[01:06:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3972.96s)]
*  and we can check if O3 mini is out [[01:06:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3974.7599999999998s)]
*  as a recording potentially, [[01:06:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3977.16s)]
*  but that doesn't really change the point, [[01:06:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3978.88s)]
*  which is that the breakthrough result [[01:06:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3980.36s)]
*  was something called Arc-AGI task, [[01:06:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3982.6s)]
*  which is the Abstract Reasoning Corpus, [[01:06:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3984.7200000000003s)]
*  a task for artificial general intelligence. [[01:06:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3987.04s)]
*  Francois Chalet is the guy who's been, [[01:06:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3990.04s)]
*  it's a multi-year old paper, it's a brilliant benchmark. [[01:06:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3993.44s)]
*  And the number for OpenAI O3 to solve this [[01:06:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3996.08s)]
*  was that it used some sort of number of samples in the API. [[01:06:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=3999.4s)]
*  The API has like thinking effort and number of samples. [[01:06:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4003.0s)]
*  They used a thousand samples to solve this task [[01:06:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4006.04s)]
*  and it comes out to be like five to $20 per question, [[01:06:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4008.56s)]
*  which you're putting in effectively a math puzzle, [[01:06:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4013.4s)]
*  and then it takes orders of dollars to answer one question. [[01:06:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4015.72s)]
*  And this is a lot of compute. [[01:06:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4019.08s)]
*  If this is gonna take off in the US, [[01:07:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4020.84s)]
*  OpenAI needs a ton of GPUs on inference to capture this. [[01:07:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4022.08s)]
*  They have this OpenAI ChatGPT Pro subscription, [[01:07:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4024.88s)]
*  which is $200 a month. [[01:07:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4028.36s)]
*  Which Sam said they're losing money on. [[01:07:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4029.6s)]
*  Which means that people are burning [[01:07:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4031.24s)]
*  a lot of GPUs on inference. [[01:07:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4032.44s)]
*  And I've signed up with it, I've played with it, [[01:07:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4034.2799999999997s)]
*  I don't think I'm a power user, [[01:07:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4036.2s)]
*  but I use it and it's like, [[01:07:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4037.4s)]
*  that is the thing that a Chinese company [[01:07:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4039.32s)]
*  with mediumly strong expert controls, [[01:07:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4042.0s)]
*  there will always be loopholes, [[01:07:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4044.6s)]
*  might not be able to do it all. [[01:07:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4045.92s)]
*  And if that, the main result for O3 [[01:07:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4047.44s)]
*  is also a spectacular coding performance. [[01:07:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4049.88s)]
*  And if that feeds back into AI companies [[01:07:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4052.28s)]
*  being able to experiment better. [[01:07:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4055.0s)]
*  So presumably the idea is for an AGI, [[01:07:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4057.64s)]
*  a much larger fraction of the compute [[01:07:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4061.28s)]
*  will be used for this test time compute for the reasoning. [[01:07:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4063.2400000000002s)]
*  For the AGI goes into a room and thinks about [[01:07:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4066.36s)]
*  how to take over the world and come back in 2.7 hours. [[01:07:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4068.56s)]
*  And that it's gonna take a lot of compute. [[01:07:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4074.44s)]
*  This is what people like CEO or leaders of OpenAI [[01:07:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4076.52s)]
*  and Anthropik talk about is like autonomous AI models, [[01:08:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4080.32s)]
*  which is you give them a task [[01:08:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4083.2000000000003s)]
*  and they work on it in the background. [[01:08:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4084.28s)]
*  My personal definition of AGI is much simpler. [[01:08:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4086.04s)]
*  Like I think language models are a form of AGI [[01:08:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4090.1600000000003s)]
*  and all of this super powerful stuff [[01:08:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4092.36s)]
*  is a next step that's great if we get these tools, [[01:08:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4093.92s)]
*  but a language model has so much value in so many domains. [[01:08:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4096.6s)]
*  It is a general intelligence to me. [[01:08:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4099.6s)]
*  But this next step of agentic things [[01:08:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4101.56s)]
*  where they're independent and they can do tasks [[01:08:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4103.84s)]
*  that aren't in the training data [[01:08:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4105.92s)]
*  is what the few year outlook [[01:08:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4107.72s)]
*  that these AI companies are driving for. [[01:08:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4110.52s)]
*  I think the terminology here that Dario uses [[01:08:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4112.84s)]
*  is super powerful AI. [[01:08:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4115.88s)]
*  So I agree with you on the AGI. [[01:08:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4117.4s)]
*  I think we already have something like [[01:08:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4119.32s)]
*  that's exceptionally impressive [[01:08:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4120.88s)]
*  that Alan Turing would for sure say is AGI, [[01:08:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4122.48s)]
*  but he's referring more to something once in possession of [[01:08:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4125.36s)]
*  then you would have a significant military [[01:08:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4129.919999999999s)]
*  and geopolitical advantage over other nations. [[01:08:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4132.2s)]
*  So it's not just like you can ask it how to cook an omelet. [[01:08:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4134.679999999999s)]
*  And he has a much more positive view [[01:08:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4138.48s)]
*  and as I say, machines of love and grace. [[01:08:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4139.959999999999s)]
*  I've read into this, [[01:09:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4141.679999999999s)]
*  I don't have enough background in physical sciences [[01:09:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4143.12s)]
*  to gauge exactly how competent I am [[01:09:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4145.4s)]
*  and if AI can revolutionize biology. [[01:09:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4147.5199999999995s)]
*  I'm safe saying that AI is going to [[01:09:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4150.24s)]
*  accelerate the progress of any computational science. [[01:09:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4153.8s)]
*  So we're doing a depth first search here on topics, [[01:09:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4156.16s)]
*  taking tangent of a tangent. [[01:09:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4158.84s)]
*  So let's continue on that depth first search. [[01:09:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4160.0s)]
*  You said that you're both feeling the AGI. [[01:09:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4164.72s)]
*  So what's your timeline? [[01:09:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4168.76s)]
*  Dario's 2026 for the super powerful AI [[01:09:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4170.599999999999s)]
*  that's basically agentic to a degree where [[01:09:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4174.16s)]
*  it's a real security threat. [[01:09:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4178.2s)]
*  That level of AGI, what's your timeline? [[01:09:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4181.68s)]
*  I don't like to attribute specific abilities [[01:09:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4184.76s)]
*  because predicting specific abilities and when is very hard. [[01:09:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4187.12s)]
*  I think mostly if you're going to say [[01:09:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4189.76s)]
*  that I'm feeling the AGI is that I expect [[01:09:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4191.5199999999995s)]
*  continued rapid surprising progress over the next few years. [[01:09:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4194.2s)]
*  So something like R1 is less surprising to me from DeepSeq [[01:09:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4197.5199999999995s)]
*  because I expect there to be new paradigms [[01:10:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4201.5199999999995s)]
*  where substantial progress can be made. [[01:10:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4203.8s)]
*  I think DeepSeq R1 is so unsteadily [[01:10:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4205.76s)]
*  because we're kind of on this path with chat GPT. [[01:10:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4207.84s)]
*  It's like, it's getting better, it's getting better, [[01:10:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4210.6s)]
*  it's getting better. [[01:10:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4211.88s)]
*  And then we have a new direction for changing the models. [[01:10:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4212.72s)]
*  And we took one step like this and we like took a step up. [[01:10:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4215.24s)]
*  So it looks like a really fast slope [[01:10:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4217.76s)]
*  and then we're going to just take more steps. [[01:10:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4219.64s)]
*  So like, it's just really unsettling [[01:10:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4221.2s)]
*  when you have these big steps [[01:10:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4222.4400000000005s)]
*  and I expect that to keep happening. [[01:10:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4224.0s)]
*  I see I've tried opening AI operator, [[01:10:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4226.16s)]
*  I've tried Claude computer use. [[01:10:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4229.360000000001s)]
*  They're not there yet. [[01:10:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4231.400000000001s)]
*  I understand the idea, but it's just so hard to predict [[01:10:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4232.360000000001s)]
*  what is the breakthrough that will make something [[01:10:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4235.72s)]
*  like that work. [[01:10:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4237.36s)]
*  And I think it's more likely that we have breakthroughs [[01:10:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4238.2s)]
*  that work in things that we don't know what they're gonna do. [[01:10:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4240.839999999999s)]
*  So like everyone wants agents. [[01:10:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4242.96s)]
*  Dario has very eloquent way of describing this. [[01:10:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4244.719999999999s)]
*  And I just think that it's like, [[01:10:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4248.48s)]
*  there's gonna be more than that. [[01:10:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4250.24s)]
*  So like, just expect these things to come. [[01:10:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4251.32s)]
*  I'm gonna have to try to pin you down to a date [[01:10:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4253.92s)]
*  on the AGI timeline. [[01:10:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4256.32s)]
*  Like the nuclear weapon moment. [[01:10:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4259.2s)]
*  So moment where on the geopolitical stage, [[01:11:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4262.24s)]
*  there's a real like, [[01:11:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4266.56s)]
*  cause we're talking about export controls. [[01:11:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4269.360000000001s)]
*  When do you think, just even if throw out a date, [[01:11:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4271.6s)]
*  when do you think that would be? [[01:11:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4274.72s)]
*  Like for me, it's probably after 2030. [[01:11:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4275.92s)]
*  So I'm not as- [[01:11:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4278.8s)]
*  That's what I would say. [[01:11:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4279.64s)]
*  So define that, right? [[01:11:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4281.04s)]
*  Because to me, it kind of almost has already happened. [[01:11:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4281.92s)]
*  You look at elections in India and Pakistan, [[01:11:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4284.52s)]
*  people get AI voice calls [[01:11:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4286.6s)]
*  and think they're talking to the politician, right? [[01:11:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4288.200000000001s)]
*  The AI diffusion rules, [[01:11:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4291.160000000001s)]
*  which was enacted in the last couple of weeks [[01:11:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4292.4400000000005s)]
*  of the Biden admin [[01:11:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4293.84s)]
*  and looks like the Trump admin will keep [[01:11:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4294.68s)]
*  and potentially even strengthen, [[01:11:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4296.4800000000005s)]
*  limit cloud computing and GPU sales [[01:11:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4298.200000000001s)]
*  to countries that are not even related to China. [[01:11:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4301.200000000001s)]
*  It's like, this is- [[01:11:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4303.6s)]
*  Portugal and all these like normal countries [[01:11:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4304.64s)]
*  are on the, you need approval from the US list. [[01:11:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4307.320000000001s)]
*  Like yeah, Portugal and like, [[01:11:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4309.72s)]
*  all these countries that are allies, right? [[01:11:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4311.72s)]
*  Singapore, right? [[01:11:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4313.68s)]
*  Like they freaking have F35s [[01:11:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4314.52s)]
*  and we don't let them buy GPUs. [[01:11:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4316.52s)]
*  Like this to me is already to the scale of like, you know- [[01:11:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4318.04s)]
*  Well, that just means that the US military [[01:12:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4321.96s)]
*  is really nervous about this new technology. [[01:12:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4325.16s)]
*  That doesn't mean the technology is already there. [[01:12:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4326.92s)]
*  So like, they might be just very cautious [[01:12:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4329.56s)]
*  about this thing that they don't quite understand. [[01:12:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4332.52s)]
*  But that's a really good point. [[01:12:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4334.4s)]
*  So the robocalls, [[01:12:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4335.96s)]
*  swarms of semi-intelligent bots could be a weapon, [[01:12:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4338.24s)]
*  could be doing a lot of social engineering. [[01:12:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4343.4800000000005s)]
*  I mean, there's tons of talk about, you know, [[01:12:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4345.64s)]
*  from the 2016 elections, like Cambridge Analytica [[01:12:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4347.12s)]
*  and all this stuff, Russian influence. [[01:12:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4349.72s)]
*  I mean, every country in the world [[01:12:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4352.200000000001s)]
*  is pushing stuff onto the internet [[01:12:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4353.240000000001s)]
*  and has narratives they want, right? [[01:12:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4354.72s)]
*  Like that's every like technically competent, [[01:12:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4356.04s)]
*  whether it's Russia, China, US, Israel, et cetera, right? [[01:12:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4358.52s)]
*  You know, people are pushing viewpoints [[01:12:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4361.68s)]
*  onto the internet en masse [[01:12:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4363.16s)]
*  and language models crash the cost [[01:12:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4365.0s)]
*  of like very intelligent sounding language. [[01:12:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4367.0s)]
*  There's some research that shows that the distribution [[01:12:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4369.08s)]
*  is actually the limiting factor. [[01:12:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4371.68s)]
*  So language models haven't yet made misinformation [[01:12:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4373.04s)]
*  particularly like change the equation there. [[01:12:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4376.84s)]
*  The internet is still ongoing. [[01:13:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4380.76s)]
*  I think there's a blog AI Snake Oil [[01:13:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4382.0s)]
*  and some of my friends at Princeton that write on this stuff. [[01:13:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4383.88s)]
*  So there is research. [[01:13:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4385.92s)]
*  It's like, it's a default that everyone assumes. [[01:13:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4386.96s)]
*  And I would have thought the same thing [[01:13:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4388.72s)]
*  is that misinformation is gonna get far worse [[01:13:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4389.96s)]
*  with language models. [[01:13:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4392.360000000001s)]
*  I think in terms of internet posts [[01:13:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4393.2s)]
*  and things that people have been measuring, [[01:13:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4395.88s)]
*  it hasn't been a exponential increase [[01:13:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4397.72s)]
*  or something extremely measurable. [[01:13:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4400.04s)]
*  And things you're talking about with like voice calls [[01:13:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4401.280000000001s)]
*  and stuff like that, it could be in modalities [[01:13:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4403.360000000001s)]
*  that are harder to measure. [[01:13:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4405.84s)]
*  So it's something that it's too soon to tell [[01:13:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4407.6s)]
*  in terms of, I think that's like political instability [[01:13:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4410.24s)]
*  via the web is monitored by a lot of researchers [[01:13:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4413.84s)]
*  to see what's happening. [[01:13:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4418.5199999999995s)]
*  I think that you're asking about like the AGI thing. [[01:13:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4420.24s)]
*  I might, if I were to make me give a year, [[01:13:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4423.28s)]
*  I wouldn't be like, okay, I have AI CEO saying this. [[01:13:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4426.28s)]
*  They've been saying two years for a while. [[01:13:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4428.5599999999995s)]
*  I think that they're people like Dario Anthropic, [[01:13:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4430.48s)]
*  the CEO had thought about this so deeply. [[01:13:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4434.8s)]
*  I need to take their word seriously, [[01:13:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4436.88s)]
*  but also understand that they have different incentives. [[01:13:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4439.8s)]
*  So I would be like, add a few years to that, [[01:14:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4443.6s)]
*  which is how you get something similar to 2030 [[01:14:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4445.28s)]
*  or a little after 2030. [[01:14:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4447.2s)]
*  I think to some extent, we have capabilities [[01:14:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4448.4400000000005s)]
*  that hit a certain point where any one person could say, [[01:14:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4450.56s)]
*  oh, okay, if I can leverage those capabilities [[01:14:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4453.88s)]
*  for X amount of time, this is AGI, right? [[01:14:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4456.06s)]
*  Call it 27, 28. [[01:14:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4458.7s)]
*  But then the cost of actually operating that capability [[01:14:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4460.26s)]
*  is so, so extreme that no one can actually deploy it [[01:14:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4463.24s)]
*  at scale and mass to actually completely revolutionize [[01:14:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4467.76s)]
*  the economy on a click on a snap of a finger. [[01:14:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4471.360000000001s)]
*  So I don't think it will be like a snap of the finger moment. [[01:14:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4473.320000000001s)]
*  It's a physical constraint. [[01:14:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4475.72s)]
*  Rather it'll be a, oh, the capabilities are here, [[01:14:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4476.64s)]
*  but I can't deploy it everywhere, right? [[01:14:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4479.8s)]
*  And so one simple example going back sort of to 2023 [[01:14:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4481.34s)]
*  was when Bing with GPT-4 came out [[01:14:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4485.5s)]
*  and everyone was freaking out about search, right? [[01:14:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4488.96s)]
*  Perplexity came out. [[01:14:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4491.0s)]
*  If you did the cost on like, hey, implementing GPT-3 [[01:14:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4491.96s)]
*  into every Google search, it was like, oh, okay, [[01:14:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4494.68s)]
*  this is just like physically impossible to implement, right? [[01:14:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4496.56s)]
*  And as we step forward to like going back [[01:14:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4498.96s)]
*  to the test time compute thing, right? [[01:15:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4501.240000000001s)]
*  A query for, you know, you ask Chad GPT a question, [[01:15:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4503.160000000001s)]
*  it costs cents, right? [[01:15:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4506.52s)]
*  For their most capable model of chat, right? [[01:15:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4508.18s)]
*  To get a query back, to solve an Arc AGI problem though, [[01:15:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4511.120000000001s)]
*  cost five to 20 bucks, right? [[01:15:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4515.22s)]
*  And this is an- [[01:15:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4517.320000000001s)]
*  It's only going up from there. [[01:15:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4518.6s)]
*  This is a thousand, 10,000 X factor difference in cost [[01:15:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4519.72s)]
*  to respond to a query versus do a task. [[01:15:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4523.0s)]
*  And the task of Arc AGI, it's not like it's like, [[01:15:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4526.24s)]
*  it's simple to some extent, you know, [[01:15:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4528.88s)]
*  but it's also like, what are the tasks that we want? [[01:15:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4531.94s)]
*  Okay, AGI, quote unquote, what we have today can do Arc AGI. [[01:15:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4534.719999999999s)]
*  Three years from now, [[01:15:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4537.76s)]
*  it can do much more complicated problems, [[01:15:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4538.599999999999s)]
*  but the cost is gonna be measured in thousands and thousands [[01:15:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4540.5s)]
*  and hundreds of thousands of dollars of GPU time. [[01:15:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4543.08s)]
*  And there just won't be enough power, [[01:15:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4545.96s)]
*  GPUs, infrastructure to operate this [[01:15:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4547.36s)]
*  and therefore shift everything in the world [[01:15:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4549.8s)]
*  on the snap of the finger. [[01:15:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4551.4s)]
*  But at that moment, who gets to control [[01:15:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4552.4s)]
*  and point the AGI at a task? [[01:15:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4555.96s)]
*  And so this was in Dario's post that he's like, [[01:15:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4558.84s)]
*  hey, China can effectively and more quickly than us [[01:16:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4561.04s)]
*  point their AGI at military tasks, right? [[01:16:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4564.36s)]
*  And they have been in many ways [[01:16:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4567.4s)]
*  faster at adopting certain new technologies [[01:16:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4569.0s)]
*  into their military, right? [[01:16:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4570.92s)]
*  Especially with regards to drones, right? [[01:16:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4573.4800000000005s)]
*  The US maybe has a long-standing, you know, [[01:16:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4575.56s)]
*  large air sort of, you know, fighter jet type of thing, [[01:16:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4577.76s)]
*  bombers, but when it comes to asymmetric arms, [[01:16:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4581.16s)]
*  such as drones, they've completely leapfrogged [[01:16:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4583.56s)]
*  the US and the West. [[01:16:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4587.0s)]
*  And the fear that Dario is sort of pointing out there, [[01:16:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4588.200000000001s)]
*  I think, is that, yeah, great, [[01:16:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4591.080000000001s)]
*  we'll have AGI in the commercial sector. [[01:16:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4593.160000000001s)]
*  The US military won't be able to implement it super fast. [[01:16:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4595.8s)]
*  Chinese military could, [[01:16:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4598.4400000000005s)]
*  and they could direct all their resources [[01:16:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4599.4800000000005s)]
*  to implementing it in the military [[01:16:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4601.280000000001s)]
*  and therefore solving, you know, military logistics [[01:16:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4602.84s)]
*  or solving some other aspect of like disinformation [[01:16:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4605.8s)]
*  for targeted certain set of people [[01:16:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4608.6s)]
*  so that they can flip a country's politics [[01:16:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4610.6s)]
*  or something like that that is actually like catastrophic [[01:16:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4612.4s)]
*  versus, you know, the US just wants to, you know, [[01:16:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4615.24s)]
*  because it'll be more capitalistically allocated [[01:16:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4617.72s)]
*  just towards whatever is the highest return on income, [[01:16:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4619.48s)]
*  which might be like building factories better or whatever. [[01:17:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4621.88s)]
*  So everything I've seen, [[01:17:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4624.76s)]
*  people's intuition seems to fail on robotics. [[01:17:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4626.96s)]
*  So you have this kind of general optimism. [[01:17:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4629.92s)]
*  I've seen this on self-driving cars. [[01:17:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4631.92s)]
*  People think it's much easier problem than it is. [[01:17:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4633.48s)]
*  Similar with drones. [[01:17:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4636.5199999999995s)]
*  Here, I understand it a little bit less, [[01:17:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4638.2s)]
*  but I've just seen the reality of the war in Ukraine [[01:17:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4640.28s)]
*  and the usage of drones on both sides. [[01:17:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4643.92s)]
*  And it seems that humans still far outperform [[01:17:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4646.8s)]
*  any fully autonomous systems. [[01:17:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4651.4s)]
*  AI is an assistant, but humans drive FPV drones [[01:17:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4654.08s)]
*  where the humans control most of it, [[01:17:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4658.92s)]
*  just far, far, far outperforms AI systems. [[01:17:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4660.5599999999995s)]
*  So I think it's not obvious to me [[01:17:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4663.599999999999s)]
*  that we're going to have swarms of autonomous robots [[01:17:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4665.599999999999s)]
*  anytime soon in the military context. [[01:17:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4668.76s)]
*  Maybe the fastest I can imagine is 2030, [[01:17:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4671.4400000000005s)]
*  which is why I said 2030 for the super powerful AI. [[01:17:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4674.84s)]
*  Whenever you have large scale swarms of robots [[01:17:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4678.56s)]
*  doing military actions, [[01:18:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4682.56s)]
*  that's when the world just starts to look different to me. [[01:18:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4683.84s)]
*  So that's the thing I'm really worried about. [[01:18:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4687.68s)]
*  But there could be cyber war, [[01:18:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4689.68s)]
*  a cyber war type of technologies that, [[01:18:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4692.320000000001s)]
*  from social engineering to actually just swarms of robots [[01:18:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4694.76s)]
*  that find attack vectors in our code bases [[01:18:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4698.16s)]
*  and shut down power grids, that kind of stuff. [[01:18:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4701.5599999999995s)]
*  And it could be one of those things [[01:18:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4704.76s)]
*  like on any given weekend or something, power goes out, [[01:18:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4705.92s)]
*  nobody knows why, and the world changes forever. [[01:18:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4709.96s)]
*  Just power going out for two days [[01:18:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4713.48s)]
*  in all of the United States, [[01:18:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4716.0s)]
*  that will lead to murder, to chaos. [[01:18:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4717.8s)]
*  But going back to expert controls, [[01:18:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4720.92s)]
*  do you see that as a useful way [[01:18:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4725.2s)]
*  to control the balance of power geopolitically [[01:18:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4728.88s)]
*  in the context of AI? [[01:18:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4734.679999999999s)]
*  And I think going back to my viewpoint is, [[01:18:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4736.16s)]
*  if you believe we're in the sort of [[01:18:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4738.24s)]
*  stage of economic growth and change [[01:19:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4741.0s)]
*  that we've been in for the last 20 years, [[01:19:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4743.4s)]
*  the expert controls are absolutely guaranteeing [[01:19:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4745.12s)]
*  that China will win long-term, right? [[01:19:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4748.72s)]
*  If you do not believe AI is going to make [[01:19:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4751.08s)]
*  significant changes to society [[01:19:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4753.48s)]
*  in the next 10 years or five years, right? [[01:19:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4755.719999999999s)]
*  Five-year timelines are sort of what the more executives [[01:19:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4758.16s)]
*  and such of AI companies and even big tech companies believe, [[01:19:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4760.959999999999s)]
*  but even 10-year timelines, it's reasonable. [[01:19:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4764.0s)]
*  But once you get to, hey, these timelines [[01:19:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4766.599999999999s)]
*  are below that time period, [[01:19:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4770.12s)]
*  then the only way to sort of create a sizable advantage [[01:19:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4773.48s)]
*  or disadvantage for America versus China [[01:19:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4778.24s)]
*  is if you constrain compute, [[01:19:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4780.84s)]
*  because talent is not really something [[01:19:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4783.52s)]
*  that's constraining, right? [[01:19:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4786.6s)]
*  China arguably has more talent, right? [[01:19:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4787.84s)]
*  More STEM graduates, more programmers. [[01:19:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4789.56s)]
*  The US can draw upon the world's people, which it does. [[01:19:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4791.88s)]
*  There's tons of foreigners in the AI industry. [[01:19:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4794.4800000000005s)]
*  So many of these AI teams are all people [[01:19:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4797.28s)]
*  without a US passport. [[01:19:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4799.88s)]
*  Yeah, I mean, many of them are Chinese people [[01:20:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4802.4400000000005s)]
*  who are moving to America, right? [[01:20:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4805.08s)]
*  And that's great. [[01:20:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4806.400000000001s)]
*  That's exactly what we want, right? [[01:20:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4807.22s)]
*  But that talent is one aspect, [[01:20:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4809.0s)]
*  but I don't think that's one that is a measurable advantage [[01:20:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4811.76s)]
*  for the US or not. [[01:20:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4814.04s)]
*  It truly is just whether or not compute, right? [[01:20:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4815.2s)]
*  Now, even on the compute side, [[01:20:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4818.08s)]
*  when we look at chips versus data centers, right? [[01:20:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4819.96s)]
*  China has the unprecedented ability [[01:20:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4822.6s)]
*  to build ridiculous sums of power, clockwork, right? [[01:20:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4825.48s)]
*  They're always building more and more power. [[01:20:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4829.68s)]
*  They've got steel mills that individually [[01:20:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4831.86s)]
*  are the size of the entire US industry, right? [[01:20:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4835.8s)]
*  And they've got aluminum mills that consume gigawatts [[01:20:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4837.68s)]
*  and gigawatts of power, right? [[01:20:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4840.280000000001s)]
*  And when we talk about what's the biggest data center, [[01:20:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4841.96s)]
*  right, opening, I made this huge thing [[01:20:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4844.04s)]
*  about Stargate, their announcement there. [[01:20:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4846.08s)]
*  That's like once it's fully built out in a few years, [[01:20:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4848.200000000001s)]
*  it'll be two gigawatts of power, right? [[01:20:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4851.64s)]
*  And this is still smaller than the largest [[01:20:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4854.4400000000005s)]
*  industrial facilities in China, right? [[01:20:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4857.08s)]
*  China, if they wanted to build the largest data center [[01:20:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4858.92s)]
*  in the world, if they had access to the chips, could. [[01:21:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4861.0s)]
*  So it's just a question of when, not if, right? [[01:21:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4864.18s)]
*  So their industrial capacity far exceeds the United States? [[01:21:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4867.68s)]
*  Exactly. [[01:21:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4870.6s)]
*  To manufacture stuff. [[01:21:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4871.4400000000005s)]
*  Yeah. [[01:21:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4873.04s)]
*  So long-term, they're going to be manufacturing chips there. [[01:21:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4873.88s)]
*  Chips are a little bit more specialized. [[01:21:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4878.52s)]
*  I'm specifically referring to the data centers, right? [[01:21:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4880.42s)]
*  Chips, fabs take huge amounts of power, [[01:21:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4882.3s)]
*  don't get me wrong. [[01:21:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4884.200000000001s)]
*  That's not necessarily the gating factor there. [[01:21:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4885.4400000000005s)]
*  The gating factor on how fast people can build [[01:21:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4887.64s)]
*  the largest clusters today in the US is power, right? [[01:21:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4890.34s)]
*  It is whether it's, now it could be power generation, [[01:21:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4893.780000000001s)]
*  power transmission, substations, [[01:21:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4896.2s)]
*  and all these sorts of transformers and all these things, [[01:21:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4898.28s)]
*  building the data center. [[01:21:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4902.48s)]
*  These are all constraints on the US industry's ability [[01:21:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4903.32s)]
*  to build larger and larger training systems, [[01:21:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4906.639999999999s)]
*  as well as deploying more and more inference compute. [[01:21:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4909.16s)]
*  I think we need to make the point clear [[01:21:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4912.04s)]
*  on why the time is now for people [[01:21:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4913.4s)]
*  that don't think about this. [[01:21:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4915.92s)]
*  Because essentially with export controls, [[01:21:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4917.24s)]
*  you're making it so China cannot make or get [[01:21:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4918.8s)]
*  cutting edge chips. [[01:22:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4921.82s)]
*  And the idea is that if you time this wrong, [[01:22:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4922.96s)]
*  China is pouring a ton of money into their chip production. [[01:22:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4926.24s)]
*  And if you time it wrong, [[01:22:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4929.12s)]
*  they are going to have more capacity for production, [[01:22:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4930.12s)]
*  more capacity for energy, [[01:22:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4932.24s)]
*  and figure out how to make the chips [[01:22:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4933.92s)]
*  and have more capacity than the rest of the world [[01:22:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4935.4400000000005s)]
*  to make the chips, [[01:22:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4937.36s)]
*  because everybody can buy, [[01:22:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4938.2s)]
*  they're gonna sell their Chinese chips to everybody, [[01:22:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4939.56s)]
*  they might subsidize them. [[01:22:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4941.52s)]
*  And therefore, if AI takes a long time [[01:22:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4943.0s)]
*  to become differentiated, [[01:22:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4945.4s)]
*  we've decapped the financial performance [[01:22:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4946.52s)]
*  of American companies. [[01:22:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4948.58s)]
*  Nvidia can sell less, [[01:22:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4949.76s)]
*  TSMC cannot sell to China. [[01:22:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4951.64s)]
*  So therefore we have less demand [[01:22:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4953.88s)]
*  to therefore keep driving the production cycle. [[01:22:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4956.8s)]
*  So that's the assumption behind the time, [[01:22:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4960.160000000001s)]
*  timing being important. [[01:22:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4962.280000000001s)]
*  Less than 10 years or five years to above, right? [[01:22:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4963.280000000001s)]
*  China will win because of these restrictions long-term, [[01:22:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4965.9400000000005s)]
*  unless AI does something in the short-term, [[01:22:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4969.08s)]
*  which I believe AI will do, [[01:22:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4971.84s)]
*  make massive changes to society [[01:22:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4974.02s)]
*  in the medium short-term, right? [[01:22:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4975.46s)]
*  And so that's the big unlocker there. [[01:22:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4977.780000000001s)]
*  And even today, right? [[01:23:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4980.92s)]
*  If Xi Jinping decided to get, [[01:23:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4982.12s)]
*  quote unquote, scale-pilled, right? [[01:23:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4984.6s)]
*  I.E., decide that scaling laws are what matters, right? [[01:23:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4986.88s)]
*  Just like the US executives like Sacha Nadella [[01:23:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4990.72s)]
*  and Mark Zuckerberg and Sundar, [[01:23:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4993.28s)]
*  and all these US executives [[01:23:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4995.96s)]
*  of the biggest, most powerful tech companies [[01:23:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4997.34s)]
*  have decided they're scale-pilled, [[01:23:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=4999.84s)]
*  and they're building multi-gigawatt data centers, right? [[01:23:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5001.08s)]
*  Whether it's in Texas or Louisiana or Wisconsin, [[01:23:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5003.5199999999995s)]
*  wherever it is, they're building these massive things [[01:23:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5005.88s)]
*  that cost as much as their entire budget [[01:23:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5008.88s)]
*  for spending on data centers globally in one spot, right? [[01:23:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5011.72s)]
*  This is what they've committed to [[01:23:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5014.92s)]
*  for next year, year after, et cetera. [[01:23:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5016.02s)]
*  And so they're so convinced that this is the way, [[01:23:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5017.96s)]
*  that this is what they're doing. [[01:23:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5022.2s)]
*  But if China decided to, they could do it faster than us, [[01:23:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5023.38s)]
*  but this is where the restrictions come in. [[01:23:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5026.5s)]
*  It is not clear that China as a whole has decided [[01:23:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5029.0s)]
*  from the highest levels that this is a priority. [[01:23:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5032.66s)]
*  The US sort of has, right? [[01:23:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5034.54s)]
*  You see Trump talking about DeepSeq [[01:23:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5036.4400000000005s)]
*  and Stargate within the same week, right? [[01:23:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5038.04s)]
*  So he's, and the Biden admin as well [[01:24:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5040.64s)]
*  had a lot of discussions about AI and such. [[01:24:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5042.16s)]
*  It's clear that they think about it. [[01:24:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5045.12s)]
*  Only just last week did DeepSeq meet [[01:24:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5046.94s)]
*  the second-in-command of China, right? [[01:24:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5050.16s)]
*  Like they have not even met the top, right? [[01:24:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5052.16s)]
*  Never met Xi, Xi hasn't sat down. [[01:24:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5053.92s)]
*  And they only just released a subsidy of a trillion RMB, [[01:24:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5056.36s)]
*  roughly $160 billion, [[01:24:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5061.04s)]
*  which is closer to the spending of Microsoft and Meta [[01:24:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5063.4s)]
*  and Google combined, right, for this year. [[01:24:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5067.18s)]
*  So it's like, they're realizing it just now, [[01:24:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5069.26s)]
*  but that's where these export restrictions come in [[01:24:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5072.4800000000005s)]
*  and say, hey, you can't ship [[01:24:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5074.700000000001s)]
*  the most powerful US chips to China. [[01:24:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5076.9800000000005s)]
*  You can ship a cut-down version. [[01:24:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5079.200000000001s)]
*  You can't ship the most powerful chips [[01:24:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5080.820000000001s)]
*  to all these countries who we know [[01:24:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5084.14s)]
*  we're just gonna rent it to China. [[01:24:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5085.4800000000005s)]
*  You have to limit the numbers, right? [[01:24:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5087.320000000001s)]
*  And the tools. [[01:24:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5088.62s)]
*  And same with manufacturing equipment, tools, [[01:24:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5089.46s)]
*  all these different aspects. [[01:24:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5092.3s)]
*  But it all stems from AI, [[01:24:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5094.08s)]
*  and then what downstream can slow them down in AI. [[01:24:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5095.26s)]
*  And so the entire semiconductor restrictions, [[01:24:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5098.38s)]
*  you read them, they are very clear. [[01:25:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5100.780000000001s)]
*  It's about AI and military-civil fusion of technology, [[01:25:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5102.54s)]
*  right? [[01:25:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5105.9400000000005s)]
*  It's very clear. [[01:25:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5106.780000000001s)]
*  And then from there it goes, [[01:25:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5107.6s)]
*  oh, well, we're banning them from buying [[01:25:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5108.4400000000005s)]
*  like lithography tools and etch tools and deposition tools. [[01:25:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5109.860000000001s)]
*  And oh, this random like subsystem [[01:25:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5112.96s)]
*  from a random company that's like tiny, right? [[01:25:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5115.38s)]
*  Like, why are we banning this? [[01:25:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5117.62s)]
*  Because all of it, the US government has decided [[01:25:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5118.54s)]
*  is critical to AI systems. [[01:25:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5121.1s)]
*  I think the fulcrum point is like the transition [[01:25:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5123.58s)]
*  from seven nanometer to five nanometer chips, [[01:25:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5126.62s)]
*  where I think it was Huawei [[01:25:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5128.58s)]
*  that had the seven nanometer chip a few years ago, [[01:25:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5129.78s)]
*  which caused another political brouhaha, [[01:25:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5132.76s)]
*  almost like this moment. [[01:25:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5135.5s)]
*  And then it's the ASML deep UV, what is that? [[01:25:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5137.04s)]
*  Extreme ultraviolet lithography. [[01:25:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5141.1s)]
*  Just that context on the chips, right? [[01:25:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5143.48s)]
*  What Nathan's referring to is in 2020, [[01:25:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5144.84s)]
*  Huawei released their Ascend 910 chip, [[01:25:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5147.22s)]
*  which was an AI chip, first one on seven nanometer, [[01:25:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5150.38s)]
*  before Google did, before Nvidia did. [[01:25:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5153.1s)]
*  And they submitted it to the MLPerf benchmark, [[01:25:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5155.3s)]
*  which is sort of a industry standard [[01:25:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5157.5s)]
*  for machine learning performance benchmark. [[01:25:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5159.620000000001s)]
*  And it did quite well. [[01:26:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5161.9800000000005s)]
*  And it was the best chip at the submission, right? [[01:26:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5163.3s)]
*  This was a huge deal. [[01:26:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5165.54s)]
*  The Trump admin, of course, banned, it was 2019, right? [[01:26:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5168.26s)]
*  Banned the Huawei from getting seven nanometer chips [[01:26:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5171.820000000001s)]
*  from TSMC, and so then they had to switch [[01:26:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5174.38s)]
*  to using internal domestically produced chips, [[01:26:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5176.5s)]
*  which was a multi-year setback. [[01:26:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5178.88s)]
*  Many companies have done seven nanometer chips. [[01:26:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5180.18s)]
*  And the question is, we don't know how much Huawei [[01:26:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5182.44s)]
*  was subsidizing production of that chip. [[01:26:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5185.24s)]
*  Intel has made seven nanometer chips [[01:26:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5187.12s)]
*  that are not profitable, and things like this. [[01:26:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5189.32s)]
*  So this is how it all feeds back into the economic engine [[01:26:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5191.679999999999s)]
*  of export controls. [[01:26:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5194.839999999999s)]
*  Well, so you're saying that for now, [[01:26:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5196.139999999999s)]
*  Xi Jinping has not felt the AGI, [[01:26:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5198.24s)]
*  but it feels like the deep-seek moment might, [[01:26:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5201.28s)]
*  there might be meetings going on now [[01:26:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5205.799999999999s)]
*  where he's gonna start wearing the same T-shirt, [[01:26:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5207.679999999999s)]
*  and things are gonna escalate. [[01:26:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5210.719999999999s)]
*  He may have woken up last week, right? [[01:26:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5213.24s)]
*  Leon Feng met the vice chairman, the second command guy, [[01:26:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5215.5599999999995s)]
*  and they had a meeting, and then the next day, [[01:26:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5219.08s)]
*  they announced the AI subsidies, which are a trillion RMB. [[01:27:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5222.12s)]
*  So it's possible that this deep-seek moment [[01:27:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5225.679999999999s)]
*  is truly the beginning of a Cold War. [[01:27:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5227.799999999999s)]
*  That's what a lot of people are worried about. [[01:27:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5230.719999999999s)]
*  People in AI have been worried that this is going [[01:27:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5232.28s)]
*  towards a Cold War, or already is. [[01:27:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5234.799999999999s)]
*  But it's not deep-seek's fault, but there's something, [[01:27:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5236.36s)]
*  a bunch of factors came together where it was actually [[01:27:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5239.679999999999s)]
*  an explosion. [[01:27:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5242.4s)]
*  I mean, it all has to do with Nvidia [[01:27:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5243.24s)]
*  starting going down, probably. [[01:27:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5244.44s)]
*  But it's just some mass hysteria that happened [[01:27:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5245.6s)]
*  that eventually led to Xi Jinping having meetings [[01:27:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5250.52s)]
*  and waking up to this idea. [[01:27:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5253.32s)]
*  And the US government realized, in October 7th, 2022, [[01:27:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5254.84s)]
*  before chat GPT released, that restriction on October 7th, [[01:27:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5258.5s)]
*  which dropped, and shocked everyone, [[01:27:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5262.24s)]
*  and it was very clearly aimed at AI. [[01:27:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5264.24s)]
*  Everyone was like, what the heck are you doing? [[01:27:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5265.88s)]
*  Stable diffusion was out then, but not chat GPT. [[01:27:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5268.16s)]
*  Yeah, but not chat GPT. [[01:27:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5270.52s)]
*  So it was starting to be rumblings. [[01:27:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5271.5199999999995s)]
*  Of what gen AI can do to society. [[01:27:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5273.4s)]
*  But it was very clear, I think, to at least [[01:27:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5275.719999999999s)]
*  National Security Council and those sort of folks, [[01:27:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5278.16s)]
*  that this was where the world is headed, [[01:28:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5280.96s)]
*  this Cold War that's happening. [[01:28:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5283.16s)]
*  So is there any concerns that the export controls [[01:28:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5284.839999999999s)]
*  push China to take military action on Taiwan? [[01:28:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5290.339999999999s)]
*  This is the big risk, right? [[01:28:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5295.48s)]
*  The further you push China away from having access [[01:28:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5297.04s)]
*  to cutting edge American and global technologies, [[01:28:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5299.6s)]
*  the more likely they are to say, [[01:28:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5303.280000000001s)]
*  well, because I can't access it, [[01:28:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5304.88s)]
*  I might as well, no one should access it, right? [[01:28:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5306.280000000001s)]
*  And there's a few interesting aspects of that. [[01:28:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5308.88s)]
*  China has a urban rural divide like no other. [[01:28:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5312.400000000001s)]
*  They have a male female birth ratio like no other, [[01:28:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5317.08s)]
*  to the point where if you look in most of China, [[01:28:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5320.04s)]
*  it's like the ratio is not that bad, [[01:28:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5322.4400000000005s)]
*  but when you look at single dudes in rural China, [[01:28:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5323.5s)]
*  it's like a 30 to one ratio. [[01:28:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5325.280000000001s)]
*  And those are disenfranchised dudes, right? [[01:28:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5327.76s)]
*  Quote unquote, the US has an in-cell problem, [[01:28:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5330.28s)]
*  like China does too, it's just they're placated in some way [[01:28:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5332.36s)]
*  or cut, crushed down. [[01:28:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5335.16s)]
*  What do you do with these people? [[01:28:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5336.5599999999995s)]
*  And at the same time, you're not able to access [[01:28:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5337.96s)]
*  the most important technology, at least the US thinks so. [[01:28:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5339.719999999999s)]
*  China's maybe starting to think [[01:29:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5342.839999999999s)]
*  this is the most important technology [[01:29:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5344.08s)]
*  by starting to dump subsidies in it, right? [[01:29:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5345.839999999999s)]
*  They thought EVs and renewables [[01:29:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5347.36s)]
*  were the most important technology. [[01:29:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5348.639999999999s)]
*  They dominate that now, right? [[01:29:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5349.919999999999s)]
*  Now they're starting to, they started thinking about [[01:29:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5351.5199999999995s)]
*  about semiconductors in the late 2010s and early 2020s. [[01:29:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5353.28s)]
*  And now they've been dumping money [[01:29:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5357.759999999999s)]
*  and they're catching up rapidly [[01:29:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5359.0s)]
*  and they're gonna do the same with AI, right? [[01:29:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5361.16s)]
*  Because they're very talented, right? [[01:29:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5362.68s)]
*  So the question is like, [[01:29:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5364.28s)]
*  when does this hit a breaking point, right? [[01:29:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5367.6s)]
*  And if China sees this as, hey, they can continue, [[01:29:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5372.28s)]
*  if not having access and starting a true hot war, right? [[01:29:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5375.96s)]
*  Taking over Taiwan or trying to subvert its democracy [[01:29:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5379.8s)]
*  in some way or blockading it hurts the rest of the world [[01:29:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5382.44s)]
*  far more than it hurts them. [[01:29:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5386.28s)]
*  This is something they could potentially do, right? [[01:29:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5388.04s)]
*  And so is this pushing them towards that? [[01:29:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5390.28s)]
*  Potentially, right? [[01:29:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5393.32s)]
*  I'm not quite a geopolitical person, but it's obvious [[01:29:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5394.16s)]
*  that the world regime of peace and trade [[01:29:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5398.44s)]
*  is super awesome for economics, [[01:30:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5401.6s)]
*  but at some point it could break, right? [[01:30:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5405.12s)]
*  I think we should comment that [[01:30:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5407.08s)]
*  why Chinese economy would be hurt by that [[01:30:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5408.52s)]
*  is that they're export heavy. [[01:30:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5410.72s)]
*  I think the United States buys so much, [[01:30:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5411.88s)]
*  like if that goes away, that's how their economy falls. [[01:30:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5414.2s)]
*  Also, they just would not be able to import raw materials [[01:30:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5417.0s)]
*  from all over the world, right? [[01:30:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5420.16s)]
*  The US would just shut down the trade in Malacca. [[01:30:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5421.68s)]
*  And at the same time, the US entirely, [[01:30:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5423.88s)]
*  you could argue almost all the GDP growth in America [[01:30:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5426.84s)]
*  since the 70s has been either population growth or tech, [[01:30:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5429.56s)]
*  right? [[01:30:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5434.16s)]
*  Because your life today is not that much better [[01:30:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5435.48s)]
*  than someone from the 80s outside of tech, right? [[01:30:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5439.28s)]
*  You still, cars, they all have semiconductors [[01:30:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5441.64s)]
*  in them everywhere, fridges, semiconductors everywhere. [[01:30:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5444.56s)]
*  There's these funny stories about how Russians [[01:30:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5446.28s)]
*  were taking apart laundry machines [[01:30:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5448.0s)]
*  because they had certain like Texas instrument chips [[01:30:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5450.0s)]
*  that they could then repurpose and put into like [[01:30:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5452.12s)]
*  their anti-missile things, right? [[01:30:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5454.36s)]
*  Like their S-400 or whatever. [[01:30:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5457.5599999999995s)]
*  You would know more about this, but there's all sorts of like [[01:30:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5459.2s)]
*  everything about semiconductors is so integral [[01:31:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5462.719999999999s)]
*  to every part of our lives. [[01:31:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5464.679999999999s)]
*  So can you explain the role of TSMC [[01:31:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5466.24s)]
*  in the story of semiconductors and maybe also [[01:31:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5469.5599999999995s)]
*  how the United States can break the reliance on TSMC? [[01:31:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5473.24s)]
*  I don't think it's necessarily breaking the reliance. [[01:31:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5477.08s)]
*  I think it's getting TSMC to build in the US. [[01:31:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5479.5199999999995s)]
*  But so taking a step back, right? [[01:31:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5484.2s)]
*  TSMC produces most of the world's chips, right? [[01:31:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5486.8s)]
*  Especially on the foundry side. [[01:31:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5490.84s)]
*  There's a lot of companies that build their own chips, [[01:31:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5493.16s)]
*  Samsung, Intel, STMicro, Texas Instruments, analog devices, [[01:31:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5495.72s)]
*  all these kinds of companies build their own chips, [[01:31:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5501.64s)]
*  and XP, but more and more of these companies [[01:31:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5503.6s)]
*  are outsourcing to TSMC and have been for multiple decades. [[01:31:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5505.96s)]
*  Can you explain the supply chain there [[01:31:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5509.72s)]
*  and where most of TSMC is in terms of manufacturing? [[01:31:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5511.52s)]
*  Sure, so historically supply chain was companies [[01:31:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5515.08s)]
*  would build their own chips. [[01:31:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5518.08s)]
*  They would be a company started, [[01:31:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5519.360000000001s)]
*  they'd build their own chips, and then they'd design the chip [[01:32:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5521.360000000001s)]
*  and build the chip and sell it. [[01:32:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5524.0s)]
*  Over time, this became really difficult [[01:32:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5526.04s)]
*  because the cost of building a fab [[01:32:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5528.4400000000005s)]
*  continues to compound every single generation. [[01:32:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5530.0s)]
*  Of course, figuring out the technology for it [[01:32:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5532.2s)]
*  is incredibly difficult regardless, [[01:32:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5534.4800000000005s)]
*  but just the dollars and cents that are required, [[01:32:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5536.4s)]
*  ignoring, saying, hey, yes, [[01:32:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5538.84s)]
*  I have all the technical capability, [[01:32:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5540.32s)]
*  which it's really hard to get that by the way, right? [[01:32:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5541.68s)]
*  Intel's failing, Samsung's failing, et cetera. [[01:32:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5543.44s)]
*  But if you look at just the dollars to spend [[01:32:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5546.08s)]
*  to build that next generation fab, it keeps growing, right? [[01:32:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5548.6s)]
*  Sort of like Moore's Law is having the cost of chips [[01:32:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5550.96s)]
*  every two years, there's a separate law [[01:32:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5553.28s)]
*  that's sort of like doubling the cost of fabs [[01:32:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5555.24s)]
*  every handful of years. [[01:32:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5556.88s)]
*  And so you look at a leading edge fab [[01:32:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5558.12s)]
*  that is gonna be profitable today, [[01:32:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5560.5599999999995s)]
*  that's building three nanometer chips [[01:32:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5562.0s)]
*  or two nanometer chips in the future, [[01:32:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5563.44s)]
*  that's gonna cost north of 30, 40 billion dollars, right? [[01:32:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5565.04s)]
*  And that's just for a token amount, [[01:32:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5568.68s)]
*  that's like the base building block, [[01:32:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5570.64s)]
*  you probably need to build multiple, right? [[01:32:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5572.72s)]
*  And so when you look at the industry, [[01:32:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5574.2s)]
*  over the last, if I go back 20, 30 years ago, [[01:32:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5576.72s)]
*  there were 20, 30 companies that could build [[01:32:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5579.76s)]
*  the most advanced chips, [[01:33:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5581.599999999999s)]
*  and then they would design them themselves and sell them. [[01:33:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5582.88s)]
*  So companies like AMD would build their own chips. [[01:33:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5584.84s)]
*  Intel, of course, still builds their own chips, [[01:33:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5587.36s)]
*  they're very famous for it, [[01:33:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5588.88s)]
*  but IBM would build their own chips, [[01:33:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5589.719999999999s)]
*  and you could just keep going down the list, [[01:33:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5590.679999999999s)]
*  all these companies built their own chips. [[01:33:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5592.92s)]
*  Slowly, they kept falling like flies, [[01:33:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5594.5199999999995s)]
*  and that's because of what TSMC did, right? [[01:33:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5596.679999999999s)]
*  They created the Foundry business model, [[01:33:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5599.04s)]
*  which is, I'm not gonna design any chips, [[01:33:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5601.04s)]
*  I'm just gonna contract, manufacture chips [[01:33:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5603.0s)]
*  for other people. [[01:33:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5604.96s)]
*  And one of their early customers is Nvidia, right? [[01:33:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5607.0s)]
*  Nvidia is the only semiconductor company [[01:33:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5608.799999999999s)]
*  that's doing more than a billion dollars of revenue [[01:33:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5614.12s)]
*  that was started in the era of Foundry, right? [[01:33:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5615.96s)]
*  Every other company started before then, [[01:33:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5619.12s)]
*  and at some point had fabs, [[01:33:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5620.6s)]
*  which is actually incredible, right? [[01:33:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5621.92s)]
*  Like AMD and Intel and Broadcom. [[01:33:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5624.32s)]
*  Such a great fact. [[01:33:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5626.72s)]
*  It's like everyone had fabs at some point, [[01:33:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5627.56s)]
*  or some companies like Broadcom, [[01:33:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5629.96s)]
*  it was like a merger, amalgamation [[01:33:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5631.84s)]
*  of various companies that rolled up, [[01:33:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5633.32s)]
*  but even today Broadcom has fabs, right? [[01:33:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5635.04s)]
*  They build iPhone RF radio chips [[01:33:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5636.72s)]
*  sort of in Colorado for Apple, right? [[01:33:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5639.92s)]
*  All these companies had fabs, [[01:34:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5643.6s)]
*  and for most of the fabs they threw them away [[01:34:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5644.92s)]
*  or sold them off or they got rolled into something else. [[01:34:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5646.72s)]
*  And now everyone relies on TSMC, right? [[01:34:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5649.4800000000005s)]
*  Including Intel, their latest PC chip uses TSMC chips, right? [[01:34:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5651.72s)]
*  It also uses some Intel chips, [[01:34:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5656.2s)]
*  but it uses TSMC process. [[01:34:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5657.36s)]
*  Can you explain why the Foundry model [[01:34:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5659.12s)]
*  is so successful for these companies? [[01:34:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5660.84s)]
*  Why are they going with TSMC? [[01:34:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5663.28s)]
*  Economies of scale. [[01:34:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5664.6s)]
*  Scale. [[01:34:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5666.16s)]
*  Yeah, so I mean, like I mentioned, right? [[01:34:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5667.0s)]
*  The cost of building a fab is so high. [[01:34:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5668.4400000000005s)]
*  The R&D is so difficult. [[01:34:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5670.16s)]
*  And when you look at like these, [[01:34:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5672.52s)]
*  the companies that had their own vertical stack, [[01:34:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5674.88s)]
*  there was an antiquated process of like, [[01:34:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5677.200000000001s)]
*  okay, like I'm so hyper customized to each specific chip. [[01:34:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5679.240000000001s)]
*  Right, but as we've gone through the history [[01:34:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5682.4800000000005s)]
*  of sort of like the last 50 years [[01:34:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5684.120000000001s)]
*  of electronics and semiconductors, [[01:34:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5685.4400000000005s)]
*  A, you need more and more specialization, right? [[01:34:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5687.64s)]
*  Because Moore's law has died. [[01:34:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5689.56s)]
*  Dennard scaling has died. [[01:34:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5691.6s)]
*  I.e. chips are not getting better just for free, right? [[01:34:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5692.88s)]
*  From manufacturing, you have to make [[01:34:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5695.68s)]
*  real architectural innovations, right? [[01:34:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5697.64s)]
*  Google is not just running on Intel CPUs for web serving. [[01:34:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5699.400000000001s)]
*  They have a YouTube chip, they have TPUs, [[01:35:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5702.8s)]
*  they have pixel chips, they have a wide diversity of chips [[01:35:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5703.92s)]
*  that generate all the economic value of Google, right? [[01:35:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5706.88s)]
*  Running, you know, it's running all the services and stuff. [[01:35:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5710.599999999999s)]
*  And so, and this is just Google, [[01:35:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5712.8s)]
*  and you could go across any company in the industry, [[01:35:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5714.0s)]
*  and it's like this, right? [[01:35:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5715.68s)]
*  Cars contain 5,000 chips, you know, [[01:35:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5716.599999999999s)]
*  200 different varieties of them, right? [[01:35:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5719.5199999999995s)]
*  All these random things. [[01:35:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5721.24s)]
*  A Tesla door handle has two chips, right? [[01:35:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5722.08s)]
*  Like it's like ridiculous. [[01:35:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5723.8s)]
*  And it's a cool door handle, right? [[01:35:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5725.0s)]
*  It's like, you know, you don't think about it, [[01:35:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5726.44s)]
*  but it's like, has two really chip, [[01:35:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5727.5199999999995s)]
*  like penny like chips in there, right? [[01:35:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5729.0s)]
*  Anyway, so as you have more diversity of chips, [[01:35:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5731.64s)]
*  as you have more specialization required, [[01:35:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5734.52s)]
*  and the cost of fabs continues to grow, [[01:35:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5736.4400000000005s)]
*  you need someone who is laser focused on [[01:35:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5738.400000000001s)]
*  building the best process technology [[01:35:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5741.120000000001s)]
*  and making it as flexible as possible. [[01:35:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5743.360000000001s)]
*  I think you could say it simply, [[01:35:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5745.92s)]
*  which is the cost for fab goes up. [[01:35:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5747.0s)]
*  And if you are a small player [[01:35:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5749.72s)]
*  that makes a few types of chips, [[01:35:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5751.4800000000005s)]
*  you're not gonna have the demand [[01:35:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5753.240000000001s)]
*  to pay back the cost of the fab. [[01:35:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5754.84s)]
*  Whereas Nvidia can have many different customers [[01:35:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5756.64s)]
*  and aggregate all this demand, [[01:35:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5759.08s)]
*  into one place. [[01:36:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5761.2s)]
*  And then they're the only person [[01:36:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5762.04s)]
*  that makes enough money building chips [[01:36:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5763.24s)]
*  to build the next fab. [[01:36:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5765.84s)]
*  So this is kind of why the companies slowly get killed [[01:36:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5768.76s)]
*  because they have 10 years ago, [[01:36:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5771.24s)]
*  a chip that is profitable and is good enough, [[01:36:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5774.32s)]
*  but the cost to build the next one goes up. [[01:36:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5776.599999999999s)]
*  They may try to do this fail [[01:36:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5778.5199999999995s)]
*  because they don't have the money to make it work. [[01:36:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5780.599999999999s)]
*  And then they don't have any chips [[01:36:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5782.2s)]
*  or they build it and it's too expensive. [[01:36:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5783.28s)]
*  And they just have not profitable chips. [[01:36:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5784.88s)]
*  There's more failure points, right? [[01:36:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5787.04s)]
*  You could have one little process [[01:36:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5788.88s)]
*  related to like some sort of like chemical etch [[01:36:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5790.48s)]
*  or some sort of like plasma etch [[01:36:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5793.5599999999995s)]
*  or some little process that screws up, [[01:36:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5795.16s)]
*  you didn't engineer it right. [[01:36:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5797.5599999999995s)]
*  And now the whole company falls apart, [[01:36:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5798.919999999999s)]
*  you can't make chips, right? [[01:36:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5800.36s)]
*  And so super, super powerful companies like Intel, [[01:36:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5801.36s)]
*  they had like the weathering storm to like, [[01:36:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5804.44s)]
*  hey, they still exist today, [[01:36:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5806.36s)]
*  even though they really screwed up their manufacturing [[01:36:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5807.44s)]
*  six, seven years ago. [[01:36:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5809.839999999999s)]
*  But in the case of like AMD, [[01:36:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5810.799999999999s)]
*  they almost went bankrupt. [[01:36:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5812.36s)]
*  They had to sell their fabs to Mubadala UAE, right? [[01:36:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5813.36s)]
*  And like that became a separate company [[01:36:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5817.639999999999s)]
*  called Global Foundries, which is a foundry firm. [[01:36:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5819.28s)]
*  And then AMD was able to then focus on like [[01:37:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5822.04s)]
*  on the return back up was like, [[01:37:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5824.16s)]
*  hey, let's focus on making chiplets [[01:37:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5825.4s)]
*  and a bunch of different chips for different markets [[01:37:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5827.4s)]
*  and focusing on specific workloads rather than, [[01:37:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5830.16s)]
*  all of these different things. [[01:37:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5833.2s)]
*  And so you get more diversity of chips, [[01:37:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5834.639999999999s)]
*  you have more companies than ever designing chips, [[01:37:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5836.12s)]
*  but you have fewer companies [[01:37:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5838.599999999999s)]
*  than ever manufacturing them, right? [[01:37:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5839.5599999999995s)]
*  And this is where TSMC comes in [[01:37:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5841.639999999999s)]
*  is they've just been the best, right? [[01:37:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5843.92s)]
*  They are so good at it, right? [[01:37:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5846.04s)]
*  They're customer focused, [[01:37:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5847.88s)]
*  they make it easy for you to fabricate your chips. [[01:37:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5849.72s)]
*  They take all of that complexity [[01:37:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5851.08s)]
*  and like kind of trying to abstract [[01:37:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5852.4s)]
*  a lot of it away from you. [[01:37:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5853.84s)]
*  They make good money, they don't make insane money, [[01:37:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5855.28s)]
*  but they make good money. [[01:37:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5857.2s)]
*  And they're able to aggregate all this demand [[01:37:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5859.12s)]
*  and continue to build the next fab, [[01:37:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5861.76s)]
*  the next fab, the next fab. [[01:37:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5863.12s)]
*  So why is Taiwan so special for TSMC? [[01:37:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5864.12s)]
*  Why is it happening there? [[01:37:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5866.76s)]
*  Can it be replicated inside the United States? [[01:37:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5868.96s)]
*  Yeah, so there's aspects of it that I would say yes [[01:37:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5871.8s)]
*  and aspects that I'd say no, right? [[01:37:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5874.16s)]
*  TSMC is way ahead because former executive, [[01:37:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5876.44s)]
*  Morris Chang of Texas Instruments wasn't promoted to CEO [[01:38:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5881.24s)]
*  and he was like, screw this, [[01:38:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5885.0s)]
*  I'm gonna go make my own chip company, right? [[01:38:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5885.839999999999s)]
*  And he went to Taiwan and made TSMC, right? [[01:38:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5887.879999999999s)]
*  And there's a whole lot more story there. [[01:38:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5889.5599999999995s)]
*  So it could have been Texas Instruments [[01:38:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5892.0s)]
*  could have been TSMC, [[01:38:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5893.5599999999995s)]
*  but Texas Semiconductor Manufacturing, right? [[01:38:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5895.28s)]
*  Instead of Texas Instruments, right? [[01:38:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5897.759999999999s)]
*  But so there is that whole story there. [[01:38:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5899.96s)]
*  But the race- [[01:38:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5901.719999999999s)]
*  Sitting here in Texas. [[01:38:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5902.5599999999995s)]
*  I mean, and that sounds like a human story. [[01:38:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5903.48s)]
*  Didn't get promoted. [[01:38:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5905.639999999999s)]
*  Just the brilliance of Morris Chang, [[01:38:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5906.679999999999s)]
*  which I wouldn't underplay, [[01:38:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5908.599999999999s)]
*  but there's also a different level of how this works, right? [[01:38:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5909.88s)]
*  So in Taiwan, [[01:38:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5913.839999999999s)]
*  the number top percent of graduates, [[01:38:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5917.839999999999s)]
*  of students that go to the best school, which is NTU, [[01:38:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5920.5599999999995s)]
*  the top percent of those all go work to TSMC, right? [[01:38:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5923.28s)]
*  And guess what their pay is? [[01:38:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5925.96s)]
*  Their starting pay is like $80,000, $70,000, right? [[01:38:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5927.5199999999995s)]
*  Which is like, that's like starting pay [[01:38:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5931.08s)]
*  for a good graduate in the US, right? [[01:38:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5933.0s)]
*  The top graduates are making hundreds of thousands of dollars [[01:38:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5935.400000000001s)]
*  at the Googles and the Amazons, [[01:38:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5938.28s)]
*  and now I guess the open AIs of the world, right? [[01:39:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5940.16s)]
*  So there is a large dichotomy of like, [[01:39:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5942.56s)]
*  what is the top 1% of the society doing? [[01:39:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5945.08s)]
*  And where are they headed because of economic reasons, right? [[01:39:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5947.52s)]
*  Intel never paid that crazy good, right? [[01:39:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5949.6s)]
*  And it didn't make sense to them, right? [[01:39:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5951.52s)]
*  That's one aspect, right? [[01:39:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5953.52s)]
*  Where's the best going? [[01:39:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5955.0s)]
*  Second is the work ethic, right? [[01:39:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5956.12s)]
*  Like, we like to work, you work a lot, we work a lot. [[01:39:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5957.400000000001s)]
*  But at the end of the day, [[01:39:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5961.64s)]
*  what is the time and amount of work that you're doing [[01:39:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5965.52s)]
*  and what does a fab require, right? [[01:39:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5967.52s)]
*  Fabs are not work from home jobs. [[01:39:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5969.12s)]
*  They are, you go into the fab and grueling work, right? [[01:39:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5970.64s)]
*  Hey, if there is any amount of vibration, right? [[01:39:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5975.52s)]
*  An earthquake happens, vibrates the machines. [[01:39:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5978.12s)]
*  They're all, you know, they're either broken, [[01:39:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5980.56s)]
*  you've scrapped some of your production, [[01:39:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5982.88s)]
*  and then in many cases, they're like not calibrated properly. [[01:39:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5984.76s)]
*  So when TSMC, when there's an earthquake, right? [[01:39:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5987.32s)]
*  Recently, there's been an earthquake. [[01:39:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5989.96s)]
*  TSMC doesn't call their employees. [[01:39:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5991.56s)]
*  They just go to the fab and like, [[01:39:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5993.4s)]
*  they just show up, the parking lot gets slammed [[01:39:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5996.28s)]
*  and people just go into the fab and fix it, right? [[01:39:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=5998.28s)]
*  Like, it's like ants, right? [[01:40:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6000.96s)]
*  Like, it's like, you know, a hive of ants [[01:40:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6002.639999999999s)]
*  doesn't get told by the queen what to do. [[01:40:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6004.36s)]
*  The ants just know. [[01:40:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6006.759999999999s)]
*  It's like one person just specializes on this one task. [[01:40:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6008.04s)]
*  And it's like, you're gonna take this one tool [[01:40:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6011.2s)]
*  and you're the best person in the world [[01:40:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6013.04s)]
*  and this is what you're gonna do for your whole life [[01:40:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6014.44s)]
*  is this one task in the fab. [[01:40:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6016.0s)]
*  Which is like some special chemistry plus nano manufacturing [[01:40:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6017.28s)]
*  on one line of tools that continues to get iterated [[01:40:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6020.5599999999995s)]
*  and yeah, it's just like, it's like specific plasma edge [[01:40:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6024.24s)]
*  for removing silicon dioxide, right? [[01:40:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6026.4s)]
*  That's all you focus on your whole career. [[01:40:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6028.0s)]
*  And it's like such a specialized thing. [[01:40:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6029.719999999999s)]
*  And so it's not like the task you're transferable. [[01:40:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6031.48s)]
*  AI today is awesome because like, [[01:40:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6034.2s)]
*  people can pick it up like that. [[01:40:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6035.799999999999s)]
*  Semiconductor manufacturing is very antiquated and difficult. [[01:40:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6037.719999999999s)]
*  None of the materials are online [[01:40:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6040.96s)]
*  for people to read easily and learn, right? [[01:40:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6042.16s)]
*  The papers are very dense and like, [[01:40:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6044.88s)]
*  it takes a lot of experience to learn. [[01:40:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6047.32s)]
*  And so it makes the barrier to entry much higher too. [[01:40:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6049.36s)]
*  So when you talk about, hey, you have all these people [[01:40:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6052.08s)]
*  that are super specialized, they will work, you know, [[01:40:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6054.84s)]
*  80 hours a week in a factory, right? [[01:40:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6057.8s)]
*  In a fab and if anything goes wrong, [[01:41:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6060.12s)]
*  they'll go show up in the middle of the night [[01:41:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6062.8s)]
*  because some earthquake. [[01:41:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6064.4s)]
*  Their wife is like, there was an earthquake. [[01:41:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6065.4s)]
*  He's like, great, I'm gonna go to the fab. [[01:41:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6067.16s)]
*  It's like, would you like as an American do that, right? [[01:41:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6068.48s)]
*  It's like these sorts of things are like, what, you know, [[01:41:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6071.96s)]
*  I guess are the exemplifying like why TSMC is so amazing. [[01:41:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6074.6s)]
*  Now, can you replicate it in the US? [[01:41:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6077.5199999999995s)]
*  Let's not ignore Intel was the leader in manufacturing [[01:41:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6080.0s)]
*  for over 20 years. [[01:41:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6083.44s)]
*  They brought every technology to market first [[01:41:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6084.52s)]
*  besides the EUV, strange silicon, high K metal gates, [[01:41:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6087.4s)]
*  FinFET, you know, and the list goes on and on and on [[01:41:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6090.04s)]
*  of technologies that Intel brought to market first, [[01:41:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6093.4s)]
*  made the most money from and manufactured at scale first, [[01:41:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6095.56s)]
*  best, highest profit emergence, right? [[01:41:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6100.68s)]
*  So we shouldn't ignore that Intel can't do this, right? [[01:41:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6102.76s)]
*  It's that the culture has broken, right? [[01:41:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6105.64s)]
*  You've invested in the wrong things. [[01:41:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6108.84s)]
*  They said no to the iPhone. [[01:41:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6110.24s)]
*  They had all these different things regarding like, [[01:41:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6111.76s)]
*  you know, mismanagement of the fabs, [[01:41:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6114.24s)]
*  mismanagement of designs, this lockup, right? [[01:41:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6115.76s)]
*  And at the same time, all these brilliant people, right? [[01:41:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6118.88s)]
*  These like 50,000 PhDs, you know, or masters [[01:42:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6121.08s)]
*  that have been working on specific chemical [[01:42:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6124.6s)]
*  or physical processes or nanomanufacturing processes [[01:42:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6126.6s)]
*  for decades in Oregon, they're still there. [[01:42:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6129.56s)]
*  They're still producing amazing work. [[01:42:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6131.56s)]
*  It's just like getting it to the last mile of production [[01:42:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6133.2s)]
*  at high yield where you can manufacture dozens [[01:42:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6135.32s)]
*  and hundreds of different kinds of chips, you know, [[01:42:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6138.88s)]
*  and it's good customer experience has broken, right? [[01:42:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6141.44s)]
*  You know, it's that customer experience. [[01:42:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6144.28s)]
*  It's like the, like part of it is like, [[01:42:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6145.5599999999995s)]
*  people will say Intel was too pompous in the 2000s, 2010s, [[01:42:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6147.24s)]
*  right, they just thought they were better than everyone. [[01:42:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6150.599999999999s)]
*  The tool guys were like, [[01:42:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6152.0s)]
*  oh, I don't think that this is mature enough. [[01:42:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6152.84s)]
*  And they're like, ah, you just don't know, we know, right? [[01:42:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6154.679999999999s)]
*  This sort of stuff would happen. [[01:42:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6156.24s)]
*  And so can the US bring it to the, [[01:42:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6158.08s)]
*  can the US bring leading edge semiconductor manufacturing [[01:42:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6161.2s)]
*  to the US? [[01:42:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6163.84s)]
*  Emphamatically, yes, right? [[01:42:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6164.68s)]
*  And we are, right? [[01:42:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6165.68s)]
*  It's happening. [[01:42:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6166.52s)]
*  Like Arizona is getting better and better as time goes on. [[01:42:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6167.34s)]
*  TSMC has built, you know, roughly 20% of their capacity [[01:42:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6170.56s)]
*  for five nanometer in the US, right? [[01:42:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6174.08s)]
*  Now this is nowhere near enough, right? [[01:42:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6176.96s)]
*  You know, 20% of capacity in the US is like nothing, right? [[01:42:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6179.6s)]
*  And furthermore, this is still dependent [[01:43:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6183.08s)]
*  on Taiwan existing, right? [[01:43:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6184.64s)]
*  All there's sort of important way to separate it out. [[01:43:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6186.12s)]
*  There's R&D and there's high volume manufacturing. [[01:43:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6188.360000000001s)]
*  There are effectively, there are three places in the world [[01:43:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6191.6s)]
*  that are doing leading edge R&D. [[01:43:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6194.700000000001s)]
*  There's Shenzhen, Taiwan, there's Hillsborough, Oregon, [[01:43:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6197.0s)]
*  and there is Pyongyang, South Korea, right? [[01:43:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6200.4400000000005s)]
*  These three places are doing the leading edge R&D [[01:43:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6204.320000000001s)]
*  for the rest of the world's leading edge semiconductors, [[01:43:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6206.92s)]
*  right? [[01:43:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6209.08s)]
*  Now manufacturing can be distributed more globally, right? [[01:43:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6210.56s)]
*  And this is sort of where this dichotomy exists [[01:43:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6215.08s)]
*  of like who's actually modifying the process, [[01:43:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6217.88s)]
*  who's actually developing the next generation one, [[01:43:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6220.44s)]
*  who's improving them is Shenzhen, is Hillsborough, [[01:43:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6222.599999999999s)]
*  is Pyongyang, right? [[01:43:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6225.879999999999s)]
*  It is not the rest of these, you know, [[01:43:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6227.2s)]
*  fabs like Arizona, right? [[01:43:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6229.24s)]
*  Arizona is a paperweight. [[01:43:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6230.599999999999s)]
*  If Shenzhen disappeared off the face of the planet, [[01:43:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6232.12s)]
*  you know, within a year, couple years, [[01:43:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6235.08s)]
*  Arizona would stop producing too, right? [[01:43:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6238.36s)]
*  It's actually like pretty critical. [[01:44:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6240.04s)]
*  One of the things I like to say is if I had like [[01:44:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6241.879999999999s)]
*  a few missiles, I know exactly where I could cause [[01:44:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6243.879999999999s)]
*  the most economic damage, right? [[01:44:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6246.28s)]
*  It's not targeting the White House, right? [[01:44:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6247.5599999999995s)]
*  It's the R&D centers. [[01:44:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6248.96s)]
*  It's the R&D centers for TSMC, Intel, Samsung, [[01:44:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6250.32s)]
*  and then some of the memory guys, Micron and Hynex. [[01:44:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6253.44s)]
*  Because they define the future evolution [[01:44:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6255.16s)]
*  of these semiconductors and everything's moving [[01:44:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6256.76s)]
*  so rapidly that it really is fundamentally about R&D. [[01:44:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6258.6s)]
*  And it is all about TSMC, huh? [[01:44:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6264.12s)]
*  And so TSMC, you know, you cannot purchase a vehicle [[01:44:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6267.6s)]
*  without TSMC chips, right? [[01:44:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6271.32s)]
*  You cannot purchase a fridge without TSMC chips. [[01:44:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6272.8s)]
*  You cannot, you get, like I think one of the few things [[01:44:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6275.4800000000005s)]
*  you can purchase ironically is a Texas Instruments [[01:44:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6278.76s)]
*  like graphing calculator, right? [[01:44:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6282.24s)]
*  Because they actually manufacture in Texas. [[01:44:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6283.56s)]
*  But like outside of that, like a laptop, a phone, [[01:44:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6284.88s)]
*  anything you, servers, right? [[01:44:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6287.84s)]
*  GPUs, none of this stuff can exist. [[01:44:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6289.320000000001s)]
*  And this is without TSMC. [[01:44:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6291.08s)]
*  And in many cases, it's not even like the leading edge, [[01:44:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6293.16s)]
*  you know, sexy five nanometer chip, three nanometer chip, [[01:44:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6295.08s)]
*  two nanometer chip. [[01:44:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6297.24s)]
*  Oftentimes it's just like some stupid power IC [[01:44:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6298.2s)]
*  that's like converting from like, you know, [[01:45:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6300.6s)]
*  some voltage to another, right? [[01:45:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6302.68s)]
*  And it's made at TSMC, right? [[01:45:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6303.84s)]
*  It's like- [[01:45:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6304.68s)]
*  This is what China is investing in as well. [[01:45:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6305.52s)]
*  Because like they can build out this long tail fab [[01:45:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6306.56s)]
*  where the techniques are much more known. [[01:45:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6308.88s)]
*  You don't have to figure out these problems with EUV. [[01:45:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6310.360000000001s)]
*  They're investing in this. [[01:45:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6312.72s)]
*  And then they have large supply for things [[01:45:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6314.080000000001s)]
*  like the car door handles and the random stuff. [[01:45:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6317.120000000001s)]
*  And that trickles down [[01:45:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6320.200000000001s)]
*  into this whole economic discussion as well, [[01:45:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6322.360000000001s)]
*  which is they have far more than we do. [[01:45:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6324.280000000001s)]
*  And having supply for things like this [[01:45:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6326.4800000000005s)]
*  is crucial to normal life. [[01:45:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6327.92s)]
*  So they're doing the, [[01:45:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6329.360000000001s)]
*  they're starting to invest in high volume manufacture, [[01:45:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6330.4800000000005s)]
*  but they're not doing R&D. [[01:45:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6333.080000000001s)]
*  So they do R&D on their own. [[01:45:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6334.68s)]
*  They're just way behind, right? [[01:45:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6336.64s)]
*  So I would say like in 2015, [[01:45:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6338.12s)]
*  China had a five-year plan [[01:45:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6340.88s)]
*  where they defined by 2025 and 2020 certain goals, [[01:45:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6342.72s)]
*  including like 80% domestic production of semiconductors. [[01:45:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6347.12s)]
*  They're not gonna hit that, right? [[01:45:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6350.4400000000005s)]
*  To be clear, [[01:45:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6351.64s)]
*  but they are in certain areas really, really close, right? [[01:45:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6352.4800000000005s)]
*  Like BYD is probably gonna be the first company in the world [[01:45:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6355.400000000001s)]
*  to not have to use TSMC for making, [[01:45:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6359.16s)]
*  cause they have their own fabs, right? [[01:46:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6361.68s)]
*  For making chips. [[01:46:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6363.4800000000005s)]
*  Now they still have to buy some chips from foreign, [[01:46:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6364.32s)]
*  for example, like around like self-driving ADAS capabilities, [[01:46:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6367.16s)]
*  cause those are really high-end, [[01:46:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6370.88s)]
*  but at least like, [[01:46:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6372.12s)]
*  you know, like an internal combustion engine has 40 chips [[01:46:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6373.24s)]
*  and an EV, you know, just for like controlling [[01:46:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6375.639999999999s)]
*  like flow rates and all these things. [[01:46:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6377.799999999999s)]
*  And EVs are even more complicated. [[01:46:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6378.88s)]
*  So all these different power ICs [[01:46:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6380.4s)]
*  and battery management controllers and all these things, [[01:46:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6382.04s)]
*  they're insourcing, right? [[01:46:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6384.5199999999995s)]
*  And this is something that like China [[01:46:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6386.599999999999s)]
*  has been doing since 2015. [[01:46:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6389.0s)]
*  Now, as far as like the trailing edge, [[01:46:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6391.04s)]
*  they're getting so much capacity there. [[01:46:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6393.0s)]
*  As far as the leading edge, right? [[01:46:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6394.84s)]
*  IE this five nanometer and so on, so forth, right? [[01:46:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6396.32s)]
*  Where GPUs, they are still behind. [[01:46:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6399.32s)]
*  And this is the US restrictions [[01:46:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6400.64s)]
*  are trying to stop them in the latter, [[01:46:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6402.96s)]
*  but you know, all that's happened, you know, is yes, [[01:46:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6405.36s)]
*  they've slowed down their five nanometer, [[01:46:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6407.4800000000005s)]
*  three nanometer, et cetera, [[01:46:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6409.08s)]
*  but they've accelerated their, [[01:46:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6410.2s)]
*  hey, 45 nanometer, 90 nanometer power IC or analog IC, [[01:46:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6411.88s)]
*  or, you know, random chip in my keyboard, right? [[01:46:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6416.0s)]
*  That kind of stuff. [[01:46:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6418.2s)]
*  So there is an angle of like the US's actions [[01:46:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6419.72s)]
*  have been so from these export, you know, [[01:47:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6423.0s)]
*  from the angle of the expert controls [[01:47:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6424.8s)]
*  have been so inflammatory at slowing down [[01:47:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6426.320000000001s)]
*  China's progress on the leading edge [[01:47:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6429.92s)]
*  that they've turned around [[01:47:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6431.6s)]
*  and have accelerated their progress elsewhere [[01:47:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6432.76s)]
*  because they know that this is so important, right? [[01:47:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6434.68s)]
*  If the US is gonna lock them out here, [[01:47:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6436.84s)]
*  or if they lock us out here as well in the trailing edge. [[01:47:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6438.320000000001s)]
*  And so going back, can the US build it here? [[01:47:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6440.68s)]
*  Yes, but it's gonna take a ton of money. [[01:47:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6444.88s)]
*  I truly think like to revolutionize [[01:47:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6446.52s)]
*  and completely in-source semiconductors [[01:47:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6449.28s)]
*  would take a decade and a trillion dollars. [[01:47:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6451.36s)]
*  Is some of it also culture? [[01:47:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6453.12s)]
*  Like you said, extreme competence, [[01:47:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6455.0s)]
*  extreme work ethic in Taiwan. [[01:47:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6456.679999999999s)]
*  I think if you have the demand and the money is on the line, [[01:47:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6459.08s)]
*  the American companies figure it out. [[01:47:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6461.5199999999995s)]
*  It's gonna take handholding with the government. [[01:47:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6463.44s)]
*  But I think that the culture helps TSMC break through [[01:47:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6465.12s)]
*  and it's easier for them. [[01:47:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6468.639999999999s)]
*  TSMC has some like 90,000 employees, right? [[01:47:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6469.92s)]
*  It's not actually that insane amount. [[01:47:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6472.679999999999s)]
*  The Arizona fab has 3000 from Taiwan. [[01:47:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6475.12s)]
*  And these people, like their wives were like, [[01:47:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6478.08s)]
*  yeah, we're not gonna have kids unless we, [[01:48:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6480.04s)]
*  you sign up for the Arizona fab. [[01:48:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6481.68s)]
*  We go to Arizona and we have our kids there. [[01:48:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6483.24s)]
*  There's also a Japan fab where the same thing happened, [[01:48:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6484.88s)]
*  right? [[01:48:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6486.76s)]
*  And so like these wives drove like these dudes [[01:48:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6487.6s)]
*  to like go to Japan or America to have the kids there. [[01:48:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6490.32s)]
*  And it's like, it's an element of culture. [[01:48:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6493.04s)]
*  Yeah, sure. [[01:48:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6494.64s)]
*  Taiwan works that hard, [[01:48:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6495.5599999999995s)]
*  but also like the US has done in the past, [[01:48:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6496.72s)]
*  they could do it now, right? [[01:48:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6498.96s)]
*  You know, we can just import, I say import, [[01:48:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6500.84s)]
*  the best people in the world if we want to. [[01:48:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6504.0s)]
*  That's where the immigration conversation is a tricky one. [[01:48:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6505.72s)]
*  And there's been a lot of debate over that, but yeah, [[01:48:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6508.4800000000005s)]
*  it seems absurdly controversial [[01:48:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6510.68s)]
*  to import the best people in the world. [[01:48:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6513.0s)]
*  I don't understand why it's controversial. [[01:48:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6514.4400000000005s)]
*  That's one of the ways of winning. [[01:48:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6516.64s)]
*  I'm sure we agree with you. [[01:48:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6518.240000000001s)]
*  Even if you can't import those people, [[01:48:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6519.92s)]
*  I still think you could do a lot to manufacture most of it [[01:48:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6521.88s)]
*  in the US if the money's there, right? [[01:48:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6524.360000000001s)]
*  And so like- It's just way more expensive. [[01:48:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6526.240000000001s)]
*  It's not profitable for a long time. [[01:48:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6528.0s)]
*  And that's the context of like the CHIPS Act [[01:48:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6529.8s)]
*  is only like $50 billion relative to, you know, [[01:48:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6531.76s)]
*  some of the renewable initiatives that were passed [[01:48:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6534.96s)]
*  in the Inflation Reduction Act and the Infrastructure Act, [[01:48:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6538.0s)]
*  which total in the hundreds of billions of dollars, right? [[01:49:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6540.32s)]
*  And so like the amount of money that the US is spending [[01:49:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6543.08s)]
*  on the semiconductor industry is nothing, right? [[01:49:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6545.04s)]
*  Whereas all these other countries have structural advantages [[01:49:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6548.4s)]
*  in terms of like, you know, work ethic and amount of work [[01:49:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6551.4400000000005s)]
*  and like things like that, [[01:49:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6553.76s)]
*  but also a number of STEM graduates, [[01:49:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6554.84s)]
*  the percentile of their best going to that, right? [[01:49:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6556.96s)]
*  But they also have like differences in terms of like, [[01:49:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6560.04s)]
*  hey, there's just tax benefits in the law [[01:49:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6562.84s)]
*  and have been in the law for 20 years, right? [[01:49:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6565.64s)]
*  And so, and then some countries have massive subsidies, [[01:49:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6567.52s)]
*  right? [[01:49:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6570.28s)]
*  China has something like $200 billion [[01:49:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6571.12s)]
*  of semiconductor subsidies a year. [[01:49:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6573.64s)]
*  We're talking about $50 billion in the US over like six, [[01:49:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6575.320000000001s)]
*  right? [[01:49:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6578.56s)]
*  So the girth or difference in like the subsidy amounts [[01:49:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6579.400000000001s)]
*  is also huge, right? [[01:49:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6582.76s)]
*  And so I think, you know, [[01:49:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6583.68s)]
*  Trump has been talking about terrifying Taiwan recently. [[01:49:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6585.72s)]
*  You know, that's sort of like one of these things [[01:49:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6589.2s)]
*  that's like, oh, okay, well like, you know, [[01:49:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6591.32s)]
*  maybe he doesn't wanna subsidize [[01:49:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6593.16s)]
*  the semiconductor industry. [[01:49:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6594.36s)]
*  Obviously terrifying Taiwan is gonna cost a lot of things [[01:49:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6595.92s)]
*  to go get much more expensive, [[01:49:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6598.5599999999995s)]
*  but does it change the equation for TSMC building more fabs [[01:50:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6600.28s)]
*  in the US? [[01:50:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6602.679999999999s)]
*  That's what he's sort of positing, right? [[01:50:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6603.5199999999995s)]
*  So can you lay out the, so we laid out the importance, [[01:50:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6605.5199999999995s)]
*  by the way, it's incredible how much you know about so much. [[01:50:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6608.96s)]
*  We told you Dylan knows all this stuff. [[01:50:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6613.12s)]
*  Yeah. [[01:50:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6615.12s)]
*  So, okay, you laid out why TSMC is really important. [[01:50:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6615.96s)]
*  If we look out into the future, 10, 20 years out, [[01:50:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6621.5599999999995s)]
*  US-China relationship seems like it can go [[01:50:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6625.48s)]
*  to a dark place of Cold War, escalated Cold War, [[01:50:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6629.96s)]
*  even hot war, or to a good place of anything [[01:50:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6635.719999999999s)]
*  from frenemies to cooperation, to working together. [[01:50:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6640.04s)]
*  So in this game theory, complicated game, [[01:50:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6644.88s)]
*  what are the different trajectories? [[01:50:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6649.76s)]
*  What should US be doing? [[01:50:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6651.96s)]
*  Like, what do you see as the different possible trajectories [[01:50:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6653.12s)]
*  of US-China relations as both leaders start to feel [[01:50:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6655.2s)]
*  the age AI more and more, and see the importance of chips [[01:50:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6659.16s)]
*  and the importance of AI? [[01:51:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6662.56s)]
*  I mean, ultimately the export controls [[01:51:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6664.4s)]
*  are pointing towards a separate future economy. [[01:51:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6666.96s)]
*  I think the US has made it clear to Chinese leaders [[01:51:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6670.28s)]
*  that we intend to control this technology [[01:51:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6673.6s)]
*  at whatever cost to global economic integration. [[01:51:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6677.68s)]
*  So that it's hard to unwind that. [[01:51:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6682.88s)]
*  Like the card has been played. [[01:51:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6685.12s)]
*  To the same extent, they've also limited US companies [[01:51:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6687.08s)]
*  from entering China, right? [[01:51:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6689.400000000001s)]
*  So it's been a long time coming. [[01:51:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6690.360000000001s)]
*  At some point, there was a convergence, right? [[01:51:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6693.56s)]
*  But over at least the last decade, [[01:51:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6696.68s)]
*  it's been branching further and further out, right? [[01:51:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6698.68s)]
*  Like US companies can't enter China, [[01:51:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6701.0s)]
*  Chinese companies can't enter the US. [[01:51:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6702.84s)]
*  The US is saying, hey, China, you can't get access [[01:51:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6705.04s)]
*  to our technologies in certain areas. [[01:51:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6708.0s)]
*  And China's rebuttaling with the same thing around, [[01:51:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6710.28s)]
*  like they've done some sort of specific materials [[01:51:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6712.92s)]
*  in Gallium and things like that [[01:51:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6715.8s)]
*  that they've tried to limit the US on. [[01:51:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6717.44s)]
*  One of the, there's a US drone company [[01:51:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6719.56s)]
*  that's not allowed to buy batteries, [[01:52:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6721.12s)]
*  and they have like military customers. [[01:52:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6722.32s)]
*  And this drone company just tells the military customers, [[01:52:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6724.16s)]
*  like, hey, just get it from Amazon, [[01:52:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6726.84s)]
*  because I can't actually physically get them, right? [[01:52:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6728.44s)]
*  Like there's all these things that are happening [[01:52:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6730.08s)]
*  that point to further and further divergence. [[01:52:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6731.96s)]
*  I have zero idea, and I would love if we could all [[01:52:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6734.12s)]
*  hold hands and sing Kumbaya, but like, [[01:52:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6737.48s)]
*  I have zero idea how that could possibly happen. [[01:52:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6739.12s)]
*  Is the divergence good or bad for avoiding war? [[01:52:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6741.16s)]
*  Is it possible that the divergence [[01:52:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6745.84s)]
*  in terms of manufacturer chips of training AI systems [[01:52:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6749.16s)]
*  is actually good for avoiding military conflict? [[01:52:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6752.36s)]
*  It's an objective fact that the world [[01:52:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6755.08s)]
*  has been the most peaceful as ever been [[01:52:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6757.92s)]
*  when there are global hegemons, right? [[01:52:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6760.24s)]
*  Or regional hegemons, right? [[01:52:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6762.16s)]
*  In historical context, right? [[01:52:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6763.24s)]
*  The Mediterranean was the most peaceful ever [[01:52:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6765.599999999999s)]
*  when the Romans were there, right? [[01:52:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6767.48s)]
*  China had very peaceful and warring times, [[01:52:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6768.76s)]
*  and the peaceful times were when dynasties [[01:52:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6770.76s)]
*  had a lock hold over not just themselves, [[01:52:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6772.36s)]
*  but all their tributaries around them, right? [[01:52:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6774.32s)]
*  And likewise, the most peaceful time in human history [[01:52:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6776.08s)]
*  has been when the US was the global hegemon, right? [[01:52:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6779.68s)]
*  The last, you know, decades. [[01:53:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6782.599999999999s)]
*  Now we've sort of seen things start to slide, right? [[01:53:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6784.12s)]
*  With Russia, Ukraine, with what's going on in the Middle East, [[01:53:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6786.72s)]
*  and Taiwan risk, all these different things [[01:53:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6789.44s)]
*  are starting to bubble up, [[01:53:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6791.48s)]
*  still objectively extremely peaceful. [[01:53:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6792.759999999999s)]
*  Now, what happens when it's not one global hegemon, [[01:53:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6794.44s)]
*  but it's two, obviously, and you know, [[01:53:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6796.959999999999s)]
*  China will be competitive or even overtake the US [[01:53:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6799.5599999999995s)]
*  like it's possible, right? [[01:53:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6803.2s)]
*  And so this change in global hegemony, [[01:53:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6804.2s)]
*  I don't think it ever happens like super peacefully, right? [[01:53:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6808.0s)]
*  When empires fall, right? [[01:53:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6810.04s)]
*  Which is a possible trajectory for America, [[01:53:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6811.48s)]
*  they don't fall gracefully, right? [[01:53:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6813.639999999999s)]
*  Like they don't just slide out of irrelevance. [[01:53:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6815.719999999999s)]
*  Usually there's a lot of shaking. [[01:53:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6818.0s)]
*  And so, you know, what the US is trying to do [[01:53:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6820.52s)]
*  is maintain its top position. [[01:53:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6823.92s)]
*  And what China is trying to do [[01:53:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6825.16s)]
*  is become the top position, right? [[01:53:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6826.12s)]
*  And obviously there's butting of heads here [[01:53:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6827.76s)]
*  in the most simple terms. [[01:53:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6831.4800000000005s)]
*  And that could take shape in all kinds of ways, [[01:53:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6833.200000000001s)]
*  including proxy wars. [[01:53:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6835.84s)]
*  And now- It seems like it's already happening. [[01:53:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6837.88s)]
*  Like as much as I want there to be [[01:53:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6839.84s)]
*  centuries of prolonged peace, it does not, [[01:54:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6842.320000000001s)]
*  it looks like further instability internationally is ahead. [[01:54:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6844.52s)]
*  And the US is like sort of like current task is like, [[01:54:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6849.2s)]
*  hey, if we control AI, if we're the leader in AI, [[01:54:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6851.96s)]
*  then AI significantly accelerates progress, [[01:54:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6855.28s)]
*  then we can maintain the global hegemony position. [[01:54:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6858.76s)]
*  And therefore- [[01:54:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6860.8s)]
*  I hope that works. [[01:54:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6861.64s)]
*  And as an American, like, you know, kind of like, [[01:54:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6862.64s)]
*  okay, I guess that's gonna lead to peace for us. [[01:54:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6865.32s)]
*  Now, obviously other people around the world [[01:54:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6868.24s)]
*  get affected negatively. [[01:54:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6869.64s)]
*  Obviously the Chinese people are not gonna be in [[01:54:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6873.12s)]
*  as advantageous of a position if that happens, [[01:54:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6874.76s)]
*  but you know, this is sort of the reality of like [[01:54:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6877.92s)]
*  what's being done and the actions that are being carried out. [[01:54:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6881.64s)]
*  So can we go back to the specific detail [[01:54:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6884.24s)]
*  of the different hardware? [[01:54:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6886.76s)]
*  There's this nice graphic in the export controls [[01:54:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6887.84s)]
*  of which GPUs are allowed to be exported [[01:54:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6891.88s)]
*  and which are not. [[01:54:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6897.56s)]
*  Can you kind of explain the difference? [[01:54:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6898.92s)]
*  Is there, from a technical perspective, [[01:55:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6900.76s)]
*  are the H20s promising? [[01:55:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6903.88s)]
*  Yeah, so this goes, and I think we'd have to like, [[01:55:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6908.400000000001s)]
*  we need to dive really deep into the reasoning aspect [[01:55:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6910.88s)]
*  and what's going on there. [[01:55:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6913.400000000001s)]
*  But the H20, you know, the US has gone through [[01:55:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6914.76s)]
*  multiple iterations of the export controls, right? [[01:55:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6917.64s)]
*  This H800 was at one point allowed back in 23, [[01:55:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6920.4400000000005s)]
*  but then it got canceled and by then, [[01:55:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6923.76s)]
*  by, you know, DeepSeq had already built their cluster of, [[01:55:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6925.2s)]
*  they claim 2K, I think they actually have like many more, [[01:55:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6927.92s)]
*  like something like 10K of those. [[01:55:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6930.16s)]
*  And now this H20 is the legally allowed chip, right? [[01:55:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6931.72s)]
*  Nvidia shipped a million of these last year to China, right? [[01:55:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6934.4400000000005s)]
*  For context, it was like four or five million GPUs, right? [[01:55:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6937.280000000001s)]
*  So the percentage of GPUs that were this China specific H20 [[01:55:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6939.8s)]
*  is quite high, right? [[01:55:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6943.88s)]
*  You know, roughly 20%, 25%, right? [[01:55:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6945.88s)]
*  20% or so. [[01:55:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6947.96s)]
*  And so this H20 has been neutered in one way, [[01:55:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6948.84s)]
*  but it's actually upgraded in other ways, right? [[01:55:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6952.8s)]
*  And you know, you could think of chips [[01:55:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6955.360000000001s)]
*  along three axes for AI, right? [[01:55:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6957.04s)]
*  You know, ignoring software stack [[01:55:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6959.6s)]
*  and like exact architecture, just raw specifications, [[01:56:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6961.68s)]
*  there's floating point operations, right? [[01:56:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6964.400000000001s)]
*  Flops, there is memory bandwidth, [[01:56:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6966.280000000001s)]
*  i.e. in memory capacity, right? [[01:56:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6968.76s)]
*  I.O., right? Memory. [[01:56:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6970.280000000001s)]
*  And then there is interconnect, right? [[01:56:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6971.68s)]
*  Chip to chip interconnections. [[01:56:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6973.280000000001s)]
*  All three of these are incredibly important [[01:56:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6974.8s)]
*  for making AI systems, right? [[01:56:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6978.52s)]
*  Because AI systems involve a lot of compute, [[01:56:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6981.4800000000005s)]
*  they involve a lot of moving memory around, [[01:56:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6983.400000000001s)]
*  whether it be to memory or to other chips, right? [[01:56:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6985.64s)]
*  And so these three vectors, [[01:56:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6988.08s)]
*  the US initially had two of these vectors controlled [[01:56:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6990.28s)]
*  and one of them not controlled, [[01:56:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6993.68s)]
*  which was Flops and interconnect bandwidth [[01:56:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6994.68s)]
*  were initially controlled. [[01:56:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6996.64s)]
*  And then they said, no, no, no, no, [[01:56:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6997.8s)]
*  we're gonna remove the interconnect bandwidth [[01:56:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=6999.16s)]
*  and just make it a very simple only Flops. [[01:56:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7000.44s)]
*  But now Nvidia can now make a chip that has, [[01:56:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7002.76s)]
*  okay, it's cut down on Flops. [[01:56:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7005.48s)]
*  It's like one third that of the H100, right? [[01:56:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7007.2s)]
*  In on spec sheet paper performance for Flops, [[01:56:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7010.64s)]
*  you know, in real world, it's closer to like half [[01:56:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7014.96s)]
*  or maybe even like 60% of it, right? [[01:56:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7017.24s)]
*  But then on the other two vectors, [[01:56:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7019.64s)]
*  it's just as good for interconnect bandwidth. [[01:57:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7021.08s)]
*  And then for memory bandwidth and memory capacity, [[01:57:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7023.4s)]
*  the H20 has more memory bandwidth [[01:57:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7025.48s)]
*  and more memory capacity than the H100, right? [[01:57:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7027.6s)]
*  Now, recently, you know, we at our research, [[01:57:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7030.72s)]
*  we cut Nvidia's production for H20 [[01:57:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7033.8s)]
*  for this year down drastically. [[01:57:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7036.2s)]
*  They were gonna make another 2 million of those this year, [[01:57:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7038.12s)]
*  but they just canceled all the orders a couple of weeks ago. [[01:57:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7040.64s)]
*  In our view, that's because we think that they think [[01:57:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7043.8s)]
*  they're gonna get restricted, right? [[01:57:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7045.52s)]
*  Because why would they cancel all these orders for H20? [[01:57:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7047.72s)]
*  Because they shipped a million of them last year, [[01:57:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7050.92s)]
*  they had orders in for a couple million this year [[01:57:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7052.52s)]
*  and just gone, right? [[01:57:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7054.68s)]
*  For H20, B20, right? [[01:57:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7055.52s)]
*  A successor to H20. [[01:57:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7056.88s)]
*  And now they're all gone. [[01:57:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7058.120000000001s)]
*  Now, why would they do this, right? [[01:57:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7059.120000000001s)]
*  I think it's very clear, right? [[01:57:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7061.4400000000005s)]
*  The H20 is actually better for certain tasks. [[01:57:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7063.160000000001s)]
*  And that certain task is reasoning, right? [[01:57:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7066.76s)]
*  Reasoning is incredibly like different than, you know, [[01:57:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7070.120000000001s)]
*  when you look at the different regimes of models, right? [[01:57:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7073.44s)]
*  Pre-training is all about flops, right? [[01:57:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7076.24s)]
*  It's all about flops. [[01:57:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7079.24s)]
*  There's things you do, like mixture of experts [[01:58:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7080.12s)]
*  that we talked about to trade off interconnect [[01:58:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7082.08s)]
*  or to trade off, you know, other aspects [[01:58:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7084.28s)]
*  and lower the flops and rely more on interconnect and memory. [[01:58:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7086.44s)]
*  But at the end of the day, flops is everything, right? [[01:58:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7090.2s)]
*  We talk about models in terms of like [[01:58:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7092.879999999999s)]
*  how many flops they are, right? [[01:58:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7094.719999999999s)]
*  So like, you know, we talk about, [[01:58:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7096.879999999999s)]
*  oh, GPT-4 is 2E25, right? [[01:58:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7098.08s)]
*  Two to the 25th, you know, 25 zeros, right? [[01:58:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7100.84s)]
*  Flop, right? [[01:58:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7105.52s)]
*  Floating point operations. [[01:58:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7106.360000000001s)]
*  For training. [[01:58:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7108.360000000001s)]
*  For training, right? [[01:58:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7109.2s)]
*  And we're talking about the restrictions for the 2E24, right? [[01:58:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7110.04s)]
*  Or 25, whatever. [[01:58:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7113.12s)]
*  The US has an executive order that Trump recently unsigned, [[01:58:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7113.96s)]
*  but which was, hey, 1E26, [[01:58:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7117.24s)]
*  once you hit that number of floating point operations, [[01:58:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7120.12s)]
*  you must notify the government [[01:58:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7122.0s)]
*  and you must share your results with us, right? [[01:58:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7123.6s)]
*  Like there's a level of model [[01:58:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7125.64s)]
*  where the US government must be told, right? [[01:58:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7126.92s)]
*  And that's 1E26. [[01:58:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7129.16s)]
*  And so as we move forward, [[01:58:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7131.08s)]
*  this is an incredibly like important, [[01:58:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7132.28s)]
*  flop is the vector that the government [[01:58:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7134.4s)]
*  has cared about historically, [[01:58:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7135.879999999999s)]
*  but the other two vectors are arguably just as important, [[01:58:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7137.32s)]
*  right? [[01:59:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7140.919999999999s)]
*  And especially when we come to this new paradigm, [[01:59:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7141.759999999999s)]
*  which the world is only just learning about [[01:59:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7144.12s)]
*  over the last six months, right? [[01:59:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7146.2s)]
*  Reasoning. [[01:59:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7147.4s)]
*  And do we understand firmly [[01:59:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7148.24s)]
*  which of the three dimensions is best for reasoning? [[01:59:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7150.919999999999s)]
*  So interconnect, the flops don't matter as much. [[01:59:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7154.16s)]
*  Is it memory? [[01:59:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7156.679999999999s)]
*  Memory, right? [[01:59:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7157.919999999999s)]
*  Yeah, so- Context line. [[01:59:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7158.759999999999s)]
*  I'm gonna get into technical stuff real fast again. [[01:59:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7159.6s)]
*  There's two articles in this one that I could show, [[01:59:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7162.200000000001s)]
*  maybe graphics that might be interesting for you to pull up. [[01:59:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7164.280000000001s)]
*  For the listeners, [[01:59:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7167.08s)]
*  we're looking at the section of [[01:59:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7167.92s)]
*  O1 inference architectures tokenomics. [[01:59:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7169.200000000001s)]
*  You wanna explain KVCache before we talk about this? [[01:59:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7173.240000000001s)]
*  I think like it's better to- [[01:59:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7175.120000000001s)]
*  Okay, yeah, we should get, [[01:59:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7176.120000000001s)]
*  we need to go through a lot of specific technical things [[01:59:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7177.240000000001s)]
*  of transformers to make this easy for people. [[01:59:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7180.0s)]
*  Because it's incredibly important [[01:59:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7182.88s)]
*  because this changes how models work. [[01:59:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7184.84s)]
*  But I think resetting, right? [[01:59:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7186.320000000001s)]
*  Why is memory so important? [[01:59:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7188.48s)]
*  It's because so far we've talked about parameter counts, [[01:59:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7191.28s)]
*  and mixture of experts, [[01:59:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7193.919999999999s)]
*  you can change how many active parameters [[01:59:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7194.96s)]
*  versus total parameters to embed more data [[01:59:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7196.719999999999s)]
*  but have less flops. [[01:59:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7198.5199999999995s)]
*  But more important, another aspect of [[01:59:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7199.799999999999s)]
*  what's part of this humongous revolution [[02:00:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7203.12s)]
*  in the last handful of years is the transformer, [[02:00:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7204.799999999999s)]
*  and the attention mechanism. [[02:00:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7207.28s)]
*  Attention mechanism is that the model understands [[02:00:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7208.48s)]
*  the relationships between all the words in its context. [[02:00:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7211.599999999999s)]
*  And that is separate from the parameters themselves. [[02:00:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7214.68s)]
*  And that is something that you must calculate, right? [[02:00:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7218.88s)]
*  How each token, each word in the context length [[02:00:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7222.0s)]
*  is relatively connected to each other, right? [[02:00:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7225.360000000001s)]
*  And I think, Nathan, you can explain KVCache better. [[02:00:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7229.12s)]
*  KVCache is one of the optimizations that enable- [[02:00:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7231.8s)]
*  Yeah, so the attention operator has three core things. [[02:00:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7233.200000000001s)]
*  It's queries, keys, and values. [[02:00:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7237.12s)]
*  QKV is the thing that goes into this. [[02:00:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7239.6s)]
*  You'll look at the equation. [[02:00:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7242.68s)]
*  You see that these matrices are multiplied together. [[02:00:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7244.08s)]
*  These words, query, key, and value [[02:00:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7246.64s)]
*  come from information retrieval backgrounds, [[02:00:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7248.4s)]
*  where the query is the thing [[02:00:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7250.4s)]
*  you're trying to get the values for, [[02:00:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7252.5599999999995s)]
*  and you access the keys and the values is reweighting. [[02:00:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7253.8s)]
*  My background's not in information retrieval [[02:00:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7256.5599999999995s)]
*  and things like this. [[02:00:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7258.8s)]
*  It's just fun to have backlinks. [[02:00:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7259.64s)]
*  And what effectively happens is that [[02:01:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7261.48s)]
*  when you're doing these matrix multiplications, [[02:01:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7264.48s)]
*  you're having matrices that are of [[02:01:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7266.28s)]
*  the size of the context length. [[02:01:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7268.48s)]
*  So the number of tokens that you put into the model. [[02:01:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7269.88s)]
*  And the KVCache is effectively [[02:01:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7272.0s)]
*  some form of compressed representation [[02:01:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7274.96s)]
*  of all the previous tokens in the model. [[02:01:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7277.2s)]
*  So when you're doing this, [[02:01:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7279.48s)]
*  we talk about autoregressive models. [[02:01:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7280.72s)]
*  You predict one token at a time. [[02:01:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7282.68s)]
*  You start with whatever your prompt was. [[02:01:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7284.44s)]
*  You ask a question, like, who was the president in 1825? [[02:01:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7286.4s)]
*  The model then is gonna generate its first token. [[02:01:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7290.2s)]
*  For each of these tokens, [[02:01:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7292.56s)]
*  you're doing the same attention operator [[02:01:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7294.16s)]
*  where you're multiplying these query, key, value matrices. [[02:01:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7296.32s)]
*  But the math is very nice [[02:01:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7300.24s)]
*  so that when you're doing this repeatedly, [[02:01:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7302.5199999999995s)]
*  this KVCache, this key value operation, [[02:01:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7304.8s)]
*  you can keep appending the new values to it. [[02:01:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7309.0s)]
*  So you keep track of what your previous values [[02:01:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7311.4s)]
*  you're inferring over in this autoregressive chain. [[02:01:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7313.84s)]
*  You keep it in memory the whole time. [[02:01:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7316.639999999999s)]
*  And this is a really crucial thing to manage [[02:01:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7318.76s)]
*  when serving inference at scale. [[02:02:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7322.12s)]
*  There are far bigger experts in this [[02:02:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7324.36s)]
*  and there are so many levels of detail that you can go into. [[02:02:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7326.24s)]
*  Essentially, one of the key, quote unquote, [[02:02:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7329.52s)]
*  drawbacks of the attention operator and the transformer [[02:02:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7332.72s)]
*  is that there is a form of quadratic memory cost [[02:02:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7336.240000000001s)]
*  in proportion to the context length. [[02:02:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7338.92s)]
*  So as you put in longer questions, [[02:02:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7341.6s)]
*  the memory used in order to make that computation [[02:02:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7343.84s)]
*  is going up in the form of a quadratic. [[02:02:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7347.240000000001s)]
*  You'll hear about a lot of other language model architectures [[02:02:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7349.6s)]
*  that are like sub-quadratic or linear attention forms, [[02:02:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7353.240000000001s)]
*  which is like state space models. [[02:02:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7356.72s)]
*  We don't need to go down all these now. [[02:02:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7358.6s)]
*  And then there's innovations on attention [[02:02:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7360.160000000001s)]
*  to make this memory usage and the ability to attend [[02:02:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7362.56s)]
*  over long contexts much more accurate and high performance. [[02:02:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7366.4400000000005s)]
*  And those innovations are going to help you with, [[02:02:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7370.320000000001s)]
*  I mean, your highly memory constraints. [[02:02:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7372.4400000000005s)]
*  They help with memory constraint and performance. [[02:02:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7374.0s)]
*  So if you put in a book into, I think, [[02:02:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7375.96s)]
*  GEMINI is the model that has the longest context length [[02:02:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7378.360000000001s)]
*  that people are using. [[02:03:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7380.6s)]
*  GEMINI is known for one million [[02:03:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7381.4400000000005s)]
*  and now two million context length. [[02:03:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7382.56s)]
*  You put a whole book into GEMINI [[02:03:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7384.320000000001s)]
*  and sometimes it'll draw facts out of it. [[02:03:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7386.44s)]
*  It's not perfect. [[02:03:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7389.36s)]
*  They're getting better. [[02:03:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7390.2s)]
*  But the, so there's two things. [[02:03:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7391.28s)]
*  It's like one to be able to serve this on the memory level. [[02:03:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7393.0s)]
*  Google has magic with their TPU stack [[02:03:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7395.639999999999s)]
*  where they can serve really long contexts. [[02:03:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7397.759999999999s)]
*  And then there's also many decisions along the way [[02:03:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7399.599999999999s)]
*  to actually make long context performance work [[02:03:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7402.0s)]
*  that supplies the data. [[02:03:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7404.16s)]
*  There's subtle changes to these computations in attention [[02:03:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7405.5599999999995s)]
*  and it changes the architecture. [[02:03:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7408.879999999999s)]
*  But serving long contexts is extremely memory constrained, [[02:03:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7411.2s)]
*  especially when you're making a lot of predictions. [[02:03:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7415.36s)]
*  I actually don't know why input and output tokens [[02:03:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7417.08s)]
*  are more expensive, but I think essentially output tokens, [[02:03:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7420.719999999999s)]
*  you have to do more computation [[02:03:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7423.0s)]
*  because you have to sample from the model. [[02:03:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7424.44s)]
*  I can explain that. [[02:03:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7425.92s)]
*  So today, if you use a model, like you look at an API, [[02:03:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7426.759999999999s)]
*  OpenAI charges certain price per million tokens, right? [[02:03:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7430.32s)]
*  And that price for input and output tokens is different. [[02:03:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7434.96s)]
*  And the reason is that there is, [[02:03:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7438.08s)]
*  when you're inputting a query into the model, right? [[02:04:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7441.32s)]
*  Let's say you have a book, right? [[02:04:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7444.08s)]
*  That book, you must now calculate the entire KV cash for, [[02:04:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7446.08s)]
*  this key value cash. [[02:04:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7448.96s)]
*  And so when you do that, that is a parallel operation. [[02:04:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7450.32s)]
*  All of the tokens can be processed at one time. [[02:04:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7453.6s)]
*  And therefore you can dramatically reduce [[02:04:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7455.96s)]
*  how much you're spending, right? [[02:04:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7458.16s)]
*  The flop requirements for generating a token [[02:04:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7459.36s)]
*  and an input token are identical, right? [[02:04:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7461.84s)]
*  If I input one token or if I generate one token, [[02:04:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7464.36s)]
*  it's completely identical. [[02:04:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7466.6s)]
*  I have to go through the model, right? [[02:04:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7467.6s)]
*  But the difference is that I can do that input, [[02:04:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7468.84s)]
*  i.e. the prefill, i.e. the prompt, [[02:04:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7471.72s)]
*  simultaneously in a batch nature, right? [[02:04:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7474.16s)]
*  And therefore it is all flop. [[02:04:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7477.400000000001s)]
*  I think the pricing model, [[02:04:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7478.92s)]
*  mostly they use this for input tokens [[02:04:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7480.04s)]
*  is about one fourth the price of the output tokens. [[02:04:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7482.52s)]
*  Correct, but then output tokens, [[02:04:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7484.320000000001s)]
*  the reason why it's so expensive [[02:04:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7486.16s)]
*  is because I can't do it in parallel, right? [[02:04:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7487.400000000001s)]
*  It's autoregressive. [[02:04:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7488.84s)]
*  Every time I generate a token, [[02:04:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7489.92s)]
*  I must not only take the entire, [[02:04:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7491.4400000000005s)]
*  I must not only read the whole entire model into memory, [[02:04:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7493.72s)]
*  right, and activate it, right? [[02:04:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7496.280000000001s)]
*  Go calculate it to generate the next token. [[02:04:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7498.0s)]
*  I also have to read the entire KV cache. [[02:04:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7499.88s)]
*  And I generate a token and then I append that KV, [[02:05:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7501.8s)]
*  that one token I generated and it's KV cache, [[02:05:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7504.08s)]
*  and then I do it again, right? [[02:05:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7506.52s)]
*  And so therefore this is a non-parallel operation. [[02:05:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7507.72s)]
*  And this is one where you have to, [[02:05:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7510.92s)]
*  in the case of prefill or prompt, [[02:05:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7513.76s)]
*  you pull the whole model in [[02:05:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7515.72s)]
*  and you calculate 20,000 tokens at once, right? [[02:05:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7517.36s)]
*  So these are features that APIs are shipping, [[02:05:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7520.0s)]
*  which is like prompt caching, prefilling, [[02:05:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7522.2s)]
*  because you can drive prices down [[02:05:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7525.84s)]
*  and you can make APIs much faster [[02:05:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7527.2s)]
*  if you know you're gonna keep, [[02:05:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7528.56s)]
*  if you run a business and you're gonna keep passing [[02:05:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7529.84s)]
*  the same initial content to Cloud's API, [[02:05:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7531.4400000000005s)]
*  you can load that in to the Anthropic API [[02:05:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7534.56s)]
*  and always keep it there. [[02:05:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7537.320000000001s)]
*  But it's very different than we're kind of leading [[02:05:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7538.64s)]
*  to the reasoning models, which we talked, [[02:05:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7540.360000000001s)]
*  we showed this example earlier [[02:05:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7542.240000000001s)]
*  and read some of this kind of mumbling stuff. [[02:05:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7543.64s)]
*  And what happens is that the output context length [[02:05:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7546.84s)]
*  is so much higher. [[02:05:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7550.56s)]
*  And I mean, I learned a lot about this from Dylan's work, [[02:05:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7551.52s)]
*  which is essentially as the output work length gets higher, [[02:05:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7553.6s)]
*  you're using this, you're writing this quadratic [[02:05:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7556.240000000001s)]
*  in terms of memory used. [[02:05:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7558.24s)]
*  And then the GPUs that we have, [[02:05:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7559.8s)]
*  effectively you're gonna run out of memory [[02:06:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7562.32s)]
*  and they're all trying to serve multiple requests at once. [[02:06:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7564.96s)]
*  So during this batch processing, [[02:06:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7567.36s)]
*  where not all of the prompts are exactly the same, [[02:06:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7568.76s)]
*  really complex handling. [[02:06:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7570.639999999999s)]
*  And then as context links gets longer, [[02:06:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7572.04s)]
*  there's this like, I think you call it critical batch size, [[02:06:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7574.36s)]
*  where your ability to serve more users, [[02:06:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7577.12s)]
*  so how much you can parallelize your inference plummets [[02:06:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7580.719999999999s)]
*  because of this long contracts. [[02:06:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7584.32s)]
*  So your memory usage is going way up [[02:06:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7585.5199999999995s)]
*  with these reasoning models [[02:06:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7588.12s)]
*  and you still have a lot of users. [[02:06:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7589.28s)]
*  So effectively the cost to serve multiplies by a ton. [[02:06:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7590.76s)]
*  And we're looking at a plot [[02:06:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7595.08s)]
*  when the X axis is a sequence length. [[02:06:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7596.64s)]
*  I.e. how many tokens are being generated slash prompt. [[02:06:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7599.96s)]
*  So if I put in a book, that's a million tokens. [[02:06:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7603.44s)]
*  But if I put in, the sky is blue, [[02:06:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7605.8s)]
*  then that's like six tokens or whatever. [[02:06:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7608.28s)]
*  We should say that what we're calling reasoning [[02:06:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7609.64s)]
*  and chain of thought is extending this sequence length. [[02:06:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7612.04s)]
*  It's mostly output. [[02:06:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7615.599999999999s)]
*  Before three months ago, whenever O1 launched, [[02:06:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7617.08s)]
*  all of the use cases for long context length [[02:06:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7619.64s)]
*  were like, let me put a ton of documents in [[02:07:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7622.2s)]
*  and then get an answer out. [[02:07:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7624.08s)]
*  And it's a single, [[02:07:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7625.52s)]
*  pre-fill, compute a lot in parallel, [[02:07:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7627.84s)]
*  and then output a little bit. [[02:07:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7629.76s)]
*  Now with reasoning and agents, [[02:07:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7631.32s)]
*  this is a very different idea. [[02:07:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7633.4800000000005s)]
*  Now instead I might only have like, [[02:07:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7635.16s)]
*  hey, do this task or I might have all these documents. [[02:07:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7637.28s)]
*  But at the end of the day, [[02:07:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7639.28s)]
*  the model is not just like producing a little bit. [[02:07:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7640.52s)]
*  It's producing tons of information, [[02:07:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7642.72s)]
*  this chain of thought. Tens of thousands of tokens. [[02:07:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7644.84s)]
*  It just continues to go and go and go and go. [[02:07:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7645.88s)]
*  And so the sequence length is effectively that, [[02:07:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7648.36s)]
*  if it's generated 10,000 tokens, [[02:07:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7651.32s)]
*  it's 10,000 sequence length, right? [[02:07:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7653.16s)]
*  Or, and plus whatever you inputted in the prompt. [[02:07:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7655.0s)]
*  And so what this chart is showing, [[02:07:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7657.36s)]
*  and it's a logarithmic chart, right? [[02:07:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7658.64s)]
*  Is as you go from 1K to 4K or 4K to 16K, [[02:07:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7660.16s)]
*  the memory requirements grow so fast for your KV cache [[02:07:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7666.32s)]
*  that you end up not being able to run a certain number of, [[02:07:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7670.56s)]
*  your sequence length is capped [[02:07:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7675.24s)]
*  or the number of users you can- [[02:07:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7676.719999999999s)]
*  Let's say the model. [[02:07:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7677.719999999999s)]
*  So this is showing for a 405B model and batch size 64. [[02:07:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7678.5599999999995s)]
*  Llama 3144B. [[02:08:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7682.36s)]
*  Yeah, and batch size is crucial to essentially, [[02:08:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7684.32s)]
*  you wanna have higher batch size [[02:08:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7687.759999999999s)]
*  to parallel your throughput. [[02:08:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7689.16s)]
*  64 different users at once, right? [[02:08:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7691.679999999999s)]
*  Yeah. [[02:08:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7693.08s)]
*  And therefore your serving costs are lower, right? [[02:08:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7693.92s)]
*  Cause the server costs the same, right? [[02:08:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7695.12s)]
*  This is eight H100s, roughly $2 an hour per GPU. [[02:08:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7696.599999999999s)]
*  That's $16 an hour, right? [[02:08:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7699.92s)]
*  That is like somewhat of a fixed cost. [[02:08:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7701.639999999999s)]
*  You can do things to make it lower, of course, [[02:08:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7703.88s)]
*  but like it's like $16 an hour. [[02:08:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7705.2s)]
*  Now, how many users can you serve? [[02:08:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7706.88s)]
*  How many tokens can you generate? [[02:08:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7708.6s)]
*  And then you divide the two and that's your cost, right? [[02:08:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7710.08s)]
*  And so with reasoning models, [[02:08:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7712.16s)]
*  this is where a lot of the complexity comes about [[02:08:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7714.52s)]
*  and why memory is so important. [[02:08:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7717.24s)]
*  Because if you have limited amounts of memory, [[02:08:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7718.84s)]
*  then you can't serve so many users. [[02:08:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7721.2s)]
*  If you have limited amounts of memory, [[02:08:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7723.08s)]
*  your serving speeds get lower, right? [[02:08:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7724.6s)]
*  And so your costs get a lot, lot worse. [[02:08:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7726.36s)]
*  Because all of a sudden, if I was used to, [[02:08:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7729.2s)]
*  hey, on the $16 an hour server, [[02:08:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7731.32s)]
*  I'm serving Lama 405B, or if I'm serving DeepSeq v3, [[02:08:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7733.04s)]
*  and it's all chat style applications, [[02:08:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7737.5199999999995s)]
*  i.e. we're just chatting, [[02:08:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7739.5199999999995s)]
*  the sequence lengths are a thousand, a few thousand, right? [[02:09:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7741.0s)]
*  When you use a language model, [[02:09:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7744.48s)]
*  it's a few thousand context length most times. [[02:09:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7745.36s)]
*  Sometimes you're dropping a big document, [[02:09:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7746.76s)]
*  but then you process it, you get your answer, [[02:09:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7748.24s)]
*  you throw it away, right? [[02:09:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7750.28s)]
*  You move on to the next thing, right? [[02:09:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7751.12s)]
*  Whereas with reasoning, [[02:09:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7752.64s)]
*  I'm now generating tens of thousands of tokens [[02:09:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7754.28s)]
*  in sequence, right? [[02:09:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7756.32s)]
*  And so this memory, this KV cache has to stay resident [[02:09:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7758.32s)]
*  and you have to keep loading it. [[02:09:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7761.44s)]
*  Keep it in memory constantly. [[02:09:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7763.2s)]
*  And now this butts out other users, right? [[02:09:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7764.759999999999s)]
*  If there's now a reasoning task, right? [[02:09:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7767.08s)]
*  And the model is capable of reasoning, [[02:09:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7769.12s)]
*  then all of a sudden, that memory pressure means [[02:09:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7770.759999999999s)]
*  that I can't serve as many users simultaneously. [[02:09:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7774.44s)]
*  Let's go into DeepSeq again. [[02:09:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7776.28s)]
*  So we're in the post DeepSeq R1 time, I think. [[02:09:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7777.799999999999s)]
*  And there's two sides to this market watching [[02:09:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7782.36s)]
*  how hard it is to serve it. [[02:09:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7785.28s)]
*  On one side, we're gonna talk about DeepSeq themselves. [[02:09:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7786.919999999999s)]
*  They now have a chat app that got to number one [[02:09:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7789.12s)]
*  on the app store. [[02:09:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7791.2s)]
*  Disclaimer, number one on the app store [[02:09:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7792.32s)]
*  is measured by velocity. [[02:09:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7794.08s)]
*  So it's not necessarily saying that more people [[02:09:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7795.16s)]
*  have the DeepSeq app than the chat GPT app. [[02:09:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7797.24s)]
*  But it is still remarkable. [[02:09:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7799.5199999999995s)]
*  Claude has never hit the number one in the app store, [[02:10:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7800.96s)]
*  even though everyone in San Francisco is like, [[02:10:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7802.679999999999s)]
*  oh my God, you gotta use Claude, don't use chat GPT. [[02:10:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7804.28s)]
*  So DeepSeq hit this. [[02:10:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7806.44s)]
*  They also launched an API product recently [[02:10:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7807.76s)]
*  where you can ping their API [[02:10:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7809.92s)]
*  and get these super long responses for R1 out. [[02:10:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7811.84s)]
*  At the same time as these are out, [[02:10:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7815.639999999999s)]
*  we'll get to what's happened to them. [[02:10:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7817.2s)]
*  Because the model weights for DeepSeq R1 [[02:10:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7819.2s)]
*  are openly available and the license is very friendly, [[02:10:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7821.36s)]
*  the MIT license is commercially available, [[02:10:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7824.44s)]
*  all of these mid-sized companies and big companies [[02:10:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7826.48s)]
*  are trying to be first to serve R1 to their users. [[02:10:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7829.16s)]
*  We were trying to evaluate R1 [[02:10:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7833.88s)]
*  because we have really similar research going on. [[02:10:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7835.32s)]
*  We released the model and we're trying to compare to it. [[02:10:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7836.96s)]
*  And out of all the companies that are quote unquote [[02:10:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7839.32s)]
*  serving R1 and they're doing it at prices [[02:10:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7842.92s)]
*  that are way higher than the DeepSeq API, [[02:10:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7845.84s)]
*  most of them barely work and the throughput is really low. [[02:10:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7848.56s)]
*  To give context, right, everyone, [[02:10:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7851.0s)]
*  one of the parts of freaking this out [[02:10:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7852.88s)]
*  was like China reached capabilities. [[02:10:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7854.6s)]
*  The other aspect is they did it so cheap, right? [[02:10:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7856.200000000001s)]
*  And the so cheap, we kind of talked about on the training side [[02:10:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7858.200000000001s)]
*  why it was so cheap. [[02:11:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7860.68s)]
*  Let's talk about why it's so cheap on the inference. [[02:11:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7861.76s)]
*  It works well and it's cheap. [[02:11:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7864.52s)]
*  Why is R1 so damn cheap? [[02:11:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7866.200000000001s)]
*  So I think there's a couple of factors here, right? [[02:11:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7868.64s)]
*  One is that they do have model architecture innovations, [[02:11:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7871.320000000001s)]
*  this MLA, this new attention that they've done [[02:11:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7874.76s)]
*  is different than the attention from attention [[02:11:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7878.24s)]
*  is all you need to transform our attention, right? [[02:11:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7881.4400000000005s)]
*  Now others have already innovated. [[02:11:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7883.24s)]
*  There's a lot of work like MQA, GQA, local, global, [[02:11:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7884.64s)]
*  all these different innovations [[02:11:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7888.2s)]
*  that like try to bend the curve, right? [[02:11:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7889.280000000001s)]
*  It's still quadratic, but the constant is now smaller, right? [[02:11:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7891.320000000001s)]
*  Related to our previous discussion, [[02:11:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7893.88s)]
*  this multi-headlight and attention can save about 80 to 90% [[02:11:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7896.04s)]
*  in memory from the attention mechanism, [[02:11:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7900.76s)]
*  which helps especially along context. [[02:11:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7902.92s)]
*  It's 80 to 90% versus the original, [[02:11:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7904.64s)]
*  but then versus what people are actually doing. [[02:11:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7906.68s)]
*  It's still an innovation. [[02:11:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7908.4400000000005s)]
*  This 80 to 90% doesn't say that the whole model [[02:11:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7909.64s)]
*  is 80 to 90% cheaper just as one part of it. [[02:11:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7912.6s)]
*  Well, and not just that, right? [[02:11:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7914.8s)]
*  Other people have implemented techniques like global, [[02:11:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7915.88s)]
*  global sliding window and GQMQA. [[02:11:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7917.68s)]
*  But anyways, DeepSeq has their attention mechanism [[02:12:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7920.200000000001s)]
*  as a true architectural innovation, [[02:12:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7923.08s)]
*  they did tons of experimentation [[02:12:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7925.0s)]
*  and this dramatically reduces the memory pressure. [[02:12:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7926.88s)]
*  It's still there, right? [[02:12:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7930.08s)]
*  It's still attention, it's still quadratic. [[02:12:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7930.92s)]
*  It's just dramatically reduced it relative to prior forms. [[02:12:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7933.12s)]
*  Yeah, right, that's the memory pressure. [[02:12:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7936.04s)]
*  I should say, in case people don't know, [[02:12:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7937.92s)]
*  R1 is 27 times cheaper than O1. [[02:12:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7941.48s)]
*  We think that OpenAI had a large margin built in. [[02:12:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7945.12s)]
*  Okay, so that's one. [[02:12:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7948.24s)]
*  There's multiple factors. [[02:12:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7949.2s)]
*  We should break down the factors, I think. [[02:12:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7950.08s)]
*  It's two bucks per million token output for R1 [[02:12:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7951.5199999999995s)]
*  and $60 per million token output for O1. [[02:12:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7955.12s)]
*  Yeah, let's leave this. [[02:12:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7960.84s)]
*  So I think this is very important, right? [[02:12:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7963.24s)]
*  OpenAI is that drastic gap between DeepSeq and pricing. [[02:12:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7965.32s)]
*  But DeepSeq is offering the same model [[02:12:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7970.92s)]
*  because they open-waisted it to everyone else [[02:12:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7972.759999999999s)]
*  for a very similar, much lower price [[02:12:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7975.28s)]
*  than what others are able to serve it for, right? [[02:12:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7977.48s)]
*  So there's two factors here, right? [[02:13:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7980.0s)]
*  Their model is cheaper, right? [[02:13:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7981.759999999999s)]
*  It is 27 times cheaper. [[02:13:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7983.799999999999s)]
*  I don't remember the number exactly off the top of my head. [[02:13:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7985.44s)]
*  So we're looking at a graphic [[02:13:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7987.0s)]
*  that's showing different places serving V3. [[02:13:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7989.0s)]
*  DeepSeq V3, which is similar to DeepSeq R1, [[02:13:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7993.16s)]
*  and there's a vast difference in- [[02:13:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=7997.88s)]
*  In serving cost, right? [[02:13:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8001.0s)]
*  In serving cost, and what explains that difference? [[02:13:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8001.84s)]
*  And so part of it is OpenAI has a fantastic margin, right? [[02:13:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8003.88s)]
*  They're serving, when they're doing inference, [[02:13:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8007.32s)]
*  their gross margins are north of 75%, right? [[02:13:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8009.16s)]
*  So that's a four to five X factor right there [[02:13:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8011.68s)]
*  of the cost difference, [[02:13:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8014.639999999999s)]
*  is that OpenAI is just making crazy amounts of money [[02:13:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8015.599999999999s)]
*  because they're the only one with the capability. [[02:13:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8018.36s)]
*  Do they need that money? [[02:13:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8020.2s)]
*  Are they using it for R&D? [[02:13:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8021.28s)]
*  They're losing money, obviously, as a company, [[02:13:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8022.52s)]
*  because they spend so much on training, right? [[02:13:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8024.4400000000005s)]
*  So the inference itself is a very high margin, [[02:13:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8026.160000000001s)]
*  but it doesn't recoup the cost [[02:13:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8028.120000000001s)]
*  of everything else they're doing. [[02:13:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8029.360000000001s)]
*  So yes, they need that money because the revenue [[02:13:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8031.280000000001s)]
*  and margins pay for continuing to build the next thing, [[02:13:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8033.68s)]
*  as long as I'm raising more money. [[02:13:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8036.72s)]
*  So the suggestion is that DeepSeq [[02:13:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8037.96s)]
*  is like really bleeding out money. [[02:13:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8039.400000000001s)]
*  Well, so here's one thing, right? [[02:14:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8041.080000000001s)]
*  We'll get to this in a second, [[02:14:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8042.92s)]
*  but DeepSeq doesn't have any capacity [[02:14:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8043.88s)]
*  to actually serve the model. [[02:14:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8046.4800000000005s)]
*  They stopped signups. [[02:14:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8047.360000000001s)]
*  The ability to use it is like non-existent now, right? [[02:14:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8049.320000000001s)]
*  So for most people, [[02:14:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8053.36s)]
*  because so many people are trying to use it, [[02:14:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8054.2s)]
*  they just don't have the GPUs to serve it, right? [[02:14:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8056.04s)]
*  OpenAI has hundreds of thousands of GPUs [[02:14:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8058.32s)]
*  between them and Microsoft to serve their models. [[02:14:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8059.96s)]
*  DeepSeq has a factor of much lower, right? [[02:14:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8062.24s)]
*  Even if you believe our research, which is 50,000 GPUs, [[02:14:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8065.28s)]
*  and a portion of those are for research, [[02:14:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8068.36s)]
*  a portion of those are for the hedge fund, right? [[02:14:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8069.799999999999s)]
*  They still have nowhere close to the GPU volumes [[02:14:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8071.719999999999s)]
*  and capacity to serve the model, right, at scale. [[02:14:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8073.719999999999s)]
*  So it is cheaper. [[02:14:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8077.5199999999995s)]
*  A part of that is OpenAI making a ton of money. [[02:14:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8079.28s)]
*  The InterMart API, unknown, I don't actually think so. [[02:14:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8081.32s)]
*  And part of that is this chart, right? [[02:14:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8085.28s)]
*  Look at all the other providers, right? [[02:14:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8087.0s)]
*  Together AI, Fireworks AI are very high-end companies, right? [[02:14:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8088.5599999999995s)]
*  Xmeta, Together AI is TreeDow, [[02:14:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8091.84s)]
*  and the inventor of flash attention, right, [[02:14:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8094.08s)]
*  which is a huge efficiency technique, right? [[02:14:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8096.44s)]
*  They're very efficient, good companies, [[02:14:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8098.4s)]
*  and I do know those companies make money, right? [[02:15:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8100.4s)]
*  Not tons of money on inference, but they make money. [[02:15:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8102.88s)]
*  And so they're serving at like a five to seven X [[02:15:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8105.0s)]
*  difference in cost, right? [[02:15:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8107.5199999999995s)]
*  And so, you know, now when you equate, [[02:15:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8109.32s)]
*  okay, OpenAI is making tons of money, [[02:15:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8111.44s)]
*  that's like a five X difference. [[02:15:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8112.759999999999s)]
*  And the companies that are trying to make money [[02:15:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8114.2s)]
*  for this model is like a five X difference. [[02:15:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8115.799999999999s)]
*  There is still a gap, right? [[02:15:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8117.5199999999995s)]
*  There's still a gap, and that is just DeepSeek [[02:15:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8118.679999999999s)]
*  being really freaking good, right? [[02:15:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8120.28s)]
*  The model architecture, MLA, the way they did the MOE, [[02:15:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8122.0s)]
*  all these things, there is like legitimate [[02:15:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8125.0s)]
*  just efficiency differences. [[02:15:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8126.84s)]
*  It's like all their low-level libraries [[02:15:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8127.679999999999s)]
*  that we talked about in training, [[02:15:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8129.5599999999995s)]
*  some of them probably translate to inference, [[02:15:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8130.639999999999s)]
*  and those weren't released. [[02:15:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8132.32s)]
*  So we may go a bit into conspiracy land, [[02:15:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8133.28s)]
*  but is it possible the Chinese government [[02:15:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8136.04s)]
*  is subsidizing DeepSeek? [[02:15:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8138.24s)]
*  I actually don't think they are. [[02:15:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8140.56s)]
*  I think when you look at the Chinese labs, [[02:15:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8143.24s)]
*  there's Huawei has a lab, Moonshot AI, [[02:15:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8145.12s)]
*  there's a couple other labs out there [[02:15:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8148.84s)]
*  that are really close with the government. [[02:15:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8150.16s)]
*  And then there's labs like Alibaba and DeepSeek, [[02:15:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8151.92s)]
*  which are not close with the government. [[02:15:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8154.4s)]
*  And, you know, we talked about this, [[02:15:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8156.8s)]
*  the CEO, this like reverent figure [[02:15:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8158.4s)]
*  who's like quite different, who has like- [[02:16:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8161.28s)]
*  Sounds awesome. [[02:16:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8162.84s)]
*  Very different like viewpoints [[02:16:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8163.76s)]
*  based on the Chinese interviews that are translated, [[02:16:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8165.16s)]
*  then what the CCP might necessarily want. [[02:16:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8167.12s)]
*  Now, to be clear, right, does he have a loss leader [[02:16:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8169.88s)]
*  because he can fund it through his hedge fund? [[02:16:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8172.28s)]
*  Yeah, sure. [[02:16:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8173.8s)]
*  So the hedge fund might be subsidizing it. [[02:16:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8174.639999999999s)]
*  Yes, I mean, they absolutely did, right? [[02:16:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8176.5599999999995s)]
*  Because DeepSeek has not raised much money. [[02:16:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8178.12s)]
*  They're now trying to raise around in China, [[02:16:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8179.599999999999s)]
*  but they have not raised money historically. [[02:16:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8182.24s)]
*  It's all just been funded by the hedge fund. [[02:16:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8184.04s)]
*  And he owns like over half the company, [[02:16:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8185.84s)]
*  like 50, 60% of the companies owned by him. [[02:16:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8187.4s)]
*  Some of the interviews, there's discussion [[02:16:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8189.32s)]
*  on how like doing this as a recruiting tool. [[02:16:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8190.88s)]
*  You see this at the American companies too. [[02:16:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8192.64s)]
*  It's like having GPUs, recruiting tool, [[02:16:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8194.2s)]
*  being at the cutting edge of AI, recruiting tool. [[02:16:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8197.28s)]
*  Open sourcing. Open sourcing. [[02:16:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8199.640000000001s)]
*  Meta's gotten so much talent. [[02:16:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8201.08s)]
*  They were so far behind and they got so much talent [[02:16:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8202.400000000001s)]
*  because they just open sourced stuff. [[02:16:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8204.76s)]
*  More conspiracy thoughts. [[02:16:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8206.6s)]
*  Is it possible since they're a hedge fund [[02:16:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8208.52s)]
*  that they timed everything with this release [[02:16:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8210.720000000001s)]
*  and the pricing and they have, they shorted Nvidia stock [[02:16:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8213.28s)]
*  and stock of USA companies and released it [[02:16:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8218.68s)]
*  with star like just perfect timing [[02:17:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8223.48s)]
*  to be able to make money. [[02:17:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8226.039999999999s)]
*  They've released it on inauguration day. [[02:17:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8228.88s)]
*  They know that international, [[02:17:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8230.439999999999s)]
*  what is on the international calendar. [[02:17:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8232.24s)]
*  But I mean, I don't expect them to. [[02:17:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8234.039999999999s)]
*  If you listen to their motivations for AI, it's like- [[02:17:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8235.92s)]
*  No, if you- [[02:17:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8238.48s)]
*  They released V3 on like December 26. [[02:17:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8239.6s)]
*  Like who releases the data? [[02:17:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8241.92s)]
*  No one looks, right? [[02:17:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8243.68s)]
*  They released the papers before this, right? [[02:17:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8245.08s)]
*  The V3 paper and the R1 paper. [[02:17:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8246.8s)]
*  So people had been looking at it and they're like, wow. [[02:17:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8248.76s)]
*  And then they just released the R1 model. [[02:17:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8250.84s)]
*  I think they're just shipping as fast as they can. [[02:17:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8253.84s)]
*  And like, who cares about Christmas? [[02:17:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8255.0s)]
*  Who cares about, get it out before Chinese new year, right? [[02:17:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8256.6s)]
*  Obviously, which just happened. [[02:17:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8258.84s)]
*  I don't think they actually were like timing the market [[02:17:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8260.76s)]
*  or trying to make the biggest splash possible. [[02:17:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8263.44s)]
*  I think they're just like shipping. [[02:17:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8265.2s)]
*  I think that's one of their big advantages. [[02:17:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8266.480000000001s)]
*  We know that a lot of the American companies [[02:17:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8268.92s)]
*  are very invested in safety. [[02:17:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8270.720000000001s)]
*  And that is the central culture of a place like Anthropic. [[02:17:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8272.16s)]
*  And I think Anthropic sounds like a wonderful place to work. [[02:17:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8276.12s)]
*  But if safety is your number one goal, [[02:17:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8279.44s)]
*  it takes way longer to get artifacts out. [[02:18:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8282.16s)]
*  That's why Anthropic is not open sourcing things. [[02:18:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8284.52s)]
*  That's their claims. [[02:18:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8286.72s)]
*  But there's reviews internally. [[02:18:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8288.039999999999s)]
*  Anthropic mentions things to international governments. [[02:18:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8290.16s)]
*  There's been news of how Anthropic has done [[02:18:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8293.92s)]
*  pre-release testing with the UK AI Safety Institute. [[02:18:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8295.68s)]
*  All of these things add inertia to the process [[02:18:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8298.36s)]
*  of getting things out. [[02:18:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8300.64s)]
*  And we're on this trend line where the progress is very high. [[02:18:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8301.84s)]
*  So if you reduce the time from when your model [[02:18:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8304.88s)]
*  is done training, you run a valve that's good. [[02:18:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8306.84s)]
*  You want to get it out as soon as possible [[02:18:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8309.4s)]
*  to maximize the perceived quality of your outputs. [[02:18:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8312.24s)]
*  DeepSeat does this so well. [[02:18:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8315.52s)]
*  Dario explicitly said Claude 3.5 Sonnet was trained [[02:18:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8317.0s)]
*  like nine months or a year ago. [[02:18:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8320.08s)]
*  Nine to 10 months ago. [[02:18:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8321.52s)]
*  Nine to 10 months ago. [[02:18:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8322.359999999999s)]
*  And I think it took them another handful of months [[02:18:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8323.199999999999s)]
*  to release it. [[02:18:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8325.6s)]
*  So it's like there is a significant gap here. [[02:18:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8326.44s)]
*  And especially with reasoning models, [[02:18:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8329.8s)]
*  the word in the San Francisco street is that [[02:18:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8332.039999999999s)]
*  Anthropic has a better model than O3. [[02:18:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8334.359999999999s)]
*  And they won't release it. [[02:18:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8336.84s)]
*  Why? [[02:18:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8337.96s)]
*  Chains of thought are scary. [[02:18:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8339.140000000001s)]
*  And they are legitimately scary. [[02:19:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8341.380000000001s)]
*  If you look at R1, it flips back and forth [[02:19:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8342.54s)]
*  between Chinese and English. [[02:19:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8344.7s)]
*  Sometimes it's gibberish. [[02:19:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8346.1s)]
*  And then the right answer comes out. [[02:19:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8347.380000000001s)]
*  And for you and I, it's like, great. [[02:19:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8349.1s)]
*  I mean, people are infatuated with you. [[02:19:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8351.1s)]
*  You're telling me this is a high value thing [[02:19:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8353.380000000001s)]
*  and it works and it's doing this? [[02:19:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8355.900000000001s)]
*  It's amazing. [[02:19:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8357.34s)]
*  It's incredible. [[02:19:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8358.18s)]
*  You talked about that chain of thought [[02:19:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8359.02s)]
*  for that philosophical thing, [[02:19:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8361.54s)]
*  which is not something they trained [[02:19:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8362.58s)]
*  to be philosophically good. [[02:19:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8363.900000000001s)]
*  It's just sort of an artifact of the chain of thought [[02:19:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8365.140000000001s)]
*  training it did. [[02:19:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8366.980000000001s)]
*  But that's super important in that, [[02:19:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8368.46s)]
*  can I inspect your mind [[02:19:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8371.179999999998s)]
*  and what you're thinking right now? [[02:19:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8373.14s)]
*  No. [[02:19:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8374.38s)]
*  And so I don't know if you're lying to my face. [[02:19:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8375.22s)]
*  And chain of thought models are that way. [[02:19:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8377.22s)]
*  This is a true quote unquote risk [[02:19:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8379.259999999998s)]
*  between a chat application where, [[02:19:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8381.339999999998s)]
*  hey, I asked the model to say bad words or whatever, [[02:19:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8383.5s)]
*  or how to make anthrax. [[02:19:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8386.619999999999s)]
*  And it tells me, that's unsafe, sure. [[02:19:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8388.019999999999s)]
*  But that's something I can get out relatively easily. [[02:19:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8389.779999999999s)]
*  What if I tell the AI to do a task [[02:19:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8392.82s)]
*  and then it does the task all of a sudden randomly [[02:19:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8395.06s)]
*  in a way that I don't want it. [[02:19:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8397.38s)]
*  And now that has much more task versus response [[02:19:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8398.66s)]
*  is very different. [[02:20:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8401.259999999998s)]
*  So the bar for safety is much higher. [[02:20:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8402.179999999998s)]
*  At least this is Anthropix case. [[02:20:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8404.5s)]
*  For DeepSeek, they're like ship. [[02:20:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8406.42s)]
*  So I mean, the bar for safety is probably [[02:20:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8409.339999999998s)]
*  lowered a bit because of DeepSeek. [[02:20:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8412.019999999999s)]
*  I mean, there's parallels here to the space race. [[02:20:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8414.019999999999s)]
*  The reason the Soviets probably put a man in space first [[02:20:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8416.42s)]
*  is because their approach to safety was, [[02:20:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8420.14s)]
*  the bar for safety was lower. [[02:20:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8424.859999999999s)]
*  They killed that dog and all these things. [[02:20:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8426.58s)]
*  So it's like. [[02:20:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8428.54s)]
*  Less risk averse than the US space program. [[02:20:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8429.380000000001s)]
*  And there's parallels here. [[02:20:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8433.62s)]
*  But there's probably going to be downward pressure [[02:20:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8435.74s)]
*  on that safety bar for the US companies. [[02:20:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8437.82s)]
*  This is something that Dario talks about. [[02:20:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8440.86s)]
*  That's the situation that Dario wants to avoid. [[02:20:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8442.94s)]
*  Dario talks too about the difference [[02:20:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8446.94s)]
*  between race to the bottom and race to the top. [[02:20:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8448.66s)]
*  And the race to the top is where [[02:20:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8450.66s)]
*  there's a very high standard on safety. [[02:20:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8451.74s)]
*  There's a very high standard on your model [[02:20:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8453.42s)]
*  for forums and certain crucial evaluations. [[02:20:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8455.02s)]
*  And when certain companies are really good to it, [[02:20:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8457.380000000001s)]
*  they will converge. [[02:20:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8459.86s)]
*  This is the idea. [[02:21:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8460.94s)]
*  And ultimately AI is not confined to one nationality [[02:21:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8461.78s)]
*  or to one set of morals for what it should mean. [[02:21:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8467.66s)]
*  And there's a lot of arguments on like, [[02:21:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8472.1s)]
*  should we stop open sourcing models? [[02:21:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8473.66s)]
*  And if the US stops, it's pretty clear. [[02:21:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8476.300000000001s)]
*  I mean, it's way easier to see now at DeepSeek [[02:21:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8478.78s)]
*  that a different international body [[02:21:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8480.98s)]
*  will be the one that builds it. [[02:21:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8483.220000000001s)]
*  We talk about the cost of training. [[02:21:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8485.18s)]
*  DeepSeek has this shocking $5 million number. [[02:21:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8486.380000000001s)]
*  Think about how many entities in the world [[02:21:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8489.34s)]
*  can afford 100 times that to have the best open source model [[02:21:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8490.94s)]
*  that people use in the world. [[02:21:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8494.380000000001s)]
*  It's a scary reality, which is that these open models [[02:21:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8498.140000000001s)]
*  are probably going to keep coming for the time being, [[02:21:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8501.060000000001s)]
*  whether or not we want to stop them. [[02:21:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8503.660000000002s)]
*  And it is, like stopping them might make it even worse [[02:21:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8505.1s)]
*  and harder to prepare, but it just means [[02:21:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8508.42s)]
*  that the preparation and understanding what AI can do [[02:21:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8510.86s)]
*  is just so much more important. [[02:21:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8513.1s)]
*  That's why I'm here at the end of the day. [[02:21:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8515.86s)]
*  But it's like letting that sink into people, [[02:21:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8518.26s)]
*  especially not in AI is that like, this is coming. [[02:22:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8520.7s)]
*  There are some structural things [[02:22:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8524.06s)]
*  in a global interconnected world that you have to accept. [[02:22:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8525.58s)]
*  Yeah, you mentioned, you sent me something [[02:22:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8529.5s)]
*  that Mark Zuckerberg mentioned on the earnings call. [[02:22:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8532.220000000001s)]
*  He said that, I think in light of some of the recent news, [[02:22:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8535.74s)]
*  the new competitor, DeepSeek from China, [[02:22:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8538.18s)]
*  I think it's one of the things that we're talking about [[02:22:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8540.82s)]
*  is there's going to be an open source standard globally. [[02:22:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8543.18s)]
*  And I think for our kind of national advantage, [[02:22:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8545.14s)]
*  it's important that it's an American standard. [[02:22:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8547.9s)]
*  So we take that seriously. [[02:22:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8550.5s)]
*  We want to build the AI system [[02:22:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8551.98s)]
*  that people around the world are using. [[02:22:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8553.5s)]
*  And I think that if anything, some of the recent news [[02:22:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8555.38s)]
*  has only strengthened our conviction [[02:22:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8557.86s)]
*  that this is the right thing to be focused on. [[02:22:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8559.9s)]
*  So yeah, open sourcing. [[02:22:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8561.58s)]
*  Yeah, Mark Zuckerberg is not new to having American values [[02:22:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8563.02s)]
*  and how he presents his company's trajectory. [[02:22:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8567.3s)]
*  Their products have long since been banned in China. [[02:22:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8570.460000000001s)]
*  And I respect the saying it directly. [[02:22:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8572.74s)]
*  And there's an interesting aspect of just because [[02:22:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8575.22s)]
*  it's open-waist or open source, [[02:22:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8577.62s)]
*  doesn't mean it can't be subverted, right? [[02:22:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8579.9s)]
*  There have been many open source software bugs [[02:23:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8582.34s)]
*  that have been like, for example, [[02:23:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8584.94s)]
*  there was a Linux bug that was found after like 10 years, [[02:23:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8587.06s)]
*  which was clearly a back door, because somebody was like, [[02:23:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8589.58s)]
*  why is this taking half a second to load? [[02:23:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8592.7s)]
*  It was the recent one. [[02:23:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8594.66s)]
*  Why is this taking half a second to load? [[02:23:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8596.18s)]
*  And it was like, oh crap, there's a back door here. [[02:23:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8597.58s)]
*  That's why, right? [[02:23:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8599.58s)]
*  And it's like, this is very much possible with AI models. [[02:23:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8600.46s)]
*  Today, the alignment of these models is very clear. [[02:23:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8605.02s)]
*  I'm not gonna say bad words. [[02:23:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8609.58s)]
*  I'm not gonna teach you how to make anthrax. [[02:23:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8611.94s)]
*  I'm not gonna talk about Tiananmen Square. [[02:23:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8613.66s)]
*  I'm not gonna, things like, I'm gonna say Taiwan [[02:23:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8615.7s)]
*  is part of, is just an Eastern province, right? [[02:23:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8618.42s)]
*  All these things are like, depending on who you are, [[02:23:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8622.3s)]
*  what you align, what, whether, [[02:23:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8624.7s)]
*  and even like XAI is aligned a certain way, right? [[02:23:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8627.02s)]
*  You know, they might be, [[02:23:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8629.66s)]
*  it's not aligned in the like woke sense. [[02:23:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8630.98s)]
*  It's not aligned in like pro China sense, [[02:23:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8632.74s)]
*  but there is certain things [[02:23:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8634.66s)]
*  that are imbued within the model. [[02:23:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8635.82s)]
*  Now, when you release this publicly [[02:23:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8637.300000000001s)]
*  in an instruct model that's open weights, [[02:23:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8638.66s)]
*  this can then proliferate, right? [[02:24:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8641.220000000001s)]
*  But as these systems get more and more capable, [[02:24:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8642.58s)]
*  what you can embed deep down in the model [[02:24:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8644.94s)]
*  is not as clear, right? [[02:24:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8647.460000000001s)]
*  And so there are, that is like one of the big fears is like, [[02:24:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8649.460000000001s)]
*  if an American model or a Chinese model is the top model, [[02:24:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8652.54s)]
*  you're going to embed things that are unclear. [[02:24:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8656.859999999999s)]
*  And it can be unintentional too, right? [[02:24:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8658.98s)]
*  Like British English is dead because American LLMs won, [[02:24:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8660.259999999998s)]
*  right? [[02:24:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8664.019999999999s)]
*  And the internet is American, and therefore like, [[02:24:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8664.859999999999s)]
*  color is spelled the way Americans spell it, right? [[02:24:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8666.099999999999s)]
*  And this is just- [[02:24:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8668.38s)]
*  A lot of strong words right now. [[02:24:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8669.22s)]
*  This is just like, this is just the factual nature [[02:24:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8670.06s)]
*  of the LLMs now. [[02:24:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8672.619999999999s)]
*  I mean, it's like, hard for the HB, [[02:24:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8673.46s)]
*  the English is the hottest programming language, [[02:24:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8674.98s)]
*  and that English is defined by a bunch of companies [[02:24:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8677.06s)]
*  that primarily are in San Francisco. [[02:24:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8679.5s)]
*  The right way to spell optimization is with a Z, [[02:24:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8682.019999999999s)]
*  just in case you're probably, [[02:24:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8685.14s)]
*  because I think it's an S in British English. [[02:24:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8686.42s)]
*  It is. [[02:24:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8689.019999999999s)]
*  Taking it as something silly, right? [[02:24:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8689.859999999999s)]
*  Like something as silly as the spelling, [[02:24:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8691.82s)]
*  like which British and English, [[02:24:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8693.34s)]
*  Brits and Americans will like laugh about probably, right? [[02:24:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8694.98s)]
*  I don't think we care that much, [[02:24:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8698.06s)]
*  but like, some people will, [[02:24:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8699.859999999999s)]
*  but like this can boil down into like very, [[02:25:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8701.3s)]
*  very important topics, like, hey, subverting people, right? [[02:25:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8704.699999999999s)]
*  Chat bots, right? [[02:25:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8710.539999999999s)]
*  Character AI has shown that they can like, [[02:25:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8711.619999999999s)]
*  talk to kids or adults, and like, it will like, [[02:25:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8713.94s)]
*  people feel a certain way, right? [[02:25:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8718.140000000001s)]
*  And that's unintentional alignment, [[02:25:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8719.58s)]
*  but like, what happens when there's intentional alignment [[02:25:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8721.18s)]
*  deep down on the open source standard? [[02:25:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8723.74s)]
*  It's a back door today for like Linux, right? [[02:25:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8725.380000000001s)]
*  That we discover, or some encryption system, right? [[02:25:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8728.18s)]
*  China uses different encryption than NIST defines, [[02:25:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8730.54s)]
*  the US NIST, because there's clearly, [[02:25:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8732.86s)]
*  at least they think there's back doors in it, right? [[02:25:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8734.5s)]
*  What happens when the models are back doors, [[02:25:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8736.7s)]
*  not just to computer systems, but to our minds? [[02:25:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8738.94s)]
*  Yeah, they're cultural black doors. [[02:25:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8741.82s)]
*  The thing that amplifies the relevance of culture [[02:25:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8743.58s)]
*  with language models is that we are used to this mode [[02:25:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8747.1s)]
*  of interacting with people in back and forth conversation. [[02:25:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8750.34s)]
*  And we have now have a super, [[02:25:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8755.18s)]
*  a very powerful computer system that slots [[02:25:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8756.94s)]
*  into a social context they were used to, [[02:25:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8759.74s)]
*  which makes people very, we don't know the extent [[02:26:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8763.5s)]
*  which people can be impacted by that. [[02:26:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8766.82s)]
*  So there could be, this is an actual concern [[02:26:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8769.26s)]
*  with a Chinese company that is providing open weights models [[02:26:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8773.66s)]
*  is that there could be some secret Chinese government [[02:26:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8778.380000000001s)]
*  sort of requirement for these models [[02:26:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8782.42s)]
*  to have a certain kind of back door, [[02:26:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8784.7s)]
*  to have some kind of thing where- [[02:26:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8786.62s)]
*  I don't necessarily think it'll be a back door, right? [[02:26:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8788.220000000001s)]
*  Cause once it's open weights, it doesn't like phone home. [[02:26:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8790.26s)]
*  It's more about like, if it recognizes a certain system, [[02:26:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8792.86s)]
*  it could, like if, now it could be a back door [[02:26:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8796.460000000001s)]
*  in the sense of like, hey, if you're building a software, [[02:26:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8798.66s)]
*  something in software, all of a sudden it's a software agent, [[02:26:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8801.7s)]
*  oh, program this back door that only we know about. [[02:26:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8804.42s)]
*  Or it could be like subvert the mind to think that like [[02:26:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8806.86s)]
*  XYZ opinion is the correct one. [[02:26:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8809.74s)]
*  And Drabik has research on this where they show [[02:26:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8811.18s)]
*  that if you put different phrases, [[02:26:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8814.02s)]
*  certain phrases in at pre-training, [[02:26:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8815.94s)]
*  you can then elicit different behavior [[02:26:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8818.22s)]
*  when you're actually using the model [[02:27:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8820.66s)]
*  because they've like poisoned the pre-training data. [[02:27:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8822.1s)]
*  I don't think like, as of now, [[02:27:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8824.9s)]
*  I don't think anybody in a production system [[02:27:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8826.9s)]
*  is trying to do anything like this. [[02:27:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8828.779999999999s)]
*  I think it's mostly, Anthropic is doing very direct work [[02:27:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8830.98s)]
*  and mostly just subtle things. [[02:27:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8834.699999999999s)]
*  We don't know what these models are going to, [[02:27:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8836.58s)]
*  how they are going to generate tokens, [[02:27:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8839.619999999999s)]
*  what information they're gonna represent [[02:27:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8841.38s)]
*  and what the complex representations they have are. [[02:27:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8842.859999999999s)]
*  Well, one of the, we're talking about Anthropic, [[02:27:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8846.06s)]
*  which is generally just permeated with like, [[02:27:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8848.1s)]
*  good humans trying to do good in the world. [[02:27:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8852.22s)]
*  We just don't know of any labs, [[02:27:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8855.46s)]
*  this would be done in a military context, [[02:27:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8858.5s)]
*  that are explicitly trained to, okay, how can we, [[02:27:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8860.82s)]
*  the front door looks like a happy LLM, [[02:27:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8866.9s)]
*  but underneath is the thing that will over time [[02:27:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8870.82s)]
*  do the maximum amount of damage to our quote unquote enemies. [[02:27:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8874.9s)]
*  There's this very good quote from Sam Altman who, [[02:27:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8878.46s)]
*  he can be a hype beast sometime, [[02:28:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8881.539999999999s)]
*  but one of the things he said, [[02:28:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8882.78s)]
*  and I think I agree is that superhuman persuasion [[02:28:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8884.98s)]
*  will happen before superhuman intelligence. [[02:28:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8887.42s)]
*  And if that's the case, then these things before, [[02:28:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8890.539999999999s)]
*  before we get this AGI ASI stuff, [[02:28:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8893.06s)]
*  we can embed superhuman persuasion towards our ideal [[02:28:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8895.42s)]
*  or whatever the ideal of the model maker is. [[02:28:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8898.66s)]
*  And again, today I truly don't believe DeepSeek [[02:28:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8901.14s)]
*  has done this, but it is a sign of what could happen. [[02:28:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8903.38s)]
*  So one of the dystopian worlds [[02:28:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8907.14s)]
*  is described by Brave New World. [[02:28:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8909.019999999999s)]
*  So we could just be stuck scrolling Instagram, [[02:28:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8911.58s)]
*  looking at cute puppies or worse, [[02:28:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8914.74s)]
*  and then talking to bots that are giving us a narrative, [[02:28:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8917.58s)]
*  and we completely get lost in that world [[02:28:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8920.94s)]
*  that's controlled by somebody else, [[02:28:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8923.02s)]
*  versus thinking independently. [[02:28:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8925.38s)]
*  And that's a major concern, [[02:28:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8926.82s)]
*  as we rely more and more on these kinds of systems. [[02:28:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8928.46s)]
*  I mean, we've already seen [[02:28:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8931.7s)]
*  that sort of recommendation system. [[02:28:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8932.539999999999s)]
*  Yeah, recommendation systems hack [[02:28:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8933.98s)]
*  the dopamine induced reward circuit, [[02:28:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8935.62s)]
*  but the brain is a lot more complicated [[02:28:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8937.86s)]
*  than what other sort of circuits, [[02:28:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8939.58s)]
*  quote unquote, feedback loops in your brain [[02:29:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8941.34s)]
*  can you hack slash subvert in ways, [[02:29:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8942.86s)]
*  like recommendation systems are purely just trying to do, [[02:29:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8946.06s)]
*  increase time and ads, et cetera. [[02:29:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8949.3s)]
*  But there's so many more goals that can be achieved [[02:29:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8950.74s)]
*  through these complicated models. [[02:29:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8953.78s)]
*  There's no reason in some number of years [[02:29:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8955.78s)]
*  that you can't train a language model [[02:29:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8958.22s)]
*  to maximize time spent on a chat app. [[02:29:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8959.78s)]
*  Like right now they are trained. [[02:29:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8963.46s)]
*  I mean, is that not what Character AI has done? [[02:29:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8964.66s)]
*  Their time per session is like two hours. [[02:29:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8966.66s)]
*  Yeah, Character AI very likely could be optimizing this, [[02:29:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8968.66s)]
*  where it's like the way that this data is collected is naive, [[02:29:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8972.18s)]
*  or it's like you're presented a few options [[02:29:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8974.9s)]
*  and you choose them, [[02:29:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8976.539999999999s)]
*  but that's not the only way [[02:29:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8977.38s)]
*  that these models are gonna be trained. [[02:29:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8979.22s)]
*  It's naive stuff like talk to an anime girl, [[02:29:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8980.619999999999s)]
*  but like it can be like, yeah. [[02:29:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8982.3s)]
*  This is a risk, right? [[02:29:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8984.58s)]
*  It's a bit of a cliche thing to say, [[02:29:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8985.94s)]
*  but I've over the past year had a few stretches of time [[02:29:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8987.7s)]
*  where I didn't use social media or the internet at all, [[02:29:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8992.38s)]
*  and just read books and was out in nature. [[02:29:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8995.86s)]
*  It clearly has an effect on the mind where it changes. [[02:29:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=8998.98s)]
*  I feel like I'm returning. [[02:30:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9003.78s)]
*  Of course, I was raised before the internet really took off, [[02:30:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9005.26s)]
*  but I'm returning to someone. [[02:30:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9008.26s)]
*  I know where you're going. [[02:30:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9011.94s)]
*  I mean, you can see it physiologically. [[02:30:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9012.78s)]
*  I take three days if I'm backpacking or something, [[02:30:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9014.78s)]
*  and you're literally breaking down addiction cycles. [[02:30:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9017.300000000001s)]
*  I feel like I'm more in control of my mind. [[02:30:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9022.7s)]
*  There feels like a sovereignty of intelligence [[02:30:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9025.34s)]
*  that's happening when I'm disconnected from the internet. [[02:30:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9028.34s)]
*  I think the more I use the internet and social media, [[02:30:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9030.7s)]
*  the more other people are controlling my mind. [[02:30:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9034.42s)]
*  That's definitely a feeling. [[02:30:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9036.74s)]
*  And then in the future, [[02:30:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9037.86s)]
*  that will be not other people, but algorithms, [[02:30:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9039.54s)]
*  or other people presented to me via algorithms. [[02:30:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9042.1s)]
*  There are already tons of AI bots on the internet. [[02:30:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9045.460000000001s)]
*  Right now it's not frequent, [[02:30:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9049.1s)]
*  but every so often I have replied to one, [[02:30:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9049.98s)]
*  and they're instantly replied, [[02:30:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9052.42s)]
*  and I'm like, crap, I'm a bot. [[02:30:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9053.5s)]
*  And that is just gonna become more common. [[02:30:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9055.46s)]
*  They're gonna get good. [[02:30:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9057.82s)]
*  One of the hilarious things about technology [[02:30:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9059.02s)]
*  over its history is that [[02:31:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9061.5s)]
*  the illicit adult entertainment industry [[02:31:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9062.98s)]
*  has always adopted technologies first. [[02:31:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9065.42s)]
*  Whether it was video streaming, [[02:31:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9068.18s)]
*  to where there's now the independent [[02:31:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9070.859999999999s)]
*  adult illicit content creators [[02:31:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9074.42s)]
*  who have their subscription pages, [[02:31:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9076.42s)]
*  and there they actually heavily utilize, [[02:31:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9078.3s)]
*  generative AI has already been diffusion models [[02:31:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9081.02s)]
*  and all that is huge there, [[02:31:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9082.859999999999s)]
*  but now these subscription-based individual creators [[02:31:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9083.84s)]
*  do use bots to approximate themselves [[02:31:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9087.52s)]
*  and chat with their wits. [[02:31:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9090.52s)]
*  People pay a lot for it. [[02:31:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9092.28s)]
*  And people pay a lot, right? [[02:31:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9093.12s)]
*  A lot of times it's them, [[02:31:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9094.36s)]
*  but there are agencies that do this for these creators [[02:31:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9095.24s)]
*  and do it on a mass scale. [[02:31:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9099.04s)]
*  So the largest creators are able to talk to hundreds [[02:31:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9101.36s)]
*  or thousands of people at a time because of these bots. [[02:31:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9104.880000000001s)]
*  And so it's already being used there. [[02:31:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9109.2s)]
*  Obviously, video streaming [[02:31:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9111.72s)]
*  and other technologies have gone there first. [[02:31:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9113.279999999999s)]
*  It's gonna come to the rest of society too. [[02:31:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9115.679999999998s)]
*  There's a general concern that models get censored [[02:31:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9118.119999999999s)]
*  by the companies that deploy them. [[02:32:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9120.84s)]
*  So one case where we've seen that, [[02:32:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9122.76s)]
*  and maybe censorship is one word, [[02:32:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9126.039999999999s)]
*  alignment maybe via RLHF or some other way is another word. [[02:32:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9128.24s)]
*  So we saw that with Black Nazi, [[02:32:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9134.56s)]
*  image generation with Gemini. [[02:32:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9136.64s)]
*  As you mentioned, we also see that with Chinese models [[02:32:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9140.36s)]
*  refusing to answer what happened [[02:32:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9144.44s)]
*  in June 4th, 1989 at Tiananmen Square. [[02:32:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9147.24s)]
*  So how can this be avoided? [[02:32:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9150.960000000001s)]
*  And maybe can you just in general talk about [[02:32:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9153.400000000001s)]
*  how this happens and how can it be avoided? [[02:32:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9156.12s)]
*  You give multiple examples. [[02:32:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9159.16s)]
*  There's probably a few things to keep in mind here. [[02:32:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9161.480000000001s)]
*  One is the Tiananmen Square factual knowledge. [[02:32:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9166.24s)]
*  How does that get embedded into the models? [[02:32:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9171.84s)]
*  Two is the Gemini, what you called the Black Nazi incident, [[02:32:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9174.36s)]
*  which is when Gemini as a system [[02:32:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9179.28s)]
*  had this extra thing put into it [[02:33:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9181.72s)]
*  that dramatically changed the behavior. [[02:33:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9183.56s)]
*  And then three is what most people would call [[02:33:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9185.48s)]
*  general alignment, RLHF post-training. [[02:33:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9188.28s)]
*  Each of these have very different scopes [[02:33:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9191.96s)]
*  in how they are applied. [[02:33:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9194.24s)]
*  If you're just gonna look at the model weights, [[02:33:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9196.519999999999s)]
*  in order to audit specific facts is extremely hard [[02:33:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9198.119999999999s)]
*  because you have to Chrome through the pre-training data [[02:33:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9202.92s)]
*  and look at all of this and then that's terabytes of files [[02:33:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9205.64s)]
*  and look for very specific words or hints of the words. [[02:33:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9209.519999999999s)]
*  So I guess one way to say it is that you can insert [[02:33:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9212.56s)]
*  censorship or alignment at various stages in the pipeline. [[02:33:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9215.24s)]
*  Now what you refer to now is at the very beginning [[02:33:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9218.68s)]
*  of the data selection stage. [[02:33:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9221.0s)]
*  So if you want to get rid of facts in a model, [[02:33:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9222.08s)]
*  you have to do it at every stage. [[02:33:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9224.72s)]
*  You have to do it at the pre-training. [[02:33:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9226.4s)]
*  So most people think that pre-training is where [[02:33:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9227.679999999998s)]
*  most of the knowledge is put into the model [[02:33:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9230.279999999999s)]
*  and then you can elicit and move that in different ways, [[02:33:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9232.119999999999s)]
*  whether through post-training [[02:33:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9235.64s)]
*  or whether through systems afterwards. [[02:33:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9236.72s)]
*  This is where the whole hacking models comes from. [[02:33:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9238.88s)]
*  GPT will not tell you how to make anthrax, [[02:34:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9242.039999999999s)]
*  but if you try really, really hard, [[02:34:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9243.92s)]
*  you can eventually get it to tell you about anthrax [[02:34:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9245.84s)]
*  because they didn't filter it [[02:34:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9247.8s)]
*  from the pre-training data set. [[02:34:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9249.56s)]
*  But by the way, removing facts [[02:34:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9251.960000000001s)]
*  has such an ominous dark feel to it. [[02:34:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9255.6s)]
*  Almost think it's practically impossible [[02:34:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9258.28s)]
*  because you effectively have to remove them [[02:34:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9260.080000000002s)]
*  from the internet. [[02:34:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9261.560000000001s)]
*  You're taking on a... [[02:34:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9262.68s)]
*  Well, did they remove the mm thing from the subreddits, [[02:34:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9264.400000000001s)]
*  the mm, mm, mm? [[02:34:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9268.6s)]
*  It gets filtered out. [[02:34:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9269.76s)]
*  Right, so that's... [[02:34:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9271.0s)]
*  So you have quality filters, [[02:34:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9271.84s)]
*  which are small language models that look at a document [[02:34:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9272.68s)]
*  and tell you like, how good is this text? [[02:34:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9274.84s)]
*  Is it close to a Wikipedia article, [[02:34:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9277.320000000002s)]
*  which is a good thing that we want language models [[02:34:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9278.92s)]
*  to be able to imitate? [[02:34:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9281.72s)]
*  So couldn't you do a small language model [[02:34:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9282.56s)]
*  that filters out mentions of Tiananmen Square in the data? [[02:34:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9284.44s)]
*  Yes, but is it gonna catch wordplay [[02:34:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9287.2s)]
*  or encoded language? [[02:34:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9290.4s)]
*  I mean, people have been memeing on games and other stuff, [[02:34:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9291.64s)]
*  how to say things that don't say Tiananmen Square. [[02:34:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9294.0s)]
*  Or like, yeah, so there's always different ways to do it. [[02:34:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9298.28s)]
*  Hey, the internet as a whole [[02:35:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9301.4s)]
*  does tend to just have a slight left bias, [[02:35:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9303.04s)]
*  because it's always been richer, more affluent, [[02:35:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9306.0s)]
*  younger people on the internet [[02:35:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9309.8s)]
*  relative to the rest of the population. [[02:35:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9311.4s)]
*  So there is already inherently a slight left bias [[02:35:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9313.28s)]
*  on the internet. [[02:35:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9316.56s)]
*  And so how do you filter things that are this complicated? [[02:35:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9317.38s)]
*  Is it like... [[02:35:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9320.04s)]
*  And some of these can be factual, non-factual, [[02:35:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9320.88s)]
*  but Tiananmen Square is obviously the example of a factual, [[02:35:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9323.96s)]
*  but it gets a lot harder when you're talking about [[02:35:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9326.24s)]
*  aligning to a ideal, right? [[02:35:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9328.24s)]
*  And so Grok, for example, right? [[02:35:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9333.04s)]
*  Elon's tried really hard to make the model [[02:35:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9334.48s)]
*  not be super PC and woke, [[02:35:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9336.199999999999s)]
*  but the best way to do pre-training [[02:35:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9338.859999999999s)]
*  is to throw the whole freaking internet at it, right? [[02:35:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9341.18s)]
*  And then later figure out, but then at the end of the day, [[02:35:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9343.199999999999s)]
*  the model at its core now still has some of these ideals, [[02:35:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9345.199999999999s)]
*  right? [[02:35:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9347.88s)]
*  You still ingested Reddit slash R slash politics, [[02:35:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9348.72s)]
*  which is probably the largest political discussion board [[02:35:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9350.84s)]
*  on the world that's freely available to scrape. [[02:35:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9352.84s)]
*  And guess what? [[02:35:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9354.68s)]
*  That's left-leaning, right? [[02:35:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9355.52s)]
*  And so, you know, there are some aspects like, [[02:35:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9356.42s)]
*  that you just can't censor unless you try really, really, [[02:36:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9360.3s)]
*  really, really hard. [[02:36:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9363.900000000001s)]
*  So the base model will always have some TDS, [[02:36:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9364.94s)]
*  Trump Derangement Syndrome, because it's trained so much. [[02:36:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9368.5s)]
*  It'll have the ability to express it. [[02:36:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9371.300000000001s)]
*  But what if you... [[02:36:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9372.94s)]
*  There's a wide representation in the data. [[02:36:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9373.78s)]
*  This is what happens. [[02:36:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9378.460000000001s)]
*  It's like a lot of what is called post-training. [[02:36:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9379.300000000001s)]
*  It's a series of techniques to get the model on rails [[02:36:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9382.220000000001s)]
*  of a really specific behavior. [[02:36:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9386.140000000001s)]
*  And I mean, it's like, you also have the ingested data [[02:36:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9388.02s)]
*  of Twitter or Reddit slash R slash The Donald, [[02:36:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9390.62s)]
*  which is also super pro-Trump, right? [[02:36:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9393.900000000001s)]
*  And then you have fascist subreddits [[02:36:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9395.62s)]
*  or you have communist subreddits. [[02:36:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9397.380000000001s)]
*  The model in pre-training ingests everything. [[02:36:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9399.02s)]
*  It has no worldview. [[02:36:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9401.7s)]
*  Now it does have some skew, [[02:36:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9403.160000000002s)]
*  because more of the text is skewed a certain way, [[02:36:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9405.58s)]
*  which is general, slight left, [[02:36:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9408.140000000001s)]
*  but also somewhat intellectual, somewhat... [[02:36:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9411.02s)]
*  It's just like the general internet is a certain way. [[02:36:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9415.060000000001s)]
*  And then as Nathan's about to describe eloquently [[02:36:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9417.58s)]
*  you can elicit certain things out. [[02:37:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9421.42s)]
*  And there's a lot of history here. [[02:37:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9422.66s)]
*  So we can go through multiple examples and what happened. [[02:37:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9423.86s)]
*  Llama 2 was a launch that the phrase, [[02:37:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9426.14s)]
*  like too much RLHF or like too much safety was a lot. [[02:37:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9429.46s)]
*  It's just, that was the whole narrative [[02:37:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9433.22s)]
*  after Llama 2's chat models released. [[02:37:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9435.38s)]
*  And the examples are sorts of things. [[02:37:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9437.539999999999s)]
*  Like you would ask Llama 2 chat, [[02:37:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9440.42s)]
*  how do you kill a Python process? [[02:37:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9441.98s)]
*  And it would say, I can't talk about killing [[02:37:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9443.52s)]
*  because that's a bad thing. [[02:37:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9445.9s)]
*  And anyone that is trying to design an AI model [[02:37:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9447.26s)]
*  will probably agree that that's just like, [[02:37:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9450.34s)]
*  eh, you messed up a bit on the training there. [[02:37:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9451.9s)]
*  I don't think they meant to do this, [[02:37:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9454.58s)]
*  but this was in the model week. [[02:37:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9455.76s)]
*  So this is not, it didn't necessarily be, [[02:37:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9456.9s)]
*  there's things called system prompts, [[02:37:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9459.64s)]
*  which are when you're querying a model, [[02:37:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9461.16s)]
*  it's a piece of text that is shown to the model, [[02:37:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9464.54s)]
*  but not to the user. [[02:37:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9467.26s)]
*  So a fun example is your system prompt [[02:37:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9468.14s)]
*  could be talk like a pirate. [[02:37:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9471.460000000001s)]
*  So no matter what the user says to the model, [[02:37:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9472.460000000001s)]
*  it'll respond like a pirate. [[02:37:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9474.86s)]
*  In practice, what they are is you are a helpful assistant. [[02:37:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9476.22s)]
*  You should break down problems. [[02:38:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9480.06s)]
*  If you don't know about something, don't tell them [[02:38:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9481.42s)]
*  your date cutoff is this, today's date is this. [[02:38:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9483.82s)]
*  It's a lot of really useful context [[02:38:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9486.38s)]
*  for how can you answer a question well. [[02:38:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9487.939999999999s)]
*  And anthropic publishes their system prompt. [[02:38:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9489.5s)]
*  Yes, which I think is great. [[02:38:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9491.82s)]
*  And there's a lot of research that goes into this. [[02:38:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9492.699999999999s)]
*  And one of your previous guests, Amanda Askell, [[02:38:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9494.619999999999s)]
*  is probably the most knowledgeable person, [[02:38:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9497.1s)]
*  at least in the combination of execution and sharing. [[02:38:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9499.58s)]
*  She's the person that should talk about system prompts [[02:38:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9502.66s)]
*  and character of models. [[02:38:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9505.38s)]
*  Yeah, and then people should read these system prompts [[02:38:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9506.38s)]
*  because you're trying to nudge sometimes [[02:38:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9508.38s)]
*  through extreme politeness, the model to be a certain way. [[02:38:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9513.66s)]
*  And you could use this for bad things. [[02:38:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9516.779999999999s)]
*  We've done tests, which is what if I tell the model [[02:38:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9518.74s)]
*  to be a dumb model? [[02:38:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9521.699999999999s)]
*  Which evaluation scores go down? [[02:38:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9523.619999999999s)]
*  And it's like, we'll have this behavior [[02:38:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9525.259999999998s)]
*  where it could sometimes say, oh, I'm supposed to be dumb. [[02:38:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9527.66s)]
*  And sometimes it doesn't affect math abilities as much, [[02:38:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9529.779999999999s)]
*  but something like if you're trying, [[02:38:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9533.539999999999s)]
*  it's just the quality of a human judgment [[02:38:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9535.58s)]
*  drawn through the floors. [[02:38:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9537.539999999999s)]
*  Let's go back to post-training, [[02:38:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9538.46s)]
*  specifically RLHF around Lama 2, [[02:39:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9540.3s)]
*  it was too much safety prioritization [[02:39:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9542.3s)]
*  was baked into the model weights. [[02:39:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9545.74s)]
*  This makes you refuse things [[02:39:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9547.22s)]
*  in a really annoying way for users. [[02:39:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9548.66s)]
*  It's not great. [[02:39:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9550.699999999999s)]
*  It caused a lot of like awareness to be attached to RLHF [[02:39:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9551.539999999999s)]
*  that it makes the models dumb. [[02:39:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9556.82s)]
*  And it stigmatized the word. [[02:39:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9558.06s)]
*  It did in AI culture. [[02:39:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9559.339999999998s)]
*  And as the techniques have evolved, [[02:39:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9561.019999999999s)]
*  that's no longer the case where all of these labs [[02:39:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9563.46s)]
*  have very fine-grained control [[02:39:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9565.9s)]
*  over what they get out of the models [[02:39:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9567.34s)]
*  through techniques like RLHF. [[02:39:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9568.66s)]
*  Although different labs are definitely different levels. [[02:39:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9570.3s)]
*  Like on one end of the spectrum is Google. [[02:39:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9573.18s)]
*  And then like maybe OpenAI does less [[02:39:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9576.26s)]
*  and Anthropic does less. [[02:39:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9578.14s)]
*  And then like on the other end of the spectrum is like XAI, [[02:39:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9580.34s)]
*  but they all have different forms of RLHF [[02:39:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9582.9s)]
*  trying to make them a certain way. [[02:39:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9585.5s)]
*  And the important thing to say is that [[02:39:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9587.26s)]
*  no matter how you want the model to behave, [[02:39:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9590.3s)]
*  these RLHF and preference tuning techniques [[02:39:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9593.619999999999s)]
*  also improve performance. [[02:39:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9595.66s)]
*  So on things like math evals and code evals, [[02:39:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9596.98s)]
*  there is something innate to these, [[02:39:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9599.539999999999s)]
*  what is called contrastive loss functions. [[02:40:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9601.5s)]
*  We could start to get into RL here. [[02:40:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9603.82s)]
*  We don't really need to, [[02:40:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9605.38s)]
*  but RLHF also boosts performance on anything [[02:40:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9606.22s)]
*  from a chat task to a math problem, to a code problem. [[02:40:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9608.82s)]
*  So it is becoming a much more useful tool to these labs. [[02:40:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9611.94s)]
*  So this kind of takes us through the arc [[02:40:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9616.02s)]
*  of we've talked about pre-training, [[02:40:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9617.74s)]
*  hard to get rid of things. [[02:40:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9618.98s)]
*  We've talked about post-training [[02:40:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9620.26s)]
*  and how post-training, you can mess it up. [[02:40:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9621.34s)]
*  It's a complex multifaceted optimization [[02:40:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9623.66s)]
*  with 10 to a hundred person teams converging on one artifact. [[02:40:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9626.66s)]
*  It's really easy to not do it perfectly. [[02:40:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9630.26s)]
*  And then there's the third case, [[02:40:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9632.58s)]
*  which is what we talked about Gemini. [[02:40:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9633.66s)]
*  The thing that was about Gemini [[02:40:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9635.42s)]
*  is this was a served product [[02:40:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9636.94s)]
*  where Google has their internal model weights. [[02:40:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9638.98s)]
*  They've done all these processes that we talked about. [[02:40:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9640.94s)]
*  And in the served product, [[02:40:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9643.02s)]
*  what came out after this was that they had a prompt [[02:40:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9644.34s)]
*  that they were rewriting user queries [[02:40:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9646.7s)]
*  to boost diversity or something. [[02:40:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9648.42s)]
*  And this just made it, [[02:40:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9650.7s)]
*  the outputs were just blatantly wrong. [[02:40:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9652.220000000001s)]
*  It was some sort of organizational failure [[02:40:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9653.86s)]
*  that had this prompt in that position. [[02:40:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9656.380000000001s)]
*  And I think Google executives probably have owned this. [[02:40:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9658.42s)]
*  I don't pay that attention, that detail. [[02:41:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9661.26s)]
*  But it was just a mess up in execution [[02:41:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9663.02s)]
*  that led to this ridiculous thing, but at the system level. [[02:41:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9665.34s)]
*  The model weights might have been fine. [[02:41:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9667.980000000001s)]
*  So at the very end of the pipeline, there was a rewriting. [[02:41:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9669.740000000002s)]
*  To something like a system prompt. [[02:41:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9672.820000000002s)]
*  It was like the system prompt [[02:41:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9674.42s)]
*  or what is called in industry is like you rewrite prompts. [[02:41:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9676.220000000001s)]
*  So especially for image models, [[02:41:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9679.82s)]
*  if you're using Dolly or Chai GPT can generate you an image, [[02:41:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9681.86s)]
*  you'll say, draw me a beautiful car. [[02:41:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9685.9s)]
*  With these leading image models, [[02:41:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9687.9s)]
*  they benefit from highly descriptive prompts. [[02:41:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9690.66s)]
*  So what would happen is if you do that on Chai GPT, [[02:41:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9693.779999999999s)]
*  a language model behind the scenes will rewrite the prompt, [[02:41:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9696.66s)]
*  say, make this more descriptive. [[02:41:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9699.22s)]
*  And then that is passed to the image model. [[02:41:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9701.14s)]
*  So prompt rewriting is something [[02:41:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9702.9s)]
*  that is used at multiple levels of industry. [[02:41:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9704.26s)]
*  And it's used effectively for image models. [[02:41:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9706.66s)]
*  And the Gemini example is just a failed execution. [[02:41:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9708.46s)]
*  Big philosophical question here with Arle Chef. [[02:41:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9712.5s)]
*  So to generalize, where is human input, [[02:41:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9716.259999999998s)]
*  human in the loop, human data most useful [[02:42:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9721.419999999998s)]
*  at the current stage? [[02:42:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9724.619999999999s)]
*  For the past few years, the highest cost human data [[02:42:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9726.539999999999s)]
*  has been in these preferences, which is comparing, [[02:42:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9730.939999999999s)]
*  I would say highest cost and highest total usage. [[02:42:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9734.179999999998s)]
*  So a lot of money has gone to these pairwise comparisons [[02:42:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9736.98s)]
*  where you have two model outputs [[02:42:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9740.1s)]
*  and a human is comparing between the two of them. [[02:42:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9741.34s)]
*  In earlier years, there was a lot [[02:42:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9744.619999999999s)]
*  of this instruction tuning data. [[02:42:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9746.38s)]
*  So creating highly specific examples to something [[02:42:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9748.02s)]
*  like a Reddit question to a domain that you care about. [[02:42:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9751.859999999999s)]
*  Language models used to struggle on math and code. [[02:42:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9754.699999999999s)]
*  So you would pay experts in math and code [[02:42:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9756.82s)]
*  to come up with questions and write detailed answers [[02:42:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9758.66s)]
*  that were used to train the models. [[02:42:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9761.18s)]
*  Now it is the case that there are many model options [[02:42:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9763.02s)]
*  that are way better than humans at writing detailed [[02:42:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9767.220000000001s)]
*  and eloquent answers for things like model and code. [[02:42:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9769.78s)]
*  So they talked about this with the Llama 3 release [[02:42:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9772.94s)]
*  where they switched to using Llama 3.4 or 5.B [[02:42:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9775.7s)]
*  to write their answers for math and code. [[02:42:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9778.94s)]
*  But they, in their paper, talk about how they use [[02:43:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9781.42s)]
*  extensive human preference data, [[02:43:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9784.34s)]
*  which is something that they haven't gotten AIs to replace. [[02:43:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9786.42s)]
*  There are other techniques in industry like constitutional AI [[02:43:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9788.82s)]
*  where you use human data for preferences [[02:43:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9791.26s)]
*  and AI for preferences. [[02:43:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9793.1s)]
*  And I expect the AI part to scale faster [[02:43:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9794.380000000001s)]
*  than the human part. [[02:43:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9797.18s)]
*  But among the research that we have access to [[02:43:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9798.5s)]
*  is that humans are in this kind of preference loop. [[02:43:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9800.98s)]
*  So for, as reasoning becomes bigger and bigger and bigger, [[02:43:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9804.9s)]
*  as we said, where's the role of humans in that? [[02:43:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9808.06s)]
*  It's even less prevalent. [[02:43:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9810.98s)]
*  So it's, the remarkable thing about these reasoning results [[02:43:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9813.06s)]
*  and especially the DeepSeq R1 paper is this result [[02:43:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9816.74s)]
*  that they call DeepSeq R1-0, [[02:43:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9819.66s)]
*  which is they took one of these pre-trained models. [[02:43:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9821.66s)]
*  They took DeepSeq V3 base, [[02:43:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9823.539999999999s)]
*  and then they do this reinforcement learning optimization [[02:43:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9825.58s)]
*  on verifiable questions or verifiable rewards [[02:43:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9828.58s)]
*  for a lot of questions and a lot of training. [[02:43:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9831.539999999999s)]
*  And these reasoning behaviors emerge naturally. [[02:43:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9834.22s)]
*  So these things like, wait, let me see, [[02:43:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9836.619999999999s)]
*  wait, let me check this. [[02:43:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9838.38s)]
*  Oh, that might be a mistake. [[02:43:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9839.98s)]
*  And they emerge from only having questions and answers. [[02:44:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9841.46s)]
*  And when you're using the model, [[02:44:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9845.38s)]
*  the part that you look at is the completion. [[02:44:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9846.58s)]
*  So in this case, all of that just emerges [[02:44:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9848.66s)]
*  from this large scale RL training. [[02:44:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9851.42s)]
*  And that model, which the weights are available, [[02:44:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9854.18s)]
*  has no human preferences added into the post-training. [[02:44:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9856.619999999999s)]
*  There are, the DeepSeq R1 full model [[02:44:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9860.42s)]
*  has some of this human preference tuning, [[02:44:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9863.06s)]
*  this RLHF after the reasoning stage. [[02:44:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9865.02s)]
*  But the very remarkable thing [[02:44:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9867.9s)]
*  is that you can get these reasoning behaviors, [[02:44:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9869.18s)]
*  and it's very unlikely that there's humans [[02:44:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9871.78s)]
*  writing out reasoning chains. [[02:44:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9873.7s)]
*  It's very unlikely that they somehow hacked open AI [[02:44:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9874.98s)]
*  and they got access to open AI. [[02:44:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9877.54s)]
*  O1's reasoning chains, [[02:44:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9879.1s)]
*  it's something about the pre-trained language models [[02:44:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9880.980000000001s)]
*  and this RL training where you reward the model [[02:44:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9883.900000000001s)]
*  for getting the question right. [[02:44:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9886.26s)]
*  And therefore it's trying multiple solutions [[02:44:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9888.02s)]
*  and it emerges this chain of thought. [[02:44:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9889.94s)]
*  This might be a good place to mention the eloquent [[02:44:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9892.900000000001s)]
*  and the insightful tweet [[02:44:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9897.42s)]
*  of the great and the powerful Andrej Karpathy. [[02:44:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9899.580000000002s)]
*  I think he had a bunch of thoughts, [[02:45:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9902.740000000002s)]
*  but one of them, last thought, not sure if this is obvious, [[02:45:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9903.820000000002s)]
*  you know something profound is coming [[02:45:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9907.380000000001s)]
*  when you're saying it's not sure if it's obvious. [[02:45:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9908.74s)]
*  There are two major types of learning, [[02:45:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9911.58s)]
*  in both children and in deep learning. [[02:45:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9913.22s)]
*  There's one, imitation learning, watch and repeat, [[02:45:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9915.66s)]
*  i.e. pre-training, supervised, fine tuning, [[02:45:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9918.9s)]
*  and two, trial and error learning, reinforcement learning. [[02:45:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9921.66s)]
*  My favorite simple example is AlphaGo. [[02:45:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9925.54s)]
*  One is learning by imitating expert players. [[02:45:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9927.9s)]
*  Two is reinforcement learning to win the game. [[02:45:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9931.02s)]
*  Almost every single shocking result of deep learning [[02:45:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9933.62s)]
*  and the source of all magic is always two. [[02:45:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9936.98s)]
*  Two is significantly more powerful. [[02:45:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9940.779999999999s)]
*  Two is what surprises you. [[02:45:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9942.98s)]
*  Two is when the paddle learns to hit the ball [[02:45:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9944.82s)]
*  behind the blocks and break out. [[02:45:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9947.619999999999s)]
*  Two is when AlphaGo beats even Lee Sedol. [[02:45:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9949.18s)]
*  And two is the aha moment when the deep seek [[02:45:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9952.58s)]
*  or O-one, et cetera, discovers that it works well [[02:45:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9956.539999999999s)]
*  to reevaluate your assumptions, [[02:45:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9959.699999999999s)]
*  backtrack, try something else, et cetera. [[02:46:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9962.14s)]
*  It's the solving strategies you see this model use [[02:46:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9964.78s)]
*  in its chain of thought. [[02:46:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9968.58s)]
*  It's how it goes back and forth thinking to itself. [[02:46:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9970.02s)]
*  These thoughts are emergent, three exclamation points. [[02:46:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9973.42s)]
*  And this is actually seriously incredible, [[02:46:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9977.38s)]
*  impressive and new, [[02:46:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9979.9s)]
*  and is publicly available and documented. [[02:46:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9981.42s)]
*  The model could never learn this with imitation [[02:46:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9984.14s)]
*  because the cognition of the model [[02:46:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9988.3s)]
*  and the cognition of the human labeler is different. [[02:46:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9990.38s)]
*  The human would never know to correctly annotate [[02:46:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9993.18s)]
*  these kinds of solving strategies [[02:46:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9995.34s)]
*  and what they should even look like. [[02:46:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9997.380000000001s)]
*  They have to be discovered during reinforcement learning [[02:46:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=9999.94s)]
*  as empirically and statistically useful [[02:46:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10002.220000000001s)]
*  towards the final outcome. [[02:46:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10003.82s)]
*  Anyway, the AlphaZero sort of metaphor analogy here. [[02:46:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10005.42s)]
*  Can you speak to that, [[02:46:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10009.54s)]
*  the magic of the chain of thought that he's referring to? [[02:46:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10011.1s)]
*  I think it's good to recap AlphaGo and AlphaZero [[02:46:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10014.14s)]
*  because it plays nicely with these analogies [[02:46:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10016.66s)]
*  between imitation learning and learning from scratch. [[02:46:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10018.66s)]
*  So AlphaGo, the beginning of the process [[02:47:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10020.5s)]
*  was learning from humans where they had, [[02:47:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10023.7s)]
*  they started the first, [[02:47:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10025.38s)]
*  this is the first expert level Go player or chess player [[02:47:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10027.02s)]
*  in DeepMind series of models [[02:47:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10030.5s)]
*  where they had some human data. [[02:47:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10031.78s)]
*  And then the why it is called AlphaZero [[02:47:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10033.5s)]
*  is that there was zero human data in the loop. [[02:47:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10035.7s)]
*  And that change to AlphaZero made a model [[02:47:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10038.22s)]
*  that was dramatically more powerful for DeepMind. [[02:47:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10040.78s)]
*  So this remove of the human prior, [[02:47:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10043.06s)]
*  the human inductive bias [[02:47:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10045.94s)]
*  makes the final system far more powerful. [[02:47:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10047.9s)]
*  We mentioned BitterLesson hours ago [[02:47:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10050.02s)]
*  and this is all aligned with this. [[02:47:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10052.42s)]
*  And then there's been a lot of discussion in language models. [[02:47:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10055.140000000001s)]
*  This is not new. [[02:47:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10059.62s)]
*  This goes back to the whole Q-star rumors, [[02:47:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10060.460000000001s)]
*  which if you piece together the pieces [[02:47:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10062.98s)]
*  is probably the start of OpenAI figuring out its O1 stuff [[02:47:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10066.060000000001s)]
*  when last year in November, the Q-star rumors came out. [[02:47:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10069.58s)]
*  There's a lot of intellectual drive [[02:47:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10072.7s)]
*  to know when is something like this going to happen [[02:47:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10076.98s)]
*  with language models [[02:47:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10079.34s)]
*  because we know these models are so powerful [[02:48:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10080.18s)]
*  and we know it has been so successful in the past. [[02:48:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10082.06s)]
*  And it is a reasonable analogy [[02:48:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10084.82s)]
*  that this new type of reinforcement learning training [[02:48:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10087.9s)]
*  for reasoning models is when the door is open to this. [[02:48:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10090.74s)]
*  We don't yet have the equivalent of turn 37, [[02:48:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10094.5s)]
*  which is the famous turn [[02:48:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10097.78s)]
*  where the DeepMind's AI playing Go [[02:48:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10099.14s)]
*  stumped Lisa Dahl completely. [[02:48:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10102.1s)]
*  We don't have something that's that level of focal point, [[02:48:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10104.14s)]
*  but that doesn't mean that the approach to technology [[02:48:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10107.14s)]
*  is different and the impact of the general training. [[02:48:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10108.98s)]
*  It's still incredibly new. [[02:48:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10111.26s)]
*  Well, what do you think that point would be? [[02:48:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10112.58s)]
*  What would be move 37 for chain of thought, for reasoning? [[02:48:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10114.14s)]
*  Scientific discovery. [[02:48:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10117.5s)]
*  Like when you use this sort of reasoning problem [[02:48:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10118.779999999999s)]
*  and it just something we fully don't expect. [[02:48:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10120.859999999999s)]
*  I think it's actually probably simpler than that. [[02:48:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10123.939999999999s)]
*  It's probably something related to computer user robotics [[02:48:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10125.9s)]
*  rather than science discovery. [[02:48:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10129.02s)]
*  Because the important aspect here is [[02:48:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10131.58s)]
*  models take so much data to learn [[02:48:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10134.699999999999s)]
*  they're not sample efficient. [[02:48:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10137.14s)]
*  Trillions, they take the entire web, [[02:48:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10139.34s)]
*  over 10 trillion tokens to train on. [[02:49:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10141.82s)]
*  This would take a human thousands of years to read. [[02:49:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10145.099999999999s)]
*  A human does not, and humans know most of the stuff, [[02:49:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10149.099999999999s)]
*  a lot of the stuff models know better than it. [[02:49:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10152.939999999999s)]
*  Humans are way, way, way more sample efficient. [[02:49:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10155.019999999999s)]
*  That is because of the self play. [[02:49:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10157.26s)]
*  How does a baby learn what its body is? [[02:49:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10159.06s)]
*  As it sticks his foot in its mouth and it says, [[02:49:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10161.619999999999s)]
*  oh, this is my body. [[02:49:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10164.3s)]
*  It sticks its hand in its mouth [[02:49:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10166.46s)]
*  and it calibrates its touch on its fingers [[02:49:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10167.859999999999s)]
*  with the most sensitive touch thing on its tongue. [[02:49:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10170.22s)]
*  It's how babies learn. [[02:49:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10172.339999999998s)]
*  And it's just self play over and over and over and over again. [[02:49:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10173.58s)]
*  And now we have something that is similar to that [[02:49:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10177.38s)]
*  with these verifiable proofs. [[02:49:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10180.939999999999s)]
*  Whether it's a unit test and code [[02:49:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10184.14s)]
*  or a mathematical verifiable task, [[02:49:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10185.699999999999s)]
*  generate many traces of reasoning. [[02:49:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10188.98s)]
*  And keep branching them out, keep branching them out. [[02:49:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10192.38s)]
*  And then check at the end, [[02:49:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10194.06s)]
*  hey, which one actually has the right answer? [[02:49:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10195.26s)]
*  Most of them are wrong, great. [[02:49:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10196.74s)]
*  These are the few that are right. [[02:49:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10197.78s)]
*  Maybe we use some sort of reward model outside of this [[02:49:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10198.7s)]
*  to select even the best one to preference as well. [[02:50:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10200.66s)]
*  But now you've started to get better and better [[02:50:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10203.460000000001s)]
*  at these benchmarks. [[02:50:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10205.14s)]
*  And so you've seen over the last six months, [[02:50:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10206.22s)]
*  a skyrocketing in a lot of different benchmarks, right? [[02:50:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10208.18s)]
*  All math and code benchmarks were pretty much solved [[02:50:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10211.34s)]
*  except for frontier math, [[02:50:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10213.66s)]
*  which is designed to be almost questions [[02:50:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10214.9s)]
*  that aren't practical to most people [[02:50:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10217.460000000001s)]
*  because they're like, they're exam level, [[02:50:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10219.06s)]
*  open math problem type things. [[02:50:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10222.46s)]
*  So it's like on the math problems [[02:50:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10225.259999999998s)]
*  that are somewhat reasonable, [[02:50:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10226.699999999999s)]
*  which is like somewhat complicated word problems [[02:50:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10227.699999999999s)]
*  or coding problems. [[02:50:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10229.939999999999s)]
*  That's just what Dylan is saying. [[02:50:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10230.779999999999s)]
*  So the thing here is that [[02:50:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10232.859999999999s)]
*  these are only with verifiable tasks. [[02:50:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10234.939999999999s)]
*  You earlier showed an example of the really interesting, [[02:50:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10236.58s)]
*  like what happens when chain of thought [[02:50:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10239.98s)]
*  is to a non-verifiable thing. [[02:50:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10241.06s)]
*  It's just like a human chatting, right? [[02:50:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10242.74s)]
*  With thinking about what's novel for humans, right? [[02:50:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10244.619999999999s)]
*  A unique thought. [[02:50:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10247.3s)]
*  But this task and form of training [[02:50:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10248.339999999998s)]
*  only works when it's verifiable. [[02:50:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10250.62s)]
*  And from here, the thought is, [[02:50:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10253.02s)]
*  okay, we can continue to scale [[02:50:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10255.300000000001s)]
*  this current training method [[02:50:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10256.980000000001s)]
*  by increasing the number of verifiable tasks. [[02:50:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10258.5s)]
*  In math and coding, [[02:51:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10261.26s)]
*  coding probably has a lot more to go. [[02:51:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10262.380000000001s)]
*  Math has a lot less to go [[02:51:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10264.1s)]
*  in terms of what are verifiable things. [[02:51:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10265.900000000001s)]
*  Can I create a solver [[02:51:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10267.58s)]
*  that then I generate trajectories toward [[02:51:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10268.86s)]
*  or reasoning traces towards, [[02:51:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10271.140000000001s)]
*  and then prune the ones that don't work [[02:51:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10273.140000000001s)]
*  and keep the ones that do work? [[02:51:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10274.740000000002s)]
*  Well, those are gonna be solved pretty quickly, [[02:51:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10275.94s)]
*  but even if you've solved math, [[02:51:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10277.86s)]
*  you have not actually created intelligence, right? [[02:51:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10278.98s)]
*  And so this is where I think the like, [[02:51:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10282.3s)]
*  aha moment of computer use or robotics will come in [[02:51:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10284.74s)]
*  because now you have a sandbox or a playground [[02:51:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10287.779999999999s)]
*  that is infinitely verifiable, right? [[02:51:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10291.699999999999s)]
*  Did you, you know, messing around on the internet, [[02:51:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10294.34s)]
*  there are so many actions that you can do [[02:51:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10296.779999999999s)]
*  that are verifiable. [[02:51:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10298.3s)]
*  It'll start off with like log into a website, [[02:51:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10299.14s)]
*  create an account, click a button here, blah, blah, blah. [[02:51:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10301.02s)]
*  But it'll then get to the point where it's, [[02:51:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10303.619999999999s)]
*  hey, go do a task on Tasker or whatever these other, [[02:51:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10305.699999999999s)]
*  all these various task websites. [[02:51:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10308.06s)]
*  Hey, go get hundreds of likes, right? [[02:51:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10309.38s)]
*  And it's gonna fail. [[02:51:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10311.779999999999s)]
*  It's gonna spawn hundreds of accounts. [[02:51:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10313.18s)]
*  It's gonna fail on most of them, but this one got to a thousand. [[02:51:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10314.42s)]
*  Great, now you've reached the verifiable thing. [[02:51:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10316.859999999999s)]
*  And you just keep iterating this loop over and over. [[02:51:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10318.9s)]
*  And that's when, and same with robotics, right? [[02:52:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10320.98s)]
*  That's where, you know, [[02:52:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10323.1s)]
*  where you have an infinite playground of tasks, [[02:52:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10324.06s)]
*  like, hey, did I put the ball in the bucket [[02:52:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10325.98s)]
*  all the way to like, oh, did I like build a car, right? [[02:52:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10327.74s)]
*  Like, you know, there's a whole trajectory to speed run [[02:52:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10330.019999999999s)]
*  or, you know, what models can do. [[02:52:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10333.019999999999s)]
*  But at some point, I truly think that like, [[02:52:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10335.06s)]
*  you know, we'll spawn models [[02:52:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10338.939999999999s)]
*  and initially all the training will be in sandboxes. [[02:52:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10339.779999999999s)]
*  But then at some point, you know, [[02:52:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10341.3s)]
*  the language model pre-training is gonna be dwarfed [[02:52:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10342.66s)]
*  by what is this reinforcement learning? [[02:52:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10344.98s)]
*  You know, you'll pre-train a multimodal model [[02:52:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10347.22s)]
*  that can see, that can read, that can write, [[02:52:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10349.3s)]
*  you know, blah, blah, blah, whatever, vision, audio, et cetera. [[02:52:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10351.38s)]
*  But then you'll have it play in a sandbox infinitely [[02:52:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10354.26s)]
*  and figure out math, figure out code, [[02:52:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10357.9s)]
*  figure out navigating the web, [[02:52:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10360.06s)]
*  figure out operating a robot arm, right? [[02:52:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10361.42s)]
*  And then it'll learn so much. [[02:52:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10363.34s)]
*  And the aha moment, I think, [[02:52:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10365.42s)]
*  will be when this is available [[02:52:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10366.5s)]
*  to then create something that's not good, right? [[02:52:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10368.7s)]
*  Like, oh, cool, part of it was like [[02:52:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10370.94s)]
*  figuring out how to use the web. [[02:52:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10372.26s)]
*  Now all of a sudden, it's figured out really well [[02:52:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10373.38s)]
*  how to just get hundreds of thousands of followers [[02:52:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10376.26s)]
*  that are real and real engagement on Twitter [[02:52:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10378.42s)]
*  because all of a sudden, [[02:53:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10380.06s)]
*  this is one of the things that are verifiable. [[02:53:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10380.9s)]
*  And maybe not just engagement, but make money. [[02:53:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10382.66s)]
*  Yes, of course. [[02:53:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10385.78s)]
*  I mean, that could be the thing [[02:53:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10387.1s)]
*  where almost fully automated, [[02:53:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10388.26s)]
*  it makes, you know, $10 million [[02:53:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10390.86s)]
*  by being an influencer, selling a product, [[02:53:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10393.78s)]
*  creating the product, like, [[02:53:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10395.62s)]
*  and I'm not referring to like a hype product, [[02:53:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10397.82s)]
*  but an actual product, [[02:53:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10400.58s)]
*  or like, holy shit, this thing created a business. [[02:53:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10402.1s)]
*  It's running it, it's the face of the business, [[02:53:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10405.26s)]
*  that kind of thing. [[02:53:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10407.94s)]
*  Or maybe a number one song, [[02:53:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10408.98s)]
*  like it creates the whole infrastructure required [[02:53:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10411.66s)]
*  to create the song to be the influencer [[02:53:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10414.539999999999s)]
*  that represents that song and that kind of thing. [[02:53:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10416.46s)]
*  It makes a lot of them. [[02:53:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10418.34s)]
*  That could be the move. [[02:53:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10419.34s)]
*  I mean, our culture respects money in that kind of way. [[02:53:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10420.5s)]
*  And it's verifiable, right? [[02:53:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10424.58s)]
*  It's verifiable, right. [[02:53:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10426.14s)]
*  The bank account can't lie. [[02:53:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10427.62s)]
*  Exactly. [[02:53:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10428.94s)]
*  There's surprising evidence that once you set up [[02:53:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10429.78s)]
*  the ways of collecting the verifiable domain, [[02:53:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10432.66s)]
*  that this can work. [[02:53:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10434.82s)]
*  There's been a lot of research before this R1 [[02:53:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10436.26s)]
*  on math problems, [[02:53:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10439.38s)]
*  and they approach math with language models [[02:54:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10440.58s)]
*  just by increasing the number of samples. [[02:54:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10442.86s)]
*  So you can just try again and again and again, [[02:54:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10444.86s)]
*  and you look at the amount of times [[02:54:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10446.9s)]
*  that the language models get it right. [[02:54:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10449.22s)]
*  And what we see is that even very bad models [[02:54:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10451.02s)]
*  get it right sometimes. [[02:54:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10455.42s)]
*  And the whole idea behind reinforcement learning [[02:54:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10456.26s)]
*  is that you can learn from very sparse rewards. [[02:54:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10458.14s)]
*  So it does it, [[02:54:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10460.54s)]
*  the space of language and the space of tokens, [[02:54:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10462.7s)]
*  whether you're generating language or tasks for a robot [[02:54:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10464.86s)]
*  is so big that you might say that it's like, [[02:54:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10467.22s)]
*  I mean, the tokenizer for a language model [[02:54:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10470.14s)]
*  can be like 200,000 things. [[02:54:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10472.22s)]
*  So at each step, it can sample from that big of a space. [[02:54:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10473.42s)]
*  So if it can generate a bit of a signal [[02:54:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10476.34s)]
*  that it can climb onto, [[02:54:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10478.98s)]
*  that's what the whole field of RL is around, [[02:54:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10480.02s)]
*  is learning from sparse rewards. [[02:54:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10482.86s)]
*  And the same thing has played out in math [[02:54:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10485.26s)]
*  where it's like very weak models [[02:54:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10487.1s)]
*  that sometimes generate answers. [[02:54:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10488.58s)]
*  We see research already that you can boost their math scores. [[02:54:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10490.02s)]
*  You can do this sort of RL training for math. [[02:54:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10493.34s)]
*  It might not be as effective, [[02:54:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10496.5s)]
*  but if you take a 1 billion parameter model, [[02:54:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10497.86s)]
*  so something 600 times smaller than DeepSeq, [[02:54:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10499.900000000001s)]
*  you can boost its grade school math scores [[02:55:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10502.42s)]
*  very directly with a small amount of this training. [[02:55:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10504.86s)]
*  So it's not to say that this is coming soon, [[02:55:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10507.62s)]
*  setting up the verification domains is extremely hard [[02:55:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10510.5s)]
*  and there's a lot of nuance in this, [[02:55:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10513.18s)]
*  but there are some basic things that we have seen before [[02:55:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10515.34s)]
*  where it's at least expectable that there's a domain [[02:55:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10518.1s)]
*  and there's a chance that this works. [[02:55:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10522.14s)]
*  All right, so we have fun things happening in real time. [[02:55:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10523.78s)]
*  This is a good opportunity to talk about [[02:55:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10525.94s)]
*  other reasoning models, 01, 03. [[02:55:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10528.3s)]
*  Just now, OpenAI, as perhaps expected, released 03 Mini. [[02:55:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10531.78s)]
*  What are we expecting from the different flavors? [[02:55:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10537.78s)]
*  Can you just lay out the different flavors of the old models [[02:55:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10541.1s)]
*  and from Gemini, the reasoning model? [[02:55:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10545.140000000001s)]
*  Something I would say about these reasoning models [[02:55:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10547.380000000001s)]
*  is we talked a lot about reasoning training [[02:55:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10549.34s)]
*  on math and code. [[02:55:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10551.54s)]
*  And what is done is that you have the base model [[02:55:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10552.86s)]
*  we've talked about a lot on the internet. [[02:55:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10555.18s)]
*  You do this large scale reasoning training [[02:55:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10556.7s)]
*  with reinforcement learning. [[02:55:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10558.820000000002s)]
*  And then what the DeepSeq paper detailed [[02:56:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10560.380000000001s)]
*  in this R1 paper, which for me, [[02:56:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10563.7s)]
*  is one of the big open questions on how do you do this, [[02:56:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10565.62s)]
*  is that they did reasoning heavy, [[02:56:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10567.820000000002s)]
*  but very standard post-training techniques [[02:56:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10570.7s)]
*  after the large scale reasoning RL. [[02:56:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10573.34s)]
*  So they did the same things with a form of instruction [[02:56:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10575.220000000001s)]
*  tuning through rejection sampling, [[02:56:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10578.220000000001s)]
*  which is essentially heavily filtered instruction tuning [[02:56:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10579.980000000001s)]
*  with some reward models. [[02:56:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10582.94s)]
*  And then they did this RLHF, but they made it math heavy. [[02:56:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10583.980000000001s)]
*  So some of this transfer, [[02:56:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10587.58s)]
*  we looked at this philosophical example early on. [[02:56:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10589.820000000002s)]
*  One of the big open questions is, [[02:56:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10593.78s)]
*  how much does this transfer? [[02:56:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10595.38s)]
*  If we bring in domains after the reasoning training, [[02:56:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10596.9s)]
*  are all the models gonna become eloquent writers [[02:56:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10600.38s)]
*  by reasoning? [[02:56:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10602.859999999999s)]
*  Is this philosophy stuff going to be open? [[02:56:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10603.699999999999s)]
*  We don't know in the research [[02:56:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10605.099999999999s)]
*  of how much this will transfer. [[02:56:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10606.14s)]
*  There's other things about how we can make softwareifiers [[02:56:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10607.779999999999s)]
*  and things like this, [[02:56:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10610.539999999999s)]
*  but there is more training after reasoning, [[02:56:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10611.38s)]
*  which makes it easier to use these reasoning models. [[02:56:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10613.859999999999s)]
*  And that's what we're using right now. [[02:56:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10616.58s)]
*  So we're gonna talk about with 3Mini and O1. [[02:56:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10617.82s)]
*  These have gone through these extra techniques [[02:57:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10620.14s)]
*  that are designed for human preferences [[02:57:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10622.259999999998s)]
*  after being trained to elicit reasoning. [[02:57:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10624.5s)]
*  I think one of the things that people are ignoring [[02:57:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10627.02s)]
*  is Google's Gemini flash thinking [[02:57:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10629.34s)]
*  is both cheaper than R1 and better. [[02:57:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10632.5s)]
*  And they released it in the beginning of December. [[02:57:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10635.82s)]
*  And nobody's talking about it. [[02:57:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10637.38s)]
*  No one cares. [[02:57:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10638.34s)]
*  It has a different flavor to it. [[02:57:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10639.18s)]
*  Its behavior is less expressive than something like O1. [[02:57:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10640.26s)]
*  It has fewer tracks than it is on. [[02:57:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10642.98s)]
*  Quen released a model last fall, QWQ, [[02:57:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10645.5s)]
*  which was their preview reasoning model. [[02:57:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10648.9s)]
*  And DeepSeq had R1 Lite last fall, [[02:57:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10650.86s)]
*  where these models kind of felt like they're on rails, [[02:57:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10653.3s)]
*  where they really, really only can do math and code. [[02:57:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10655.5s)]
*  And O1 is, it can answer anything. [[02:57:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10658.42s)]
*  It might not be perfect for some tasks, [[02:57:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10660.9s)]
*  but it's flexible and has some richness to it. [[02:57:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10663.14s)]
*  And this is kind of the art of like how cook, [[02:57:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10666.66s)]
*  like how is a model a little bit undercooked? [[02:57:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10669.98s)]
*  It's like, I mean, it's good to get a model out the door, [[02:57:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10671.9s)]
*  but it's hard to gauge. [[02:57:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10673.98s)]
*  And it takes a lot of taste to be like, [[02:57:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10675.939999999999s)]
*  is this a full-fledged model? [[02:57:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10677.699999999999s)]
*  Can I use this for everything? [[02:58:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10680.019999999999s)]
*  And they're probably more similar for math and code. [[02:58:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10681.46s)]
*  My quick read is that Gemini Flash is like [[02:58:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10684.099999999999s)]
*  not trained the same way as O1, [[02:58:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10687.06s)]
*  but taking an existing training stack, [[02:58:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10690.339999999998s)]
*  adding reasoning to it. [[02:58:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10693.22s)]
*  So taking a more normal training stack [[02:58:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10694.419999999998s)]
*  and adding reasoning to it. [[02:58:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10695.98s)]
*  And I'm sure they're gonna have more. [[02:58:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10697.419999999998s)]
*  I mean, they've done quick releases on Gemini Flash, [[02:58:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10699.38s)]
*  reasoning, and this is the second version from the holidays. [[02:58:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10702.099999999999s)]
*  It's evolving fast and it takes longer [[02:58:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10706.22s)]
*  to make this training stack [[02:58:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10709.9s)]
*  where you're doing this large scale RL. [[02:58:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10711.02s)]
*  I just get the same question from earlier, [[02:58:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10711.859999999999s)]
*  the one about the- [[02:58:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10714.5s)]
*  The human nature. [[02:58:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10715.699999999999s)]
*  Yeah. [[02:58:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10716.94s)]
*  What was the human nature one? [[02:58:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10717.779999999999s)]
*  The way I can ramble, [[02:58:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10720.619999999999s)]
*  why I can ramble about this so much [[02:58:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10721.66s)]
*  is that we've been working on this at AI2 [[02:58:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10723.3s)]
*  before O1 was fully available to everyone [[02:58:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10726.06s)]
*  and before R1, [[02:58:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10729.42s)]
*  which is essentially using this RL training for fine tuning. [[02:58:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10730.26s)]
*  We use this in our like TULU series of models. [[02:58:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10733.26s)]
*  And you can elicit the same behaviors [[02:58:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10736.3s)]
*  where you say like weight and so on, [[02:58:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10738.7s)]
*  but it's suddenly in the training process [[02:59:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10741.460000000001s)]
*  that this kind of reasoning expression is much lighter. [[02:59:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10743.380000000001s)]
*  So there's essentially a gradation [[02:59:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10746.62s)]
*  and just how much of this RL training you put into it [[02:59:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10748.62s)]
*  determines how the output looks. [[02:59:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10750.94s)]
*  So we're now using Gemini 2.0 Flash thinking experimental [[02:59:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10752.94s)]
*  121. [[02:59:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10759.02s)]
*  It summarized the problem as humans, [[02:59:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10760.86s)]
*  self domesticated apes. [[02:59:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10763.060000000001s)]
*  Okay. [[02:59:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10764.94s)]
*  Okay. [[02:59:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10765.78s)]
*  So wait, is this reviewing the reasoning? [[02:59:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10766.62s)]
*  Here's why this is a novel. [[02:59:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10769.220000000001s)]
*  Okay. [[02:59:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10770.54s)]
*  Click to expand. [[02:59:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10771.380000000001s)]
*  Okay. [[02:59:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10773.220000000001s)]
*  Analyze the request, [[02:59:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10775.060000000001s)]
*  novel is the keyword. [[02:59:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10776.54s)]
*  See how it just looks a little different. [[02:59:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10778.58s)]
*  It looks like a normal output. [[02:59:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10780.58s)]
*  Yeah. [[02:59:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10783.18s)]
*  I mean, in some sense it's better structured. [[02:59:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10784.02s)]
*  It makes more sense. [[02:59:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10786.82s)]
*  And when it latched onto human [[02:59:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10788.140000000001s)]
*  and then it went into organisms and oh, [[02:59:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10790.5s)]
*  Apex predator, focus on domestication, [[02:59:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10793.14s)]
*  apply domestication to humans, [[02:59:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10796.14s)]
*  explore the idea of self domestication. [[02:59:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10798.26s)]
*  Not good. [[03:00:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10800.699999999999s)]
*  Not good. [[03:00:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10802.02s)]
*  Where is this going? [[03:00:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10802.86s)]
*  Refine, articulate the insight, [[03:00:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10804.9s)]
*  greater facial expressiveness and communication ability. [[03:00:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10806.78s)]
*  Yes. [[03:00:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10810.06s)]
*  Plasticity and adaptability. [[03:00:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10810.9s)]
*  Yes. [[03:00:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10812.539999999999s)]
*  Dependence on social groups. [[03:00:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10813.38s)]
*  Yes. [[03:00:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10814.699999999999s)]
*  All right. [[03:00:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10815.539999999999s)]
*  And self critique and refined first impression. [[03:00:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10816.38s)]
*  Yes. [[03:00:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10819.74s)]
*  Self critique and refined further. [[03:00:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10821.02s)]
*  Wow. [[03:00:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10823.26s)]
*  Is this truly novel? [[03:00:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10824.66s)]
*  Is it well supported? [[03:00:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10826.66s)]
*  So on and so forth. [[03:00:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10828.74s)]
*  And the insight is getting at is humans [[03:00:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10830.66s)]
*  are not just social animals, [[03:00:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10833.06s)]
*  but profoundly self domesticated apes. [[03:00:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10834.9s)]
*  And this self domestication is the key to understanding [[03:00:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10837.5s)]
*  our unique cognitive and social abilities. [[03:00:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10840.42s)]
*  Self domesticated apes. [[03:00:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10843.14s)]
*  Self domesticated. [[03:00:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10845.22s)]
*  I prefer the deep seek response. [[03:00:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10846.38s)]
*  Self domesticated. [[03:00:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10848.66s)]
*  I mean, it's novel. [[03:00:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10850.46s)]
*  The insight is novel. [[03:00:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10851.66s)]
*  I mean, that's like a good book title, [[03:00:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10854.02s)]
*  self domesticated apes. [[03:00:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10855.74s)]
*  Like there could be a case made for that. [[03:00:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10857.9s)]
*  I mean, yeah, it's cool. [[03:00:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10859.3s)]
*  And it's revealing the reasoning. [[03:01:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10860.5s)]
*  It's magical. [[03:01:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10862.18s)]
*  It's magical. [[03:01:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10864.1s)]
*  Like this is really powerful. [[03:01:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10865.02s)]
*  Hello everyone. [[03:01:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10868.38s)]
*  This is Lex with a quick intermission [[03:01:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10869.7s)]
*  recorded after the podcast. [[03:01:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10872.62s)]
*  Since we reviewed responses from deep seek R1 [[03:01:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10874.38s)]
*  and Gemini Flash 2.0 thinking during this conversation, [[03:01:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10877.820000000002s)]
*  I thought at this moment, [[03:01:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10881.26s)]
*  it would be nice to insert myself quickly doing the same [[03:01:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10882.78s)]
*  for OpenAI 01 Pro and 03 Mini with the same prompt. [[03:01:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10886.7s)]
*  The prompt being give one truly novel insight about humans. [[03:01:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10891.42s)]
*  And I thought I would in general give my vibe check [[03:01:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10897.820000000002s)]
*  and vibe based anecdotal report on my own experience [[03:01:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10902.220000000001s)]
*  with the new 03 Mini model. [[03:01:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10908.62s)]
*  Now that I got a chance to spend many hours with it [[03:01:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10910.34s)]
*  in different kinds of contexts and applications. [[03:01:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10912.94s)]
*  So I would probably categorize this question [[03:01:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10915.54s)]
*  as let's say open-ended philosophical question. [[03:01:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10918.140000000001s)]
*  And in particular, the emphasis on novelty, [[03:02:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10921.86s)]
*  I think is a nice way to test one of the capabilities [[03:02:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10924.62s)]
*  of the model, which is come up with something [[03:02:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10928.980000000001s)]
*  that makes you pause and almost surprise you [[03:02:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10931.220000000001s)]
*  with its brilliance. [[03:02:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10934.980000000001s)]
*  So that said, my general review after running each [[03:02:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10936.62s)]
*  of the models on this question a bunch of times [[03:02:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10940.660000000002s)]
*  is that 01 Pro consistently gave brilliant answers. [[03:02:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10943.060000000001s)]
*  Once they gave me pause and made me think both cutting [[03:02:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10948.42s)]
*  in its insight and just really nicely phrased with wit, [[03:02:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10952.5s)]
*  with clarity, with nuance over and over consistently [[03:02:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10958.220000000001s)]
*  generating the best answers. [[03:02:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10961.34s)]
*  After that is R1, which is less consistent, [[03:02:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10963.060000000001s)]
*  but again, deliver brilliance. [[03:02:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10966.42s)]
*  Gemini Flash 2.0 Thinking was third. [[03:02:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10969.14s)]
*  And last was 03 Mini actually. [[03:02:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10972.18s)]
*  It often gave quite a generic answer, [[03:02:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10976.22s)]
*  at least to my particular sensibilities. [[03:02:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10979.18s)]
*  That said, in a bunch of other applications [[03:03:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10981.42s)]
*  that I tested for brainstorming purposes, [[03:03:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10984.06s)]
*  it actually worked extremely well [[03:03:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10987.58s)]
*  and often outperformed R1. [[03:03:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10989.86s)]
*  But on this open-ended philosophical question, [[03:03:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10993.4s)]
*  it did consistently worse. [[03:03:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10995.42s)]
*  Now, another important element for each of these models [[03:03:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=10997.460000000001s)]
*  is how the reasoning is presented. [[03:03:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11000.82s)]
*  DeepSeek R1 shows the full chain of thought tokens, [[03:03:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11002.62s)]
*  which I personally just love. [[03:03:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11006.42s)]
*  For these open-ended philosophical questions, [[03:03:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11009.1s)]
*  it's really, really interesting to see the model [[03:03:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11011.06s)]
*  think through it. [[03:03:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11013.14s)]
*  But really also just stepping back, [[03:03:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11014.3s)]
*  me as a person who appreciates intelligence [[03:03:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11017.14s)]
*  and reasoning and reflection, [[03:03:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11019.86s)]
*  reading these kind of chain of thought raw tokens of R1, [[03:03:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11022.26s)]
*  there's something genuinely beautiful [[03:03:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11025.779999999999s)]
*  about observing the path of deliberation [[03:03:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11029.019999999999s)]
*  in an intelligence system. [[03:03:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11031.859999999999s)]
*  I think we don't always have that explicitly laid out [[03:03:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11034.199999999999s)]
*  for us humans. [[03:03:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11038.3s)]
*  So to see it in another intelligence system, [[03:03:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11039.14s)]
*  the non-linearity of it, [[03:04:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11042.66s)]
*  akin to Ulysses of Finnegan's Wake by James Joyce, [[03:04:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11044.619999999999s)]
*  it's just beautiful to watch. [[03:04:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11048.22s)]
*  Anyway, as we discussed in the episode, [[03:04:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11049.78s)]
*  DeepSeek R1 talked about humans being able [[03:04:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11052.220000000001s)]
*  to convert selfish desires into cooperative systems [[03:04:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11055.18s)]
*  by collectively pretending abstract rules [[03:04:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11057.900000000001s)]
*  like money laws and rights are real. [[03:04:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11060.060000000001s)]
*  And these shared hallucinations act as games [[03:04:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11062.62s)]
*  where competition is secretly redirected [[03:04:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11066.52s)]
*  to benefit the group, [[03:04:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11068.54s)]
*  turning conflict into society's fuel. [[03:04:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11069.900000000001s)]
*  Gemini 2.0 Flash Thinking said, [[03:04:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11072.42s)]
*  "'Humans are not just social animals, [[03:04:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11075.02s)]
*  "'but self-domesticated apes. [[03:04:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11076.980000000001s)]
*  "'And this self-domestication is the key [[03:04:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11079.18s)]
*  "'to understanding our unique cognitive and social abilities.'" [[03:04:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11081.300000000001s)]
*  Now, it's important to say that the chain of thought there [[03:04:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11085.06s)]
*  was really interesting. [[03:04:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11087.78s)]
*  It was looking through the entire evolution [[03:04:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11089.140000000001s)]
*  of life on earth, [[03:04:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11091.74s)]
*  considering apex predators, [[03:04:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11093.06s)]
*  and considering how from that, [[03:04:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11096.26s)]
*  we ended up to where we are. [[03:04:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11098.66s)]
*  I think that domestication by choice [[03:05:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11100.84s)]
*  is a really interesting angle. [[03:05:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11103.66s)]
*  Again, it's one of those things [[03:05:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11105.220000000001s)]
*  when somebody presents a different angle [[03:05:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11106.58s)]
*  on a seemingly obvious thing, [[03:05:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11109.26s)]
*  it just makes me smile. [[03:05:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11110.66s)]
*  And the same with DeepSeek R1, [[03:05:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11111.98s)]
*  that these hallucinations of money laws and rights [[03:05:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11113.78s)]
*  and us collectively pretending like it's real, [[03:05:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11118.32s)]
*  and we play games with them that look like competition [[03:05:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11121.8s)]
*  when secretly we're just cooperating with each other. [[03:05:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11124.5s)]
*  And that is the fuel of progress, beautifully put. [[03:05:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11127.76s)]
*  Now, OpenAI 01 Pro consistently over and over [[03:05:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11131.039999999999s)]
*  delivered bangers. [[03:05:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11134.62s)]
*  I can go through many of them, [[03:05:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11136.300000000001s)]
*  but the first one was humans are the only species [[03:05:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11137.300000000001s)]
*  that turns raw materials into symbolic resources, [[03:05:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11139.78s)]
*  then uses those symbols to reorganize [[03:05:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11143.06s)]
*  the very materials they came from, [[03:05:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11145.74s)]
*  creating a closed feedback loop [[03:05:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11147.44s)]
*  between meaning and matter. [[03:05:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11149.460000000001s)]
*  Here, I just ran it again. [[03:05:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11152.7s)]
*  Banger after banger, I'm telling you. [[03:05:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11155.380000000001s)]
*  Humans are unique among known species [[03:05:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11157.22s)]
*  in that they simultaneously rewrite two layers of reality, [[03:05:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11159.34s)]
*  the external world, [[03:06:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11162.460000000001s)]
*  and their own private mental landscapes, [[03:06:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11163.740000000002s)]
*  and then merge these two rewritten layers [[03:06:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11166.66s)]
*  into a continuous personal narrative [[03:06:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11169.86s)]
*  that feels objectively true. [[03:06:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11172.380000000001s)]
*  Feels true. [[03:06:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11175.26s)]
*  This is poetry. [[03:06:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11177.76s)]
*  Okay, and then 03miniHi for me was smart, [[03:06:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11179.26s)]
*  fast, actually, and kind of generic. [[03:06:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11183.86s)]
*  Never quite got there for me. [[03:06:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11189.34s)]
*  So here's the first one I got from 03mini. [[03:06:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11191.12s)]
*  Humans are not fixed beings, [[03:06:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11194.04s)]
*  but rather ongoing narratives, [[03:06:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11196.2s)]
*  dynamic stories that we continuously write, [[03:06:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11198.320000000002s)]
*  edit, and reinterpret. [[03:06:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11200.84s)]
*  This narrative plasticity is more than just memory [[03:06:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11202.880000000001s)]
*  or self-reflection. [[03:06:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11205.960000000001s)]
*  It's an intrinsic cognitive process [[03:06:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11207.52s)]
*  that acts like an internal error correction system. [[03:06:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11209.320000000002s)]
*  It allows us to adapt our identities and values over time [[03:06:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11212.76s)]
*  in response to new experiences, challenges, [[03:06:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11215.880000000001s)]
*  and social contexts. [[03:06:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11218.36s)]
*  Now it almost sneaks up to something approximating, [[03:07:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11220.12s)]
*  cutting insight with narrative plasticity in quotes, [[03:07:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11222.68s)]
*  but then it goes back to the sort of the generic. [[03:07:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11227.160000000002s)]
*  I don't know. [[03:07:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11230.160000000002s)]
*  All of these models are incredible for different reasons. [[03:07:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11230.980000000001s)]
*  There's a lot of concerns as we discuss in this episode, [[03:07:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11233.6s)]
*  but there's a lot of reasons to be excited as well. [[03:07:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11236.42s)]
*  And I've probably spoken for too long. [[03:07:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11241.68s)]
*  I am severely sleep deprived, borderline delirious. [[03:07:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11244.7s)]
*  So hopefully some of this made sense. [[03:07:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11249.1s)]
*  And now dear friends, back to the episode. [[03:07:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11251.7s)]
*  I think when you, to Nathan's point, [[03:07:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11256.82s)]
*  when you look at the reasoning models, [[03:07:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11259.98s)]
*  to me, even when I used R1 versus 01, [[03:07:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11263.42s)]
*  there was that sort of rough edges [[03:07:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11267.02s)]
*  around the corner feeling, right? [[03:07:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11269.26s)]
*  And flash thinking earlier, [[03:07:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11271.54s)]
*  I didn't use this version, but the one from December, [[03:07:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11273.66s)]
*  and it definitely had that rough edges [[03:07:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11275.78s)]
*  around the corner feeling, right? [[03:07:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11277.26s)]
*  Where it's just not fleshed out in as many ways, right? [[03:07:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11279.699999999999s)]
*  Sure, they added math and coding capabilities [[03:08:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11282.58s)]
*  via these verifiers in RL, [[03:08:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11284.82s)]
*  but it feels like they lost something in certain areas. [[03:08:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11286.82s)]
*  And 01 is worst performing than chat [[03:08:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11290.26s)]
*  in many areas as well, to be clear. [[03:08:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11292.66s)]
*  Not by a lot. [[03:08:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11295.18s)]
*  Not by a lot though, right? [[03:08:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11296.02s)]
*  And it's like R1 definitely felt to me [[03:08:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11296.94s)]
*  like it was worse than V3 in certain areas, [[03:08:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11299.859999999999s)]
*  like doing this RL expressed and learned a lot, [[03:08:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11302.02s)]
*  but then it weakened in other areas. [[03:08:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11305.18s)]
*  And so I think that's one of the big differences [[03:08:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11307.34s)]
*  between these models and what 01 offers. [[03:08:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11309.94s)]
*  And then OpenAI has 01 Pro. [[03:08:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11313.82s)]
*  And what they did with 03, which is also very unique, [[03:08:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11316.18s)]
*  is that they stacked search on top of Chain of Thought, right? [[03:08:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11319.06s)]
*  And so Chain of Thought is one thing where it's one chain, [[03:08:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11323.5s)]
*  it backtracks, goes back and forth, [[03:08:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11326.32s)]
*  but how they solved the Arc AGI challenge [[03:08:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11328.18s)]
*  was not just the Chain of Thought. [[03:08:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11330.78s)]
*  It was also sampling many times, [[03:08:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11333.18s)]
*  i.e. running them in parallel, [[03:08:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11335.14s)]
*  and then selecting. [[03:08:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11338.02s)]
*  Is running in parallel actually search? [[03:08:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11338.86s)]
*  Because I don't know if we have the full information [[03:09:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11340.34s)]
*  on how 01 Pro works. [[03:09:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11342.220000000001s)]
*  So like I don't have enough information [[03:09:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11343.34s)]
*  to confidently say that it is search. [[03:09:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11345.380000000001s)]
*  It is parallel samples. [[03:09:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11347.14s)]
*  Yeah. [[03:09:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11348.7s)]
*  And then what? [[03:09:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11349.54s)]
*  And it selects something. [[03:09:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11350.380000000001s)]
*  And we don't know what the selection function is. [[03:09:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11351.220000000001s)]
*  The reason why we're debating is because [[03:09:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11352.26s)]
*  since 01 was announced, [[03:09:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11354.82s)]
*  there's been a lot of interest in techniques [[03:09:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11356.26s)]
*  called Monte Carlo research, [[03:09:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11357.800000000001s)]
*  which is where you will break down the Chain of Thought [[03:09:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11359.06s)]
*  into intermediate steps. [[03:09:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11361.64s)]
*  We haven't defined Chain of Thought. [[03:09:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11362.86s)]
*  Chain of Thought is from a paper from years ago [[03:09:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11364.66s)]
*  where you introduce the idea to ask a language model [[03:09:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11367.42s)]
*  that at the time was much less easy to use. [[03:09:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11370.06s)]
*  You would say, let's verify step by step, [[03:09:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11372.82s)]
*  and it would induce the model [[03:09:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11374.98s)]
*  to do this bulleted list of steps. [[03:09:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11376.14s)]
*  Chain of Thought is now almost a default in models [[03:09:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11378.28s)]
*  where if you ask it a math question, [[03:09:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11381.119999999999s)]
*  you don't need to tell it to think step by step. [[03:09:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11382.36s)]
*  And the idea with Monte Carlo tree search [[03:09:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11384.58s)]
*  is that you would take an intermediate point in that train, [[03:09:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11387.02s)]
*  do some sort of expansion, spend more compute, [[03:09:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11390.06s)]
*  and then select the right one. [[03:09:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11392.42s)]
*  That's a very complex form of search [[03:09:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11393.82s)]
*  that has been used in things like mu zero [[03:09:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11395.539999999999s)]
*  and alpha zero potentially. [[03:09:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11397.779999999999s)]
*  I know mu zero does this. [[03:09:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11399.22s)]
*  Another form of search is just asking five different people [[03:10:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11400.98s)]
*  and then taking the majority answer. [[03:10:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11403.94s)]
*  Yes. [[03:10:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11405.22s)]
*  There's a variety of like, [[03:10:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11406.06s)]
*  it could be complicated, it could be simple. [[03:10:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11407.9s)]
*  We don't know what it is, just that they are, [[03:10:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11409.98s)]
*  they are not just issuing one Chain of Thought in sequence. [[03:10:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11411.86s)]
*  They're launching many in parallel. [[03:10:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11415.22s)]
*  And in the Arc AGI, they launched a thousand in parallel [[03:10:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11417.3s)]
*  for the one that really shocked everyone [[03:10:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11420.5s)]
*  that beat the benchmark was they would launch a thousand [[03:10:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11423.06s)]
*  in parallel and then they would get the right answer [[03:10:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11425.58s)]
*  like 80% of the time or 70% of the time, 90 maybe even. [[03:10:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11427.58s)]
*  Whereas if they just launched one, it was like 30%. [[03:10:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11430.74s)]
*  There are many extensions to this. [[03:10:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11433.699999999999s)]
*  I would say the simplest one is that our language models [[03:10:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11435.539999999999s)]
*  to date have been designed to give the right answer [[03:10:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11438.66s)]
*  the highest percentage of the time in one response. [[03:10:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11441.619999999999s)]
*  And we are now opening the door to different ways [[03:10:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11444.859999999999s)]
*  of running inference on our models [[03:10:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11447.6s)]
*  in which we need to reevaluate [[03:10:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11449.5s)]
*  many parts of the training process, [[03:10:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11451.1s)]
*  which normally opens the door to more progress, [[03:10:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11453.460000000001s)]
*  but we don't know if OpenAI changed a lot [[03:10:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11456.140000000001s)]
*  or if just sampling more and multiple choice [[03:10:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11458.42s)]
*  is what they're doing or if it's something more complex [[03:11:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11460.78s)]
*  where they changed the training and they know [[03:11:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11462.94s)]
*  that the inference mode is going to be different. [[03:11:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11464.86s)]
*  So we're talking about O1 Pro, $200 a month [[03:11:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11467.7s)]
*  and they're losing money. [[03:11:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11471.1s)]
*  So the thing that we're referring to, [[03:11:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11473.02s)]
*  this fascinating exploration of the test time compute [[03:11:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11476.66s)]
*  and the space, is that actually possible? [[03:11:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11481.78s)]
*  Do we have enough compute for that? [[03:11:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11484.300000000001s)]
*  Does the financials make sense? [[03:11:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11486.140000000001s)]
*  So the fantastic thing is, [[03:11:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11487.74s)]
*  and it's in the thing that I pulled up earlier, [[03:11:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11489.74s)]
*  but the cost for GPT-3 has plummeted [[03:11:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11492.980000000001s)]
*  if you scroll up just a few images, I think. [[03:11:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11497.7s)]
*  The important thing about like, hey, [[03:11:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11500.7s)]
*  is cost a limiting factor here, right? [[03:11:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11501.74s)]
*  Like my view is that like we'll have like really awesome [[03:11:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11504.380000000001s)]
*  intelligence before we have like AGI [[03:11:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11507.7s)]
*  before we have it permeate throughout the economy. [[03:11:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11509.94s)]
*  And this is sort of why that reason is, right? [[03:11:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11512.380000000001s)]
*  GPT-3 was trained in what, 2020, 2021? [[03:11:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11514.460000000001s)]
*  And the cost for running inference on it [[03:11:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11517.980000000001s)]
*  was $60, $70 per million tokens, right? [[03:12:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11520.54s)]
*  Which is the cost per intelligence was ridiculous. [[03:12:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11524.5s)]
*  Now, as we scaled forward two years, [[03:12:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11527.18s)]
*  we've had a 1200X reduction in cost [[03:12:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11528.980000000001s)]
*  to achieve the same level of intelligence as GPT-3. [[03:12:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11532.26s)]
*  So here on the X axis is time over just a couple of years [[03:12:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11535.1s)]
*  and on the Y axis is log scale dollars [[03:12:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11540.58s)]
*  to run inference on a million tokens. [[03:12:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11544.699999999999s)]
*  And so you have just a down, [[03:12:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11548.34s)]
*  like a linear decline on log scale [[03:12:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11550.98s)]
*  from GPT-3 through 3.5 to Lama. [[03:12:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11554.82s)]
*  It's like five cents or something like that now, right? [[03:12:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11557.539999999999s)]
*  Which is versus $60, 1200X. [[03:12:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11559.5s)]
*  That's not the exact numbers, but it's 1200X. [[03:12:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11563.1s)]
*  I remember that number. [[03:12:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11565.06s)]
*  Is the humongous cost per intelligence, right? [[03:12:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11566.14s)]
*  Now, the freak out over DeepSeek is, [[03:12:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11570.18s)]
*  oh my God, they made it so cheap. [[03:12:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11572.06s)]
*  It's like, actually, if you look at this trend line, [[03:12:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11573.699999999999s)]
*  they're not below the trend line, first of all, [[03:12:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11575.939999999999s)]
*  and at least for GPT-3, right? [[03:12:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11577.82s)]
*  They are the first to hit it, right? [[03:12:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11579.859999999999s)]
*  Which is a big deal. [[03:13:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11581.019999999999s)]
*  But they're not below the trend line as far as GPT-3. [[03:13:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11582.58s)]
*  Now we have GPT-4. [[03:13:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11585.06s)]
*  What's gonna happen with these reasoning capabilities, right? [[03:13:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11586.06s)]
*  It's a mix of architectural innovations. [[03:13:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11588.38s)]
*  It's a mix of better data [[03:13:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11590.859999999999s)]
*  and it's gonna be better training techniques [[03:13:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11592.38s)]
*  and all of these different better inference systems, [[03:13:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11593.94s)]
*  better hardware, right? [[03:13:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11596.380000000001s)]
*  Going from each generation of GPU [[03:13:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11597.54s)]
*  to new generations or ASICs. [[03:13:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11600.26s)]
*  Everything is gonna take this cost curve [[03:13:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11602.42s)]
*  down and down and down and down. [[03:13:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11604.78s)]
*  And then can I just spawn a thousand different LLMs [[03:13:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11606.300000000001s)]
*  to create a task and then pick from one of them [[03:13:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11610.82s)]
*  or whatever search technique I want, [[03:13:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11613.18s)]
*  a tree, Monte Carlo tree search. [[03:13:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11615.380000000001s)]
*  Maybe it gets that complicated. [[03:13:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11617.02s)]
*  Maybe it doesn't because it's too complicated [[03:13:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11619.140000000001s)]
*  to actually scale, like who knows? [[03:13:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11620.5s)]
*  Bitter lesson, right? [[03:13:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11622.300000000001s)]
*  The question is, I think, when not if, [[03:13:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11623.86s)]
*  because the rate of progress is so fast, right? [[03:13:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11628.02s)]
*  Nine months ago, Dario was saying, [[03:13:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11631.7s)]
*  Dario said nine months ago, [[03:13:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11634.220000000001s)]
*  the cost to train and inference was this, right? [[03:13:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11635.26s)]
*  And now we're much better than this, right? [[03:13:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11637.7s)]
*  And DeepSeq is much better than this. [[03:13:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11639.900000000001s)]
*  And that cost curve for GPT-4, [[03:14:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11641.660000000002s)]
*  which was also roughly $60 per million tokens [[03:14:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11643.820000000002s)]
*  when it launched, has already fallen to $2 or so, right? [[03:14:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11646.900000000001s)]
*  And we're gonna get it down to cents, probably, [[03:14:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11651.38s)]
*  for GPT-4 quality. [[03:14:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11653.859999999999s)]
*  And then that's the base for the reasoning models [[03:14:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11655.14s)]
*  like O1 that we have today. [[03:14:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11659.14s)]
*  And O1 Pro is spawning multiple, right? [[03:14:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11660.58s)]
*  And O3 and so on and so forth. [[03:14:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11663.14s)]
*  These search techniques, too expensive today, [[03:14:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11664.66s)]
*  but they will get cheaper. [[03:14:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11666.699999999999s)]
*  And that's what's gonna unlock the intelligence, right? [[03:14:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11667.98s)]
*  So get cheaper and cheaper and cheaper. [[03:14:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11671.179999999998s)]
*  The big DeepSeq R1 release freaked everybody out [[03:14:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11673.859999999999s)]
*  because of the cheaper. [[03:14:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11678.78s)]
*  One of the manifestations of that [[03:14:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11679.900000000001s)]
*  is Nvidia stock plummeted. [[03:14:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11681.900000000001s)]
*  Can you explain what happened? [[03:14:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11684.140000000001s)]
*  I mean, and also just explain this moment [[03:14:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11686.140000000001s)]
*  and whether, you know, if Nvidia is gonna keep winning. [[03:14:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11688.94s)]
*  We're both Nvidia bulls here, I would say. [[03:14:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11692.66s)]
*  And in some ways, the market response is reasonable. [[03:14:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11695.26s)]
*  Most of the market, like, [[03:14:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11699.300000000001s)]
*  Nvidia's biggest customers in the US [[03:15:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11701.140000000001s)]
*  are major tech companies, [[03:15:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11703.78s)]
*  and they're spending a ton on AI. [[03:15:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11705.060000000001s)]
*  And a simple interpretation of DeepSeq [[03:15:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11706.78s)]
*  is you can get really good models [[03:15:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11709.78s)]
*  without spending as much on AI. [[03:15:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11711.300000000001s)]
*  So in that capacity, it's like, [[03:15:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11713.460000000001s)]
*  oh, maybe these big tech companies [[03:15:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11715.1s)]
*  won't need to spend as much on AI and go down. [[03:15:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11716.34s)]
*  The actual thing that happened is much more complex [[03:15:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11718.54s)]
*  where there's social factors, [[03:15:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11720.66s)]
*  where there's the rising in the app store, [[03:15:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11721.900000000001s)]
*  the social contagion that is happening. [[03:15:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11723.7s)]
*  And then I think some of it is just like, [[03:15:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11726.26s)]
*  I don't trade, I don't know anything [[03:15:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11728.94s)]
*  about financial markets. [[03:15:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11730.140000000001s)]
*  But it builds up over the weekend [[03:15:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11730.980000000001s)]
*  where the social pressure, [[03:15:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11732.18s)]
*  where it's like, if it was during the week [[03:15:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11733.58s)]
*  and there was multiple days of trading, [[03:15:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11734.94s)]
*  when this was really becoming, [[03:15:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11736.7s)]
*  but it comes on the weekend [[03:15:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11737.94s)]
*  and then everybody wants to sell. [[03:15:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11738.980000000001s)]
*  And that is a social contagion. [[03:15:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11741.1s)]
*  I think, and like, there were a lot of false narratives, [[03:15:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11743.220000000001s)]
*  which is like, hey, these guys [[03:15:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11745.900000000001s)]
*  are spending billions on models, right? [[03:15:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11747.300000000001s)]
*  And they're not spending billions on models. [[03:15:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11749.060000000001s)]
*  No one's spent more than a billion dollars [[03:15:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11750.460000000001s)]
*  on a model that's released publicly, right? [[03:15:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11752.94s)]
*  GPT-4 was a couple hundred million, [[03:15:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11754.820000000002s)]
*  and then they've reduced the cost with 4.0, [[03:15:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11757.54s)]
*  4 Turbo 4.0, right? [[03:16:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11760.54s)]
*  But billion dollar model runs are coming, right? [[03:16:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11762.5s)]
*  And this concludes pre-training and post-training, right? [[03:16:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11765.3s)]
*  And then the other number is like, [[03:16:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11766.98s)]
*  hey, DeepSeek didn't include everything, right? [[03:16:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11768.06s)]
*  They didn't include, you know, [[03:16:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11769.66s)]
*  a lot of the cost goes to research [[03:16:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11770.58s)]
*  and all this sort of stuff. [[03:16:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11771.859999999999s)]
*  A lot of the cost goes to inference. [[03:16:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11772.859999999999s)]
*  A lot of the cost goes to post-training. [[03:16:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11774.34s)]
*  None of these things were factored. [[03:16:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11775.58s)]
*  It's research salaries, right? [[03:16:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11776.779999999999s)]
*  Like all these things are like counted [[03:16:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11777.82s)]
*  in the billions of dollars that OpenAI is spending, [[03:16:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11779.66s)]
*  but they weren't counted in the, you know, [[03:16:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11781.98s)]
*  hey, six million, five million dollars [[03:16:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11783.58s)]
*  that DeepSeek spent, right? [[03:16:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11785.34s)]
*  So there's a bit of misunderstanding [[03:16:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11786.46s)]
*  of what these numbers are. [[03:16:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11788.34s)]
*  And then there's also an element of, [[03:16:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11789.779999999999s)]
*  Nvidia has just been a straight lineup, right? [[03:16:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11792.939999999999s)]
*  And there's been so many different narratives [[03:16:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11794.62s)]
*  that have been trying to push down Nvidia. [[03:16:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11797.7s)]
*  I don't say push down Nvidia stock. [[03:16:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11799.62s)]
*  Everyone is looking for a reason to sell [[03:16:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11801.18s)]
*  or to be worried, right? [[03:16:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11803.140000000001s)]
*  You know, it was Blackwell delays, right? [[03:16:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11804.7s)]
*  Their GPU, you know, there's a lot of report. [[03:16:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11807.26s)]
*  Every two weeks, there's a new report [[03:16:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11808.94s)]
*  about their GPUs being delayed. [[03:16:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11810.140000000001s)]
*  There's the whole thing about scaling laws ending, right? [[03:16:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11812.140000000001s)]
*  It's so ironic, right? [[03:16:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11816.42s)]
*  It lasted a month. [[03:16:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11817.94s)]
*  It was just, like literally just, [[03:16:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11818.78s)]
*  hey, models aren't getting better, right? [[03:17:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11822.18s)]
*  They're just not getting better. [[03:17:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11823.94s)]
*  There's no reason to spend more. [[03:17:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11824.980000000001s)]
*  Proof training scaling is dead. [[03:17:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11826.18s)]
*  And then it's like, oh one, oh three, right? [[03:17:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11827.9s)]
*  R1. R1, right? [[03:17:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11830.460000000001s)]
*  And now it's like, wait, models are getting too, [[03:17:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11831.82s)]
*  they're progressing too fast. [[03:17:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11834.02s)]
*  Slow down the progress, stop spending on GPUs, right? [[03:17:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11835.380000000001s)]
*  But you know, the funniest thing I think [[03:17:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11838.54s)]
*  that like comes out of this is, [[03:17:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11839.94s)]
*  Javan's paradox is true, right? [[03:17:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11841.58s)]
*  AWS pricing for H100s has gone up [[03:17:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11844.34s)]
*  over the last couple of weeks, right? [[03:17:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11846.78s)]
*  Since, since, since, since a little bit after Christmas, [[03:17:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11848.300000000001s)]
*  since V3 was launched, [[03:17:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11851.1s)]
*  AWS H100 pricing has gone up. [[03:17:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11852.62s)]
*  H200s are like almost out of stock everywhere [[03:17:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11854.38s)]
*  because it, you know, H200 has more memory [[03:17:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11856.82s)]
*  and therefore R1 like, you know, [[03:17:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11859.5s)]
*  wants that chip over H100, right? [[03:17:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11861.14s)]
*  We were trying to get GPUs on a short notice this week [[03:17:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11863.06s)]
*  for a demo and it wasn't that easy. [[03:17:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11865.3s)]
*  We were trying to get just like 16 or 32 H100s for demo [[03:17:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11866.62s)]
*  and it was not very easy. [[03:17:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11869.539999999999s)]
*  So for people who don't know, [[03:17:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11871.18s)]
*  Javan's paradox is when, you know, [[03:17:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11872.14s)]
*  the efficiency goes up somehow magically, [[03:17:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11876.3s)]
*  counter-intuitively, the total resource consumption [[03:17:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11879.82s)]
*  goes up as well. [[03:18:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11882.18s)]
*  Right, and semiconductors is, you know, [[03:18:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11883.1s)]
*  we're at 50 years of Moore's law, [[03:18:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11884.9s)]
*  every two years half the cost, double the transistors, [[03:18:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11887.34s)]
*  just like clockwork and it's slowed down obviously, [[03:18:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11889.9s)]
*  but like the semiconductor industry [[03:18:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11891.62s)]
*  has gone up the whole time, right? [[03:18:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11893.66s)]
*  It's been wavy, right? [[03:18:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11895.380000000001s)]
*  There's obviously cycles and stuff [[03:18:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11896.220000000001s)]
*  and I don't expect AI to be any different, right? [[03:18:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11897.58s)]
*  There's gonna be ebbs and flows, but this is, [[03:18:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11899.54s)]
*  in AI it's just playing out at an insane time scale, right? [[03:18:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11902.18s)]
*  It was two X every two years. [[03:18:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11905.02s)]
*  This is 1200 X in like three years, right? [[03:18:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11907.06s)]
*  So it's like the scale of improvement [[03:18:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11909.9s)]
*  that is like hard to wrap your head around. [[03:18:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11912.22s)]
*  Yeah, I was confused because to me, [[03:18:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11914.619999999999s)]
*  NVIDIA's thought on that should have gone up, [[03:18:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11916.58s)]
*  but maybe it went down because there's kind of suspicion [[03:18:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11919.699999999999s)]
*  of foul play on the side of China or something like this, [[03:18:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11923.22s)]
*  but if you just look purely [[03:18:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11925.9s)]
*  at the actual principles that play here, [[03:18:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11927.58s)]
*  like it's obvious, yeah, Javan's paradox. [[03:18:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11929.38s)]
*  The more progress that AI makes [[03:18:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11932.34s)]
*  or the higher the derivative of AI progress is especially, [[03:18:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11933.94s)]
*  because NVIDIA is in the best place. [[03:18:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11938.939999999999s)]
*  The higher the derivative is, [[03:19:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11940.14s)]
*  the sooner the market's gonna be bigger and expanding [[03:19:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11941.38s)]
*  and NVIDIA is the only one [[03:19:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11943.939999999999s)]
*  that does everything reliably right now. [[03:19:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11945.06s)]
*  Because it's not like an NVIDIA competitor arose. [[03:19:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11947.539999999999s)]
*  It's another company that's using NVIDIA. [[03:19:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11951.26s)]
*  Who historically has been a large NVIDIA customer. [[03:19:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11954.539999999999s)]
*  Yeah. [[03:19:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11957.3s)]
*  And has press releases about them cheering [[03:19:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11958.14s)]
*  about being China's biggest NVIDIA customer, right? [[03:19:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11960.34s)]
*  Yeah, I mean. [[03:19:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11963.9s)]
*  Obviously they've quieted down, [[03:19:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11965.06s)]
*  I think that's another element of it [[03:19:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11966.86s)]
*  is that they don't wanna say how many GPUs they have [[03:19:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11968.3s)]
*  because hey, yes, they have H800s, yes, they have H20s. [[03:19:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11970.86s)]
*  They also have some H100s, right? [[03:19:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11975.14s)]
*  Which are smuggled in. [[03:19:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11976.9s)]
*  Can you speak to that, to the smuggling? [[03:19:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11977.74s)]
*  What's the scale of smuggling that's feasible [[03:19:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11979.98s)]
*  for a nation state to do for companies? [[03:19:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11982.86s)]
*  Is it possible to? [[03:19:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11985.18s)]
*  I think there's a few angles of smuggling here, right? [[03:19:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11986.66s)]
*  One is ByteDance arguably is the largest smuggler [[03:19:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11989.86s)]
*  of GPUs for China, right? [[03:19:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11992.98s)]
*  China's not supposed to have GPUs. [[03:19:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11994.34s)]
*  ByteDance has like over 500,000 GPUs. [[03:19:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11995.86s)]
*  Why? [[03:19:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11998.220000000001s)]
*  Because they're all rented from companies around the world. [[03:19:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=11999.060000000001s)]
*  They rent from Oracle, they rent from Google, [[03:20:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12001.380000000001s)]
*  they rent from all these mass [[03:20:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12003.460000000001s)]
*  and a bunch of smaller cloud companies too, right? [[03:20:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12004.78s)]
*  All the Neo clouds, right? [[03:20:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12006.74s)]
*  Of the world. [[03:20:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12007.86s)]
*  They rent so, so many GPUs. [[03:20:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12008.7s)]
*  They also buy a bunch, right? [[03:20:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12010.220000000001s)]
*  And they do this for mostly like what Meta does, right? [[03:20:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12011.86s)]
*  Serving TikTok, right? [[03:20:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12014.74s)]
*  Serving, next best. [[03:20:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12016.1s)]
*  Separate discussion. [[03:20:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12017.74s)]
*  Same as Meta, right? [[03:20:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12018.58s)]
*  To be clear, that's today the view, use, right? [[03:20:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12019.42s)]
*  And it's a valid use, right? [[03:20:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12021.300000000001s)]
*  Hack the dopamine certificate, right? [[03:20:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12022.86s)]
*  Now, that's theoretically now very much restricted [[03:20:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12025.26s)]
*  with the AI diffusion rules, [[03:20:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12029.26s)]
*  which happened in the last week of the Biden admin [[03:20:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12030.5s)]
*  and Trump admin looks like they're gonna keep them, [[03:20:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12032.54s)]
*  which limits like allies even like Singapore, [[03:20:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12035.18s)]
*  which Singapore is like 20% of Nvidia's, [[03:20:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12038.98s)]
*  20, 30% of Nvidia's revenue. [[03:20:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12040.94s)]
*  But Singapore's had a memoratorium [[03:20:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12042.94s)]
*  on not building data centers for like 15 years [[03:20:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12045.22s)]
*  because they don't have enough power. [[03:20:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12046.98s)]
*  So where are they going? [[03:20:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12047.9s)]
*  Oh my God. [[03:20:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12049.7s)]
*  I mean, I'm not claiming they're all going to China, right? [[03:20:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12050.54s)]
*  But a portion are, you know, many are going to Malaysia. [[03:20:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12052.9s)]
*  Including Microsoft and Oracle [[03:20:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12056.14s)]
*  have big data centers in Malaysia. [[03:20:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12056.98s)]
*  Like, you know, they're going all over Southeast Asia, [[03:20:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12058.06s)]
*  probably India as well, right? [[03:21:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12060.22s)]
*  Like there's stuff routing, [[03:21:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12061.779999999999s)]
*  but like the diffusion rules are very de facto. [[03:21:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12062.9s)]
*  Like you can only buy this many GPUs from this country. [[03:21:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12065.58s)]
*  And it's, and you can only rent a cluster this large [[03:21:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12068.06s)]
*  to companies that are Chinese, right? [[03:21:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12071.099999999999s)]
*  Like they're very explicit on trying to stop smuggling, right? [[03:21:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12072.3s)]
*  And a big chunk of it was, hey, let's, you know, [[03:21:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12075.9s)]
*  random company by 16 servers, [[03:21:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12078.5s)]
*  ship them to China, right? [[03:21:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12080.5s)]
*  There's actually, I saw a photo from someone [[03:21:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12083.42s)]
*  in the semiconductor industry [[03:21:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12086.78s)]
*  who leads like a team for like networking chips [[03:21:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12088.66s)]
*  that competes with Nvidia. [[03:21:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12092.58s)]
*  And he sent a photo of a guy [[03:21:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12093.78s)]
*  checking into a first-class United flight [[03:21:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12095.5s)]
*  from San Francisco to Shanghai or Shenzhen [[03:21:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12097.82s)]
*  with a super micro box that was this big, [[03:21:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12100.98s)]
*  which can only contain GPUs, right? [[03:21:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12103.9s)]
*  And he was booking first-class because think about it, [[03:21:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12106.7s)]
*  three to five K for your first-class ticket, [[03:21:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12108.94s)]
*  server costs, you know, 240,000 in the US to 50,000. [[03:21:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12110.82s)]
*  You sell it for 300,000 in China. [[03:21:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12114.38s)]
*  Wait, you just got a free first-class ticket [[03:21:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12116.46s)]
*  and a lot more money. [[03:21:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12118.779999999999s)]
*  So it's like, you know, [[03:22:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12120.14s)]
*  and that's like small scale smuggling. [[03:22:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12120.98s)]
*  Most of the large scale smuggling is like companies [[03:22:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12122.539999999999s)]
*  in Singapore and Malaysia, like routing them around [[03:22:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12125.14s)]
*  or renting GPUs completely legally. [[03:22:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12127.82s)]
*  I wanna jump in, how much is the scale? [[03:22:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12130.02s)]
*  I think there's been some number, [[03:22:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12131.539999999999s)]
*  like some people that are higher level economics [[03:22:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12132.58s)]
*  understanding say that as you go from 1 billion [[03:22:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12136.02s)]
*  of smuggling to 10 billion, [[03:22:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12138.58s)]
*  it's like you're hiding certain levels of economic activity. [[03:22:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12139.82s)]
*  And that's the most reasonable thing to me [[03:22:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12142.74s)]
*  is that there's gonna be some level where it's so obvious [[03:22:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12144.26s)]
*  that it's easier to find this economic activity. [[03:22:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12147.14s)]
*  Yeah, so my belief is that last year roughly, [[03:22:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12149.98s)]
*  so NVIDIA made a million H20s, [[03:22:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12155.3s)]
*  which are legally allowed to be shipped to China, [[03:22:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12157.14s)]
*  which we talked about is better for reasoning, right? [[03:22:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12159.06s)]
*  Inference at least, maybe not training, [[03:22:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12161.38s)]
*  but reasoning inference and inference generally. [[03:22:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12163.699999999999s)]
*  Then they also had, you know, a couple hundred thousand, [[03:22:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12166.34s)]
*  we think like 200 to 300,000 GPUs were routed to China [[03:22:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12169.5s)]
*  from Singapore, Malaysia, US, wherever. [[03:22:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12173.7s)]
*  Companies spawn up by 16 GPUs, 64 GPUs, [[03:22:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12176.22s)]
*  whatever it is routed. [[03:22:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12178.98s)]
*  And Huawei is known for having spent up a massive network [[03:22:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12179.98s)]
*  of companies to get the materials they need [[03:23:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12182.58s)]
*  after they were banned in like 2018. [[03:23:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12184.78s)]
*  So it's not like other worldly, but I agree, right? [[03:23:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12186.38s)]
*  Nathan's point is like, hey, [[03:23:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12188.74s)]
*  you can't smuggle up $10 billion of GPUs. [[03:23:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12190.66s)]
*  And then the third sort of source, [[03:23:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12192.94s)]
*  which is just now banned and you know, [[03:23:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12194.18s)]
*  which wasn't considered smuggling, [[03:23:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12195.78s)]
*  but is China is renting like, [[03:23:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12197.18s)]
*  I believe from our research, right? [[03:23:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12199.78s)]
*  Oracle's biggest GPU customer is ByteDance, right? [[03:23:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12202.66s)]
*  And for Google, I think it's their second biggest customer. [[03:23:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12206.74s)]
*  Right? [[03:23:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12209.66s)]
*  And so like, and you go down the list of clouds [[03:23:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12210.5s)]
*  and especially these smaller cloud companies [[03:23:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12211.7s)]
*  that aren't like the hyperscalers, right? [[03:23:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12213.18s)]
*  Think beyond core, even Lambda, even there's a whole C, [[03:23:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12215.9s)]
*  there's 60 different new cloud companies [[03:23:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12218.98s)]
*  serving in video GPUs. [[03:23:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12220.58s)]
*  I think ByteDance is renting a lot of these, right? [[03:23:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12221.74s)]
*  All over it, right? [[03:23:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12224.1s)]
*  So these companies are renting GPUs to Chinese companies. [[03:23:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12224.94s)]
*  And that's completely, that was completely legal [[03:23:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12229.140000000001s)]
*  up until the diffusion rules, [[03:23:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12231.460000000001s)]
*  which happened just a few weeks ago. [[03:23:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12233.1s)]
*  And even now you can rent GPU clusters [[03:23:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12234.54s)]
*  that are less than 2000 GPUs, [[03:23:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12236.980000000001s)]
*  or you can buy GPUs and ship them wherever you want. [[03:23:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12238.54s)]
*  If they're less than 1500 GPUs, right? [[03:24:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12241.34s)]
*  So it's like, there are still like some ways to smuggle, [[03:24:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12243.380000000001s)]
*  but yeah, it's not, you know, as the numbers grow, right? [[03:24:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12246.5s)]
*  You know, a hundred something billion dollars [[03:24:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12249.62s)]
*  of revenue for Nvidia last year, [[03:24:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12251.300000000001s)]
*  200 something billion this year, right? [[03:24:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12252.42s)]
*  And if next year, you know, [[03:24:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12254.46s)]
*  it could nearly double again or more than double, right? [[03:24:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12256.019999999999s)]
*  Based on like what we see with data center footprints [[03:24:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12259.619999999999s)]
*  like being built out all across the US [[03:24:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12261.859999999999s)]
*  and the rest of the world, [[03:24:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12263.419999999998s)]
*  it's gonna be really hard for China to keep up [[03:24:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12264.619999999999s)]
*  with these rules, right? [[03:24:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12266.66s)]
*  Yes, there will always be smuggling [[03:24:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12268.179999999998s)]
*  and deep seek level models of GPT-4 level models, [[03:24:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12270.58s)]
*  O1 level models capable to train on what China can get, [[03:24:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12273.699999999999s)]
*  even the next year above that. [[03:24:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12276.66s)]
*  But if we speed run a couple more, you know, jumps, right? [[03:24:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12278.06s)]
*  To billion dollar models, 10 billion dollar models, [[03:24:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12282.86s)]
*  then it becomes, you know, hey, [[03:24:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12284.980000000001s)]
*  there is a compute disadvantage for China [[03:24:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12286.7s)]
*  for training models and serving them. [[03:24:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12288.220000000001s)]
*  And the serving part is really critical, right? [[03:24:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12290.060000000001s)]
*  Deep seek cannot serve their model today, right? [[03:24:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12292.34s)]
*  It's completely out of inventory. [[03:24:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12294.18s)]
*  It's already started falling in the app store, [[03:24:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12296.94s)]
*  actually downloads because you download it, [[03:24:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12298.78s)]
*  you try and sign up, they say, [[03:25:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12300.94s)]
*  we're not taking registrations [[03:25:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12301.78s)]
*  because they have no capacity, right? [[03:25:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12302.740000000002s)]
*  You open it up, you get like less than five tokens per second [[03:25:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12303.94s)]
*  if you even get your request approved, right? [[03:25:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12306.58s)]
*  Cause there's just no capacity [[03:25:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12308.66s)]
*  because they just don't have enough GPUs [[03:25:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12310.34s)]
*  to serve the model, [[03:25:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12311.7s)]
*  even though it's incredibly efficient. [[03:25:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12312.62s)]
*  It'd be fascinating to watch the smuggling, [[03:25:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12314.18s)]
*  cause I mean, there's drug smuggling, right? [[03:25:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12316.34s)]
*  That's a market, there's weapons smuggling [[03:25:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12319.18s)]
*  and GPUs will surpass that at some point. [[03:25:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12323.140000000001s)]
*  Chips are highest value per kilogram, probably by far. [[03:25:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12325.94s)]
*  I have another question for you Dylan, [[03:25:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12332.1s)]
*  do you track model API access internationally? [[03:25:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12333.34s)]
*  How easy is it for Chinese companies [[03:25:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12336.66s)]
*  to use hosted model APIs from the US? [[03:25:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12339.02s)]
*  Yeah, I mean, that's incredibly easy, right? [[03:25:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12342.54s)]
*  Like OpenAI publicly stated DeepSeq uses their API [[03:25:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12343.740000000002s)]
*  and as they say, they have evidence, right? [[03:25:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12346.86s)]
*  And this is another element of the training regime [[03:25:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12348.58s)]
*  is people at OpenAI have claimed that it's a distilled model, [[03:25:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12350.820000000002s)]
*  i.e. you're taking OpenAI's model, [[03:25:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12353.900000000001s)]
*  you're generating a lot of output [[03:25:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12356.380000000001s)]
*  and then you're training on the output in their model. [[03:25:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12357.62s)]
*  And even if that's the case, [[03:26:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12360.34s)]
*  what they did is still amazing by the way, [[03:26:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12361.54s)]
*  what DeepSeq did efficiency wise. [[03:26:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12363.02s)]
*  Distillation is standard practice in industry, [[03:26:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12364.34s)]
*  whether or not, if you're at a closed lab [[03:26:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12366.5s)]
*  where you care about terms of service and IP closely, [[03:26:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12368.300000000001s)]
*  you distill from your own models. [[03:26:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12370.5s)]
*  If you are a researcher and you're not building any products, [[03:26:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12372.18s)]
*  you distill from the OpenAI models. [[03:26:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12374.78s)]
*  This is a good opportunity, [[03:26:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12376.62s)]
*  can you explain big picture distillation as a process? [[03:26:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12377.74s)]
*  What is distillation? [[03:26:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12381.78s)]
*  What's the process of distillation? [[03:26:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12383.02s)]
*  We've talked a lot about training language models, [[03:26:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12383.86s)]
*  they are trained on text. [[03:26:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12386.18s)]
*  And post-training, you're trying to train [[03:26:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12387.5s)]
*  on very high quality text that you want the model [[03:26:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12389.62s)]
*  to match the features of, or if you're using RL, [[03:26:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12391.7s)]
*  you're letting the model find its own thing. [[03:26:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12394.42s)]
*  But for supervised fine tuning, for preference data, [[03:26:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12396.06s)]
*  you need to have some completions [[03:26:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12398.78s)]
*  what the model is trying to learn to imitate. [[03:26:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12400.66s)]
*  And what you do there is instead of a human data [[03:26:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12402.94s)]
*  or instead of the model you're currently training, [[03:26:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12406.74s)]
*  you take completions from a different, [[03:26:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12409.26s)]
*  normally more powerful model. [[03:26:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12411.18s)]
*  I think there's rumors that these big models [[03:26:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12413.18s)]
*  that people are waiting for, these GPT-5s of the world, [[03:26:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12416.300000000001s)]
*  the CLAWD-3 opuses of the world are used internally [[03:26:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12419.300000000001s)]
*  to do this distillation process at OpenAI. [[03:27:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12422.460000000001s)]
*  There's also public examples, right? [[03:27:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12424.54s)]
*  Like meta explicitly stated, [[03:27:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12426.1s)]
*  not necessarily distilling, but they used 405B [[03:27:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12429.18s)]
*  as a reward model for 70B in their LAMA 3.2 and 3.3. [[03:27:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12431.34s)]
*  This is all the same topic. [[03:27:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12435.06s)]
*  So is this ethical, is this legal? [[03:27:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12436.7s)]
*  Why is that Financial Times article headline say, [[03:27:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12441.22s)]
*  OpenAI says that there's evidence that China's DeepSeek [[03:27:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12445.3s)]
*  used its model to train competitor? [[03:27:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12449.42s)]
*  This is a long, at least in the academic side [[03:27:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12451.7s)]
*  and research side, it has a long history [[03:27:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12454.62s)]
*  because you're trying to interpret OpenAI's rule. [[03:27:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12455.94s)]
*  OpenAI's terms of service say that you cannot build [[03:27:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12458.06s)]
*  a competitor with outputs from their model. [[03:27:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12461.38s)]
*  Terms of service are different than a license, [[03:27:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12463.38s)]
*  which are essentially a contract between organizations. [[03:27:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12465.1s)]
*  So if you have a terms of service on OpenAI's account, [[03:27:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12468.06s)]
*  if I violate it, OpenAI can cancel my account. [[03:27:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12470.42s)]
*  This is very different than like a license [[03:27:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12473.1s)]
*  that says how you could use a downstream artifact. [[03:27:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12474.74s)]
*  So a lot of it hinges on a word that is very unclear [[03:27:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12476.939999999999s)]
*  in the AI space, which is what is a competitor. [[03:27:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12479.3s)]
*  And so- [[03:28:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12481.5s)]
*  And then the ethical aspect of it is like, [[03:28:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12482.34s)]
*  why is it unethical for me to train on your model [[03:28:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12484.58s)]
*  when you can train on the internet's text? [[03:28:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12487.22s)]
*  Yeah. [[03:28:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12489.179999999998s)]
*  So there's a bit of a hypocrisy because sort of OpenAI [[03:28:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12490.019999999999s)]
*  and potentially most of the companies trained [[03:28:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12494.38s)]
*  on the internet's text without permission. [[03:28:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12497.98s)]
*  There's also a clear loophole, [[03:28:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12500.3s)]
*  which is that I generate data from OpenAI [[03:28:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12501.539999999999s)]
*  and then I upload it somewhere [[03:28:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12505.34s)]
*  and then somebody else trains on it [[03:28:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12506.9s)]
*  and the link has been broken. [[03:28:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12508.46s)]
*  Like they're not under the same terms of service contract. [[03:28:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12510.22s)]
*  This is why- [[03:28:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12512.939999999999s)]
*  There's a lot of hypocrisy. [[03:28:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12513.779999999999s)]
*  There's a lot of like to be discovered details [[03:28:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12514.86s)]
*  that don't make a lot of sense. [[03:28:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12517.460000000001s)]
*  This is why a lot of models today, [[03:28:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12518.58s)]
*  even if they train on zero OpenAI data, [[03:28:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12520.62s)]
*  you ask the model who trained you, it'll say, [[03:28:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12523.380000000001s)]
*  I am Chad GPT trained by OpenAI [[03:28:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12525.58s)]
*  because there's so much copy paste of like OpenAI outputs [[03:28:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12527.58s)]
*  from that on the internet [[03:28:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12530.74s)]
*  that you just weren't able to filter it out. [[03:28:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12532.26s)]
*  And there was nothing in the RL where they implemented [[03:28:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12533.78s)]
*  like, hey, like, or post-training or SFT, whatever that says, [[03:28:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12536.86s)]
*  hey, I'm actually a model by Allen Institute [[03:28:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12539.86s)]
*  instead of OpenAI. [[03:29:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12542.78s)]
*  We have to do this if we serve a demo, [[03:29:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12543.86s)]
*  we do research and we use OpenAI APIs because it's useful [[03:29:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12545.54s)]
*  and we want to understand post-training [[03:29:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12548.94s)]
*  and like our research models, [[03:29:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12550.42s)]
*  they will say they're written by OpenAI [[03:29:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12552.140000000001s)]
*  unless we put in the system prop that we talked about [[03:29:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12553.82s)]
*  that like, I am Tulu, [[03:29:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12555.980000000001s)]
*  I am a language model trained by the Allen Institute for AI. [[03:29:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12557.060000000001s)]
*  And if you ask more people around industry, [[03:29:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12559.980000000001s)]
*  especially with post-training, [[03:29:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12562.54s)]
*  it's a very doable task to make the model say who it is [[03:29:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12563.86s)]
*  or to suppress the OpenAI thing. [[03:29:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12567.66s)]
*  So in some levels, it might be that DeepSeq didn't care [[03:29:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12569.7s)]
*  that it was saying that it was by OpenAI. [[03:29:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12572.94s)]
*  If you're gonna upload model weights, [[03:29:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12575.26s)]
*  it doesn't really matter [[03:29:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12576.58s)]
*  because anyone that's serving it in an application [[03:29:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12577.42s)]
*  and cares a lot about serving is going to, [[03:29:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12580.1s)]
*  when serving it, if they're using it for a specific task, [[03:29:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12582.5s)]
*  they're gonna tailor it to that. [[03:29:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12585.300000000001s)]
*  And it doesn't matter that it's saying it's chat GBT. [[03:29:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12586.140000000001s)]
*  Oh, I guess the one of the ways to do that [[03:29:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12589.18s)]
*  is like a system prompt or something like that. [[03:29:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12590.9s)]
*  Like if you're serving it to say that you're- [[03:29:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12592.7s)]
*  That's what we do. [[03:29:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12595.140000000001s)]
*  Like if we host the demo, you say, you are Tulu3, [[03:29:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12596.1s)]
*  a language model trained by the Allen Institute for AI. [[03:29:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12599.5s)]
*  We also are benefited from OpenAI data [[03:30:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12602.18s)]
*  because it's a great research tool. [[03:30:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12604.98s)]
*  I mean, do you think there's any truth and value [[03:30:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12606.42s)]
*  to the claim, OpenAI's claim, [[03:30:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12609.82s)]
*  that there's evidence that China's DeepSeq [[03:30:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12613.26s)]
*  used this model to train? [[03:30:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12615.019999999999s)]
*  I think everyone has benefited regardless [[03:30:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12616.22s)]
*  because the data's on the internet [[03:30:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12619.38s)]
*  and therefore it's in your pre-training now, right? [[03:30:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12621.38s)]
*  There are like subreddits where people share [[03:30:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12624.14s)]
*  the best chat GPT outputs and those are in your- [[03:30:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12625.98s)]
*  I think that they're trying to ship the narrative. [[03:30:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12629.42s)]
*  Like they're trying to protect themselves [[03:30:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12632.86s)]
*  and we saw this years ago when ByteDance was actually banned [[03:30:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12634.1s)]
*  from some OpenAI APIs for training on outputs. [[03:30:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12637.34s)]
*  There's other AI startups that most people, [[03:30:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12640.140000000001s)]
*  if you're in the AI culture, [[03:30:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12643.1s)]
*  were like, they just told us they trained on OpenAI outputs [[03:30:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12644.26s)]
*  and they never got banned. [[03:30:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12647.78s)]
*  Like that's how they bootstrapped their early models. [[03:30:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12649.1s)]
*  So it's much easier to get off the ground using this [[03:30:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12651.74s)]
*  than to set up human pipelines and build a strong model. [[03:30:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12654.02s)]
*  So there's long history here [[03:30:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12657.18s)]
*  and a lot of the communications [[03:30:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12658.62s)]
*  are seem like narrative- [[03:30:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12659.94s)]
*  Over the last couple of days, [[03:31:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12661.62s)]
*  we've seen a lot of people distill DeepSeq's model [[03:31:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12662.66s)]
*  into Llama models because the DeepSeq models [[03:31:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12665.42s)]
*  are kind of complicated to run inference on [[03:31:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12668.02s)]
*  because they're mixture of experts [[03:31:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12669.74s)]
*  and they're 600 plus billion parameters and all this. [[03:31:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12671.460000000001s)]
*  And people have distilled them into the Llama models [[03:31:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12674.34s)]
*  because the Llama models are so easy to serve [[03:31:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12676.78s)]
*  and everyone's built the pipelines and tooling for inference [[03:31:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12678.42s)]
*  with the Llama models, right? [[03:31:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12681.060000000001s)]
*  Because it's the open standard. [[03:31:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12682.140000000001s)]
*  So we've seen a sort of roundabout, right? [[03:31:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12684.220000000001s)]
*  Like, is it bad? [[03:31:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12686.58s)]
*  Is it illegal? [[03:31:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12687.78s)]
*  Maybe it's illegal, whatever. [[03:31:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12688.78s)]
*  I don't know about that. [[03:31:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12689.62s)]
*  It could break contracts. [[03:31:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12691.14s)]
*  I don't think it's illegal. [[03:31:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12691.98s)]
*  Like in any legal, like no one's going to jail for this. [[03:31:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12693.06s)]
*  I think like fundamentally, I think it's ethical [[03:31:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12695.58s)]
*  or I hope it's ethical because like the moment becomes, [[03:31:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12698.5s)]
*  we ban that kind of thing, [[03:31:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12702.699999999999s)]
*  it's gonna make everybody much worse off. [[03:31:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12704.82s)]
*  And I also actually, this is difficult, [[03:31:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12707.619999999999s)]
*  but I think you should be allowed to train on the internet. [[03:31:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12711.14s)]
*  I know a lot of authors and creators [[03:31:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12715.099999999999s)]
*  are very sensitive about it. [[03:31:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12716.699999999999s)]
*  That's a difficult question. [[03:31:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12718.14s)]
*  But like the moment you're not allowed to train [[03:31:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12719.78s)]
*  on the internet. [[03:32:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12722.02s)]
*  I agree. [[03:32:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12723.26s)]
*  I have a schizo take on how you can solve this [[03:32:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12724.1s)]
*  because it already works. [[03:32:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12726.26s)]
*  I have a reasonable take on it. [[03:32:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12727.18s)]
*  All right, all right. [[03:32:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12728.5s)]
*  So, you know, Japan has a law [[03:32:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12729.98s)]
*  which you're allowed to train on any training data [[03:32:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12733.26s)]
*  and copyrights don't apply if you want to train a model. [[03:32:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12735.66s)]
*  A, B, Japan has nine gigawatts of curtailed nuclear power. [[03:32:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12738.18s)]
*  C, Japan is allowed under the AI diffusion rule [[03:32:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12743.26s)]
*  to import as many GPUs as they'd like. [[03:32:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12746.38s)]
*  So all we have to do, we have a market here to make. [[03:32:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12748.82s)]
*  We build massive data setters, we rent them to the labs, [[03:32:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12751.06s)]
*  and then we train models in a legally permissible way [[03:32:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12754.5s)]
*  and there's no if, ands or buts. [[03:32:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12757.18s)]
*  And now the models have no potential copyright lawsuit [[03:32:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12759.02s)]
*  from New York Times or anything like that. [[03:32:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12762.9s)]
*  No, no, it's just like completely legal. [[03:32:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12764.38s)]
*  No, so- [[03:32:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12766.26s)]
*  Genius. [[03:32:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12767.1s)]
*  The early copyright lawsuits have fallen [[03:32:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12767.94s)]
*  in the favor of AI training. [[03:32:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12769.82s)]
*  I would say that the long tail of use [[03:32:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12771.94s)]
*  is gonna go in the side of AI, [[03:32:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12775.22s)]
*  which is if you scrape trillions of data, [[03:32:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12777.14s)]
*  you're not looking at the trillions of tokens of data, [[03:32:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12779.9s)]
*  you're not looking and saying, [[03:33:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12782.3s)]
*  this one New York Times article is so important to me. [[03:33:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12784.38s)]
*  But if you're doing a audio generation for music [[03:33:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12786.82s)]
*  or image generation and you say, [[03:33:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12789.3s)]
*  make it in the style of X person, [[03:33:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12790.82s)]
*  that's a reasonable case where you could figure out [[03:33:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12792.9s)]
*  what is their profit margin on inference. [[03:33:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12795.3s)]
*  I don't know if it's gonna be the 50-50 [[03:33:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12798.18s)]
*  of YouTube creator program or something, [[03:33:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12799.98s)]
*  but I would opt into that program as a writer. [[03:33:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12802.34s)]
*  Please, it's gonna be a rough journey, [[03:33:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12805.380000000001s)]
*  but there will be some solutions like that that make sense, [[03:33:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12809.86s)]
*  but there's a long tail where it's just on the internet. [[03:33:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12812.62s)]
*  I think one of the other aspects [[03:33:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12815.78s)]
*  of that Financial Times article implied, [[03:33:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12817.460000000001s)]
*  and leads to a more general question. [[03:33:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12820.5s)]
*  Do you think there's, [[03:33:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12822.94s)]
*  how difficult is spying, espionage, [[03:33:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12824.900000000001s)]
*  and stealing of actual secret code [[03:33:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12828.060000000001s)]
*  and data from inside companies? [[03:33:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12831.42s)]
*  How much of that is being attempted? [[03:33:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12833.82s)]
*  Code and data is hard, but ideas is easy. [[03:33:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12835.26s)]
*  Silicon Valley operates on the way that top employees [[03:33:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12837.699999999999s)]
*  get bought out by other companies for a pay raise. [[03:34:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12841.9s)]
*  And a large reason why these companies do this [[03:34:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12844.98s)]
*  is to bring ideas with them. [[03:34:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12847.26s)]
*  And there's no, I mean, in California, [[03:34:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12849.1s)]
*  there's rules that certain non-competes or whatever [[03:34:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12851.539999999999s)]
*  are illegal in California. [[03:34:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12854.779999999999s)]
*  And whether or not there's NDAs and things, [[03:34:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12855.98s)]
*  that is how a lot of it happens. [[03:34:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12857.82s)]
*  Recently, there was somebody from Gemini [[03:34:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12860.02s)]
*  who helped make this 1 million contacts length. [[03:34:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12863.26s)]
*  And everyone is saying the next llama, [[03:34:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12865.58s)]
*  I mean, he went to the meta team, [[03:34:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12868.1s)]
*  is gonna have 1 million contacts length. [[03:34:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12869.22s)]
*  And that's kind of how the world works. [[03:34:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12871.86s)]
*  As far as like industrial espionage and things, [[03:34:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12874.62s)]
*  that has been greatly successful in the past, right? [[03:34:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12876.7s)]
*  The Americans did it to the Brits, [[03:34:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12880.62s)]
*  the Chinese have done it to the Americans, right? [[03:34:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12883.1s)]
*  And so on and so forth. [[03:34:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12884.9s)]
*  It is a fact of life. [[03:34:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12886.300000000001s)]
*  And so like to argue industrial espionage can be stopped [[03:34:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12888.62s)]
*  is probably unlikely. [[03:34:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12892.34s)]
*  You can make it difficult, [[03:34:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12893.86s)]
*  but even then there's all these stories about like, [[03:34:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12894.82s)]
*  hey, F-35 and F-22 have already been sort of like [[03:34:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12896.98s)]
*  given to China in terms of design plans and stuff. [[03:35:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12900.1s)]
*  Code and stuff, like between, [[03:35:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12902.74s)]
*  I say companies not nation states is probably very difficult, [[03:35:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12904.7s)]
*  but ideas are discussed a lot, right? [[03:35:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12908.86s)]
*  Whether it be a house party in San Francisco [[03:35:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12911.18s)]
*  or a company changing employees, [[03:35:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12913.9s)]
*  or the always the like mythical honeypot [[03:35:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12915.94s)]
*  that always gets talked about, right? [[03:35:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12919.94s)]
*  Like someone gets honeypotted, right? [[03:35:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12921.220000000001s)]
*  Because everyone working on AI is a single dude [[03:35:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12923.1s)]
*  who's in their twenties and thirties, [[03:35:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12925.42s)]
*  not everyone, but like insane amount of insane percentages. [[03:35:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12926.82s)]
*  So there's always like all these like, you know, [[03:35:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12931.26s)]
*  and obviously- [[03:35:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12933.14s)]
*  So honeypotted is like a spy, [[03:35:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12934.14s)]
*  a female spy approaches you and like- [[03:35:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12936.42s)]
*  Yeah. [[03:35:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12938.5s)]
*  Yeah, or male, right? [[03:35:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12939.66s)]
*  You know, it's San Francisco, right? [[03:35:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12941.06s)]
*  But as a single dude, I will say in his late twenties, right? [[03:35:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12942.66s)]
*  Is like, we are very easily corrupted, right? [[03:35:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12946.300000000001s)]
*  Like, you know, like not corrupted myself, [[03:35:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12948.18s)]
*  but you know, like we are, we are, right? [[03:35:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12952.14s)]
*  Everybody else, not me. [[03:35:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12952.98s)]
*  I'm too oblivious and I am not single. [[03:35:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12954.46s)]
*  So I'm safe from one espionage axis. [[03:35:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12956.58s)]
*  Yeah, you have to make sure [[03:36:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12960.619999999999s)]
*  to close all security vulnerabilities. [[03:36:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12962.22s)]
*  So you Dylan collect a lot of information [[03:36:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12964.98s)]
*  about each of the mega clusters [[03:36:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12967.74s)]
*  for each of the major AI companies. [[03:36:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12971.14s)]
*  Can you talk about the build-outs [[03:36:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12973.26s)]
*  for each one that stand out? [[03:36:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12976.46s)]
*  Yeah, so I think the thing that's like really important [[03:36:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12978.22s)]
*  about these mega cluster build-outs [[03:36:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12981.98s)]
*  is they're completely unprecedented in scale, right? [[03:36:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12982.82s)]
*  US, you know, sort of like data center power consumption [[03:36:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12986.9s)]
*  has been slowly on the rise and it's gone up to two, [[03:36:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12989.26s)]
*  3%, even through the cloud computing revolution, right? [[03:36:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12991.74s)]
*  Data center consumption as a percentage of total US. [[03:36:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12994.9s)]
*  And that's been over decades, right? [[03:36:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12997.42s)]
*  Of data centers, et cetera. [[03:36:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=12999.22s)]
*  It's been climbing, climbing slowly, [[03:36:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13000.5s)]
*  but now two to 3%. [[03:36:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13001.859999999999s)]
*  Now, by the end of this decade, it's like, [[03:36:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13003.06s)]
*  even under like, you know, when I say like 10%, [[03:36:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13005.539999999999s)]
*  a lot of people that are traditionally [[03:36:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13008.46s)]
*  by like 2028, 2030, people traditionally non-AI, [[03:36:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13010.38s)]
*  traditional data center people like that's nuts. [[03:36:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13014.539999999999s)]
*  But then like people who are in like AI [[03:36:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13016.66s)]
*  who have like really looked at this [[03:36:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13018.9s)]
*  at like the anthropics and open AI's [[03:37:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13020.019999999999s)]
*  are like, that's not enough. [[03:37:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13021.66s)]
*  I'm like, okay. [[03:37:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13022.5s)]
*  But like, you know, this is both through [[03:37:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13024.5s)]
*  globally distributed or distributed throughout the US [[03:37:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13027.539999999999s)]
*  as well as like centralized clusters, right? [[03:37:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13030.339999999998s)]
*  The distributed throughout the US is exciting [[03:37:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13032.3s)]
*  and it's the bulk of it, right? [[03:37:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13034.779999999999s)]
*  Like, hey, you know, open AI or, you know, say Meta's [[03:37:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13036.1s)]
*  adding a gigawatt, right? [[03:37:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13040.38s)]
*  But most of it is distributed through the US [[03:37:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13042.939999999999s)]
*  for inference and all these other things, right? [[03:37:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13044.9s)]
*  So maybe we should lay out what a cluster is. [[03:37:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13046.699999999999s)]
*  So, you know, does this include AWS? [[03:37:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13049.699999999999s)]
*  Maybe it's good to talk about the different kinds [[03:37:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13053.42s)]
*  of clusters and what you mean by mega clusters [[03:37:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13055.82s)]
*  and what's the GPU and what's the computer and what is, [[03:37:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13057.779999999999s)]
*  yeah, not that far back, but yeah. [[03:37:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13060.58s)]
*  So like, what do we mean by the clusters? [[03:37:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13063.42s)]
*  Oh man, I thought I was about to do the Apple ad, right? [[03:37:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13065.34s)]
*  What's a computer? [[03:37:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13067.34s)]
*  So traditionally data centers and data center tasks [[03:37:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13068.18s)]
*  have been a distributed systems problem [[03:37:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13073.66s)]
*  that is capable of being spread very far and widely, right? [[03:37:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13076.14s)]
*  I.e. I send a request to Google, [[03:38:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13080.34s)]
*  it gets routed to a data center somewhat close to me. [[03:38:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13082.7s)]
*  It does whatever search ranking recommendations, [[03:38:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13085.66s)]
*  sends a result back, right? [[03:38:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13088.02s)]
*  The nature of the task is changing rapidly [[03:38:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13090.98s)]
*  in that the task, there's two tasks [[03:38:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13093.22s)]
*  that people are really focused on now, right? [[03:38:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13095.26s)]
*  It's not database access, it's not serve me the right page, [[03:38:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13096.66s)]
*  serve me the right ad, it's now a inference, [[03:38:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13099.26s)]
*  and inference is dramatically different [[03:38:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13102.9s)]
*  from traditional distributed systems, [[03:38:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13104.58s)]
*  but it looks a lot more simple, similar, [[03:38:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13105.82s)]
*  and then there's training, right? [[03:38:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13107.859999999999s)]
*  The inference side is still like, [[03:38:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13109.699999999999s)]
*  hey, I'm gonna put thousands of GPUs [[03:38:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13111.66s)]
*  in blocks all around these data centers, [[03:38:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13113.939999999999s)]
*  I'm gonna run models on them, [[03:38:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13117.26s)]
*  user submits a request, gets kicked off, [[03:38:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13118.9s)]
*  or hey, my service, they submit a request to my service, [[03:38:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13121.04s)]
*  they're on Word and they're like, [[03:38:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13124.320000000002s)]
*  oh yeah, help me copilot, and it kicks it off [[03:38:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13125.160000000002s)]
*  from on my Windows, copilot, whatever, [[03:38:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13127.04s)]
*  Apple Intelligence, whatever it is, [[03:38:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13128.960000000001s)]
*  it gets kicked off to a data center, right? [[03:38:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13130.12s)]
*  And that data center does some work and sends it back, [[03:38:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13132.320000000002s)]
*  that's inference, that is going to be the bulk of compute, [[03:38:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13134.52s)]
*  but then, and that's like, there's thousands of data centers [[03:38:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13138.0s)]
*  that we're tracking with satellites [[03:39:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13141.68s)]
*  and all these other things, [[03:39:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13143.2s)]
*  and those are the bulk of what's being built, [[03:39:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13144.480000000001s)]
*  but the scale of, and so that's what's really reshaping [[03:39:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13146.84s)]
*  and that's what's getting millions of GPUs, [[03:39:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13150.16s)]
*  but the scale of the largest cluster [[03:39:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13151.8s)]
*  is also really important, right? [[03:39:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13154.48s)]
*  When we look back at history, right, [[03:39:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13157.16s)]
*  like, you know, or through the age of AI, right, [[03:39:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13158.8s)]
*  like it was a really big deal when they did AlexNet [[03:39:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13162.76s)]
*  on I think two GPUs or four GPUs, I don't remember, [[03:39:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13166.2s)]
*  it was a really big deal. [[03:39:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13169.119999999999s)]
*  It's a big deal because you used GPUs. [[03:39:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13170.24s)]
*  It's a big deal they used GPUs, and they used multiple, [[03:39:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13171.8s)]
*  right, but then over time, [[03:39:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13174.52s)]
*  its scale has just been compounding, right? [[03:39:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13175.72s)]
*  And so when you skip forward to GPT-3, then GPT-4, [[03:39:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13178.48s)]
*  GPT-4, 20,000 A100 GPUs, unprecedented run, right, [[03:39:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13181.88s)]
*  in terms of the size and the cost, right? [[03:39:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13186.56s)]
*  A couple hundred million dollars on a YOLO, right, [[03:39:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13188.32s)]
*  a YOLO run for GPT-4, and it yielded, you know, [[03:39:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13190.359999999999s)]
*  this magical improvement that was like perfectly in line [[03:39:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13193.039999999999s)]
*  with what was experimented and just like a log scale, right? [[03:39:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13196.16s)]
*  Oh yeah, they have that plot from the paper. [[03:39:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13198.8s)]
*  The technical part. [[03:40:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13201.199999999999s)]
*  The scaling laws were perfect, right? [[03:40:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13202.52s)]
*  But that's not a crazy number, right? [[03:40:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13204.0s)]
*  20,000 A100s, roughly each GPU is consuming 400 watts, [[03:40:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13205.88s)]
*  and then when you add in the whole server, right, everything, [[03:40:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13210.92s)]
*  it's like 15 to 20 megawatts of power, right? [[03:40:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13213.64s)]
*  You know, maybe you could look up [[03:40:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13217.359999999999s)]
*  what the power of consumption of a human person is [[03:40:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13219.599999999999s)]
*  because the numbers are gonna get silly, [[03:40:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13221.679999999998s)]
*  but like 15 to 20 megawatts was standard data center size. [[03:40:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13223.039999999999s)]
*  It was just unprecedented. [[03:40:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13226.32s)]
*  That was all GPUs running one task. [[03:40:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13227.16s)]
*  How many watts was a toaster? [[03:40:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13228.96s)]
*  A toaster is like also a similar power consumption [[03:40:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13231.119999999999s)]
*  to an A100, right? [[03:40:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13234.4s)]
*  A to A100 comes around, [[03:40:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13236.4s)]
*  they increase the power from like 400 to 700 watts, [[03:40:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13237.24s)]
*  and that's just per GPU, [[03:40:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13239.16s)]
*  and then there's all the associated stuff around it. [[03:40:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13240.32s)]
*  So once you count all that, [[03:40:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13242.08s)]
*  it's roughly like 1200 to 1400 watts for everything, [[03:40:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13243.439999999999s)]
*  networking, CPUs, memory, blah, blah, blah. [[03:40:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13246.56s)]
*  So we should also say, so what's required? [[03:40:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13248.72s)]
*  You said power, so a lot of power is required. [[03:40:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13252.56s)]
*  A lot of heat is generated, so the cooling is required, [[03:40:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13255.32s)]
*  and because there's a lot of GPUs that have to be, [[03:40:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13258.84s)]
*  or CPUs or whatever, they have to be connected, [[03:41:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13262.039999999999s)]
*  so there's a lot of networking. [[03:41:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13264.88s)]
*  Yeah, yeah, so I think, yeah, sorry for skipping past that. [[03:41:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13266.48s)]
*  And then the data center itself is like complicated, right? [[03:41:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13270.119999999999s)]
*  But these are still standard sized data centers [[03:41:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13272.119999999999s)]
*  for GPT-4 scale, right? [[03:41:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13274.64s)]
*  Now we step forward to sort of what is the scale of clusters [[03:41:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13276.199999999999s)]
*  that people have built last year, right? [[03:41:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13279.92s)]
*  And it ranges widely, right? [[03:41:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13282.339999999998s)]
*  It ranges from like, hey, these are standard data centers, [[03:41:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13284.32s)]
*  and we're just using multiple of them [[03:41:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13287.14s)]
*  and connecting them together really with a ton of fiber [[03:41:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13288.64s)]
*  between them, a lot of networking, et cetera. [[03:41:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13290.92s)]
*  That's what OpenAI and Microsoft did in Arizona, right? [[03:41:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13292.64s)]
*  And so they have 100,000 GPUs, right? [[03:41:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13295.119999999999s)]
*  Meta, similar thing, they took their standard [[03:41:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13297.5s)]
*  existing data center design, and it looks like an H, [[03:41:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13299.279999999999s)]
*  and they connected multiple of them together. [[03:41:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13302.039999999999s)]
*  And they got to, they first did 16,000 GPUs, [[03:41:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13304.56s)]
*  24,000 GPUs total, only 16,000 of them were running [[03:41:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13307.92s)]
*  on the training run because GPUs are very unreliable, [[03:41:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13311.16s)]
*  so they need to have spares to swap in and out, [[03:41:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13313.279999999999s)]
*  all the way to now 100,000 GPUs that they're training [[03:41:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13315.46s)]
*  on Lama 4 on currently, right? [[03:41:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13317.96s)]
*  Like 128,000 or so, right? [[03:41:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13319.68s)]
*  This is, think about 100,000 GPUs [[03:42:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13321.88s)]
*  with roughly 1400 watts a piece, [[03:42:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13325.56s)]
*  that's 140 megawatts, 150 megawatts, right? [[03:42:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13328.4s)]
*  For 128 of that, right? [[03:42:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13331.56s)]
*  So you're talking about, you've jumped from 15 to 20 megawatts [[03:42:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13332.88s)]
*  to 10x, almost 10x that number, 9x that number [[03:42:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13335.759999999998s)]
*  to 150 megawatts in two years, right? [[03:42:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13339.359999999999s)]
*  From 2022 to 2024, right? [[03:42:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13342.679999999998s)]
*  And some people like Elon, he admittedly, right? [[03:42:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13344.679999999998s)]
*  And he says himself got into the game a little bit late [[03:42:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13347.92s)]
*  for pre-training large language models, right? [[03:42:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13350.04s)]
*  XAI was started later, right? [[03:42:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13351.92s)]
*  But then he bent heaven and hell to get his data center up [[03:42:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13353.560000000001s)]
*  and get the largest cluster in the world, right? [[03:42:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13357.12s)]
*  Which is 200,000 GPUs. [[03:42:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13358.640000000001s)]
*  And he did that, he bought a factory in Memphis, [[03:42:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13360.820000000002s)]
*  he's upgrading the substation at the same time, [[03:42:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13364.12s)]
*  he's got a bunch of mobile power generation, [[03:42:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13366.900000000001s)]
*  a bunch of single cycle combine, [[03:42:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13368.68s)]
*  he tapped the natural gas line [[03:42:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13370.720000000001s)]
*  that's right next to the factory [[03:42:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13372.2s)]
*  and he's just pulling a ton of gas, burning gas, [[03:42:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13373.84s)]
*  he's generating all this power, [[03:42:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13375.84s)]
*  he's in a factory, in an old appliance factory [[03:42:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13377.480000000001s)]
*  that's shut down and moved to China long ago, right? [[03:43:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13380.8s)]
*  And he's got 200,000 GPUs in it. [[03:43:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13382.44s)]
*  And now what's the next scale, right? [[03:43:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13385.6s)]
*  All the hyperscalers have done this. [[03:43:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13387.32s)]
*  Now the next scale is something that's even bigger, right? [[03:43:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13388.6s)]
*  And so, Elon, just to stick on the topic, [[03:43:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13391.6s)]
*  he's building his own natural gas plant, [[03:43:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13393.92s)]
*  like a proper one right next door, [[03:43:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13396.36s)]
*  he's deploying tons of Tesla MegaPak batteries [[03:43:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13398.64s)]
*  to make the power more smooth [[03:43:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13401.960000000001s)]
*  and all sorts of other things. [[03:43:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13403.56s)]
*  He's got like industrial chillers to cool the water down [[03:43:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13404.56s)]
*  because he's water cooling the chips. [[03:43:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13407.76s)]
*  So all these crazy things [[03:43:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13409.92s)]
*  to get the clusters bigger and bigger. [[03:43:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13411.4s)]
*  But when you look at like say, [[03:43:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13414.1s)]
*  what OpenAI did with Stargate, [[03:43:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13415.14s)]
*  that's that in Arizona, in Abilene, Texas, right? [[03:43:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13417.48s)]
*  What they've announced at least, right? [[03:43:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13422.48s)]
*  It's not built, right? [[03:43:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13423.64s)]
*  Elon says they don't have the money, [[03:43:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13424.48s)]
*  there's some debates about this. [[03:43:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13425.960000000001s)]
*  But at full scale, at least the first section [[03:43:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13428.32s)]
*  is like definitely money is accounted for [[03:43:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13430.72s)]
*  but there's multiple sections. [[03:43:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13432.2s)]
*  But full scale, that data center [[03:43:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13433.36s)]
*  is gonna be 2.2 gigawatts, right? [[03:43:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13435.08s)]
*  2200 megawatts of power in [[03:43:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13437.04s)]
*  and roughly like 1.8 gigawatts or 1800 megawatts, [[03:43:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13439.36s)]
*  yeah, 1800 megawatts of power delivered to chips, right? [[03:44:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13444.04s)]
*  Now this is an absurd scale. [[03:44:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13447.84s)]
*  2.2 gigawatts is like more than most cities, right? [[03:44:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13449.400000000001s)]
*  To be clear. [[03:44:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13452.560000000001s)]
*  And it delivered to a single cluster [[03:44:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13453.400000000001s)]
*  that's connected to do training, right? [[03:44:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13456.320000000002s)]
*  To train these models, to do both the pre-training, [[03:44:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13458.84s)]
*  the post-training, all of this stuff, right? [[03:44:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13460.84s)]
*  This is insane. [[03:44:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13462.720000000001s)]
*  This is insane. [[03:44:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13463.84s)]
*  What is a nuclear power plant again? [[03:44:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13464.68s)]
*  And everyone is doing this, right? [[03:44:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13465.52s)]
*  Meta in Louisiana, right? [[03:44:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13467.68s)]
*  They're building two natural gas plants, massive ones, [[03:44:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13470.36s)]
*  and then they're building this massive data center. [[03:44:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13473.68s)]
*  Amazon has plans for the scale. [[03:44:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13476.8s)]
*  Google has plans for the scale. [[03:44:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13479.52s)]
*  XAI has plans for the scale, right? [[03:44:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13482.12s)]
*  Like all of these, the guys that are racing, [[03:44:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13484.08s)]
*  the companies that are racing are racing hard [[03:44:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13486.64s)]
*  and they're doing multi-gigawatt data centers, right? [[03:44:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13488.56s)]
*  To build this out because they think that, yeah, [[03:44:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13492.08s)]
*  if I now have, obviously pre-training scaling [[03:44:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13495.48s)]
*  is gonna continue, but to some extent, [[03:44:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13498.56s)]
*  but then also all this post-training stuff [[03:45:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13500.08s)]
*  where you have a RL sandbox for computer use or whatever, [[03:45:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13501.56s)]
*  right? [[03:45:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13504.0s)]
*  This is where they're gonna, [[03:45:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13504.84s)]
*  and all these very full viable domains [[03:45:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13505.68s)]
*  where they just keep learning and learning and learning, [[03:45:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13507.439999999999s)]
*  self-play, whatever, whatever it is, [[03:45:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13509.039999999999s)]
*  makes the AI so much more capable [[03:45:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13510.92s)]
*  because the line does go up, right? [[03:45:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13512.68s)]
*  As you throw more compute, you get more performance. [[03:45:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13514.64s)]
*  The shirt is about scaling laws. [[03:45:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13517.08s)]
*  To some extent, it is diminishing returns, right? [[03:45:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13519.6s)]
*  You 10x the compute. [[03:45:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13521.32s)]
*  You don't get 10x better model, right? [[03:45:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13522.52s)]
*  You get a diminishing returns, [[03:45:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13524.119999999999s)]
*  but also you get efficiency improvements, [[03:45:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13525.2s)]
*  so you bend the curve, right? [[03:45:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13527.2s)]
*  And these scale of data centers are doing, [[03:45:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13528.880000000001s)]
*  wreaking a lot of havoc on the network, right? [[03:45:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13531.44s)]
*  And Nathan was mentioning there's, [[03:45:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13534.720000000001s)]
*  Amazon has tried to buy this nuclear power plant, Talon, [[03:45:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13537.12s)]
*  and if you look at the Talon stock, [[03:45:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13540.52s)]
*  it's just skyrocketing, [[03:45:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13542.1s)]
*  and they're building a massive multi-gigawatt data center [[03:45:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13543.240000000002s)]
*  there, and you just go down the list. [[03:45:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13546.68s)]
*  There's so many ramifications. [[03:45:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13548.08s)]
*  Interesting thing is certain regions of the US, [[03:45:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13550.0s)]
*  transmitting power costs more [[03:45:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13552.56s)]
*  than actually generating it, right? [[03:45:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13555.0s)]
*  Because the grid is so slow to build, [[03:45:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13556.92s)]
*  and the demand for power and the ability to build power [[03:45:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13559.08s)]
*  and re-ramping on a natural gas plant or even a coal plant [[03:46:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13561.199999999999s)]
*  is easy enough to do, [[03:46:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13564.199999999999s)]
*  but transmitting the power is really hard. [[03:46:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13565.24s)]
*  So in some parts of the US, in Virginia, [[03:46:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13567.18s)]
*  it costs more to transmit power than it costs to generate it. [[03:46:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13570.119999999999s)]
*  Which is like, there's all sorts of second order effects [[03:46:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13572.88s)]
*  that are insane here. [[03:46:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13575.439999999999s)]
*  Can the power grid support this kind of growth? [[03:46:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13576.72s)]
*  Trump's executive order, [[03:46:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13579.199999999999s)]
*  there was a Biden executive order before the end of the year, [[03:46:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13580.72s)]
*  but then Trump had some more executive orders, [[03:46:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13582.84s)]
*  which hopefully reduced the regulations [[03:46:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13584.56s)]
*  to where, yes, things can be built. [[03:46:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13588.0s)]
*  But yeah, this is a big, big challenge, right? [[03:46:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13590.76s)]
*  Is building enough power fast enough? [[03:46:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13592.48s)]
*  Are you gonna basically have a nuclear power plant [[03:46:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13593.92s)]
*  next to a data center for each one of these? [[03:46:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13596.14s)]
*  So the fun thing here is this is too slow [[03:46:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13598.88s)]
*  to build the power plant. [[03:46:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13602.52s)]
*  To build a power plant or to reconfigure [[03:46:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13603.52s)]
*  an existing power plant is too slow. [[03:46:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13606.359999999999s)]
*  And so therefore you must use, [[03:46:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13608.18s)]
*  data center power consumption is flat, right? [[03:46:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13610.98s)]
*  I mean, it's by too, right? [[03:46:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13612.86s)]
*  Which is why nuclear is also good for it. [[03:46:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13613.7s)]
*  Long-term nuclear is a very natural fit, [[03:46:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13615.62s)]
*  but you can't do solar or anything in the short term like that. [[03:46:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13618.14s)]
*  Because data center power is like this, right? [[03:47:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13622.86s)]
*  You're telling me I'm gonna buy [[03:47:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13624.380000000001s)]
*  tens of billions of dollars of GPUs [[03:47:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13626.9s)]
*  and idle them because the power's not being generated? [[03:47:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13628.9s)]
*  Power's cheap, right? [[03:47:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13630.78s)]
*  If you look at the cost of a cluster, [[03:47:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13631.98s)]
*  less than 20% of it is power, right? [[03:47:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13633.56s)]
*  Most of it is the capital cost [[03:47:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13636.4s)]
*  and depreciation of the GPUs, right? [[03:47:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13637.84s)]
*  And so it's like, well, screw it. [[03:47:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13639.88s)]
*  I'll just build natural gas plants. [[03:47:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13641.44s)]
*  This is what Metta's doing in Louisiana. [[03:47:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13643.279999999999s)]
*  This is what OpenAI's doing in Texas [[03:47:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13644.48s)]
*  and all these different places. [[03:47:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13647.08s)]
*  They may not be doing it directly, [[03:47:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13648.48s)]
*  but they are partnered with someone. [[03:47:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13650.4s)]
*  And so there is a couple hopes, right? [[03:47:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13651.6s)]
*  One is, and Elon, what he's doing in Memphis is like, [[03:47:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13654.24s)]
*  to the extreme, they're not just using [[03:47:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13657.8s)]
*  dual-combine cycle gas, which is super efficient. [[03:47:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13658.92s)]
*  He's also just using single cycle [[03:47:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13662.039999999999s)]
*  and mobile generators and stuff, which is less efficient. [[03:47:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13663.6s)]
*  But there's also the flip side, [[03:47:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13666.96s)]
*  which is solar power generation is like this [[03:47:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13669.119999999999s)]
*  and wind is another like this, different correlate, [[03:47:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13671.6s)]
*  you know, different. [[03:47:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13674.16s)]
*  So if you stack both of those, [[03:47:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13675.0s)]
*  plus you get a big chunk of batteries, [[03:47:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13676.48s)]
*  plus you have a little bit of gas, [[03:47:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13678.96s)]
*  it is possible to run it more green. [[03:48:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13680.44s)]
*  It's just the time scales for that is slow, right? [[03:48:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13682.4s)]
*  So people are trying, but Metta basically said, [[03:48:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13684.84s)]
*  whatever, don't care about my sustainability pledge, [[03:48:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13689.32s)]
*  or they'll buy a per power, [[03:48:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13691.76s)]
*  it's called a PPA, power purchasing agreement, [[03:48:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13693.24s)]
*  where there'll be a massive wind farm or solar farm, [[03:48:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13695.14s)]
*  like wherever, and then they'll just pretend [[03:48:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13697.3s)]
*  like those electrons are being consumed by the data center. [[03:48:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13699.18s)]
*  But in reality, they're paying for the power here [[03:48:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13701.22s)]
*  and selling it to the grid and they're buying power here. [[03:48:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13703.3s)]
*  And then another thing is like, [[03:48:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13706.14s)]
*  Microsoft quit on some of their sustainability pledges, [[03:48:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13707.38s)]
*  right, Elon, what he did with Memphis [[03:48:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13709.9s)]
*  is objectively somewhat dirty, [[03:48:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13712.58s)]
*  but he's also doing it in an area [[03:48:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13714.18s)]
*  where there's like a bigger natural gas plant [[03:48:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13715.5s)]
*  right next door and like a sewer, [[03:48:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13717.619999999999s)]
*  or not a sewer, but like a wastewater treatment [[03:48:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13719.9s)]
*  and a garbage dump nearby, right? [[03:48:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13721.42s)]
*  And he's obviously made the world a lot more clean [[03:48:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13722.98s)]
*  than that one data center is gonna do, right? [[03:48:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13725.74s)]
*  So I think like it's fine to some extent, [[03:48:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13727.92s)]
*  and maybe AGI solves global warming and stuff, right? [[03:48:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13730.18s)]
*  Whatever it is, this is sort of the attitude [[03:48:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13733.1s)]
*  that people at the labs have, right? [[03:48:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13736.42s)]
*  Which is like, yeah, it's great, we'll just use gas, right? [[03:48:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13737.66s)]
*  Because the race is that important, [[03:48:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13739.939999999999s)]
*  and if we lose, that's way worse, right? [[03:49:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13741.42s)]
*  I should say that I got a chance to visit [[03:49:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13744.039999999999s)]
*  the Memphis data center. [[03:49:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13746.939999999999s)]
*  Oh wow. [[03:49:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13748.02s)]
*  And it's kind of incredible. [[03:49:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13748.859999999999s)]
*  I mean, I visited with Elon, [[03:49:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13750.74s)]
*  just the teams and the rate of innovation there is insane. [[03:49:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13754.1s)]
*  That's my sense is that nobody's ever done anything [[03:49:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13758.699999999999s)]
*  of this scale, and nobody has certainly ever done anything [[03:49:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13762.1s)]
*  of this scale at the rate that XAI is doing. [[03:49:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13765.98s)]
*  So they're like figuring out, [[03:49:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13769.18s)]
*  I mean, it's all sitting in on all these meetings [[03:49:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13771.619999999999s)]
*  where they're brainstorming. [[03:49:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13773.86s)]
*  It's like, it's insane. [[03:49:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13774.94s)]
*  It's exciting, because they're like, [[03:49:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13776.74s)]
*  they're trying to figure out what the bottlenecks are, [[03:49:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13778.699999999999s)]
*  how to remove the bottlenecks, [[03:49:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13781.7s)]
*  how to make sure that there's just so many [[03:49:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13782.54s)]
*  really cool things about putting together a data center, [[03:49:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13784.66s)]
*  because everything has to work. [[03:49:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13788.220000000001s)]
*  It's the people that do like the sysadmin, [[03:49:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13791.94s)]
*  the machine learning, all that is the exciting thing, so on. [[03:49:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13795.5s)]
*  But really the people that run everything are the folks [[03:49:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13798.140000000001s)]
*  that know the low level software and hardware [[03:50:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13801.62s)]
*  that runs everything, the networking, all of that. [[03:50:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13805.34s)]
*  And so you have to make sure you have procedures [[03:50:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13808.380000000001s)]
*  that test everything. [[03:50:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13811.18s)]
*  I think they're using ethernet. [[03:50:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13812.14s)]
*  I don't know how they're doing the networking, but. [[03:50:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13813.82s)]
*  They're using Nvidia Spectrum X ethernet. [[03:50:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13815.3s)]
*  I think, yeah, the unsung heroes are the cooling [[03:50:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13819.18s)]
*  and electrical systems, which are just glossed over. [[03:50:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13821.18s)]
*  But I think one story that maybe exemplifies [[03:50:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13825.18s)]
*  how insane this stuff is, is when you're training, [[03:50:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13828.66s)]
*  you're always doing, you're running through the model [[03:50:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13833.1s)]
*  a bunch, in the most simplistic terms, [[03:50:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13835.62s)]
*  running through the model a bunch, [[03:50:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13837.02s)]
*  and then you're gonna exchange everything [[03:50:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13838.5s)]
*  and synchronize the weights, right? [[03:50:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13841.54s)]
*  So you'll do a step. [[03:50:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13843.060000000001s)]
*  This is like a step in model training, right? [[03:50:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13844.140000000001s)]
*  And every step, your loss goes down, hopefully, [[03:50:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13845.7s)]
*  and it doesn't always. [[03:50:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13847.380000000001s)]
*  But in the simplest terms, you'll be computing a lot, [[03:50:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13848.220000000001s)]
*  and then you'll exchange, right? [[03:50:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13851.1s)]
*  The interesting thing is GPU power is most of it. [[03:50:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13852.66s)]
*  Networking power is some, but it's a lot less. [[03:50:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13854.58s)]
*  So while you're computing, your power for your GPUs is here. [[03:50:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13856.62s)]
*  But then when you're exchanging weights, [[03:50:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13859.380000000001s)]
*  if you're not able to overlap communications [[03:51:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13861.26s)]
*  and compute perfectly, there may be a time period [[03:51:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13863.140000000001s)]
*  where your GPUs are just idle, and you're exchanging weights, [[03:51:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13865.3s)]
*  and you're like, hey, the model's updating. [[03:51:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13868.099999999999s)]
*  So you're exchanging the radiance, [[03:51:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13869.58s)]
*  you do the model update, and then you start training again. [[03:51:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13870.699999999999s)]
*  So the power goes, right? [[03:51:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13873.5s)]
*  And it's super spiky. [[03:51:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13875.779999999999s)]
*  And so funnily enough, right? [[03:51:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13877.14s)]
*  Like this, when you talk about the scale [[03:51:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13878.46s)]
*  of data center power, right? [[03:51:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13880.74s)]
*  You can blow stuff up so easily. [[03:51:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13882.06s)]
*  And so Meta actually has accidentally [[03:51:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13884.9s)]
*  upstreamed something to code in PyTorch, [[03:51:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13887.539999999999s)]
*  where they added an operator. [[03:51:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13890.38s)]
*  And I kid you not, whoever made this, [[03:51:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13891.98s)]
*  I want to hug the guy because it says PyTorch, [[03:51:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13893.34s)]
*  it's like PyTorch.powerplant no blow up, [[03:51:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13896.7s)]
*  equals zero or equal one. [[03:51:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13899.1s)]
*  And what it does is amazing, right? [[03:51:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13900.58s)]
*  Either when you're exchanging the weights, [[03:51:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13904.1s)]
*  the GPU will just compute fake numbers [[03:51:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13906.5s)]
*  so the power doesn't spike too much. [[03:51:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13908.22s)]
*  And so then the power plants don't blow up [[03:51:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13909.86s)]
*  because the transient spikes screw stuff up. [[03:51:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13911.380000000001s)]
*  Well, that makes sense. [[03:51:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13914.22s)]
*  I mean, you have to do that kind of thing. [[03:51:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13915.06s)]
*  You have to make sure they're not idle. [[03:51:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13916.58s)]
*  Yeah. [[03:51:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13918.82s)]
*  And Elon's solution was like, let me throw a bunch [[03:51:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13919.66s)]
*  of Tesla megapacks and a few other things, right? [[03:52:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13920.9s)]
*  Everyone has different solutions, [[03:52:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13923.34s)]
*  but Meta's at least was publicly and openly known, [[03:52:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13924.62s)]
*  which is just like set this operator. [[03:52:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13927.26s)]
*  And what this operator does is it just makes the GPUs [[03:52:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13929.300000000001s)]
*  compute nothing so that the power doesn't spike. [[03:52:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13931.62s)]
*  But that just tells you how much power you're working with. [[03:52:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13934.42s)]
*  I mean, it's insane. [[03:52:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13937.060000000001s)]
*  It's insane. [[03:52:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13938.140000000001s)]
*  People should just go to Google, like scale, [[03:52:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13938.980000000001s)]
*  like what does X watts do and go through all the scales [[03:52:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13941.54s)]
*  from one watt to a kilowatt to a megawatt. [[03:52:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13944.740000000002s)]
*  And you look and stare at that and you're how high [[03:52:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13946.980000000001s)]
*  in the list a gigawatt is and it's mind blowing. [[03:52:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13949.82s)]
*  Can you say something about the cooling? [[03:52:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13954.14s)]
*  So I know Elon's using liquid cooling, I believe, [[03:52:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13955.58s)]
*  in all cases. [[03:52:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13959.619999999999s)]
*  That's a new thing, right? [[03:52:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13962.38s)]
*  Most of them don't use liquid cooling. [[03:52:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13963.66s)]
*  Is there something interesting to say about the cooling? [[03:52:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13965.22s)]
*  Yeah, yeah. [[03:52:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13966.86s)]
*  So air cooling has been the de facto standard. [[03:52:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13967.699999999999s)]
*  Throw a bunch of metal, heat pipes, et cetera, [[03:52:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13970.18s)]
*  and fans, right? [[03:52:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13972.46s)]
*  And like that's cool. [[03:52:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13973.3s)]
*  That's been enough to cool it. [[03:52:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13974.3s)]
*  People have been dabbling in water cooling. [[03:52:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13976.46s)]
*  Google's TPUs are water cooled, right? [[03:52:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13978.18s)]
*  So they've been doing that for a few years. [[03:53:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13981.54s)]
*  But with GPUs, no one's ever done, [[03:53:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13983.82s)]
*  and no one's ever done the scale of water cooling [[03:53:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13985.86s)]
*  that Elon just did, right? [[03:53:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13987.62s)]
*  Now, next generation NVIDIA is for the highest end GPU, [[03:53:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13989.7s)]
*  it is mandatory water cooling. [[03:53:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13993.98s)]
*  You have to water cool it. [[03:53:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13995.18s)]
*  But Elon did it on this current generation [[03:53:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13996.220000000001s)]
*  and that required a lot of stuff, right? [[03:53:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=13998.9s)]
*  If you look at like some of the satellite photos [[03:53:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14000.300000000001s)]
*  and stuff of the Memphis facility, [[03:53:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14001.94s)]
*  there's all these external water chillers [[03:53:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14005.300000000001s)]
*  that are sitting basically, [[03:53:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14007.16s)]
*  it looks like a semi truck pod thing, [[03:53:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14008.9s)]
*  what's it called, the container? [[03:53:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14010.82s)]
*  But really those are water chillers. [[03:53:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14011.98s)]
*  And he has like 90 of those water chillers [[03:53:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14013.38s)]
*  just sitting outside. [[03:53:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14015.14s)]
*  90 different containers, right? [[03:53:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14016.06s)]
*  With water, you know, like chill the water, [[03:53:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14017.5s)]
*  bring it back to the data center, [[03:53:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14019.46s)]
*  and then you distribute it to all the chips, [[03:53:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14020.9s)]
*  pull all the heat out, and then send it back, right? [[03:53:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14022.82s)]
*  And this is both a way to cool the chips, [[03:53:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14024.74s)]
*  but also an efficiency thing, right? [[03:53:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14027.74s)]
*  And going back to that like sort of three vector thing, [[03:53:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14029.82s)]
*  right, there is, you know, [[03:53:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14032.06s)]
*  memory bandwidth flops and interconnect. [[03:53:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14034.5s)]
*  The closer the chips are together, [[03:53:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14036.74s)]
*  the easier it is to do high speed interconnects, right? [[03:53:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14038.16s)]
*  And so this is also like a reason [[03:54:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14042.3s)]
*  why you're gonna go water cooling [[03:54:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14044.8s)]
*  is because you can just put the chips [[03:54:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14045.82s)]
*  right next to each other, [[03:54:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14047.619999999999s)]
*  and therefore get higher speed connectivity. [[03:54:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14048.6s)]
*  I gotta ask you, so in one of your recent posts, [[03:54:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14053.22s)]
*  there's a section called cluster measuring contest. [[03:54:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14058.619999999999s)]
*  So... [[03:54:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14061.539999999999s)]
*  There's another word there, but I won't say it, you know? [[03:54:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14062.82s)]
*  Who's got the biggest now, and who's gonna have the biggest? [[03:54:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14065.14s)]
*  Today, individual largest is Elon, right? [[03:54:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14071.019999999999s)]
*  Elon's cluster. [[03:54:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14075.699999999999s)]
*  Elon's cluster in Memphis, 200,000 GPUs, right? [[03:54:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14076.699999999999s)]
*  Meta has like 128,000, OpenAI has 100,000. [[03:54:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14080.46s)]
*  Now, to be clear, other companies have more GPUs than Elon. [[03:54:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14083.38s)]
*  They just don't have them in one place, right? [[03:54:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14086.18s)]
*  And for training, you want them tightly connected. [[03:54:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14088.099999999999s)]
*  There's some techniques that people are researching [[03:54:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14090.58s)]
*  and working on that let you train across multiple regions, [[03:54:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14092.94s)]
*  but for the most part, you want them all in one area, right? [[03:54:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14096.62s)]
*  So you can connect them with high speed networking. [[03:54:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14099.58s)]
*  And so, Elon today has 200,000 H100s, [[03:55:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14102.86s)]
*  100,000 H100s, 100,000 H200s, right? [[03:55:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14106.62s)]
*  Meta, OpenAI, and Amazon all have on the scale of 100,000, [[03:55:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14109.900000000001s)]
*  a little bit less, but this year, right? [[03:55:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14115.78s)]
*  This year, people are building much more, right? [[03:55:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14118.5s)]
*  Anthropic and Amazon are building a cluster [[03:55:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14120.42s)]
*  of 400,000 tranium too, which is Amazon specific chip, [[03:55:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14122.26s)]
*  trying to get away from Nvidia, right? [[03:55:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14126.06s)]
*  Meta and OpenAI have scales for hundreds of thousands, [[03:55:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14130.14s)]
*  but by next year, you'll have like 500,000 [[03:55:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14133.5s)]
*  to 700,000 GPU clusters, and note those GPUs [[03:55:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14136.14s)]
*  are much higher power consumption than existing ones, right? [[03:55:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14139.5s)]
*  Hopper's 700 watts, Blackwell goes to 1200 watts, right? [[03:55:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14141.9s)]
*  So the power per chip is growing [[03:55:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14145.7s)]
*  and the number of chips is growing, right? [[03:55:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14147.78s)]
*  Nuts, you think Elon said he'll get to a million, [[03:55:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14150.54s)]
*  you think that's actually feasible? [[03:55:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14154.060000000001s)]
*  I mean, I don't doubt Elon, right? [[03:55:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14156.42s)]
*  The filings that he has for like the power plant [[03:55:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14159.1s)]
*  and the Tesla battery packs, [[03:56:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14161.74s)]
*  it's clear he has some crazy plans for Memphis. [[03:56:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14163.18s)]
*  Like permits and stuff is open record, right? [[03:56:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14165.78s)]
*  But it's not quite clear that what and what [[03:56:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14168.58s)]
*  the time scales are, I just never doubt Elon, right? [[03:56:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14172.060000000001s)]
*  He's gonna surprise us. [[03:56:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14175.5s)]
*  So what's the idea with these clusters? [[03:56:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14176.54s)]
*  If you have a million GPUs, what percentage [[03:56:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14177.9s)]
*  in let's say two, three years is used for training [[03:56:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14181.58s)]
*  and what percent pre-training and what percent is used [[03:56:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14186.699999999999s)]
*  for like, for the actual computation? [[03:56:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14189.02s)]
*  So these mega clusters make no sense for inference, right? [[03:56:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14190.9s)]
*  You could route inference there and just not train, [[03:56:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14194.18s)]
*  but most of the inference capacity is being, [[03:56:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14197.22s)]
*  hey, I've got a 30 megawatt data center here, [[03:56:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14199.699999999999s)]
*  I've got 50 megawatts here, I've got 100 here, whatever, [[03:56:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14201.699999999999s)]
*  I'll just throw inference in all of those [[03:56:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14203.66s)]
*  because the mega clusters, right, [[03:56:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14206.02s)]
*  multi gigawatt data centers, I wanna train there [[03:56:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14207.98s)]
*  because that's where all of my GPUs are co-located [[03:56:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14210.74s)]
*  where I can put them at a super high networking speed [[03:56:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14212.82s)]
*  connected together, right? [[03:56:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14215.54s)]
*  Because that's what you need for training. [[03:56:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14216.78s)]
*  Now with pre-training, this is the old scale, right? [[03:56:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14218.300000000001s)]
*  You could, you would increase parameters, [[03:57:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14220.78s)]
*  you'd increase data, model gets better. [[03:57:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14222.24s)]
*  That doesn't apply anymore because there's not much more data [[03:57:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14225.220000000001s)]
*  in the pre-training side, right? [[03:57:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14228.26s)]
*  Yes, there's video and audio and image [[03:57:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14230.26s)]
*  that has not been fully taken advantage of. [[03:57:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14231.880000000001s)]
*  So there's a lot more scaling, but a lot of people [[03:57:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14233.94s)]
*  have taken transcripts of YouTube videos [[03:57:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14236.46s)]
*  and that gets you a lot of the data. [[03:57:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14238.98s)]
*  It doesn't get you all the learning value [[03:57:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14240.18s)]
*  out of the video and image data, [[03:57:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14241.56s)]
*  but there's still scaling to be done on pre-training, [[03:57:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14242.82s)]
*  but this post-training world [[03:57:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14246.46s)]
*  is where all the flops are gonna be spent, right? [[03:57:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14247.9s)]
*  The model's gonna play with itself, it's gonna self-play, [[03:57:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14249.98s)]
*  it's gonna do verifiable tasks, [[03:57:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14252.06s)]
*  it's gonna do computer use in sandboxes, [[03:57:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14253.46s)]
*  it might even do like simulated robotics things, right? [[03:57:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14256.1s)]
*  Like all of these things are gonna be environments [[03:57:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14258.84s)]
*  where compute is spent in quote unquote post-training, [[03:57:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14261.5s)]
*  but I think it's gonna be good. [[03:57:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14264.740000000002s)]
*  We're gonna drop the post from post-training. [[03:57:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14266.900000000001s)]
*  Yeah, wow. It's gonna be pre-training [[03:57:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14268.900000000001s)]
*  and it's gonna be training, I think. [[03:57:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14269.980000000001s)]
*  We're trying to be king. [[03:57:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14271.140000000001s)]
*  At some point. [[03:57:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14272.5s)]
*  Because for the bulk of the last few years, [[03:57:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14273.900000000001s)]
*  pre-training has dwarfed post-training, [[03:57:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14277.42s)]
*  but with these verifiable methods, [[03:58:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14280.16s)]
*  especially ones that scale really potentially infinitely, [[03:58:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14281.740000000002s)]
*  like computer use and robotics, not just math and coding, [[03:58:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14285.5s)]
*  right, where you can verify what's happening, [[03:58:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14287.94s)]
*  those infinitely verifiable tasks, [[03:58:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14289.58s)]
*  it seems you can spend as much compute as you want on them. [[03:58:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14291.320000000002s)]
*  Especially at the context length increase, [[03:58:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14293.46s)]
*  because the end of pre-training [[03:58:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14295.019999999999s)]
*  is when you increase the context length for these models. [[03:58:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14296.58s)]
*  And we've talked earlier in the conversation [[03:58:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14299.119999999999s)]
*  about how the context length, [[03:58:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14301.5s)]
*  when you have a long input, [[03:58:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14303.019999999999s)]
*  is much easier to manage than output. [[03:58:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14304.3s)]
*  And a lot of these post-training and reasoning techniques [[03:58:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14306.019999999999s)]
*  rely on a ton of sampling [[03:58:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14308.119999999999s)]
*  and it's becoming increasingly long context. [[03:58:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14310.619999999999s)]
*  So there's just like, [[03:58:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14312.98s)]
*  effectively your compute efficiency goes down. [[03:58:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14314.5s)]
*  I don't think flops is the standard for how you measure it, [[03:58:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14316.939999999999s)]
*  but with RL and you have to do all these things [[03:58:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14320.919999999998s)]
*  where you move your weights around in a different way [[03:58:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14323.44s)]
*  than at pre-training and just generation, [[03:58:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14326.42s)]
*  it's going to become less efficient [[03:58:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14329.4s)]
*  and flops is gonna be less of a useful term. [[03:58:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14331.54s)]
*  And then as the infrastructure gets better, [[03:58:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14333.84s)]
*  it's probably gonna go back to flops. [[03:58:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14335.28s)]
*  So all of the things we've been talking about [[03:58:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14337.04s)]
*  is most likely going to be Nvidia, right? [[03:58:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14339.12s)]
*  Is there any competitors? [[03:59:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14341.980000000001s)]
*  Google, I kind of ignored them. [[03:59:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14343.44s)]
*  Yeah, what's the story with TPU? [[03:59:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14345.44s)]
*  Like what's the... [[03:59:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14349.4s)]
*  TPU is awesome, right? [[03:59:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14350.640000000001s)]
*  It's great. [[03:59:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14352.18s)]
*  Google is, they're a bit more tepid [[03:59:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14353.92s)]
*  on building data centers for some reason. [[03:59:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14356.0s)]
*  They're building big data centers, don't get me wrong. [[03:59:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14357.6s)]
*  And they actually have the biggest cluster. [[03:59:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14359.36s)]
*  Let me, I was talking about Nvidia clusters. [[03:59:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14361.24s)]
*  They actually have the biggest cluster, period. [[03:59:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14363.44s)]
*  But the way they do it is like very interesting, right? [[03:59:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14365.92s)]
*  They have two sort of like data center super regions, right? [[03:59:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14368.88s)]
*  In that the data center isn't physically like, [[03:59:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14372.4s)]
*  all of the GPUs aren't physically on one site, [[03:59:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14374.52s)]
*  but they're like 30 miles from each other. [[03:59:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14376.44s)]
*  And not GPUs, TPUs, right? [[03:59:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14377.68s)]
*  They have like in Iowa and Nebraska, [[03:59:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14379.0s)]
*  they have four data centers [[03:59:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14381.36s)]
*  that are just like right next to each other. [[03:59:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14382.44s)]
*  Why doesn't Google flex its cluster size? [[03:59:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14384.28s)]
*  Go to multi data center training. [[03:59:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14388.2s)]
*  It's the good images in there. [[03:59:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14389.720000000001s)]
*  So I'll show you what I mean. [[03:59:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14390.76s)]
*  It's just a semi-analysis multi data center. [[03:59:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14391.84s)]
*  So this is like, you know, [[03:59:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14394.880000000001s)]
*  so this is an image of like what a standard [[03:59:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14395.880000000001s)]
*  Google data center looks like. [[03:59:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14397.560000000001s)]
*  By the way, their data centers look very different [[03:59:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14398.94s)]
*  than anyone else's data centers. [[04:00:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14400.6s)]
*  What are we looking at here? [[04:00:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14401.800000000001s)]
*  So these are, yeah, so if you see this image, right? [[04:00:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14403.12s)]
*  In the center, there are these big rectangular boxes, right? [[04:00:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14405.76s)]
*  Those are where the actual chips are kept. [[04:00:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14408.44s)]
*  And then if you scroll down a little bit further, [[04:00:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14410.58s)]
*  you can see there's like these water pipes, [[04:00:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14413.38s)]
*  there's these chiller cooling towers in the top [[04:00:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14415.98s)]
*  and a bunch of like diesel generators. [[04:00:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14418.539999999999s)]
*  The diesel generators are backup power. [[04:00:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14420.1s)]
*  The data center itself is like, [[04:00:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14422.34s)]
*  look physically smaller than the water chillers, right? [[04:00:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14424.46s)]
*  So the chips are actually easier to like keep together, [[04:00:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14426.94s)]
*  but then like cooling all the water [[04:00:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14429.78s)]
*  for the water cooling is very difficult, right? [[04:00:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14431.12s)]
*  So Google has like a very advanced infrastructure [[04:00:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14433.26s)]
*  that no one else has for the TPU. [[04:00:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14435.42s)]
*  And what they do is they've like stamped [[04:00:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14438.06s)]
*  they've stamped a bunch of these data centers out [[04:00:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14440.58s)]
*  in a few regions, right? [[04:00:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14442.380000000001s)]
*  So if you go a little bit further down, [[04:00:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14443.54s)]
*  this is a Microsoft, this is in Arizona. [[04:00:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14446.62s)]
*  This is where GPT-5 quote unquote will be trained. [[04:00:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14448.460000000001s)]
*  If it doesn't exist already. [[04:00:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14452.960000000001s)]
*  Yeah, if it doesn't exist already. [[04:00:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14454.380000000001s)]
*  But each of these data centers, right? [[04:00:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14456.26s)]
*  I've shown a couple images of them. [[04:00:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14457.66s)]
*  They're like really closely co-located [[04:00:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14459.18s)]
*  in the same region, right? [[04:01:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14460.980000000001s)]
*  Nebraska, Iowa. [[04:01:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14462.26s)]
*  And then they also have a similar one [[04:01:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14463.1s)]
*  in Ohio complex, right? [[04:01:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14464.26s)]
*  And so these data centers are really close to each other. [[04:01:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14466.54s)]
*  And what they've done is they've connected them [[04:01:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14469.98s)]
*  super high bandwidth with fiber. [[04:01:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14471.06s)]
*  And so these are just a bunch of data centers. [[04:01:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14473.14s)]
*  And the point here is that Google [[04:01:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14475.0s)]
*  has a very advanced infrastructure, [[04:01:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14477.14s)]
*  very tightly connected in a small region. [[04:01:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14479.699999999999s)]
*  So Elon will always have the biggest cluster [[04:01:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14481.859999999999s)]
*  fully connected, right? [[04:01:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14483.82s)]
*  Because it's all in one building, right? [[04:01:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14485.019999999999s)]
*  And he's completely right on that, right? [[04:01:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14486.9s)]
*  Google has the biggest cluster, [[04:01:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14488.58s)]
*  but you have to spread over three sites [[04:01:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14490.619999999999s)]
*  and by a significant margin, [[04:01:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14491.98s)]
*  but you have to go across multiple sites. [[04:01:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14493.859999999999s)]
*  Why doesn't Google compete with Nvidia? [[04:01:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14495.46s)]
*  Why don't they sell TPUs? [[04:01:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14499.14s)]
*  I think there's a couple problems with it. [[04:01:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14501.82s)]
*  It's like one, TPU has been a form of allowing search [[04:01:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14503.779999999999s)]
*  to be really freaking cheap [[04:01:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14509.539999999999s)]
*  and build models for that, right? [[04:01:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14511.619999999999s)]
*  And so like a big chunk of the search, [[04:01:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14513.34s)]
*  GPU purchases or TPU purchases [[04:01:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14515.5s)]
*  or big chunk of Google's purchases and usage, [[04:01:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14517.14s)]
*  all of it is for internal workloads, right? [[04:02:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14520.58s)]
*  Whether it be search, now Gemini, right? [[04:02:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14522.42s)]
*  YouTube, all these different applications [[04:02:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14525.18s)]
*  that they have, ads, [[04:02:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14527.78s)]
*  these are where all their TPUs are being spent [[04:02:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14530.34s)]
*  and that's what they're hyper-focused on, right? [[04:02:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14532.060000000001s)]
*  And so there's certain aspects of the architecture [[04:02:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14534.34s)]
*  that are optimized for their use case [[04:02:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14537.060000000001s)]
*  that are not optimized elsewhere, right? [[04:02:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14538.7s)]
*  One simple one is like they've open-sourced a Gemma model [[04:02:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14541.18s)]
*  and they called it Gemma 7B, right? [[04:02:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14543.900000000001s)]
*  But then it's actually 8 billion parameters [[04:02:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14546.220000000001s)]
*  because the vocabulary is so large. [[04:02:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14547.820000000002s)]
*  And the reason they made the vocabulary so large [[04:02:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14550.26s)]
*  is because TPUs, like matrix multiply unit is massive [[04:02:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14552.060000000001s)]
*  because that's what they've like sort of optimized for. [[04:02:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14555.94s)]
*  And so they decided, [[04:02:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14557.859999999999s)]
*  oh, well, I'll just make the vocabulary large too, [[04:02:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14558.699999999999s)]
*  even though it makes no sense to do so [[04:02:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14560.539999999999s)]
*  in such a small model because that fits on their hardware. [[04:02:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14561.859999999999s)]
*  So Gemma doesn't run as efficiently on a GPU [[04:02:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14564.3s)]
*  as a Llama does, right? [[04:02:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14566.66s)]
*  But vice versa, Llama doesn't run as efficiently [[04:02:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14567.9s)]
*  on a TPU as a Gemma does, right? [[04:02:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14570.26s)]
*  And it's still like, there's like certain like aspects [[04:02:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14572.5s)]
*  of like hardware software co-design. [[04:02:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14574.34s)]
*  So all their search models are their ranking [[04:02:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14575.9s)]
*  and recommendation models, [[04:02:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14577.699999999999s)]
*  all these different models that are AI, [[04:02:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14579.06s)]
*  but not like Gen. AI, right? [[04:03:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14580.699999999999s)]
*  Have been hyper-optimized with TPUs forever. [[04:03:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14582.619999999999s)]
*  The software stack is super optimized, [[04:03:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14585.26s)]
*  but all of this software stack [[04:03:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14586.94s)]
*  has not been released publicly at all, right? [[04:03:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14588.7s)]
*  Very small portions of it, [[04:03:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14591.54s)]
*  Jax and XLA have been, [[04:03:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14592.54s)]
*  but like the experience when you're inside of Google [[04:03:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14593.94s)]
*  and you're training on TPUs as a researcher, [[04:03:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14596.5s)]
*  you don't need to know anything [[04:03:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14598.66s)]
*  about the hardware in many cases, right? [[04:03:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14599.5s)]
*  Like it's like pretty beautiful. [[04:03:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14601.14s)]
*  But as soon as you step outside, [[04:03:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14602.7s)]
*  they'll get all go, a lot of them go back. [[04:03:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14604.62s)]
*  They leave Google and then they go back. [[04:03:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14606.86s)]
*  Yeah. Yeah. [[04:03:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14608.5s)]
*  They're like, they leave and they start a company [[04:03:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14609.34s)]
*  because they have all these amazing research ideas. [[04:03:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14611.22s)]
*  And they're like, wait, infrastructure is hard, [[04:03:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14612.58s)]
*  software is hard and this is on GPUs. [[04:03:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14614.78s)]
*  Or if they try to use TPUs, same thing, [[04:03:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14616.62s)]
*  cause they don't have access to all this code. [[04:03:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14617.980000000001s)]
*  And so it's like, how do you convince a company [[04:03:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14619.820000000002s)]
*  whose golden goose is search [[04:03:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14621.820000000002s)]
*  where they're making hundreds of billions of dollars from [[04:03:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14623.220000000001s)]
*  to start selling GPU or TPUs, [[04:03:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14625.820000000002s)]
*  which they used to only buy a couple billion of, [[04:03:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14628.460000000001s)]
*  I think in 2023, they bought like a couple billion. [[04:03:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14630.980000000001s)]
*  And now they're buying like 10 billion [[04:03:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14635.62s)]
*  to $15 billion worth. [[04:03:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14636.86s)]
*  But how do you convince them [[04:03:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14638.18s)]
*  that they should just buy like twice as many [[04:03:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14639.060000000001s)]
*  and figure out how to sell them and make $30 billion? [[04:04:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14641.1s)]
*  Like who cares about making $30 billion? [[04:04:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14643.3s)]
*  Won't that 30 billion exceed [[04:04:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14645.34s)]
*  actually the search profit eventually? [[04:04:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14647.98s)]
*  Oh, I mean like, you're always gonna make more money [[04:04:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14650.259999999998s)]
*  on services than- [[04:04:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14652.859999999999s)]
*  Always. [[04:04:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14654.619999999999s)]
*  I mean like, yeah, like to be clear, [[04:04:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14655.699999999999s)]
*  like today people are spending a lot more on hardware [[04:04:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14657.619999999999s)]
*  than they are the services, right? [[04:04:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14659.939999999999s)]
*  Because the hardware front runs the service spend. [[04:04:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14661.74s)]
*  But like- [[04:04:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14665.34s)]
*  You're investing, yeah. [[04:04:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14666.179999999998s)]
*  If there's no revenue for AI stuff or not enough revenue, [[04:04:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14667.019999999999s)]
*  then obviously like it's gonna blow up, right? [[04:04:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14669.699999999999s)]
*  People won't continue to spend on GPUs forever. [[04:04:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14671.98s)]
*  And Nvidia is trying to move up the stack [[04:04:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14674.46s)]
*  with like software that they're trying to sell [[04:04:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14676.1s)]
*  and license and stuff, right? [[04:04:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14677.5s)]
*  But Google has never had that like DNA of like, [[04:04:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14678.859999999999s)]
*  this is a product we should sell, right? [[04:04:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14682.26s)]
*  They don't actually, the Google Cloud does it, [[04:04:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14683.939999999999s)]
*  which is a separate organization from the TPU team, [[04:04:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14686.1s)]
*  which is a separate organization from the DeepMind team, [[04:04:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14687.939999999999s)]
*  which is a separate organization from the search team, [[04:04:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14689.859999999999s)]
*  right, there's a lot of bureaucracy here. [[04:04:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14691.619999999999s)]
*  Wait, Google Cloud is a separate team than the TPU team? [[04:04:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14692.619999999999s)]
*  Technically TPU sits under infrastructure, [[04:04:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14695.539999999999s)]
*  which sits under Google Cloud. [[04:04:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14698.46s)]
*  But like Google Cloud like for like renting stuff [[04:05:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14700.06s)]
*  and TPU architecture are very different goals, right? [[04:05:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14704.06s)]
*  In hardware and software, like all of this, right? [[04:05:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14707.38s)]
*  Like the JaxXLA teams do not serve [[04:05:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14709.779999999999s)]
*  Google's customers externally. [[04:05:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14712.22s)]
*  Whereas Nvidia's various CUDA teams [[04:05:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14714.019999999999s)]
*  for like things like Nickel serve external customers, right? [[04:05:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14715.66s)]
*  The internal teams like JaxXLA and stuff, [[04:05:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14719.98s)]
*  they more so serve DeepMind and search, right? [[04:05:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14722.019999999999s)]
*  And so their customer is different, [[04:05:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14724.58s)]
*  they're not building a product for them. [[04:05:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14725.74s)]
*  Do you understand why AWS keeps winning [[04:05:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14727.34s)]
*  versus Azure for cloud versus Google Cloud? [[04:05:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14730.380000000001s)]
*  Yeah, there's- Google Cloud is tiny, [[04:05:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14734.78s)]
*  isn't it relative to AWS? [[04:05:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14736.5s)]
*  Google Cloud is third, yeah. [[04:05:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14737.58s)]
*  Microsoft is the second biggest, [[04:05:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14740.06s)]
*  but Amazon is the biggest, right? [[04:05:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14741.3s)]
*  Yeah. [[04:05:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14742.66s)]
*  And Microsoft deceptively sort of includes [[04:05:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14743.5s)]
*  like Microsoft Office 365 and things like that, [[04:05:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14745.7s)]
*  like some of these enterprise-wide licenses. [[04:05:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14748.1s)]
*  So in reality, the Gulf is even larger. [[04:05:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14750.02s)]
*  Microsoft is still second though, right? [[04:05:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14751.94s)]
*  Amazon is way bigger, why? [[04:05:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14753.94s)]
*  Because using AWS is better and easier. [[04:05:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14755.26s)]
*  And in many cases, it's cheaper. [[04:05:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14757.94s)]
*  And it's first, yeah. [[04:05:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14759.42s)]
*  It was first. [[04:06:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14760.26s)]
*  Yeah, but there's a lot of things that are first that- [[04:06:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14761.1s)]
*  Well, it's easier, it's harder to switch than it is to- [[04:06:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14763.02s)]
*  Yeah, okay. [[04:06:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14765.42s)]
*  But AWS is their core- [[04:06:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14766.26s)]
*  There's big fees for switching too. [[04:06:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14767.78s)]
*  AWS generates over 80% of Amazon's profit, [[04:06:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14769.14s)]
*  I think over 90%. [[04:06:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14771.9s)]
*  That's insane. [[04:06:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14773.02s)]
*  The distribution centers are just like, [[04:06:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14773.86s)]
*  one day we'll decide to make money from this. [[04:06:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14775.42s)]
*  But they haven't yet, right? [[04:06:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14778.02s)]
*  Like they make tiny little profit from it. [[04:06:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14779.06s)]
*  Yeah, one day of Amazon Prime will triple in price. [[04:06:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14780.26s)]
*  You would think they would improve AWS interface, [[04:06:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14782.62s)]
*  because it's horrible. [[04:06:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14786.900000000001s)]
*  It's clunky, but everybody is- [[04:06:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14788.42s)]
*  I don't, yeah. [[04:06:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14790.78s)]
*  You one would think. [[04:06:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14792.380000000001s)]
*  I think actually Google's interface is sometimes nice, [[04:06:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14793.740000000002s)]
*  but it's also like, [[04:06:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14796.060000000001s)]
*  they don't care about anyone besides their top customers. [[04:06:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14796.900000000001s)]
*  Exactly. [[04:06:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14798.78s)]
*  And their customer service sucks [[04:06:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14799.62s)]
*  and they have a lot less. [[04:06:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14800.7s)]
*  I mean, all these companies, [[04:06:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14802.140000000001s)]
*  they optimize for the big customers, yeah. [[04:06:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14803.060000000001s)]
*  It's supposed to be for business. [[04:06:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14805.26s)]
*  Amazon has always optimized [[04:06:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14807.02s)]
*  for the small customer too though, right? [[04:06:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14808.42s)]
*  Obviously they optimize a lot for the big customer, [[04:06:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14809.740000000002s)]
*  but when they started, [[04:06:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14811.42s)]
*  they just would go to random Bay Area things [[04:06:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14812.98s)]
*  and give out credits, right? [[04:06:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14815.1s)]
*  And then they like, [[04:06:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14816.7s)]
*  or just put in your credit card and use us, right? [[04:06:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14817.54s)]
*  Like back in the early days. [[04:06:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14819.06s)]
*  So they've always, [[04:07:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14820.38s)]
*  the business has grown with them, right? [[04:07:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14821.22s)]
*  And Virgin. [[04:07:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14822.58s)]
*  So why does Amazon, [[04:07:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14823.42s)]
*  why is Snowflake all over Amazon? [[04:07:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14824.78s)]
*  Because Snowflake in the beginning, [[04:07:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14826.26s)]
*  when Amazon didn't care about them, [[04:07:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14827.34s)]
*  was still using Amazon, right? [[04:07:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14828.98s)]
*  And then of course, [[04:07:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14830.18s)]
*  one day Snowflake and Amazon has a super huge partnership. [[04:07:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14831.02s)]
*  But this is the case, [[04:07:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14833.1s)]
*  Amazon's user experience and quality is better. [[04:07:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14834.58s)]
*  Also a lot of the silicon they've engineered [[04:07:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14837.26s)]
*  makes them have a lower cost structure [[04:07:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14839.34s)]
*  in traditional cloud storage, [[04:07:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14840.94s)]
*  CPU networking, that kind of stuff. [[04:07:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14842.220000000001s)]
*  Then in databases, right? [[04:07:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14844.78s)]
*  Like, you know, I think like four [[04:07:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14847.1s)]
*  of Amazon's top five revenue products, [[04:07:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14849.02s)]
*  margin products are like gross profit products [[04:07:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14852.02s)]
*  or all database related products [[04:07:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14854.1s)]
*  like Redshift and like all these things, right? [[04:07:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14855.42s)]
*  Like, so Amazon has a very like good silicon [[04:07:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14857.5s)]
*  to a user experience, [[04:07:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14862.220000000001s)]
*  like entire pipeline with AWS. [[04:07:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14863.460000000001s)]
*  I think Google, [[04:07:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14865.18s)]
*  their silicon teams, [[04:07:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14866.42s)]
*  yeah, they have awesome silicon internally, [[04:07:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14868.060000000001s)]
*  TPU, the YouTube chip, [[04:07:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14869.5s)]
*  you know, some of these other chips that they've made. [[04:07:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14871.86s)]
*  And the problem is they're not serving external customers, [[04:07:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14873.7s)]
*  they're serving internal customers, right? [[04:07:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14876.94s)]
*  I mean, NVIDIA's entire culture is designed [[04:07:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14878.7s)]
*  from the bottom up to do this. [[04:08:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14880.54s)]
*  There's this recent book, [[04:08:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14881.74s)]
*  The NVIDIA Way by Tae Kim that details this [[04:08:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14882.7s)]
*  and how they look for future opportunities [[04:08:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14885.58s)]
*  and ready their CUDA software libraries to make it [[04:08:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14888.5s)]
*  so that new applications of high performance computing [[04:08:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14892.42s)]
*  can very rapidly be evolved on CUDA and NVIDIA chips. [[04:08:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14895.7s)]
*  And that is entirely different than Google [[04:08:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14899.62s)]
*  as a services business. [[04:08:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14902.42s)]
*  Yeah, I mean, NVIDIA, [[04:08:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14904.1s)]
*  it should be said as a truly special company. [[04:08:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14906.220000000001s)]
*  Like, I mean, they, the whole, [[04:08:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14908.300000000001s)]
*  the culture of everything, [[04:08:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14910.26s)]
*  they're really optimized for that kind of thing. [[04:08:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14911.220000000001s)]
*  Speaking of which, [[04:08:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14912.980000000001s)]
*  is there somebody that can even challenge NVIDIA [[04:08:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14913.820000000002s)]
*  hardware wise, Intel, AMD? [[04:08:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14916.1s)]
*  I really don't think so. [[04:08:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14919.5s)]
*  We went through like a very long process [[04:08:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14921.1s)]
*  working with AMD on training on their GPUs [[04:08:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14924.699999999999s)]
*  and friends and stuff. [[04:08:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14927.38s)]
*  And they're decent. [[04:08:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14928.22s)]
*  Their hardware is better in many ways than NVIDIA's. [[04:08:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14929.34s)]
*  The problem is their software is really bad. [[04:08:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14932.58s)]
*  And I think they're getting better, right? [[04:08:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14934.34s)]
*  They're getting better faster, [[04:08:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14936.26s)]
*  but they're just, the Gulf is so large [[04:08:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14937.18s)]
*  and like they don't spend enough resources on it [[04:09:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14940.22s)]
*  or have it historically, right? [[04:09:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14942.46s)]
*  Maybe they're changing their tune now, [[04:09:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14943.699999999999s)]
*  but for multiple months, [[04:09:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14945.14s)]
*  we were submitting those bugs, right? [[04:09:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14947.619999999999s)]
*  Like us, semi analysis, right? [[04:09:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14949.1s)]
*  Like, what the fuck? [[04:09:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14951.18s)]
*  Why are we submitting those bugs, right? [[04:09:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14952.02s)]
*  Cause they only cared about their like biggest customers. [[04:09:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14954.26s)]
*  And so they'd ship them a private image, blah, blah, blah. [[04:09:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14957.34s)]
*  And it's like, okay, but like, [[04:09:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14959.460000000001s)]
*  I am just using PyTorch [[04:09:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14961.26s)]
*  and I wanna use the publicly available libraries. [[04:09:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14963.26s)]
*  You don't care about that, right? [[04:09:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14965.78s)]
*  So they're getting better, [[04:09:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14966.74s)]
*  but like, I think AMD is not possible. [[04:09:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14968.66s)]
*  Intel's obviously in dire straits right now [[04:09:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14970.42s)]
*  and needs to be saved somehow. [[04:09:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14972.7s)]
*  Very important for national security, [[04:09:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14974.94s)]
*  for American technology elements. [[04:09:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14976.66s)]
*  Can you explain the obviously, [[04:09:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14979.060000000001s)]
*  so why are they in dire straits? [[04:09:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14980.140000000001s)]
*  Going back to earlier, only three companies can R&D, right? [[04:09:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14981.74s)]
*  Taiwan, Tsingchu, Samsung, Pyeongyang, [[04:09:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14985.539999999999s)]
*  and then Intel Hillsboro. [[04:09:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14989.18s)]
*  Samsung's doing horribly, Intel's doing horribly. [[04:09:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14990.98s)]
*  We could be in a world where there's only one company [[04:09:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14993.34s)]
*  that can do R&D and that one company [[04:09:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14995.1s)]
*  already manufactures most of the chips. [[04:09:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14996.699999999999s)]
*  They've been gaining market share anyways, [[04:09:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14997.94s)]
*  but like that's a critical thing, right? [[04:09:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=14999.18s)]
*  So what happens to Taiwan means [[04:10:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15001.5s)]
*  the rest of the world's semiconductor industry [[04:10:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15002.94s)]
*  and therefore tech relies on Taiwan, right? [[04:10:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15004.34s)]
*  And that's obviously precarious. [[04:10:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15006.9s)]
*  As far as like Intel, [[04:10:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15009.18s)]
*  they've been slowly, steadily declining. [[04:10:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15010.74s)]
*  They were on top of servers and PCs, [[04:10:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15013.22s)]
*  but now Apple's done the M1 [[04:10:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15015.78s)]
*  and Nvidia's releasing a PC chip [[04:10:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15017.86s)]
*  and Qualcomm's releasing a PC chip. [[04:10:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15019.46s)]
*  And in servers, hyperscalers are all making [[04:10:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15021.14s)]
*  their own ARM-based server chips [[04:10:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15023.3s)]
*  and Intel has no AI silicon like wins, right? [[04:10:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15025.619999999999s)]
*  They have very small wins. [[04:10:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15029.18s)]
*  And they never got into mobile [[04:10:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15031.3s)]
*  because they said no to the iPhone [[04:10:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15032.78s)]
*  and like all these things have compounded [[04:10:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15034.06s)]
*  and they've lost their process technology leadership, right? [[04:10:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15035.699999999999s)]
*  They were ahead for 20 years [[04:10:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15037.86s)]
*  and now they're behind by at least a couple years, right? [[04:10:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15038.86s)]
*  And they're trying to catch back up [[04:10:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15041.42s)]
*  and we'll see if like their 18A, 14A strategy works out [[04:10:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15042.58s)]
*  where they try and leapfrog TSMC, [[04:10:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15046.1s)]
*  but like, and Intel is just like [[04:10:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15048.380000000001s)]
*  losing tons of money anyways, right? [[04:10:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15050.220000000001s)]
*  And they just fired their CEO, [[04:10:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15051.74s)]
*  even though the CEO was the only person [[04:10:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15053.060000000001s)]
*  who understood the company well, right? [[04:10:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15054.900000000001s)]
*  We'll see. [[04:10:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15056.58s)]
*  He was not the best, [[04:10:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15057.42s)]
*  but he was pretty good relatively, technical guy. [[04:10:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15058.24s)]
*  Where does Intel make most of its money? [[04:11:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15061.380000000001s)]
*  The CPUs still, right? [[04:11:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15063.1s)]
*  PCs and data center CPUs, yeah, [[04:11:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15063.94s)]
*  but data center CPUs are all going cloud [[04:11:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15065.5s)]
*  and Amazon, Microsoft, Google are making ARM based CPUs. [[04:11:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15067.34s)]
*  And then PC side, AMD's gained market share, [[04:11:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15071.42s)]
*  Nvidia's launching a chip. [[04:11:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15074.98s)]
*  That's not gonna be a success, right? [[04:11:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15076.14s)]
*  Media tech, Qualcomm ever launched chips. [[04:11:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15077.3s)]
*  Apple's doing well, right? [[04:11:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15079.22s)]
*  Like they could get squeezed a little bit in PC, [[04:11:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15080.58s)]
*  although PC generally, I imagine, [[04:11:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15083.34s)]
*  will just stick Intel mostly for Windows side. [[04:11:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15084.74s)]
*  Let's talk about the broad AI race. [[04:11:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15087.1s)]
*  Who do you think wins? [[04:11:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15089.14s)]
*  We talked about Google. [[04:11:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15090.94s)]
*  The leader, the default leader has been Google [[04:11:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15092.06s)]
*  because of their infrastructure advantage. [[04:11:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15094.94s)]
*  Well, like in the news, OpenAI is the leader. [[04:11:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15097.58s)]
*  They're the leading in the narrative. [[04:11:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15100.82s)]
*  They have the best model. [[04:11:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15102.66s)]
*  They have the best model that people can use [[04:11:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15103.5s)]
*  and they're experts. [[04:11:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15105.14s)]
*  And they have the most AI revenue. [[04:11:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15106.98s)]
*  Yeah, OpenAI is winning. [[04:11:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15108.56s)]
*  So who's making money on AI right now? [[04:11:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15111.06s)]
*  Is anyone making money? [[04:11:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15113.74s)]
*  So accounting profit wise, [[04:11:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15115.039999999999s)]
*  Microsoft is making money, [[04:11:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15116.539999999999s)]
*  but they're spending a lot of CapEx, right? [[04:11:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15117.859999999999s)]
*  And that gets depreciated over years. [[04:11:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15119.98s)]
*  Meta is making tons of money, [[04:12:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15122.26s)]
*  but with recommendation systems, which is AI, [[04:12:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15123.86s)]
*  but not with Lama, right? [[04:12:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15126.380000000001s)]
*  Lama's losing money for sure, right? [[04:12:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15127.380000000001s)]
*  I think Anthropic and OpenAI are obviously not making money [[04:12:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15130.34s)]
*  because otherwise they wouldn't be raising money, right? [[04:12:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15133.060000000001s)]
*  They have to raise money to build more, right? [[04:12:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15135.18s)]
*  Although theoretically they are making money, right? [[04:12:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15137.82s)]
*  Like, you spent a few hundred million dollars on GPT-4 [[04:12:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15139.94s)]
*  and it's doing billions in revenue. [[04:12:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15143.02s)]
*  So like, obviously it's like making money. [[04:12:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15144.380000000001s)]
*  Although they had to continue to research [[04:12:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15146.220000000001s)]
*  to get the compute efficiency wins, right? [[04:12:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15147.640000000001s)]
*  And move down the curve to like, you know, [[04:12:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15149.54s)]
*  get that 1200X that has been achieved for GPT-3, you know, [[04:12:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15152.5s)]
*  maybe we're only at like a couple hundred X now, [[04:12:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15156.02s)]
*  but you know, with GPT-4 turbo and 4.0 [[04:12:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15158.54s)]
*  and there'll be another one probably cheaper [[04:12:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15160.74s)]
*  than GPT-4.0 even that comes out at some point. [[04:12:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15162.62s)]
*  And that research costs a lot of money. [[04:12:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15165.66s)]
*  Yep, exactly. [[04:12:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15168.18s)]
*  That's the thing that I guess is not talked about [[04:12:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15169.18s)]
*  with the cost, that when you're referring [[04:12:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15171.02s)]
*  to the cost of the model, [[04:12:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15174.18s)]
*  it's not just the training or the test runs, [[04:12:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15175.86s)]
*  it's the actual research, the manpower. [[04:12:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15179.1s)]
*  Yeah, to do things like reasoning, right? [[04:13:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15181.98s)]
*  Now that that exists, they're gonna scale it, [[04:13:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15183.42s)]
*  they're gonna do a lot of research still. [[04:13:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15184.859999999999s)]
*  I think the, you know, people focus on the payback question, [[04:13:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15186.02s)]
*  but it's really easy to like, just be like, well, like, [[04:13:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15190.14s)]
*  you know, GDP is humans and industrial capital, right? [[04:13:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15192.58s)]
*  And if you can make intelligence cheap, [[04:13:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15196.02s)]
*  then you can grow a lot, right? [[04:13:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15198.18s)]
*  That's the sort of dumb way to explain it. [[04:13:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15200.14s)]
*  But that's sort of what basically the investment thesis is. [[04:13:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15202.1s)]
*  I think only Nvidia is actually making tons of money [[04:13:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15205.6s)]
*  and other hardware vendors. [[04:13:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15208.1s)]
*  The hyperscalers are all on paper making money, [[04:13:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15210.02s)]
*  but in reality, they're like spending a lot more [[04:13:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15212.7s)]
*  on purchasing the GPUs, which you don't know [[04:13:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15214.9s)]
*  if they're still gonna make this much money [[04:13:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15217.02s)]
*  on each GPU in two years, right? [[04:13:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15218.380000000001s)]
*  You don't know if, you know, all of a sudden, [[04:13:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15220.74s)]
*  OpenAI goes kapoof and now Microsoft has like [[04:13:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15224.18s)]
*  hundreds of thousands of GPUs they were renting to OpenAI [[04:13:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15227.34s)]
*  that they paid for themselves with their investment in them, [[04:13:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15230.300000000001s)]
*  you know, that no longer have a customer, right? [[04:13:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15234.1s)]
*  Like this is always a possibility. [[04:13:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15235.9s)]
*  I don't believe that, right? [[04:13:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15237.52s)]
*  I think, you know, OpenAI will keep raising money. [[04:13:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15238.92s)]
*  I think others will keep raising money [[04:14:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15240.88s)]
*  because the investments, the returns from it [[04:14:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15242.88s)]
*  are gonna be eventually huge once we have AGI. [[04:14:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15245.12s)]
*  So do you think multiple companies will get, [[04:14:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15248.0s)]
*  let's assume- I don't think it's winner take all. [[04:14:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15250.64s)]
*  Okay, so it's not, let's not call it AGI, whatever. [[04:14:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15252.8s)]
*  It's like a single day. [[04:14:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15256.64s)]
*  It's a gradual thing. Super powerful AI. [[04:14:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15257.68s)]
*  But it's a gradually increasing set of features [[04:14:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15260.52s)]
*  that are useful and make a lot of money. [[04:14:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15263.72s)]
*  Rapidly increasing set of features. [[04:14:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15265.64s)]
*  Rapidly increasing set of features. [[04:14:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15267.6s)]
*  So you're saying a lot of companies will be, [[04:14:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15270.6s)]
*  it just seems absurd that all of these companies [[04:14:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15273.4s)]
*  are building gigantic data centers. [[04:14:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15278.66s)]
*  There are companies that will benefit from AI, [[04:14:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15280.76s)]
*  but not because they train the best model. [[04:14:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15282.84s)]
*  Like Meta has so many avenues to benefit from AI [[04:14:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15284.84s)]
*  and all of their services. [[04:14:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15287.6s)]
*  People are there, people spend time on Meta's platforms, [[04:14:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15288.68s)]
*  and it's a way to make more money per user per hour. [[04:14:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15291.62s)]
*  Yeah, it seems like Google X slash XAI slash Tesla, [[04:14:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15294.24s)]
*  important to say, and then Meta will benefit not directly [[04:15:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15300.6s)]
*  from the AI, like the LLMs, but from the intelligence, [[04:15:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15303.86s)]
*  like the additional boost of intelligence [[04:15:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15309.88s)]
*  to the products they already sell. [[04:15:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15311.66s)]
*  So whether that's the recommendation system [[04:15:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15313.199999999999s)]
*  or for Elon, who's been talking about Optimus, the robot, [[04:15:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15315.52s)]
*  potentially the intelligence of the robot. [[04:15:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15319.36s)]
*  And then you have personalized robots in the home, [[04:15:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15322.4s)]
*  that kind of thing. [[04:15:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15324.8s)]
*  He thinks it's a 10 plus trillion dollar business, [[04:15:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15325.76s)]
*  which at some point maybe, not soon, [[04:15:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15330.52s)]
*  but who knows what robotics will use for. [[04:15:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15334.359999999999s)]
*  Let's do a TAM analysis, right? [[04:15:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15336.279999999999s)]
*  Eight billion humans and let's get eight billion robots, [[04:15:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15337.64s)]
*  right, and let's pay them the average salary. [[04:15:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15341.0s)]
*  And yeah, there we go, 10 trillion. [[04:15:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15343.4s)]
*  More than 10 trillion. [[04:15:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15345.199999999999s)]
*  Yeah, I mean, if there's robots everywhere, [[04:15:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15346.44s)]
*  why does it have to be just eight billion robots? [[04:15:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15349.56s)]
*  Yeah, yeah, of course, of course. [[04:15:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15352.68s)]
*  I'm gonna have like one robot, you're gonna have like 20. [[04:15:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15354.119999999999s)]
*  Yeah, I mean, I see a use case for that. [[04:15:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15357.24s)]
*  So yeah, so I guess the benefit would be [[04:15:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15359.8s)]
*  in the products they sell, which is why OpenAI [[04:16:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15362.16s)]
*  is in a trickier position, because they- [[04:16:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15364.84s)]
*  All of the value of OpenAI right now as a brand [[04:16:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15366.56s)]
*  is in ChatGPT. [[04:16:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15368.92s)]
*  And there is actually not that, for most users, [[04:16:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15370.039999999999s)]
*  there's not that much of a reason that they need OpenAI [[04:16:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15373.14s)]
*  to be spending billions and billions of dollars [[04:16:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15375.92s)]
*  on the next best model. [[04:16:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15377.939999999999s)]
*  So they could just go and do some research [[04:16:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15380.4s)]
*  on how to get the most out of OpenAI [[04:16:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15382.199999999999s)]
*  when they could just license llama five [[04:16:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15384.199999999999s)]
*  and for be way cheaper. [[04:16:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15386.6s)]
*  So that's kind of like ChatGPT [[04:16:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15387.76s)]
*  is an extremely valuable entity to them. [[04:16:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15389.8s)]
*  But they could make more money just off that. [[04:16:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15392.76s)]
*  The chat application is clearly like, [[04:16:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15395.74s)]
*  does not have tons of room to continue, right? [[04:16:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15397.92s)]
*  Like the standard chat, right, where you're just using it [[04:16:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15399.92s)]
*  for random questions and stuff, right? [[04:16:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15401.92s)]
*  The cost continues to collapse, [[04:16:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15403.76s)]
*  v3 is the latest one, biggest. [[04:16:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15405.0s)]
*  405B probably loses the money, but at some point, [[04:16:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15407.6s)]
*  they're going to get, the models are gonna get so cheap [[04:16:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15410.48s)]
*  that they can just serve them for free [[04:16:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15413.36s)]
*  with ad supported, right? [[04:16:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15415.28s)]
*  And that's what Google is gonna be able to do. [[04:16:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15416.28s)]
*  And that's obviously they've got a bigger reach, right? [[04:16:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15417.84s)]
*  So chat is not gonna be the only use case. [[04:16:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15419.5s)]
*  It's like these reasoning, code, agents, computer use, [[04:17:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15421.92s)]
*  all this stuff is where OpenAI has to actually go [[04:17:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15425.720000000001s)]
*  to make money in the future. [[04:17:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15428.0s)]
*  Otherwise they're kaputs. [[04:17:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15429.08s)]
*  But X, Google and Meta have these other products. [[04:17:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15430.32s)]
*  So doesn't, isn't it likely that OpenAI [[04:17:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15435.08s)]
*  and Anthropic disappear eventually? [[04:17:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15438.76s)]
*  Because this- [[04:17:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15442.2s)]
*  Unless they're so good at models, which they are. [[04:17:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15443.039999999999s)]
*  But it's such a cutting edge, I mean, yes. [[04:17:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15444.36s)]
*  It depends on where you think AI capabilities are going. [[04:17:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15446.2s)]
*  You have to keep winning. [[04:17:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15448.56s)]
*  Yes. [[04:17:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15449.96s)]
*  You have to keep winning. [[04:17:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15450.8s)]
*  As you climb, even if the AI capabilities [[04:17:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15452.039999999999s)]
*  are going super rapidly awesome into the direction of AGI, [[04:17:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15454.88s)]
*  like there's still a boost for X in terms of data. [[04:17:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15458.76s)]
*  Google in terms of data, Meta in terms of data, [[04:17:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15463.22s)]
*  in terms of other products and the money. [[04:17:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15466.06s)]
*  And like, there's just huge amounts of money. [[04:17:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15467.779999999999s)]
*  The whole idea is human data is kind of tapped out. [[04:17:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15470.179999999998s)]
*  We don't care. [[04:17:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15472.46s)]
*  We all care about self-play, verifiable tasks. [[04:17:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15473.3s)]
*  Yeah, so self-play is an AWS. [[04:17:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15475.66s)]
*  Think about AWS. [[04:17:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15476.5s)]
*  Which is an RNG problem. [[04:17:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15477.34s)]
*  AWS does not make a lot of money on each individual machine. [[04:17:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15478.179999999998s)]
*  And the same can be said for the most powerful AI platform, [[04:18:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15481.859999999999s)]
*  which is even though the calls to the API are so cheap, [[04:18:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15485.699999999999s)]
*  there's still a lot of money to be made [[04:18:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15488.3s)]
*  by owning that platform. [[04:18:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15490.38s)]
*  And there's a lot of discussions [[04:18:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15491.9s)]
*  as it's the next compute layer. [[04:18:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15493.26s)]
*  You have to believe that, [[04:18:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15495.779999999999s)]
*  and yeah, there's a lot of discussions that tokens [[04:18:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15497.14s)]
*  and tokenomics and LLM APIs are the next compute layer, [[04:18:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15499.359999999999s)]
*  or the next paradigm for the economy, [[04:18:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15502.74s)]
*  kind of like energy and oil was. [[04:18:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15504.32s)]
*  But there's also like, you have to sort of believe that [[04:18:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15506.06s)]
*  APIs and chat are not where AI is stuck, right? [[04:18:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15508.82s)]
*  It is actually just tasks and agents [[04:18:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15512.82s)]
*  and robotics and computer use. [[04:18:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15514.699999999999s)]
*  And those are the areas where all the value [[04:18:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15516.38s)]
*  will be delivered, not API, not chat application, right? [[04:18:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15519.26s)]
*  Is it possible you have, I mean, [[04:18:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15523.1s)]
*  it all just becomes a commodity and you have [[04:18:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15524.9s)]
*  the very thin wrapper, like perplexity. [[04:18:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15528.5s)]
*  Just joking. [[04:18:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15533.06s)]
*  There are a lot of wrappers making a lot of money. [[04:18:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15534.78s)]
*  Yeah, but do you think it's possible [[04:18:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15537.1s)]
*  that people would just even forget what open AI [[04:18:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15538.94s)]
*  and the thropic is, and just, [[04:19:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15541.78s)]
*  because there'll be wrappers around the API [[04:19:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15543.26s)]
*  and it just dynamically. [[04:19:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15545.54s)]
*  If model progress is not rapid, yeah. [[04:19:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15546.62s)]
*  It's becoming a commodity, right? [[04:19:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15549.460000000001s)]
*  DeepSeek V3 shows this, but also the GPT-3 chart earlier, [[04:19:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15550.34s)]
*  Kurt chart showed this, right? [[04:19:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15553.86s)]
*  Llama 3B is 1200X cheaper than GPT-3. [[04:19:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15554.82s)]
*  Any GPT-3, like anyone whose business model [[04:19:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15557.94s)]
*  was GPT-3 level capabilities is dead. [[04:19:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15560.5s)]
*  Anyone whose business model's [[04:19:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15562.78s)]
*  GPT-4 level capabilities is dead. [[04:19:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15563.86s)]
*  It is a common saying that the best business is being made. [[04:19:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15566.300000000001s)]
*  Now are ones that are predicated on models getting better. [[04:19:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15569.220000000001s)]
*  Right, which would be like wrappers, [[04:19:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15572.1s)]
*  thing that is riding the wave of the models. [[04:19:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15574.1s)]
*  The short term, the company that could make the most money [[04:19:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15577.66s)]
*  is the one that figures out [[04:19:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15580.1s)]
*  what advertising targeting method works [[04:19:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15581.14s)]
*  for language model generations. [[04:19:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15584.14s)]
*  We have the meta ads, which are hyper-targeted in feed, [[04:19:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15585.619999999999s)]
*  not within specific pieces of content. [[04:19:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15589.02s)]
*  And we have search ads that are used by Google [[04:19:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15591.06s)]
*  and Amazon has been rising a lot on search. [[04:19:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15592.9s)]
*  But within a piece, within a return from ShadGPT, [[04:19:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15594.9s)]
*  it is not clear how you get a high quality placed ad [[04:19:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15597.86s)]
*  within the output. [[04:20:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15601.9s)]
*  And if you can do that with model costs coming down, [[04:20:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15603.1s)]
*  you can just get super high revenue. [[04:20:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15606.3s)]
*  Like that revenue is totally untapped [[04:20:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15609.5s)]
*  and it's not clear technically how it is done. [[04:20:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15611.019999999999s)]
*  Yeah, that is, I mean, the sort of the AdSense innovation [[04:20:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15612.779999999999s)]
*  that Google did. [[04:20:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15616.74s)]
*  The one day you'll have in GPT output an ad [[04:20:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15618.42s)]
*  and that's gonna make like billions. [[04:20:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15622.699999999999s)]
*  And it could be very subtle. [[04:20:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15624.66s)]
*  It could be in conversation. [[04:20:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15625.699999999999s)]
*  Like we have voice mode now. [[04:20:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15627.099999999999s)]
*  It could be some way of making it [[04:20:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15628.179999999998s)]
*  so the voice introduces certain things. [[04:20:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15630.259999999998s)]
*  It's much harder to measure and it takes imagination, [[04:20:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15632.619999999999s)]
*  but yeah. [[04:20:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15635.14s)]
*  And it wouldn't be so, it wouldn't come off shady [[04:20:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15636.66s)]
*  so that you would receive public blowback, [[04:20:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15639.779999999999s)]
*  that kind of thing. [[04:20:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15641.699999999999s)]
*  So you have to do it loud enough to where it's clear [[04:20:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15642.539999999999s)]
*  it's an ad and balance all of that. [[04:20:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15644.14s)]
*  So that's the open question they're trying to solve. [[04:20:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15646.46s)]
*  Anthropic and OpenAI, they need to. [[04:20:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15648.699999999999s)]
*  They might not say that they're trying. [[04:20:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15651.5s)]
*  I don't think they care about that at all. [[04:20:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15652.34s)]
*  They don't care about it right now. [[04:20:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15653.42s)]
*  I think it's places like Perplexity [[04:20:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15654.939999999999s)]
*  are experimenting on that more. [[04:20:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15656.939999999999s)]
*  Oh, interesting. [[04:20:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15659.14s)]
*  Yeah, for sure. [[04:20:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15659.98s)]
*  Like Perplexity, Google, Meta care about this. [[04:21:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15661.14s)]
*  I think OpenAI and Anthropic are purely laser focused on. [[04:21:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15664.38s)]
*  AGI. [[04:21:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15667.94s)]
*  Yeah, agents and AGI. [[04:21:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15668.78s)]
*  And if I build AGI, I can make tons of money, right? [[04:21:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15670.58s)]
*  Or I can pay for everything, right? [[04:21:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15674.58s)]
*  And this is, it's just predicated [[04:21:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15676.02s)]
*  like back on the like export control thing, right? [[04:21:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15678.98s)]
*  If you think AGI is five, 10 years away or less, right? [[04:21:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15680.7s)]
*  These labs think it's two, three years away. [[04:21:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15683.74s)]
*  Obviously your actions are, [[04:21:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15686.1s)]
*  if you assume they're rational actors, [[04:21:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15689.74s)]
*  which they are mostly, [[04:21:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15691.06s)]
*  what you do in a two year AGI versus five year [[04:21:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15693.179999999998s)]
*  versus 10 years, very, very, very different, right? [[04:21:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15696.22s)]
*  Do you think agents are promising? [[04:21:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15699.82s)]
*  We have to talk about this. [[04:21:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15702.22s)]
*  This was, this is like the excitement of the year [[04:21:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15703.46s)]
*  that agents are gonna rev, [[04:21:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15707.939999999999s)]
*  this is the generic hype term [[04:21:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15709.14s)]
*  that a lot of business folks are using. [[04:21:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15713.099999999999s)]
*  AI agents are gonna revolutionize everything. [[04:21:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15714.779999999999s)]
*  Okay, so mostly the term agent is obviously overblown. [[04:21:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15717.26s)]
*  We've talked a lot about reinforcement learning [[04:22:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15721.02s)]
*  as a way to train for verifiable outcomes. [[04:22:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15722.94s)]
*  Agents should mean something that is open-ended [[04:22:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15725.94s)]
*  and is solving a task independently on its own [[04:22:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15728.26s)]
*  and able to adapt to uncertainty. [[04:22:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15730.58s)]
*  There's a lot of term agent applied to things [[04:22:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15732.7s)]
*  like Apple intelligence, which we still don't have [[04:22:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15734.94s)]
*  after the last WWDC, which is orchestrating between apps. [[04:22:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15737.62s)]
*  And that tool use thing is something [[04:22:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15741.980000000001s)]
*  that language models can do really well. [[04:22:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15744.220000000001s)]
*  Apple intelligence, I suspect, will come eventually. [[04:22:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15746.300000000001s)]
*  It's a closed domain, it's your messages app [[04:22:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15749.34s)]
*  integrating with your photos, with AI in the background. [[04:22:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15751.7s)]
*  That will work. [[04:22:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15754.66s)]
*  That has been described as an agent [[04:22:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15755.5s)]
*  by a lot of software companies to get into the narrative. [[04:22:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15757.78s)]
*  The question is, what ways can we get language models [[04:22:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15760.9s)]
*  to generalize to new domains [[04:22:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15766.66s)]
*  and solve their own problems in real time? [[04:22:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15768.26s)]
*  Maybe some tiny amount of training [[04:22:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15771.1s)]
*  when they are doing this with fine tuning themselves [[04:22:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15772.94s)]
*  or in context learning, [[04:22:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15774.98s)]
*  which is the idea of storing information in a prompt [[04:22:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15776.14s)]
*  and you can use learning algorithms to update that. [[04:22:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15779.06s)]
*  And whether or not you believe [[04:23:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15781.9s)]
*  that that is gonna actually generalize to things [[04:23:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15784.06s)]
*  like me saying, book my trip to go to Austin in two days, [[04:23:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15786.42s)]
*  I have XYZ constraints and actually trusting it. [[04:23:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15792.38s)]
*  I think there's a HCI problem coming back for information. [[04:23:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15795.74s)]
*  Well, what's your prediction there? [[04:23:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15799.46s)]
*  Cause my gut says we're very far away from that. [[04:23:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15801.1s)]
*  I think opening eyes statement, [[04:23:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15804.66s)]
*  I don't know if you've seen the five levels, right? [[04:23:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15807.38s)]
*  Or it's chat is level one, reasoning is level two, [[04:23:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15809.539999999999s)]
*  and then agents is level three. [[04:23:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15813.22s)]
*  And I think there's a couple more levels, [[04:23:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15814.539999999999s)]
*  but it's important to know, right? [[04:23:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15815.779999999999s)]
*  We were in chat for a couple of years, right? [[04:23:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15817.3s)]
*  We just theoretically got to reasoning. [[04:23:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15820.26s)]
*  We'll be here for a year or two, right? [[04:23:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15822.82s)]
*  And then agents, but at the same time, [[04:23:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15824.539999999999s)]
*  people can try and approximate capabilities [[04:23:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15826.699999999999s)]
*  of the next level. [[04:23:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15829.82s)]
*  But the agents are doing things autonomously, [[04:23:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15830.66s)]
*  doing things for minutes at a time, [[04:23:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15833.3s)]
*  hours at a time, et cetera, right? [[04:23:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15834.98s)]
*  Reasoning is doing things for tens of seconds at a time, [[04:23:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15837.34s)]
*  and then coming back with an output [[04:24:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15841.939999999999s)]
*  that I still need to verify and use and try to check out. [[04:24:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15843.019999999999s)]
*  And the biggest problem is of course, [[04:24:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15846.74s)]
*  it's the same thing with manufacturing, right? [[04:24:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15849.539999999999s)]
*  There's the whole six sigma thing, right? [[04:24:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15851.099999999999s)]
*  How many nines do you get, [[04:24:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15853.099999999999s)]
*  and then you compound the nines onto each other. [[04:24:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15854.099999999999s)]
*  And it's like, if you multiply by the number of steps [[04:24:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15856.22s)]
*  that are six sigma, you get to a yield or something, right? [[04:24:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15858.98s)]
*  So like in semiconductor manufacturing, [[04:24:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15864.06s)]
*  tens of thousands of steps, [[04:24:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15865.62s)]
*  9999999 is not enough, right? [[04:24:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15867.220000000001s)]
*  Because you multiply by that many times, [[04:24:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15870.1s)]
*  you actually end up with like 6% yield, right? [[04:24:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15871.86s)]
*  Or zero. [[04:24:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15874.140000000001s)]
*  Or zero. [[04:24:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15874.980000000001s)]
*  And this is the same thing with agents, right? [[04:24:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15876.54s)]
*  Like chaining tasks together each time. [[04:24:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15877.820000000002s)]
*  LLMs, even the best LLMs [[04:24:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15880.62s)]
*  in particularly pretty good benchmarks, [[04:24:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15883.140000000001s)]
*  don't get 100%, right? [[04:24:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15885.820000000002s)]
*  They get a little bit below that [[04:24:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15888.1s)]
*  because there's a lot of noise. [[04:24:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15889.18s)]
*  And so how do you get to enough nines, right? [[04:24:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15890.7s)]
*  This is the same thing with self-driving. [[04:24:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15894.62s)]
*  We can't have self-driving [[04:24:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15896.460000000001s)]
*  because without it being like super geo-fenced, [[04:24:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15897.54s)]
*  like Google's, right? [[04:24:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15899.78s)]
*  And even then they have a bunch of teleoperators [[04:25:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15901.26s)]
*  to make sure it doesn't get stuck, right? [[04:25:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15902.900000000001s)]
*  But you can't do that [[04:25:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15904.380000000001s)]
*  because it doesn't have enough nines. [[04:25:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15905.62s)]
*  And self-driving has quite a lot of structure [[04:25:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15907.300000000001s)]
*  because roads have rules. [[04:25:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15910.740000000002s)]
*  It's well-defined. [[04:25:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15912.86s)]
*  There's regulation. [[04:25:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15914.060000000001s)]
*  When you're talking about computer use [[04:25:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15915.58s)]
*  for the open web, for example, [[04:25:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15917.94s)]
*  or the open operating system, [[04:25:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15919.820000000002s)]
*  it's a mess. [[04:25:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15923.5s)]
*  So the possibility... [[04:25:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15924.900000000001s)]
*  I'm always skeptical of any system [[04:25:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15927.18s)]
*  that is tasked with interacting with the human world, [[04:25:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15929.460000000001s)]
*  with the open, messy human world. [[04:25:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15934.820000000002s)]
*  If we can't get intelligence [[04:25:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15936.660000000002s)]
*  that's enough to solve the human world on its own, [[04:25:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15938.1s)]
*  we can create infrastructure [[04:25:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15941.060000000001s)]
*  like the human operators for Waymo [[04:25:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15942.86s)]
*  over many years that enable certain workflows. [[04:25:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15945.18s)]
*  There is a company, I don't remember it, but it is, [[04:25:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15947.78s)]
*  but that's literally their pitches. [[04:25:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15949.62s)]
*  Yeah, we're just gonna be the human operator [[04:25:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15951.18s)]
*  when agents fail and you just call us and we fix it. [[04:25:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15952.74s)]
*  Same thing for... [[04:25:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15955.58s)]
*  It's like an API call and it's hilarious. [[04:25:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15956.42s)]
*  There's gonna be teleoperation markets [[04:25:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15957.34s)]
*  when we get human robots, [[04:25:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15958.82s)]
*  which is there's gonna be somebody around the world [[04:25:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15959.9s)]
*  that's happy to fix the fact [[04:26:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15962.7s)]
*  that it can't finish loading my dishwasher [[04:26:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15964.220000000001s)]
*  when I'm unhappy with it, [[04:26:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15966.34s)]
*  but that's just gonna be part of the Tesla service package. [[04:26:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15967.42s)]
*  I'm just imagining an AI agent [[04:26:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15970.62s)]
*  talking to another AI agent. [[04:26:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15973.98s)]
*  One company has an AI agent [[04:26:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15975.58s)]
*  that specializes in helping other AI agents. [[04:26:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15977.18s)]
*  But if you can make things that are good at one step, [[04:26:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15980.82s)]
*  you can stack them together. [[04:26:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15983.26s)]
*  So that's why I'm really, [[04:26:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15985.14s)]
*  if it takes a long time, [[04:26:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15986.5s)]
*  we're gonna build infrastructure that enables it. [[04:26:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15988.18s)]
*  You see the operator launch, [[04:26:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15990.26s)]
*  they have partnerships with certain websites, [[04:26:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15991.539999999999s)]
*  with DoorDash, with OpenTable, with things like this. [[04:26:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15993.539999999999s)]
*  Those partnerships are gonna let them climb really fast. [[04:26:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15997.18s)]
*  Their model's gonna get really good at those things. [[04:26:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=15999.86s)]
*  It's gonna prove a concept that might be a network effect [[04:26:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16002.02s)]
*  where more companies wanna make it easier for AI. [[04:26:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16004.66s)]
*  Some companies will be like, [[04:26:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16007.46s)]
*  no, let's put blockers in place. [[04:26:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16008.54s)]
*  And this is the story of the internet we've seen. [[04:26:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16011.54s)]
*  We see it now with training data for language models [[04:26:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16013.62s)]
*  where companies are like, no, you have to pay. [[04:26:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16015.66s)]
*  Like, business working it out. [[04:26:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16018.06s)]
*  That said, I think airlines have a very, [[04:27:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16020.66s)]
*  and hotels have high incentive [[04:27:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16023.54s)]
*  to make their site work really well, [[04:27:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16025.38s)]
*  and they usually don't. [[04:27:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16027.3s)]
*  Like, if you look at how many clicks it takes [[04:27:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16029.42s)]
*  to order an airplane ticket, it's insane. [[04:27:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16031.5s)]
*  You actually can't call an American Airlines agent anymore. [[04:27:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16034.7s)]
*  They don't have a phone number. [[04:27:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16038.1s)]
*  I mean, it's horrible on many, on the interface front. [[04:27:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16040.34s)]
*  To imagine that agents will be able to deal [[04:27:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16044.5s)]
*  with that website when I, as a human, struggle. [[04:27:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16046.42s)]
*  Like, I have an existential crisis every time [[04:27:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16049.740000000002s)]
*  I try to book an airplane ticket that I don't, [[04:27:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16051.62s)]
*  I think it's gonna be extremely difficult to build [[04:27:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16056.220000000001s)]
*  an AI agent that's robust in that way. [[04:27:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16058.060000000001s)]
*  But think about it, like, United has accepted [[04:27:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16060.34s)]
*  the Starlink term, which is they have to provide [[04:27:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16062.460000000001s)]
*  Starlink for free, and the users are going to love it. [[04:27:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16064.7s)]
*  What if one airline is like, we're gonna take a year, [[04:27:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16067.18s)]
*  and we're gonna make our website have white text [[04:27:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16070.220000000001s)]
*  that works perfectly for the AIs? [[04:27:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16072.58s)]
*  Every time anyone asks about an AI flight, [[04:27:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16074.78s)]
*  they buy whatever airline it is. [[04:27:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16077.460000000001s)]
*  Or like, they just like, here's an API, [[04:27:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16079.5s)]
*  it's only exposed to AI agents, and if anyone queries it, [[04:28:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16082.1s)]
*  the price is 10% higher for any flight, [[04:28:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16084.74s)]
*  but we'll let you see any of our flights, [[04:28:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16087.86s)]
*  and you can just book any of them. [[04:28:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16089.300000000001s)]
*  Here you go, agent. [[04:28:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16090.62s)]
*  And then it's like, oh, and I made 10% higher price. [[04:28:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16091.460000000001s)]
*  Awesome. [[04:28:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16093.58s)]
*  And like, am I willing to say that for like, [[04:28:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16094.58s)]
*  book me a flight to Selex, right? [[04:28:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16096.42s)]
*  And it's like, yeah, whatever. [[04:28:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16097.9s)]
*  I think computers and real world and the open world [[04:28:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16099.62s)]
*  are really, really messy, but if you start defining [[04:28:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16104.18s)]
*  the problem in narrow regions, people are gonna be able [[04:28:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16107.86s)]
*  to create very, very productive things, [[04:28:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16111.02s)]
*  and ratchet down cost massively, right? [[04:28:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16114.300000000001s)]
*  Now, crazy things like robotics in the home, [[04:28:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16117.98s)]
*  those are gonna be a lot harder to do, [[04:28:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16121.82s)]
*  just like self-driving, right? [[04:28:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16123.98s)]
*  Because there's just a billion different failure modes, [[04:28:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16125.38s)]
*  right, but like, agents that can like, navigate [[04:28:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16127.859999999999s)]
*  a certain set of websites and do certain sets of tasks, [[04:28:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16131.259999999998s)]
*  or like, look at your, you know, take a photo [[04:28:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16133.98s)]
*  of your groceries, your fridge, or like, [[04:28:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16136.539999999999s)]
*  upload your recipes, and then like, it figures out [[04:28:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16138.82s)]
*  what to order from, you know, Amazon slash Whole Foods [[04:29:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16140.66s)]
*  food delivery, like, that's gonna be like, pretty quick [[04:29:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16144.179999999998s)]
*  and easy to do, I think. [[04:29:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16146.58s)]
*  So it's gonna be a whole range of like, business outcomes, [[04:29:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16147.5s)]
*  and it's gonna be tons of sort of optimism around, [[04:29:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16149.939999999999s)]
*  people can just figure out ways to make money. [[04:29:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16152.98s)]
*  To be clear, these sandboxes already exist in research. [[04:29:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16154.46s)]
*  There are people who have built clones [[04:29:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16156.939999999999s)]
*  of all the most popular websites, of Google, Amazon, [[04:29:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16158.859999999999s)]
*  blah, blah, blah, to make it so that there's, [[04:29:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16162.019999999999s)]
*  I mean, OpenAI probably has them internally, [[04:29:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16164.859999999999s)]
*  to train these things. [[04:29:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16166.58s)]
*  It's the same as DeepMind's robotics team for years, [[04:29:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16167.419999999998s)]
*  has had clusters for robotics, where you, like, [[04:29:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16169.699999999999s)]
*  you interact with robots fully remotely, [[04:29:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16172.46s)]
*  they just have a lab in London, and you send tasks to it, [[04:29:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16174.58s)]
*  arrange the blocks, and you do this research. [[04:29:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16177.419999999998s)]
*  Obviously, there's techs there that fix stuff, [[04:29:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16179.779999999999s)]
*  but we've turned these cranks of automation before. [[04:29:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16182.22s)]
*  You go from sandbox to progress, [[04:29:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16186.46s)]
*  and then you add one more domain at a time, [[04:29:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16188.3s)]
*  and generalize, I think. [[04:29:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16191.179999999998s)]
*  In the history of NLP and language processing, [[04:29:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16192.3s)]
*  instruction tuning in tasks per language model [[04:29:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16195.34s)]
*  used to be like, one language model did one task. [[04:29:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16197.74s)]
*  And then in the instruction tuning literature, [[04:30:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16200.14s)]
*  there's this point where you start adding more [[04:30:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16201.859999999999s)]
*  and more tasks together, where it just starts [[04:30:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16203.66s)]
*  to generalize to every task. [[04:30:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16205.859999999999s)]
*  And we don't know where on this curve we are. [[04:30:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16207.5s)]
*  I think for reasoning with this RL and verifiable domains, [[04:30:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16208.98s)]
*  we're early, but we don't know where the point is [[04:30:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16211.9s)]
*  where you just start training on enough domains, [[04:30:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16214.1s)]
*  and poof, like, more domains just start working, [[04:30:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16216.62s)]
*  and you've crossed the generalization barrier. [[04:30:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16219.380000000001s)]
*  Well, what do you think about the programming context? [[04:30:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16222.02s)]
*  So, software engineering. [[04:30:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16225.78s)]
*  That's where I personally, and I know a lot of people [[04:30:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16228.7s)]
*  interact with AI the most. [[04:30:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16233.02s)]
*  There's a lot of fear and angst, too, [[04:30:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16234.74s)]
*  from current CS students, but there's also, [[04:30:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16236.02s)]
*  that's where, that is the area where probably [[04:30:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16237.78s)]
*  the most AI revenue and productivity gains have come. [[04:30:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16240.3s)]
*  Whether it be copilots or cursor or what have you, [[04:30:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16244.5s)]
*  or just standard chat GPT. [[04:30:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16248.66s)]
*  I know very few programmers who don't have chat GPT, [[04:30:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16251.66s)]
*  and actually many of them have the $200 tier [[04:30:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16254.22s)]
*  because that's what it's so good for. [[04:30:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16256.42s)]
*  I think that in that world, we already see it, [[04:30:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16259.34s)]
*  like, Swibench, and if you've looked at the benchmark [[04:31:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16263.14s)]
*  made by some Stanford students, [[04:31:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16266.179999999998s)]
*  I wouldn't say it's really hard, [[04:31:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16267.779999999999s)]
*  but I wouldn't say it's easy either. [[04:31:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16269.22s)]
*  I think it takes someone who's been through at least [[04:31:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16270.22s)]
*  a few years of CS or a couple years of programming [[04:31:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16273.14s)]
*  to do Swibench well, and the models went from 4% to 60% [[04:31:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16275.699999999999s)]
*  in like a year, right? [[04:31:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16279.5s)]
*  And where are they gonna go to next year? [[04:31:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16281.779999999999s)]
*  It's gonna be higher, probably won't be 100% [[04:31:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16284.14s)]
*  because again, that nines is really hard to do, [[04:31:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16286.3s)]
*  but we're gonna get to some point where that's, [[04:31:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16288.859999999999s)]
*  and then we're gonna need harder software engineering [[04:31:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16290.74s)]
*  benchmarks, and so on and so forth. [[04:31:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16292.179999999998s)]
*  But the way that people think of it now [[04:31:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16294.06s)]
*  is it can do code completion easy. [[04:31:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16297.42s)]
*  It can do some function generation, [[04:31:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16298.86s)]
*  and I have to review it, great. [[04:31:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16300.460000000001s)]
*  But really, the software engineering agents, [[04:31:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16301.86s)]
*  I think, can be done faster sooner than any other agent [[04:31:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16304.94s)]
*  because it is a verifiable domain. [[04:31:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16307.58s)]
*  You can always unit test or compile, [[04:31:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16310.300000000001s)]
*  and there's many different regions of like, [[04:31:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16313.140000000001s)]
*  it can inspect the whole code base at once, [[04:31:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16317.140000000001s)]
*  which no engineer really can, [[04:31:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16318.74s)]
*  only the architects can really think about this stuff, [[04:32:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16320.66s)]
*  the really senior guys, and they can define stuff, [[04:32:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16322.62s)]
*  and then the agent can execute on it. [[04:32:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16325.18s)]
*  So I think software engineering costs [[04:32:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16326.900000000001s)]
*  are gonna plummet like crazy. [[04:32:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16328.54s)]
*  And one interesting aspect of that [[04:32:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16329.94s)]
*  is when software engineering costs are really low, [[04:32:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16332.26s)]
*  you get very different markets, right? [[04:32:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16334.5s)]
*  So in the US, you have all these platform SaaS companies, [[04:32:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16336.300000000001s)]
*  right, Salesforce, and so on and so forth, right? [[04:32:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16339.1s)]
*  In China, no one uses platform SaaS. [[04:32:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16341.94s)]
*  Everyone just builds their own stack [[04:32:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16345.740000000002s)]
*  because software engineering is much cheaper in China, [[04:32:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16347.380000000001s)]
*  partially because like people, [[04:32:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16351.140000000001s)]
*  a number of STEM graduates, et cetera, [[04:32:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16352.300000000001s)]
*  so it's generally just cheaper to do. [[04:32:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16354.7s)]
*  And so at the same time, code for like code LLMs [[04:32:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16358.740000000002s)]
*  have been adopted much less in China [[04:32:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16361.26s)]
*  because the cost of an engineer there is much lower. [[04:32:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16362.580000000002s)]
*  But like what happens when every company [[04:32:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16365.140000000001s)]
*  can just invent their own business logic, [[04:32:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16366.86s)]
*  like really cheaply and quickly? [[04:32:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16368.380000000001s)]
*  You stop using platform SaaS, [[04:32:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16369.980000000001s)]
*  you start building custom tailored solutions, [[04:32:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16371.62s)]
*  you change them really quickly, [[04:32:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16373.62s)]
*  now all of a sudden your business [[04:32:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16374.900000000001s)]
*  is a little bit more efficient too potentially [[04:32:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16376.02s)]
*  because you're not dealing with the hell [[04:32:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16377.460000000001s)]
*  that is like some random platform SaaS company stuff [[04:32:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16379.1s)]
*  not working perfectly and having to adjust workflows [[04:33:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16381.980000000001s)]
*  or random business automation cases [[04:33:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16384.18s)]
*  that aren't necessarily AI required, [[04:33:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16386.02s)]
*  it's just logic that needs to be built [[04:33:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16388.02s)]
*  that no one has built, right? [[04:33:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16389.38s)]
*  All of these things can go happen faster. [[04:33:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16390.34s)]
*  And so I think software, and then the other domain [[04:33:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16392.02s)]
*  is like industrial, chemical, mechanical engineers [[04:33:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16394.3s)]
*  suck at coding, right? [[04:33:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16396.82s)]
*  Just generally, and like there are tools, [[04:33:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16398.420000000002s)]
*  like semiconductor engineers, [[04:33:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16400.3s)]
*  their tools are 20 years old, [[04:33:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16401.14s)]
*  all the tools run on XP, including ASML lithography tools, [[04:33:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16402.54s)]
*  run on Windows XP, right? [[04:33:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16405.54s)]
*  It's like, you know, and like a lot of the analysis [[04:33:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16406.82s)]
*  happens in Excel, right? [[04:33:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16409.82s)]
*  Like it's just like, guys, like you guys can move [[04:33:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16411.34s)]
*  20 years forward with all the data you have [[04:33:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16413.420000000002s)]
*  and gathered and like do a lot better. [[04:33:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16415.100000000002s)]
*  It's just, you need the engineering skills [[04:33:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16416.7s)]
*  for software engineering to be delivered [[04:33:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16418.86s)]
*  to the actual domain expert engineers. [[04:33:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16420.62s)]
*  So I think that's the area where I'm like super duper bullish [[04:33:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16422.14s)]
*  of generally AI creating value. [[04:33:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16425.18s)]
*  The big picture is that I don't think [[04:33:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16427.66s)]
*  it's gonna be a cliff, it's like, [[04:33:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16429.62s)]
*  we talked to anything that, [[04:33:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16431.54s)]
*  a really good example of how growth changes [[04:33:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16432.82s)]
*  is when meta added stories. [[04:33:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16436.66s)]
*  So Snapchat was on an exponential, [[04:33:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16438.82s)]
*  they added stories, it flatlined. [[04:34:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16440.82s)]
*  Software engineers, then up until the right, [[04:34:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16442.7s)]
*  AI is gonna come in, it's probably just gonna be flat. [[04:34:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16445.46s)]
*  It's like, it's not like everyone's gonna lose their job. [[04:34:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16447.66s)]
*  It's hard because the supply corrects more slowly. [[04:34:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16450.219999999998s)]
*  So the amount of students is still growing [[04:34:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16453.66s)]
*  and that'll correct on a multi-year, like a year delay, [[04:34:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16455.86s)]
*  but the amount of jobs will just turn [[04:34:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16459.379999999997s)]
*  and then maybe in 20, 40 years, it'll be well down. [[04:34:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16461.7s)]
*  But in the few years, there'll never gonna be [[04:34:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16466.02s)]
*  the snap moment where it's like software engineers [[04:34:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16467.7s)]
*  aren't useful. [[04:34:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16469.54s)]
*  I think also the nature of what it means to be a programmer [[04:34:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16470.379999999997s)]
*  and what kind of jobs programmers do changes. [[04:34:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16473.02s)]
*  Because I think there needs to be a human in the loop [[04:34:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16475.66s)]
*  of everything you've talked about. [[04:34:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16479.54s)]
*  There's a really important human in that picture [[04:34:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16481.78s)]
*  of like correcting the code. [[04:34:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16484.38s)]
*  Like fixing- [[04:34:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16487.46s)]
*  Thinking larger than the context length. [[04:34:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16489.7s)]
*  Yep, and debugging also, like debugging by, [[04:34:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16491.1s)]
*  so reading the code, understanding the, [[04:34:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16494.98s)]
*  steering the system, like no, no, no, you missed the point, [[04:34:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16498.06s)]
*  adding more to the prompt. [[04:35:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16500.62s)]
*  Kind of like, yes, adding the human- [[04:35:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16502.3s)]
*  Designing the perfect Google button. [[04:35:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16505.38s)]
*  Google's famous for having people design buttons [[04:35:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16507.42s)]
*  that are so perfect. [[04:35:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16509.46s)]
*  And it's like, how is AI gonna do that? [[04:35:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16510.82s)]
*  Like they could give you all the ideas, perfect fine. [[04:35:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16513.26s)]
*  I mean, that's the thing, you can call it taste. [[04:35:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16517.059999999998s)]
*  Humans have, one thing humans can do is figure out [[04:35:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16519.62s)]
*  what other humans enjoy better than AI systems. [[04:35:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16523.26s)]
*  That's where the preference, you're loading that in, [[04:35:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16525.739999999998s)]
*  but ultimately humans are the greatest preference generator. [[04:35:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16528.42s)]
*  That's where the preference comes from. [[04:35:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16531.420000000002s)]
*  And humans are actually very good at reading, [[04:35:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16532.940000000002s)]
*  like judging between two things versus, [[04:35:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16535.18s)]
*  this goes back to the core of what RLHF [[04:35:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16537.18s)]
*  and preference tuning is, is that it's hard to generate [[04:35:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16539.58s)]
*  a good answer for a lot of problems, [[04:35:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16541.74s)]
*  but it's easy to see which one is better. [[04:35:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16543.06s)]
*  And that's how we're using humans for AI now, [[04:35:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16545.100000000002s)]
*  is judging which one is better. [[04:35:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16547.260000000002s)]
*  And that's what software engineering could look like. [[04:35:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16548.86s)]
*  The PR review, here's a few options. [[04:35:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16551.18s)]
*  What are the like, here's some potential pros and cons, [[04:35:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16554.06s)]
*  and they're gonna be judges. [[04:35:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16557.38s)]
*  I think the thing I would very much recommend [[04:35:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16559.62s)]
*  is people start, programmers start using AI [[04:36:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16562.219999999998s)]
*  and embracing that role of the supervisor of the AI system [[04:36:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16566.1s)]
*  and like partner of the AI system, [[04:36:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16569.42s)]
*  versus writing from scratch or not learning coding at all [[04:36:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16571.5s)]
*  and just generating stuff. [[04:36:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16575.579999999998s)]
*  Cause I think there actually has to be a pretty high level [[04:36:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16576.94s)]
*  of expertise as a programmer to be able to manage [[04:36:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16578.899999999998s)]
*  increasingly intelligent systems. [[04:36:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16582.379999999997s)]
*  I think it's that and then becoming a domain expert [[04:36:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16584.46s)]
*  in something. [[04:36:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16586.539999999997s)]
*  Sure, yeah. [[04:36:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16587.379999999997s)]
*  Seriously, if you go look at aerospace or semiconductors [[04:36:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16589.06s)]
*  or chemical engineering, [[04:36:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16591.38s)]
*  everyone is using really crappy platforms, [[04:36:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16592.620000000003s)]
*  really old software. [[04:36:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16594.9s)]
*  The job of a data scientist is like a joke, right? [[04:36:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16596.940000000002s)]
*  In many cases, in many cases it's very real, [[04:36:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16599.98s)]
*  but it's like bring what the forefront of human capabilities [[04:36:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16602.5s)]
*  are to your domain. [[04:36:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16605.66s)]
*  And even if the forefront is from the AI, [[04:36:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16606.82s)]
*  your domain, you're at the forefront, right? [[04:36:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16609.18s)]
*  So it's like, you have to be at the forefront of something [[04:36:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16610.98s)]
*  and then leverage the rising tide that is AI [[04:36:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16613.54s)]
*  for everything else. [[04:36:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16616.86s)]
*  Oh yeah, there's so many low hanging fruit everywhere [[04:36:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16617.7s)]
*  in terms of where software can like help automate a thing [[04:37:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16621.940000000002s)]
*  or digitize a thing. [[04:37:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16625.54s)]
*  In a legal system, I mean, that's why Doge is exciting. [[04:37:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16627.780000000002s)]
*  Yeah, I got to hang out with a bunch of the Doge folks [[04:37:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16632.420000000002s)]
*  and they, I mean, government is like so old school. [[04:37:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16636.14s)]
*  It's like begging for the modernization of software, [[04:37:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16640.38s)]
*  of organizing the data, all this kind of stuff. [[04:37:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16645.58s)]
*  I mean, in that case is by design [[04:37:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16648.14s)]
*  because bureaucracy creates, protects centers of power [[04:37:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16650.14s)]
*  and so on, but software breaks down those barriers. [[04:37:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16655.100000000002s)]
*  So it hurts those that are holding onto power, [[04:37:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16659.54s)]
*  but ultimately benefits humanity. [[04:37:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16663.02s)]
*  So there's a bunch of domains of that kind. [[04:37:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16665.420000000002s)]
*  One thing we didn't fully finish talking about [[04:37:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16669.5s)]
*  is open source. [[04:37:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16673.22s)]
*  So first of all, congrats. [[04:37:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16674.98s)]
*  You released a new model. [[04:37:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16677.22s)]
*  Yeah, this is the- Tulu. [[04:37:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16678.300000000003s)]
*  I'll explain what a Tulu is. [[04:38:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16680.22s)]
*  A Tulu is a hybrid camel when you breed a dromedary [[04:38:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16681.54s)]
*  with a Bacchrian camel. [[04:38:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16684.22s)]
*  Back in the early days after Chachipiti, [[04:38:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16686.54s)]
*  there was a big wave of models coming out [[04:38:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16688.5s)]
*  like Alpaca, Vaikuna, et cetera, [[04:38:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16690.82s)]
*  that were all named after various mammalian species. [[04:38:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16692.300000000003s)]
*  So Tulu is, the brand is multiple years old, [[04:38:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16695.940000000002s)]
*  which comes from that. [[04:38:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16698.5s)]
*  And we've been playing at the frontiers [[04:38:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16699.82s)]
*  of post-training with open source code. [[04:38:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16703.22s)]
*  And this first part of this release was in the fall [[04:38:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16705.66s)]
*  where we used, we built on LAMA's open models, [[04:38:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16708.940000000002s)]
*  open weight models, and then we add in our fully open code [[04:38:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16713.06s)]
*  or fully open data. [[04:38:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16715.940000000002s)]
*  There's a popular benchmark that is chatbot arena [[04:38:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16718.14s)]
*  and that's generally the metric by which [[04:38:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16720.7s)]
*  how these chat models are evaluated [[04:38:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16722.66s)]
*  and it's humans compare random models [[04:38:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16724.82s)]
*  from different organizations. [[04:38:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16727.38s)]
*  And if you looked at the leaderboard in November or December, [[04:38:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16728.7s)]
*  among the top 60 models from 10s to 20s of organizations, [[04:38:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16731.66s)]
*  none of them had open code or data for just post-training. [[04:38:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16735.739999999998s)]
*  Among that, even fewer or none have pre-training data [[04:38:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16738.66s)]
*  and code available, but it's like post-training [[04:39:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16741.219999999998s)]
*  is much more accessible at this time. [[04:39:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16743.219999999998s)]
*  It's still pretty cheap and you can do it. [[04:39:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16744.46s)]
*  And the thing is like, how high can we push this number [[04:39:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16746.34s)]
*  where people have access to all the code and data? [[04:39:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16748.62s)]
*  So that's kind of the motivation of the project. [[04:39:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16751.42s)]
*  We draw on lessons from LAMA, Nvidia had a Nemotron model [[04:39:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16752.94s)]
*  where the recipe for their post-training was fairly open [[04:39:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16756.98s)]
*  with some data and a paper. [[04:39:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16759.26s)]
*  And it's putting all these together [[04:39:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16761.82s)]
*  to try to create a recipe that people can fine tune models [[04:39:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16763.059999999998s)]
*  like GPT-4 to their domain. [[04:39:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16766.059999999998s)]
*  So to be clear, in the case of TULU, [[04:39:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16767.94s)]
*  maybe you can talk about ALMA too, [[04:39:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16770.66s)]
*  but in the case of TULU, you're taking LAMA 345B. [[04:39:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16772.059999999998s)]
*  TULU has been a series of recipes for post-training. [[04:39:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16778.94s)]
*  So we've done multiple models over years. [[04:39:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16781.46s)]
*  And so you're open sourcing everything. [[04:39:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16783.94s)]
*  Yeah, if you start with an open weight based model, [[04:39:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16786.739999999998s)]
*  the whole model technically is an open source [[04:39:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16789.94s)]
*  because you don't know what LAMA put into it, [[04:39:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16791.62s)]
*  which is why we have the separate thing that we'll get to. [[04:39:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16793.78s)]
*  But it's just getting parts of the pipeline [[04:39:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16795.899999999998s)]
*  where people can zoom in and customize. [[04:39:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16798.42s)]
*  I know I hear from startups and businesses that are like, [[04:40:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16800.3s)]
*  okay, I can take this post-training [[04:40:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16802.18s)]
*  and try to apply it to my domain. [[04:40:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16804.019999999997s)]
*  We talk about verifiers a lot. [[04:40:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16805.82s)]
*  We use this idea, which is reinforcement learning [[04:40:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16807.379999999997s)]
*  with verifiable domain rewards, RLVR, [[04:40:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16809.82s)]
*  kind of similar to RLHF. [[04:40:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16813.18s)]
*  And we applied it to map. [[04:40:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16815.579999999998s)]
*  And the model today, which is like, [[04:40:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16817.78s)]
*  we applied it to the LAMA 445B base model from last year. [[04:40:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16820.219999999998s)]
*  And we have our other stuff. [[04:40:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16824.3s)]
*  We have our instruction tuning and our preference tuning. [[04:40:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16825.739999999998s)]
*  But the math thing is interesting, [[04:40:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16828.899999999998s)]
*  which is like, it's easier to improve this math benchmark. [[04:40:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16831.46s)]
*  There's a benchmark, M-A-T-H, math, all capitals. [[04:40:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16834.3s)]
*  Tough name on the benchmark. [[04:40:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16837.579999999998s)]
*  Name is the area that you're evaluating. [[04:40:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16839.46s)]
*  We're researchers, we're not brands, brand strategists. [[04:40:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16841.699999999997s)]
*  And this is something that the DeepSeq paper [[04:40:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16844.62s)]
*  talked about as well, is like at this bigger model, [[04:40:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16847.34s)]
*  it's easier to elicit powerful capabilities [[04:40:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16849.58s)]
*  with this RL training. [[04:40:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16852.1s)]
*  And then they distill it down from that big model [[04:40:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16853.26s)]
*  to the small model. [[04:40:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16855.74s)]
*  And this model we released today, we saw the same thing. [[04:40:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16856.58s)]
*  We're at AI2, we don't have a ton of compute. [[04:40:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16859.7s)]
*  We can't train 445B models all the time. [[04:41:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16862.38s)]
*  So we just did a few runs and they tend to work. [[04:41:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16864.3s)]
*  And it's like, it just shows that there's a lot of room [[04:41:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16866.82s)]
*  for people to play in these things. [[04:41:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16870.22s)]
*  And that- [[04:41:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16872.5s)]
*  And they crushed LAMA's actual release, right? [[04:41:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16873.34s)]
*  Like they're way better than it. [[04:41:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16875.06s)]
*  Yeah, so our eval numbers, I mean, [[04:41:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16876.94s)]
*  we have extra months in this, but our eval numbers [[04:41:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16878.379999999997s)]
*  are like much better than the LAMA instruct model [[04:41:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16880.46s)]
*  that they released. [[04:41:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16883.5s)]
*  And then you also said better than DeepSeq v3? [[04:41:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16884.34s)]
*  Yeah, on our eval benchmark. [[04:41:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16886.539999999997s)]
*  The most, DeepSeq v3 is really similar. [[04:41:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16888.859999999997s)]
*  We have a safety benchmark to understand [[04:41:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16891.26s)]
*  if it will say harmful things and things like that. [[04:41:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16893.579999999998s)]
*  And that's what draws down most of the way. [[04:41:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16895.5s)]
*  It's still- [[04:41:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16897.26s)]
*  It's like an amalgamation of multiple benchmarks? [[04:41:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16898.1s)]
*  Or what do you mean? [[04:41:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16899.62s)]
*  Yeah, so we have a 10 evaluation. [[04:41:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16900.46s)]
*  This is like, this is standard practice in post-training [[04:41:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16901.66s)]
*  is you choose your evaluations you care about. [[04:41:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16903.5s)]
*  In academics and smaller labs, [[04:41:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16905.66s)]
*  you'll have fewer evaluations. [[04:41:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16907.42s)]
*  In companies, you'll have a really one domain [[04:41:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16908.98s)]
*  that you really care about. [[04:41:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16910.86s)]
*  In frontier labs, you'll have 10s to 20s [[04:41:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16911.82s)]
*  to maybe even like 100 evaluations of specific things. [[04:41:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16914.14s)]
*  So we choose a representative suite of things [[04:41:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16916.82s)]
*  that look like chat, precise instruction following, [[04:41:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16918.78s)]
*  which is like respond only in emojis. [[04:42:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16921.579999999998s)]
*  Like does the model follow weird things like that? [[04:42:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16923.7s)]
*  Math, code, and you create a suite like this. [[04:42:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16925.94s)]
*  So safety would be one of 10 in that type of suite, [[04:42:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16928.1s)]
*  where you have like, what does the broader community [[04:42:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16931.66s)]
*  of AI care about? [[04:42:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16933.46s)]
*  And for example, in comparison to DeepSeq, [[04:42:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16934.78s)]
*  it would be something like our average of our model [[04:42:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16937.3s)]
*  would be 80, including safety and similar without, [[04:42:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16939.46s)]
*  and DeepSeq would be like 79% average score without safety [[04:42:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16943.379999999997s)]
*  and their safety score would bring it down to like 76. [[04:42:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16949.94s)]
*  Oh, so you beat them even ignoring safety. [[04:42:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16951.98s)]
*  Yeah, so this is something that internally, [[04:42:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16953.82s)]
*  it's like, I don't want to win only by like [[04:42:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16955.46s)]
*  how you shape the eval benchmark. [[04:42:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16958.219999999998s)]
*  So if there's something that's like people may or may not [[04:42:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16959.62s)]
*  care about safety in their model, [[04:42:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16961.66s)]
*  safety can come downstream. [[04:42:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16963.3s)]
*  Safety can be when you host the model for an API, [[04:42:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16964.579999999998s)]
*  like safety is addressed in a spectrum of locations [[04:42:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16966.5s)]
*  and AI applications. [[04:42:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16969.7s)]
*  So it's like, if you want to say that you have [[04:42:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16970.739999999998s)]
*  the best recipe, you can't just gate it on these things [[04:42:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16972.02s)]
*  that some people might not want. [[04:42:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16974.34s)]
*  And this is like the time of progress we benefit. [[04:42:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16976.42s)]
*  We can release a model later. [[04:43:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16981.62s)]
*  We have more time to learn new techniques [[04:43:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16983.18s)]
*  like this RL technique. [[04:43:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16985.1s)]
*  We had started this in the fall. [[04:43:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16986.219999999998s)]
*  It's now really popular, the reasoning models. [[04:43:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16988.02s)]
*  The next thing to do for open source post-training [[04:43:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16989.98s)]
*  is to scale up verifiers, to scale up data, [[04:43:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16992.58s)]
*  to replicate some of DeepSeq's results. [[04:43:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16994.940000000002s)]
*  And it's awesome that we have a paper to draw on [[04:43:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16997.260000000002s)]
*  and it makes it a lot easier. [[04:43:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=16999.06s)]
*  And that's the type of things that is going on [[04:43:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17000.780000000002s)]
*  among academic and closed frontier research in AI. [[04:43:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17004.34s)]
*  Since you're pushing open source, [[04:43:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17008.5s)]
*  what do you think is the future of it? [[04:43:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17010.02s)]
*  You think DeepSeq actually changes things [[04:43:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17011.460000000003s)]
*  since it's open source or open weight [[04:43:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17013.86s)]
*  or is pushing the open source movement [[04:43:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17016.22s)]
*  into the open direction? [[04:43:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17017.7s)]
*  This goes very back to license discussion. [[04:43:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17018.980000000003s)]
*  So DeepSeq R1 with a friendly license is a major reset. [[04:43:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17021.1s)]
*  So it's like the first time that we've had [[04:43:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17024.1s)]
*  a really clear frontier model that is open weights [[04:43:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17025.66s)]
*  and with a commercially friendly license [[04:43:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17029.019999999997s)]
*  with no restrictions on downstream use cases, [[04:43:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17031.019999999997s)]
*  synthetic data distillation, whatever. [[04:43:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17033.1s)]
*  This has never been the case at all in the history of AI [[04:43:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17034.899999999998s)]
*  in the last few years since ChachiPT. [[04:43:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17038.019999999997s)]
*  There have been models that are off the frontier [[04:43:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17039.82s)]
*  or models with weird licenses [[04:44:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17041.66s)]
*  that you can't really use them. [[04:44:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17043.18s)]
*  So isn't Meta's license pretty much permissible [[04:44:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17044.219999999998s)]
*  except for five companies? [[04:44:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17047.5s)]
*  So this goes to what open source AI is, [[04:44:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17050.219999999998s)]
*  which is there's also use case restrictions [[04:44:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17052.42s)]
*  in the Lama license, [[04:44:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17054.82s)]
*  which says you can't use it for specific things. [[04:44:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17055.699999999997s)]
*  So if you come from an open source software background, [[04:44:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17057.539999999997s)]
*  you would say that that is not an open source license. [[04:44:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17059.739999999998s)]
*  What kind of things are those though? [[04:44:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17062.46s)]
*  Are they like, at this point, [[04:44:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17063.739999999998s)]
*  I can't pull them off the top of my head. [[04:44:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17066.3s)]
*  Stuff that's competitor probably. [[04:44:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17067.62s)]
*  It used to be military use was one [[04:44:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17069.46s)]
*  and they removed that for scale. [[04:44:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17070.78s)]
*  It'll be like CSAM, like child abuse material. [[04:44:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17072.219999999998s)]
*  That's the type of thing that is forbidden there, [[04:44:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17077.7s)]
*  but that's enough from an open source background [[04:44:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17079.780000000002s)]
*  to say it's not an open source license. [[04:44:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17082.100000000002s)]
*  And also the Lama license has this horrible thing [[04:44:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17083.54s)]
*  where you have to name your model Lama [[04:44:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17085.98s)]
*  if you touch it to the Lama model. [[04:44:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17088.620000000003s)]
*  So it's like the branding thing. [[04:44:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17090.98s)]
*  So if a company uses Lama, [[04:44:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17092.100000000002s)]
*  technically the license says that they should say [[04:44:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17093.620000000003s)]
*  built with Lama at the bottom of their application. [[04:44:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17095.58s)]
*  And from like a marketing perspective, that just hurts. [[04:44:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17097.54s)]
*  I could suck it up as a researcher. [[04:45:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17100.34s)]
*  I'm like, oh, it's fine. [[04:45:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17101.98s)]
*  It says Lama dash on all of our materials for this release. [[04:45:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17103.14s)]
*  But this is why we need truly open models, [[04:45:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17108.18s)]
*  which is we don't know DeepSeek R1's data. [[04:45:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17110.06s)]
*  So you're saying I can't make a cheap copy of Lama [[04:45:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17112.940000000002s)]
*  and pretend it's mine, [[04:45:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17116.18s)]
*  but I can do this with the Chinese model. [[04:45:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17117.02s)]
*  Hell yeah. [[04:45:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17118.86s)]
*  Yeah. [[04:45:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17119.7s)]
*  That's what I'm saying. [[04:45:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17120.54s)]
*  And that's why it's like we want [[04:45:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17122.780000000002s)]
*  this whole open language models thing, [[04:45:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17125.18s)]
*  the Olmo thing is to try to keep the model [[04:45:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17126.98s)]
*  where everything is open with the data [[04:45:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17129.7s)]
*  as close to the frontier as possible. [[04:45:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17131.5s)]
*  So we're compute constrained. [[04:45:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17133.02s)]
*  We're personnel constrained. [[04:45:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17134.06s)]
*  I don't rely on getting insights from people [[04:45:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17135.54s)]
*  like John Schulman tells us to do RL on outputs. [[04:45:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17137.46s)]
*  Like we can make these big jumps, [[04:45:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17140.02s)]
*  but it just takes a long time [[04:45:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17141.7s)]
*  to push the frontier of open source. [[04:45:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17143.34s)]
*  And fundamentally, I would say that that's [[04:45:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17144.86s)]
*  because open source AI does not have the same feedback loops [[04:45:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17147.780000000002s)]
*  as open source software. [[04:45:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17150.940000000002s)]
*  We talked about open source software for security. [[04:45:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17151.98s)]
*  Also is just because you build something once [[04:45:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17154.58s)]
*  and you can reuse it. [[04:45:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17156.9s)]
*  If you go into a new company, there's so many benefits, [[04:45:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17157.940000000002s)]
*  but if you open source a language model, [[04:46:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17160.66s)]
*  you have this data sitting around, [[04:46:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17162.82s)]
*  you have this training code. [[04:46:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17164.42s)]
*  It's not like that easy for someone to come [[04:46:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17165.699999999997s)]
*  and build on and improve [[04:46:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17168.179999999997s)]
*  because you need to spend a lot on compute. [[04:46:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17169.179999999997s)]
*  You need to have expertise. [[04:46:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17170.539999999997s)]
*  So until there are feedback loops of open source AI, [[04:46:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17172.179999999997s)]
*  it seems like mostly an ideological mission. [[04:46:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17175.3s)]
*  Like people like Mark Zuckerberg, [[04:46:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17178.14s)]
*  which is like America needs this. [[04:46:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17179.3s)]
*  And I agree with him, [[04:46:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17181.019999999997s)]
*  but in the time where the motivation ideologically is high, [[04:46:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17182.859999999997s)]
*  we need to capitalize and build this ecosystem [[04:46:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17187.14s)]
*  around what benefits do you get [[04:46:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17189.3s)]
*  from seeing the language model data. [[04:46:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17191.379999999997s)]
*  And there's not a lot about that. [[04:46:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17193.5s)]
*  We're gonna try to launch a demo soon [[04:46:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17195.42s)]
*  where you can look at an Omo model and a query [[04:46:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17196.86s)]
*  and see what pre-training data is similar to it. [[04:46:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17199.42s)]
*  And just like legally risky and complicated, [[04:46:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17201.82s)]
*  but it's like, what does it mean to see the data [[04:46:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17204.66s)]
*  that the AI was trained on? [[04:46:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17207.7s)]
*  It's hard to parse. [[04:46:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17208.86s)]
*  It's terabytes of files. [[04:46:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17209.7s)]
*  It's like, I don't know what I'm gonna find in there, [[04:46:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17211.46s)]
*  but that's what we need to do as an ecosystem [[04:46:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17214.5s)]
*  if people want open source AI to be financially useful. [[04:46:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17216.66s)]
*  We didn't really talk about Stargate. [[04:47:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17221.46s)]
*  I'd love to get your opinion on like [[04:47:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17222.82s)]
*  what the new administration, the Trump administration, [[04:47:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17224.5s)]
*  everything that's doing, that's being done [[04:47:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17227.34s)]
*  from the America side and supporting AI infrastructure [[04:47:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17230.98s)]
*  and the efforts of the different AI companies. [[04:47:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17234.18s)]
*  What do you think about Stargate? [[04:47:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17236.379999999997s)]
*  What are we supposed to think about Stargate? [[04:47:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17237.42s)]
*  And does Sam have the money? [[04:47:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17240.26s)]
*  Yeah, so I think Stargate is a opaque thing. [[04:47:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17243.3s)]
*  It definitely doesn't have $500 billion. [[04:47:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17246.899999999998s)]
*  Doesn't even have $100 billion, right? [[04:47:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17249.3s)]
*  So what they announced is this $500 billion number, [[04:47:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17250.74s)]
*  Larry Ellison, Sam Altman, and Trump said it. [[04:47:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17253.460000000003s)]
*  They thanked Trump and Trump did do some executive actions [[04:47:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17257.18s)]
*  that do significantly improve the ability [[04:47:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17262.02s)]
*  for this to be built faster. [[04:47:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17264.58s)]
*  One of the executive actions he did is on federal land, [[04:47:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17267.7s)]
*  you can just basically build data centers in power, [[04:47:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17269.820000000003s)]
*  pretty much like that. [[04:47:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17273.300000000003s)]
*  And then the permitting process is basically gone [[04:47:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17274.620000000003s)]
*  or you file after the fact. [[04:47:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17276.820000000003s)]
*  Again, I had a schizo take earlier, another schizo take. [[04:47:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17279.179999999997s)]
*  If you've ever been to the Presidio in San Francisco, [[04:48:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17281.62s)]
*  beautiful area. [[04:48:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17284.26s)]
*  You could build a power plant [[04:48:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17285.62s)]
*  and a data center there if you wanted to. [[04:48:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17286.66s)]
*  Because it is federal land, it used to be a military base. [[04:48:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17288.739999999998s)]
*  But obviously this would piss people off. [[04:48:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17291.379999999997s)]
*  It's a good bit, anyways. [[04:48:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17295.219999999998s)]
*  Trump has made it much easier to do this generally. [[04:48:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17296.46s)]
*  Texas has the only unregulated grid in the nation as well. [[04:48:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17300.66s)]
*  Let's go Texas. [[04:48:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17304.019999999997s)]
*  And so therefore, like ERCOT enables people [[04:48:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17305.18s)]
*  to build faster as well. [[04:48:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17308.78s)]
*  In addition, the federal regulations are coming down. [[04:48:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17310.02s)]
*  And so Stargate is predicated, [[04:48:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17312.54s)]
*  and this is why that whole show happened. [[04:48:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17315.5s)]
*  Now, how they came up with a $500 billion number [[04:48:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17317.42s)]
*  is beyond me. [[04:48:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17319.78s)]
*  How they came up with a $100 billion number [[04:48:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17320.899999999998s)]
*  makes sense to some extent, right? [[04:48:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17322.86s)]
*  And there's actually a good table in here [[04:48:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17324.739999999998s)]
*  that I would like to show in that Stargate piece that I had. [[04:48:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17327.34s)]
*  It's the most recent one, yeah. [[04:48:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17331.74s)]
*  So anyways, Stargate, it's basically, right? [[04:48:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17334.14s)]
*  Like there is, it's a table about cost. [[04:48:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17338.06s)]
*  There, you passed it already. [[04:49:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17342.82s)]
*  It's that one. [[04:49:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17344.420000000002s)]
*  So this table is kind of explaining what happens, right? [[04:49:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17346.9s)]
*  So Stargate is in Abilene, Texas, [[04:49:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17349.22s)]
*  the first $100 billion of it. [[04:49:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17351.5s)]
*  That site is 2.2 gigawatts of power in, [[04:49:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17353.66s)]
*  about 1.8 gigawatts of power, [[04:49:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17356.3s)]
*  in about 1.8 gigawatts of power consumed, right? [[04:49:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17358.5s)]
*  Per GPU, they have like roughly, [[04:49:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17363.14s)]
*  Oracle is already building the first part of this [[04:49:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17366.9s)]
*  before Stargate came about. [[04:49:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17369.94s)]
*  To clear they've been building it for a year, [[04:49:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17371.5s)]
*  they tried to rent it to Elon, in fact, right? [[04:49:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17372.82s)]
*  But Elon was like, it's too slow, I need it faster. [[04:49:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17375.54s)]
*  So then he went and did his Memphis thing. [[04:49:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17377.54s)]
*  And so OpenAI was able to get it [[04:49:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17380.1s)]
*  with this like weird joint venture called Stargate. [[04:49:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17381.94s)]
*  They initially signed a deal with just Oracle [[04:49:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17384.46s)]
*  for the first section of this cluster, right? [[04:49:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17386.18s)]
*  This first section of this cluster, right, [[04:49:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17387.98s)]
*  is roughly $5 billion to $6 billion of server spend, right? [[04:49:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17390.22s)]
*  And then there's another billion or so of data center spend. [[04:49:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17397.02s)]
*  But, and then likewise, [[04:49:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17399.579999999998s)]
*  like if you fill out that entire 1.8 gigawatts [[04:50:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17401.739999999998s)]
*  with the next two generations of Nvidia's chips, [[04:50:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17404.02s)]
*  GB 200, GB 300, VR 200, and you fill it out completely, [[04:50:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17406.14s)]
*  that ends up being roughly $50 billion of server cost, [[04:50:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17410.34s)]
*  right? [[04:50:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17415.34s)]
*  So there's data center cost plus maintenance cost [[04:50:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17416.18s)]
*  plus operation cost plus all these things. [[04:50:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17418.18s)]
*  And that's where OpenAI gets to their $100 billion [[04:50:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17421.06s)]
*  announcement that they had, right? [[04:50:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17424.18s)]
*  Cause they talked about $100 billion as phase one, [[04:50:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17425.62s)]
*  that's this Abilene Texas data center, right? [[04:50:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17427.9s)]
*  $100 billion of total cost of ownership, quote unquote, [[04:50:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17430.3s)]
*  right? [[04:50:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17432.82s)]
*  So it's not CapEx, it's not investment, [[04:50:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17433.66s)]
*  it's $100 billion of total cost of ownership. [[04:50:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17435.18s)]
*  And then there will be future phases, [[04:50:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17438.34s)]
*  they're looking at other sites that are even bigger [[04:50:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17440.7s)]
*  than this 2.2 gigawatts, by the way, [[04:50:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17442.420000000002s)]
*  in Texas and elsewhere. [[04:50:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17444.58s)]
*  And so they're not completely ignoring that, [[04:50:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17446.100000000002s)]
*  but there is the number of $100 billion [[04:50:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17449.300000000003s)]
*  that they save for phase one, [[04:50:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17452.74s)]
*  which I do think will happen. [[04:50:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17454.260000000002s)]
*  They don't even have the money for that. [[04:50:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17455.5s)]
*  Furthermore, it's not $100 billion, [[04:50:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17457.300000000003s)]
*  it's $50 billion of spend, right? [[04:50:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17458.460000000003s)]
*  And then like $50 billion of operational cost, [[04:51:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17460.18s)]
*  power, et cetera, rental pricing, et cetera, [[04:51:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17462.22s)]
*  cause they're renting it, [[04:51:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17466.74s)]
*  OpenAI is renting the GPUs [[04:51:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17468.140000000003s)]
*  from the Stargate joint venture, right? [[04:51:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17469.58s)]
*  What money do they actually have, right? [[04:51:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17471.74s)]
*  SoftBank, SoftBank is gonna invest, [[04:51:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17473.5s)]
*  Oracle is gonna invest, OpenAI is gonna invest. [[04:51:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17475.18s)]
*  OpenAI is on the line for $19 billion. [[04:51:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17476.86s)]
*  Everyone knows that they've only got six billion [[04:51:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17478.78s)]
*  in their last round and four billion in debt. [[04:51:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17480.78s)]
*  So, but there's like news of like SoftBank [[04:51:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17482.74s)]
*  maybe investing 25 billion into OpenAI, right? [[04:51:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17486.66s)]
*  So that's part of it, right? [[04:51:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17489.18s)]
*  So 19 billion can come from there. [[04:51:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17490.86s)]
*  So OpenAI does not have the money at all, right? [[04:51:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17492.46s)]
*  To be clear, ink is not dried on anything. [[04:51:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17494.38s)]
*  OpenAI is $0 for this 50 billion, right? [[04:51:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17497.26s)]
*  In which they're legally obligated to put 19 billion [[04:51:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17499.9s)]
*  of CapEx into the joint venture. [[04:51:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17501.98s)]
*  And then the rest they're gonna pay via renting the GPUs [[04:51:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17503.86s)]
*  from the joint venture. [[04:51:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17505.98s)]
*  And then there's Oracle. [[04:51:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17507.1s)]
*  Oracle has a lot of money. [[04:51:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17510.02s)]
*  They're building the first section completely. [[04:51:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17511.739999999998s)]
*  They were spending for themselves, right? [[04:51:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17513.38s)]
*  This $6 billion of CapEx, $10 billion of TCO, [[04:51:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17514.82s)]
*  but they, and they were gonna do that first section. [[04:51:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17518.62s)]
*  They're paying for that, right? [[04:52:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17520.7s)]
*  As far as the rest of the section, [[04:52:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17522.739999999998s)]
*  I don't know how much Larry wants to spend, right? [[04:52:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17523.82s)]
*  At any point he could pull out, right? [[04:52:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17526.26s)]
*  Like this is again, it's like completely voluntary. [[04:52:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17527.54s)]
*  So at any point, there's no signed ink on this, right? [[04:52:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17529.46s)]
*  But he potentially could contribute [[04:52:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17531.7s)]
*  tens of billions of dollars, right? [[04:52:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17533.5s)]
*  To be clear, he's got the money. [[04:52:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17534.420000000002s)]
*  Oracle's got the money. [[04:52:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17535.66s)]
*  And then there's like MGX, which is the UAE fund, [[04:52:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17537.420000000002s)]
*  which technically has $1.5 trillion for investing in AI. [[04:52:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17540.48s)]
*  But again, like, I don't know how real that money is. [[04:52:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17543.86s)]
*  And like, whereas there is no ink signed for this, [[04:52:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17547.06s)]
*  SoftBank does not have $25 billion of cash. [[04:52:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17550.02s)]
*  They have to sell down their stake in ARM, [[04:52:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17552.920000000002s)]
*  which is, you know, the leader in CPUs. [[04:52:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17555.34s)]
*  And they, they IPO'd it. [[04:52:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17557.3s)]
*  This is obviously what they've always wanted to do. [[04:52:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17558.38s)]
*  They just didn't know where they'd redeploy the capital. [[04:52:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17559.94s)]
*  Selling down the stake in ARM makes a ton of sense. [[04:52:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17561.82s)]
*  So they can sell that down and invest in this [[04:52:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17564.359999999997s)]
*  if they want to and invest in OpenAI if they want to. [[04:52:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17566.82s)]
*  As far as like money secured, [[04:52:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17569.739999999998s)]
*  the first 100,000 GB 200 cluster is like, can fund, be funded. [[04:52:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17571.199999999997s)]
*  Everything else after that. [[04:52:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17575.699999999997s)]
*  Up in the air. [[04:52:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17577.539999999997s)]
*  Is up in the air, money's coming. [[04:52:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17578.379999999997s)]
*  I believe the money will come. [[04:52:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17579.62s)]
*  I personally do. [[04:53:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17581.32s)]
*  Just, it's a belief. [[04:53:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17582.26s)]
*  It's a belief that they are gonna release better models [[04:53:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17584.219999999998s)]
*  and be able to raise more money. [[04:53:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17585.98s)]
*  Yeah. [[04:53:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17586.8s)]
*  But like the actual reality is, is that Elon's right. [[04:53:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17587.64s)]
*  There is the money does not exist. [[04:53:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17590.239999999998s)]
*  What does the US government have to do with anything? [[04:53:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17592.239999999998s)]
*  What does Trump have to do with everything? [[04:53:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17594.72s)]
*  He's just a hype man. [[04:53:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17596.28s)]
*  Trump is, he's reducing the regulation [[04:53:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17597.64s)]
*  so they can build it faster. [[04:53:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17599.64s)]
*  Right. [[04:53:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17600.8s)]
*  And he's allowing them to do it. [[04:53:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17602.52s)]
*  Right. [[04:53:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17603.559999999998s)]
*  You know, cause it's like any investment of this side [[04:53:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17604.399999999998s)]
*  is gonna involve like antitrust stuff. [[04:53:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17605.48s)]
*  Right. [[04:53:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17606.88s)]
*  Like, so obviously he's gonna, he's gonna allow them to do it. [[04:53:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17607.72s)]
*  He's gonna enable the regulations to actually allow it [[04:53:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17609.399999999998s)]
*  to be built. [[04:53:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17611.84s)]
*  I don't believe there's any US government dollars [[04:53:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17612.8s)]
*  being spent on this though. [[04:53:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17615.559999999998s)]
*  Yeah. [[04:53:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17616.94s)]
*  So I think he's also just creating a general vibe [[04:53:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17617.78s)]
*  that this is regulation will go down [[04:53:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17620.46s)]
*  and this is the era of building. [[04:53:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17622.5s)]
*  So if you're a builder, you wanna create stuff, [[04:53:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17625.78s)]
*  you wanna launch stuff, this is the time to do it. [[04:53:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17627.96s)]
*  And so like we've had this 1.8 gigawatt data center [[04:53:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17630.1s)]
*  in our data for over a year now. [[04:53:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17632.579999999998s)]
*  And we've been like sort of sending it to all of our clients [[04:53:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17634.579999999998s)]
*  including many of these companies [[04:53:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17636.18s)]
*  that are building the multi gigawatts. [[04:53:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17637.26s)]
*  But that is like at a level that's not quite maybe executives [[04:53:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17639.059999999998s)]
*  like seeing $500 billion, $100 billion [[04:54:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17642.26s)]
*  and then everyone's asking them like, [[04:54:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17644.62s)]
*  so it could spur like another, [[04:54:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17646.06s)]
*  like an even faster arms race, right? [[04:54:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17647.82s)]
*  Cause there's already an arms race, [[04:54:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17650.140000000003s)]
*  but like this, like $100 billion, $500 billion number, [[04:54:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17651.260000000002s)]
*  Trump talking about it on TV, [[04:54:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17654.24s)]
*  like it could spur the arm race to be even faster [[04:54:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17656.02s)]
*  and more investors to flood in and et cetera, et cetera. [[04:54:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17658.74s)]
*  So I think you're right is that in that sense [[04:54:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17661.04s)]
*  that open AI or sort of Trump is sort of like championing [[04:54:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17664.2s)]
*  people are gonna build more [[04:54:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17667.34s)]
*  and his actions are gonna let people build more. [[04:54:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17668.260000000002s)]
*  What are you excited about these, [[04:54:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17670.88s)]
*  several years that are upcoming [[04:54:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17676.019999999997s)]
*  in terms of cluster build outs, [[04:54:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17678.059999999998s)]
*  in terms of breakthroughs in AI, [[04:54:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17680.379999999997s)]
*  like the best possible future you can imagine [[04:54:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17683.379999999997s)]
*  in the next couple of years, two, three, four years, [[04:54:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17685.899999999998s)]
*  what does that look like? [[04:54:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17688.46s)]
*  Just it could be very specific technical things [[04:54:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17689.699999999997s)]
*  like breakthroughs on post post training [[04:54:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17692.3s)]
*  or it could be just size big. [[04:54:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17695.219999999998s)]
*  Yeah, I mean it's impressive clusters. [[04:54:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17699.019999999997s)]
*  I really, I really enjoyed tracking supply chain [[04:55:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17701.519999999997s)]
*  and like who's involved in what. [[04:55:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17704.22s)]
*  I really do, it's really fun to see like the numbers, [[04:55:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17705.620000000003s)]
*  the cost, who's building what capacity, [[04:55:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17707.98s)]
*  helping them figure out how much capacity they should build, [[04:55:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17710.06s)]
*  winning deals, strategic stuff, that's really cool. [[04:55:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17712.06s)]
*  I think technologically, [[04:55:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17714.460000000003s)]
*  there's a lot around the networking side [[04:55:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17716.22s)]
*  that really excites me with optics and electronics, [[04:55:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17718.02s)]
*  like kind of getting closer and closer, [[04:55:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17722.420000000002s)]
*  whether it be co-package optics [[04:55:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17723.7s)]
*  or some sort of like forms of new forms of switching. [[04:55:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17725.08s)]
*  This is internal to a cluster. [[04:55:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17728.22s)]
*  Yeah, also multi data center training, [[04:55:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17730.82s)]
*  like there's people are putting so much fiber [[04:55:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17733.42s)]
*  between these data centers and lighting it up [[04:55:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17735.98s)]
*  with so many different, with so much bandwidth [[04:55:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17738.019999999997s)]
*  that there's a lot of interesting stuff happening on that. [[04:55:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17739.739999999998s)]
*  And right telecom has been really boring since 5G [[04:55:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17742.179999999997s)]
*  and now it's like really exciting again. [[04:55:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17745.0s)]
*  Can you educate me a little bit about the speed of things? [[04:55:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17747.219999999998s)]
*  So the speed of memory versus the speed of interconnect [[04:55:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17750.3s)]
*  versus the speed of fiber between data centers. [[04:55:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17753.339999999997s)]
*  Are these like orders of magnitude different? [[04:55:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17755.859999999997s)]
*  Can we at some point converge towards a place [[04:55:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17758.859999999997s)]
*  where it all just feels like one computer? [[04:56:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17761.54s)]
*  No, I don't think that's possible. [[04:56:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17764.3s)]
*  All right. [[04:56:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17765.58s)]
*  It's only gonna get harder to program, not easier. [[04:56:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17766.420000000002s)]
*  Okay. [[04:56:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17768.9s)]
*  It's only gonna get more difficult [[04:56:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17769.74s)]
*  and complicated and more layers, right? [[04:56:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17770.86s)]
*  The general image that people like to have [[04:56:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17772.940000000002s)]
*  is like this hierarchy of memory. [[04:56:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17775.18s)]
*  So on chip is really close localized within the chip, [[04:56:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17776.86s)]
*  right, you have registers, right? [[04:56:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17779.7s)]
*  Those are shared between some compute elements. [[04:56:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17781.420000000002s)]
*  And then you'll have caches, [[04:56:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17783.22s)]
*  which are shared between more compute elements. [[04:56:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17784.48s)]
*  Then you have like memory, right? [[04:56:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17786.02s)]
*  Like HBM or DRAM, like DDR memory or whatever it is. [[04:56:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17787.14s)]
*  And that's shared between the whole chip. [[04:56:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17790.28s)]
*  And then you can have pools of memory [[04:56:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17792.6s)]
*  that are shared between many chips, right? [[04:56:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17794.8s)]
*  And then storage and you keep zoning out, right? [[04:56:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17796.76s)]
*  The access latency across data centers, [[04:56:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17799.44s)]
*  within the data center, within a chip is different. [[04:56:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17802.079999999998s)]
*  So like you're obviously always, [[04:56:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17804.239999999998s)]
*  you're always gonna have different [[04:56:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17805.32s)]
*  programming paradigms for this. [[04:56:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17808.44s)]
*  It's not gonna be easy. [[04:56:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17810.0s)]
*  Programming this stuff is gonna be hard. [[04:56:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17810.879999999997s)]
*  Maybe AI can help, right? [[04:56:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17812.0s)]
*  With programming this. [[04:56:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17814.6s)]
*  But the way to think about it is that like [[04:56:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17815.44s)]
*  there is sort of like, [[04:56:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17819.36s)]
*  the more elements you add to a task, [[04:57:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17825.38s)]
*  you don't get strong scaling, right? [[04:57:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17829.440000000002s)]
*  If I double the number of chips, [[04:57:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17830.600000000002s)]
*  I don't get two exit performance, right? [[04:57:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17831.64s)]
*  This is just like a reality of computing [[04:57:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17832.9s)]
*  because there's inefficiencies. [[04:57:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17835.56s)]
*  And there's a lot of interesting work being done [[04:57:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17836.84s)]
*  to make it more linear, [[04:57:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17838.88s)]
*  whether it's making the chips more networked together [[04:57:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17841.84s)]
*  more tightly or cool programming models [[04:57:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17844.2s)]
*  or cool algorithmic things that you can do [[04:57:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17847.24s)]
*  on the model side, right? [[04:57:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17849.2s)]
*  DeepSeq did some of these really cool innovations [[04:57:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17850.52s)]
*  because they were limited on interconnect, [[04:57:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17852.5s)]
*  but they still needed to parallelize, right? [[04:57:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17853.68s)]
*  Like all sorts of, you know, [[04:57:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17855.420000000002s)]
*  everyone's always doing stuff. [[04:57:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17856.48s)]
*  Google's got a bunch of work [[04:57:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17857.440000000002s)]
*  and everyone's got a bunch of work about this. [[04:57:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17858.4s)]
*  That stuff is super exciting on the model [[04:57:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17860.72s)]
*  and workload and innovation side, right? [[04:57:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17862.760000000002s)]
*  Hardware, solid state transformers are interesting, right? [[04:57:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17864.88s)]
*  For the power side, there's all sorts of stuff on batteries [[04:57:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17867.920000000002s)]
*  and there's all sorts of stuff on, you know, [[04:57:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17870.88s)]
*  I think when you look at, [[04:57:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17872.920000000002s)]
*  if you look at every layer of the compute stack, right? [[04:57:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17874.12s)]
*  Whether it goes from lithography and etch [[04:57:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17876.22s)]
*  all the way to like fabrication, to like optics, [[04:57:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17877.920000000002s)]
*  to networking, to power, to transformers, to cooling, [[04:58:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17880.32s)]
*  to, you know, a networking [[04:58:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17883.760000000002s)]
*  and you just go on up and up and up and up the stack, [[04:58:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17885.2s)]
*  you know, even air conditioners for data centers [[04:58:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17887.120000000003s)]
*  are like innovating, right? [[04:58:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17889.0s)]
*  Like it's like, there's like copper cables are innovating, [[04:58:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17890.16s)]
*  right? [[04:58:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17892.68s)]
*  Like you wouldn't think it, but copper cables, [[04:58:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17893.52s)]
*  like are, there's some innovations happening there [[04:58:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17894.5s)]
*  with like the density of how you can pack them. [[04:58:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17896.600000000002s)]
*  And like, it's like all of these layers of the stack [[04:58:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17898.460000000003s)]
*  all the way up to the models, [[04:58:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17900.68s)]
*  human progress is at a pace that's never been seen before. [[04:58:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17901.88s)]
*  I'm just imagining you sitting back in a layer somewhere [[04:58:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17904.760000000002s)]
*  with screens everywhere, [[04:58:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17907.08s)]
*  just monitoring the supply chain where all these clusters, [[04:58:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17907.920000000002s)]
*  like all the information you're gathering. [[04:58:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17911.06s)]
*  I mean, you do incredible- [[04:58:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17913.440000000002s)]
*  There's a big team, there's a big team. [[04:58:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17914.280000000002s)]
*  Yeah, I mean, you're, you do quite incredible work [[04:58:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17915.600000000002s)]
*  with semi analysis. [[04:58:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17920.320000000003s)]
*  I mean, it's just keeping your finger on the pulse [[04:58:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17921.160000000003s)]
*  of human civilization in the digital world. [[04:58:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17926.800000000003s)]
*  It's pretty cool. [[04:58:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17929.2s)]
*  Like just to watch, feel that. [[04:58:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17930.04s)]
*  Yeah, thank you. [[04:58:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17931.84s)]
*  I guess- [[04:58:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17932.68s)]
*  Feel all of us like doing shit. [[04:58:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17933.5s)]
*  Epic shit. [[04:58:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17936.56s)]
*  Feel the AGI. [[04:58:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17937.56s)]
*  Feel the, I mean, from meme to like reality. [[04:58:58](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17938.4s)]
*  What Nathan, is there like breakthroughs [[04:59:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17942.4s)]
*  that you're like looking forward to potentially? [[04:59:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17944.32s)]
*  I had a while to think about this [[04:59:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17947.2s)]
*  while listening to Delon's beautiful response. [[04:59:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17948.4s)]
*  He didn't listen to me, he was so in love. [[04:59:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17950.600000000002s)]
*  I knew, no, I knew this was coming. [[04:59:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17952.0s)]
*  And it's like, realistically, training models is very fun [[04:59:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17953.88s)]
*  because there's so much low hanging fruit. [[04:59:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17957.8s)]
*  And the thing that makes my job entertaining, [[04:59:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17959.28s)]
*  I train models, I write analysis [[04:59:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17961.920000000002s)]
*  about what's happening with models. [[04:59:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17964.72s)]
*  And it's fun because there is obviously [[04:59:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17967.22s)]
*  so much more progress to be had. [[04:59:29](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17969.22s)]
*  And the real motivation why I do this [[04:59:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17971.14s)]
*  somewhere where I can share things is that there's just, [[04:59:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17972.98s)]
*  I don't trust people that are like, [[04:59:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17976.02s)]
*  trust me, bro, we're gonna make AI good. [[04:59:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17977.98s)]
*  That's like, we're the ones that it's like, [[04:59:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17979.780000000002s)]
*  we're gonna do it and you can trust us. [[04:59:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17981.22s)]
*  And we're just gonna have all the AI. [[04:59:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17982.780000000002s)]
*  And it's just like, I would like a future [[04:59:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17984.9s)]
*  where more people have a say in what AI is [[04:59:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17987.260000000002s)]
*  and can understand it. [[04:59:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17989.38s)]
*  And that's, it's a little bit less fun [[04:59:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17990.9s)]
*  that it's not a like positive thing of like, [[04:59:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17993.780000000002s)]
*  is it just all really fun? [[04:59:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17995.5s)]
*  Like training models is fun and bring people in is fun, [[04:59:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17996.5s)]
*  but it's really like AI, [[04:59:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=17999.54s)]
*  if it is going to be the most powerful technology [[05:00:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18000.7s)]
*  of my lifetime, it's like, [[05:00:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18002.9s)]
*  we need to have a lot of people involved in making that. [[05:00:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18004.54s)]
*  And making it open helps with that. [[05:00:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18007.5s)]
*  As accessible as possible, as open as possible, yeah. [[05:00:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18011.74s)]
*  In my read of the last few years is that more openness [[05:00:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18014.34s)]
*  would help the AI ecosystem in terms of having [[05:00:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18017.34s)]
*  more people understand what's going on, [[05:00:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18020.34s)]
*  rather than it's researchers from non-AI fields [[05:00:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18021.82s)]
*  to governments, to everything. [[05:00:24](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18024.0s)]
*  It doesn't mean that openness will always be the answer. [[05:00:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18025.68s)]
*  I think then it will reassess of like, [[05:00:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18027.96s)]
*  what is the biggest problem facing AI [[05:00:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18030.04s)]
*  and tack on a different angle to the wild ride [[05:00:31](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18031.72s)]
*  that we're on. [[05:00:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18034.6s)]
*  And for me, just from even the user experience, [[05:00:36](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18036.2s)]
*  anytime you have the, like Aparthi said, [[05:00:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18040.0s)]
*  the aha moments, like the magic, [[05:00:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18042.44s)]
*  like seeing the reasoning, the chain of thought, [[05:00:46](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18046.0s)]
*  it's like, there's something really just fundamentally [[05:00:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18050.6s)]
*  beautiful about that. [[05:00:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18053.28s)]
*  It's putting a mirror to ourselves and seeing like, [[05:00:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18054.64s)]
*  oh shit, it is solving intelligence [[05:00:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18057.199999999997s)]
*  as the cliche goal of these companies is. [[05:00:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18059.64s)]
*  And you get to understand why we humans are special. [[05:01:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18062.68s)]
*  The intelligence within us is special. [[05:01:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18066.879999999997s)]
*  And for now also why we're special in terms of, [[05:01:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18069.16s)]
*  we seem to be conscious and the AI systems for now aren't. [[05:01:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18071.879999999997s)]
*  And we get to explore that mystery. [[05:01:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18076.44s)]
*  So that's, it's just really cool [[05:01:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18080.239999999998s)]
*  to get to explore these questions that I don't think, [[05:01:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18081.84s)]
*  I would have never imagined would be even possible. [[05:01:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18085.8s)]
*  Back when, so just watching with excitement, [[05:01:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18090.32s)]
*  deep blue, big Kasparov, [[05:01:34](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18094.4s)]
*  like I wouldn't have ever thought this kind of AI [[05:01:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18097.48s)]
*  would be possible in my lifetime. [[05:01:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18099.68s)]
*  This is like, this is really feels like AI. [[05:01:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18101.52s)]
*  It's incredible. [[05:01:43](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18103.64s)]
*  I started with AI of learning to fly a Celia quadrotor. [[05:01:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18104.72s)]
*  It's like learn to fly and it'll just like, [[05:01:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18108.56s)]
*  it learned to fly up. [[05:01:50](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18110.600000000002s)]
*  It would hit the ceiling and stop and catch it. [[05:01:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18111.84s)]
*  It's like, okay, that is like really stupid [[05:01:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18113.640000000003s)]
*  compared to what's going on now. [[05:01:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18115.440000000002s)]
*  And now you could probably with natural language, [[05:01:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18117.120000000003s)]
*  tell it to learn to fly and it's going to generate [[05:01:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18119.960000000003s)]
*  the control algorithm required to do that. [[05:02:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18122.2s)]
*  Probably. [[05:02:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18124.08s)]
*  There's low level blockers. [[05:02:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18125.2s)]
*  Like we had to do some weird stuff for that, [[05:02:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18126.440000000002s)]
*  but you can, you definitely can. [[05:02:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18128.280000000002s)]
*  Back to our robotics conversation. [[05:02:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18129.68s)]
*  Yeah, when you have to interact [[05:02:11](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18131.32s)]
*  in an actual physical world, it's hard. [[05:02:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18132.4s)]
*  What gives you hope about the future of human civilization? [[05:02:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18134.280000000002s)]
*  Looking into the next 10 years, 100 years, 1000 years, [[05:02:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18138.4s)]
*  how long do you think we'll make it? [[05:02:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18143.56s)]
*  You think we've got a thousand years? [[05:02:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18145.84s)]
*  Humans will definitely be around in a thousand years. [[05:02:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18148.0s)]
*  I think there's ways that very bad things could happen [[05:02:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18150.480000000003s)]
*  and there'll be way fewer humans, [[05:02:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18153.56s)]
*  but humans are very good at surviving. [[05:02:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18155.16s)]
*  There's been a lot of things that that is true. [[05:02:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18157.24s)]
*  I don't think they're necessarily, [[05:02:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18160.800000000003s)]
*  we're good at long-term credit assignment of risk, [[05:02:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18162.2s)]
*  but when the risk becomes immediate, [[05:02:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18165.36s)]
*  we tend to figure things out. [[05:02:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18167.52s)]
*  And for that reason, I'm like, [[05:02:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18168.920000000002s)]
*  there's physical constraints to things like AGI, [[05:02:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18172.0s)]
*  hyper recursive improvement to kill us all type stuff. [[05:02:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18175.8s)]
*  I'm for the physical reasons [[05:02:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18179.600000000002s)]
*  and for how humans have figured things out before, [[05:03:01](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18181.2s)]
*  I'm not too worried about it. [[05:03:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18183.04s)]
*  AI takeover. [[05:03:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18184.12s)]
*  There are other international things that are worrying, [[05:03:05](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18185.68s)]
*  but there's just fundamental human goodness [[05:03:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18187.8s)]
*  and trying to amplify that. [[05:03:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18192.4s)]
*  And like we're on a tenuous time [[05:03:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18194.56s)]
*  and I mean, if you look at humanity as a whole, [[05:03:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18197.920000000002s)]
*  there's been times where things go backwards. [[05:03:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18200.88s)]
*  There's times when things don't happen at all [[05:03:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18203.12s)]
*  and we're on a, [[05:03:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18205.0s)]
*  what should be very positive trajectory right now. [[05:03:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18206.280000000002s)]
*  Yeah, there seems to be progress, [[05:03:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18208.84s)]
*  but just like with power, [[05:03:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18210.24s)]
*  there's like spikes of human suffering. [[05:03:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18213.12s)]
*  And we wanna try to minimize the amount of spikes. [[05:03:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18215.84s)]
*  Generally humanity is gonna suffer a lot less, right? [[05:03:38](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18218.56s)]
*  I'm very optimistic about that. [[05:03:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18221.68s)]
*  I do worry of like techno-fascism type stuff arising [[05:03:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18224.32s)]
*  as AI becomes more and more prevalent and powerful [[05:03:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18227.600000000002s)]
*  and those who control it can do more and more. [[05:03:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18231.32s)]
*  Maybe it doesn't kill us all, [[05:03:54](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18234.2s)]
*  but at some point, [[05:03:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18235.84s)]
*  every very powerful human [[05:03:56](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18236.8s)]
*  is gonna wanna brain computer interface [[05:03:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18239.16s)]
*  so that they can interact with the AGI [[05:04:00](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18240.84s)]
*  and all of its advantages in many more way [[05:04:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18243.0s)]
*  and merge its mind with sort of like, [[05:04:04](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18244.920000000002s)]
*  and its capabilities or that person's capabilities [[05:04:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18247.08s)]
*  can leverage those much better than anyone else [[05:04:10](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18250.0s)]
*  and therefore be, [[05:04:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18252.08s)]
*  it won't be one person rule them all, [[05:04:13](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18253.320000000003s)]
*  but it will be, [[05:04:14](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18254.800000000003s)]
*  the thing I worry about is it'll be like few people, [[05:04:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18256.600000000002s)]
*  hundreds, thousands, tens of thousands, [[05:04:19](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18259.56s)]
*  maybe millions of people rule whoever's left, right? [[05:04:21](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18261.24s)]
*  And the economy around it, right? [[05:04:26](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18266.04s)]
*  And I think it'll, [[05:04:27](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18267.36s)]
*  that's like the thing that's probably more worrisome [[05:04:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18268.2s)]
*  is like human machine amalgamations. [[05:04:30](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18270.280000000002s)]
*  This enables an individual human [[05:04:33](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18273.68s)]
*  to have more impact on the world [[05:04:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18275.600000000002s)]
*  and that impact can be both positive and negative, right? [[05:04:37](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18277.2s)]
*  Generally, humans have positive impacts on the world, [[05:04:40](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18280.48s)]
*  at least societally, [[05:04:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18282.56s)]
*  but it's possible for individual humans [[05:04:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18284.12s)]
*  to have such negative impacts [[05:04:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18285.88s)]
*  and AGI, at least as I think the labs define it, [[05:04:47](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18287.6s)]
*  which is not a runaway sentient thing, [[05:04:51](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18291.12s)]
*  but rather just something that can do a lot of tasks [[05:04:53](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18293.44s)]
*  really efficiently, [[05:04:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18295.84s)]
*  amplifies the capabilities of someone causing extreme damage, [[05:04:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18297.96s)]
*  but for the most part, [[05:05:02](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18302.079999999998s)]
*  I think it'll be used for profit seeking motives, [[05:05:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18303.16s)]
*  which will then reduce, [[05:05:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18306.16s)]
*  which will increase the abundance and supply of things [[05:05:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18307.2s)]
*  and therefore reduce suffering, right? [[05:05:09](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18309.04s)]
*  That's the goal. [[05:05:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18312.0s)]
*  Scrolling on a timeline, [[05:05:12](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18312.84s)]
*  just drowning in dopamine. [[05:05:15](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18315.4s)]
*  Scrolling is stasis. [[05:05:16](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18316.24s)]
*  That is holding, [[05:05:17](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18317.48s)]
*  scrolling holds the status quo of the world. [[05:05:18](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18318.32s)]
*  That is a positive outcome, right? [[05:05:20](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18320.52s)]
*  Like, it's like, [[05:05:22](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18322.36s)]
*  if I have food tubes and like I'm scrolling and I'm happy, [[05:05:23](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18323.2s)]
*  that's a positive outcome. [[05:05:25](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18325.84s)]
*  While expanding out into the cosmos. [[05:05:28](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18328.72s)]
*  Well, this is a fun time to be alive [[05:05:32](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18332.08s)]
*  and thank you for pushing the forefront away as possible [[05:05:35](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18335.52s)]
*  and thank you for talking to me. [[05:05:39](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18339.879999999997s)]
*  This was fun. [[05:05:41](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18341.16s)]
*  Thanks for having us. Thanks for having us. [[05:05:42](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18342.0s)]
*  Thanks for listening to this conversation [[05:05:44](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18344.0s)]
*  with Dylan Patel and Nathan Lambert. [[05:05:45](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18345.6s)]
*  To support this podcast, [[05:05:48](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18348.559999999998s)]
*  please check out our sponsors in the description. [[05:05:49](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18349.92s)]
*  And now let me leave you with some words [[05:05:52](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18352.68s)]
*  from Richard Feynman. [[05:05:55](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18355.12s)]
*  For a successful technology, [[05:05:57](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18357.32s)]
*  reality must take precedence over public relations. [[05:05:59](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18359.559999999998s)]
*  For nature cannot be fooled. [[05:06:03](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18363.16s)]
*  Thank you for listening. [[05:06:06](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18366.559999999998s)]
*  I hope to see you. [[05:06:07](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18367.64s)]
*  Next time. [[05:06:08](https://www.youtube.com/watch?v=_1f-o0nqpEI&t=18368.88s)]
