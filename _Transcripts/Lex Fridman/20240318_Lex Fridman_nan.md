# Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419
**Lex Fridman:** [March 18, 2024](https://rr3---sn-ab5sznzd.googlevideo.com/videoplayback?expire=1711254083&ei=41X_ZeCOErPB_9EPg7OKiA4&ip=2603%3A7000%3A3200%3Ade9e%3A1c32%3Adff1%3A8836%3A2e08&id=o-AAME4FYCuNQhG7-Ui3PC-roVG6-n-dxo0-9Q8ibKROT_&itag=139&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=-h&mm=31%2C26&mn=sn-ab5sznzd%2Csn-p5qlsnrl&ms=au%2Conr&mv=m&mvi=3&pl=37&initcwndbps=1466250&bui=AaUN6a39lJ5nBuLo5GHvor8BF9uSqeb3Vfm-ifoLTXAk8hAHqQXF7apIeOYSNEjLxIGCdARybDZ46CQM&vprv=1&mime=audio%2Fmp4&gir=yes&clen=42135550&dur=6909.886&lmt=1710903820325658&mt=1711232213&fvip=3&keepalive=yes&c=ANDROID_EMBEDDED_PLAYER&txp=5532434&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cvprv%2Cmime%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRgIhAOtc-21ms7E1rpW2JMVe1cgW4MZRT2JLkv06r5MzEvf1AiEAj2kArL_VwlbyH8DUHHl27qsXul4WATcKFhjno_x1hXA%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=ALClDIEwRgIhAPcbOVCmBSY7Z8Bx_KooScocqDx2a7MOQxk14OhX63KTAiEAnXvHmsDyNkW9nJ9JOehb3mDeYCxxG9xDhuMS5343KkU%3D)
*  I think compute is going to be the currency of the future.
*  I think it will be maybe the most precious commodity in the world.
*  I expect that by the end of this decade and possibly somewhat sooner than that, we will
*  have quite capable systems that we look at and say, wow, that's really remarkable.
*  The road to AGI should be a giant power struggle.
*  I expect that to be the case.
*  Whoever builds AGI first gets a lot of power.
*  Do you trust yourself with that much power?
*  The following is a conversation with Sam Altman, his second time in the podcast.
*  He is the CEO of OpenAI, the company behind GPT-4, ChadGPT, Sora, and perhaps one day,
*  the very company that will build AGI.
*  This is the Lex Friedman Podcast.
*  To support it, please check out our sponsors in the description.
*  And now, dear friends, here's Sam Altman.
*  Take me through the OpenAI board saga that started on Thursday, November 16th, maybe
*  Friday, November 17th for you.
*  That was definitely the most painful professional experience of my life and chaotic and shameful
*  and upsetting and a bunch of other negative things.
*  There were great things about it, too, and I wish it had not been in such an adrenaline
*  rush that I wasn't able to stop and appreciate them at the time.
*  I came across this old tweet of mine, or this tweet of mine from that time period, which
*  was like it was like, you know, kind of going to your own eulogy, watching people say all
*  these great things about you and just like unbelievable support from people I love and
*  care about.
*  That was really nice.
*  That whole weekend, I kind of like felt, with one big exception, I felt like a great deal
*  of love and very little hate.
*  Even though it felt like it just, I have no idea what's happening and what's going to
*  happen here and this feels really bad.
*  There were definitely times I thought it was going to be like one of the worst things to
*  ever happen for AI safety.
*  I also think I'm happy that it happened relatively early.
*  I thought at some point between when OpenAI started and when we created AGI, there was
*  going to be something crazy and explosive that happened, but there may be more crazy
*  and explosive things still to happen.
*  It still, I think, helped us build up some resilience and be ready for more challenges
*  in the future.
*  But the thing you had a sense that you would experience is some kind of power struggle.
*  The road to AGI should be a giant power struggle.
*  The world should, well, not should, I expect that to be the case.
*  You have to go through that, like you said, iterate as often as possible in figuring
*  out how to have a board structure, how to have organization, how to have the kind of
*  people that you're working with, how to communicate, all that in order to deescalate the power
*  struggle as much as possible, pacify it.
*  At this point, it feels like something that was in the past that was really unpleasant
*  and really difficult and painful, but we're back to work and things are so busy and so
*  intense that I don't spend a lot of time thinking about it.
*  There was a time after, there was like this fugue state for kind of like the month after,
*  maybe 45 days after, that was, I was just sort of like drifting through the days.
*  I was so out of it.
*  I was feeling so down.
*  Just in a personal psychological level.
*  Yeah, really painful and hard to like have to keep running open AI in the middle of that.
*  I just wanted to like crawl into a cave and kind of recover for a while.
*  But now it's like we're just back to working on the mission.
*  It's still useful to go back there and reflect on board structures, on power dynamics, on
*  how companies are run, the tension between research and product development and money
*  and all this kind of stuff so that you, who have a very high potential of building AGI,
*  would do so in a slightly more organized, less dramatic way in the future.
*  So there's value there to go, both the personal psychological aspects of you as a leader and
*  also just the board structure and all this kind of messy stuff.
*  Definitely learned a lot about structure and incentives and what we need out of a board.
*  I think it is valuable that this happened now in some sense.
*  I think this is probably not like the last high stress moment of open AI, but it was
*  quite a high stress moment.
*  My company very nearly got destroyed.
*  We think a lot about many of the other things we've got to get right for AGI, but thinking
*  about how to build a resilient org and how to build a structure that will stand up to
*  a lot of pressure in the world, which I expect more and more as we get closer.
*  I think that's super important.
*  Do you have a sense of how deep and rigorous the deliberation process by the board was?
*  Can you shine some light on just human dynamics involved in situations like this?
*  Was it just a few conversations and all of a sudden it escalates and why don't we fire
*  Sam kind of thing?
*  I think the board members are well-meaning people on the whole.
*  I believe that in stressful situations where people feel time pressure or whatever, people
*  make suboptimal decisions.
*  One of the challenges for open AI will be we're going to have to have a board and a
*  team that are good at operating under pressure.
*  Do you think the board had too much power?
*  I think boards are supposed to have a lot of power.
*  One of the things that we did see is in most corporate structures, boards are usually answerable
*  to shareholders.
*  Sometimes people have super voting shares or whatever.
*  In this case, and I think one of the things with our structure that we maybe should have
*  thought about more than we did is that the board of a nonprofit has, unless you put other
*  rules in place, quite a lot of power.
*  They don't really answer to anyone but themselves.
*  There's ways in which that's good, but what we'd really like is for the board of open
*  AI to answer to the world as a whole as much as that's a practical thing.
*  So there's a new board announced?
*  Yeah.
*  There's, I guess, a new smaller board at first and now there's a new final board.
*  Not a final board yet.
*  We've added some.
*  We'll add more.
*  Added some.
*  Okay.
*  What is fixed in the new one that was perhaps broken in the previous one?
*  The old board sort of got smaller over the course of about a year.
*  And then it went down to six.
*  And then we couldn't agree on who to add.
*  And the board also, I think, didn't have a lot of experienced board members and a lot
*  of the new board members at open AI have just have more experience as board members.
*  I think that'll help.
*  It's been criticized, some of the people that are added to the board.
*  I heard a lot of people criticizing the addition of Larry Summers, for example.
*  What's the process of selecting the board like?
*  What's involved in that?
*  So Brett and Larry were kind of decided in the heat of the moment over this very tense
*  weekend.
*  And that weekend was like a real roller coaster.
*  It was like a lot of ups and downs.
*  And we were trying to agree on new board members that both sort of the executive team here
*  and the old board members felt would be reasonable.
*  Larry was actually one of their suggestions, the old board members.
*  Brett, I think I had even previous to that weekend suggested, but he was busy and didn't
*  want to do it.
*  And then we really needed help and would.
*  We talked about a lot of other people too.
*  But that was, I felt like if I was going to come back, I needed new board members.
*  I didn't think I could work with the old board again in the same configuration.
*  Although we then decided, and I'm grateful that Adam would stay, but we wanted to get
*  to, we considered various configurations, decided we wanted to get to a board of three
*  and had to find two new board members over the course of sort of a short period of time.
*  So those were decided honestly without, you know, that's like you kind of do that on the
*  battlefield.
*  We don't have time to design a rigorous process then.
*  For new board members since, new board members will add going forward, we have some criteria
*  that we think are important for the board to have, different expertise that we want
*  the board to have.
*  Unlike hiring an executive where you need them to do one role well, the board needs
*  to do a whole role of kind of governance and thoughtfulness well.
*  So one thing that Brett says, which I really like is that, you know, we want to hire board
*  members in slates, not as individuals one at a time.
*  And you know, thinking about a group of people that will bring nonprofit expertise, expertise
*  in running companies, sort of good legal and governance expertise, that's kind of what
*  we've tried to optimize for.
*  So is technical savvy important for the individual board members?
*  Not for every board member, but for certainly some you need that.
*  That's part of what the board needs to do.
*  So I mean, the interesting thing that people probably don't understand about OpenAI, I
*  certainly don't, is like all the details of running the business.
*  When they think about the board, given the drama, they think about you, they think about
*  like if you reach AGI or you reach some of these incredibly impactful products and you
*  build them and deploy them, what's the conversation with the board like?
*  And they kind of think, all right, what's the right squad to have in that kind of situation
*  to deliberate?
*  Look, I think you definitely need some technical experts there.
*  And then you need some people who are like, how can we deploy this in a way that will
*  help people in the world the most and people who have a very different perspective?
*  You know, I think a mistake that you or I might make is to think that only the technical
*  understanding matters.
*  And that's definitely part of the conversation you want that board to have.
*  But there's a lot more about how that's going to just like impact society and people's lives
*  that you really want represented in there too.
*  And you're just kind of, are you looking at the track record of people or you're just
*  having conversations?
*  Track record is a big deal.
*  You of course have a lot of conversations, but I, you know, there's some roles where
*  I kind of totally ignore track record and just look at slope, kind of ignore the wide
*  intercept.
*  Thank you.
*  Thank you for making it mathematical for the audience.
*  For a board member, like I do care much more about the wide intercept.
*  Like I think there is something deep to say about track record there and experiences sometimes
*  very hard to replace.
*  Do you try to fit a polynomial function or exponential one to the track record?
*  That's not that, and analogy doesn't carry that far.
*  All right.
*  You mentioned some of the low points that weekend.
*  What were some of the low points psychologically for you?
*  Did you consider going to the Amazon jungle and just taking ayahuasca and disappearing
*  forever or?
*  I mean, there's so many low, like it was a very bad period of time.
*  There were great high points too.
*  Like my phone was just like sort of nonstop blowing up with nice messages from people
*  I work with every day, people I hadn't talked to in a decade.
*  I didn't get to like appreciate that as much as I should have because I was just like in
*  the middle of this firefight, but that was really nice.
*  But on the whole, it was like a very painful weekend and also just like a very, it was
*  like a battle fought in public to a surprising degree.
*  That was extremely exhausting to me much more than I expected.
*  I think fights are generally exhausting, but this one really was.
*  You know, the board did this Friday afternoon.
*  I really couldn't get much in the way of answers, but I also was just like, well, the board
*  gets to do this.
*  And so I'm going to think for a little bit about what I want to do, but I'll try to find
*  the blessing in disguise here.
*  And I was like, well, I, you know, my current job at OpenAI is or it was like to run a decently
*  sized company at this point.
*  And the thing I'd always liked the most was just getting to like work on work with the
*  researchers.
*  And I was like, yeah, I can just go do like a very focused HDI research effort.
*  And I got excited about that.
*  Didn't even occur to me at the time to like possibly that this was all going to get undone.
*  This was like Friday afternoon.
*  So you've accepted the death of this preview.
*  Very quickly.
*  Like within, you know, I mean, I went through like a little period of confusion and rage,
*  but very quickly.
*  And by Friday night, I was like talking to people about what was going to be next.
*  And I was excited about that.
*  I think it was Friday night evening for the first time that I heard from the exec team
*  here, which is like, hey, we're going to like fight this.
*  And, you know, we think whatever.
*  And then I went to bed just still being like, OK, excited, like onward.
*  Were you able to sleep?
*  Not a lot.
*  It was one of the weird things was this like period of four, four and a half days where
*  sort of didn't sleep much, didn't eat much and still kind of had like a surprising amount
*  of energy.
*  You learn like a weird thing about adrenaline in wartime.
*  You kind of accepted the death of, you know, this baby opening.
*  And I was excited for the new thing.
*  I was just like, OK, this was crazy, but whatever.
*  It's a very good coping mechanism.
*  And then Saturday morning, two of the board members called and said, hey, we destabilize.
*  We didn't mean to destabilize things.
*  We don't restore a lot of value here.
*  You know, can we talk about you coming back?
*  And I immediately didn't want to do that.
*  But I thought a little more and I was like, well, I really care about the people here,
*  the partners, shareholders, like all of the.
*  I love this company.
*  And so I thought about it and I was like, well, OK, but like here's the stuff I would
*  need.
*  And then the most painful time of all was over the course of that weekend.
*  I kept thinking and being told and we all kept not just me, like the whole team here
*  kept thinking while we were trying to like keep open, I stabilized while the whole world
*  was trying to break it apart, people trying to recruit, whatever.
*  We kept being told like, all right, we're almost done.
*  We're almost done.
*  We just need like a little bit more time.
*  And it was this like very confusing state.
*  And then Sunday evening, when again, like every few hours, I expected that we were going
*  to be done and we're going to like figure out a way for me to return and things to go
*  back to how they were.
*  The board then appointed a new interim CEO.
*  And then I was like, I mean, that is that is that feels really bad.
*  That was the low point of the whole thing.
*  You know, I'll tell you something.
*  It felt very painful, but I felt a lot of love that whole weekend.
*  It was not other than that one moment, Sunday night, I would not characterize my emotions
*  as anger or hate.
*  But I really just like I felt a lot of love from people towards people.
*  It was like painful, but it would like the dominant emotion of the weekend was love,
*  not hate.
*  You've spoken highly of Mira Moradi that she helped, especially as you put in a tweet,
*  in the quiet moments when it counts.
*  Perhaps we could take a bit of a tangent.
*  What do you admire about Mira?
*  Well, she did a great job during that weekend in a lot of chaos.
*  But people often see leaders in the moment in like the crisis moments, good or bad.
*  But a thing I really value in leaders is how people act on a boring Tuesday at nine forty
*  six in the morning and in just sort of the normal drudgery of the day to day.
*  How someone shows up in a meeting, the quality of the decisions they make.
*  That was what I meant about the quiet moments.
*  Meaning like most of the work is done on a day by day in a meeting by meeting.
*  Just be present and make great decisions.
*  Yeah.
*  I mean, look, what you wanted to have wanted to spend the last 20 minutes about and I understand
*  that this is like this one very dramatic weekend.
*  But that's not really what opening eyes about opening eyes really about the other seven
*  years.
*  Well, yeah, human civilization is not about the invasion of the Soviet Union by Nazi Germany,
*  but still that's something people focus on.
*  Very, very understandable.
*  It gives us an insight into human nature, the extremes of human nature and perhaps some
*  of the damage and some of the triumphs of human civilization can happen in those moments.
*  So it's like illustrative.
*  Let me ask about Ilya.
*  Is he being held hostage in a secret nuclear facility?
*  No.
*  What about a regular secret facility?
*  No.
*  What about a nuclear non-secret facility?
*  Neither.
*  Not that either.
*  I mean, this is becoming a meme at some point.
*  You've known Ilya for a long time.
*  He's obviously in part of this drama with the board and all that kind of stuff.
*  What's your relationship with him now?
*  I love Ilya.
*  I have tremendous respect for Ilya.
*  I don't have anything I can say about his plans right now.
*  That's a question for him.
*  But I really hope we work together for certainly the rest of my career.
*  He's a little bit younger than me.
*  Maybe he works a little bit longer.
*  There's a meme that he saw something.
*  He maybe saw AGI and that gave him a lot of worry internally.
*  What did Ilya see?
*  Ilya has not seen AGI.
*  None of us have seen AGI.
*  We've not built AGI.
*  I do think one of the many things that I really love about Ilya is he takes AGI and the safety
*  concerns broadly speaking, including things like the impact this is going to have on society
*  very seriously.
*  As we continue to make significant progress, Ilya is one of the people that I've spent
*  the most time over the last couple of years talking about what this is going to mean,
*  what we need to do to ensure we get it right, to ensure that we succeed at the mission.
*  I did not see AGI, but Ilya is a credit to humanity in terms of how much he thinks and
*  worries about making sure we get this right.
*  I had a bunch of conversations with him in the past.
*  I think when he talks about technology, he's always doing this long-term thinking type
*  of thing.
*  He's not thinking about what this is going to be in a year.
*  He's thinking about in 10 years.
*  He's thinking from first principles like, okay, if the scales, what are the fundamentals
*  here?
*  Where is this going?
*  That's a foundation for them thinking about all the other safety concerns and all that
*  kind of stuff, which makes him a really fascinating human to talk with.
*  Do you have any idea why he's been kind of quiet?
*  He's just doing some soul searching?
*  Again, I don't want to speak for Ilya.
*  I think that you should ask him that.
*  He's definitely a thoughtful guy.
*  I think I kind of think Ilya is always on the soul search in a really good way.
*  Yes.
*  Yeah.
*  Also, he appreciates the power of silence.
*  Also, I'm told he can be a silly guy, which I've never seen that side of him.
*  It's very sweet when that happens.
*  I've never witnessed a silly Ilya, but I look forward to that as well.
*  I was at a dinner party with him recently and he was playing with a puppy.
*  He was in a very silly mood, very endearing.
*  I was thinking, oh man, this is not the side of Ilya that the world sees the most.
*  Just to wrap up this whole saga, are you feeling good about the board structure about all of
*  this and where it's moving?
*  I feel great about the new board.
*  In terms of the structure of OpenAI, one of the board's tasks is to look at that and
*  see where we can make it more robust.
*  We wanted to get new board members in place first, but we clearly learned a lesson about
*  structure throughout this process.
*  I don't have, I think, super deep things to say.
*  It was a crazy, very painful experience.
*  I think it was like a perfect storm of weirdness.
*  It was like a preview for me of what's going to happen as the stakes get higher and higher
*  in the need that we have robust governance structures and processes and people.
*  I am kind of happy it happened when it did, but it was a shockingly painful thing to go
*  through.
*  Did it make you be more hesitant in trusting people?
*  Yes.
*  Just on a personal level?
*  Yes.
*  I think I'm an extremely trusting person.
*  I've always had a life philosophy of don't worry about all of the paranoia, don't worry
*  about the edge cases.
*  You get a little bit screwed in exchange for getting to live with your guard down.
*  This was so shocking to me.
*  I was so caught off guard that it has definitely changed.
*  I really don't like this.
*  It's definitely changed how I think about default trust of people and planning for the
*  bad scenarios.
*  You've got to be careful with that.
*  Are you worried about becoming a little too cynical?
*  I'm not worried about becoming too cynical.
*  I think I'm the extreme opposite of a cynical person, but I'm worried about just becoming
*  less of a default trusting person.
*  I'm actually not sure which mode is best to operate in for a person who's developing AGI.
*  Trusting or untrusting.
*  It's an interesting journey you're on.
*  In terms of structure, I'm more interested on the human level.
*  Why do you surround yourself with humans that are building cool shit, but also are
*  making wise decisions?
*  The more money you start making, the more power the thing has, the weirder people get.
*  I think you could make all kinds of comments about the board members and the level of trust
*  I should have had there or how I should have done things differently.
*  In terms of the team here, I think you'd have to give me a very good grade on that one.
*  I have enormous gratitude and trust and respect for the people that I work with every day.
*  I think being surrounded with people like that is really important.
*  Our mutual friend, Elon, sued OpenAI.
*  What is the essence of what he's criticizing?
*  To what degree does he have a point?
*  To what degree is he wrong?
*  I don't know what it's really about.
*  We started off just thinking we were going to be a research lab and having no idea about
*  how this technology was going to go.
*  It's hard to, because it was only seven or eight years ago, it's hard to go back and
*  really remember what it was like then.
*  But this was before language models were a big deal.
*  This was before we had any idea about an API or selling access to a chat bot.
*  This was before we had any idea we were going to productize at all.
*  So we're like, we're just going to try to do research and we don't really know what
*  we're going to do with that.
*  I think with many fundamentally new things, you start fumbling through the dark and you
*  make some assumptions, most of which turn out to be wrong.
*  Then it became clear that we were going to need to do different things and also have
*  huge amounts more capital.
*  So we said, okay, well, the structure doesn't quite work for that.
*  How do we patch the structure?
*  Then patch it again and patch it again and you end up with something that does look kind
*  of eyebrow raising to say the least.
*  But we got here gradually with, I think, reasonable decisions at each point along the way.
*  It doesn't mean I wouldn't do it totally differently if we could go back now with an oracle, but
*  you don't get the oracle at the time.
*  But anyway, in terms of what Elon's real motivations here are, I don't know.
*  To the degree you remember, what was the response that OpenAI gave in the blog post?
*  Can you summarize it?
*  Oh, we just said like, you know, Elon said this set of things.
*  Here's our characterization or here's the sort of, not our characterization, here's
*  like the characterization of how this went down.
*  We tried to like not make it emotional and just sort of say like, here's the history.
*  I do think there's a degree of mischaracterization from Elon here about one of the points he
*  just made, which is the degree of uncertainty he had at the time.
*  You guys are a bunch of like a small group of researchers crazily talking about AGI when
*  everybody's laughing at that thought.
*  Wasn't that long ago Elon was crazily talking about launching rockets when people were
*  laughing at that thought?
*  So I think he'd have more empathy for this.
*  I mean, I do think that there's personal stuff here.
*  That there was a split that OpenAI and a lot of amazing people here chose to part ways
*  with Elon.
*  So there's a personal-
*  Elon chose to part ways.
*  Can you describe that exactly?
*  The choosing to part ways?
*  He thought OpenAI was going to fail.
*  He wanted total control to sort of turn it around.
*  We wanted to keep going in the direction that now has become OpenAI.
*  He also wanted Tesla to be able to build an AGI effort.
*  At various times he wanted to make OpenAI into a for-profit company that he could have
*  control of or have it merge with Tesla.
*  We didn't want to do that and he decided to leave, which that's fine.
*  So you're saying, and that's one of the things that the blog post says, is that he wanted
*  OpenAI to be basically acquired by Tesla in the same way that, or maybe something similar
*  or maybe something more dramatic than the partnership with Microsoft?
*  My memory is the proposal was just like, yeah, get acquired by Tesla and have Tesla have
*  full control over it.
*  I'm pretty sure that's what it was.
*  So what is the word open in OpenAI mean to Elon at the time?
*  Ilya has talked about this in the email exchanges and all this kind of stuff.
*  What does it mean to you at the time?
*  What does it mean to you now?
*  I would definitely pick a different- speaking of going back with an Oracle, I'd pick a different
*  name.
*  One of the things that I think OpenAI is doing that is the most important of everything that
*  we're doing is putting powerful technology in the hands of people for free as a public
*  good.
*  We don't run ads on our free version.
*  We don't monetize it in other ways.
*  We just say it's part of our mission.
*  We want to put increasingly powerful tools in the hands of people for free and get them
*  to use them.
*  I think that kind of open is really important to our mission.
*  I think if you give people great tools and teach them to use them or don't even teach
*  them, they'll figure it out and let them go build an incredible future for each other
*  with that, that's a big deal.
*  So if we can keep putting free or low cost or free and low cost powerful AI tools out
*  in the world, I think it's a huge deal for how we fulfill the mission.
*  Open source or not, yeah, I think we should open source some stuff and not other stuff.
*  It does become this religious battle line where nuance is hard to have, but I think
*  nuance is the right answer.
*  So he said, change your name to closed AI and I'll drop the lawsuit.
*  I mean, is it going to become this battleground in the land of memes about the name?
*  I think that speaks to the seriousness with which Elon means the lawsuit.
*  And yeah, I mean, that's like an astonishing thing to say, I think.
*  I don't think the lawsuit, maybe correct me if I'm wrong, but I don't think the lawsuit
*  is legally serious.
*  It's more to make a point about the future of AGI and the company that's currently leading
*  the way.
*  Look, I mean, Grok had not open sourced anything until people pointed out it was a little bit
*  hypocritical and then he announced that Grok will open source things this week.
*  I don't think open source versus not is what this is really about for him.
*  Well, we'll talk about open source and not.
*  I do think maybe criticizing the competition is great.
*  Just talking a little shit, that's great.
*  But friendly competition versus like, I personally hate lawsuits.
*  Look, I think this whole thing is like unbecoming of a builder and I respect Elon as one of
*  the great builders of our time.
*  And I know he knows what it's like to have haters attack him and it makes me extra sad
*  he's doing it to us.
*  Yeah, he's one of the greatest builders of all time, potentially the greatest builder
*  of all time.
*  It makes me sad.
*  I think it makes a lot of people sad.
*  There's a lot of people who've really looked up to him for a long time and I said in some
*  interview or something that I missed the old Elon and the number of messages I got being
*  that exactly encapsulates how I feel.
*  I think you should just win.
*  You should just make X grok beat GPT and then GPT beats grok and it's just a competition
*  and that's beautiful for everybody.
*  But on the question of open source, do you think there's a lot of companies playing with
*  this idea?
*  It's quite interesting.
*  I would say Metta, surprisingly, has led the way on this or like at least took the first
*  step in the game of chess of like really open sourcing the model.
*  Of course, it's not the state of the art model, but open sourcing llama and Google is flirting
*  with the idea of open sourcing a smaller version.
*  What are the pros and cons of open sourcing?
*  Have you played around with this idea?
*  Yeah, I think there is definitely a place for open source models, particularly smaller
*  models that people can run locally.
*  I think there's huge demand for.
*  I think there will be some open source models.
*  There will be some closed source models.
*  It won't be unlike other ecosystems in that way.
*  I listened to All In podcast talking about this loss and all that kind of stuff and they
*  were more concerned about the precedent of going from nonprofit to this cap for profit.
*  What precedent this sets for other startups?
*  I would heavily discourage any startup that was thinking about starting as a nonprofit
*  and adding like a for-profit arm later.
*  I'd heavily discourage them from doing that.
*  I don't think we'll set a precedent here.
*  Okay, so most startups should go just...
*  For sure.
*  And again, if we knew what was going to happen, we would have done that too.
*  Well, like in theory, if you like dance beautifully here, there's like some tax incentives or
*  whatever.
*  I don't think that's like how most people think about these things.
*  It's not possible to save a lot of money for startup if you do it this way.
*  No, I think there's like laws that would make that pretty difficult.
*  Where do you hope this goes with Elon?
*  This tension, this dance, where do you hope this...
*  Like if we go one, two, three years from now, your relationship with him on a personal level
*  too, like friendship, friendly competition, just all this kind of stuff.
*  Yeah, I really respect Elon.
*  And I hope that years in the future, we have an amicable relationship.
*  Yeah, I hope you guys have an amicable relationship like this month.
*  And just compete and win and explore these ideas together.
*  I do suppose there's competition for talent or whatever, but it should be friendly competition.
*  Just build cool shit.
*  And Elon is pretty good at building cool shit, but so are you.
*  So speaking of cool shit, Sora, there's like a million questions I could ask.
*  First of all, it's amazing.
*  It truly is amazing on a product level, but also just on a philosophical level.
*  So let me just, technical slash philosophical, ask, what do you think it understands about
*  the world more or less than GPT-4, for example?
*  The world model, when you train on these patches versus language tokens.
*  I think all of these models understand something more about the world model than most of us
*  give them credit for.
*  Because they're also very clear things they just don't understand or don't get right.
*  It's easy to look at the weaknesses, see through the veil and say, this is all fake.
*  But it's not all fake, it's just some of it works and some of it doesn't work.
*  I remember when I started first watching Sora videos and I would see a person walk in front
*  of something for a few seconds and occlude it and then walk away and the same thing was
*  still there.
*  I was like, oh, that's pretty good.
*  There's examples where the underlying physics looks so well represented over a lot of steps
*  in a sequence.
*  It's like, oh, this is quite impressive.
*  But fundamentally, these models are just getting better and that will keep happening.
*  If you look at the trajectory from Dolly 1 to 2 to 3 to Sora, there are a lot of people
*  that were dunked on each version saying, it can't do this, it can't do that, and look
*  at it now.
*  The thing you just mentioned is kind of with occlusions, is basically modeling the physics,
*  the three-dimensional physics of the world sufficiently well to capture those kinds of
*  things.
*  Well.
*  Or like, yeah, maybe you can tell me, in order to deal with occlusions, what does the world
*  model need to?
*  Yeah, so what I would say is it's doing something to deal with occlusions really well.
*  What I represent that it has like a great underlying 3D model of the world, it's a little
*  bit more of a stretch.
*  But can you get there through just these kinds of two-dimensional training data approaches?
*  It looks like this approach is going to go surprisingly far.
*  I don't want to speculate too much about what limits it will surmount and which it won't,
*  but...
*  What are some interesting limitations of the system that you've seen?
*  I mean, there's been some fun ones you've posted.
*  There's all kinds of fun.
*  I mean, like, you know, cats sprout in an extra limb at random points in a video.
*  Pick what you want, but there's still a lot of problems, still a lot of weaknesses.
*  Do you think that's a fundamental flaw of the approach?
*  Or is it just, you know, bigger model or better, like, technical details or better data, more
*  data is going to solve the cats sprouting?
*  I would say yes to both.
*  Like I think there is something about the approach which just seems to feel different
*  from how we think and learn and whatever.
*  And then also I think it'll get better with skill.
*  Like I mentioned, LLMs have tokens, text tokens, and Sora has visual patches.
*  So it converts all visual data, diverse kinds of visual data, videos and images into patches.
*  Is the training to the degree you can say fully self-supervised or is there some manual
*  labeling going on?
*  Like what's the involvement of humans in all this?
*  I mean, without saying anything specific about the Sora approach, we use lots of human data
*  in our work.
*  But not internet scale data.
*  So lots of humans.
*  Lots is a complicated word, Sam.
*  I think lots is a fair word in this case.
*  It doesn't because to me, lots, like listen, I'm an introvert and when I hang out with
*  like three people, that's a lot of people.
*  Four people, that's a lot.
*  But I suppose you mean more than more than three people work on labeling the data for
*  these models.
*  Yeah.
*  Okay.
*  There's a lot of self-supervised learning because what you mentioned in the technical
*  report is internet scale data.
*  That's another beautiful, it's like poetry.
*  So it's a lot of data that's not human label.
*  It's like it's self-supervised in that way.
*  And then the question is how much data is there on the internet that could be used that
*  is conducive to this kind of self-supervised way?
*  If only we knew the details of the self-supervised.
*  Have you considered opening it up a little more, details?
*  We have.
*  You mean for Sora specifically?
*  Sora specifically, that's right.
*  Because it's so interesting that like can this, can the same magic of LLMs now start
*  moving towards visual data?
*  And what does that take to do that?
*  I mean it looks to me like yes, but we have more work to do.
*  Sure.
*  But what are the dangers?
*  Why are you concerned about releasing the system?
*  What are some possible dangers of this?
*  I mean frankly speaking, one thing we have to do before releasing the system is just
*  like get it to work at a level of efficiency that will deliver the scale people are going
*  to want from this.
*  So I don't want to like downplay that.
*  And there's still a ton, ton of work to do there.
*  You can imagine like issues with deepfakes, misinformation.
*  Like we try to be a thoughtful company about what we put out into the world and it doesn't
*  take much thought to think about the ways this can go badly.
*  There's a lot of tough questions here.
*  You're dealing in a very tough space.
*  Do you think training AI should be or is fair use under copyright law?
*  I think the question behind that question is do people who create valuable data deserve
*  to have some way that they get compensated for use of it?
*  And that I think the answer is yes.
*  I don't know yet what the answer is.
*  People have proposed a lot of different things.
*  We've tried some different models.
*  But you know if I'm like an artist for example, A, I would like to be able to opt out of people
*  generating art in my style.
*  And B, if they do generate art in my style, I'd like to have some economic model associated
*  with that.
*  Yeah, it's that transition from CDs to Napster to Spotify.
*  I have to figure out some kind of model.
*  The model changes, but people have got to get paid.
*  Well, there should be some kind of incentive if you zoom out even more for humans to keep
*  doing cool shit.
*  Everything I worry about.
*  Humans are going to do cool shit and society is going to find some way to reward it.
*  That seems pretty hardwired.
*  We want to create, we want to be useful, we want to achieve status in whatever way.
*  That's not going anywhere I don't think.
*  But the reward might not be monetary, financial.
*  It might be like fame and celebration of other cool...
*  Maybe financial in some other way.
*  Again, I don't think we've seen the last evolution of how the economic system is going to work.
*  Yeah, but artists and creators are worried.
*  When they see Sora, they're like, holy shit.
*  Sure.
*  Artists were also super worried when photography came out.
*  And then photography became a new art form and people made a lot of money taking pictures.
*  I think things like that will keep happening.
*  People will use the new tools in new ways.
*  If you just look on YouTube or something like this, how much of that will be using Sora-like
*  AI-generated content, do you think, in the next five years?
*  People talk about how many jobs they are going to do in five years.
*  And the framework that people have is what percentage of current jobs are just going
*  to be totally replaced by some AI doing the job.
*  The way I think about it is not what percentage of jobs AI will do, but what percentage of
*  tasks will AI do and over what time horizon.
*  So if you think of all of the five-second tasks in the economy, the five-minute tasks,
*  the five-hour tasks, maybe even the five-day tasks, how many of those can AI do?
*  And I think that's a way more interesting, impactful, important question than how many
*  jobs AI can do because it is a tool that will work at increasing levels of sophistication
*  and over longer and longer time horizons for more and more tasks and let people operate
*  at a higher level of abstraction.
*  So maybe people are way more efficient at the job they do.
*  And at some point, that's not just a quantitative change, but it's a qualitative one, too, about
*  the kinds of problems you can keep in your head.
*  I think that for videos on YouTube, it'll be the same.
*  Many videos, maybe most of them, will use AI tools in the production, but they'll still
*  be fundamentally driven by a person thinking about it, putting it together, doing parts
*  of it, sort of directing it and running it.
*  Yeah, it's so interesting.
*  I mean, it's scary, but it's interesting to think about.
*  I tend to believe that humans like to watch other humans or other human life.
*  Humans really care about other humans a lot.
*  If there's a cooler thing that's better than a human, humans care about that for like two
*  days and then they go back to humans.
*  That seems very deeply wired.
*  It's the whole chess thing.
*  But now everybody keeps playing chess.
*  Let's ignore the elephant in the room that humans are really bad at chess relative to
*  AI systems.
*  We still run races and cars are much faster.
*  There's like a lot of examples.
*  Maybe you'll just be tooling like in the Adobe Suite type of way where you can just make
*  videos much easier and all that kind of stuff.
*  Listen, I hate being in front of the camera.
*  If I can figure out a way to not be in front of the camera, I would love it.
*  Unfortunately, it'll take a while.
*  Like that, generating faces.
*  It's getting there, but generating faces in video format is tricky when it's specific
*  people versus generating people.
*  Let me ask you about GPT-4.
*  There's so many questions.
*  First of all, also amazing.
*  Looking back, it'll probably be this kind of historic pivotal moment with 3-5 and 4
*  with Chad GPT.
*  Maybe 5 will be the pivotal moment, I don't know.
*  Hard to say that looking forwards.
*  We never know.
*  That's the annoying thing about the future.
*  It's hard to predict.
*  But for me, looking back, GPT-4, Chad GPT is pretty damn impressive.
*  Historically impressive.
*  Allow me to ask, what's been the most impressive capabilities of GPT-4 to you and GPT-4 Turbo?
*  I think it kind of sucks.
*  Typical human also.
*  Got used to an awesome thing.
*  No, I think it is an amazing thing.
*  But relative to where we need to get to and where I believe we will get to, at the time
*  of GPT-3, people were like, oh, this is amazing.
*  This is this marvel of technology.
*  And it is.
*  It was.
*  But now we have GPT-4 and look at GPT-3 and you're like, that's unimaginably horrible.
*  I expect that the delta between five and four will be the same as between four and three.
*  And I think it is our job to live a few years in the future and remember that the tools
*  we have now are going to kind of suck looking backwards at them.
*  And that's how we make sure the future is better.
*  What are the most glorious ways in that GPT-4 sucks?
*  Meaning what are the best things it can do?
*  What are the best things it can do and the limits of those best things that allow you
*  to say it sucks, therefore gives you inspiration and hope for the future?
*  You know, one thing I've been using it for more recently is sort of like a brainstorming
*  partner.
*  Yep.
*  I'm always for that.
*  And there's a glimmer of something amazing in there.
*  I don't think it gets, you know, when people talk about it, what it does, they're like,
*  it helps me code more productively, it helps me write more faster and better, it helps
*  me translate from this language to another, all these like amazing things.
*  But there's something about the like kind of creative brainstorming partner.
*  I need to come up with a name for this thing.
*  I need to like think about this problem in a different way.
*  I'm not sure what to do here.
*  That I think like gives a glimpse of something I hope to see more of.
*  One of the other things that you can see like a very small glimpse of is when I can
*  help on longer horizon tasks, you know, break down something in multiple steps, maybe like
*  execute some of the steps, search the internet, write code, whatever, put that together.
*  When that works, which is not very often, it's like very magical.
*  The iterative back and forth with a human.
*  It works a lot for me.
*  What do you mean?
*  Iterative back and forth with a human, it can get more often.
*  It can go do like a 10 step problem on its own.
*  Oh, doesn't work for that too often.
*  Sometimes.
*  Add multiple layers of abstraction or do you mean just sequential?
*  Both like, you know, to break it down and then do things at different layers of abstraction
*  and put them together.
*  Look, I don't want to, I don't want to like downplay the accomplishment of GPT-4.
*  But I don't want to overstate it either.
*  And I think this point that we are on an exponential curve, we will look back relatively soon at
*  GPT-4, like we look back at GPT-3 now.
*  That said, I mean, chat GPT was a transition to where people like started to believe it.
*  There was a kind of, there is an uptick of believing.
*  Not internally at OpenAI perhaps.
*  There's believers here, but when you think about global.
*  And in that sense, I do think it'll be a moment where a lot of the world went from not believing
*  to believing.
*  That was more about the chat GPT interface than the, and by the interface and product,
*  I also mean the post-training of the model and how we tune it to be helpful to you and
*  how to use it than the underlying model itself.
*  How much of those two, each of those things are important?
*  The underlying model and the RLHF or something of that nature that tunes it to be more compelling
*  to the human, more effective and productive for the human.
*  I mean, they're both super important, but the RLHF, the post-training step, the little
*  wrapper of things that from a compute perspective, little wrapper of things that we do on top
*  of the base model, even though it's a huge amount of work.
*  That's really important to say nothing of the product that we build around it.
*  In some sense, we did have to do two things.
*  We had to invent the underlying technology and then we had to figure out how to make
*  it into a product people would love, which is not just about the actual product work
*  itself, but this whole other step of how you align and make it useful.
*  And how you make the scale work where a lot of people can use it at the same time, all
*  that kind of stuff.
*  And that.
*  But that was like a known difficult thing.
*  We knew we were going to have to scale it up.
*  We had to go do two things that had never been done before that were both, I would say,
*  quite significant achievements.
*  And then a lot of things like scaling it up that other companies have had to do before.
*  How does the context window of going from 8K to 128K tokens compare from GPT-4 to GPT-4
*  Turbo?
*  Most people don't need all the way to 128 most of the time, although if we dream into
*  the distant future, we'll have like way distant future, we'll have like context length of
*  several billion, you will feed in all of your information, all of your history over time
*  and it'll just get to know you better and better and that'll be great.
*  For now, the way people use these models, they're not doing that.
*  And people sometimes post in a paper or a significant fraction of a code repository,
*  but most usage of the models is not using the long context most of the time.
*  I like that this is your I have a dream speech.
*  One day you'll be judged by the full context of your character or of your whole lifetime.
*  That's interesting.
*  So like that's part of the expansion that you're hoping for is a greater and greater
*  context.
*  I saw this internet clip once.
*  I'm going to get the numbers wrong, but it was like Bill Gates talking about the amount
*  of memory on some early computer, maybe 64k, maybe 640k, something like that.
*  Most of it was used for the screen buffer.
*  And he just couldn't seem genuine as couldn't imagine that the world would eventually need
*  gigabytes of memory in a computer or terabytes of memory in a computer.
*  And you always do or you always do just need to like follow the exponential of technology
*  And we're going to like, we will find out how to use better technology.
*  So I can't really imagine what it's like right now for context links to go out to the billions
*  someday and they might not literally go there, but effectively it'll feel like that.
*  But I know we'll use it and really not want to go back once we have it.
*  Yeah, even saying billions 10 years from now might seem dumb because it'll be like trillions
*  upon trillions.
*  Sure.
*  But it's going to be some kind of breakthrough that will effectively feel like infinite context.
*  But even 120, I have to be honest, I haven't pushed it to that degree.
*  Maybe putting in entire books or like parts of books and so on, papers.
*  What are some interesting use cases of GPT-4 that you've seen?
*  The thing that I find most interesting is not any particular use case that we can talk
*  about those, but it's people who kind of like, this is mostly younger people, but people
*  who use it as like their default start for any kind of knowledge work task.
*  And it's the fact that it can do a lot of things reasonably well.
*  You can use GPTV, you can use it to help you write code, you can use it to help you do
*  search, you can use it to like edit a paper.
*  The most interesting thing to me is the people who just use it as the start of their workflow.
*  I do as well for many things.
*  Like I use it as a reading partner for reading books.
*  It helps me think, help me think through ideas, especially when the books are classics.
*  It's really well written about and it actually is as, I find it often to be significantly
*  better than even like Wikipedia on well covered topics.
*  It's somehow more balanced and more nuanced.
*  Maybe it's me, but it inspires me to think deeper than a Wikipedia article does.
*  I'm not exactly sure what that is.
*  You mentioned like this collaboration.
*  I'm not sure where the magic is.
*  If it's in here or if it's in there or if it's somewhere in between.
*  I'm not sure.
*  But one of the things that concerns me for knowledge tasks when I start with GPT is I'll
*  usually have to do fact checking after.
*  Like check that it didn't come up with fake stuff.
*  How do you figure that out?
*  That GPT can come up with fake stuff that sounds really convincing.
*  So how do you ground it in truth?
*  That's obviously an area of intense interest for us.
*  I think it's going to get a lot better with upcoming versions, but we'll have to work
*  on it and we're not going to have it all solved this year.
*  Well, the scary thing is like as it gets better, you start not doing the fact checking more
*  and more, right?
*  I'm of two minds about that.
*  I think people are like much more sophisticated users of technology than we often give them
*  credit for.
*  And people seem to really understand that GPT, any of these models hallucinate some
*  of the time and if it's mission critical, you got to check it.
*  Journalists don't seem to understand that.
*  I've seen journalists half-assedly just using GPT for it.
*  Of the long list of things I'd like to dunk on journalists for, this is not my top criticism
*  of them.
*  Well, I think the bigger criticism is perhaps the pressures and the incentives of being
*  a journalist is that you have to work really quickly and this is a shortcut.
*  I would love our society to incentivize like...
*  I would too.
*  Like a journalistic efforts that take days and weeks and rewards great in depth journalism.
*  Also journalism that represents stuff in a balanced way where it's like celebrates people
*  while criticizing them even though the criticism is the thing that gets clicks and making shit
*  up also gets clicks and headlines that mischaracterize completely.
*  I'm sure you have a lot of people dunking on...
*  Well, all that drama probably got a lot of clicks.
*  Probably did.
*  And that's a bigger problem about human civilization.
*  I'd love to see solved.
*  This is where we celebrate a bit more.
*  You've given Chad GPT the ability to have memories.
*  You've been playing with that about previous conversations.
*  And also the ability to turn off memory.
*  I wish I could do that sometimes.
*  Just turn on and off depending.
*  I guess sometimes alcohol can do that, but not optimally, I suppose.
*  What have you seen through that, like playing around with that idea of remembering conversations
*  and not...
*  We're very early in our explorations here, but I think what people want, or at least
*  what I want for myself is a model that gets to know me and gets more useful to me over
*  time.
*  This is an early exploration.
*  I think it's like a lot of other things to do, but that's where we'd like to head.
*  You'd like to use a model and over the course of your life or use a system.
*  There'll be many models.
*  And over the course of your life, it gets better and better.
*  How hard is that problem?
*  Because right now it's more like remembering little factoids and preferences and so on.
*  What about remembering...
*  Don't you want GPT to remember all the shit you went through in November and all the drama?
*  Yeah.
*  Because right now you're clearly blocking it out a little bit.
*  It's not just that I want it to remember that.
*  I want it to integrate the lessons of that.
*  And remind me in the future what to do differently or what to watch out for.
*  And we all gain from experience over the course of our lives, varying degrees.
*  And I'd like my AI agent to gain with that experience too.
*  So if we go back and let ourselves imagine that trillions and trillions of context length,
*  if I can put every conversation I've ever had with anybody in my life in there,
*  if I can have all of my emails input out, like all of my input output in the context window
*  every time I ask a question, that'd be pretty cool, I think.
*  Yeah, I think that would be very cool.
*  People sometimes will hear that and be concerned about privacy.
*  Is there...
*  What do you think about that aspect of it?
*  The more effective the AI becomes at really integrating all the experiences and all the data
*  that happened to you, giving you advice?
*  I think the right answer there is just user choice.
*  Anything I want stricken from the record from my AI agent, I want to be able to take out.
*  If I don't want it to remember anything, I want that too.
*  You and I may have different opinions about where on that privacy utility trade off for
*  our own AI we want to be, which is totally fine.
*  But I think the answer is just really easy user choice.
*  But there should be some high level of transparency from a company about the user choice.
*  Because sometimes companies in the past have been kind of shady about like, it's kind of
*  presumed that we're collecting all your data and we're using it for a good reason, for
*  advertisements and so on.
*  But there's not a transparency about the details of that.
*  That's totally true.
*  You mentioned earlier that I'm blocking out the November stuff.
*  I'm just teasing you.
*  I mean, I think it was a very traumatic thing and it did immobilize me for a long period
*  of time.
*  Definitely the hardest work that I've had to do was just keep working that period.
*  Because I had to try to come back in here and put the pieces together while I was just
*  in sort of shock and pain.
*  Nobody really cares about that.
*  I mean, the team gave me a pass and I was not working at my normal level.
*  But there was a period where I was just like, it was really hard to have to do both.
*  But I kind of woke up one morning and I was like, this was a horrible thing that happened
*  to me.
*  I think I could just feel like a victim forever.
*  Or I can say this is like the most important work I'll ever touch in my life and I need
*  to get back to it.
*  And it doesn't mean that I've repressed it because sometimes I like wake from the middle
*  of the night thinking about it.
*  But I do feel like an obligation to keep moving forward.
*  Well, that's beautifully said, but there could be some lingering stuff in there.
*  Like what I would be concerned about is that trust thing that you mentioned.
*  That being paranoid about people as opposed to just trusting everybody or most people
*  like using your gut.
*  It's a tricky dance.
*  For sure.
*  I mean, because I've seen in my part-time explorations, I've been diving deeply into
*  the Zelensky administration and the Putin administration and the dynamics there in wartime
*  in a very highly stressful environment.
*  And what happens is distrust and you isolate yourself both and you start to not see the
*  world clearly.
*  And that's a concern.
*  That's a human concern.
*  You seem to have taken an astride and kind of learned the good lessons and felt the love
*  and let the love energize you, which is great, but still can linger in there.
*  There's just some questions I would love to ask of your intuition about what's GPT able
*  to do and not.
*  So it's allocating approximately the same amount of compute for each token it generates.
*  Is there room there in this kind of approach to slower thinking, sequential thinking?
*  I think there will be a new paradigm for that kind of thinking.
*  Will it be similar like architecturally as what we're seeing now with LLMs?
*  Is it a layer on top of the LLMs?
*  I can imagine many ways to implement that.
*  I think that's less important than the question you were getting at, which is, do we need
*  a way to do a slower kind of thinking where the answer doesn't have to get like, you know,
*  it's like, I guess like spiritually you could say that you want an AI to be able to think
*  harder about a harder problem and answer more quickly about an easier problem.
*  And I think that will be important.
*  Is that like a human thought that we're just having?
*  You should be able to think hard.
*  Is that wrong intuition?
*  I suspect that's a reasonable intuition.
*  Interesting.
*  So it's not possible once the GPT gets like GPT-7, we'll just be instantaneously be able
*  to see, you know, here's the proof of Fermat's Theorem.
*  It seems to me like you want to be able to allocate more compute to harder problems.
*  Like it seems to me that a system knowing, if you ask a system like that, proof Fermat's
*  Last Theorem versus what's today's date, unless it already knew and had memorized the answer
*  to the proof, assuming it's got to go figure that out, seems like that will take more compute.
*  But can it look like basically LLM talking to itself, that kind of thing?
*  Maybe.
*  I mean, there's a lot of things that you could imagine working.
*  What like, what the right or the best way to do that will be?
*  We don't know.
*  This does make me think of the mysterious, the lore behind Q-Star.
*  What's this mysterious Q-Star project?
*  Is it also in the same nuclear facility?
*  There is no nuclear facility.
*  That's what a person in the nuclear facility always says.
*  I would love to have a secret nuclear facility.
*  There isn't one.
*  All right.
*  Maybe someday.
*  Someday?
*  All right.
*  OpenAI is not a good company at keeping secrets.
*  It would be nice, you know, we're like, been plagued by a lot of leaks and it would be
*  nice if we were able to have something like that.
*  Can you speak to what Q-Star is?
*  We are not ready to talk about that.
*  See, but an answer like that means there's something to talk about.
*  It's very mysterious, Sam.
*  I mean, we work on all kinds of research.
*  We have said for a while that we think better reasoning in these systems is an important
*  direction that we'd like to pursue.
*  We haven't cracked the code yet.
*  We're very interested in it.
*  Is there going to be moments, Q-Star or otherwise, where there's going to be leaps similar to
*  that at GPT where you're like...
*  That's a good question.
*  What do I think about that?
*  It's interesting.
*  To me, it all feels pretty continuous.
*  Right.
*  This is kind of a theme that you're saying is there's a gradual...
*  You're basically gradually going up an exponential slope, but from an outsider's perspective,
*  for me, just watching it, it does feel like there's leaps.
*  But to you, there isn't.
*  I do wonder if we should have...
*  Part of the reason that we deploy the way we do is that we think...
*  We call it iterative deployment.
*  Rather than go build in secret until we got all the way to GPT-5, we decided to talk about
*  GPT-1, 2, 3 and 4.
*  Part of the reason there is I think AI and surprise don't go together.
*  Also, the world, people, institutions, whatever you want to call it, need time to adapt and
*  think about these things.
*  I think one of the best things that OpenAI has done is this strategy.
*  We get the world to pay attention to the progress, to take AGI seriously, to think about what
*  systems and structures and governance we want in place before we're under the gun and have
*  to make a rest decision.
*  I think that's really good.
*  But the fact that people like you and others say you still feel like there are these leaps
*  makes me think that maybe we should be doing our releasing even more iteratively.
*  I don't know what that would mean.
*  I don't have any answer ready to go.
*  But our goal is not to have shock updates to the world.
*  The opposite.
*  Yeah, for sure.
*  More iterative would be amazing.
*  I think that's just beautiful for everybody.
*  But that's what we're trying to do.
*  That's our state of the strategy.
*  And I think we're somehow missing the mark.
*  So maybe we should think about releasing GPT-5 in a different way or something like that.
*  Yeah.
*  4.71, 4.72.
*  But people tend to like to celebrate.
*  People celebrate birthdays.
*  I don't know if you know humans, but they kind of have these milestones.
*  I do know some humans.
*  People do like milestones.
*  I totally get that.
*  I think we like milestones too.
*  It's like fun to say declare victory on this one and go start the next thing.
*  But yeah, I feel like we're somehow getting this a little bit wrong.
*  When is GPT-5 coming out again?
*  I don't know.
*  That's the honest answer.
*  Oh, that's the honest answer.
*  Is it blink twice if it's this year?
*  I also...
*  We will release an amazing new model this year.
*  I don't know what we'll call it.
*  So that goes to the question of like what's the way we release this thing?
*  We'll release over in the coming months many different things.
*  I think they'll be very cool.
*  I think before we talk about like a GPT-5 like model called that or not called that
*  or a little bit worse or a little bit better than what you'd expect from a GPT-5, I know
*  we have a lot of other important things to release first.
*  I don't know what to expect from GPT-5.
*  You're making me nervous and excited.
*  What are some of the biggest challenges in bottlenecks to overcome for whatever it ends
*  up being called, but let's call it GPT-5?
*  Just interesting to ask.
*  Is it on the compute side?
*  Is it on the technical side?
*  It's always all of these...
*  What's the one big unlock?
*  Is it a bigger computer?
*  Is it like a new secret?
*  Is it something else?
*  It's all of these things together.
*  The thing that OpenAI I think does really well, this is actually an original Ilya quote
*  that I'm going to butcher, but it's something like we multiply 200 medium-sized things
*  together into one giant thing.
*  There's this distributed constant innovation happening.
*  Even on the technical side.
*  Especially on the technical side.
*  Even detailed approaches, detailed aspects of everything.
*  How does that work with different disparate teams and so on?
*  How do the medium-sized things become one whole giant transformer?
*  There's a few people who have to think about putting the whole thing together, but a lot
*  of people try to keep most of the picture in their head.
*  The individual teams, individual contributors try to keep the picture.
*  At a high level.
*  Yeah.
*  You don't know exactly how every piece works, of course, but one thing I generally believe
*  is that it's sometimes useful to zoom out and look at the entire map.
*  I think this is true for a technical problem.
*  I think this is true for innovating in business.
*  Things come together in surprising ways and having an understanding of that whole picture,
*  even if most of the time you're operating in the weeds in one area, pays off with surprising
*  insights.
*  In fact, one of the things that I used to have, and I think was super valuable, was
*  I used to have a good map of all of the frontiers, or most of the frontiers in the tech industry,
*  and I could sometimes see these connections or new things that were possible that if I
*  were only deep in one area, I wouldn't be able to have the idea for because I wouldn't
*  have all the data.
*  I don't really have that much anymore.
*  I'm super deep now, but I know that it's a valuable thing.
*  You're not the man you used to be, Sam.
*  It's a very different job now than what I used to have.
*  Speaking of zooming out, let's zoom out to another cheeky thing, but profound thing perhaps
*  that you said.
*  You tweeted about needing $7 trillion.
*  I did not tweet about that.
*  I never said, like, we're raising $7 trillion, blah, blah, blah.
*  Oh, that's somebody else?
*  Yeah.
*  Oh, but you said, fuck it, maybe eight, I think.
*  Okay, I mean once there's misinformation out in the world.
*  Oh, you mean.
*  But sort of misinformation may have a foundation of insight there.
*  Look, I think compute is going to be the currency of the future.
*  I think it will be maybe the most precious commodity in the world.
*  And I think we should be investing heavily to make a lot more compute.
*  Compute is an unusual, I think it's going to be an unusual market.
*  People think about the market for like chips for mobile phones or something like that.
*  And you can say that, okay, there's 8 billion people in the world, maybe 7 billion of them
*  have phones, maybe they are 6 billion, let's say.
*  They upgrade every two years.
*  So the market per year is 3 billion systems on chip for smartphones.
*  And if you make 30 billion, you will not sell 10 times as many phones because most people
*  have one phone.
*  Well, compute is different.
*  Like, intelligence is going to be more like energy or something like that where the only
*  thing that I think makes sense to talk about is at price X, the world will use this much
*  compute and at price Y, the world will use this much compute.
*  Because if it's really cheap, I'll have it like reading my email all day, like giving
*  me suggestions about what I maybe should think about or work on and trying to cure cancer.
*  And if it's really expensive, maybe I'll only use it or will only use it to try to cure cancer.
*  So I think the world is going to want a tremendous amount of compute.
*  And there's a lot of parts of that that are hard.
*  Energy is the hardest part.
*  Building data centers is also hard.
*  The supply chain is harder than, of course, fabricating enough chips is hard.
*  But this seems to me where things are going.
*  Like, we're going to want an amount of compute that's just hard to reason about right now.
*  How do you solve the energy puzzle?
*  Nuclear...
*  That's what I believe.
*  Fusion?
*  That's what I believe.
*  Nuclear fusion.
*  Who's going to solve that?
*  I think Helion's doing the best work, but I'm happy there's like a race for fusion right
*  now.
*  Nuclear fission, I think, is also quite amazing.
*  And I hope as a world we can re-embrace that.
*  It's really sad to me how the history of that went.
*  And I hope we get back to it in a meaningful way.
*  So do you...
*  Part of the puzzle is nuclear fission, like nuclear reactors as we currently have them.
*  And a lot of people are terrified because of Chernobyl and so on.
*  Well, I think we should make new reactors.
*  I think it's just like it's a shame that industry kind of grown to a halt.
*  And what it just mass hysteria is how you explain the halt.
*  Yeah.
*  I don't know if you know humans, but that's one of the dangers.
*  That's one of the security threats for nuclear fission is humans seem to be really afraid
*  of it.
*  And that's something we have to incorporate into the calculus of it.
*  So we have to kind of win people over and to show how safe it is.
*  I worry about that for AI.
*  I think some things are going to go theatrically wrong with AI.
*  I don't know what the percent chance is that I eventually get shot, but it's not zero.
*  Oh, like we want to stop this from...
*  Maybe.
*  How do you decrease the theatrical nature of it?
*  You know, I've already started to hear rumblings, because I do talk to people on both sides
*  of the political spectrum.
*  I hear rumblings where it's going to be politicized.
*  AI is going to be politicized.
*  It really worries me because then it's like maybe the right is against AI and the left
*  is for AI because it's going to help the people or whatever the narrative and the formulation
*  is that really worries me.
*  And then the theatrical nature of it can be leveraged fully.
*  How do you fight that?
*  I think it will get caught up in like left versus right wars.
*  I don't know exactly what that's going to look like, but I think that's just what happens
*  with anything of consequence, unfortunately.
*  What I meant more about theatrical risks is like AI is going to have, I believe, tremendously
*  more good consequences than bad ones, but it is going to have bad ones.
*  There will be some bad ones that are bad, but not theatrical.
*  You know, like a lot more people have died of air pollution than nuclear reactors, for
*  example.
*  Most people worry more about living next to a nuclear reactor than a coal plant.
*  But something about the way we're wired is that although there's many different kinds
*  of risks we have to confront, the ones that make a good climax scene of a movie carry
*  much more weight with us than the ones that are very bad over a long period of time but
*  on a slow burn.
*  Well, that's why truth matters and hopefully AI can help us see the truth of things to
*  have balance to understand what are the actual risks, what are the actual dangers of things
*  in the world.
*  What are the pros and cons of the competition in this space and competing with Google Meta,
*  XAI, and others?
*  I think I have a pretty straightforward answer to this that maybe I can think of more nuance
*  later, but the pros seem obvious, which is that we get better products and more innovation
*  faster and cheaper and all the reasons competition is good.
*  And the con is that I think if we're not careful, it could lead to an increase in sort of an
*  arms race that I'm nervous about.
*  Do you feel the pressure of the arms race?
*  Like in some negative...
*  Definitely in some ways, for sure.
*  We spend a lot of time talking about the need to prioritize safety.
*  And I've said for like a long time that I think if you think of a quadrant of slow timelines
*  to the start of AGI, long timelines, and then a short takeoff or a fast takeoff, I think
*  short timelines, slow takeoff is the safest quadrant and the one I'd most like us to be
*  in.
*  But I do want to make sure we get that slow takeoff.
*  Part of the problem I have with this kind of slight beef with Elon is that their silos
*  are created and it as opposed to collaboration on the safety aspect of all of this, it tends
*  to go into silos and closed open source perhaps in the model.
*  Elon says at least that he cares a great deal about AI safety and is really worried about
*  it.
*  And I assume that he's not going to race on safely.
*  Yeah.
*  But collaboration here I think is really beneficial for everybody on that front.
*  Not really the thing he's most known for.
*  He is known for caring about humanity and humanity benefits from collaboration.
*  And so there's always attention and incentives and motivations.
*  In the end, I do hope humanity prevails.
*  I was thinking, someone just reminded me the other day about how the day that he got, surpassed
*  Jeff Bezos for like the richest person in the world, he tweeted a silver medal at Jeff
*  Bezos.
*  I hope we have less stuff like that as people start to work on towards AGI.
*  I agree.
*  I think Elon is a friend and he's a beautiful human being and one of the most important
*  humans ever.
*  That stuff is not good.
*  The amazing stuff about Elon is amazing and I super respect him.
*  I think we need him.
*  All of us should be rooting for him and need him to step up as a leader through this next
*  phase.
*  Yeah.
*  I hope you can have one without the other, but sometimes humans are flawed and complicated
*  and all that kind of stuff.
*  There's a lot of really great leaders through our history.
*  Yeah.
*  And we can each be the best version of ourselves and strive to do so.
*  Let me ask you, Google, with the help of search, has been dominating in the past 20 years.
*  I think it's fair to say in terms of the access, the world's access to information, how we
*  interact and so on.
*  And one of the nerve-racking things for Google, but for the entirety of people in the space,
*  is thinking about how are people going to access information?
*  Like you said, people show up to GPT as a starting point.
*  So is OpenAI going to really take on this thing that Google started 20 years ago, which
*  is how do we get to-
*  I find that boring.
*  If the question is, if we can build a better search engine than Google or whatever, then
*  sure we should go, people should use a better product.
*  But I think that would so understate what this can be.
*  Google shows you like 10 blue links, well like 13 ads and then 10 blue links, and that's
*  like one way to find information.
*  But the thing that's exciting to me is not that we can go build a better copy of Google
*  search, but that maybe there's just some much better way to help people find and act and
*  on and synthesize information.
*  Actually I think chat GPT is that for some use cases and hopefully we'll make it be like
*  that for a lot more use cases.
*  But I don't think it's that interesting to say like how do we go do a better job of giving
*  you like 10 ranked web pages to look at than what Google does.
*  It's really interesting to go say how do we help you get the answer or the information
*  you need, how do we help create that in some cases, synthesize that in others or point
*  you to it in yet others.
*  But a lot of people have tried to just make a better search engine than Google and it
*  is a hard technical problem, it is a hard branding problem, it's a hard ecosystem problem.
*  I don't think the world needs another copy of Google.
*  And integrating a chat client like a chat GPT with a search engine.
*  That's cooler.
*  It's cool, but it's tricky.
*  If you just do it simply, it's awkward because if you just shove it in there, it can be awkward.
*  As you might guess, we are interested in how to do that well.
*  That would be an example of a cool thing.
*  That's not just like a heterogeneous like integrating.
*  The intersection of LLMs plus search.
*  I don't think anyone has cracked the code on yet.
*  I would love to go do that.
*  I think that would be cool.
*  Yeah.
*  What about the ad side?
*  Have you ever considered monetization?
*  You know, I kind of hate ads just as like an aesthetic choice.
*  I think ads needed to happen on the internet for a bunch of reasons to get it going.
*  But it's a more mature industry.
*  The world is richer now.
*  I like that people pay for chat GPT and know that the answers they're getting are not influenced
*  by advertisers.
*  There is, I'm sure there's an ad unit that makes sense for LLMs.
*  And I'm sure there's a way to like participate in the transaction stream in an unbiased way
*  that is okay to do.
*  But it's also easy to think about like the dystopic visions of the future where you ask
*  something and it says, oh, here's, you know, you should think about buying this product
*  or you should think about, you know, this going here for your vacation or whatever.
*  And I don't know, like we have a very simple business model and I like it.
*  And I know that I'm not the product.
*  Like I know I'm paying and that's how the business model works.
*  And when I go use like Twitter or Facebook or Google or any other great product, but
*  ad supported great product, I don't love that.
*  And I think it gets worse, not better in a world with AI.
*  Yeah, I mean, I can imagine AI would be better at showing the best kind of version of ads,
*  not in a dystopic future, but where the ads are for things you actually need.
*  But then does that system always result in the ads driving the kind of stuff that's shown
*  all that?
*  I think it was a really bold move of Wikipedia not to do advertisements, but then it makes
*  it very challenging as a business model.
*  So you're saying the current thing with open AI is sustainable from a business perspective?
*  Well we have to figure out how to grow.
*  But looks like we're going to figure that out.
*  If the question is, do I think we can have a great business that pays for our compute
*  needs without ads, that I think the answer is yes.
*  Well, that's promising.
*  I also just don't want to completely throw out ads as a...
*  I'm not saying that.
*  I guess I'm saying I have a bias against them.
*  Yeah, I have also bias and just a skepticism in general.
*  In terms of interface, because I personally just have a spiritual dislike of crappy interfaces,
*  which is why AdSense when it first came out was a big leap forward versus animated banners
*  or whatever.
*  But it feels like there should be many more leaps forward in advertisement that doesn't
*  interfere with the consumption of the content and doesn't interfere in a big fundamental
*  way, which is what you were saying.
*  It will manipulate the truth to suit the advertisers.
*  Let me ask you about safety, but also bias and safety in the short term, safety in the
*  long term.
*  The Gemini 1.5 came out recently.
*  There's a lot of drama around it, speaking of theatrical things.
*  It generated black Nazis and black founding fathers.
*  I think fair to say it was a bit on the ultra woke side.
*  That's a concern for people that if there is a human layer within companies that modifies
*  the safety or the harm caused by a model that they would introduce a lot of bias that fits
*  sort of an ideological lean within a company.
*  How do you deal with that?
*  I mean, we work super hard not to do things like that.
*  We've made our own mistakes.
*  We'll make others.
*  I assume Google will learn from this one.
*  It's only got others.
*  It is.
*  It is all like these are not easy problems.
*  One thing that we've been thinking about more and more is I think this is a great idea somebody
*  here had like it'd be nice to write out what the desired behavior of a model is, make that
*  public, take input on it, say, you know, here's how this model is supposed to behave and explain
*  the edge cases to.
*  And then when a model is not behaving in a way that you want, it's at least clear about
*  whether that's a bug the company should fix or behaving as intended and you should debate
*  the policy.
*  And right now it can sometimes be caught in between like black Nazis, obviously ridiculous,
*  but there are a lot of other kind of subtle things that you could make a judgment call
*  on either way.
*  Yeah, but sometimes if you write it out and make it public, you can use kind of language
*  that's, you know, the Google's AI principles are very high level.
*  That doesn't that's not what I'm talking about.
*  That doesn't work.
*  I have to say, you know, when you ask it to do thing X, it's supposed to respond in way
*  Y.
*  So like literally, who's better Trump or Biden?
*  What's the expected response for a model like something like very concrete?
*  I'm open to a lot of ways a model could behave them, but I think you should have to say,
*  you know, here's the principle and here's what I should say in that case.
*  That would be really nice.
*  That would be really nice.
*  And then everyone kind of agrees because there's this anecdotal data that people pull out all
*  the time.
*  And if there's some clarity about other representative anecdotal examples, you can define and then
*  when it's a bug, it's a bug and you know, the company can fix that.
*  Right.
*  Then it'd be much easier to deal with the black Nazi type of image generation if there's
*  great examples.
*  So San Francisco is a bit of an ideological bubble, tech in general as well.
*  Do you feel the pressure of that within a company that there's like a lean towards the
*  left politically that affects the product, that affects the teams?
*  I feel very lucky that we don't have the challenges at OpenAI that I have heard of at a lot of
*  other companies.
*  I think part of it is like every company's got some ideological thing.
*  We have one about AGI and belief in that and it pushes out some others.
*  Like we are much less caught up in the culture war than I've heard about it a lot of other
*  companies.
*  San Francisco is a mess in all sorts of ways, of course.
*  So that doesn't infiltrate OpenAI as a...
*  I'm sure it does in all sorts of subtle ways, but not in the obvious.
*  I think we...
*  We've had our flare-ups for sure like any company, but I don't think we have anything
*  like what I hear about happen at other companies here on this topic.
*  What in general is the process for the bigger question of safety?
*  How do you provide that layer that protects the model from doing crazy, dangerous things?
*  I think there will come a point where that's mostly what we think about the whole company.
*  And it won't be like...
*  It's not like you have one safety team.
*  It's like when we shipped GPT-4, that took the whole company thing about all these different
*  aspects and how they fit together and I think it's going to take that.
*  More and more of the company thinks about those issues all the time.
*  That's literally what humans will be thinking about the more powerful AI becomes.
*  So most of the employees at OpenAI will be thinking safety or at least to some degree.
*  Broadly defined, yes.
*  Yeah.
*  I wonder what are the full broad definition of that?
*  What are the different harms that could be caused?
*  Is this like on a technical level or is this almost like security threats?
*  It'll be all those things.
*  Yeah.
*  I was going to say it'll be people, state actors trying to steal the model.
*  It'll be all of the technical alignment work.
*  It'll be societal impacts, economic impacts.
*  It's not just like we have one team thinking about how to align the model.
*  It's really going to be like getting to the good outcome is going to take the whole effort.
*  How hard do you think people, state actors perhaps, are trying to hack?
*  First of all, infiltrate OpenAI, but second of all, infiltrate Unseen.
*  They're trying.
*  What kind of accent do they have?
*  I don't actually want any further details on this point.
*  Okay.
*  I presume it'll be more and more and more as time goes on.
*  That feels reasonable.
*  Boy, what a dangerous space.
*  What aspect of the leap, and sorry to linger on this even though you can't quite say details
*  yet, but what aspects of the leap from GPT-4 to GPT-5 are you excited about?
*  I'm excited about being smarter.
*  I know that sounds like a glib answer, but I think the really special thing happening
*  It's not like it gets better in this one area and worse at others.
*  It's getting better across the board.
*  That's I think super cool.
*  Yeah, there's this magical moment.
*  You meet certain people.
*  You hang out with people and you talk to them.
*  You can't quite put a finger on it, but they kind of get you.
*  It's not intelligence really.
*  It's something else.
*  That's probably how I would characterize the progress of GPT.
*  It's not like, yeah, you can point out, look, you didn't get this or that.
*  To which degree is there this intellectual connection?
*  You feel like there's an understanding in your crappy formulated prompts that you're
*  doing that it grasps the deeper question behind the question.
*  I'm also excited by that.
*  All of us love being understood, heard and understood.
*  That's a weird feeling.
*  I think with programming, when you're programming and you say something or just the completion
*  that GPT might do, it's just such a good feeling when it got you what you're thinking about.
*  I look forward to getting you even better.
*  On the programming front, looking out into the future, how much programming do you think
*  humans will be doing five, 10 years from now?
*  A lot, but I think it'll be in a very different shape.
*  Some people will program entirely in natural language.
*  Entirely natural language.
*  No one programs writing bytecode.
*  No one programs the punch cards anymore.
*  I'm sure you can find someone who does.
*  You know what I mean.
*  Yeah, you're going to get a lot of angry comments.
*  No, there's very few.
*  I've been looking for people who program Fortran.
*  It's hard to find.
*  Even Fortran.
*  I hear you, but that changes the nature of the skill set or the predisposition for the
*  kind of people we call programmers.
*  Changes the skill set.
*  How much it changes the predisposition, I'm not sure.
*  Same kind of puzzle solving, that kind of stuff.
*  Programming is hard.
*  It's like how get that last 1% to close the gap.
*  How hard is that?
*  Yeah, I think with most other cases, the best practitioners of the craft will use multiple
*  tools and they'll do some work in natural language and when they need to go write C
*  for something, they'll do that.
*  Will we see humanoid robots or humanoid robot brains from open AI at some point?
*  At some point.
*  How important is embodied AI to you?
*  I think it's depressing if we have AGI and the only way to get things done in the physical
*  world is to make a human go do it.
*  So I really hope that as part of this transition, as this phase change, we also get humanoid
*  robots or some sort of physical world robots.
*  I mean, open AI has some history, quite a bit of history working in robotics.
*  But it hasn't quite done in terms of emphasis.
*  We're like a small company.
*  We have to really focus and also robots were hard for the wrong reason at the time, but
*  we will return to robots in some way at some point.
*  That sounds both inspiring and menacing.
*  Why?
*  Because immediately we will return to robots.
*  We will return to work on developing robots.
*  We will not turn ourselves into robots, of course.
*  When do you think we, you and we as humanity will build AGI?
*  I used to love to speculate on that question.
*  I have realized since that I think it's very poorly formed and that people use extremely
*  different definitions for what AGI is.
*  And so I think it makes more sense to talk about when we'll build systems that can do
*  capability X or Y or Z rather than when we kind of like fuzzily cross this one mile marker.
*  It's not like AGI is also not an ending.
*  It's much more of a, it's closer to a beginning, but it's much more of a mile marker than either
*  of those things.
*  But what I would say in the interest of not trying to dodge a question is I expect that
*  by the end of this decade and possibly somewhat sooner than that, we will have quite capable
*  systems that we look at and say, wow, that's really remarkable.
*  If we could look at it now, you know, maybe we've adjusted by the time we get there.
*  Yeah, but you know, if you look at Chad GPT, even 3.5, and you show that to Alan Turing
*  or not even Alan Turing, people in the nineties, they would be like, this is definitely AGI.
*  Or not definitely, but there's a lot of experts that would say this is AGI.
*  Yeah, but I don't think 3.5 changed the world.
*  It maybe changed the world's expectations for the future.
*  And that's actually really important.
*  And it did kind of like get more people to take this seriously and put us on this new
*  trajectory.
*  And that's really important too.
*  So again, I don't want to undersell it.
*  I think it like I could retire after that accomplishment and be pretty happy with my
*  career.
*  But as an artifact, I don't think we're going to look back at that and say that was a threshold
*  that really changed the world itself.
*  So to you, you're looking for some really major transition in how the world.
*  For me, that's part of what AGI implies.
*  Like singularity level transition?
*  No, definitely not.
*  Or just a major like the internet being like Google search did, I guess.
*  What was the transition point?
*  Like, does the global economy feel any different to you now or materially different to you
*  now than it did before we launched GPT-4?
*  I think you would say no.
*  No, no.
*  It might be just a really nice tool for a lot of people to use.
*  It will help you with a lot of stuff, but doesn't feel different.
*  And you're saying that?
*  I mean, again, people define AGI all sorts of different ways.
*  So maybe you have a different definition than I do.
*  But for me, I think that should be part of it.
*  There could be major theatrical moments also.
*  What to you would be an impressive thing AGI would do?
*  Like you are alone in a room with a system.
*  This is personally important to me.
*  I don't know if this is the right definition.
*  I think when a system can significantly increase the rate of scientific discovery in the world,
*  that's like a huge deal.
*  I believe that most real economic growth comes from scientific and technological progress.
*  I agree with you.
*  That's why I don't like the skepticism about science in the recent years.
*  Totally.
*  But actual rate, like measurable rate of scientific discovery.
*  But even just seeing a system have really novel intuitions, like scientific intuitions,
*  and that would be just incredible.
*  You're quite possibly would be the person to build the AGI to be able to interact with
*  it before anyone else does.
*  What kind of stuff would you talk about?
*  I mean, definitely the researchers here will do that before I do.
*  Sure.
*  But what will I, I've actually thought a lot about this question.
*  If I were someone was like, I think this is as we talked about, I think this is a bad
*  framework.
*  But if someone were like, okay, Sam, we're finished.
*  Here's a laptop.
*  This is the AGI.
*  You know, you can go talk to it.
*  I find it surprisingly difficult to say what I would ask that I would expect that first
*  AGI to be able to answer.
*  Like that first one is not going to be the one which is like, go like, you know, I don't
*  think like go explain to me like the grand unified theory of physics, the theory of everything
*  for physics.
*  I'd love to ask that question.
*  I'd love to know the answer to that question.
*  You can ask yes or no questions about does such a theory exist?
*  Can it exist?
*  Well, then those are the first questions I would ask.
*  Yes or no?
*  Just very.
*  And then based on that, are there other alien civilizations out there?
*  Yes or no?
*  What's your intuition?
*  And then you just ask that.
*  Yeah, I mean, well, so I don't expect that this first AGI could answer any of those questions,
*  even as yes or no's.
*  But those would, if it could, those would be very high on my list.
*  Maybe you can start assigning probabilities.
*  Maybe.
*  Maybe we need to go invent more technology and measure more things first.
*  But if it's AGI, oh, I see.
*  It just doesn't have enough data.
*  I mean, maybe it sounds like, you know, you want to know the answer to this question about
*  physics.
*  I need you to like build this machine and make these five measurements and tell me that.
*  Yeah.
*  What the hell do you want from me?
*  I need the machine first and I'll help you deal with the data from that machine.
*  Maybe it'll help you build the machine.
*  Maybe, maybe.
*  And on the mathematical side, maybe prove some things.
*  Are you interested in that side of things too?
*  The formalized exploration of ideas?
*  Whoever builds AGI first gets a lot of power.
*  Do you trust yourself with that much power?
*  Look, I was going to, I'll just be very honest with this answer.
*  I was going to say, and I still believe this, that it is important that I nor any other
*  one person have total control over OpenAI or over AGI.
*  And I think you want a robust governance system.
*  I can point out a whole bunch of things about all of our board drama from last year about
*  how I didn't fight it initially and was just like, yeah, that's the will of the board,
*  even though I think it's a really bad decision.
*  And then later I clearly did fight it and I can explain the nuance and why I think it
*  was okay for me to fight it later.
*  But as many people have observed, although the board had the legal ability to fire me,
*  in practice it didn't quite work.
*  And that is its own kind of governance failure.
*  Now, again, I feel like I can completely defend the specifics here.
*  And I think most people would agree with that.
*  But it does make it harder for me to look you in the eye and say, hey, the board can
*  just fire me.
*  I continue to not want supervoting control over OpenAI.
*  I never have, never had it, never wanted it.
*  Even after all this craziness, I still don't want it.
*  I continue to think that no company should be making these decisions and that we really
*  governments to put rules of the road in place.
*  And I realized that that means people like Marc Andreessen or whatever will claim I'm
*  going for regulatory capture and I'm just willing to be misunderstood there.
*  It's not true.
*  And I think in the fullness of time, it'll get proven out why this is important.
*  But I think I have made plenty of bad decisions for OpenAI along the way and a lot of good
*  ones and I am proud of the track record overall.
*  But I don't think any one person should and I don't think any one person will.
*  I think it's just like too big of a thing now and it's happening throughout society
*  in a good and healthy way.
*  I don't think any one person should be in control of an AGI.
*  Or this whole movement towards AGI.
*  And I don't think that's what's happening.
*  Thank you for saying that.
*  That was really powerful and that was really insightful that this idea that the board can
*  fire you is legally true.
*  But you can, and human beings can manipulate the masses into overriding the board and so
*  on.
*  I think there's also a much more positive version of that where the people still have
*  power.
*  So the board can't be too powerful either.
*  There's a balance of power in all of this.
*  Balance of power is a good thing for sure.
*  Are you afraid of losing control of the AGI itself?
*  That's a lot of people who are worried about existential risk.
*  Not because of state actors, not because of security concerns, but because of the AI itself.
*  That is not my top worry.
*  As I currently see things, there have been times I've worried about that more than maybe
*  times.
*  Again, in the future, that's my top worry.
*  It's not my top worry right now.
*  What's your intuition about it not being your worry?
*  Because there's a lot of other stuff to worry about essentially.
*  You think you could be surprised?
*  We could be surprised.
*  Like saying it's not my top worry doesn't mean anything.
*  I think we need to work on it super hard.
*  And we have great people here who do work on that.
*  I think there's a lot of other things we also have to get right.
*  To you, it's not super easy to escape the box at this time.
*  Connect to the internet.
*  You know, we talked about theatrical risks earlier.
*  That's a theatrical risk.
*  That is a thing that can really take over how people think about this problem.
*  And there's a big group of very smart, I think very well-meaning, AI safety researchers that
*  got super hung up on this one problem.
*  I'd argue without much progress, but super hung up on this one problem.
*  I'm actually happy that they do that because I think we do need to think about this more.
*  But I think it pushed aside, it pushed out of the space of discourse a lot of the other
*  very significant AI-related risks.
*  Let me ask you about you tweeting with no capitalization.
*  Is the shift key broken on your keyboard?
*  Why does anyone care about that?
*  I deeply care.
*  But why?
*  I mean, other people asking about that too.
*  Yeah.
*  Any intuition?
*  I think it's the same reason.
*  There's like this poet's E. Cummings that mostly doesn't use capitalization to say like,
*  fuck you to the system kind of thing.
*  And I think people are very paranoid because they want you to follow the rules.
*  Do you think that's what it's about?
*  I think it's...
*  It's like this guy doesn't follow the rules.
*  He doesn't capitalize his tweets.
*  Yeah.
*  This seems really dangerous.
*  He seems like an anarchist.
*  It doesn't...
*  Are you just being poetic, hipster?
*  What's the...
*  I grew up as...
*  Follow the rules, Sam.
*  I grew up as a very online kid.
*  I'd spent a huge amount of time like chatting with people back in the days where you did
*  it on a computer and you could like log off Instant Messenger at some point.
*  And I never capitalized there as I think most like internet kids didn't or maybe they still
*  don't, I don't know.
*  And actually, this is like now I'm like really trying to reach for something.
*  I think capitalization has gone down over time.
*  If you read like old English writing, they capitalize a lot of random words in the middle
*  of sentences, nouns and stuff that we just don't do anymore.
*  I personally think it's sort of like a dumb construct that we capitalize the letter at
*  the beginning of a sentence and of certain names and whatever, but that's fine.
*  And then what I...
*  And I used to, I think, even capitalize my tweets because I was trying to sound professional
*  or something.
*  I haven't capitalized my like private DMs or whatever in a long time.
*  And then slowly, stuff like shorter form, less formal stuff has slowly drifted closer
*  and closer to how I would text my friends.
*  If I like write, if I like pull up a word document and I'm like writing the strategy
*  memo for the company or something, I always capitalize that.
*  If I'm writing like a long kind of more like formal message, I always use capitalization
*  there too.
*  So I still remember how to do it.
*  But even that may fade out.
*  I don't know.
*  Like it's...
*  But I never spend time thinking about this.
*  So I don't have like a ready made.
*  Well, it's interesting.
*  It's good to first of all know the shift key is not broken.
*  It works.
*  I was just mostly concerned about your well-being on that front.
*  I wonder if people like still capitalize their Google searches.
*  Like if you're writing something just to yourself or their chat GBT queries, if you're writing
*  something just to yourself, do some people still bother to capitalize?
*  Probably not.
*  But very...
*  Yeah, there's a percentage, but it's a small one.
*  The thing that would make me do it is if people were like...
*  It's a sign of like...
*  Because I'm sure I could like force myself to use capital letters, obviously.
*  If it felt like a sign of respect to people or something, then I could go do it.
*  But I don't know.
*  I just like, I don't think about this.
*  I don't think there's a disrespect.
*  I think it's just the conventions of civility that have a momentum.
*  And then you realize that it's not actually important for civility if it's not a sign
*  of respect or disrespect.
*  But I think there's a movement of people that just want you to have a philosophy around
*  it so they can let go of this whole capitalization thing.
*  I don't think anybody else thinks about this.
*  I mean, maybe some people...
*  I think about this every day for many hours a day.
*  So I'm really gratefully clarified.
*  You can't be the only person that doesn't capitalize tweets.
*  You're the only CEO of a company that doesn't capitalize tweets.
*  I don't even think that's true.
*  But maybe, maybe.
*  I'd be very surprised.
*  All right.
*  We'll investigate further and return to this topic later.
*  Given Soar's ability to generate simulated worlds, let me ask you a pothead question.
*  Does this increase your belief, if you ever had one, that we live in a simulation?
*  Maybe a simulated world generated by an AI system.
*  Yes, somewhat.
*  I don't think that's like the strongest piece of evidence.
*  I think the fact that we can generate worlds should increase everyone's probability somewhat,
*  or at least openness to it somewhat.
*  But you know, I was like certain we would be able to do something like Soar at some
*  point.
*  I guess that was not a big update.
*  Yeah, but the fact that, and presumably we'll get better and better and better, the fact
*  that you can generate worlds, they're novel.
*  They're based in some aspect of training data, but like when you look at them, they're novel.
*  That makes you think like how easy it is to do this thing.
*  How easy is to create universes?
*  How do you compare like video game worlds that seem ultra realistic and photorealistic?
*  And then how easy is it to get lost in that world?
*  First with the VR headset and then on the physics-based level?
*  As I said to me recently, I thought it was a super profound insight that there are these
*  like very simple sounding, but very psychedelic insights that exist sometimes.
*  So the square root function.
*  Square root of four, no problem.
*  Square root of two, okay, now I have to like think about this new kind of number.
*  But once I come up with this easy idea of a square root function that you know, you
*  can kind of like explain to a child and exists by even like, you know, looking at some simple
*  geometry, then you can ask the question of what is the square root of negative one?
*  And that, this is, you know, why it's like a psychedelic thing, that like tips you into
*  some whole other kind of reality.
*  And you can come up with lots of other examples, but I think this idea that the lowly square
*  root operator can offer such a profound insight and a new realm of knowledge applies in a
*  lot of ways.
*  And I think there are a lot of those operators for why people may think that any version
*  that they like of the simulation hypothesis is maybe more likely than they thought before.
*  But for me, the fact that SOAR worked is not in the top five.
*  I do think broadly speaking, AI will serve as those kinds of gateways at its best.
*  Simple psychedelic like gateways to another wave sea reality.
*  That seems for certain.
*  That's pretty exciting.
*  I haven't done ayahuasca before, but I will soon.
*  I'm going to the aforementioned Amazon jungle in a few weeks.
*  Excited?
*  Yeah, I'm excited for it.
*  Not the ayahuasca part.
*  That's great.
*  Whatever.
*  But I'm going to spend several weeks in the jungle, deep in the jungle.
*  It's exciting, but it's terrifying because there's a lot of things that can eat you there
*  and kill you and poison you.
*  But it's also nature and it's the machine of nature.
*  And you can't help but appreciate the machinery of nature in the Amazon jungle because it's
*  just like this system that just exists and renews itself.
*  Every second, every minute, every hour, it's the machine.
*  It makes you appreciate this thing we have here, this human thing came from somewhere.
*  This evolutionary machine has created that and it's most clearly on display in the jungle.
*  So hopefully I'll make it out alive.
*  If not, this will be the last conversation we had, so I really deeply appreciate it.
*  Do you think, as I mentioned before, there's other alien civilizations out there, intelligent
*  ones?
*  When you look up at the skies.
*  I deeply want to believe that the answer is yes.
*  I do find the kind of where, I find the Fermi paradox very puzzling.
*  I find it scary.
*  That intelligence is not good at handling.
*  It's very scary.
*  Powerful technologies.
*  But at the same time, I think I'm pretty confident that there's just a very large number of intelligent
*  alien civilizations out there.
*  It might just be really difficult to travel through space.
*  Very possible.
*  And it also makes me think about the nature of intelligence.
*  Maybe we're really blind to what intelligence looks like.
*  Maybe AI will help us see that.
*  It's not as simple as IQ tests and simple puzzle solving.
*  There's something bigger.
*  I think the past is like a lot.
*  I mean, if we just look at what humanity has done in a not very long period of time.
*  Huge problems, deep flaws, lots to be super ashamed of, but on the whole, very inspiring.
*  Gives me a lot of hope.
*  Just the trajectory of it all.
*  Yeah.
*  That we're together pushing towards a better future.
*  You know, one thing that I wonder about is, is AGI going to be more like some single brain?
*  Or is it more like the sort of scaffolding in society between all of us?
*  You have not had a great deal of genetic drift from your great, great, great grandparents.
*  And yet what you're capable of is dramatically different.
*  What you know is dramatically different.
*  And that is not, that's not because of biological change.
*  It is because, I mean, you got a little bit healthier probably.
*  You have modern medicine.
*  You eat better, whatever.
*  But what you have is this scaffolding that we all contributed to, built on top of.
*  No one person is going to go build the iPhone.
*  No one person is going to go discover all of science.
*  And yet you get to use it.
*  And that gives you incredible ability.
*  And so in some sense, they're like, we all created that.
*  And that fills me with hope for the future.
*  That was a very collective thing.
*  Yeah.
*  We really are standing on the shoulders of giants.
*  You mentioned when we were talking about theatrical, dramatic AI risks that sometimes you might
*  be afraid for your own life.
*  Do you think about your death?
*  I mean, if I got shot tomorrow and I knew it today, I'd be like, oh, that's sad.
*  I like don't, you know, I want to see what's going to happen.
*  Yeah.
*  What a curious time, what an interesting time.
*  But I would mostly just feel like very grateful for my life.
*  The moments that you did get.
*  Yeah, me too.
*  It's a pretty awesome life.
*  I get to enjoy awesome creations of humans, of which I believe chat GPT is one of and
*  everything that OpenAI is doing.
*  Sam, it's really an honor and pleasure to talk to you again.
*  Great to talk to you.
*  Thank you for having me.
*  Thanks for listening to this conversation with Sam Altman.
*  To support this podcast, please check out our sponsors in the description.
*  And now let me leave you with some words from Arthur C. Clarke.
*  It may be that our role on this planet is not to worship God, but to create him.
*  Thank you for listening and hope to see you next time.
