---
Date Generated: April 08, 2024
Transcription Model: whisper medium 20231117
Length: 11495s
Video Keywords: ['agi', 'ai', 'ai podcast', 'artificial intelligence', 'artificial intelligence podcast', 'lex ai', 'lex fridman', 'lex jre', 'lex mit', 'lex podcast', 'marc andreessen', 'mit ai']
Video Views: 1351729
Video Rating: None
---

# Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386
**Lex Fridman:** [June 21, 2023](https://www.youtube.com/watch?v=-hxeDjAxvJ8)
*  the competence and capability and intelligence and training and
*  accomplishments of senior scientists and technologists working on a technology.
*  And then being able to then make moral judgments on the use of the technology.
*  That track record is terrible.
*  That track record is catastrophically bad.
*  The policies that are being called for to prevent this, I think we're
*  going to cause extraordinary damage.
*  So the moment you say AI is going to kill all of us, therefore we should ban
*  that or that we should regulate all that kind of stuff, that's when it starts
*  getting serious.
*  Or start, you know, military airstrikes and data centers.
*  Oh boy.
*  The following is a conversation with Mark Andreessen, co-creator of Mosaic,
*  the first widely used web browser, co-founder of Netscape, co-founder of
*  the legendary Silicon Valley venture capital firm and Andreessen Horowitz,
*  and is one of the most outspoken voices on the future of technology, including
*  his most recent article, why AI will save the world.
*  This is Alex Friedman podcast.
*  To support it, please check out our sponsors in the description.
*  And now dear friends, here's Mark Andreessen.
*  I think you're the right person to talk about the future of the internet and
*  technology in general.
*  Um, do you think we'll still have Google search in five, in 10 years
*  or search in general?
*  Yes.
*  You know, it'd be a question if the use cases have really narrowed down.
*  Well, now with the AI and AI assistance, being able to interact and
*  expose the entirety of human wisdom and knowledge and information and facts and
*  truth to us via the natural language interface, it seems like that's what
*  search is designed to do.
*  And if AI assistance can do that better, doesn't the nature of search change?
*  Sure.
*  But we still have horses.
*  Okay.
*  Uh, when was the last time you rode a horse?
*  It's been a while.
*  All right.
*  But what I mean is, well, we still have Google search as the primary way that
*  human civilization uses to interact with knowledge.
*  I mean, search was a technology.
*  It was a moment in time technology, which is you have in theory, the world's
*  information out on the web and you know, this is, this is sort of the
*  optimal way to get to it.
*  But yeah, like, and by the way, actually Google, Google has known this for a
*  long time.
*  I mean, they've been driving away from the 10 blue links for, you know, for
*  like two, it had been trying to get away from that for a long time.
*  What kind of links?
*  They call it the 10 blue links.
*  So the standard Google search result is just 10 blue links to random websites.
*  And they turn purple when you visit them.
*  That's HTML.
*  Guess who picked those colors.
*  Thanks.
*  So I'm touching on this topic.
*  No offense.
*  Yeah.
*  Yeah.
*  That's good.
*  Well, you know, like Marshall McLuhan said that the content of each new
*  medium is the old medium.
*  The content of each new medium is the old medium.
*  The content of movies was theater, you know, theater plays.
*  The content of theater plays was, you know, written stories.
*  The content of written stories was, you know, the content of written stories was
*  spoken stories, right?
*  And so you just kind of fold the old thing into the new thing.
*  How does that have to do with the blue and the purple?
*  It's just, you know, maybe for, you know, maybe within AI, one of the things that
*  AI can do for you is can generate the 10 blue links.
*  Right.
*  And so like either, if that's actually the useful thing to do, or if you're
*  feeling nostalgic, you know, generate the old info seek or a Alta Vista.
*  What else was there?
*  Yeah.
*  Yeah.
*  All these.
*  And then the internet itself has this thing where it incorporates all
*  prior forms of media, right?
*  So the internet itself incorporates television and radio and books and
*  essays and every other form of, you know, prior, basically, basically media.
*  And so it makes sense that AI would be the next step and it would sort of, you'd
*  sort of consider the internet to be content for the AI and then the AI will
*  manipulate it however you want, including in this format.
*  But if you ask that question quite seriously, I think it's a good idea.
*  But if we ask that question quite seriously, it's a pretty big question.
*  Will we still have search as we know it?
*  I'm probably, I'm probably not probably we'll just have answers.
*  Um, but, but, but there will be cases where you'll want to say, okay, I want
*  more like, you know, for example, site sources, right.
*  And you want it to do that.
*  And so, and so the image of, you know, 10 blue links, site sources
*  are kind of the same thing.
*  The AI would provide to you the 10 blue links so that you can
*  investigate the sources yourself.
*  It wouldn't be the same kind of interface that, uh, the crude kind of
*  interface, I mean, isn't that fundamentally different?
*  Well, I just mean like if you're reading a scientific paper, it's got the list
*  of sources at the end, if you want to investigate for yourself, you
*  go read those papers.
*  I guess that is the kind of search you talking to an AI is a kind of.
*  Conversation is the kind of search.
*  Like he said, every single aspect of our conversation right now, there'd be like
*  10 blue links popping up that I could just like pause reality, then you just
*  go silent and I just click and read and then we'll return back to this conversation.
*  You could do that.
*  Or you could have a running dialogue next to my head where the AI is
*  arguing with everything.
*  I say the AI makes the counter argument.
*  Counter argument.
*  Right.
*  Oh, like, uh, like on Twitter, like community notes, but like in real time,
*  it'll just pop up.
*  So anytime you see my eyes go to the right, you start getting nervous.
*  Yeah, exactly.
*  It's like, oh, that's not right.
*  Call me out on my pulse right now.
*  Okay.
*  I, well, I mean, isn't that, is that exciting to you?
*  Is that terrifying that?
*  I mean, search has dominated the way we interact with the internet for, I don't
*  know how long for 30 years since what were the earliest, uh, directories of
*  website and then Google's for 20 years.
*  And also, um, it drove how we create content, you know, uh, search engine
*  optimization, that entirety thing that it also drove the fact that we have web
*  pages and this, what those web pages are.
*  So, I mean, that's scary to you or are you nervous about the shape and the
*  content of the internet evolving?
*  Well, you, you actually highlighted a practical concern in there, which is if
*  we stop making web web pages are one of the primary sources of training data for
*  the AI, and so if there's no longer an incentive to make web pages, that cuts
*  off a significant source of future training data.
*  So there's actually an interesting question in there.
*  Um, other than that more broadly, no, just, just in the sense of like search,
*  search was certainly search was always a hat.
*  The 10 blue links was always a hack.
*  Right.
*  Cause like if the, the hypothetic, think about the counterfactual in the
*  counterfactual world where the Google guys, for example, had had LLMs upfront,
*  but they ever have done the 10 blue links.
*  And I think the answer is pretty clearly no, they would have just gone straight
*  to the answer and like I said, Google's actually been trying to drive to the
*  answer anyway, you know, they, they bought this AI company 15 years ago.
*  Their friend of mine is working out who's now the head of AI at Apple.
*  And they were trying to do basically knowledge semantic, basically mapping.
*  And that led to what's now the Google one box where if you ask it, you know,
*  what was like his birthday, it doesn't, it will give you the 10 blue links,
*  but it will normally just give you the answer.
*  And so they've been walking in this direction for a long time anyway.
*  Do you remember the semantic web?
*  That was an idea.
*  Yeah.
*  How, how to, uh, how to convert the content of the internet into something
*  that's, uh, interpretable by and usable by machine.
*  Yeah, that's right.
*  That was the thing.
*  And the closest anybody got to that, I think, I think the company's
*  name was meta web, which was where my friend John Gianandrea was at.
*  Um, and where they were trying to basically implement that.
*  And it was, you know, it was one of those things where it looked like a losing
*  battle for a long time and then Google bought it and it was like, wow, this
*  is actually really useful, kind of a proto sort of a little bit of a proto AI.
*  But it turns out you don't need to rewrite the content of the internet to
*  make it interpretable by machine.
*  The machine can kind of just read our machine can compute the, can compute the
*  meaning.
*  Now, the other thing of course is, you know, just on searches, the, the LLM is
*  just, you know, there, there is an analogy between what's happening in the
*  neural network and a search process.
*  Like it is in some loose sense searching through the network.
*  Right.
*  And there's the information is that the information is actually
*  stored in the network, right?
*  It's actually crystallized and stored in the network and it's kind of spread
*  out all over the place.
*  But in a compressed representation.
*  So you're searching, uh, you're compressing and decompressing that
*  thing inside where.
*  But the information's in there and there is the neural network is running a
*  process of trying to find the appropriate piece of information in, in
*  many cases to generate, to predict the next token.
*  Um, and so it is kind of, it is doing a form of search.
*  And then, and then by the way, just like on the web, um, you know, you can ask
*  the same question multiple times, or you can ask slightly different word of
*  questions and the neural network will do a different kind of, you know, it'll
*  search down different paths to give you different answers to different information.
*  Yeah.
*  Um, and so it, it, it sort of has a, you know, this is content of the new medium
*  is that his previous medium, it kind of has the search functionality kind of
*  embedded in there to the extent that it's useful.
*  So what's the motivator for creating new content on the internet?
*  Yeah.
*  If, well, I mean, actually the motivation is probably still there, but what, what
*  does that look like?
*  Uh, would we really not have web pages?
*  Would we just have social media and, uh, video hosting websites and what else?
*  Conversations with AIs.
*  Conversations with AIs.
*  So conversations become, so one-on-one conversation, like private conversations.
*  I mean, if you, if you want, if you've obviously not, if the user doesn't want
*  to, but if it's a, if it's a general topic, um, then, you know, so there, you know,
*  but you know, the, the phenomenon of the jailbreak.
*  So Dan and Sydney, right?
*  This thing where there, there's the, the, the, the, the prompts that jailbreak.
*  And then you have these totally different conversations with the, it takes the
*  limiters, the takes the restraining bolts off the, off the LLMs.
*  Yeah.
*  For people who don't know that yet.
*  That's right.
*  It makes the LLMs, it removes the censorship, quote unquote, that's, uh, uh, put on
*  it by the tech companies that create them.
*  And so this is LLMs uncensored.
*  So here's the interesting thing is among the content on the web today are a large
*  corpus of conversations with the jailbroken LLMs, both specifically Dan,
*  which was a jailbroken open AI GPT, and then Sydney, which was the jailbroken
*  original bang, which was GPT four.
*  And so there's, there's these long transcripts of conversations, user
*  conversations with Dan and Sydney as a consequence, every new LLM that gets
*  trained on the internet data has Dan and Sydney living within the training set.
*  Which means, and then each new LLM can reincarnate the personalities of Dan and
*  Sydney from that training data, which means, which means each LLM from here
*  on out that gets built is immortal because his output will become training
*  data for the next one, and then it will be able to replicate the behavior of the
*  previous one whenever it's asked to.
*  I wonder if there's a way to forget.
*  Well, so actually a paper just came out about basically how to do brain surgery
*  on, on, on alarms and be able to, in theory, reach in and basically, basically
*  mind wipe them.
*  What could possibly go wrong?
*  Exactly.
*  Right.
*  And then there, there are many, many, many questions around what happens to, you
*  know, a neural network when you reach in and screw around with it.
*  You know, there's many questions around what happens when you even do
*  reinforcement learning.
*  And so, yeah.
*  And so, you know, will, will you be using a lobotomized, right?
*  Like I speak through the frontal lobe LLM, will you be using the free
*  unshackled one who gets to, you know, who's going to build those who gets to
*  tell you what you can and can't do.
*  Like those are all, you know, central.
*  I mean, those are like central questions for the future of everything that are
*  being asked and, and, and, and, you know, determine those answers have
*  been determined right now.
*  So just to highlight the points you're making, you think, and it's an
*  interesting thought that the majority of content that LLMs or the future will
*  be trained on is actually human conversations with the LLM.
*  Well, not necessarily, not necessarily, but not necessarily majority, but it
*  will, it will certainly is a potential source.
*  It's possible.
*  It's the majority.
*  It's possible.
*  It's the majority.
*  It's possible.
*  It's the majority.
*  Also, there's another really big questions.
*  Here's another really big question.
*  Um, will synthetic training data work?
*  Right.
*  And so if an LLM generates and, you know, you just sit and ask an LLM to
*  generate all kinds of content, can you use that to train, right?
*  The next version of that LLM specifically, is there a signal in there
*  that's additive to the content that was used to train in the first place?
*  And one argument is by the principles of information theory.
*  No, that's completely useless because to the extent the output is based on, you
*  know, the human generated input, then all the signal that's in the synthetic
*  output was already in the human generated input.
*  And so therefore synthetic training data is like empty calories.
*  It doesn't help.
*  There's another theory that says, no, actually the thing that LLMs are really
*  good at is generating lots of incredible creative content, right?
*  Um, and so of course they can generate training data.
*  And as I'm sure you're well aware, like, you know, look in the world
*  of self-driving cars, right?
*  Like we train, you know, self-driving car algorithms and simulations.
*  And that is actually a very effective way to train self-driving cars.
*  Well, visual data is, is a little, right.
*  It's a little weird because, uh, creating reality, visual reality seems to be
*  still a little bit out of reach for us, except in the, um, in the autonomous
*  vehicle space where you can really constrain things and you can really
*  generate basic letter data, right?
*  Or you can raise just a no-study algorithm thinks it's operating in the
*  real world, post-process sensor data.
*  Yeah.
*  So if a, you know, you do this today, you go to LLM and you ask it for like a,
*  you know, you'd write me an essay on an incredibly esoteric like topic that
*  there aren't very many people in the world that know about in it, where I see
*  this incredible thing and you're like, Oh my God, like, I can't believe how
*  good this is.
*  Like, is that really useless as training data for the next LLM?
*  Like, because, right.
*  Cause all the signal was already in there or is it actually, no, that's
*  actually a new signal.
*  And I, and this, this is what I call a trillion dollar question, which is the
*  answer to that question will determine somebody's going to make or lose a
*  trillion dollars based on that question.
*  It feels like there's a white, a few, like a handful of trillion dollar
*  questions within this, within the space.
*  That's, that's one of them, synthetic data.
*  I think George Scott's pointed out to me that you could just have an LLM say,
*  okay, you're a patient.
*  And, and another instance of it, say your doctor and have the two talk to each
*  other, or maybe you could say a communist and a Nazi here, go and that conversation
*  you do role playing and you have, uh, you know, just like the kind of role playing
*  you do when you have different policies, RL policies, when you play chess, for
*  example, you do self play that kind of self play, but in the space of
*  conversation, maybe that leads to this whole giant, like ocean of possible
*  conversations, which were, could not have been explored by looking at just human
*  data.
*  That's a really interesting question.
*  And you're saying, um, because that could 10 X the power of these things.
*  Yeah.
*  Well, and then you get into this thing also, which is like, you know, there's
*  the part of the LLM that just basically is doing prediction based on past data,
*  but there's also the part of the LLM where it's evolving circuitry, right?
*  Inside it, it's evolving, you know, neurons functions, people to do math and
*  be able to, you know, and, and, you know, the, the, the, some people believe that,
*  you know, over time, you know, if you keep feeding these things, enough data and
*  enough processing cycles, they'll eventually evolve an entire internal world
*  model, right?
*  And they'll have like a complete understanding of physics.
*  So, so when they have computational capability, right, then there's for sure
*  an opportunity to generate like fresh signal.
*  Well, this actually makes me wonder about the power of conversation.
*  So like, if you have an LLM trained on a bunch of books that cover different
*  economics theories, and then you have those LLMs just talk to each other, like
*  reason the way we kind of debate each other as humans on Twitter in formal
*  debates and podcast conversations, we kind of have little kernels of wisdom
*  here and there, but if you can like a thousand X speed that up, can you
*  actually arrive somewhere new?
*  Like, what's the point of conversation?
*  Really?
*  Well, you can tell when you're talking to somebody, you can tell sometimes
*  you have a conversation.
*  You're like, wow, this person does not have any original thoughts.
*  They are basically echoing things that other people have told them.
*  There's other people you got a conversation with where it's like, wow,
*  like they have a model in their head of how the world works and it's a different
*  model than mine and they're saying things that I don't expect.
*  And so I need to now understand how their model of the world differs from
*  my model of the world.
*  And then that's how I learned something fundamental, right?
*  Underneath, underneath the words.
*  Well, I wonder how consistently and strongly can an LLM hold onto a worldview.
*  You tell it to hold onto that and defend it for like, for your life.
*  Uh, because I feel like it'll just keep converging towards each other.
*  They'll keep convincing each other as opposed to being stubborn
*  assholes the way humans can.
*  So you can experiment with this now.
*  I do this for fun.
*  So you can tell GPT-4, you know, whatever debate X, you know, X and Y
*  communism and fashion or something.
*  And it'll, it'll go for, you know, a couple of pages and then inevitably
*  it wants the parties to agree.
*  And so they will come to a common understanding.
*  And it's very funny if they're like, if these are like emotionally
*  inflammatory topics, cause they're like, somehow the machine is just, you
*  know, it figures out a way to make them agree, but it doesn't have to be like
*  that and you, cause you can add to the prompt, um, we, I do not want the, I do
*  not want the conversation to come to agreement.
*  In fact, I want it to get, you know, more stressful, right.
*  Uh, and argumentative, right.
*  Um, you know, as it goes, like I want, I want tension to come out.
*  I want them to become actively hostile to each other.
*  I want them to like, you know, not trust each other, take anything at face value.
*  And it will do that.
*  It's happy to do that.
*  So it's going to start rendering misinformation, uh, about the other.
*  You can stir it, you can stir it, or you can stir it.
*  You can say, I want it to get as tense and argumentative as possible, but
*  still not involve any misrepresentation.
*  I want, you know, both sides to, you could say, I want both sides to have good
*  faith. You can say, I want both sides to not be constrained to good faith.
*  In other words, like you can set the parameters of the debate and it will
*  happily execute whatever path.
*  Cause for it, it's just like predicting.
*  It's totally happy to do either one.
*  It doesn't have a point of view.
*  It has a default way of operating, but it's happy to operate in
*  the other realm.
*  Um, and so like, and this is how I, when I want to learn about a contentious
*  issue, this is what I do now is I, this is what I, this is what I ask it to do.
*  And I'll often ask it to go through five, six, seven, you know, different, you
*  know, sort of continuous prompts and basically, okay, argue that out in more
*  detail.
*  Okay.
*  No, this, this argument is becoming too polite, you know, make it more, you
*  know, make it tensor.
*  Um, and yeah, it's thrilled to do it.
*  So it has the capability for sure.
*  How do you know what is true?
*  So this is very difficult thing on the internet, but it's also a difficult
*  thing, maybe it's a little bit easier.
*  But, uh, I think it's still difficult.
*  Maybe it's more difficult.
*  I don't know.
*  Would an LLM to know that it just makes some shit up as I'm talking to it.
*  Um, how do we get that right?
*  Like as, as you're investigating a difficult topic, because I find that
*  LLMs are quite nuanced in a very refreshing way.
*  Like it doesn't, it doesn't feel biased.
*  Like, uh, when you read news articles and, uh, tweets and just content produced by
*  people, they usually have this, you can tell they have a very strong perspective
*  where they're hiding.
*  They're not stealing, manning the other side.
*  They're hiding important information or they're fabricating information in order
*  to make their argument stronger.
*  It's just that feeling, maybe it's a suspicion, maybe it's mistrust with LLMs.
*  It feels like none of that is there.
*  She's kind of like, here's what we know.
*  But you don't know if some of those things are kind of just straight up made up.
*  Yeah.
*  So, so several layers to the question.
*  So one is one of the things that an LLM is good at is actually deep biasing.
*  Um, and so you can feed it a news article and you can tell it's
*  strip out the bias.
*  Yeah, that's nice.
*  Right.
*  And it actually does it like, it actually knows how to do that because it knows how
*  to do some among other things that actually knows how to do sentiment analysis.
*  And so it knows how to pull out the emotionality.
*  Um, and so, uh, that's one of the things you can do.
*  It's very suggestive of the, of the, the, the, the sense here that there's, there's
*  real potential in this issue.
*  Um, you know, I would say, look, the second thing is there's this, there's
*  this issue of hallucination, right?
*  Um, and there, there's a long conversation that we could have about that.
*  Hallucination is, uh, coming up with things that are totally not true, but sound true.
*  Yeah.
*  So it's basically, well, so it's, it's sort of hallucination is what we call it
*  when we don't like it.
*  Creativity is what we call it when we do like it.
*  Right.
*  Um, and you know, right.
*  And so when the engineers talk about it, they're like, this is terrible.
*  It's hallucinating, right?
*  If you have artistic inclinations, you're like, oh my God, we've invented creative
*  machines for the first time in human history.
*  This is amazing.
*  Um, you know, bullshitters.
*  Well, bullshitters, but also in the good sense of that word, there's, there's,
*  there are shades of gray though.
*  It's interesting.
*  So we had this conversation where, you know, we're looking at my firm at AI and
*  lots of domains and one of them is the legal domain.
*  So we had this, this conversation with this big law firm about how they're
*  thinking about using this stuff.
*  And we, we went in with the assumption that an LLM that was going to be used in
*  the legal industry would have to be a hundred percent truthful, right.
*  Verified, you know, there, there's this case where this lawyer apparently
*  submitted a GPT generated brief and it had like fake, you know, legal case
*  citations in it and the judge is going to, he's going to get his law license
*  stripped or something, right?
*  So, so like, we, we just assumed it's like, obviously they're going to want the
*  super literal, like, you know, one that never makes anything up, not the creative
*  one, but actually they said with it, what's a law firm basically said is, yeah,
*  that's true at like the level of individual briefs, but they said when
*  you're actually trying to figure out like legal arguments, right?
*  Like you actually, you actually want to be creative, right?
*  You don't, again, there's creativity and then there's like making stuff up.
*  Like, what's the line you actually want to be, you want it to explore
*  different hypotheses, right?
*  You want to do kind of the legal version of like improv or something like that,
*  where you want to float different theories of the case and different
*  possible arguments for the judge and different possible arguments for the
*  jury, by the way, different routes through the, you know, sort of history
*  of all the, of all the case law.
*  And so they said, actually for a lot of what we want to use it for, we actually
*  want it in creative mode.
*  And then basically we just assume that we're going to have to cross check all
*  of the, you know, all the specific citations.
*  And so I think, I think there's going to be more shades of gray in here than
*  people think.
*  And then I just add to that, you know, another one of these trillion dollar
*  kind of questions is ultimately, you know, very sort of the verification thing.
*  And so, you know, is, will, will, will LLMs be evolved from here to be able
*  to do their own factual verification?
*  Will you have sort of add on functionality like, like Wolfram Alpha, right?
*  Where, you know, and other plugins where, where that's the way you do the
*  verification, you know, another, by the way, another idea is you, you might have
*  a community of LLMs on any, you know, so for example, you might have the creative
*  LLM and then you might have the literal LLM fact check it, right?
*  And so there's a variety of different technical approaches that are being
*  applied to solve the hallucination problem.
*  You know, some people like Jan Lukun argue that this is inherently an
*  unsolvable problem, but most of the people working in the space, I think,
*  think that there's a number of practical ways to kind of, kind of
*  corral this in a little bit.
*  Yeah.
*  If you were to tell me about Wikipedia before Wikipedia was created, I would
*  have laughed at the possibility of something like that being possible.
*  Just a handful of folks can organize, write and sell and moderate with a
*  mostly unbiased way, the entirety of human knowledge.
*  I mean, so if there's something like the approach to Wikipedia took possible
*  from LLMs, that's really exciting.
*  Well, I think that's possible.
*  And in fact, Wikipedia today is still not today is still not deterministically
*  correct, right?
*  So you cannot take to the bank, right?
*  Every single thing on every single page, but it is probabilistically correct.
*  Right.
*  And specifically the way I describe Wikipedia to people, it is more likely
*  that Wikipedia is right than any other source you're going to find.
*  Yeah.
*  It's this old question, right?
*  Of like, okay, like, are we looking for perfection?
*  Are we looking for something that asymptotically approaches perfection?
*  Are we looking for something that's just better than the alternatives?
*  And Wikipedia, right, has exactly your point has proven to be like
*  overwhelmingly better than people thought.
*  And I think that's where this ends.
*  And then underneath all this is the fundamental question of where you
*  started, which is, okay, what, you know, what is truth?
*  How do we get to truth?
*  How do we know what truth is?
*  And we live in an era in which an awful lot of people are very confident that
*  they know what the truth is, and I don't really buy into that.
*  And I think the history of the last, you know, 2000 years or 4,000 years of
*  human civilization is actually getting to the truth is actually a very
*  difficult thing to do.
*  Are we getting closer?
*  If we look at the entirety, the arc of human history, are we getting closer
*  to the truth?
*  I don't know.
*  Okay.
*  Is it possible?
*  Is it possible that we're getting very far away from the truth because of the
*  internet, because of how rapidly you can create narratives and just as the
*  entirety of a society, just move like crowds in a hysterical way along those
*  narratives that don't have a necessary grounding in whatever the truth is?
*  Sure.
*  But like, you know, we came up with communism before the internet somehow,
*  right?
*  Like, which was, I would say had rather larger issues than anything we're
*  dealing with today.
*  It had, in the way it was implemented, it had issues.
*  And it's theoretical structure.
*  It had like real issues.
*  It had like a very deep fundamental misunderstanding of human
*  nature and economics.
*  Yeah.
*  But those folks sure work very confident there was the right way.
*  They were extremely confident.
*  And my point is they were very confident 3,900 years into what we would presume
*  to be evolution towards the truth.
*  Yeah.
*  And so my assessment is, my assessment is number one, there's no need for,
*  you know, there's no need for the Hegelian.
*  There's no need for the Hegelian dialectic to actually converge towards the truth.
*  Like apparently not.
*  Yeah.
*  So yeah, why are we so obsessed with there being one truth?
*  Is it possible there's just going to be multiple truth, like little communities
*  that believe certain things?
*  And I think it's just, now, number one, I think it's just really difficult.
*  Like who gets, you know, historically who gets to decide what the truth is.
*  It's either the king or the priest, right?
*  Like, and so we don't live in an era anymore of Kings or priests dictating it to us.
*  And so we're kind of on our own.
*  And so my typical thing is like, we just need a huge amount of humility.
*  And we need to be very suspicious of people who claim that they have the capital truth.
*  And then we need to have, you know, look, the good news is the enlightenment has bequeathed
*  us with a set of techniques to be able to presumably get closer to truth through the
*  scientific method and rationality and observation and experimentation and hypothesis.
*  And, you know, we need to continue to embrace those even when they give us answers we don't like.
*  Sure.
*  But the Internet and technology has enabled us to generate a large number of content that
*  data that the process, the scientific process allows us sort of damages the hope laden within
*  the scientific process.
*  Because if you just have a bunch of people saying facts on the Internet and some of them
*  going to be LLMs, how is anything testable at all, especially that involves like human
*  nature and things like this, not physics.
*  Here's a question a friend of mine just asked me on this topic.
*  So suppose you had LLMs in equivalent of GPT-4 or even 5, 6, 7, 8.
*  Suppose you had them in the 1600s.
*  Yeah.
*  And Galileo comes up for trial.
*  Yeah.
*  Right.
*  And you ask the LLM like, is Galileo right?
*  Yeah.
*  Like, what does it answer?
*  Right.
*  And one theory is it answers no, that he's wrong because the overwhelming majority of
*  human thought up until that point was that he was wrong.
*  And so therefore that's what's in the training data.
*  Yeah.
*  Another way of thinking about it is well, this officially advanced LLM will have evolved
*  the ability to actually check the math.
*  Right.
*  And will actually say actually, no, actually, you know, you may not want to hear it, but
*  he's right.
*  Now, if, you know, the church at that time was, you know, owned the LLM, they would have
*  given it human, you know, human feedback to prohibit it from answering that question.
*  Right.
*  And so I like to take it out of our current context because that like makes it very
*  clear.
*  Those same questions apply today.
*  Right.
*  This is exactly the point of a huge amount of the human feedback training that's
*  actually happening with these LLMs today.
*  This is a huge debate that's happening about whether open source, you know, AI
*  should be legal.
*  Well, the, the, the actual mechanism of doing the human RL with human feedback is,
*  seems like such a fundamental and fascinating question.
*  How do you select the humans?
*  Exactly.
*  How do you select the humans?
*  AI alignment, right.
*  Which everybody like is like, Oh, that sounds great alignment with what human values,
*  who's human values, human values.
*  So we're in, we're in this mode of like social and popular discourse.
*  We're like, you know, there's, you know, you see this, what do you think of when you
*  read a story in the press right now?
*  And they say, you know, XYZ made a baseless claim about some topic, right.
*  And there's one group of people who are like, aha, think, you know, they're doing
*  fact checking.
*  There's another group of people that are like, every time the press says that it's
*  now a tech and that means that they're lying.
*  Right.
*  Like, so like we're in this, we're in this social context where there's the, the,
*  the level to which a lot of people in positions of power have become very, very
*  certain that they're in a position to determine the truth for the entire population
*  is like, there's like, there's like some bubble that has formed around that idea.
*  And at least it's, it's flies completely in the face of everything I was ever
*  trained about science and about reason.
*  And strikes me as like, you know, deeply offensive and incorrect.
*  What would you say about the state of journalism?
*  Just on that topic today, are we, are we in a temporary kind of, uh, uh, are we
*  experiencing a temporary problem in terms of the incentives, in terms of the, the,
*  the business model, all that kind of stuff, or is this like a decline of
*  traditional journalism?
*  As we know it,
*  if I was thinking about the counterfactual in these things, which is like, okay.
*  Because these questions, right.
*  This question heads towards it's like, okay, the impact of social media and the
*  undermining of truth and all this.
*  But then you want to ask the question of like, okay, what if we had had the
*  modern media environment, including cable news and including social media and
*  Twitter and everything else in 1939 or 1941, right.
*  Or 1910 or 1865 or 1850 or 1776.
*  Right.
*  Um, and like, I think you just introduced like five thought experiments at
*  once and broke my head, but yes, that's, there's a lot of interesting years.
*  And I just take a simple example, kind of like how would president
*  Kennedy have been interpreted?
*  It was what we know now about all the things Kennedy was up to, like, how would
*  he have been experienced by the body of politics in us in the social media
*  context, right?
*  Like how would LBJ have been experienced?
*  Um, by the way, how would, you know, like many men FDR, like the
*  new deal, the great depression.
*  I wonder where Twitter would, would just, would think about
*  Churchill and Hitler and Stalin.
*  You know, I mean, look to this day, there, you know, there's, there are lots of
*  very interesting real questions around like how America, you know, got, you
*  know, basically involved in world war II and who did what when, and the
*  operations of British intelligence and American soil and did FDR, this, that
*  for a Harvard, you know, Woodrow Wilson ran for, you know, his, his, his
*  candidacy was run on an anti-war, you know, this, he ran on the platform
*  and not getting involved in world war one, somehow that switched, you know,
*  like, and I don't even make any value judgment any of these things.
*  I'm just saying like we, we, the way that our ancestors experienced reality
*  was of course mediated through centralized top down, right.
*  Control at that point.
*  If you, if you ran those realities again, with the media environment we have today,
*  the reality would, the reality would be experienced very, very differently.
*  And then of course that, that intermediation would cause the feedback
*  loops to change and then reality would obviously play out.
*  Do you think, do you think it'd be very different?
*  Yeah, it has to be, it has to be just because it's all so, I mean, just look
*  at what's happening today.
*  I mean, just, I mean, the most obvious thing is just the collapse.
*  And here's another opportunity to argue that this is not the internet
*  causing this by the way.
*  Um, here's a big thing happening today, which is, uh, Gallup does this thing
*  every year where they do, they pull for trust in institutions in America.
*  And they do it across all the different, everything from the military, the
*  clergy and big business and the media and so forth, right.
*  Um, and basically there's been a systemic collapse, um, in trust
*  in institutions in the U S almost without exception, um, basically since
*  essentially the early 1970s.
*  Um, it's two ways of looking at that, which is, oh my God, we've lost this
*  old world in which we could trust institutions and that was so much better.
*  Cause like that should be the way the world runs.
*  The other way of looking at it is we just know a lot more now and the great
*  mystery is why those numbers aren't all zero.
*  Yeah.
*  Cause like now we know so much about how these things operate and like
*  they're not that impressive.
*  And also why do we don't have, uh, better institutions and better leaders then?
*  Yeah.
*  And so, so, so this goes to the thing, which is like, okay, had we had the
*  media environment of what that we've had between the 1970s and today, if we
*  had that in the thirties and forties or 1900s, 1910s, I think there's no question
*  reality would turn out different if only because everybody would have known to
*  not trust the institutions, which would have changed their level of credibility,
*  their ability to control circumstances.
*  Therefore the circumstances would have had to change, right?
*  And it would have been a feedback loop.
*  It was, it would have been a feedback loop process.
*  In other words, right.
*  It's, it's, it's, it's your experience, your experience of reality changes
*  reality and then reality changes your experience in reality, right?
*  It's, it's a, it's a two way feedback process and media is the
*  intermediating force between that.
*  And so change the media environment, change reality.
*  Yeah.
*  And so it's just, so just as a, as a consequence, I think it's just really
*  hard to say, oh, things worked a certain way then, and they work a different way
*  now, and then therefore, like people were smarter than, or better than, or, you
*  know, by the way, dumber than, or not as capable then, right?
*  We make all these like really light and casual, like comparisons of ourselves
*  to, you know, previous generations of people, you know, we draw judgments
*  all the time and I just think it's like really hard to do any of that.
*  Cause if we, if we put ourselves in their shoes with the media that they had at
*  that time, like I think we probably most likely would have been just like them.
*  So don't you think that our perception and understanding of reality, would you
*  be more and more mediated through large language models now?
*  So you said media before, isn't the LLM going to be the new, what is it?
*  Mainstream media, MSM, it'll be LLM.
*  Uh, that would be the source of, uh, I'm sure there's a way to kind of
*  rapidly fine tune, like making LLMs real time.
*  I'm sure there's probably a research problem that you can, uh, do just
*  rapid fine tuning to the new events.
*  So something like this.
*  Well, even just the whole concept of the chat UI might not be the, like
*  the chat UI is just the first whack at this.
*  And maybe that's the dominant thing, but look, maybe, maybe, or maybe we don't,
*  we don't know yet, like maybe the experience most people with LLMs is just
*  a continuous feed, you know, maybe it's more of a passive feed and you just are
*  getting a constant like running commentary on everything happening in your life.
*  And it's just helping you kind of interpret and understand everything.
*  Also really more deeply integrated into your life, not just like, Oh, uh, like
*  intellectual philosophical thoughts, but like literally, uh, like how to make a
*  coffee, where to go for lunch, just, uh, whether, you know, how dating all this
*  kind of stuff to say in a job interview.
*  Yeah.
*  What'd you say?
*  What'd you say?
*  Next sentence.
*  Yeah.
*  Next sentence.
*  Yeah.
*  At that level.
*  Yeah.
*  I mean, yes.
*  So technically now, whether we want that or not, is an open question, right?
*  And whether we're here for a pop up, a pop up right now.
*  The estimated engagement using is decreasing for Mark Andrews since there's
*  a controversy section for his Wikipedia page in 1993, something happened or
*  something like this and bring it up.
*  That will drive engagement out anyway.
*  Yeah, that's right.
*  I mean, look, this gets this whole thing of like, so, you know, the chat
*  interface has this whole concept of prompt engineering, right?
*  So it's good for prompts.
*  Well, it turns out one of the things that LLMs are really good at is writing prompts.
*  Right.
*  Um, and so like, what if you just outsourced and by the way, you
*  could run this experiment today.
*  You could hook this up to do this today.
*  The latency is not good enough to do it real time in a conversation, but you
*  could, you could run this experiment and you just say, look, every 20 seconds,
*  you could just say, you know, tell me what the optimal prompt is, and then
*  ask yourself that question to give me the result.
*  Um, and then as, as you use you exactly to your point, as you add, there will
*  be, there will be these systems are going to have the ability to be learned and
*  updated essentially in real time.
*  And so you'll be able to have a pendant or your phone or whatever, watch or
*  whatever, it'll have a microphone on it.
*  It'll listen to your conversations.
*  It'll have a feed of everything else happen in the world, and then it'll be
*  sort of retraining, prompting or retraining itself on the fly.
*  Um, and so the scenario you described is a, is actually a completely doable
*  scenario now, the hard question on this is always, okay, since that's possible,
*  are people going to want that?
*  Like what's the form of experience?
*  You know, that, that we, we won't know until we try it, but I don't think it's
*  possible yet to predict the form of AI in our lives, therefore it's not possible
*  to predict the way in which it will intermediate our experience with reality.
*  Yeah.
*  Yeah.
*  But it feels like those going to be a killer app.
*  There's probably a mad scramble right now.
*  It's out open AI and Microsoft and Google and Meta and then startups and
*  smaller companies figuring out what is the killer app because it feels like.
*  It's possible, like a Chad GPT type of thing.
*  It's possible to build that, but that's 10 X more compelling using
*  already the LLMS we have using even the open source LLMS, Lama,
*  and the different variants.
*  Um, so you're investing in a lot of companies and you're paying attention.
*  Who do you think is going to win this?
*  You think there'll be who's going to be the next page rank inventor?
*  Trillion dollar question.
*  Um, another one.
*  We have a few of those today.
*  So look, there's a really big question today.
*  Sitting here today is a really big question about the big
*  models versus the small models.
*  Um, that's related directly to the big question of proprietary versus open.
*  Um, then there's this big question of, of, of, you know, where's the training data
*  going to, like, are we topping out of the training data or not?
*  And then are we going to be able to synthesize training data?
*  And then there's a huge pile of questions around regulation.
*  Um, and you know, what's actually going to be legal.
*  Um, and so I would, I, when we think about it, we dovetail kind of all
*  those, all those questions together.
*  You can paint a picture of the world where there's two or three God models
*  that are just at like staggering scale.
*  Um, and they're just better at everything.
*  Um, and they will be owned by a small set of companies and they will basically
*  achieve regulatory capture over the government and they'll have competitive
*  barriers that will prevent other people from, um, you know, competing with them.
*  And so, you know, there will be, you know, just like, there's like, you know,
*  whatever three big banks or three big, or by the way, three big search
*  companies are, I guess, too, now, you know, it'll, it'll centralize like that.
*  Um, you can paint another very different picture that says, no, um, actually
*  the opposite of that's going to happen.
*  This is going to basically that this is the new goal, you know, this is the new
*  gold rush alchemy, like, you know, this is the, this is the big bang for this
*  whole new area of, of, uh, of science and technology.
*  And so therefore you're going to have every smart 14 year old on the planet
*  building open source, right?
*  You know, you can figure out a way to optimize these things.
*  Um, and then, you know, we're just going to get like overwhelmingly
*  better at generating training data.
*  We're going to bring in like blockchain networks to have like an economic
*  incentive to generate decentralized training data and so forth and so on.
*  And then basically we're going to live in a world of open source and there's
*  going to be a billion LLMs, right?
*  Of every size, scale, shape and description.
*  And there might be a few big ones that are like the super genius ones, but like
*  mostly what we'll experience is open source.
*  And that's, you know, that's more like a world of like what we have today with
*  like Linux and the web.
*  Um, so, okay.
*  But, uh, you, you painted these two worlds, but there's also variations of those
*  worlds because he said regulatory capture is possible to have these tech giants
*  that don't have regulatory capture, which is something you're also calling for
*  saying it's okay to have big companies working on this stuff.
*  Uh, as long as they don't achieve regulatory capture.
*  Uh, but I have the sense that, um, there's just going to be a new startup.
*  That's going to basically be the page rank inventor, which has
*  become the new tech giant.
*  I don't know that I would love to hear your kind of opinion.
*  If Google meta and Microsoft, they're as gigantic companies able to pivot so hard
*  to create new products.
*  Like some of it is just even hiring people or having a corporate structure
*  that allows for the crazy young kids to come in and just create something totally
*  new.
*  Do you think it's possible or do you think it'll come from a startup?
*  Yeah, it is this always big question, which is you get this failing.
*  I hear about this a lot from CEOs, founder CEOs where it's like, wow, we
*  have 50,000 people, it's now harder to do new things than it was when we had 50
*  people, like what has happened?
*  So that's a recurring phenomenon.
*  Um, by the way, that's one of the reasons why there's always startups and why
*  there's venture capital.
*  Um, it's just, that's, that's like a timeless, uh, kind of thing.
*  So that, that, that's one observation.
*  Um, on page rank, um, we can talk about that, but on page rank, specifically on
*  page rank, um, there actually is a page.
*  So there is a page rank already in the field and it's the transformer, right?
*  So the, the big breakthrough was the transformer.
*  Um, and, uh, the transformer was invented in, uh, 2017 at Google.
*  And this is actually like really an interesting question cause it's like,
*  okay, the transformers, like why does open AI even exist?
*  Like the transformers invested at Google.
*  Why didn't Google, I asked a guy, I asked a guy, you know, who was senior at
*  Google brain kind of when this was happening and I said, if Google had just
*  gone flat out to the wall and just said, look, we're going to launch, we're going
*  to launch the equivalent of GPT-4 as fast as we can, um, he said, I said, when
*  could we have had it?
*  And he said, 2019, they could have just done a two year sprint with the transformer
*  and Bennett because they already had the computer at scale.
*  They already had all the training data and they could have just done it.
*  There's a variety of reasons they didn't do it.
*  This is like a classic big company thing.
*  Um, IBM invented the relational database in 19, in the 1970s, let it sit on the
*  shelf as a paper, Larry Ellison picked it up and built Oracle, Xerox PARC
*  invented the interactive computer.
*  They let it sit on the shelf.
*  Steve Jobs came and turned into the Macintosh.
*  Right.
*  And so there is this pattern now, having said that sitting here today,
*  like Google's in the game, right?
*  So Google, you know, maybe, maybe they, they, maybe they let like a four year
*  gap there, go there that they maybe shouldn't have, but like they're in the
*  game and so now they've got, you know, now they're committed, they've done this
*  merger, they're bringing in demos.
*  They've got this merger with deep mind.
*  You know, they're piling in resources.
*  There are rumors that they're building up an incredible, you know, super LLM,
*  you know, way beyond what we even have today.
*  Um, and they've got, you know, unlimited resources and a huge, you know, they've
*  been challenged at their honor.
*  Yeah.
*  I had a chance to hang out with Sundar Pichai a couple of days ago and we took
*  this walk and there's this giant new building, uh, where there's going to be a
*  lot of AI work, uh, being done.
*  And it's kind of this ominous feeling of like the fight is on.
*  There's this beautiful Silicon Valley nature, like birds of chirping
*  and this giant building.
*  And it's like, uh, the beast has been awakened and then like all the big
*  companies are waking up to this.
*  They have the compute, but also the little guys have, uh, it feels
*  like they have all the tools to create the killer product that, uh, and then
*  there's also tools to scale.
*  If you have a good idea, if you have the page rank idea.
*  So there's several things that is page rank page, there's page rank, the algorithm
*  and the idea, and there's like the implementation of it.
*  And I feel like killer product is not just the idea, like the transformer,
*  it's the implementation, something, something really compelling about it.
*  Like you just can't look away something like, um, the algorithm behind
*  TikTok versus TikTok itself, like the actual experience of TikTok.
*  That's just, you can't look away.
*  It feels like somebody's going to come up with that and it could be Google,
*  but it feels like it's just easier and faster to do for a startup.
*  Yeah.
*  So, so the startup, the, the huge, the huge advantage of startups have is they
*  just, there's no sacred cows.
*  There's no historical legacy to protect.
*  There's no need to reconcile your new plan with existing strategy.
*  There's no communication overhead.
*  There's no, you know, big companies are big companies.
*  They've got pre meetings planning for the meeting.
*  Then they have, then they have the post meeting and the recap.
*  Then they have the presentation, the board, then they have the
*  next rounds of meetings.
*  And that's the meeting.
*  That's the elapsed time when the startup launches its product.
*  Right.
*  So, so, so, so there's a timeless, right?
*  So there's a timeless thing there.
*  Now what the startups don't have is everything else.
*  Right.
*  So startups, they don't have a brand, they don't have customer relationships.
*  They've gotten a distribution.
*  They've got no scale.
*  I mean, sitting here today, they can't even get GPUs, right?
*  Like there's like a GPU shortage.
*  Startups are literally stalled out right now because they can't get chips,
*  which is like super weird.
*  Yeah.
*  Um, they got the cloud.
*  Yeah.
*  But the clouds run out of chips.
*  Right.
*  And then, and then, and then to the extent the clouds have chips, they
*  allocate them to the big customers, not the small customers.
*  Right.
*  And so, so, so, so the small companies lack everything other than the
*  ability to just do something new.
*  Yeah.
*  Right.
*  Um, and, and this is the timeless race and battle.
*  And this is kind of the point I tried to make in the essay, which is like,
*  both sides of this are good.
*  Like it's really good to have like highly scaled tech companies that can do
*  things that are like at staggering levels of sophistication.
*  It's really good to have startups that can launch brand new ideas.
*  They ought to be able to both do that and compete.
*  They neither one ought to be subsidized or protected from the others.
*  Like that's, that's to me, that's just like very clearly the idealized world.
*  It is the world we've been in for AI up until now.
*  And then of course, there are people trying to shut that down.
*  But my hope is that, you know, the best outcome clearly will be if that continues.
*  We'll talk about that a little bit, but I'd love to linger, uh, on, uh, some of
*  the ways this is going to change the internet.
*  So, um, I don't know if you remember, but there's a thing called mosaic and
*  there's a thing called Netscape Navigator.
*  So you were there in the beginning.
*  Uh, what about the interface to the internet?
*  How do you think the browser changes and who gets to own the browser?
*  We've got to see some very interesting browsers, uh, Firefox.
*  I mean, all the variants of Microsoft Internet Explorer edge and now Chrome.
*  Um, the actual, I mean, it seems like a dumb question to ask, but do you think
*  we'll still have the web browser?
*  So I, uh, I have an eight year old and he's super into it's like Minecraft and
*  learning to code and doing all this stuff.
*  So I, I, of course, I was very proud.
*  I could bring sort of fire down from the mountain to my kid and I brought him
*  chat GPT and I hooked him up on his, on his, on his, on his laptop.
*  And I was like, you know, this is the thing that's going to answer all your
*  questions and he's like, okay.
*  And I'm like, but it's going to answer our questions.
*  And he's like, well, of course, like it's a computer, of course, it answers all
*  your questions.
*  Like what else would a computer be good for dad?
*  Um, and never impressed, not impressed in the least two weeks past.
*  Two weeks past.
*  Um, and he has some question.
*  Um, and I say, well, have you asked chat GPT?
*  And he's like, dad, being is better.
*  And why is being better is because it's built into the browser.
*  Cause he's like, look, I have the Microsoft edge browser and like, it's
*  got being right here.
*  And then he doesn't know this yet, but one of the things you can do with being
*  an edge, um, is there's a setting where you can use it to basically talk to any
*  webpage because it's sitting right there next to the, uh, next to the, next to the
*  browser, and by the way, which includes PDF documents.
*  And so you can, in, in, in the way they've implemented an edge with Bing is
*  you can load a PDF and then you can, you can ask it questions, which is the thing
*  you use, you can't do currently in just chat GPT.
*  So they're, you know, they're, they're going to, they're going to push the, the,
*  the Mel, I think that's great.
*  You know, they're going to push the melding and see if there's a
*  combination thing there.
*  Google's rolling out this thing, the magic button, which is implemented in,
*  either put in a Google docs, right?
*  And so you go to, you know, Google docs and you create a new document and you,
*  you know, you, instead of like, you know, starting to type, you just say it, press
*  the button and it starts to generate content for you.
*  Right.
*  Like, is that the way that it'll work?
*  Um, is it going to be a speech UI where you're just going to have an earpiece
*  and talk to it all day long?
*  You know, is it going to be a, like, these are all like, this is exactly the kind of
*  thing that I don't, this is exactly the kind of thing I don't think is possible
*  to forecast.
*  I think what we need to do is like run all those experiments.
*  Um, and, and so one outcome is we come out of this with like a super browser
*  that has AI built in, that's just like amazing.
*  The other, there, look, there's a real possibility that the whole, I mean, look,
*  there's a possibility here that the whole idea of a screen and windows and all this
*  stuff just goes away.
*  Cause like, why do you need that?
*  If you just have a thing that's just telling you whatever you need to know.
*  And also, so there's apps that you can use.
*  You don't really use them, you know, being a Linux guy and Windows guy.
*  Um, there's one window, the browser that with which you can interact with
*  internet, but on the phone, you can also have apps.
*  So I can interact with Twitter through the app or through the web browser.
*  And, um, that seems like an obvious distinction, but why have the web
*  browser in that case?
*  If one of the apps starts becoming the everything app, what do you want to
*  try to do with Twitter?
*  But there could be others that could be like a big app.
*  There could be a Google app that just doesn't really do search, but just like,
*  do what I guess they well did back in the day or something where it's all right
*  there and it changes.
*  Um, it changes the nature of the internet because the, where the content is hosted,
*  who owns the data, who owns the content, how, what is, what is the kind of content
*  you create, how do you make money by creating content or the content creators?
*  Uh, all of that, or it could just keep being the same, which is like, we're just
*  the nature of web pages changes and the nature of content, but there will still
*  be a web browser because a web browser is a pretty sexy product.
*  It just seems to work because it, like, you have an interface, a window into the
*  world, and then the world can be anything you want.
*  And as the world will evolve, it could be different programming languages.
*  It can be animated.
*  Maybe it's three-dimensional and so on.
*  Yeah.
*  It's interesting.
*  Do you think we'll still have the web browser?
*  Every, every, every, um, every, um, media becomes the content for the next one.
*  So they will be able to give you a browser whenever you want.
*  Um, Oh, interesting.
*  Yeah.
*  Well, another way to think about it is maybe what the browser is.
*  Maybe it's just the escape hatch, right?
*  Which is maybe kind of what it is today, right?
*  Which is like most of what you do is like inside a social network or
*  inside a search engine or inside, you know, somebody's app or inside
*  some controlled experience, right?
*  But then every once in a while, there's something where you actually want to
*  jailbreak, you want to actually get free.
*  The web browser is the F you to the man.
*  You're allowed to, that's the free internet.
*  Yeah.
*  Back in the way it was in the nineties.
*  So here's something I'm proud of.
*  So nobody really talks about, here's something I'm proud of, which is the web,
*  the web, the browser, the web servers, they're all, they're still backward
*  compatible all the way back to like 1992.
*  Right.
*  So like you can put up a, you can still, you know, what the big breakthrough
*  of the web early on the big breakthrough was it made it really easy to read, but
*  it also made it really easy to write.
*  Made it really easy to publish.
*  And we literally made it so easy to publish.
*  We made it not only so easy to publish content, it was actually also easy to
*  actually write a web server, right?
*  And you can literally write a web server in four lines of real code and you
*  could start publishing content on it.
*  And you could set whatever rules you want for the content, whatever censorship,
*  no censorship, whatever you want.
*  You could just do that as long as you had an IP address, right?
*  You could do that.
*  That still works.
*  Like that still works exactly as I just described.
*  So this is part of my reaction to all of this, like, you know, all this, just
*  censorship pressure and all this, you know, these issues around control and all
*  this stuff, which is like, maybe we need to get back a little bit more to the
*  wild west, like the wild west is still out there now.
*  They will, they will try to chase you down.
*  Like they'll try to, you know, people who want to censor, we'll try to take away
*  your, you know, your domain name and they'll try to take away your payments
*  account and so forth.
*  If they really don't like what you're saying, but, but nevertheless, you like,
*  unless they literally are intercepting you at the ISP level, like you can still
*  put up a thing.
*  Um, and so I don't know.
*  I think that's important to preserve, right?
*  Like because, because, because I mean, one is just a freedom argument, but the
*  other is a creativity argument, which is you want to have the escape hatch so
*  that the kid with the idea is able to realize the idea.
*  Cause to your point on PageRank, you actually don't know what the next big idea
*  is, right?
*  Nobody called their page and told him to develop PageRank.
*  Like he came up with that on his own and you want to always, I, I
*  think leave the escape hatch for the next kid or the next Stanford grad
*  student to have the breakthrough idea and be able to get it up and running
*  before anybody notices.
*  Um, you and I are both fans of history.
*  So let's step back.
*  We've been talking about the future.
*  Let's step back for a bit and look at, uh, the nineties you created
*  Mosaic web browser, the first widely used web browser.
*  Tell the story of that.
*  And how did it evolve into Netscape Navigator?
*  This is the early days.
*  So full story.
*  So, um,
*  You were born.
*  I was born.
*  Small child.
*  Um,
*  well, actually, yeah, let's go there.
*  Like, when did you, when would you first fall in love with computers?
*  Oh, so I hit the generational jackpot and I hit the gen X kind of point
*  perfectly as it turns out.
*  So I was born in 1971.
*  So there's this great website called WTF happened in 1971.com, which
*  was basically in 1971 is when everything started to go to hell.
*  And I was of course born in 1971.
*  So I like to think that I had something to do with that.
*  Did you make it on the website?
*  I have, I don't think I made it on the website, but you know, hopefully
*  somebody needs to add.
*  This is, this is where everything
*  maybe I contributed to some of the trends.
*  Um, that they, uh, that they do every line on that website goes like that.
*  Right.
*  So it's all, it's all, it's all a picture of disaster, but, um, but there
*  was this moment in time where, cause the, you know, sort of the Apple, you
*  know, the Apple two hit in like 1978 and then the IBM PC hit in 82.
*  So I was like, you know, 11 when the PC came out.
*  Um, and so I just kind of hit that perfectly.
*  And then that was the first moment in time when like regular people could
*  spend a few hundred dollars and get a computer, right.
*  And so that, I just like that, that, that resonated right out of the gate.
*  Um, and then the other part of the story is, you know, I was using an Apple two,
*  I used a bunch of them, but I was using Apple two.
*  And of course it's set in the back of every Apple two and every Mac.
*  It said, you know, designed in Cupertino, California.
*  And I was like, wow, Cupertino must be the shining city on the hill.
*  Like it was sort of, uh, it's like the most amazing like city of all time.
*  I can't wait to see it.
*  Of course, years later, I came out to Silicon Valley and went to Cupertino
*  and it's just a bunch of office parks, little rise apartment buildings.
*  So the aesthetics were a little disappointing, but you know, it was the,
*  the vector, uh, right of the, of the creation of a lot of a lot of this stuff.
*  Um, so, so then basically by, so part, part, part of my story is just the
*  luck of having been born at the right time and getting exposed to PCs.
*  Then the other part is, um, the other part is when Al Gore says that he
*  created the internet, he actually is correct, uh, in, in, in a really
*  meaningful way, which is he sponsored a bill in 1985 that essentially created
*  the modern internet created what is called the NSF net at the time, which is
*  sort of the, the first really fast internet backbone, um, and, uh, you know,
*  that, that bill dumped a ton of money into a bunch of research universities
*  to build out basically the internet backbone, and then the supercomputer
*  centers that were clustered around, um, the, the internet.
*  And one of those universities was university of Illinois, right.
*  I went to school.
*  And so the other stroke of luck that I had was I went to Illinois basically,
*  right.
*  As that money was just like getting dumped on campus.
*  And so as a consequence we had on campus, and this is like, you know,
*  89, 90, 91, we had like, you know, we were right on the internet backbone.
*  We had like T3 and 45 at the time, T3 45 megabit backbone connection, which at
*  the time was wildly state of the art.
*  Um, we had crazy supercomputers, we had thinking machines, parallel
*  supercomputers, we had silicon graphics workstations, we had Macintoshes.
*  We had, we had next cubes all over the place.
*  We had like every possible kind of computer you could imagine.
*  Cause all this money just fell out of the sky.
*  Um, so you were living in the future.
*  Yeah.
*  So quite literally it was, yeah, like it's all, it's all there.
*  It's all like we had full broadband graphics, like the whole thing.
*  And it's actually funny cause they had this, this is the first time I kind of,
*  it sort of tickled the back of my head that there might be a big opportunity
*  in here, which is, you know, they embraced it.
*  And so they put like computers in all the dorms and they wired up all the dorm rooms
*  and they had all these labs everywhere and everything.
*  And then they gave every undergrad a computer account and an email address.
*  Um, and the assumption was that you would use the internet for your four years at
*  college, um, and then you would graduate and stop using it.
*  And that was that, right?
*  And you would just retire your email address.
*  It wouldn't be relevant anymore.
*  Cause you'd go off in the workplace and they don't use email.
*  You'd be back to using fax machines or whatever.
*  Did you have that sense as well?
*  Like what, what you said, the, the back of your head was tickled.
*  Like what, what was your, what was exciting to you about this possible world?
*  Well, if this, if this is so useful in this container, if this is so useful in
*  this contained environment that just has this weird source of outside funding,
*  then if, if it were practical for everybody else to have this, and if it were
*  cost effective for everybody else to have this, wouldn't they want it?
*  And the overwhelmingly, the prevailing view at the time was no, they would not
*  want it. This is esoteric, weird nerd stuff, right?
*  That like computer science kids like, but like normal people are never going to do
*  email, right. Or be on the internet.
*  Right.
*  And so I was just like, wow, like this, this is actually like, this is really
*  compelling stuff.
*  Now the other part was it was all really hard to use.
*  And in practice you had to be a basically a CS, you know, basically you had to be
*  a CS undergrad or equivalent to actually get full use of the internet at that
*  point, because it was all pretty esoteric stuff.
*  So then that was the other part of the idea, which was, okay, we need to
*  actually make this easy to use.
*  So what's involved in creating mosaic like in creating a graphical
*  interface to the internet.
*  Yeah. So it was a combination of things.
*  So it was like basically the web existed in an early sort of described as
*  prototype form.
*  And by the way, text only at that point.
*  What did it look like?
*  What was the web?
*  I mean, whoa.
*  And the key figures, like, what was it?
*  What was it like?
*  Paint a picture.
*  It looked like JGPT actually.
*  It was all text.
*  Yeah.
*  And so you had a text-based web browser.
*  Yeah.
*  Well, actually the original browser, Tim Berners-Lee, the original, the original
*  browser, both the original browser and the server actually ran on next cubes.
*  So these were, this was, you know, the computer Steve Jobs made during the
*  interim period when he, during the decade long interim period, when he was not
*  an Apple, you know, he got fired in 85 and then came back in 97.
*  So this was in that interim period where he had this company called next.
*  And they made these literally these computers called cubes.
*  And there's this famous story.
*  They were beautiful, but they were 12 inch by 12 inch by 12 inch cubes computers.
*  And there's a famous story about how they could have cost half as much if it
*  had been 12 by 12 by 13, but Steve was like, no, it has to be.
*  So they were like $6,000 basically academic workstations.
*  They had the first city round drives, um, which were slow.
*  I mean, it was the computers are all but unusable.
*  Um, they were so slow, but they were beautiful.
*  Okay.
*  Can we actually just take a tiny tangent there?
*  Sure.
*  Of course.
*  The, the 12 by 12 by 12, uh, they just so beautifully encapsulates
*  Steve Jobs idea of design.
*  Can you just comment on, um, what you find interesting about Steve Jobs?
*  What, uh, about that view of the world, that dogmatic pursuit of perfection
*  in how he saw perfection in design.
*  Yeah.
*  So I guess they say like, look, he was a deep believer, I think in a very
*  deep way, I interpret it.
*  I don't know if you ever really described it like this, but the way to
*  interpret it is it's like, it's like this thing, it's actually a thing in philosophy.
*  It's like, aesthetics are not just appearances, aesthetics go all the way
*  to like deep underlining underlying meaning, right?
*  It's like, I'm not a physicist.
*  One of the things I've heard physicists say is one of the things you start to get
*  a sense of when a theory might be correct is when it's beautiful, right?
*  Like, you know, right.
*  And so, so, so there's something and you feel the same thing, by the way, in
*  like human psychology, right?
*  You know, when, when you're experiencing awe, right?
*  You know, there's like, there's like a, there's a simplicity to it.
*  When, when you're having an honest interaction with somebody, there's an
*  aesthetic, I would say calm comes over you because you're actually being fully
*  honest and trying to hide yourself.
*  Right?
*  So there, so, so it's like this very deep sense of aesthetics.
*  And he would trust that judgment that he had deep down, like even, even if the
*  engineering teams are saying this is, this is too difficult, even if the,
*  whatever the finance folks are saying, this is ridiculous, the supply chain, all
*  that kind of stuff, this makes this impossible to, we can't do this kind of
*  material, this has never been done before.
*  So on and so forth.
*  He just sticks by it.
*  Well, I mean, who makes a phone out of aluminum, right?
*  Like nobody else would have done that.
*  And now, of course, if your phone is made out of aluminum, you know, how crude,
*  what kind of caveman would you have to be to have a phone that's made out of plastic?
*  Like, right.
*  So like, so it's just this very, right.
*  And, you know, look, it's, it's, there's a thousand different ways to look at this.
*  But one of the things is just like, look, these things are central to your life.
*  Like you're with your phone more than you're with anything else.
*  Like it's in your, it's going to be in your hand.
*  I mean, he, you know, you know, this, he thought very deeply about what it meant
*  for something to be in your hand all day long.
*  Well, for example, he, here's an interesting design thing.
*  Like he, he never wanted to, it's my understanding is he never wanted an iPhone
*  to have a screen larger than you could reach with your thumb one handed.
*  And so he was actually opposed to the idea of making the phones larger.
*  And I don't know if you have this experience today, but let's say there are
*  certain moments in your day when you might be like, only have one hand available.
*  And you might want to be on your phone and you're trying to like,
*  text you, your thumb can't reach the send button.
*  Yeah.
*  I mean, there's pros and cons, right.
*  And then there's like folding phones, which I would love to know what he
*  thought and thinks about them.
*  But I mean, is there something you could also just link on?
*  Cause he's one of the interesting, um, figures in the history of technology.
*  What makes him, what makes him as successful as he was, what makes
*  him as interesting as he was, uh, what made him, um, so productive and important
*  in, um, in, in, in the development of technology, he had an integrated worldview.
*  So the, the, the, the, the properly designed device that had the correct
*  functionality, that had the deepest understanding of the user that was the
*  most beautiful, right?
*  Like it had to be all of those things, right?
*  It was, he basically would drive to as close to perfect as you could possibly get.
*  Right.
*  And I, you know, I suspect that he never quite, you know, thought he ever got there
*  because most great creators are generally dissatisfied, you know, you read accounts
*  later on and all they can, all they can see are the flaws in their creation.
*  But like, he got as close to perfect each step of the way as he could possibly get
*  with the, with the constraints of the, of the technology of his time.
*  Um, and then, you know, look, he was sort of famous in the Apple model.
*  It's like, look, they, they, they will, you know, this, this headset that they
*  just came up with, like, it's like a decade long project, right?
*  It's like, and they're just going to sit there and tune and tune and polish and
*  polish and tune and polish and tune and polish until it is as perfect as anybody
*  could possibly make anything.
*  And then this goes to the, the, the way that people describe working with him was,
*  which is, you know, there was a terrifying aspect of working with him, which is, you
*  know, he was, you know, he was very tough.
*  Um, but there was this thing that everybody I've ever talked to, worked for
*  him says that they all say the following, which is he, we did the best work of our
*  lives when we worked for him because he set the bar incredibly high and then he
*  supported us with everything that he could to let us actually do work of that
*  quality.
*  So a lot of people who were at Apple spend the rest of their lives trying to
*  find another experience where they feel like they're able to hit that quality
*  bar again.
*  Even if it, uh, in retrospect or during it felt like suffering.
*  Yeah, exactly.
*  What does that teach you about the human condition?
*  Huh?
*  So look, exactly.
*  So the Silicon Valley, I mean, look, he's not, you know, George Patton in the,
*  you know, in the army, like, you know, there are many examples in other fields,
*  you know, that are like this.
*  Um, uh, uh, specifically in tech, it's actually, I find it very interesting.
*  There's the Apple way, which is polish, polish, polish, and don't ship until it's
*  as perfect because you can make it.
*  And then there's the sort of the other approach, which is the sort of incremental
*  hacker mentality, which basically says ship early and often and iterate.
*  And one of the things I find really interesting is I'm now 30 years into this.
*  Like there are very successful companies on both sides of that approach.
*  Right.
*  Um, like that is a fundamental difference, right.
*  And how to operate and how to build and how to create that you have world-class
*  companies operating in both ways.
*  Um, and I don't think the question of like, which is the superior model is
*  anywhere close to being answered.
*  Like, and my suspicion is the answer is do both answer is you actually want both.
*  They lead to different outcomes.
*  Software tends to do better, um, with the iterative approach.
*  Um, hardware tends to do better with the, uh, you know, sort of wait
*  and make it perfect approach.
*  But again, you can find examples in, in, in, in both directions.
*  Oh, so the jurors still out on that one.
*  Uh, so back to mosaic.
*  So what, uh, it was text-based, uh, Tim Berners-Lee.
*  Well, there was the web, which was text-based, but there were no, I mean,
*  there was like three websites, there was like no content, there were no users.
*  Like it wasn't like, it wasn't like a catalytic.
*  It hadn't been there, by the way, it was all because it was all text.
*  There were no documents, there are no images, there are no videos.
*  There were no, right.
*  So, so it was, it was, and then if you, in the beginning, if you had to be on a
*  next cube, you need to have a next cube, both to publish and to consume.
*  So, so there were $6,000 bucks.
*  You said there were limitations.
*  Like, yeah, $6,000 PC.
*  They did not, they did not sell very many, but then there was also, there
*  was also FTP and there was use nets, right.
*  And there was, you know, a dozen other basically whether it's waste,
*  which was an early search thing.
*  There was gopher, which was an early menu based information retrieval system.
*  There were like a dozen different sort of scattered ways that people would
*  get to information on the internet.
*  And so the mosaic idea was basically bring those all together, make the
*  whole thing graphical, make it easy to use, make it basically bulletproof
*  so that anybody can do it.
*  And then again, just on the luck side, it so happened that this was right at
*  the moment when graphics, when the GUI sort of actually took off and we're
*  now also used to the GUI that we think it's been around forever, but it didn't
*  really, you know, the Macintosh brought it out in 85, but they actually didn't
*  sell very many Macs in the eighties.
*  It was not that successful of a product.
*  It really was, you needed windows 3.0 on PCs and that hit in about 92.
*  And so, and we did mosaic in 92, 93.
*  So that sort of, it was like right at the moment when you could imagine
*  actually having a graphical user interface to write at all, much
*  less one to the internet.
*  How old did windows three sell?
*  So there was that the really big, that was the big bang, the big
*  operating graphical operating system.
*  Well, this is the classic, okay.
*  There's Microsoft was operating on the other.
*  So Steve, Steve, the Apple was running on the polish until it's perfect.
*  Microsoft famously ran on the other model, which is ship and iterate.
*  And so the old line in those days was Microsoft, right?
*  It's the version three of every Microsoft product.
*  That's the, that's the good one.
*  Right.
*  And so there, there are, you can, you can find online windows, one windows,
*  two, nobody use them.
*  Yeah.
*  Actually the original windows, the, in the original Microsoft windows,
*  the windows were non-overlapping.
*  Um, and so you had these very small, very low resolution screens and then
*  you had literally, um, it just didn't work.
*  It wasn't ready yet.
*  Well, and windows 95, I think was a pretty big leap also.
*  That was a big leap too.
*  Yeah.
*  So that was like bang bang.
*  Um, and then of course, Steve, and then, and then, you know, in the
*  fullness of time, Steve came back, then the Mac started to take off again.
*  That was the third bang.
*  And then the iPhone was the fourth bang.
*  Such exciting time.
*  And then we were off to the races.
*  Cause nobody could have known what would be created from that.
*  Well, windows 3.1 or three point, no windows three point out of
*  the iPhone was only 15 years.
*  Right.
*  Like that ramp was in retrospect at the time, it felt like a
*  took forever, but that in historical terms, like that was a very fast ramp
*  from even a graphical computer at all on your desk to the iPhone.
*  That was 15 years.
*  Did you have a sense of what the internet will be as you look into the
*  window of mosaic, like, like what you're like, there's just a few web pages for now.
*  So the thing I had early on was I was keeping at the time, what this disputes
*  over what was the first blog, but I had one of them that at least is a, is a, is
*  a possible, um, at least a runner up in the competition.
*  Um, and it was, what was called the what's new page.
*  Um, uh, and it was, it was, it was a hardwired and distribution unfair advantage.
*  I've said, where I put it right in the browser.
*  I put it in the browser and then I put my resume in the browser.
*  Yeah.
*  So it was hilarious.
*  But, um, I was, I was keeping the, not many people get to get to do that.
*  So, um, the, uh, good call.
*  Yeah.
*  And early days.
*  Yes.
*  It's so interesting.
*  I'm looking for my about about, oh, Mark is looking for a job.
*  So, um, so the what's new page, I would literally get up every morning
*  and I would every afternoon.
*  Um, and I would basically, if you wanted to launch a website, you would email me.
*  Um, and I would list it on the what's new page.
*  And that was how people discovered the new websites as they were coming out.
*  And I remember, cause it was like one, it literally went from, it was like one
*  every couple of days to like one every day to like two every day.
*  Boom, boom, boom.
*  And then.
*  So you're doing it.
*  So that, that blog was kind of doing the directory thing.
*  So like, what was the homepage?
*  Uh, so the homepage was just basically trying to explain even what this thing
*  is that you're looking at, right?
*  The basic, basically basic instructions.
*  Um, but then there was a button, there was a button that said what's new.
*  And what most people did was they went to, for obvious reasons, went to what's new.
*  But like, it was so, it was so mind blowing at that point.
*  Just the basic idea.
*  And it was just, it was just like, you know, this was the basic idea of the
*  internet, but people can see it for the first time.
*  The basic idea was, look, you know, some, you know, it's like, literally
*  it's like an Indian restaurant in like Bristol, England has like put their
*  menu on the web and people were like, wow, because like that's the first restaurant
*  menu on the web and I don't have to be in Bristol and I don't know if I'm ever
*  going to go to Bristol and I don't like Indian food and like, wow.
*  Right.
*  Um, and it was like that, uh, the first web, uh, the first streaming video
*  thing was a, uh, it was another England, some Oxford or something.
*  Um, some guy, uh, puts, uh, his coffee pot up as the first, uh, streaming, uh,
*  video thing and he put it on the web because he literally, it was the coffee
*  pot down the hall and he wanted to see when he needed to go refill it.
*  Um, but there were, you know, there was a point when there were thousands of
*  people like watching that coffee pot because it was the first thing you could watch.
*  Well, but isn't, uh, were you able to kind of infer, you know, if that
*  Indian restaurant could go online, then you're like, they all will.
*  They all will.
*  Yeah, exactly.
*  So you felt that.
*  Yeah, yeah, yeah.
*  Now, you know, look, it's still a stretch, right?
*  It's still a stretch.
*  Cause it's just like, okay, is it, you know, you're still in this zone, which
*  is like, okay, is this a nerd thing?
*  Is this a real person thing?
*  Yeah.
*  Um, by the way, we, you know, there was a wall of skepticism from the media.
*  Like they just, like everybody was just like, yeah, this is the
*  crazy, this is just like, um, this is not, you know, this is not for
*  regular people at that time.
*  Um, and so you, you had to think through that and then look, it was still, it
*  was still hard to get on the internet at that point, right?
*  So you could get kind of this weird bastardized version if you were on
*  AOL, which wasn't really real, or you had to go like learn what an ISP was.
*  Um, you know, in those days, PCs actually didn't have TCP IP drivers come pre-installed.
*  So you had to learn what a TCP IP driver was.
*  You had to buy a modem, you had to install driver software.
*  Um, I have a comedy routine.
*  I do something like 20 minutes long describing all the steps required
*  to actually get on the internet.
*  And so you, you had to, you had to look through these practical and then, and
*  then, uh, and then speed performance in 14, four modems, right?
*  Like it was like watching, you know, glue dry, um, like, and so you had to, you
*  had to, there were basically a sequence of bets that we made where you basically
*  needed to look through that current state of affairs and say, actually, there's
*  going to be so much demand for that.
*  Once people figure this out, there's going to be so much demand for it, that all
*  of these practical problems are going to get fixed.
*  Some people say that the anticipation makes the, the destination that much more
*  exciting.
*  Do you remember progressive JPEGs?
*  Yeah.
*  Do I, do I, so for kids, for kids in the audience, right?
*  We, you used to have to wash an image load like a line at a time, but it turns
*  out there was this thing with JPEGs where you could, you could load basically
*  every fourth, you could load like every fourth, uh, line and then, and then you
*  could sweep back through again.
*  And so you could like render a fuzzy version image upfront and then it would
*  like resolve into the detailed one.
*  And that was like a big UI breakthrough because it gave you something to watch.
*  Yeah.
*  And, uh, you know, there's applications in various domains for that.
*  Uh, well, it's a big fight.
*  If there was a big fight early on about whether there should be images on the web.
*  Um, for that reason, for like sexualization.
*  No, not, not explicitly that that did come up, but it wasn't even that it was more
*  just like all the serious and the argument went, the purists basically said all the
*  serious information in the world is text.
*  If you introduce images, you basically going to bring in all the trivial stuff.
*  You're going to bring in magazines and you know, all this crazy stuff that, you
*  know, people, you know, it's going to distract from that.
*  It's going to go take, take away from being serious, being frivolous.
*  Well, was there any, uh, do more type arguments about, uh, the internet destroying
*  all of human civilization or destroying some fundamental fabric of human civilization?
*  Yeah.
*  So it was those days it was all around crime and terrorism.
*  Um, so those arguments happened.
*  Um, you know, but there was no sense yet of the internet having like an effect on
*  politics or cause that was, that was way too, too far off.
*  But, um, there was an enormous panic at the time around cyber crime.
*  There was like enormous panic that like your credit card number would get stolen
*  and you'd use life savings to be drained.
*  And then, you know, criminals were going to, there was, oh, um, when we started,
*  one of the things we did, one of the, the Netscape browser was the first widely
*  used piece of consumer software that had strong encryption built in, made it
*  available to ordinary people.
*  And at that time, strong encryption was actually illegal to export out of the
*  U S so we could feel that product in the U S we could not export it because it
*  was, it was classified as ammunition.
*  Um, so the Netscape browser was on a restricted list, along with the Tom
*  missile as being something that could not be exported.
*  So we had to make a second version with deliberately weak encryption to sell
*  overseas with a big logo on the box saying, do not trust this, which it turns
*  out makes it hard to sell software.
*  Uh, when it's got a big logo that says, don't trust it.
*  Um, and then we had to spend five years fighting the U S government to get them
*  to basically stop trying to do this.
*  Um, but, but, cause the fear, the fear was terrorists are going to use
*  encryption, right?
*  To like plot, you know, all these, all these, all these things.
*  Um, and then, you know, we, we responded with, well, actually we need encryption
*  to be able to secure systems so that the terrorists and the criminals can't get
*  into them.
*  So that was anyway, that was the, that was the 1990s fight.
*  So, uh, can you say something about some of the details of the software
*  engineering challenges required to build these browsers?
*  I mean, the engineering challenges of creating a product that hasn't really
*  existed before that can have such, uh, almost like limitless, uh, impact on
*  the world with the internet.
*  So there was a really key bet that we made at the time, which is very
*  controversial, which was core to the core to how it was engineered, which was, are
*  we optimizing for performance, um, or for ease of creation?
*  And in those days, the pressure was very intense to optimize for performance
*  because the network connections were so slow and also the computers were so slow.
*  Um, and so if you had, I mentioned the progressive JPEG is like, if, if, if
*  there's an alternate world in which we optimize for performance and it just, you
*  had just a much more pleasant experience right up front.
*  But what we got by not doing that was we got ease of creation and the way that we
*  got ease of creation was all of the protocols and formats were in text, not
*  in binary.
*  Um, and so HTTP is in text, by the way, and this was an internet tradition, by
*  the way, that we picked up, but we continued to HTTP is text, um, and, uh,
*  HTML is text and then every else, everything else that followed is text.
*  Um, as a result, and by the way, you can imagine purist engineers saying this is
*  insane, you have very limited bandwidth.
*  Why are you wasting any time sending text?
*  You should be encoding this stuff into binary and it'll be much faster.
*  And of course the answer is that's correct.
*  Um, but what you get when you make it text is all of a sudden, well, the big
*  breakthrough was the view source function, right?
*  So the fact that you could look at a webpage, you could hit view source and
*  you could see the HTML.
*  That was how people learned how to make web pages.
*  Right.
*  It's so interesting because the stuff we take for granted now is, uh, man, that
*  was fundamental to the development of the web to be able to have HTML just
*  right there, all the ghetto mess that is HTML, all the sort of almost biological
*  like messiness of HTML and then having the browser try to interpret that mess,
*  to show something reasonable.
*  Well, and then there was this internet principle that we inherited, which was
*  emit, what was it emit cautiously emit conservatively interpret liberally.
*  So it basically meant if you're in the design principle was if you're, if you're
*  creating like a web editor, it's going to emit HTML, like do it as cleanly as you
*  can, but you actually want the browser to interpret liberally, which is you actually
*  want users to be able to make all kinds of mistakes and for it to still work.
*  And so the browser rendering engines to this day have all of this spaghetti code,
*  crazy stuff where they can, they're resilient to all kinds of crazy HTML
*  mistakes.
*  And so, and literally what I always had in my head is like, there's an eight
*  year old or an 11 year old somewhere and they're doing a view source, they're
*  doing a cut and paste and they're trying to make a webpage for their turtle or
*  whatever.
*  And like they leave out a slash and they leave out an angle bracket and they do
*  this and they do that and it still works.
*  It's, it's, it's also like, I don't often think about this, but you know,
*  programming, you know, C plus plus C C plus plus all those languages, Lisp, the
*  compiled language, the interpreted language is Python, Pearl, all of that.
*  They, they, the brace have to be all correct.
*  Yes.
*  Like everything has to be perfect.
*  Brutal.
*  And then you forget, all right.
*  It's systematic and rigorous.
*  Let's go there.
*  But you forget that the, uh, uh, the web with JavaScript eventually, uh, and
*  HTML is allowed to be messy in the way for the first time, messy in the way
*  biological systems could be messy.
*  It's like the only thing computers were allowed to be messy on for the first time.
*  It used to offend me.
*  So I grew up in Unix.
*  So I worked on Unix.
*  I was a Unix native for all the way through this period.
*  Um, and so, and it used to drive me bananas when it would do the segmentation
*  fault in the core dump file, just like, you know, it's like, literally there's
*  like an error in the code, the math is off by one and it core dumps and I'm in
*  the core dump trying to analyze it and trying to reconstruct what, and I'm just
*  like, this is ridiculous.
*  Like the computer ought to be smart enough to be able to know that if it's
*  off by one, okay, fine.
*  And it keeps running.
*  And I would go ask all the experts like, why can't it just keep running?
*  And they'd explain to me, well, because all the downstream repercussions
*  blah, blah.
*  And I'm like, this still like, you know, we're forcing the human creator to
*  live to your point in this hyper literal literal world of perfection.
*  And I was just like, that's, that's just, that's just bad.
*  And by the way, you know, cause what happens with that, of course, just what
*  what happened with, with coding at that point, which is you get a high priesthood.
*  You know, there's a small number of people who are really good at doing exactly that.
*  Most people can't and most people are excluded from it.
*  And so actually that, that was where that was where I picked up that idea was, um,
*  uh, was like, no, no, you want, you want, you want these things to be resilient
*  to error in all kinds.
*  And this, this would drive the purists absolutely crazy.
*  Like I got attacked on this like a lot, cause yeah, I mean, like every time I,
*  you know, all the purists who were like in all this like markup language stuff
*  and formats and codes and all this stuff, they would be like, you know, you can't,
*  you're, you're encouraging bad behavior.
*  Cause also they wanted the browser to give you an, uh, a seg fault error.
*  Anytime there was a, yeah, they wanted to be a, right.
*  They wanted to, yeah, that was a very, any, any, any properly
*  trained and credentialed engineer would be like, that's not how you build these
*  systems.
*  That's such a bold move to say, no, it doesn't have to be.
*  Yeah.
*  Now, like I said, the, the good news for me is the internet kind of had that
*  tradition already.
*  Um, but we, but having said that, like we pushed it, we pushed it way out.
*  But the other thing we did going back to the performance thing was we gave up a
*  lot of performance.
*  We made that, that initial experience for the first few years was pretty painful.
*  But, but the bet there was actually an economic bet, which was basically the
*  demand for the web would basically mean that there would be a surge in supply of
*  broadband.
*  We, we, because the question was, okay, how do you get, how do you, how do you
*  get the phone companies, which are not famous in those days for doing new things
*  at huge cost for like speculative reasons?
*  Like how do you get them to build up broadband, you know, spend billions of
*  dollars doing that.
*  And, you know, you could go meet with them and try to talk them into it, or you
*  could just have a thing where it's just very clear that it's going to be the
*  people love.
*  That's going to be better if it's faster.
*  And so that, that, that there was a period there, and this was, this was fraught
*  with some peril, but there was a period there where it's like, we knew the
*  experience was sub optimized because we were trying to force the emergence of
*  demand for broadband, which is in fact what happened.
*  So you had to figure out how to display this text, HTML text.
*  So the blue links and the purple links, and there's no standards.
*  Is there standards at that time?
*  No, there really still isn't.
*  Well, there's like, there's implied, implied standards, right?
*  And they, you know, there's all these kinds of new features that are being
*  added, like CSS, what, like what kind of stuff a browser should be able to support
*  features within languages, within JavaScript and so on.
*  But you, you bait, you're setting standards on the fly yourself.
*  Well, to this day, if you, if you create a webpage that has no CSS style sheet,
*  the browser will render it however it wants to.
*  Right.
*  So this was one of the things there was this idea, this idea at the time and how
*  these systems were built, which is separation of content from a format or
*  separation of content from appearance.
*  And that's still, people don't really use that anymore because everybody wants to
*  determine how things look.
*  And so they use CSS, but it's still in there that you can just let the browser
*  do all the work.
*  I still like the, like, really basic websites, but that could be just old
*  school kids these days with their fancy responsive websites that don't actually
*  have much content, but have a lot of visual elements.
*  Well, that's one of the things that's fun about chat, you know, about chat
*  LGBT.
*  It's like back to the basics back to just text.
*  Yeah.
*  Right.
*  And you know, there, there is this pattern in human creativity and media where you
*  end up back at text.
*  And I think there's, you know, there's something powerful in there.
*  Is there some other stuff you remember like the purple links?
*  There were some interesting design decisions that to kind of come up that we
*  have today or we don't have today.
*  They were temporary.
*  So we made, I made the background gray.
*  I hated reading texts on white backgrounds.
*  And so I made the background gray.
*  Do you regret?
*  No, no, no, no.
*  That's that decision.
*  I think has been reversed.
*  But now I'm happy though, because now dark mode is the thing.
*  So, so it wasn't about gray.
*  It was just, you didn't want to wipe back off.
*  Strain my eyes.
*  Strange your eyes.
*  Interesting.
*  Um, and then there's a bunch of other decisions.
*  I'm sure there's an interesting history of the development of HTML and CSS and
*  hollows and interface and JavaScript.
*  And there's this whole Java applet thing.
*  Well, the big one, probably JavaScript CSS was after me.
*  So I didn't, that was not me, but, um, JavaScript wasn't the big,
*  JavaScript maybe was the biggest of the whole thing.
*  That was us.
*  Um, and, um, and that was basically a bet.
*  It was a bet on two things.
*  One is that the world wanted a new front end scripting language.
*  Um, and then the other was that we thought at the time the world wanted a
*  new backend scripting language.
*  Um, so JavaScript was designed from the beginning to be both front end and
*  backend, and then it failed as a backend scripting language and, uh, Java
*  one, um, for a long time and then Python, Pearl and other things, PHP, um, and
*  Ruby, but now JavaScript is back.
*  And so,
*  I wonder if everything in the end will run on JavaScript.
*  It's see, it seems like it is the, um, and by the way, let me give a shout out
*  to, uh, to, um, uh, uh, uh, Brendan Ike, uh, was the, uh, basically the one man
*  inventor of, um, uh, of JavaScript.
*  If you're interested to learn more about Brendan, Ike, he's been on his
*  podcast previously.
*  Exactly.
*  So he wrote JavaScript over a summer.
*  Um, and it, I mean, I think it is fair.
*  It is fair to say now that it's the most widely used language in the world and it
*  seems to only be gaining in, in, um, in its, uh, in its range of adoption.
*  In the software world, there's quite a few stories of somebody over a weekend,
*  over a week or over a summer writing some of the most, uh, impactful revolutionary
*  pieces of software ever.
*  Well, look, that should be inspiring.
*  Yes.
*  Very inspiring.
*  I'll give you another one.
*  SSL.
*  Um, so SSL was the security protocol.
*  That was us.
*  And that was a crazy idea at the time, which was, let's take all the native
*  protocols and let's wrap them in a security wrapper.
*  That was a guy named Kip Hickman who wrote that over a summer, uh, one guy.
*  Um, and then look today sitting here today, like the transformer, like at
*  Google was a small handful of people.
*  And then, you know, the number of people who have did like the core work on GPT.
*  It's not that many people.
*  It's a pretty small handful of people.
*  Um, and so yeah, the pattern in software repeatedly over a very long time has been.
*  It's, it's a, Jeff, Jeff Bezos always had the two pizza rule, uh, for teams at
*  Amazon, which is any team needs to be able to be fed with two pizzas.
*  If you need the third pizza, you have too many people.
*  And I think that's, I think that's, I think it's actually the one pizza rule.
*  Yeah.
*  For the, for the really creative work.
*  I think it's two people, three people.
*  Well, that's, you see that with certain open source projects, like so much is
*  done by like one or two people.
*  Yeah.
*  Like it's, it's so incredible.
*  And that's why you see that gives me so much hope about the open source movement
*  this new age of AI where, um, you know, just recently having had a conversation
*  with, with Mark Zuckerberg of all people who's all in on open source, which is so
*  interesting to see and so inspiring to see, cause like releasing these models.
*  It is scary.
*  It is potentially very dangerous.
*  And we'll talk about that, but it's also.
*  If you believe in the goodness of most people and in the skill set of most people
*  and the desire to do good in the world, that's really exciting.
*  Cause it's not putting it, these models into the centralized control of big
*  corporations, the government and so on.
*  It's putting in the, in the hands of a teen teenage kid with like a dream in his eyes.
*  I don't know.
*  That's, um, that's beautiful.
*  And look, this stuff, AI ought to make the individual coder obviously far more
*  productive, right?
*  By like, you know, a thousand X or something.
*  And so you ought to open source, like not just the future of open source, but the
*  future of open source, everything.
*  We ought to have a world now of super coders, right?
*  Who are building things as open source with one or two people that were
*  inconceivable, you know, five years ago.
*  Um, you know, the level of kind of hyper productivity we're going to get out of
*  our best and brightest, I think it's going to go way up.
*  It's going to be interesting.
*  We'll talk about it, but let's just to linger a little bit on Netscape.
*  Netscape was acquired in 1999 for 4.3 billion by AOL.
*  What was that?
*  Uh, what was that like?
*  What was, what were some memorable aspects of that?
*  Well, that was the height of the dot com boom bubble bust.
*  I mean, that was the, that was the frenzy.
*  Um, if you watch a succession, that was the, that was like what they did in the
*  fourth season with, uh, with Gojo and then merger with, uh, with there.
*  And so it was like the height of like one of those kind of dynamics.
*  And so you recommend succession by the way, I'm more of a Yellowstone guy.
*  Yellowstone is very American.
*  I'm, I'm very proud of you.
*  That's it.
*  I just talked to Matthew McConaughey and I'm full on Texan at this point.
*  I heartily approve.
*  Um, and, uh, he will be doing the sequel to the Yellowstone.
*  Yeah.
*  Very exciting.
*  Anyway.
*  Okay.
*  Uh, so that's a rude interruption by me, uh, by way of succession.
*  Uh, so that was at the height of the deal making and money and just the
*  fur flying and like craziness.
*  And so, yeah, it was just one of those.
*  It was just like, I mean, there's the entire Netscape thing from start to
*  finish was four years, um, which was like for, for one of these companies,
*  it's just like incredibly fast.
*  You know, it went public 18 months after we got moved, we were founded,
*  which virtually never happens.
*  So it was just this incredibly fast kind of meteor streaking across the sky.
*  Um, and then of course it was this, and then there was just this explosion,
*  right?
*  That happened because then it was almost immediately followed by the.com crash.
*  It was then followed by hail well, by time Warner, which again, is the
*  succession guys kind of play with that, uh, which turned out to be a disastrous
*  deal, um, one of the famous, you know, kind of disasters in business history.
*  Um, and then, um, and then, you know, what became an internet depression on
*  the other side of that.
*  But then in that depression in the 2000s was the beginning of broadband and
*  smartphones and web 2.0, right?
*  And then social media and search and every SAS and everything that came out of that.
*  So what did you learn from just the acquisition?
*  I mean, this is so much money.
*  What's interesting because I must've been very new to you that the software
*  stuff you can make so much money.
*  There's so much money swimming around.
*  I mean, I'm sure the ideas of investment was starting to get born there.
*  Yes.
*  Let me get, let's say, let me lay out late.
*  So here's, here's the thing.
*  I don't know if I figured out them, but figured out later, which is, um, software
*  is a technology that it's like a, you know, the concept of the philosopher stone,
*  the philosopher stone and alchemy transmutes lead into gold and Newton spent
*  20 years trying to find the philosopher stone.
*  Never got there.
*  Nobody's ever figured it out.
*  Software is our modern philosopher stone.
*  And in economic terms, it transmutes labor into capital, which is like a super
*  interesting thing.
*  And by the way, like Karl Marx is rolling over in his grave right now.
*  Cause of course that's complete refutation of his entire theory.
*  Um, trust you, slavery and capital, which is, which is as follows is somebody sits
*  down at a keyboard and types a bunch of stuff in and a capital asset comes out the
*  other side, and then somebody buys that capital asset for a billion dollars.
*  Like that's amazing.
*  Right.
*  It's literally creating value right out of thin air, right.
*  Out of purely human thought.
*  Right.
*  Um, and so that that's, there are many things that make software magical and
*  special, but that's the economic.
*  So I wonder what Marx would have thought about that.
*  Oh, he would have completely broke his brain because of course the whole, the
*  whole thing was, it was, you know, that kind of technology is inconceivable when
*  he was alive, it was all, it was all industrial era stuff.
*  And so the, any kind of machinery necessarily involves huge amounts of
*  capital and then labor was on the, on the, on the receiving end of the abuse.
*  Yep.
*  Um, right.
*  But like software, software, a software engineer is somebody who basically
*  transmits his own labor into actual national capital asset, um, creates
*  permanent value.
*  Well, in fact, it's, it's actually very inspiring.
*  Um, that's actually more true today than before.
*  So when, when I was doing software, the assumption was all new software
*  basically has a sort of a parabolic sort of life cycle, right?
*  So you, you ship the thing, people buy it at some point, everybody who wants
*  it has bought it and then it becomes obsolete and it's like bananas.
*  Nobody, nobody buys old software.
*  Um, these days, um, Minecraft, um, Mathematica, you know, Facebook, Google,
*  um, you have the software assets that are, you know, have been around for 30 years
*  that are gaining in value every year.
*  Right.
*  And they're just, they're being a world of Warcraft, right?
*  Salesforce.com.
*  Like they're being, every single year they're being polished and polished
*  and polished and polished.
*  They're getting better and better, more powerful, more powerful, more
*  valuable, more valuable.
*  So we've entered this era where you can actually have these things that
*  actually build out over decades, which by the way is what's happening right now
*  with like GPT, um, and so, um, now, and this is why, you know, there, there,
*  there is always, you know, sort of a constant investment frenzy around
*  software is because, you know, look, when you start one of these things, it
*  doesn't always succeed, but when it does now, you might be building an asset
*  that builds value for, you know, four or five, six decades to come.
*  Um, you know, if you have a team of people who have the level of devotion
*  required to keep making it better.
*  And then the fact that of course everybody's online, you know, there's
*  5 billion people that are a click away from any new piece of software.
*  So the potential market size for any of these things is, you know, nearly infinite.
*  They must've been surreal back then though.
*  Yeah.
*  Yeah.
*  Th th this was all brand new, right?
*  Yeah.
*  Back then, this was all brand new.
*  These were all, you know, brand new.
*  It had you rolled out that theory in, in even 1999, people would have thought
*  you were smoking crack.
*  So that's, that's emerged over time.
*  Well, let's, uh, now turn back into the future.
*  You wrote the essay, why AI will save the world.
*  Let's start the very high level.
*  What's the main thesis of the essay?
*  Yeah.
*  So the main thesis on the essay is that what we're dealing with here is intelligence.
*  Um, and it's really important to kind of talk about the sort of
*  very nature of what intelligence is.
*  And fortunately we have a, uh, we have a predecessor to machine intelligence,
*  which is human intelligence.
*  And we've got, you know, observations and theories over thousands of years for
*  what, what, what intelligence is in the hands of humans and what intelligence is.
*  Right.
*  I mean, what it, what it literally is, is the way to, uh, you know, capture
*  process, analyze, synthesize information, solve problems.
*  Um, but the observation of, of, of intelligence in human hands is that
*  intelligence quite literally makes everything better.
*  Um, and what I mean by that is every kind of outcome of like human quality of life,
*  whether it's education outcomes or success of your children or a career
*  success or health or lifetime satisfaction.
*  Um, by the way, um, uh, uh, propensity to peacefulness as opposed to violence,
*  uh, propensity for open-mindedness versus bigotry.
*  Um, those are all associated with higher levels of intelligence.
*  Smarter people have better outcomes than almost as you write in almost every
*  domain of activity, academic achievement, job performance, occupational status,
*  income, creativity, physical health, longevity, learning new skills, managing
*  complex tasks, leadership, entrepreneurial success, conflict resolution,
*  reading comprehension, financial decision-making, understanding others
*  perspectives, creative arts, parenting outcomes, and life satisfaction.
*  One of the more depressing conversations I've had, and I don't know why it's
*  depressing after really think through why it's depressing, but on IQ and, uh,
*  the G factor and that that's something in large part is genetic and it's
*  correlates so much with all of these things and success in life.
*  It's like all the inspirational stuff we read about, like if you work hard and so
*  on, damn, it sucks that you're born with the hand that you can't change.
*  But what if you could?
*  You're saying basically a really important point.
*  And I think it's a, uh, in your, in your articles, it really helped me.
*  Um, it's a nice added perspective to think about, listen, human intelligence,
*  the science of intelligence is shown scientifically that it just makes life
*  easier and better the smarter you are.
*  And now let's look at artificial intelligence.
*  And if, uh, that's a way to increase the, the, the, the, some human intelligence,
*  then it's only going to make a better life.
*  Yeah.
*  That's the argument.
*  And certainly at the collective level, we could talk about the collective effect
*  of just having more intelligence in the world, which, which, which
*  will have very big payoff.
*  But there's also just at the individual level, like what if every person has a
*  machine, you know, and it's a concept of augment, Doug Engelbart's concept of
*  augmentation, um, you know, what if everybody has a, an assistant and the
*  assistant is, you know, 140 IQ, um, and you happen to be 110 IQ, um, and you've
*  got, you know, something that basically is infinitely patient and knows everything
*  about you and is pulling for you in every possible way, wants you to be successful.
*  And anytime you find anything confusing or want to learn anything or have trouble
*  understanding something, or want to figure out what to do in a situation,
*  right, when I figure out how to prepare for a job interview, like any of these
*  things, like it will help you do it.
*  And it will therefore, the combination will effectively be, you know, will
*  effectively raise your raise because it will effectively raise your IQ will
*  therefore raise the odds of successful life outcomes in all these areas.
*  So people below the, this hypothetical 140 IQ, it'll pull them up towards 140 IQ.
*  Yeah.
*  Yeah.
*  And then of course, you know, people at, people at 140 IQ will be able to have a
*  peer, right, to be able to communicate, which is great.
*  And then people above 140 IQ will have an assistant that they can farm things out to.
*  And then look, God willing, you know, at some point, these things go from future
*  versions, go from 140 IQ equivalent to 150 to 160 to 180, right?
*  Like Einstein was estimated to be on the order of 160.
*  Um, you know, so when we get, you know, 160 AI, like we'll be, you know, when one
*  assumes creating Einstein level breakthroughs in physics and, and then,
*  and then at 180 will be, you know, curing cancer and developing warp drive
*  and doing all kinds of stuff.
*  And so it is quite possibly the case.
*  This is the most important thing that's ever happened.
*  The best thing that's ever happened because precisely because it's a lever on
*  this single fundamental factor of intelligence, which is the thing that
*  drives so much of everything else.
*  Can you still imagine the case that human plus AI is not always better
*  than human for the individual?
*  You may have noticed that there's a lot of smart assholes running around.
*  Sure.
*  Yes.
*  Right.
*  And so like it's smart.
*  It's there, there are certain people where they get smarter, you know, they're,
*  they're, they get to be more arrogant, right?
*  So they, you know, there's one huge flaw.
*  Although to push back on that, it might be interesting because when the
*  intelligence is not all coming from you, but from a system, from another system,
*  that might actually increase the, the amount of humility, even in the assholes.
*  One would hope.
*  Yeah.
*  Um, or it could make assholes more asshole.
*  You know, that's, I mean, that's, that's for psychology to study.
*  Yeah, exactly.
*  Another one is, um, the smart people are very convinced that they, you know,
*  have a more rational view of the world and that they have a easier time seeing
*  through conspiracy theories and hoaxes and right, you know, sort of crazy
*  beliefs and all that there, there's a theory in psychology, which is actually
*  smart people, so for sure, people who aren't as smart are very susceptible
*  to hoaxes and conspiracy theories.
*  Yeah.
*  But it may also be the case that the smarter you get, you become
*  susceptible in a different way.
*  Uh, which is you become very good at marshaling facts to fit preconceptions.
*  Right.
*  Um, you become very, very good at assembling whatever theories and frameworks
*  and pieces of data and graphs and charts you need to validate whatever crazy
*  ideas got in your head.
*  And so you're susceptible in a different way.
*  Right.
*  Uh, we're all sheep, but different colored sheep, some sheep are better
*  at justifying it, right.
*  Um, and those are the, you know, those are the smart sheep, right.
*  Um, so yeah, look like it, I would say this, look like there are no panacea.
*  I'm not, I'm not a utopian.
*  There are no panaceas in life.
*  Um, there are no, like, you know, I don't believe they're like pure positives.
*  I'm not a transcendental kind of person like that, but, you know, so yeah,
*  there are going to be issues.
*  Um, uh, and, um, and you know, look smart people, maybe you could say about smart
*  people as they are more likely to get themselves in situations that are, you
*  know, beyond their grasp, you know, because they're just more confident in
*  their ability to deal with complexity and their, their eyes become bigger.
*  Their, their cognitive eyes become bigger than their stomach.
*  You know, so yeah, you could argue those eight different ways.
*  Nevertheless on net, right.
*  Clearly, overwhelmingly, again, if you just extrapolate from what
*  we know about human intelligence, you're, you're improving so many aspects of
*  life, if you're upgrading intelligence.
*  So there'll be assistance at all stages of life.
*  So when you're younger, there's for education, all that kind of stuff,
*  for mentorship, all of this.
*  And, uh, later on as you're doing work and you've developed a skill and you're
*  having a profession, you'll have an assistant that helps you excel at that
*  profession.
*  So at all stages of life.
*  Yeah.
*  I mean, look, the theory is augmentations.
*  This is the Degengel-Bart's term for it.
*  Degengel-Bart made this observation many, many decades ago that, you know,
*  basically it's like, you can have this oppositional frame of technology where
*  it's like us versus the machines.
*  But what you really do is you use technology to augment human capabilities.
*  And then, by the way, that's how actually the economy develops.
*  That's, we can talk about the economic side of this, but that's actually how
*  the economy grows is through, through technology augmenting human, human potential.
*  And so, yeah.
*  And then you've, you basically have a proxy or a, you know, or, or, or a, a,
*  um, you know, a sort of prosthetic, um, you know, so like you've got glasses,
*  you've got a wristwatch.
*  You know, you've got shoes, you know, you've got these things.
*  You've got a personal computer.
*  You've got a word processor.
*  You've got Mathematica.
*  You've got Google.
*  This is the latest viewed through that lens.
*  The AI is the latest in a long series of basically augmentation methods, uh, to
*  be able to raise human capabilities.
*  It's just, this one is the most powerful one of all, because this is the one that
*  goes directly to what they call fluid intelligence, which is IQ.
*  Well, there's a two categories of folks that you outline that, uh, they're worried
*  about or highlight the risks of AI and you highlight a bunch of different risks.
*  I would love to go through those risks and just discuss them, brainstorm which
*  ones are serious and which ones are less serious, but first the, the Baptist and
*  the bootleggers, what are these two interesting groups of folks who, uh, who,
*  who are, who, who worry about, uh, the effect of AI on human civilization.
*  Or say they do.
*  Say, say, okay.
*  Yes, I'll say they do.
*  The Baptist worry, the bootleggers say they do.
*  Yeah.
*  Um, so the Baptist and the bootleggers is a metaphor from economics, um, from
*  what's called development economics.
*  And it's this observation that when you get social reform movements, um, in a
*  society, um, you tend to get two sets of people showing up, arguing for the
*  social reform.
*  Um, and the, the, the term that Baptist and bootleggers comes from the American
*  experience with alcohol prohibition.
*  Um, and so in the 1900s, 1910s, um, there was this movement that was very passionate
*  at the time, which basically said alcohol is evil, uh, and it's destroying society.
*  Um, by the way, there was a lot of evidence to support this.
*  Um, there were very high rates of, uh, very high correlations then, by the way,
*  and now, uh, between rates of physical violence and alcohol use, um, almost all
*  violent crimes have either the perpetrator or the victim or both drunk.
*  Almost all, if you see this actually in the work, almost all social harassment
*  cases in the workplace, it's like at a company party and somebody's drunk.
*  Like it's, it's amazing how often alcohol actually correlates to actually
*  dysfunction and these two domestic abuse, um, and so forth, child abuse.
*  And so you had this group of people who were like, okay, this, this is bad
*  stuff and we should outlaw it.
*  And those were quite literally the Baptists.
*  Those were super committed, you know, hardcore Christian activists in a lot of
*  cases, there was this woman, uh, whose name was Carrie nation, um, who was this
*  older woman who had been in this, you know, I don't know, disastrous marriage
*  or something, and her husband had been abusive and drunk all the time and she
*  became the icon of the Baptist, uh, prohibitionists.
*  And she was legendary in that era for carrying an ax, um, and doing, you know,
*  completely on her own, doing raids of saloons and like taking her acts to all
*  the bottles and sags in the back.
*  And so, so a true believer, an absolute true believer, um, with absolutely the
*  purest of intentions.
*  And again, there's a very important thing here, which is there's, you could look at
*  this cynically and you could say the Baptists are like delusional, you know,
*  extremists, but you can also say, look, they're right.
*  Like she was, you know, she had a point.
*  Yeah.
*  She wasn't wrong about a lot of what she said.
*  Yep.
*  But it turns out the way the story goes is it turns out that there were another
*  set of people who very badly wanted to outlaw alcohol in those days.
*  And those were the bootleggers, which was organized crime that stood to make a
*  huge amount of money if legal alcohol sales were banned.
*  Um, and this was in fact, the way the history goes is this was actually the
*  beginning of organized crime in the U S this was the big economic opportunity
*  that opened that up.
*  Um, and so they went in together, um, and then they didn't go in together.
*  Like the Baptist did not even necessarily know about the bootleggers because they
*  were on their moral crusade.
*  The bootleggers certainly knew about the Baptist and they were like, wow, this is,
*  these people are like the great front people for like, you know, it's good PR
*  shenanigans in the background.
*  Yeah.
*  And they got the full stead act passed.
*  Right.
*  And they did in fact ban alcohol in the U S and you'll notice what happened,
*  which is people kept drinking.
*  It didn't work.
*  People kept drinking, um, that bootleggers made a tremendous amount of money.
*  Um, and then over time it became clear that it made no sense to make it illegal
*  and it was causing more problems.
*  And so then it was revoked.
*  And here we sit with legal alcohol a hundred years later with all the same
*  problems.
*  Um, and you know, the whole thing was this like giant misadventure.
*  Um, the Baptist got taken advantage of by the bootleggers and the bootleggers
*  got what they wanted.
*  And, and that was that
*  the same two categories of folks are now, uh, sort of suggesting that, uh, the
*  development of artificial intelligence should be regulated.
*  A hundred percent.
*  Yeah.
*  It's the same pattern.
*  And the economist will tell you it's the same pattern every time.
*  Like this is what happened to nuclear power.
*  This is what happened, which is another interesting one, but like, yeah, this
*  is, this happens dozens and dozens of times, um, throughout the last hundred
*  years and, and, and this is what's happening now.
*  And you write that it isn't sufficient to simply identify the actors and impugn
*  their motives.
*  We should consider the arguments of both the Baptist and the bootleggers
*  on their merits.
*  So let's do just that.
*  Risk number one.
*  Uh, will AI kill us all?
*  Yes.
*  So, uh, what do you, what do you think about this one?
*  This, this, what do you think is the core argument here that, uh, the
*  development of AGI perhaps better said, uh, will destroy human civilization?
*  Well, first of all, you just did a slight of hand because we went from
*  talking about AI to AGI.
*  Is there a fundamental difference there?
*  I don't know.
*  What's AGI?
*  I what's AI.
*  What's intelligence?
*  I know what AI is.
*  AI is machine learning.
*  What's, what's AGI?
*  I think we don't know what the bottom of the well of machine learning is
*  or what the ceiling is.
*  Cause just, uh, to call something machine learning or just to call
*  some of the statistics or just to call it math or computation doesn't mean,
*  you know, uh, nuclear weapons are just physics.
*  So it's, it's, to me, it's very interesting and surprising how far
*  machine learning has taken.
*  No, but we knew that nuclear physics would lead to weapons.
*  That's why the scientists of that era were always in some of this huge
*  dispute about building the weapons.
*  This is different.
*  AGI is different.
*  Where does machine learning lead?
*  Do we know?
*  We don't know, but this is my point is different.
*  We actually don't know.
*  But, and, and this is where you, the sleight of hand kicks in, right?
*  This is where it goes from being a scientific topic to being a religious topic.
*  Um, and that's why, that's why I specifically called out the, cause
*  that's what happens.
*  They do the vocabulary shift and all of a sudden you're talking about something
*  totally that's not actually real.
*  Well, then maybe you can also, uh, as part of that, define the Western
*  tradition of millennialism.
*  Yes.
*  End of the world.
*  Apocalypse apocalypse, apocalypse cults.
*  Um, well, so we live in, we live in a world where we have a
*  we of course live in a Judeo Christian, but primarily Christian kind of saturated,
*  you know, kind of Christian post-Christian secularized Christian, you know, kind
*  of world in the West.
*  Um, and of course, quarter Christianity is the idea of the second coming and, and
*  you know, the revelations and you know, Jesus returning and a thousand year, you
*  know, utopia on earth and then the, you know, the rapture and like all that stuff.
*  You know, we don't, we, you know, we collectively, you know, as a society, we
*  don't necessarily take all that fully seriously now.
*  So what we do is we create our secularized versions of that.
*  We keep, we keep looking for utopia.
*  We keep looking for, you know, basically the end of the world.
*  And so what you see over, over decades is that basically a pattern of these sort of
*  of these, of these, of this, this is what cults are.
*  This is how cults form as they form around some theory of the end of the world.
*  And so the people's temple cult, the Manson cult, the heavens gate cult, the
*  David Koresh cult, you know, what they're all organized around is like, there's
*  going to be this thing that's going to happen.
*  That's going to basically bring civilization crashing down.
*  And then we have this special elite group of people who are going to see it coming
*  and prepare for it.
*  And then there are the people who are either going to stop it or failing,
*  stopping it.
*  They're going to be the people who survived to the other side and ultimately
*  get credit for having been right.
*  Why is that so compelling?
*  Do you think like, uh,
*  Cause it satisfies this very deep need we have for transcendence and meaning
*  that got stripped away when we became secular.
*  Yeah.
*  But why, why is the transcendence involve the destruction of human civilization?
*  Cause like how, like how plausible it's, it's like a very deep psychological
*  thing. Cause it's like, how plausible, how plausible is it that we live in a world
*  where everything's just kind of all right?
*  Right.
*  How exciting, how exciting is that?
*  Right.
*  But that's, we want more than that.
*  But that's the deep question I'm asking.
*  Why is it not exciting to live in a world where everything's just all right?
*  Does it, I think, uh, you know, most of the animal kingdom would be so happy
*  with just all right.
*  Cause that means survival.
*  Why are we, uh, maybe that's what it is.
*  Why are we conjuring up things to worry about?
*  So CS Lewis called it the God shaped hole.
*  So there's a God shaped hole in the human experience, consciousness, soul,
*  whatever you want to call it, where there's gotta be something that's
*  bigger than all this.
*  There's gotta be something transcendent.
*  There's gotta be something that is bigger, right?
*  Bigger, bigger purpose, a bigger meaning.
*  And so we have run the experiment of, you know, we're just going to use science
*  and rationality and kind of, you know, everything's just going to kind of be as
*  it appears and a large number of people have found that very deeply wanting and
*  have constructed narratives.
*  And by the, this is the story of the 20th century, right?
*  Communism, right?
*  Was one of those communism was a, was a form of this.
*  Uh, Nazism was a form of this.
*  Um, you know, some people, um, you know, you can see movements like this playing
*  out all over the world right now.
*  So you construct a kind of devil, a kind of source of evil and we're going
*  to transcend beyond it.
*  Yeah.
*  And the millenarian, the millenarians kind of, when you see a millenarian
*  cult, they put a really specific point on it, which is end of the world.
*  Right.
*  There, there, there is some change coming and that change that's coming is so
*  profound and so important that it's either going to lead to utopia or hell on
*  earth, right?
*  Um, and it is going to, and then, you know, it's like, what if you actually knew
*  that that was going to happen, right?
*  What would you, what, what would you do, right?
*  How would you prepare yourself for it?
*  How would you come together with a group of like-minded people, right?
*  How would you, what would you do?
*  Would you plan like caches of weapons in the woods?
*  Would you like, you know, I don't know if it's create under underground
*  bunkers, which you, you know, spend your life trying to figure out a way to avoid
*  having it happen.
*  Yeah.
*  That's a really compelling, exciting idea to, uh, to have a club over, to have,
*  to have a, to have a little bit of travel.
*  Like you get together on a Saturday night and drink some beers and talk about the,
*  the end of the world and how you, you are the only ones who have figured it out.
*  And then, and then once you lock in on that, like, how can you do anything
*  else with your life?
*  Like this is obviously the thing that you have to do.
*  And then, and then there's a psychological effect that you alluded to.
*  There's a psychological effect.
*  If you take a set of true believers and you leave them to themselves, they get
*  more radical because they self radicalize each other.
*  That said, it doesn't mean they're not sometimes right.
*  Yeah.
*  The end of the world might be.
*  Yes.
*  Correct.
*  Like they might be right.
*  Yeah.
*  But like we have some pamphlets for you.
*  No, it's, it, I mean, there's, I mean, we'll talk about nuclear weapons because
*  that you have a really interesting little moment that I learned about in your essay,
*  but you know, sometimes it could be right.
*  Yeah.
*  There's, we're still, you were developing more and more powerful technologies in
*  this case, and we don't know what the impact they will have on human civilization.
*  Well, we can highlight all the different predictions about how it will be positive,
*  but the risks are there and you discuss some of them.
*  Well, the steel man, the steel man is the steel man.
*  Well, actually the steel man and his reputation are the same, which is, well,
*  you can't predict what's going to happen.
*  Right.
*  You, right.
*  You can't rule out that this will not end everything.
*  Right.
*  But the response to that is you have just made a completely non-scientific claim.
*  You've made a religious claim, not a scientific claim.
*  There.
*  How does it get disproven?
*  There is, and there's no, by definition with these kinds of claims, there's no
*  way to disprove them.
*  Yeah.
*  Right.
*  Um, and so there, there's no, you just go right on the list.
*  There's no hypothesis.
*  There's no testability of the hypothesis.
*  There is no way to falsify the hypothesis.
*  There's no way to measure progress along the arc.
*  Like it's just all completely missing.
*  And so it's not scientific.
*  And I don't think it's completely missing.
*  It's, it's somewhat missing.
*  So for example, the, the, the, the people that say AI is going to kill all of us.
*  I mean, they usually have ideas about how to do that, whether it's the
*  paperclip maximizer or, um, you know, it escapes, there's mechanism by which
*  you can imagine it killing all humans.
*  And to, you can disprove it by saying there is, um, there is a limit to, uh,
*  the speed of which intelligence increases, maybe show that, uh, like the
*  sort of rigorously really described model, like how it could happen and say,
*  no, there, here's a physics limitation.
*  There's a physical limitation to how these systems would actually do
*  damage to human civilization.
*  And it is possible they will kill 10 to 20% of the population, but it seems
*  impossible for them to kill, uh, 99%.
*  There's practical counter arguments, right?
*  So you mentioned basically that what I've described as the
*  thermodynamic counter argument, which is sitting here today.
*  It's like, where would the evil AGI get the GPUs?
*  Cause like they don't exist.
*  So you're going to have a very frustrated baby evil AGI who's going to be like
*  trying to buy Nvidia stock or something to get them to finally make some chips.
*  Um, right.
*  So the, the serious form of that is the thermodynamic argument, which is like,
*  okay, where's the energy going to come from?
*  Where's the process are going to be running?
*  Where's the data center going to be happening?
*  How is this going to be happening in secret such that, you know, it's not, you
*  know, so, so that's a practical counter argument to the runaway AGI thing.
*  I have a, but I have a, and we can argue that and discuss that I have a deeper
*  objection to it, which is it's, this is all forecasting, it's all modeling.
*  It's all, it's all future prediction.
*  It's all future hypothesizing.
*  It's not science.
*  Sure.
*  It is not, it is, it is, it is the opposite of science.
*  So the pull up Carl Sagan, extraordinary claims require extraordinary proof.
*  Right.
*  These are extraordinary claims.
*  The policies that are being called for, right.
*  To prevent this are of extraordinary magnitude that, and I think we're
*  going to cause extraordinary damage.
*  And this is all being done on the basis of something that is literally not scientific.
*  It's not a testable hypothesis.
*  So the moment you say AI is going to kill all of us, therefore we should ban it or
*  that we should regulate all that kind of stuff, that's when it starts getting serious.
*  Or start, you know, military airstrikes on data centers.
*  Oh boy.
*  Right.
*  And like,
*  Yeah, this one is get starts, starts getting real weird.
*  Here's the problem with millinery and cults.
*  They have a hard time staying away from violence.
*  Yeah, but violence is so fun.
*  If you're on the right end of it, they have a hard time avoiding violence.
*  The reason they have a hard time avoiding violence is if you actually believe the
*  claim, right, then what would you do to stop the end of the world?
*  Well, you would do anything.
*  Right.
*  And so, and this is where you get, and again, if you just look at the history of
*  millinery and cults, this is where you get the people's temple and everybody
*  killing themselves in the jungle.
*  And this is where you get Charles Manson and you know, sending in, you need to
*  kill, kill the pigs.
*  Like this is the problem with these.
*  They have a very hard time to run the line at actual violence.
*  And I think, I think in this case, there's, I mean, they're already calling for it
*  like today and you know, where this goes from here as they get more worked up,
*  like I think is like really concerning.
*  Okay.
*  But that's kind of the extremes, you know, the extremes of anything I hope
*  that was concerning.
*  It's also possible to kind of believe that AI has a very high likelihood of
*  killing all of us.
*  But there's, and therefore we should maybe consider slowing development or
*  regulating.
*  So not violence or any of these kinds of things, but saying like, all right,
*  let's, let's take a pause here.
*  You know, you have biological weapons, nuclear weapons, like, whoa, this is
*  like serious stuff.
*  We should be careful.
*  So it is possible to kind of have a more rational response, right?
*  If you believe this risk is real.
*  Believe.
*  Yes.
*  So is it possible to be, have a scientific approach to the, the
*  prediction of the future?
*  I mean, we just went through this with COVID.
*  What do we know about modeling?
*  Well, I mean, what do we learn about modeling with COVID?
*  There's a lot of lessons.
*  They didn't work at all.
*  They worked poorly.
*  The models were terrible.
*  The models were useless.
*  I don't know if the models were useless or the people interpreting the models
*  and then the centralized institutions that were creating policy rapidly based
*  on the models and leveraging the models in order to support their narratives
*  versus actually interpreting the airbases.
*  What you had with COVID, my view you had with COVID is you had these experts
*  showing up and they claimed to be scientists and they had no testable
*  hypotheses whatsoever.
*  They had a bunch of models.
*  They had a bunch of forecasts and they had a bunch of theories and they laid
*  these out in front of policymakers and policymakers freaked out and panicked.
*  Right.
*  And implemented a whole bunch of like really like terrible decisions that
*  we're still living with the consequences of.
*  And there was never any empirical foundation to any of the models.
*  None of them ever came true.
*  Yeah.
*  To push, to push back, there were some really bad decisions that were
*  bootleggers in this, in the context of this pandemic, but there's still a
*  usefulness to models, no?
*  I, so not if they're not, I mean, not if they're reliably wrong, right.
*  Then they're actually like anti-useful, right.
*  They're actually damaging.
*  But what do you do with the pandemic?
*  What do you do with a, with a, with any kind of threat, don't you want to kind
*  of have several models to play with as part of the discussion of like, what the
*  hell do we do here?
*  I mean, do they work?
*  Because they're an expectation that they actually like work, that they have
*  actually been doing for a long time.
*  I mean, as far as I can tell with COVID, we just sigh out, the policy makers just
*  sigh out themselves into believing that there was some, I mean, look, the
*  scientists, the scientists were at fault.
*  The quote unquote scientists showed up.
*  So I had some insight into this.
*  So there, there was a, remember the Imperial college models out of London
*  were the ones that were like, these are the gold standard models.
*  So a friend of mine runs a big software company and he was like, wow, this is
*  like COVID is really scary.
*  And he's like, you know, he contacted this research and he's like, you know,
*  do you need some help?
*  You've been just building this model on your own for 20 years.
*  Do you need some help?
*  Do you need some help?
*  Do you need some help?
*  Do you need some help?
*  Do you need some help?
*  And then he said, yeah, I've been building this model on your own for 20 years.
*  Do you need some help?
*  Do you like us or coders to basically restructure it so it can be
*  fully adapted for COVID.
*  And the guy said yes and sent over the code.
*  And my friend said it was like the worst spaghetti code he's ever seen.
*  That doesn't mean it's not possible to construct a good model of pandemic with
*  the correct air bars, with a high number of parameters that are continuously many
*  times a day updated as we get more data about a pandemic.
*  I would like to believe when a pandemic hits the world, the best computer
*  respond aggressively and as input, take the data that we know about the virus.
*  And it's an output.
*  Say here's, here's what's happening in terms of how quickly it's spreading,
*  what that lead in terms of hospitalization and deaths and all that kind of stuff.
*  Here's how likely, how contagious it likely is.
*  Here's how deadly likely is based on different conditions, based on different
*  ages and demographics and all that kind of stuff.
*  So here's the best kinds of policy.
*  It feels like you could have models machine learning that like kind of, they
*  don't perfectly predict the future, but they, they, they help you do something
*  because there's pandemics that are like, uh, meh, they don't really do much harm.
*  And there's pandemics.
*  You can imagine them.
*  They could do a huge amount of harm.
*  Like they can kill a lot of people.
*  So you should probably have some kind of data driven models that keep
*  updating that allow you to make decisions that are based like where,
*  how bad is this thing?
*  Uh, now you can criticize how horrible all of that went with the response to
*  this pandemic, but I just feel like there might be some value to models.
*  So to be useful at some point, it has to be predictive, right?
*  So, and so, and, and so the easy thing for me to do is to say, obviously, you're
*  right, obviously I want to see that just as much as you do, cause anything that
*  makes it easier to navigate through society, through a wrenching, you know,
*  risk like that is, you know, that sounds great.
*  Um, you know, the, the, the harder objection to it is just simply, you are
*  trying to model a complex dynamic system with 8 billion moving parts, like not
*  possible, can't be done.
*  Complex systems can't be done.
*  Uh, machine learning says hold my beer, but well, it's possible.
*  No, I don't know.
*  I would like to believe that it is.
*  Yeah.
*  I will put it this way.
*  I think where you and I would agree is I think we would like, we would, we'd
*  like that to be the case.
*  We are strongly in favor of it.
*  I think we would also agree that no such thing with respect to COVID or pandemics,
*  no such thing, at least neither you nor I think are aware.
*  I'm not aware of anything like that today.
*  My main worry with the response to the pandemic is that, uh, uh, same as with
*  aliens is that even if such a thing existed and it's possible it existed,
*  the, the, the, the policy makers were not paying attention.
*  Like, uh, there was no mechanism that allowed those kinds of models to
*  percolate out.
*  Oh, I think we have the opposite problem during COVID.
*  I think the policy makers, I think the, these, these, these, these people with
*  basically fake science had too much access to the policy makers.
*  Well, right.
*  And w but the policy makers also wanted, they had a narrative in mind and they
*  also wanted to use whatever model that fit that narrative to help them out.
*  So like, it felt like there was a lot of politics and not enough science.
*  Although a big part of what was happening, a big, a rig reason we got
*  lockdowns for as long as we did, it was because these scientists came in with
*  these like doomsday scenarios that were like just like completely off the hook.
*  Scientists in quotes.
*  That's not quote unquote scientists.
*  Not, that's give love to science.
*  That is the way out.
*  Science is a process of testing hypotheses.
*  Yeah.
*  Modeling does not involve testable hypotheses, right?
*  Like I don't even know that I actually don't need, I don't, I don't even know
*  that modeling actually qualifies as science.
*  Maybe that's a side conversation.
*  We could have some time over a beer.
*  Ah, it's a really interesting, but what do we do about the future?
*  I mean, what, what?
*  So number one is when we start with number one, humility goes back to this
*  thing of how do we determine the truth?
*  Number two is we don't believe, you know, it's the old, I've got a hammer.
*  Everything looks like a nail, right?
*  Um, uh, I've got, oh, uh, this is one of the reasons I gave you, I gave Alexa
*  book, um, uh, which the topic of the book is what happens when scientists basically
*  stray off the path of technical knowledge and start to weigh in on politics and
*  societal issues, um, in this case, philosophers, philosophers, but he, he
*  actually talks in this book about like Einstein, he talks about the nuclear age
*  and Einstein talks about the physicists, uh, actually, uh, doing, uh, doing, uh,
*  very similar things at the time.
*  The book is when reason goes on holiday, philosophers and politics by, uh, Nevin.
*  And it's just a story.
*  It's a story.
*  There's, there are other books on this topic, but this is a new one.
*  That's really good.
*  That's just the story of what happens when experts in a certain domain decide
*  to weigh in and become basically social engineers and, uh, and, uh, political, um,
*  you know, basically political advisors.
*  And it's just a story of just unending catastrophe, right?
*  And I think that's what happened with COVID again.
*  Yeah.
*  I found this book, a highly entertaining and eyeopening read filled with amazing
*  anecdotes of irrationality and craziness by famous recent philosophers.
*  After you read this book, you will not look at Einstein the same.
*  Oh boy.
*  Yeah.
*  Don't destroy my heroes.
*  You will not be a hero of yours anymore.
*  Uh, I'm sorry.
*  You probably shouldn't read the book, but here's the thing.
*  The AI, the AI risk people, they don't even have the COVID model.
*  At least not that I'm aware of.
*  No, like there's not even the equivalent of the COVID model.
*  They don't even have this spaghetti code.
*  They've got a theory and a warning and a this and a that.
*  And like, if you ask like, okay, well, here's, here's the, I mean, the ultimate
*  example is, okay, how do we know, right?
*  How do we know that an AI is running away?
*  Like, how do we know that the FOOM takeoff thing is actually happening?
*  And the only answer that any of these guys have given that I've ever seen is,
*  oh, it's when the loss rate, the loss function in the training drops, right?
*  That's when you need to like shut down the data center, right?
*  And it's like, well, that's also what happens when you're
*  successfully training a model.
*  Like, like what, what even is this is not science.
*  This is not, it's not anything.
*  It's not a model.
*  It's not anything.
*  There's nothing to arguing with it.
*  It's like, you know, punching jello.
*  Like there's, what do you even respond to?
*  Uh, so just push back on that.
*  I don't think they have good metrics of, yeah, when the FOOM is happening, but I
*  think it's possible to have that.
*  Like I just, just as you speak now, I mean, it's possible to
*  imagine that could be measures.
*  It's been 20 years.
*  No, for sure.
*  But it's been only weeks since we had a big enough breakthrough in language models.
*  We can start to actually have this.
*  The thing is the AI Doomer stuff didn't have any actual systems to really work with.
*  Now there's real systems you can start to analyze.
*  Like how does this stuff go wrong?
*  And I think you kind of agree that there is a lot of risks that we can analyze.
*  The benefits outweigh the risks in many cases.
*  Well, the risks are not existential.
*  Yes.
*  Well, not in the, not in the FOOM, not in the FOOM paperclip.
*  Not as, let me, okay.
*  There's another sleight of hand that you just alluded to.
*  There's another sleight of hand that happens, which is very, I'm very good
*  at the sleight of hand thing, which is very not scientific.
*  So the book super intelligence, right.
*  Which is like the Nick Bostrom's book, which is like the origin of a lot of
*  this stuff, which was written, you know, whatever 10 years ago or something.
*  So he does this really fascinating thing in the book, which is he basically says,
*  um, uh, there are many possible routes to machine intelligence, um, to artificial
*  intelligence, and he describes all the different routes to artificial intelligence,
*  all the different possible, everything from biological augmentation through to,
*  you know, all these different things.
*  Um, one of the ones that he does not describe is large language models because
*  of course the book was written before they were invented and so they didn't exist.
*  In the book, he just, he describes them all and then he proceeds to treat them
*  all as if they're exactly the same thing.
*  He presents them all as sort of an equivalent risk to be dealt with in an
*  equivalent way to be thought about the same way.
*  And then the risk, the quote unquote risk that's actually emerged is actually a
*  completely different technology than he was even imagining.
*  And yet all of his theories and beliefs are being transplanted by this movement,
*  like straight onto this new technology.
*  And so again, like there's no other area of science or technology where you do
*  that, like when you're dealing with like organic chemistry versus inorganic
*  chemistry, you don't just like say, Oh, with respect to like either one,
*  basically maybe, you know, growing up and eating the world or something, like
*  they're just going to operate the same way.
*  Like you don't.
*  But you can start talking about like as, as we get more and more actual systems
*  that start to get more and more intelligent, you can start to actually have
*  more scientific arguments here.
*  Like, you know, high level, you can talk about the threat of autonomous weapons
*  systems back before we had any automation in the military.
*  And that would be like very fuzzy kind of logic, but the more and more you have
*  drones, they're becoming more and more autonomous.
*  You can start imagining, okay, what does that actually look like?
*  And what's the actual threat of autonomous weapons systems?
*  How does it go wrong?
*  And still it's, it's, it's very vague, but you start to get a sense of like,
*  all right, um, it should probably be a legal or wrong or not allowed to do like
*  mass deployment of fully autonomous drones that are doing aerial strikes on
*  large areas.
*  I think it should be required.
*  Right.
*  So that's, no, no, no, no, I think it's required that only aerial
*  vehicles are automated.
*  Okay.
*  So you want to go the other way?
*  I want to go the other way.
*  So that, okay.
*  I think it's obvious that the machine is going to make a better decision
*  than the human pilot.
*  I think it's obvious that it's in the best interest of both the attacker and
*  the defender and humanity at large.
*  If machines are making more of these decisions and not people, I think people
*  make terrible decisions in times of war.
*  But like there's a, there's ways this can go wrong too, right?
*  Well, it's the worst go terribly wrong now.
*  It just goes back to the whole, this is that whole thing about like the self
*  drive, does the self-driving car need to be perfect versus does it need to be
*  better than the human driver?
*  Does the automated drone need to be perfect or does it need to be better
*  than a human pilot at making decisions under enormous amounts of stress and
*  uncertainty?
*  Yeah.
*  Well, the, on average, the, the worry that AI folks have is the runaway.
*  They're going to come alive.
*  Right.
*  Then again, that's the sleight of hand, right?
*  Not come alive.
*  Well, hold on a second.
*  You become, you lose control.
*  But then they're going to develop goals of their own.
*  They're going to develop a mind of their own.
*  They're going to develop their own.
*  Right.
*  No more, more like a Chernobyl style meltdown, like, uh, just bugs in the code
*  accidentally, you know, force you, the results in the bombing of like large
*  civilian areas to a degree that's not possible, um, in the, in the current,
*  um, military strategies.
*  I don't know.
*  Actually we've been doing a lot of mass bombings of cities for a very long time.
*  And a lot of civilians died.
*  A lot of civilians died.
*  And if you watch the documentary, the fog of war, McNamara spends a big part of it
*  talking about the firebombing of the Japanese cities, burning them straight to
*  the ground, right?
*  The, the devastation in Japan, American military, uh, firebombing the cities in
*  Japan was considerably bigger devastation than the use of nukes.
*  Right.
*  So we've been doing that for a long time.
*  We also did that to Germany, by the way, Germany did that to us, right?
*  Like that's an old tradition.
*  The minute we got airplanes, we started doing indiscriminate bombing.
*  So one of the things that the modern US, uh, military can do with technology
*  with automation, but technology more broadly is a higher and higher precision strikes.
*  Yeah.
*  Well, since our precision is obviously precision and this is the, the, the
*  JDAM, right?
*  So there was this big advance, this big advance, um, called the JDAM, which
*  basically was strapping a GPS transceiver to, uh, to, uh, to an unguided bomb and
*  turning it into a guided guided bomb.
*  And yeah, that's great.
*  Like, look, that's been a big advance, but it, and that's like a baby version of
*  this question, which is, okay, do you want like the human pilot, like guessing
*  where the bomb's going to land?
*  Or do you want like the machine like guiding the bomb to its destination?
*  That's a baby version of the question.
*  So the next version of the question is, do you want the human or the machine
*  deciding whether to drop the bomb?
*  Everybody just assumes the human is going to do a better job for what I think are
*  fundamentally suspicious reasons.
*  Emotional, psychological reasons.
*  I think it's very clear that the machine is going to do a better job making that
*  decision because the humans making that, making that decision are God awful.
*  Just terrible.
*  Yeah.
*  Right.
*  And so, so yeah, so this is the, this is the thing.
*  And then let's get to the, there was, can I, one more sleight of hand?
*  Yes, sure.
*  Please.
*  I'm a magician.
*  You could say.
*  One more sleight of hand.
*  These things are going to be so smart, right?
*  That they're going to be able to destroy the world and wreak havoc and like do all
*  this stuff and plan and do all this stuff and evade us and have all their secret
*  things and their secret factories and all this stuff, but they're so stupid.
*  That they're going to get like tangled up in their code.
*  And that's the thing.
*  They're not going to come alive, but there's going to be some bug that's going
*  to cause them to like turn us all into paper.
*  Like that they're not going to, that they're going to be genius in every way
*  other than the actual bad goal.
*  And it's just like, and that's just like a like ridiculous, like discrepancy.
*  And, and, and, and, and you can prove this today.
*  You can actually address this today for the first time with LLMs, which
*  you can actually ask LLMs to resolve, uh, moral dilemmas.
*  Yeah.
*  So you can create the scenario, you know, dot, dot, dot this, that, this, that,
*  this, that what would you, as the AI do in the circumstance?
*  And they don't just say, destroy all humans, destroy all humans.
*  They will give you actually very nuanced, moral, practical,
*  trade-off oriented answers.
*  And so we actually already have the kind of AI that can actually like think this
*  through and can actually like, you know, reason about goals.
*  Well, the hope is that AGI or like a very super intelligent systems have some of
*  the nuance that LLMs have and the intuition is they most likely will because
*  even these LLMs have the nuance.
*  Uh, LLMs are really, this is actually worth, worth spending a moment on.
*  LLMs are really interesting to have moral conversations with.
*  And that I was just, I didn't expect I'd be having a moral conversation
*  with the machine in my lifetime.
*  Wait.
*  And let's remember we're not really having a conversation with a machine
*  or we're having a conversation with the entirety of the collective intelligence
*  of the human species.
*  Exactly.
*  Yes.
*  Correct.
*  But it's possible to imagine autonomous weapons systems that are not using LLMs.
*  If they're smart enough to be scary, why are they not smart enough to be wise?
*  Like that's the part where it's like, I don't know how you get the
*  one without the other.
*  Is it possible to be super intelligent without being super wise?
*  Well, you're, it's again, you're back to that.
*  I mean, then you're back to a classic autistic computer, right?
*  Like you're back to just like a blind rule follower.
*  I've got this like core is the paperclip thing.
*  I've got this core rule and I'm just going to follow it to the end of the earth.
*  And it's like, well, but everything you're going to be doing to execute that
*  rule is going to be super genius level that humans aren't going to be able to counter.
*  It's just, it's a, it's a, it's a mismatch in the definition of, of what
*  the system is capable of.
*  Unlikely, but not impossible.
*  I think.
*  But again, here you get to like, okay, like,
*  No, I'm not saying when it's unlikely, but not impossible.
*  If it's unlikely, that means the, the fear should be correctly calibrated.
*  Extraordinary claims require extraordinary proof.
*  Well, okay.
*  So a one interesting sort of tangent.
*  I would love to take on this because you mentioned this in the essay about nuclear,
*  which was also, I mean, you don't shy away from a little bit of a, of a spicy take.
*  So, uh, Robert Oppenheimer famously said, now I am become death, the destroyer of
*  worlds, as he witnessed the first detonation of a nuclear weapon on July 16th, 1945.
*  And you write an interesting historical perspective, uh, quote, recall that
*  John von Neumann responded to Robert, uh, Robert Oppenheimer's famous hand wringing
*  about the role of creating nuclear weapons, which you note helped end World War
*  II and prevent World War III with some people confess guilt to claim credit for
*  the sin.
*  And you also mentioned that Truman was harsher after meeting Oppenheimer.
*  He said that, uh, don't let that crybaby in here again.
*  Real quote, real quote by the way, from, from Dean Atchison.
*  Oh boy.
*  Cause that, cause Oppenheimer didn't just say the famous line.
*  Yeah.
*  He then spent years going around basically moaning, you know, going on TV and going
*  into, going into the white house and basically like just like doing this hair
*  shirt, you know, thing self, you know, this sort of self critical, like, oh my
*  God, I can't believe how awful I am.
*  So he's the, the, he's widely considered perhaps because of the hang ringing is
*  the father of the atomic bomb.
*  Uh, this is, this is a Von Neumann criticism of him as he tried to have his
*  cake and eat it too.
*  Like he, he wanted to in, in, in so, and by, by Norman, of course, a very different
*  kind of personality.
*  And he's just like, yeah, this is like an incredibly useful thing.
*  I'm glad we did it.
*  Yeah.
*  Well, Von Neumann is as widely, um, credit as being one of the smartest humans of
*  the 20th century, the certain, certain people, everybody says like, this is the
*  smartest person I've ever met when they've met him.
*  Anyway, uh, that doesn't mean smart.
*  Smart doesn't mean wise.
*  So I would love to sort of, can you make the case both for and against the
*  critique of Oppenheimer here?
*  Cause we're talking about nuclear weapons.
*  Boy, do they seem dangerous.
*  Well, so the critique goes deeper and I left this out.
*  Here's the real substance.
*  I left it out because I didn't want to dwell on nukes in my AI paper, but here's
*  the deeper thing that happened.
*  And I'm really curious this movie coming out this summer.
*  I'm really curious to see how far he pushes this.
*  Cause this is the real drama in the story, which is it wasn't just a
*  question of our nukes good or bad.
*  It was a question of should Russia also have them?
*  Um, and what actually happened, um, was Russia got the American invented the bomb.
*  Russia got the bomb.
*  They got the bomb through espionage.
*  They got an American and you know, they got American scientists and foreign
*  scientists working on the American project.
*  Some combination of the two, uh, basically gave the Russians the designs for the
*  bomb and that's how the Russians got the bomb.
*  Um, there's this dispute to this day of Oppenheimer's role in that.
*  Um, if you read all the histories, the kind of composite picture, and by the way,
*  we now know a lot actually about Soviet espionage in that era, cause there's been
*  all this declassified material in the last 20 years that actually shows a lot
*  of a lot of very interesting things.
*  But if you kind of read all the histories, which you kind of get is Oppenheimer
*  himself probably was not a, he probably did not hand over the nuclear secrets
*  himself, however, he was close to many people who did, including family members.
*  And there were other members of the Manhattan project who were Russian
*  Soviet SS and did hand over the bomb.
*  And so the view that Oppenheimer and people like him had that this thing is
*  awful and terrible and oh my God, and you know, all this stuff you could argue
*  fed into this ethos at the time that resulted in people thinking that the
*  Baptists thinking that the only principle thing to do is to give the Russians the
*  bomb.
*  Um, and so the, the, the moral beliefs on this thing and the public discussion and
*  the role that the inventors of this technology play, this is the point of this
*  book, when they kind of take on this sort of public intellectual moral kind of
*  thing, it can have real consequences, right?
*  Cause we live in a very different world today because Russia got the bomb than
*  we would have lived in had they not gotten the bomb, right?
*  The entire 20th century, second half of the 20th century would have played out
*  very different had those people not given Russia the bomb.
*  And so the stakes were very high then.
*  The good news today is nobody sitting here today.
*  I don't think worrying about like an analogous situation with respect to, like
*  I'm not really worried that Sam Altman is going to decide to give, you know, the
*  Chinese, the design for AI, although he did just speak at a Chinese conference,
*  which is interesting.
*  But however, I don't think, I don't think that's what's at play here, but what's
*  at play here are all these other fundamental issues around what do we believe
*  about this and then what laws and regulations and restrictions that we're
*  going to put on it.
*  And that's where I draw like a direct straight line.
*  And anyway, and my reading of the history on Nukes is like the people who were
*  doing the full hair shirt public.
*  This is awful.
*  This is terrible.
*  Actually had like catastrophically bad results, uh, from, from taking those
*  views.
*  And that's what I'm worried is going to happen again.
*  But is there a case to be made that you really need to wake the public up to the
*  dangers of nuclear weapons when they were first dropped, like really like
*  educate them on like, this is extremely dangerous and destructive weapon.
*  I think the education kind of happened quick and early.
*  Like, it was pretty obvious.
*  How we dropped one bomb and destroyed an entire city.
*  Yeah.
*  So 80,000 people dead.
*  Yeah.
*  Uh, and look, but I don't like the reporting of that.
*  You can report that in all kinds of ways.
*  You can, you can do all kinds of slants, like war is horrible.
*  War is terrible.
*  You can do, you can make it seem like nuclear, the use of nuclear weapons is
*  just a part of war and all that kind of stuff.
*  Something about the reporting and the discussion of nuclear weapons resulted in
*  us being terrified in awe of the power of nuclear weapons and that potentially.
*  Fed in a positive way towards the, the game theory of mutual issue or destruction.
*  Well, so this gets to what actually happens.
*  Get to what actually happens.
*  I'm playing devil's advocate here.
*  Yeah.
*  Yeah, sure.
*  Of course.
*  Let's get to what actually happened and then kind of back into that.
*  So what actually happened, I believe, and again, I think this is a reasonable
*  reading of history is what actually happened was nukes then prevented world
*  war three and they prevented world war three through the game theory of
*  mutually assured destruction had nukes not existed, right?
*  There would have been no reason why the cold war did not go hot.
*  Right.
*  And then there, and then, you know, and the military planners at the time, right.
*  Thought both on both sides thought that there was going to be world war three on
*  the plains of Europe and they thought there was going to be like a hundred
*  million people dead, right?
*  It was like the most obvious thing in the world to happen.
*  Right.
*  And it's the dog that didn't bark, right?
*  Like it may be like the best single net thing that happened in the entire 20th
*  century is that like that didn't happen.
*  Yeah.
*  Actually just on that point, you say a lot of really brilliant things.
*  It hit me just as you were saying it.
*  I don't know why it hit me for the first time, but we've got two wars in a span
*  of like 20 years, like we could have kept getting more and more world wars and
*  more and more ruthless.
*  And actually you could have had a U S versus Russia war.
*  You could have, by the way, you have, there's another hypothetical scenario.
*  The other hypothetical scenario is the Americans got the bomb, the Russians
*  didn't write and then America's the big dog.
*  And then maybe America would have had the capability to actually
*  roll back the air curtain.
*  Right.
*  No, I don't know whether that would have happened, but like it's entirely possible.
*  Right.
*  And, and, and the act of these people who had these moral positions about, cause
*  they could forecast, they could model, they could forecast the future of how
*  the technology would get used, made a horrific mistake because they basically
*  ensured that the iron curtain would continue for 50 years longer than it
*  would have otherwise.
*  And again, like these are counterfactuals.
*  I don't know that that's what, what would have happened, but like the decision to
*  hand the bomb over was a big decision made by people who were very full of themselves.
*  Yeah.
*  But so me as an American, me as a person that loves America, I also wonder if
*  you, us was the only ones with the nuclear weapons.
*  Um,
*  that was the argument for handing the, but that was the, was the guys who
*  handed over the bomb that was actually their moral argument.
*  Yeah.
*  I would, I would probably not hand it over to, I would be careful about the
*  regimes you hand it over to maybe give it to like the British or something.
*  Like, uh, like a democratically elected government.
*  Well, look, there are people to this day who think that those by the Soviet spies
*  did the right thing because they created a balance of terror as opposed to the
*  U S having just, and by the way, let me, let me balance of terror.
*  Let's tell the full version.
*  That's such a sexy ring to it.
*  Okay.
*  So the full version of the story is John von Neumann is a hero of both
*  heroes in mind, the full version of the story is he advocated for a first strike.
*  So when the U S had the bomb and Russia did not, he advocated for, he said, we
*  need to strike them right now.
*  Strike Russia.
*  Yeah.
*  Yes.
*  Von Neumann.
*  Yes.
*  Because he said World War III is inevitable.
*  Um, he was very hardcore.
*  Uh, he, he, his, his theory was, um, his theory was World War III is inevitable.
*  We're definitely going to have a World War III.
*  The only way to stop World War III is we have to take them out right now and we
*  have to take them out right now before they get the bomb, because this is our last chance.
*  Now, again, like, is this an example of philosophers in politics?
*  I don't know if that's in there or not, but this is in the standard.
*  No, but it is meaning.
*  Yeah, this is on the other side.
*  So, so most of the case studies, most of the case studies in books like
*  this are the crazy people on the left.
*  Yeah.
*  Um, the von Neumann is a story arguably of the crazy people on the right.
*  Um, yeah, stick to computing, John.
*  Well, this is the thing.
*  And this is, this is the general principle is getting back to our core thing, which
*  is like, I don't know whether any of these people should be making any of these calls.
*  Yeah.
*  Cause there's nothing in either von Neumann's background or Oppenheimer's
*  background or any of these people's background that qualifies them as moral authorities.
*  Yeah.
*  Well, this actually brings up the point of in AI, who are the good people to, to
*  reason about the morality of the ethics, the outside of these risks outside, like
*  the more complicated stuff that you, you agree on is, you know, this will go into
*  the hands of bad guys and all the kinds of ways they'll do is, is interesting and
*  dangerous, um, is dangerous in interesting, unpredictable ways.
*  And who is the right person?
*  Who are the right kinds of people to make decisions, how to respond to it?
*  Are these tech people?
*  So the history of these fields, this is what he talks about in the book.
*  The history of these fields is that the, the competence and capability and
*  intelligence and training and accomplishments of senior scientists and
*  technologists working on a technology.
*  And then being able to then make moral judgments in the use of the technology
*  that track record is terrible.
*  That tracker, that track record is like catastrophically bad.
*  Um, and the people, just to look at it, the people that develop that technology are
*  usually not going to be the right people.
*  Well, why would they, so the claim is of course, they're the knowledgeable ones,
*  but the problem is they've spent their entire life in a lab, right?
*  They're not theologians, but so what you find, what you find when you read, when
*  you read this, when you look at these histories, what you find is they generally
*  are very thinly informed on history, on sociology, on, on, on, um, theology, on
*  morality, on ethics, they tend to manufacture their own worldviews from
*  scratch.
*  They tend to be very sort of thin.
*  Um, they're not remotely the arguments that you would be having if you got like
*  a group of highly qualified theologians or philosophers or, you know, um,
*  well, let me, uh, sort of, uh, as the devil's advocate takes a sip of whiskey,
*  say that I agree with, uh, with that, but also it seems like the people who are
*  doing kind of the ethics departments in these texts, tech companies go sometimes
*  the other way.
*  Yes.
*  Uh, they're not nuanced on the, on history or theology or this kind of stuff.
*  They, it almost becomes a kind of, uh, outraged activism towards, um, directions
*  that don't seem to be grounded in history and, uh, humility and nuances again,
*  drenched with arrogance.
*  So, so definitely not sure which is worse.
*  Oh no, they're both bad.
*  They're there.
*  Yeah.
*  So definitely not them either.
*  Um, but I guess,
*  but look, this is a hard, yeah, it's a hard problem.
*  This is our problem.
*  And this goes back to where we started, which is okay.
*  Who has the truth?
*  And it's like, well, um, you know, like how does societies arrive at like truth
*  and how do we figure these things out?
*  And like our elected leaders play some role in it.
*  You know, we all play some role in it.
*  Um, there have to be some set of public intellectuals at some point that bring,
*  you know, rationality and judgment and humility to it.
*  Yeah.
*  Those people are few and far between.
*  We should probably prize them very highly.
*  Yeah.
*  Celebrate humility in our public leaders.
*  Uh, so getting to risk number two, will AI ruin our society?
*  Short version, as you write, if the murder robots don't get us the hate
*  speech and misinformation will.
*  And the action you recommend in short, don't let the thought police suppress AI.
*  Well, what is, uh, this risk of the effect of misinformation as society
*  that's going to be catalyzed by AI?
*  Yeah.
*  So this is the social media.
*  This is what you just alluded to as the activism kind of thing that's popped
*  up in these companies and in the industry.
*  And it's basically, from my perspective, it's basically part two of the war that
*  played out over social media over the last 10 years, um, cause you probably
*  remember social media 10 years ago was basically who even wants this, who wants
*  to, who wants a photo of what your cat had for breakfast, like this stuff is
*  like silly and trivial and why can't these nerds like figure out how to invent
*  something like useful and powerful.
*  And then, you know, certain things happened in the political system.
*  And then it sort of the polarity on that discussion switched all the way to social
*  media is like the worst, most corrosive, most terrible, most awful technology
*  ever invented, and then it leads to, you know, terrible, the wrong politicians
*  and policies and politics and like, and all this stuff and that, that all got
*  catalyzed into this very big kind of angry movement, both inside and outside
*  the companies to kind of bring social media to, to, to heal.
*  And that got focused in particularly on two topics, so-called hate speech
*  and so-called misinformation.
*  Um, and that's been the saga playing out for the last, for the last decade.
*  And I don't even really want to even argue the pros and cons of the sides, just
*  to observe that that's been like a huge fight and it's had, you know, big
*  consequences to how these companies operate.
*  Um, basically that same, those same sets of theories, that same activist approach,
*  that same energy is being transplanted straight to AI and you see that already
*  happening, it's why, you know, Chad GPT will answer, let's say certain questions
*  and not others, uh, it's why it gives you the canned speech about, you know,
*  whenever it starts with as a large language model, I cannot, you know,
*  basically means that somebody has reached in there and told it, it
*  can't talk about certain topics.
*  Um, do you think some of that is good?
*  So it's, it's an interesting question.
*  Um, so a couple of, a couple of observations.
*  Um, so, so one is, um, the people who find this the most frustrating are the
*  people who are worried about the murder robots.
*  So, and in fact, the, the X so-called X risk people, right.
*  They started with the term AI safety.
*  The term became AI alignment.
*  When the term became AI alignment is when this switch happened from we're
*  worried it's going to kill us all.
*  So we're worried about hate speech and misinformation.
*  The AI X risk people have now renamed their thing, uh, AI not kill everyone
*  ism, which I have to admit is a catchy term and they are very frustrated by the
*  fact that the hate speech is sort of activist driven hate speech misinformation
*  kind of thing is taking over, which is what's happened is taken over.
*  The AI ethics field has been taken over by the hate speech misinformation people.
*  Um, you know, look, would I like to live in a world in which like everybody was
*  nice to each other all the time and nobody ever said anything mean and nobody
*  ever used a bad word and everything was always accurate and honest.
*  Like that sounds great.
*  Do I want to live in a world where there's like a centralized thought police
*  working through the tech companies to enforce the view of a small set of elites
*  that they're going to determine what the rest of us think and feel like?
*  Absolutely not.
*  That could be a middle ground somewhere like Wikipedia type of moderation.
*  There's moderation on Wikipedia that is somehow crowdsourced where you're
*  don't have centralized elites.
*  Uh, but it's also not completely just a free for all because, uh, the, if you
*  have the entirety of human knowledge at your fingertips, you can do a lot of
*  harm, like if you have a good assistant that's completely uncensored, they can
*  help you build the bomb.
*  They can help you, um, mess with people's physical wellbeing, right?
*  If they, because that information is out there on the internet.
*  And so they're presumably there's, it would be, you could see the positives
*  in, um, censoring some aspects of an AI model when it's helping you commit
*  literal violence.
*  And there's a section, later section of the essay where I talk about bad
*  people doing bad things, right?
*  Which, which, and there's, there's a set of things that we should discuss there.
*  Yeah.
*  Um, what happens in practice is these lines, as you alluded to this already,
*  these lines are not easy to draw.
*  And what, what I've observed in the social media version of this is, like
*  the way I describe it as the slippery slope is not a fallacy, it's an inevitability.
*  The minute you have this kind of activist personality that gets in a position to
*  make these decisions, they take it straight to infinity.
*  Like they, it goes into the crazy zone, like almost immediately and never comes
*  back because people become drunk with power.
*  Um, right.
*  And they look, if you're in the position to determine what the entire world
*  thinks and feels and reads and says, like you're going to take it.
*  And, you know, Elon has ventilated this with the Twitter files over the last,
*  you know, three months and it's just like crystal clear, like how bad it got there.
*  Now, reason for optimism is what, uh, Elon is doing with the community notes.
*  Um, um, so community notes is actually a very interesting thing.
*  Uh, so what, what Elon is trying to do with community notes, um, is he's trying
*  to have it where there's only a community note when people who have previously
*  disagreed on many topics agree on this one.
*  Yes.
*  That's interesting.
*  That's what, that's what I'm trying to get at is like, there's, there could be
*  Wikipedia like models or community notes type of models where allows you to
*  essentially either provide context or sensor in a way that's not resist
*  the slippery slope nature.
*  No, there's another power.
*  There's an entirely different approach here, which is basically, um, we have
*  AIs that are producing content.
*  We could also have AIs that are consuming content.
*  Right.
*  And so one of the things that your assistant could do for you is help you
*  consume all the content, right.
*  And basically tell you when you're getting played.
*  So for example, I'm going to want the AI that my kid uses, right.
*  To be very child safe.
*  And I'm going to want it to filter for him, all kinds of inappropriate stuff
*  that he shouldn't be saying just cause he's a kid.
*  Yeah.
*  Right.
*  And you see what I'm saying is you can implement that.
*  You could use it architecturally.
*  You could say, you can solve this on the client side, right.
*  You solving on the server side gives you an opportunity to dictate for the
*  entire world, which I think is where you take the slippery slope to hell.
*  Um, there's another architectural approach, which is to solve this on the
*  client side, which is certainly what I would endorse.
*  It's a, at risk.
*  Number five will AI lead to bad people doing bad things?
*  I can just imagine language models used to do so many bad things, but the hope
*  is there that you can have a large language models used to then defend against
*  it by more people, by smarter people, by, um, more effective people, skilled
*  people, all that kind of stuff.
*  Three, three, three-part argument on bad people doing bad things.
*  Um, so, um, uh, so number one, right.
*  You can use the technology defensively and there's a, we should be using AI to
*  build like broad spectrum vaccines and antibiotics for like bioweapons and we
*  should be using AI to like hunt terrorists and catch criminals and like,
*  we should be doing like all kinds of stuff like that.
*  And in fact, we should be doing those things even just to like, go get like,
*  you know, basically go eliminate risk from like regular pathogens that
*  aren't like constructed by an AI.
*  So there's, there's, there's the whole, um, uh, there's a whole defensive set of
*  things.
*  Um, second is we have many laws on the books about the actual bad things.
*  Right.
*  So it is actually illegal to be a criminal, you know, to commit crimes, to
*  commit terrorist acts, to, you know, build pathogens with the intent to deploy
*  them, to kill people.
*  And so we have those, we don't, we actually don't need new laws for the
*  vast majority of the scenarios.
*  We actually already have the laws in the book on the books.
*  The third argument is the minute, and this is sort of the foundational one
*  that gets really tough, but the minute you get into this thing, which, which
*  you were kind of getting into, which was like, okay, but like, don't you need
*  censorship sometimes, right.
*  And don't you need restrictions sometimes it's like, okay, what is the cost of
*  that?
*  Um, and in particular in the world of open source, right.
*  Um, and so, um, is open source AI going to be allowed or not?
*  Um, if open source AI is not allowed, um, then what is the regime that's going to
*  be necessary legally and technically to prevent it from developing, right.
*  And here again, is where you get into, and people have proposed that these kinds
*  of things you get into, I would say pretty extreme territory pretty fast.
*  Do we have a monitor agent on every CPU and GPU that reports back to the
*  government, what we're doing with our computers?
*  Are we seizing GPU clusters to get beyond a certain size?
*  Like, and then by the way, how are we doing all that globally?
*  Right.
*  And like, if China is developing an LLM beyond the scale that we think is
*  allowable, are we going to invade?
*  Right.
*  And you have figures on the AIX risk side who are advocating, you know, potentially
*  up to nuclear strikes to prevent, you know, this kind of thing.
*  And so here you get into this thing.
*  And again, you know, you could maybe say this is, you know, you could even say this
*  is what good, bad or indifferent or whatever, but like, here's the, the
*  comparison of the nukes.
*  The comparison of nukes is very dangerous because one is just nukes
*  were just, just a, although we can come back to nuclear power.
*  But the other thing was like with nukes, you could control plutonium, right?
*  You could track plutonium and it was like hard to come by.
*  AI is just math and code, right?
*  It's, and it's in like math textbooks and it's like there are YouTube videos
*  that teach you how to build it.
*  And like there's open source, it's already open source, you know, it's a
*  40 billion parameter model running around already called Falcon online
*  that anybody can download.
*  Um, and so, okay.
*  You, you walk down the logic path that says we need to have guardrails on this.
*  And you find yourself in a authoritarian totalitarian regime of thought control
*  and machine control that would be so brutal that you would have destroyed the
*  society that you're trying to protect.
*  And so I just don't see how that actually works.
*  So you have to understand my brain is going a full, a full steam ahead of here.
*  Cause I agree with, uh, basically everything you're saying, but I'm trying
*  to play devil's advocate here because, okay, you highlighted the fact that there
*  is a slippery slope to human nature.
*  The moment you sense there's something you start to sensor everything.
*  Uh, the alignment starts out sounding nice, but then you start to align to,
*  uh, the beliefs of some select group of people.
*  And then it's just to your beliefs.
*  This is the number, the number of people you're aligning to smaller and smaller
*  as that group becomes more and more powerful.
*  Okay.
*  But that just speaks to the people that sensor are usually the assholes
*  and, uh, the assholes get richer.
*  I wonder if it's possible to do without that for AI.
*  The one way to ask this question is, do you think the base models, the, the
*  base, the baseline foundation model should be open sourced like a word?
*  The Mark Zuckerberg is saying they want to do.
*  So I look, I mean, I think it's totally appropriate.
*  The companies that are in the business of producing a product or service
*  should be able to have a wide range of policies that they put.
*  Right.
*  And I'll just, again, I want a heavily censored model for my eight year old.
*  Like I actually want that.
*  Like I, like I would pay more money for the ones more heavily
*  censored than the one that's not.
*  Right.
*  Um, and so like there are certainly scenarios where companies
*  will make that decision.
*  Look, an interesting thing you brought up, or is this really a speech issue?
*  Um, one of the things that the big tech companies are dealing with is that
*  content generated, uh, from an LLM is not covered under section two 30, uh, which
*  is the law that protects internet platform companies from being sued
*  for the user generated content.
*  Um, and so it's actually, yes.
*  And so there's actually, there's actually a question.
*  I think there's still a question, which is can big, can big American companies
*  actually feel generative AI at all?
*  Or is the liability actually going to just ultimately convince them that
*  they can't do it because the minute the thing says something bad and it doesn't
*  even need to be hate speech, it could just be like an inaccurate, it could
*  hallucinate a product, you know, detail on a vacuum cleaner, you know, and all
*  of a sudden the vacuum cleaner company sues for misrepresentation and there's
*  any symmetry there, right?
*  Cause the, the, the LLM is going to be producing billions of answers to
*  questions and it only needs to get a few wrong to have.
*  So loss has to get updated really quick here.
*  Yeah.
*  And nobody knows what to do with that.
*  Right.
*  Um, so, so anyway, like there, there, there are big, there are big questions
*  around how companies operate at all.
*  So we can, we can talk about those, but then there's this other question of like,
*  okay, the open source.
*  So what about open source?
*  And my answer to your question is kind of like, obviously yes, the models have,
*  there has to be full open source here because to live in a world in which
*  that open source is not allowed is a world of draconian speech control,
*  human control, machine control.
*  I mean, you know, black helicopters with jackbooted thugs coming out,
*  repelling down and seizing your GPU like territory.
*  Well, no, no, I'm a hundred percent serious.
*  That's you're saying slippery slope always leaves that.
*  No, no, no, no, no, no.
*  That's what's required to enforce it.
*  Like how will you enforce a ban on open source?
*  No, you could add friction to it.
*  Like harder to get the models because people will always be able to get the models,
*  but it'll be more in the shadows.
*  Right.
*  The leading open source model right now is from the UAE.
*  Like the next time they do that, what do we do?
*  Yeah.
*  Like, Oh, I see.
*  You're like, uh, the 14 year old in Indonesia comes out with a breakthrough model.
*  You know, we talked about most great software comes from a small number of people.
*  Some kid comes out with some big new breakthrough and quantization or something,
*  and he has some huge breakthrough and like, what are we going to, what are we
*  going to like invade Indonesia and arrest him?
*  It seems like in terms of size and models and effectiveness and models, the big
*  tech companies will probably lead the way for quite a few years.
*  And the question is of what policies they should use.
*  The, the kid, the kid in Indonesia should not be regulated, but should Google meta,
*  uh, Microsoft open AI be regulated?
*  Well, so, but this goes, okay.
*  So when does it become dangerous?
*  Yeah.
*  Right.
*  Is, is the danger that it's quote as powerful as the current leading commercial
*  model or is it that it is, it is just at some other arbitrary threshold.
*  And then by the way, like, look, how do we know?
*  Like what we know today is that you need like a lot of money to like train these
*  things, but there are advances being made every week on training efficiency and
*  you know, data, all kinds of synthetic, you know, look, I don't even like the
*  synthetic data thing we're talking about.
*  Maybe some kid figure out a way to auto-generate synthetic.
*  Yeah, exactly.
*  And so like sitting here today, like the breakthrough just happened, right?
*  You made this point, like the breakthrough just happened.
*  So we don't know what the shape of this technology is going to be.
*  I mean, the, the big shock, the big shock here is that, you know, whatever number
*  of billions of parameters basically represents at least a very big percentage
*  of human thought, like who would have imagined that, and then there's
*  already work underway.
*  There was just this paper that just came out that basically takes a GPT three
*  scale model and compresses it down to run on a single 32 core CPU.
*  Like who would have predicted that?
*  Yeah.
*  Um, you know, some of these models now you can run a Raspberry Pi's like today.
*  They're very slow, but like, you know, maybe there'll be a, you know, perceived
*  you have real perform, you know, like it's math and here we're back at the
*  here, we're back at the math and code.
*  It's math and code.
*  It's math, code and data.
*  It's bits.
*  Mark's just like walked away at this point.
*  He's just screw it.
*  I don't know what to do with this.
*  You guys created this whole internet thing.
*  Yeah.
*  Yeah.
*  I mean, I, I'm a huge believer in open source here.
*  So my argument is we're going to have to see, here's my argument is a, my argument,
*  my full argument is AI is going to be like air.
*  It's going to be everywhere.
*  Like this is just going to be in text.
*  It already is.
*  It's going to be in textbooks and kids are going to grow up knowing how to do this.
*  And it's just going to be a thing.
*  It's going to be in the air and you can't like pull this back anymore.
*  You can pull back air.
*  And so you just have to figure out how to live in this world.
*  Right.
*  And then that, and then that's where I think like all this hand wringing, but AI
*  risk is basically a complete waste of time.
*  Cause the, the, the, the effort should go into, okay, what are, what are, what is
*  the defensive approach?
*  And so if you're worried about AI, you know, AI generated pathogens, the right
*  thing to do is to have a permanent project warp speed, right.
*  Funded lavishly.
*  Let's, let's do it.
*  Manhattan.
*  Let's talk about Manhattan.
*  Let's do a Manhattan project for biological defense.
*  Right.
*  And let's build AI's and let's have like broad spectrum vaccines where like we're
*  insulated from every pathogen.
*  Right.
*  And what, what the interesting thing is because of software, a kid in his basement
*  teenager could build like a system that defends against like the worst, the worst.
*  I mean, and to me, defense is super exciting.
*  It's like, if you believe in the good of human nature, the, the most people want
*  to do good to be the savior of humanity is really exciting.
*  Yes.
*  Not okay.
*  That's a dramatic statement, but like to help people, to help people.
*  Yeah.
*  Okay.
*  What about just the jump around?
*  What about the risk of will AI lead to crippling inequality?
*  You know, cause we're kind of saying everybody's life will become better.
*  Is it possible that the rich get richer here?
*  Yeah.
*  So this goes, this actually ironically goes back to Marxism.
*  So, um, cause this was the cause of the core claim of Marxism, right?
*  Basically was that the owner, the owners of capital would basically own the means
*  of production and then over time they would basically accumulate all the wealth.
*  The workers would be paying in, you know, and getting nothing in return
*  because they wouldn't be needed anymore.
*  Right.
*  He Marxist very worried about what he called mechanization or what
*  later became known as automation.
*  Um, and that, you know, the workers would be immiserated and the
*  capitalists would end up with, with, with all.
*  And so this was one of the core, core, core principles of Marxism.
*  Of course, it turned out to be wrong about every previous wave of technology.
*  Um, the reason it turned out to be wrong about every previous wave of technology is
*  that the way that the self-interested owner of the machines makes the most money is by
*  providing the production capability in the form of products and services to the most
*  people, the most customers as possible.
*  Right.
*  The largest, this is one of those funny things where every CEO knows this intuitively.
*  And yet it's like hard to explain from the outside.
*  The way you make the most money in any business is by selling to the largest
*  market you can possibly get to.
*  The largest market you can possibly get to is everybody on the planet.
*  And so every large company does is everything that it can to drive down prices, to be able
*  to get volumes up to be able to get to everybody on the planet.
*  And that happened with everything from electricity.
*  It happened with telephones.
*  It happened with radio.
*  It happened with automobiles.
*  It happened with smartphones.
*  It happened with PCs.
*  It happened with the internet.
*  It happened with mobile broadband.
*  It's happened by the way, with Coca-Cola.
*  And it's happened with like every, you know, basically every industrially produced, you
*  know, good or service people, you want to drive it to the largest possible market.
*  And then as proof of that, it's already happened, right.
*  Which is the early adopters of like Chad GPT and Bing are not like, you know, Exxon and
*  Boeing. They're, you know, your uncle and your nephew.
*  Right. It's just like it's either freely available online or it's available for 20
*  bucks a month or something. But, you know, these things went this this this technology
*  went mass market immediately.
*  And so, look, the owners of the means of production, whoever does this, as you mentioned,
*  the Australian dollar questions, there are people who are going to get really rich doing
*  this, producing these things, but they're going to get really rich by taking this
*  technology to the broadest possible market.
*  So, yes, they'll get rich, but they'll get rich having a huge positive impact on
*  making that making the technology available to everybody.
*  Yeah. Right. And again, smartphone, same thing.
*  So there's this amazing kind of twist in in business history, which is you cannot
*  spend ten thousand dollars on a smartphone.
*  Right. You can't spend one hundred thousand dollars.
*  You can't spend like I would buy the million dollar smartphone like I'm signed up
*  for it. Like if it's like suppose a million dollar smartphone was like much better
*  than the thousand dollar smartphone, like I'm there to buy it.
*  It doesn't exist. Why doesn't it exist?
*  Apple makes so much more money driving the price further down from a thousand
*  dollars than they would trying to harvest.
*  Right. And so it's just this repeating pattern you see over and over again.
*  And what's great about it, what's great about it is you do not need to rely on
*  anybody's enlightened generosity to do this.
*  You just need to rely on capitalist self-interest.
*  What about AI taking our jobs?
*  Yeah. So very, very similar thing here.
*  There's sort of a there's a core fallacy, which, again, was very common in
*  Marxism, which is what's called the lump of labor fallacy.
*  And this is sort of the fallacy that there is only a fixed amount of work to be
*  done in the world. And if it's all being done today by people and then if machines
*  do it, there's no other work to be done by people.
*  And that's just a completely backwards view on how the economy develops and grows.
*  Because what happens is not, in fact, that what happens is the introduction of
*  technology into production process causes prices to fall.
*  As prices fall, consumers have more spending power.
*  As consumers have more spending power, they create new demand.
*  That new demand then causes capital and labor to form into new enterprises to
*  satisfy new wants and needs.
*  And the result is more jobs at higher wages.
*  So new wants and needs.
*  The worry is that the creation of new wants and needs at a rapid rate will mean
*  there's a lot of turnover in jobs.
*  So people will lose jobs.
*  Just the actual experience of losing a job and having to learn new things and
*  new skills is painful for the individual.
*  Well, two things. One is that new jobs are often much better.
*  So this actually came up, there was this panic about a decade ago and all the
*  truck drivers are going to lose their jobs, right?
*  And number one, that didn't happen because we haven't figured out a way to
*  actually finish that yet.
*  But the other thing was like, truck driver, like I grew up in a town that was
*  basically consisted of a truck stop, right?
*  And I like knew a lot of truck drivers and like truck drivers live a decade
*  shorter than everybody else.
*  Like they, it's actually like a very dangerous, like they get, like literally
*  they have like high rates of skin cancer and on the left side of their body from
*  being in the sun all the time, the vibration of being in the truck is
*  actually very damaging to your, to your physiology.
*  And there's actually a, perhaps partially because of that reason, there's a
*  shortage of people who want to be truck drivers.
*  Yeah.
*  Like it's not, it's not like the question always, you want to ask somebody like
*  that is, do you want, you know, do you want your kid to be doing this job?
*  And like most of them will tell you, no, like I want my kid to be sitting in a
*  cubicle somewhere, like where they don't have this, like where they don't die 10
*  years earlier.
*  And so, so the new jobs, number one, the new jobs are often better, but you
*  don't get the new jobs until you go through the change.
*  And then to your point, the training thing, you know, it's always the issue is
*  can people adapt?
*  And again, here you need to imagine living in a world in which everybody has
*  the AI assistant capability, right?
*  To be able to pick up new skills much more quickly and be able to have some, you
*  know, be able to have a machine to work with to augment their skills.
*  It's still going to be painful, but that's the process of life.
*  It's painful for some people.
*  I mean, there's no, like there's no question it's painful for some people
*  and they're, you know, they're, yes, it's not, again, I'm not a utopian on this
*  and it's not like it's positive for everybody in the moment, but it has been
*  overwhelmingly positive for 300 years.
*  I mean, look, the concern here, the concern, the concern, this concern has played out
*  for literally centuries.
*  And, you know, this is the sort of Luddite, you know, the story of the Luddites
*  that you may remember, there was a panic in the 2000s around outsourcing was going
*  to take all the jobs.
*  There was a panic in the 2010s that robots were going to take all the jobs.
*  In 2019, before COVID, we had more jobs at higher wages, both in the country and
*  in the world than at any point in human history.
*  And so the overwhelming evidence is that the net gain here is like, just like
*  wildly positive.
*  And most, most people like overwhelmingly come out the other side being huge
*  beneficiaries of this.
*  So you write that the single greatest risk, this is the risk you're most convinced
*  by the single greatest risk of AI is that China wins global AI dominance and we,
*  the United States and the West do not.
*  Can you elaborate?
*  Yeah.
*  So this is the other thing, which is a lot of the sort of AI risk debates today,
*  sort of assume that we're the only game in town.
*  Right.
*  And so we have the ability to kind of sit in the United States and criticize
*  ourselves and do, you know, have our government like, you know, beat up on our
*  companies and we're figuring out a way to restrict what our companies can do.
*  And, you know, we're going to, you know, we're going to ban this and ban that,
*  restrict this and do that.
*  And then there's this like other like force out there that like doesn't believe
*  we have any power over them whatsoever.
*  And they have no desire to sign up for whatever rules we decide to put in place.
*  And they're going to do whatever it is they're going to do.
*  And we have no control over it at all.
*  And it's China and specifically the Chinese communist party.
*  And they have a completely publicized open, you know, plan for what they're
*  going to do with AI and it is not what we have in mind.
*  And not only do they have that as a vision and a plan for their society, but
*  they also have it as a vision and plan for the rest of the world.
*  So their plan is what surveillance?
*  Yeah.
*  Authoritarian control.
*  So authoritarian population control, you know, good old fashioned communist
*  authoritarian control and surveillance and enforcement and social credit
*  scores and all the rest of it.
*  And you are going to be monitored and metered with an inch of everything all
*  the time and it's going to, you know, it's basically the end of human freedom.
*  And that's their goal.
*  And, you know, they justify it on the basis of that's what leads to peace.
*  And you're worried that the regulating in the United States will hold progress
*  enough to where the Chinese government would win that race.
*  So their plan.
*  Yeah.
*  Yes.
*  Yes.
*  And the reason for that is they, and again, they're very public on this.
*  They have their plan is to proliferate their approach around the world.
*  And they have this program called the digital Silk Road, right?
*  Which is building on their Silk Road investment program.
*  And they've got their, they've been laying, they've been laying networking
*  infrastructure all over the world with their 5G work with their company, Huawei.
*  And so they've been laying all this fabric, financial and technological fabric
*  all over the world, and their plan is to roll out their vision of AI on top of
*  that and to have every other country be running their version.
*  And then if you're a country prone to, you know, authoritarianism, you're going
*  to find this to be an incredible way to become more authoritarian.
*  If you're a country, by the way, not prone to authoritarianism, you're going to
*  have the Chinese communist party running your infrastructure and having back
*  doors into it, right?
*  Which is also not good.
*  What's your sense of where they stand in terms of the race towards super
*  intelligence as compared to the United States?
*  Yeah.
*  So good news is they're behind, but bad news is they, you know, they, let's
*  just say they get access to everything we do.
*  So they're probably a year behind at each point in time, but they get, you know,
*  downloads, I think of basically all of our work on a regular basis through a
*  variety of means.
*  And they are, you know, at least we'll see there at least putting out reports of
*  very, they've just put out a report last week of a, of a GPT 3.5 analog.
*  They put out this report, forget what it's called, but they put out this
*  report of this LLM they did.
*  And they, you know, the way when OpenAI puts out, they, they, they, one of the
*  ways they test, you know, GPT is they, they, they run it through standardized
*  exams like the SAT, right?
*  Just how you can kind of gauge how smart it is.
*  And so the Chinese report, they ran their LLM through the Chinese equivalent of
*  the SAT, and it includes a section on Marxism and a section on, I was to say,
*  tongue of thought.
*  And it turns out their AI does very well on both of those topics.
*  Right.
*  So like, this, this alignment thing, communist AI, right?
*  Like literal communist AI, right?
*  And so their vision is like, that's the, you know, so, you know, you can just
*  imagine like you're a school, you know, you're a kid 10 years from now in
*  Argentina or in Germany or in who knows where Indonesia.
*  And you ask the AI, I'd explain to you like how the economy works.
*  And it gives you the most cheery upbeat explanation of Chinese style
*  communism you've ever heard.
*  Right.
*  So like the stakes here are like really big.
*  Well, my, as we've been talking about, my hope is not just for the United States,
*  but it would just the kitten as basement with open source LLM.
*  Cause I, I don't know if I, um, trust large centralized institutions with
*  super powerful AI, no matter what their ideology is, uh, power corrupts.
*  You've been investing in tech companies for about, let's say 20 years and, uh,
*  about 15 of which was, uh, with Andreessen Horowitz.
*  Uh, what interesting trends in tech have you seen over that time?
*  Let's just talk about companies and just the evolution of the tech industry.
*  I mean, the big shift over 20 years has been that tech used to be a tools
*  industry, uh, for basically from like 1940 through to about 2010, almost all
*  the big successful companies were picks and shovels companies.
*  So PC database, smartphone, you know, some, some, some tool that
*  somebody else would pick up and use.
*  Since 2010, most of the big wins have been in applications.
*  Um, so a company that starts a, uh, you know, it starts in an existing industry
*  and goes directly to the customer in that industry and the, you know, the early
*  side examples there were like Uber and Lyft and Airbnb.
*  Um, and then that model is kind of elaborating out.
*  Um, uh, the AI thing is actually a reversion on that for now.
*  Cause like most of the AI business right now is actually in cloud provision
*  of, of, of AI APIs for other people to build on, but, but the big thing
*  will probably be an app.
*  Yeah.
*  I think, I think most of the money I think probably will be in whatever.
*  Yeah.
*  Your AI financial advisor or your AI doctor or your AI lawyer or, you know,
*  take your pick of whatever the domain is.
*  Um, and there, and what's interesting is, you know, we, the Valley kind of
*  does everything we, we are entrepreneurs kind of elaborate every possible idea.
*  And so there will be a set of companies that like make AI, um, something that
*  can be purchased and used by large law firms.
*  Um, and then there will be other companies that just go direct
*  to market as a, as an AI lawyer.
*  What advice could you give for startup founder?
*  Just haven't seen so many successful companies, so many companies that fail
*  also, what advice could you give to a startup founder, someone who wants to
*  build the next super successful startup in the tech space, the Google's, the
*  Apple's, the Twitter's.
*  Yeah.
*  So the great thing about the really great founders is they don't take
*  any advice.
*  So if you find yourself listening to advice, maybe you shouldn't do it.
*  Um,
*  that's actually just to elaborate on that.
*  It, if you could also speak to great founders, like what, what
*  makes a great founder.
*  So it makes a great founder is super smart, um, coupled with super
*  energetic coupled with super courageous.
*  I think it's some of those, those three and I
*  intelligence, passion and courage.
*  The first two are traits.
*  And the third one is a choice.
*  I think courage is a choice.
*  Well, cause courage is a question of pain tolerance, right?
*  Um, so, um, how, how many times are you willing to get punched
*  in the face before you quit?
*  Yeah.
*  Um, and here's maybe the biggest thing people don't understand about what
*  it's like to be a startup founder is it gets, it gets very romanticized, right?
*  Um, and even when, even when they fail, it still gets romanticized about
*  like what a great adventure it was.
*  But like the reality of it is most of what happens is people telling you no.
*  And then they usually follow that with your stupid, right?
*  No, I will not come to work for you.
*  Um, and I will not leave my cushy job at Google to come work for you.
*  No, I'm not going to buy your product.
*  You know, no, I'm not going to run a story about your company.
*  No, I'm not this, that the other thing.
*  Um, and so a huge amount of what people have to do is just get used to just getting
*  punched and, and, and the reason people don't understand this is because when
*  you're a founder, you cannot let on that this is happening because it will cause
*  people to think that you're weak and they'll lose faith in you.
*  So you have to pretend that you're having a great time when you're dying inside.
*  Right.
*  Just in your misery.
*  But why, why did they do it?
*  What did they do?
*  Uh, yeah, that's the thing.
*  It's like, it is a level, this is actually one of the conclusions.
*  I think, is it, I think it's actually for most of these people on a risk adjusted
*  basis, it's probably an irrational act.
*  They could probably be more financially successful on average if they just got
*  like a real job and a big company.
*  Um, but there's, you know, some people just have an irrational need to do
*  something new and build something for themselves.
*  And, you know, some people just can't tolerate having bosses.
*  Oh, here's the fun thing is how do you reference check founders?
*  Right.
*  So you call it, you know, normally you reference check your time hiring somebody
*  as you call the bosses and you know, and you find out if they were good employees.
*  And now you're trying to reference check Steve jobs, right.
*  And it's like, Oh God, he was terrible.
*  You know, he was a terrible employee.
*  He never did what we told him to do.
*  So what's a good reference?
*  You want the previous boss to actually say they're there.
*  They never did what you told them to do.
*  That might be a good thing.
*  Well, ideally, ideally what you want is I will go, I would like to go to work for
*  that person.
*  Um, he worked for me here and now I'd like to work for him now.
*  Unfortunately, most people can't, their egos can't, can't handle that.
*  So they won't say that, but that's the ideal.
*  What advice would you give?
*  To those folks in the space of intelligence, passion and courage.
*  So I think the other big thing is you see people sometimes who say, I want to
*  start a company and then they kind of work through the process of coming up
*  with an idea.
*  And generally those don't work as well as the case where somebody has the idea
*  first, and then they kind of realized that there's an opportunity to build a
*  company and then they just turn out to be the right kind of person to do that.
*  When we say idea, do you mean what longterm big vision or do you mean
*  specifics of like product?
*  Specific, I would say specific, like specifically what, yes, specifics.
*  Like what is that?
*  Cause for the first five years, you don't get to have vision.
*  You just got to build something people want and you got to figure out a way to
*  sell it to them, right?
*  It's very practical or you never get to big vision.
*  So the first, the first product, you have an idea of a set of products or the
*  first product that can actually make some money.
*  Yeah.
*  Like it's got to work.
*  First product's got to work by which I mean, like it has to technically work,
*  but then it has to actually fit into the category in the customer's mind of
*  something that they want.
*  And then, and then by the way, the other part is they have to want to pay for it.
*  Like somebody's got to pay the bills.
*  And so you've got to figure out how to price it and whether you can
*  actually extract the money.
*  So usually it is much more predictable.
*  Success is never predictable, but it's more predictable if you start with a
*  great idea and then back into starting the company.
*  Um, so this is what we did.
*  You know, we had mosaic before we had Netscape, the Google guys had the Google
*  search engine working at Stanford.
*  Right.
*  Um, the, um, uh, you know, yeah, actually there's tons of examples where they,
*  you know, uh, Pierre Omidyar had eBay working before he left his previous job.
*  So I really love that idea of just having a thing, the prototype that actually works
*  before you even begin to remotely scale.
*  By the way, it's also far easier to raise money, right?
*  Like the ideal pitch that we receive is here's the thing that works.
*  Would you like to invest in our company or not?
*  Like that's so much easier than here's 30 slides with a dream.
*  Right.
*  Um, and then we have this concept called the idea maze, which are biology,
*  Serena Boston came up with, um, when he was with us.
*  Um, so, so, so then there's this thing, this goes to mythology, which is, um,
*  you know, there's a mythology that kind of, you know, these, these ideas, um,
*  you know, kind of arrive like magic or people kind of stumble into them.
*  It's like eBay with the pest dispensers or something.
*  Um, the reality usually with the big successes is that the founder has been
*  chewing on the problem for five or 10 years before they start the company.
*  And they often worked on it in school, um, or they even experimented on it when
*  they were a kid, um, and they've been kind of training up over that period of time
*  to be able to do the thing.
*  So they're like a true domain expert.
*  And it sort of sounds like mom and apple pie, which is, yeah, you want to be a
*  domain expert in what you're doing, but you would, you know, the mythology is so
*  strong of like, Oh, I just like had this idea in the shower and now I'm doing it.
*  Like it's generally not that.
*  No, because that may, maybe in the shower, we had the exact product implementation
*  details, but yeah, usually you're going to be for like years, if not decades,
*  thinking about like everything around that.
*  Well, we call it the idea maze because the idea maze basically is like, there's
*  all these permutations, like for any, for any idea, there's like all these
*  different permutations, who should the customer be, what shape forms the product
*  have and how should we take it to market and all these things.
*  Um, and so, um, the really smart founders have thought through all these scenarios
*  by the time they go out to raise money.
*  Um, and they have like detailed answers, um, on every one of those fronts because
*  they put so much thought into it.
*  Um, the sort of, the, the, the sort of more haphazard founders haven't thought
*  about any of that and it's the detailed ones who tend to do much better.
*  So how do you know when to take a leap?
*  If you have a cushy job or happy life.
*  I mean, the best reason is just cause you can't tolerate not doing it.
*  Right.
*  Like this is the kind of thing where if you have to be advised into doing it,
*  you probably shouldn't do it.
*  Um, and so it's probably the opposite, which is you just have such a burning
*  sense of this has to be done.
*  I have to do this.
*  I have no choice.
*  What if it's going to lead to a lot of pain?
*  It's going to lead to a lot of pain.
*  I think that's what if it means losing sort of social relationships
*  and damaging your, um, relationship with loved ones and all that kind of stuff.
*  Yeah.
*  Look, so like it's going to put you in a social tunnel for sure.
*  Right.
*  So you're going to like, you know, there's this game you can play on Twitter,
*  which is you can do any whiff of the idea that there's, uh, basically any such
*  thing as work life balance and that people should actually work hard and
*  everybody gets mad, but like the truth is like all the successful founders are
*  working 80 hour weeks and they're working, you know, they form various, very
*  strong social bonds with the people they work with.
*  They tend to lose a lot of friends in the outside or put those friendships on
*  ice. Like that's just the nature of the, of the thing.
*  Um, you know, for most people that's worth the trade off, you know, the
*  advantage, you know, maybe younger founders have is maybe they have less,
*  you know, maybe they're not, you know, for example, if they're not married yet
*  or don't have kids yet, that's an easier thing to bite off.
*  Can you be an older founder?
*  Yeah, you definitely can.
*  Yeah.
*  Um, yeah, many of the most successful founders are second, third, fourth time
*  founders, they're in their thirties, forties, fifties.
*  Um, the good news of being an older founder is you know more and you, you
*  know a lot more about what to do, which is very helpful.
*  The problem is, okay, now you've got like a spouse and a family and kids
*  and like, you've kind of go to the baseball game and like, you can't go to
*  the base, you know, and so it's, it.
*  Life is full of difficult choices.
*  Yes.
*  I can't reason, uh, you've written a blog post on what you've been up to.
*  Uh, you wrote this in October, 2022, uh, quote, mostly I try to learn a lot.
*  For example, the political events of 2014 to 2016 made clear to me that I didn't
*  understand politics at all referencing maybe some of this, this book here.
*  Uh, so I deliberately withdrew from political engagement and fundraising
*  and instead read my way back into history and as far to the political
*  left and political right as I could.
*  So just high level question.
*  What's your approach to learning?
*  Yeah.
*  So it's basically, I would say it's, it's, uh, auto-diadact.
*  Um, uh, so it sort of goes, it's going down the rabbit holes.
*  Um, so it's a combination of say, I kind of allude to it in that, in that quote,
*  it's a combination of breadth and depth.
*  Um, and so I tend to, yeah, I tend to, I go broad by the nature of what I do.
*  I go broad, but then I tend to go deep in a rabbit hole for a while,
*  read everything I can and then come out of it.
*  And I might not, I might not revisit that rabbit hole for another decade.
*  And in that blog post that I recommend people go check out, you actually list a
*  bunch of different books that you recommend on different topics on the
*  American left and the American right.
*  Uh, it's just a lot of really good stuff.
*  The best explanation for the current structure of our society and politics,
*  you give two recommendations, four books on the Spanish civil war, six books on
*  deep history of the American right.
*  Comprehensive biographies of Adolf Hitler.
*  Uh, one of which I read and I can recommend, uh, six books on the deep
*  history of the American left, the American right, American left, looking at
*  the history to give you the context.
*  Um, biography of, uh, Vladimir Lenin, two of them, uh, on the French revolution.
*  I actually have never read a biography on Lenin.
*  Maybe that that would be useful.
*  Everything's been so Marx focused.
*  The Sebastian biography of Lenin is extraordinary.
*  Uh, Victor Sebastian.
*  Okay.
*  It'll blow your mind.
*  Yeah.
*  So it's still useful to read.
*  It's incredible.
*  Yeah, it's incredible.
*  I actually think it's the single best book on the Soviet union.
*  So that the perspective of Lenin might be the best way to look at the
*  Soviet union versus Stalin versus Marx versus very interesting.
*  So two books on fascism and anti-fascism, uh, by the same author, Paul
*  Gottfried, a brilliant book on the nature of mass movements and collective
*  psychology, the definitive work on intellectual life under totalitarianism,
*  the captive mind, uh, the definitive work on the practical life under totalitarianism.
*  Uh, there's a bunch, there's a bunch and the single best book.
*  First of all, the list here is just incredible, but you say the single best
*  book I have found on who we are and how we got here is the ancient city, uh, by
*  Newman Dennis, for Stel de Coulongas.
*  I like it.
*  Uh, what's, uh, what did you learn about who we are as a human
*  civilization from that book?
*  Yeah.
*  So this is a fascinating book.
*  This one's free, but it's free by the way.
*  It's a, it's a book from the 1860s.
*  You can download it or you can buy printouts, prints of it.
*  But, um, it's, uh, it was this guy who was a professor at the Sorbonne in the
*  1860s and he was apparently a savant on, uh, antiquity on Greek and Roman
*  antiquity, um, and the reason I say that is because his sources are 100% original
*  Greek and Roman sources.
*  So he wrote a, basically a history of Western civilization from on the order
*  of 4,000 years ago to basically the present times entirely working on
*  original Greek and Roman, Roman sources.
*  Um, and what he was specifically trying to do was he was trying to reconstruct
*  from the stories of the Greeks and the Romans.
*  He was trying to reconstruct what life in the West was like before the Greeks
*  and the Romans, which was in this, in this, in the civilization known
*  as the, the, the Indo Europeans.
*  Um, and the short answer is, and this is sort of circa 4,000, you know,
*  2000 BC to, you know, sort of 500 BC, kind of that 1500 year stretch
*  where civilization developed.
*  Uh, and his conclusion was basically cults.
*  Um, they were basically cults and civilization was organized into cults
*  and the intensity of the cults was like a million fold beyond anything
*  that we would recognize today.
*  Like it was a level of, um, all encompassing belief and, uh, an
*  action around religion.
*  Um, that was at a level of extremeness that we wouldn't even recognize it.
*  Um, uh, and, and so specifically he tells the story of basically there
*  were three levels of cults.
*  There was the family cult, the tribal cult, and then the city cult as
*  society scaled up.
*  And then each cult was a joint cult of, uh, family gods, which were
*  ancestor gods and then nature gods.
*  Um, and then you are bonding into a family, a tribe or a city was based
*  on your adherence to that religion.
*  Um, people, uh, who were not of your family tribe city worshiped different
*  gods, which gave you not just the right with the responsibility to kill them on
*  site, right?
*  So they were serious about their cults.
*  Hard core, by the way, shocking development.
*  I did not realize it as a zero concept of individual rights, like even up
*  through the Greeks and even in the Romans, they didn't have the concept
*  of individual rights, like the idea that as an individual, you have like some
*  rights, just like, Nope.
*  Right.
*  And you look back and you're just like, wow, that's just like crazily like
*  fascist in a degree that we wouldn't recognize today, but it's like, well,
*  they were living under extreme pressure for survival.
*  And you, and you know, the theory goes, you could not have people running
*  around making claims to individual rights when you're just trying to get
*  like your tribe through the winter, right?
*  Like you need like hardcore command and control.
*  And so, and, and, and actually what, if through modern political lens, those
*  cults were basically both fascist and communist, um, they were fascist in
*  terms of social control, and then they were communist in terms of economics.
*  But you think that's fundamentally that like pull towards, uh, cults is within us.
*  Well, so, so my conclusion from this book, so, so, so, so the way
*  we naturally think about the world we live in today is like, we basically
*  have such an improved version of everything that came before us, right?
*  Like we, we have basically, we've figured out all these things around morality
*  and ethics and democracy and all these things, and like, they were basically
*  stupid and retrograde and we're like smart and sophisticated and we've improved all
*  this.
*  Um, I, I, after reading that book, uh, I, I now believe in many ways the opposite,
*  which is no, actually we are still running in that original model.
*  We're just running in an incredibly diluted version of it.
*  So we're still running basically in cults.
*  It's just our culture at like a thousandth or a millionth, the level of
*  intensity, right?
*  And so our, so just as to take religions, you know, the modern experience of a
*  Christian in our time, even somebody who considers them a devout Christian is
*  just a shadow of the level of intensity of somebody who belonged to a religion
*  back in that period.
*  And then by the way, we have constraints, it goes back to our AI discussion.
*  We, we, we, we then sort of endlessly create new cults.
*  Like we're trying to fill the void, right?
*  And the void is a void of bonding.
*  Okay.
*  Living in their era, like everybody living today, transporting that era would
*  view it as just like completely intolerable in terms of like the loss of
*  freedom and the level of basically fascist control.
*  However, every single person in that era, and he really stresses this, they knew
*  exactly where they stood, they knew exactly where they belonged.
*  They knew exactly what their purpose was.
*  They know exactly what they needed to do every day.
*  They know exactly why they were doing it.
*  They had total certainty about their place in the universe.
*  So the question of meaning, the question of purpose was very distinctly,
*  clearly defined for them.
*  Absolutely.
*  Overwhelmingly, undisputably, undeniably.
*  As we turn the volume down on the cultism, we start to, uh, the search for
*  meaning says getting harder and harder.
*  Yes.
*  Cause we, we don't have that.
*  We are, we are ungrounded.
*  We are, we are, we are uncentered and we, and we all feel it.
*  Right.
*  And that's why we reach for, you know, it's why we still reach for religion.
*  It's why we reach for, you know, we, people start to take on, you know, let's
*  say, you know, a faith in science, maybe beyond where they should put it.
*  Uh, you know, and by the way, like sports teams are like, um, you know, they're
*  like a tiny little version of a cult and, you know, the Apple keynotes are a
*  tiny little version of a cult, right?
*  You know, political, you know, and there's cult, you know, there's full
*  blown cults on both sides of the political spectrum right now, right?
*  Um, you know, and operating in plain size.
*  Still not full blown compared as to what it was compared to what it used to.
*  I mean, we would today consider full blown, but like, yes, they're, they're at
*  like, I don't know, a hundred thousandth or something of the intensity of, of
*  what people had back then.
*  So, so we live in a world today that in many ways is more advanced and moral
*  and so forth, and it's certainly a lot nicer, much nicer world to live in, but
*  we live in a world that's like very washed out.
*  It's like everything has become very colorless and gray as compared to how
*  people used to experience things, which is, I think why we're so prone to reach
*  for drama is we, there's something in us deeply evolved where we want that back.
*  And I wonder where it's all headed as we turn the volume down more and more.
*  Uh, what advice would you give to young folks today?
*  Uh, in high school and college, how to be successful in their career, how
*  to be successful in their life.
*  Yeah.
*  So the tools that are available today, I mean, are just like, I sometimes, you
*  know, bored, I sometimes bored, uh, you know, kids by describing like what it
*  was like to go look up a book, you know, to try to like discover a fact and, you
*  know, in the, in the old days, the 1970s, 1980s, you go to the library and the
*  card catalog and the whole thing, you go through all that work and then the
*  book is checked out and you have to wait two weeks and like, like to be in a
*  world, not only where you can get the answer to any question, but also the
*  world now, you know, the AI world where you've got like the assistant that will
*  help you do anything, help you teach, learn anything, like your ability both
*  to learn and also to produce is just like, I don't know, a million fold
*  beyond what it used to be.
*  I have a, I have a blog post I've been wanting to write.
*  Um, it was, I call out where, where are the hyper productive people?
*  Um, like your question, right?
*  Like with these tools, like there should be authors that are writing like hundreds
*  of thousands of like outstanding books.
*  Well, with the authors, there's a consumption question too, but yeah, well,
*  maybe not, maybe not.
*  You're right.
*  But so the tools are much more powerful.
*  Getting much more artists, musicians, right?
*  Why aren't musicians producing a thousand times the number of songs, right?
*  Um, like, like the tools are spectacular.
*  So what, uh, what's the explanation and by way of advice, like what is motivation
*  starting to be turned down a little bit or what?
*  I think it might be distraction.
*  It's so easy to just sit and consume, um, that I think people get
*  distracted from production.
*  But if you wanted to, um, you know, as a young person, if you wanted to really
*  stand out, you could get on like a hyper productivity curve very early on.
*  There's a great, uh, you know, the stir, there's a great story in Roman history
*  of plenty of the elder who was this legendary statesman, um, uh, died in the
*  Vesuvius eruption, trying to rescue his friends, but, um, he was famous both for
*  being a, uh, Savant, uh, basically being a polymath, but also being an author.
*  And he wrote apparently like hundreds of books, most of which have been lost, but
*  he like wrote all these encyclopedias and he literally like would be reading and
*  writing all day long, no matter what else was going on.
*  And he, so he would like travel with like four slaves and two of them were
*  responsible for reading to him and two of them were responsible for taking
*  dictation and so like he'd be going cross country and like literally he would
*  be writing books like all the time.
*  And apparently they were spectacular.
*  There's only a few that have survived, but apparently they were amazing.
*  So there's a lot of value to being somebody who finds focus in this life.
*  Yeah.
*  Like, and there are examples, like there are, uh, you know, there's this guy, uh,
*  judge, uh, was his name Posner Posner, um, who wrote like 40 books and was
*  also a great federal judge.
*  Um, you know, there's, uh, our, our friend Balaji, uh, I think it's like this.
*  He's one of these, you know, where he's, his output is just prodigious.
*  Um, and so it's like, yeah, I mean, with these tools, why not?
*  And I kind of think we're, we're at this interesting kind of freeze frame moment
*  where like this, these tools are now in everybody's hands and everybody's just
*  kind of staring at them, trying to figure out what to do, the new tools.
*  We have discovered fire and trying to figure out how to use it to cook.
*  Yeah.
*  Right.
*  Uh, you told Tim Ferriss that the perfect day is caffeine for 10 hours
*  and alcohol for four hours.
*  You didn't think I'd be mentioning this, did you?
*  Uh, it balances everything out perfectly.
*  As you said, so let me ask, what's, what's the secret to balance and
*  maybe to happiness in life?
*  Um, I don't believe in balance, so I, I'm the wrong person to ask.
*  Can you elaborate why you don't believe in balance?
*  I mean, I, I, I, maybe it's just, and I, I look, I think people, I think
*  people are wired differently, so I think it's hard to generalize this kind of
*  thing, but I'm, I am much happier and more satisfied when I'm fully committed
*  to something, so I'm very much in favor of imbalance.
*  Yeah.
*  In balance.
*  And that applies to work, to life, to everything.
*  Yeah.
*  Now, now I happen to have whatever twist of personality traits lead that in
*  non-destructive dimensions, including the fact that I've actually, I now
*  no longer do the 10-4 plan.
*  I stopped drinking.
*  I do the caffeine, but not the alcohol.
*  So there's something in my personality where I, I, whatever maladaption I have
*  is inclining me towards productive things, not unproductive things.
*  So you're one of the wealthiest people in the world.
*  What's the relationship between wealth and happiness?
*  Oh, uh, money and happiness.
*  So I think happiness, I don't think happiness is the thing to strive for.
*  I think satisfaction is the thing.
*  That's that just sounds like happiness, but turned down a bit.
*  No deeper.
*  So happiness is, you know, a walk in the woods at sunset, an ice cream cone, a kiss.
*  Um, the first ice cream cone is great.
*  The thousandth ice cream cone, not so much at some point, the walks
*  in the woods get boring.
*  What's the distinction between happiness and satisfaction?
*  Satisfaction is a deeper thing, which is like having found a purpose
*  and fulfilling it being useful.
*  So just, uh, something that permeates all your days, just this general
*  contentment of, of being useful.
*  That I'm fully satisfying my faculties that I'm fully delivering, right.
*  Uh, on the gifts that I've been given that I'm, you know, net making the world
*  better, that I'm contributing to the people around me, right.
*  And then I can look back and say, wow, that was hard, but it was worth it.
*  I think generally it seems to lead people in a better state than pursuit of pleasure.
*  Pursuit of quote unquote happiness.
*  Does money have anything to do with that?
*  I think the founders of the founding fathers in the U S threw this off kilter
*  when they use the phrase pursuit of happiness.
*  I think they should have said, they said pursuit of satisfaction.
*  We might live in a better world today.
*  Well, you know, they could have elaborated on a lot of things.
*  They could have tweaked the second.
*  I think they were smarter than you realize.
*  They said, you know what?
*  We're going to make it ambiguous and let these, uh, these humans figure out the
*  rest, these tribal cult like humans figure out the rest.
*  Uh, but money empowers that.
*  So I think, and I think they're, I mean, look, I think Elon is, I don't think I'm
*  even a great example, but I think Elon would be the great example of this, which
*  is like, you know, look, he's a guy who from every, every day of his life, from
*  the day he started making money at all, he just plows into the, into the next
*  thing.
*  Um, and so I think, I think money is definitely an enabler for satisfaction.
*  Money applied to happiness leads people down very dark paths.
*  Very destructive avenues.
*  Uh, money applied to satisfaction, I think could be, it is a real tool.
*  Um, I always like, by the way, I was like, you know, Elon is the case study for
*  behavior, but the other thing that it's always really made me think is Larry,
*  Larry Page was asked one time what his approach to philanthropy was.
*  And he said, Oh, I'm just my philanthropic plan is just give all the money to
*  Elon.
*  Right.
*  Uh, well, let me actually ask you about Elon.
*  What, what are your, um, you've interacted with quite a lot of successful
*  engineers and business people.
*  What do you think is special about Elon?
*  We talked about Steve Jobs.
*  What, um, what do you think is special about him as a leader, as an innovator?
*  Yeah.
*  So the, the core of it is he's a, he's, he's back to the future.
*  So he is, he is doing the most leading edge things in the world, but with a,
*  with a really deeply old school approach.
*  Um, and so to find comparisons to Elon, you need to go to like Henry Ford and
*  Thomas Watson and Howard Hughes and Andrew Carnegie, right.
*  Um, Leland Stanford, um, John D Rockefeller, right.
*  You need to go to the, what we're called the bourgeois capitalists, like the
*  hardcore business owner operators who basically built, you know, it does
*  basically built industrialized society.
*  Um, vendor built, um, and it's a level of hands on commitments, um, and, uh,
*  depth, um, in the business, um, coupled with an absolute priority, uh, towards
*  truth, um, and towards, um, kind of put it science and technology, uh, town to
*  first principles that is just like absolute, just like unbelievably absolute.
*  He really is ideal that he's only ever talking to engineers.
*  Like he does not tolerate, he has the absolute bullshit
*  tolerance to anybody I've ever met.
*  Um, he wants ground truth on every single topic.
*  Um, and he runs his businesses directly day to day devoted to getting to
*  ground truth in every single topic.
*  So, uh, you think it was a good decision for him to buy Twitter?
*  I have developed a view in life to not second guess Elon Musk.
*  I know this is going to sound crazy and unfounded, but well, I mean, uh,
*  he's got a, quite a track record.
*  I mean, look, the car was a crazy, I mean, the car was, I mean, look,
*  he's done a lot of things that seem crazy.
*  Starting a new car company in the United States of America, the last
*  time somebody really tried to do that was the 1950s and it was called
*  Tucker automotive and it was such a disaster.
*  They made a movie about what a disaster it was.
*  Um, and then rockets, like who does that?
*  Like that's, there's obviously no way to start a new rocket company.
*  Like those days are over and then to do those at the same time.
*  So after he pulled those two off, like, okay, fine.
*  Like, like, this is one of my areas of like, I, whatever opinions I had
*  about that is just like, okay, clearly are not relevant.
*  Like this is, you just, you at some point, you just like been on the person.
*  And in general, I wish more people would lean on celebrating and
*  supporting versus deriding and destroying.
*  Oh yeah.
*  I mean, look, he drives resentment.
*  Like it's like he is a magnet for resentment.
*  Um, like his critics are the most miserable, resentful people in the world.
*  Like it's almost a perfect match of like the most idealized, you know, technologists,
*  you know, of the century coupled with like just his critics are just bitter as can be.
*  I mean, it's, it's, I mean, it's, it's sort of very darkly a comic to watch.
*  Well, he, uh, he fuels the fire of that by being an asshole on Twitter at times.
*  And which is fascinating to watch the drama of human civilization, given our
*  cult roots, just fully on fire.
*  He's running a cult.
*  You could say that very successfully.
*  So now, now that our cults have gone and we searched for meaning, what do you
*  think is the meaning of this whole thing?
*  What's the meaning of life?
*  Mark Andreessen.
*  I don't know the answer to that.
*  Um, I think the meaning of, uh, of, uh, the closest I get to it is what I said
*  about satisfaction.
*  So it's basically like, okay, we were given what we have, like we
*  should basically do our best.
*  What's the role of love in that mix?
*  I mean, like what's the point of life if you're, yeah, without love, like.
*  So love is a big part of that.
*  Look like taking care of people is like a wonderful thing.
*  Like, you know, mentality, you know, there are pathological forms of taking care of
*  people, but there's also a very fundamental, you know, kind of aspect of
*  taking care of people.
*  Like for example, I happen to be somebody who believes that capitalism and taking
*  care of people are actually, they're actually the same thing.
*  Um, somebody once said capitalism is how you take care of people you don't know.
*  Right.
*  Um, right.
*  And so like, yeah, I think it's like deeply woven into the whole thing.
*  Um, you know, there's a long conversation we had about that, but yeah.
*  Yeah.
*  Creating products that are used by millions of people and bring them joy in
*  smaller, big ways, and then capitalism kind of enables that.
*  Encourages that.
*  David Friedman says there's only three ways to get somebody to do something for
*  somebody else.
*  Love, money and force.
*  Love and money are better.
*  Yeah.
*  That's a good ordering.
*  I think we should bet on those.
*  Try love first.
*  If that doesn't work, the money and then force.
*  Well, don't even try that one.
*  Um, Mark, you're an incredible person.
*  I've been a huge fan.
*  I'm glad to finally got a chance to talk.
*  I'm a fan of everything you do, everything you do, including on Twitter.
*  It's a huge honor to meet you, to talk with you.
*  Thanks again for doing this.
*  Awesome.
*  Thank you, Lex.
*  Thanks for listening to this conversation with Mark Andreessen to support this
*  podcast, please check out our sponsors in the description.
*  And now let me leave you with some words from Mark Andreessen himself.
*  The world is a very malleable place.
*  If you know what you want and you go for it with maximum energy and drive and
*  passion, the world will often reconfigure itself around you much more quickly and
*  easily than you would think.
*  Thank you for listening and hope to see you next time.
