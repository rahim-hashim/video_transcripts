# 'Hard Fork': An Interview With Sam Altman
**New York Times Podcasts:** [November 24, 2023](https://www.youtube.com/watch?v=wBX4xeefPiA)
*  Hey, it's Michael.
*  I hope you're having a wonderful Thanksgiving holiday.
*  If you didn't catch us yesterday, let me just say, in addition to you and everyone
*  who listens to The Daily, one of the things we are so grateful for here at the show is
*  our amazing colleagues, reporters and editors throughout the newsroom, and also throughout
*  the Times audio department.
*  So yesterday and today, we're doing something a little bit different.
*  We're turning the stage over to those colleagues in the audio department to showcase their
*  terrific work.
*  Today it's our friends at Hardfork.
*  If you're not familiar, Hardfork is a weekly tech conversation hosted by Kevin Roos and
*  Casey Newton.
*  It's excellent.
*  Case in point, for today's show, we're going to play you an interview that Kevin
*  and Casey did with Sam Altman, the CEO at OpenAI, just two days before Altman was abruptly
*  ousted and later reinstated by his board.
*  If you're a Daily Listener, earlier this week, we covered the entire saga in our episode
*  Inside the Coup at OpenAI.
*  Anyway, here's Kevin and here's Casey, who are going to say a little bit more about their
*  interview with Altman and about their show, Hardfork.
*  Take a listen.
*  Hello, Daily Listeners.
*  Hope you had a good Thanksgiving.
*  I'm Kevin Roos, a tech columnist for the New York Times.
*  I'm Casey Newton for Platformer.
*  As you just heard, we are the hosts of Hardfork.
*  It's a weekly podcast from the New York Times about technology, Silicon Valley, AI, the
*  future, all that stuff.
*  Casey, how would you describe our show Hardfork to the daily audience?
*  I would say if you're somebody who is curious about the future, but you're also the sort
*  of person who likes to get a drink at the end of the week with your friends, we are
*  the show for you.
*  Okay, we're going to tell you what is going on in this crazy world, but we're also going
*  to try to make sure you have a good time while we do it.
*  So this week, for example, we have been talking about the never ending saga at OpenAI that
*  Michael just mentioned.
*  If you haven't been following this news, let's just summarize what's going on in the quickest
*  way possible.
*  So last Friday, Sam Altman, the CEO of OpenAI and arguably one of the most important people
*  in the tech industry was fired by his board.
*  This firing shocked everyone, investors, employees, seemingly Sam Altman himself, who seemed not
*  to know what was coming.
*  Then over the next few days, there was a wild campaign by investors, employees, and eventually
*  some of the board members to bring back Sam Altman as CEO.
*  And late on Tuesday night, that campaign was successful.
*  The company announced that Sam was coming back and that the board was going to be reconstituted
*  and basically back to normal.
*  Yeah.
*  So on the one hand, Kevin, a shocking turn of events and on the other, by the time we
*  got here, basically the only turn of events possible, I think.
*  Yeah.
*  So it's been a totally insane few days.
*  We've done several emergency podcasts about this.
*  And today we are going to bring you something that I think is really important, which is
*  an interview with Sam Altman.
*  Now this interview predates Sam Altman's firing.
*  We recorded it on Wednesday of last week, just two days before he was fired.
*  We obviously weren't able to ask him about the firing or the events that followed.
*  But I think this conversation lays out Sam's worldview and is really important to understanding
*  why he's been such a controversial figure inside OpenAI and how he's thinking about
*  the way that AI is developing and how it's going to influence the future.
*  Yeah, in a way, Kevin, it's almost as if the firing never happened because what we were
*  curious about was how are you going to be leading OpenAI into the future?
*  And as of Tuesday evening, he now will once again be leading OpenAI into the future.
*  Totally.
*  So when we come back, our conversation with Sam Altman, the CEO of OpenAI, recorded just
*  two days before all of this drama started.
*  Sam Altman, welcome back to Hard Fork.
*  Thank you.
*  It has been just about a year since ChatGPT was released.
*  And I wonder if you have been doing some reflecting over the past year and kind of where it has
*  brought us in the development of AI.
*  Frankly, it has been such a busy year.
*  There has like not been a ton of time for reflections.
*  Well, that's why we brought you in.
*  We want you to reflect here.
*  Great.
*  I can do it now.
*  I mean, I definitely think this was the year so far, there will be maybe more in the future,
*  but the year so far where the general average tech person went from taking AI not that seriously
*  to taking it pretty seriously.
*  And the sort of recompiling of expectations given that.
*  So I think in some sense, that's like the most significant update of the year.
*  I would imagine that for you, a lot of the past year has been watching the world catch
*  up to things that you have been thinking about for some time.
*  Does it feel that way?
*  Yeah, it does.
*  We kind of always thought on the inside of OpenAI that it was strange that the rest of
*  the world didn't take this more seriously.
*  Like it wasn't more excited about it.
*  I think if five years ago you had explained what chat GPT was going to be, I would have
*  thought, wow, that sounds pretty cool.
*  And presumably I could have just looked into it more and I would have smartened myself
*  up.
*  But I think until I actually used it, as is often the case, it was just hard to know what
*  it was going to be.
*  Yeah, I actually think we could have explained it and it wouldn't have made that much of
*  a difference.
*  We tried.
*  Like people are busy with their lives.
*  They don't have like a lot of time to sit there and like listen to some tech people
*  prognosticate about something that may or may not happen.
*  But you should have a product that people use, like get real value out of and then it's
*  different.
*  Yeah.
*  I remember reading about the early days of the run up to the launch of chat GPT.
*  And I think you all have said that you did not expect it to be a hit when it launched.
*  No, we thought it would be a hit.
*  We didn't think it would be like this.
*  We did it because we thought it was going to be a hit.
*  We didn't think it was going to be like this big of a hit.
*  Right.
*  As we're sitting here today, I believe it's the case that you can't actually sign up for
*  chat GPT plus right now.
*  Is that right?
*  Correct.
*  Yeah.
*  So what's that all about?
*  We have like not enough capacity always, but at some point it gets really bad.
*  So over the last 10 days or so we have done, you know, we've like done everything we can.
*  We've rolled out new optimizations.
*  We've like disabled some features and then people just keep signing up.
*  It keeps getting slower and slower.
*  And there's like a limit at some point to what you can do there.
*  And you can't, we just don't want to offer like a bad quality of service.
*  And so it gets like slow enough that we just say, you know what, until we can make more
*  progress either with more GPUs or more optimizations, we're going to put this on hold.
*  Not a great place to be in to be honest, but you know, it's like the least of several bad
*  options.
*  Sure.
*  And I feel like in the history of tech development, there often is a moment with really popular
*  products where you just have to close signups for a little while.
*  Right.
*  And what's different about this than others is it's just, it's so much more compute intensive
*  than the world is used to for internet services.
*  So you don't usually have to do this.
*  Like usually by the time you're at this scale, you've like solved your scaling bottlenecks.
*  Yeah.
*  One of the interesting things for me about covering all the AI changes over the past year
*  is that it often feels like journalists and researchers and companies are discovering
*  properties of these systems sort of at the same time altogether.
*  I mean, I remember when we had you and Kevin Scott from Microsoft on the show earlier this
*  year around the Bing relaunch and you both said something to the effect of, well, to
*  discover what these models are or what they're capable of, you kind of have to put them out
*  into the world and have millions of people using them.
*  Then we saw, you know, all kinds of crazy, but also inspiring things.
*  You had Bing Sydney and, but you also had people starting to use these things in their
*  lives.
*  I guess I'm curious what you feel like you have learned about language models and your
*  language model specifically from putting them out into the world.
*  What we don't want to be surprised by is the capabilities of the model.
*  That would be bad.
*  And we were not, you know, with GPT-4, for example, we took a long time between finishing
*  the model and releasing it.
*  Red team did heavily really studied it, did all of the work internally, externally.
*  And there's, I'd say there's at least so far, and maybe now it's been long enough that we
*  have not been surprised by any capabilities the model had that we just didn't know about
*  it all in a way that we were for GPT-3, frankly, sometimes the people found stuff.
*  But what I think you can't do in the lab is understand how technology and society are
*  going to co-evolve.
*  So you can say, here's what the model can do and not do, but you can't say like, and
*  here's exactly how society is going to progress given that.
*  And that's where you just have to see what people are doing, how they're using it.
*  And that, like, well, one thing is they use it a lot.
*  That's one takeaway that we did not, clearly we did not appropriately plan for.
*  But more interesting than that is the way in which this is transforming people's productivity,
*  learning and how, like, you know, one example that I think is instructive because it was
*  the first and the loudest is what happened with chat GPT and education.
*  Days at least weeks, but I think days after the release of chat GPT, school districts
*  were like falling all over themselves to ban chat GPT.
*  And that didn't really surprise us.
*  Like that, we could have predicted and did predict.
*  The thing that happened after that quickly was, you know, like weeks to months was school
*  districts and teachers saying, hey, actually, we made a mistake.
*  And this is really important part of the future of education and the benefits far out with
*  the downside.
*  And not only are we unbanning it, we're encouraging our teachers to make use of it in the classroom.
*  We're encouraging our students to get really good at this tool because it's going to be
*  part of the way people live.
*  And, you know, then there was like a big discussion about what the kind of path forward should
*  be.
*  And that is just not something that could have happened without releasing.
*  And part, can I say one more thing?
*  Part of the decision that we made with the chat GPT release, the original plan had been
*  interface and GPT four together in March.
*  And we really believe in this idea of iterative deployment.
*  And we had realized that chat, the chat interface plus GPT four was a lot.
*  I don't think we realized quite how much it was too much for society to split it.
*  And we put out, we put it out with GPT 3.5 first, which we thought was much weaker model
*  turned out to still be powerful enough for a lot of use cases.
*  But I think that in retrospect was a really good decision and helped with that process
*  of gradual adaptation for society.
*  Looking back, do you wish that you had done more to sort of, I don't know, give people
*  some sort of a manual to say, here's how you can use this at school or at work?
*  Two things.
*  One, I wish we had done something intermediate between the release of 3.5 in the API and
*  chat GPT.
*  Now I don't know how well that would have worked because I think there was just going
*  to be some moment where it went like viral in the mind of society.
*  And I don't know how incremental that could have been.
*  That's sort of a like, either it goes like this or doesn't kind of thing.
*  And I think I have reflected on this question a lot.
*  I think the world was going to have to have that moment.
*  It was better sooner than later.
*  It was good.
*  We did it when we did.
*  Maybe we should have tried to push it even a little earlier, but it's a little chancey
*  about when it hits.
*  And I think only a consumer product could have done what happened there.
*  Now the second thing is, should we have released more of a how to manual?
*  And I honestly don't know.
*  I think we could have done some things there that would have been helpful, but I really
*  believe that it's not optimal for tech companies to tell people like, here is how to use this
*  technology and here's how to do whatever.
*  And the organic thing that happened there actually was pretty good.
*  I'm curious about the thing that you just said about we thought it was important to
*  get this stuff into folks hands sooner rather than later.
*  Say more about what that is.
*  More time to adapt for our institutions and leaders to understand, for people to think
*  about what the next version of the model should do, what they'd like, what would be useful,
*  what would not be useful, what would be really bad.
*  How society and the economy need to co-evolve.
*  Like the thing that many people in the field or adjacent to the field have advocated or
*  used to advocate for, which I always thought was super bad was, you know, this is so disruptive,
*  such a big deal.
*  It's got to be done in secret by the small group of us that can understand it.
*  And then we will fully build the AGI and push a button all at once when it's ready.
*  And I think that'd be quite bad.
*  Yeah, because it would just be way too much change too fast.
*  Again, society and technology have to co-evolve and people have to decide what's going to
*  work for them and not and how they want to use it.
*  And we're, you know, you can criticize open-air about many, many things, but we do try to
*  like really listen to people and adapt it in ways that make it better or more useful
*  and we're able to do that.
*  But we wouldn't get it right without that feedback.
*  Yeah.
*  I want to talk about AGI and the path to AGI later on.
*  But first I want to just define AGI and have you talk about sort of where we are on the
*  continuum.
*  I think it's a ridiculous and meaningless term.
*  Yeah.
*  So I apologize that I keep using it.
*  It's deep in the muscle memory.
*  I just never know what people are talking about.
*  No one else is talking about it.
*  They mean like really smart AI.
*  Yeah, so it stands for artificial general intelligence and you could probably ask a
*  hundred different AI researchers and they would give you a hundred different definitions
*  of what AGI is.
*  Researchers at Google DeepMind just released a paper this month that sort of offers a framework.
*  They have five levels, level, or I guess they have levels ranging from level zero,
*  which is no AI, all the way up to level five, which is superhuman.
*  And they suggest that currently chat GPT, Bard, Lama2 are all at level one, which is
*  sort of equal to or slightly better than an unskilled human.
*  Would you agree with that?
*  Like where are we?
*  If you would, if you'd say this is a term that means something and you sort of define
*  it that way, how close are we?
*  I think the thing that matters is the curve and the rate of progress.
*  And there's not going to be some milestone that we all agree like, okay, we've passed
*  it and now it's called AGI.
*  Like what I would say is we currently have systems that are like, there will be researchers
*  who will write papers like that and you know, academics will debate it and people in the
*  industry will debate it.
*  And I mean, most of the world just cares like, is this thing useful to me or not?
*  We currently have systems that are somewhat useful clearly, like, and you know, whether
*  we want to say like it's a level one or two, I don't know, but people use it a lot and
*  they really love it.
*  There's huge weaknesses in the current systems, but it doesn't mean that like I'm, you know,
*  a little embarrassed by GPTs, but people still like them and that that's good.
*  Like it's nice to do useful stuff for people.
*  So yeah, call it a level one doesn't bother me at all.
*  I am embarrassed by it.
*  We will make them much better.
*  But at the current state, they're still like delighting people.
*  I mean, useful to people.
*  Yeah.
*  I also think it underrates them slightly to say that they're just better than unskilled
*  humans.
*  Like when I use ChachiPD, it is better than skilled humans for some things.
*  And worse than unskilled, worse than any human in many other things.
*  But I guess this is one of the questions that people ask me the most.
*  What are today's AI systems useful and not useful for doing?
*  I would say the main thing that they're bad at, well, many things, but one that is on
*  my mind a lot is they're bad at reasoning.
*  And a lot of the valuable human things require some degree of complex reasoning.
*  But they're good at a lot of other things.
*  Like GPT-4 is vastly superhuman in terms of its world knowledge.
*  It knows there's a lot of things in there.
*  And it's just very different than how we think about evaluating human intelligence.
*  So it can't do these basic reasoning tasks.
*  On the other hand, it knows more than any human has ever known.
*  On the other hand, again, sometimes it like totally makes stuff up in a way that a human
*  would not.
*  But if you're using it to be a coder, for example, it can hugely increase your productivity.
*  And there's value there, even though it has all of these other weak points.
*  If you were a student, you can learn a lot more than you could without using this tool
*  in some ways.
*  Value there, too.
*  Let's talk about GPTs, which you announced at your recent developer conference.
*  For those who haven't had a chance to use one yet, Sam, what's a GPT?
*  It's like a custom version of chat GPT that you can get to behave in a certain way.
*  You can give it limited ability to do actions.
*  You can give it knowledge to refer to.
*  You can say, like, act this way.
*  But it's super easy to make.
*  And it's a first step towards more powerful AI systems and agents.
*  We've had some fun with them on the show.
*  There's a hard fork bot that you can sort of ask about anything that's happened on any
*  episode of the show.
*  It works pretty well, we found, when we did some testing.
*  But I want to talk about where this is going.
*  What is the GPTs that you've released a first step toward?
*  Um.
*  AIs that can accomplish useful tasks.
*  Like the the.
*  I think we need to move towards this with great care.
*  You know, we don't.
*  I think it would be a bad idea to put like turn powerful agents free on the Internet.
*  But.
*  AIs that can act on your behalf to do something with a company that can access your data,
*  that can like help you be good at your task.
*  I think that's that's going to be an exciting way we use computers.
*  Like we have this belief that we're heading towards a vision where there are.
*  New interfaces, new user experience as possible because finally the computer can understand
*  you and think.
*  And so the sci fi vision of a computer that you just like tell what you want and it figures
*  out how to do it.
*  This is a step towards that.
*  Right now, I think what's holding a lot of people back in a lot of companies and organizations
*  back in sort of using this kind of AI in their work is that it can be unreliable.
*  It can make up things.
*  It can give wrong answers, which is fine if you're doing creative writing assignments,
*  but not if you're a hospital or a law firm or something else with big stakes.
*  Um, how how do we solve this problem of reliability?
*  And do you think we'll ever get to the sort of low fault tolerance that is needed for
*  these really high stakes applications?
*  So first of all, I think this is like.
*  A great example of people understanding the technology, making smart decisions with it.
*  Society and the technology co-evolving together like what what you see is that people are
*  using it where appropriate and where it's helpful and not using it where you shouldn't.
*  For all of the sort of like fear that people have had, like both users and companies seem
*  to really understand the limitations and are making appropriate decisions about where to
*  roll it out.
*  It.
*  The kind of controllability, reliability, whatever you want to call it, that is going to get
*  much better.
*  Um, I think we'll see a big step forward there over the coming years.
*  And.
*  And I think that there will be a time.
*  I don't know if it's like 2026, 2028, 2030, whatever, but there will be a time where we
*  just don't talk about this anymore.
*  Yeah, it seems to me, though, that that is something that becomes very important to get
*  right in the as you build these more powerful GPTs, right?
*  Once I tell like I would love to have a GPT be my assistant, go through my emails.
*  Hey, don't forget to respond to this before the end of the day.
*  Reliability has got to be way up before that happens.
*  Yeah, yeah, that makes sense.
*  Um, you mentioned as we started to talk about GPTs that you have to do this carefully.
*  Um, for folks who haven't spent as much time reading about this, explain what are some
*  things that could go?
*  You know, you guys are obviously going to be very careful with this.
*  Other people are going to build GPT like things might not put the same kind of controls in
*  place.
*  So what can you imagine other people's doing that like you as the CEO would say your folks,
*  hey, it's not going to be able to do that.
*  Well, that example that you just gave, like if you let it act as your assistant and go
*  like, you know, send emails, do finance, financial transfers for you, like it's very easy to
*  imagine how that could go wrong.
*  But I think most people who would use this don't want that to happen on their behalf
*  either.
*  And so there's more resilience to this sort of stuff than people think.
*  I think there's I mean, for what's worth on the whole on the hallucination thing, which
*  it does feel like has maybe been the longest conversation that we've had about chat GPT
*  in general since it launched.
*  I just always think about Wikipedia as a resource I use all the time.
*  And I don't want Wikipedia to be wrong.
*  But 100 percent of the time, it doesn't matter if it does.
*  I am not relying on it for life saving information.
*  Right.
*  Chat GPT for me is the same.
*  Right.
*  It's like, hey, you know, it's I mean, it's like great.
*  And just kind of bar trivia like, hey, you know, what's like the history of this conflict
*  in the world?
*  Yeah.
*  I mean, we want to get that a lot better.
*  And we will.
*  Like, I think the next model will just loosen it much less.
*  Is there an optimal level of hallucination in an AI model?
*  Because I've heard researchers say, well, you actually don't want it to never hallucinate
*  because that would mean making it not creative.
*  That new ideas come from making stuff up that's not necessarily tethered to.
*  This is why I tend to use the word controllability and not reliability.
*  You want it to be reliable when you want.
*  You want it to either you instruct it or it just knows based off of the context that you
*  are asking a factual query and you want the 100% black and white answer.
*  But you also want to know when you want it to hallucinate or you want it to make stuff
*  up.
*  As you just said, like new discovery happens because you come up with new ideas, most of
*  which are wrong and you discard those and keep the good ones and sort of add those to
*  your understanding of reality.
*  Or if you're telling a creative story, you want that.
*  So if these models didn't hallucinate at all ever, they wouldn't be so exciting.
*  They wouldn't do a lot of things that they can do, but you only want them to do that
*  when you want them to do that.
*  And so the way I think about it is like model capability, personalization and controllability.
*  And those are like the three axes we have to push on.
*  And controllability means no hallucinations when you don't want lots of it when you're
*  trying to invent something new.
*  Let's maybe start moving into some of the debates that we've been having about AI over
*  the past year.
*  And I actually want to start with something that I haven't heard as much, but that I do
*  bump into when I use your products, which is like, they can be quite restrictive in
*  how you use them.
*  I think mostly for great reasons, right?
*  I think you guys have learned a lot of lessons from the past era of tech development.
*  At the same time, I feel like I've tried to ask Chachi P.T. a question about sexual health.
*  I feel like it's going to call the police on me, right?
*  So I'm just curious how you've approached that subject.
*  Look, one thing, no one wants to be scolded by a computer ever.
*  That is not a good feeling.
*  And so you should never feel like you're going to have the police call you.
*  It's more like horrible, horrible, horrible.
*  We have started very conservative, which I think is a defensible choice.
*  Other people may have made a different one.
*  But again, that principle of controllability, what we'd like to get to is a world where
*  If you want some of the guardrails relaxed a lot and that's like, you know, you're not
*  like a child or something, then fine, we'll relax the guardrails.
*  It should be up to you.
*  But I think starting super conservative here, although annoying, is a defensible decision.
*  And I wouldn't have gone back and made it differently.
*  We have relaxed it already.
*  We will relax it much more, but we want to do it in a way where it's user controlled.
*  Yeah.
*  Are there certain red lines you won't cross, things that you will never let your models
*  be used for other than things that are obviously illegal or dangerous?
*  Yeah, certainly things that are illegal and dangerous we won't.
*  There's a lot of other things that I could say, but they so depend.
*  Where those red lines will be so depend on how the technology evolves that it's hard
*  to say right now, here's the exhaustive set.
*  We really try to just study the models and predict capabilities as we go, but if we learn
*  something new, we change our plans.
*  One other area where things have been shifting a lot over the past year is in AI regulation
*  and governance.
*  I think a year ago, if you had asked the average congressperson, what do you think of AI?
*  They would have said, what's that?
*  Get out of my office.
*  We just recently saw the Biden White House put out an executive order about AI.
*  You have obviously been meeting a lot with lawmakers and regulators, not just in the
*  US, but around the world.
*  What's your view of how AI regulation is shaping up?
*  It's a really tricky point to get across.
*  What we believe is that on the frontier systems, there does need to be proactive regulation
*  there, but heading into overreach and regulatory capture would be really bad.
*  There's a lot of amazing work that's going to happen with smaller models, smaller companies,
*  open source efforts, and it's really important that regulation not strangle that.
*  It's like, I've sort of become a villain for this, but I think there was...
*  You have.
*  How do you feel about this?
*  Like annoyed, but have bigger problems in my life right now.
*  But this message of regulate us, regulate the really capable models that can have significant
*  consequences, but leave the rest of the industry alone, it's just a hard message to get across.
*  There is an argument that was made to me by a high ranking executive at a major tech company
*  as some of this debate was playing out.
*  This person said to me that there is essentially no harms that these models can have that the
*  internet itself doesn't enable, and that to do any sort of work like it is proposed in
*  this executive order to have to inform the Biden administration is just essentially pulling
*  up the ladder behind you and ensuring that the folks who've already raised the money
*  can sort of reap all of the profits of this new world and will leave the little people
*  behind.
*  So I'm curious what you make of that argument.
*  I disagree with that on a bunch of levels.
*  First of all, I wish the threshold for when you do have to report was set differently
*  and based off of evals and capability thresholds.
*  Not flops?
*  Not flops.
*  Okay.
*  There's no small company training with that many flops anyway.
*  So that's like a little bit.
*  Yeah.
*  For the listener who maybe didn't listen to our last episode about this.
*  Listen to our flops episode.
*  The flops are the sort of measure of the amount of computing that is used to train these models.
*  The executive order says if you're above a certain computing threshold, you have to tell
*  the government that you're training a model that big.
*  But no small effort is training at 10 to the 26 flops.
*  Currently no big effort is either.
*  So that's like a dishonest comment.
*  Second of all, the burden of just saying like, here's what we're doing is not that great.
*  But third of all, the underlying thing there, there's nothing you can do here that you couldn't
*  already do on the internet.
*  That's the real either dishonesty or lack of understanding.
*  You could maybe say with GPT-4, you can't do anything.
*  You can't do on the internet.
*  I don't think that's really true even at GPT-4.
*  There are some new things.
*  In GPT-5 and 6, there will be very new things.
*  Saying that we're going to be cautious and responsible and have some testing around that,
*  I think that's going to look more prudent in retrospect than it maybe sounds right now.
*  I have to say for me, these seem like the absolute gentlest regulations you could imagine.
*  It's like tell the government and report on any safety testing you did.
*  Seems reasonable.
*  Yeah.
*  Just saying that these fears are unjustified of AI and existential risk.
*  Some people, some of the more vocal critics of OpenAI have said that OpenAI, that you
*  are specifically lying about the risks of human extinction from AI, creating fear so
*  that regulators will come in and make laws or give executive orders that prevent smaller
*  competitors from being able to compete with you.
*  Andrew Ng, who's I think one of your professors at Stanford, recently said something to this
*  effect.
*  What's your response to that?
*  I'm curious if you have thoughts about that.
*  Yeah.
*  I actually don't think we're all going to go extinct.
*  I think it's going to be great.
*  I think we're heading towards the best world ever.
*  When we deal with a dangerous technology as a society, we often say that we have to confront
*  and successfully navigate the risks to get to enjoy the benefits.
*  That's a pretty consensus thing.
*  I don't think that's a radical position.
*  I can imagine that if this technology stays on the same curve, there are systems that
*  are capable of significant harm in the future.
*  Andrew also said not that long ago that he thought it was totally irresponsible to talk
*  about AGI because it was just never happening.
*  I think he compared it to worrying about overpopulation on Mars.
*  I think now he might say something different.
*  Humans are very bad at having intuition for exponentials.
*  Again, I think it's going to be great.
*  I wouldn't work on this if I didn't think it was going to be great.
*  People love it already.
*  I think they're going to love it a lot more.
*  But that doesn't mean we don't need to be responsible and accountable and thoughtful
*  about what the downsides could be.
*  In fact, I think the tech industry often has only talked about the good and not the bad.
*  That doesn't go well either.
*  The exponential thing is real.
*  I have dealt with this.
*  I've talked about the fact that I was only using GPT 3.5 until a few months ago and finally
*  at the urging of a friend upgraded and I thought, oh.
*  I would have given you a free account.
*  I'm sorry you waited.
*  I wish I should have asked.
*  But it's a real improvement.
*  It is a real improvement and not just in the sense of, oh, the copy that it generates is
*  better.
*  It actually transformed my sense of how quickly the industry was moving.
*  It made me think, oh, the next generation is going to be sort of radically better.
*  I think that part of what we're dealing with is just that it has not been widely distributed
*  enough to get people to reckon with the implications.
*  I disagree with that.
*  I think that maybe the tech experts say, oh, this is not a big deal, whatever.
*  Most of the world who has used even the free version is like, oh, man, they got real AI.
*  You went around the world this year talking to people in a lot of different countries.
*  I'd be curious to what extent that informed what you just said.
*  Significantly.
*  I had a little bit of a sample bias because the people that wanted to meet me were probably
*  pretty excited.
*  You do get a sense and there's quite a lot of excitement, maybe more excitement, than
*  the rest of the world and the US.
*  Sam, I want to ask you about something else that people are not happy about when it comes
*  to these language and image models, which is this issue of copyright.
*  I think a lot of people view what OpenAI and other companies did, which is sort of hoovering
*  up work from across the internet, using it to train these models that can, in some cases,
*  output things that are similar to the work of living authors or writers or artists.
*  They just think this is the original sin of the AI industry and we are never going to
*  forgive them for doing this.
*  What do you think about that and what would you say to artists or writers who just think
*  that this was a moral lapse?
*  Forget about the legal, whether you're allowed to do it or not, that it was just unethical
*  for you and other companies to do that in the first place.
*  Well, we block that stuff.
*  You can't go to Dolly and generate something.
*  Speaking of being annoyed, we may be too aggressive on that.
*  But I think it's the right thing to do until we figure out some sort of economic model
*  that works for people.
*  We're doing some things there now, but we've got more to do.
*  Other people in the industry do allow quite a lot of that.
*  I get why artists are annoyed.
*  I guess I'm talking less about the output question than just the act of taking all of
*  of it copyrighted without the explicit permission of the people who created it and using it
*  to train these models.
*  What would you say to the people who just say, Sam, that was the wrong move.
*  You should have asked and we will never forgive you for it.
*  First of all, always have empathy for people who are like, hey, you did this thing and
*  it's affecting me and we didn't talk about it first or it was just a new thing.
*  I do think that in the same way humans can read the Internet and learn, AI should be
*  allowed to read the Internet and learn.
*  Shouldn't be regurgitating.
*  Shouldn't be violating any copyright laws.
*  But if we're really going to say that AI doesn't get to read the Internet and learn, and if
*  you read a physics textbook and learn how to do a physics calculation, not every time
*  you do that in the rest of your life, you've got to figure out how to...
*  That seems like not a good solution to me.
*  But on individuals' private work, we try not to train on that stuff.
*  We really don't want to be here upsetting people.
*  Again, I think other people in the industry have taken different approaches.
*  And we've also done some things that I think now that we understand more, we'll do differently
*  in the future.
*  Like what?
*  Like what we do differently.
*  We want to figure out new economic models so that, say, if you're an artist, we don't
*  just totally block you.
*  We don't just not train on your data, which a lot of artists also say, no, I want this
*  in here.
*  I want like whatever.
*  But we have a way to help share revenue with you.
*  GPTs are maybe going to be an interesting first example of this because people will
*  be able to put private data in there and say, hey, use this version and there could be a
*  revenue share around it.
*  Well, I had one question about the future that kind of came out of what we were talking
*  about, which is what is the future of the Internet as chat GPT rises?
*  And the reason I ask is I now have a hotkey on my computer that I type when I want to
*  know something and it just accesses chat GPT directly through software called Raycast.
*  And because of this, I am using Google search, not nearly as much.
*  I am visiting websites, not nearly as much.
*  That has implications for all the publishers and for frankly, just the model itself because
*  presumably if the economics change, there will be fewer web pages created.
*  There's less data for chat GPT to access.
*  So I'm just curious what you have thought about the Internet in a world where your product
*  succeeds in the way you want it to.
*  I do think.
*  If this all works, it should really change how we use the Internet.
*  There's a lot of things that the interface for is like perfect.
*  Like if you want to like mindlessly watch TikTok videos, perfect.
*  But if you're trying to like get information or get a task accomplished, it's actually
*  like quite bad relative to what we should all aspire for.
*  And you can totally imagine a world where you have a task that right now takes like
*  hours of stuff clicking around the Internet and bringing stuff together.
*  And you just ask chat GPT to do one thing and it goes off and computes and you get the
*  answer back.
*  And I'll be disappointed if we don't use the Internet differently.
*  Yeah.
*  Do you think that the economics of the Internet as it is today are robust enough to withstand
*  the challenge that AI poses?
*  Okay.
*  Well, I worry in particular about the publishers.
*  The publishers have been having a hard time already for a million other reasons.
*  But to the extent that they're driven by advertising and visits to web pages and to the extent
*  that the visits to the web pages are driven by Google search in particular, a world where
*  web search is just no longer the front page to most of the Internet, I think does require
*  a different kind of web economics.
*  I think it does require a shift, but I think the value is so what I thought you were asking
*  about was like, is there going to be enough value there for some economic model to work?
*  And I think that's definitely going to be the case.
*  Yeah, the model may have to shift.
*  I would love it if ads become less a part of the Internet.
*  I was thinking the other day, I just had this like, for whatever reason, this thought in
*  my head as I was browsing around the Internet being like, there's more ads than content
*  everywhere.
*  I was reading a story today scrolling on my phone and I managed to get it to a point where
*  between all of the ads on my relatively large phone screen, there was one line of text from
*  the article visible.
*  You know, one of the reasons I think people like Chad GPD, even if they can't articulate
*  is we don't do ads.
*  As an intentional choice, because there's plenty of ways you could imagine us putting
*  ads.
*  Totally.
*  But we made the choice that ads plus AI can get a little dystopic.
*  We're not saying never, like we do want to offer a free service, but like a big part
*  of our mission fulfillment, I think, is if we can continue to offer Chad GPD for free
*  at a high quality of service to anybody who wants it and just say like, hey, here's free
*  and good free.
*  And no ads, because I think that really does, especially as the AI gets really smart, that
*  really does get a little strange.
*  Yeah.
*  I know we talked about AGI and it not being your favorite term, but it is a term that
*  people in the industry use as sort of a benchmark or a milestone or something that they're aiming
*  for.
*  And I'm curious what you think the barriers between here and AGI are.
*  Maybe let's define AGI as sort of a computer that can do any cognitive task that a human
*  can.
*  If it, like, let's say we make an AI that is really good, but it can't go discover novel
*  physics.
*  Would you call that AGI?
*  I probably would.
*  Yeah.
*  Would you?
*  Well, again, I don't like the term, but I wouldn't call that we're done with the mission.
*  I'd say we still got a lot more work to do.
*  The vision is to create something that is better at humans than doing original science
*  that can invent, can discover.
*  Well, I am a believer that all real sustainable human progress comes from scientific and technological
*  progress.
*  And if we can have a lot more of that, I think it's great.
*  And if the system can do things that we unaided on our own can't, just even as a tool that
*  helps us go do that, then I will consider that a massive triumph and happily, you know,
*  I can happily retire at that point.
*  But before that, I can imagine that we do something that creates incredible economic
*  value, but is not the kind of AGI, super intelligence, whatever you want to call it, thing that
*  we should aspire to.
*  Right.
*  What are some of the barriers to getting to that place where we're doing novel physics
*  research?
*  And keep in mind, Kevin, I don't know anything about technology.
*  And if you start talking about, you know, like retrieval, augmented generation or anything,
*  like I might, you might all follow, but you'll follow.
*  Yeah.
*  We talked earlier about just the models, limited ability to reason.
*  And I think that's one thing that needs to be better.
*  The model needs to be better at reasoning.
*  Like GBT four.
*  An example of this that my co-founder, Ilya, uses sometimes that is really stuck in my
*  mind is there was like a time in Newton's life where the right thing for him to do-
*  You're talking to me about Isaac Newton, not my life.
*  Isaac Newton.
*  Well, maybe you do.
*  But maybe my life.
*  Stay tuned.
*  Where the right thing for him to do is to read every math textbook he could get his
*  hands on.
*  He should like talk to every smart professor, talk to his peers, you know, do problem sets,
*  whatever.
*  And that's kind of what our models do today.
*  And at some point, there was, he was never going to invent calculus doing that, what
*  didn't exist in any textbook.
*  And at some point he had to go think of new ideas and then test them out and build on
*  them, whatever else.
*  And that phase, that second phase, we don't do yet.
*  And I think you need that before.
*  It's something I want to call an AGI.
*  One thing that I hear from AI researchers is that a lot of the progress that has been
*  made over the past, call it five years in this type of AI has been just the result of
*  just things getting bigger, right?
*  Bigger models, more compute.
*  Obviously, there's work around the edges in how you build these things that makes them
*  more useful.
*  There hasn't really been a shift on the architectural level of the systems that these models are
*  built on.
*  Do you think that that is going to remain true?
*  Or do you think that we need to invent some new process or new, you know, new mode or
*  new technique to get through some of these barriers?
*  We will need new research ideas and we have needed them.
*  I don't think it's fair to say there haven't been any here.
*  I think a lot of the people who say that are not the people building GPT-4, but they're
*  the people sort of opining from the sidelines.
*  But there is some kernel of truth to it.
*  And the answer is we have, OpenAI has a philosophy of we will just do whatever works.
*  Like if it's time to scale the models and work on the engineering challenges, we'll
*  go do that.
*  If now we need a new algorithmic breakthrough, we'll go work on that.
*  If now we need a different kind of data mix, we'll go work on that.
*  So like, we just do the thing in front of us and then the next one and then the next
*  one and then the next one.
*  And there are a lot of other people who want to write papers about, you know, level one,
*  two, three and whatever.
*  And there are a lot of other people who want to say, well, it's not real progress.
*  They just made this like incredible thing that people are using and loving and it's
*  not real.
*  But our belief is like, we will just do whatever we can to usefully drive the progress forward
*  and we're kind of open minded about how we do that.
*  What is super alignment?
*  You all just recently announced that you are devoting a lot of resources and time and computing
*  power to super alignment and I don't know what it is.
*  So can you help me understand?
*  That's alignment that comes with sour cream and guacamole.
*  There you go.
*  San Francisco taco shop.
*  That's a very San Francisco specific joke, but it's pretty good.
*  I'm sorry.
*  Go ahead, Sam.
*  I don't, can I leave it at that?
*  I don't really want to follow.
*  I mean, that was such a good answer.
*  No, it's so alignment is how you sort of get these models to behave in accordance with
*  the human who's using them, what they want.
*  And super alignment is how you do that for super capable systems.
*  So we know how to align GPT-4 pretty well, but like better than people thought we were
*  going to be able to do.
*  Now there's this like when we put out GPT-2 and three people are like, oh, it's irresponsible
*  because this is always going to just like spew toxic shit.
*  You're never going to get it.
*  And it actually turns out like we're able to align GPT-4 reasonably well, maybe too
*  well.
*  Yeah.
*  It's, I mean, good luck getting it to talk about sex is my official comment about GPT-4.
*  But that's, you know, in some sense that's an alignment failure because that's, it's
*  not doing what you want it there.
*  So, but now we have that.
*  Now we have like the social part of the problem.
*  We can technically do it.
*  Right.
*  We don't yet know what the new challenges will be for much more capable systems.
*  And so that's what that team research is.
*  So like what kinds of questions are they investigating or what research are they doing?
*  Because I, you know, I confess, I sort of, I lose my grounding in reality when you start
*  talking about super capable systems and the problems that can emerge with them.
*  Is this sort of a theoretical future forecasting team?
*  Well, they try to do work that is useful today, but for the theoretical systems of the future.
*  So they'll have their first result coming out, I think pretty soon.
*  But yeah, they're interested in these questions of as the systems get more capable than humans,
*  what is it going to take to reliably solve the alignment challenge?
*  Yeah.
*  And I mean, this is the stuff where my brain does feel like it starts to melt as I ponder
*  the implications, right?
*  Because you've made something that is smarter than every human.
*  But you, the human, have to be smart enough to ensure that it always acts in your interest,
*  even though by definition is way smarter.
*  We need some help there.
*  Yeah, I do want to stick on this issue of alignment or super alignment, because I think
*  there's, there's an unspoken assumption in there that what you just, you just put it
*  as alignment is sort of what, what the user wants it to behave like.
*  And obviously there are a lot of users with good intentions.
*  No, no, yeah, it has to be like what society and the user can intersect on.
*  There are going to have to be some rules here.
*  And I guess where do you derive those rules?
*  Because if you're anthropic, you use, you know, the UN declaration of human rights and the
*  Apple terms of service.
*  And that becomes your most important documents in rights governance.
*  If you're not just going to borrow someone else's rules, how do you decide which values
*  these things should align themselves to?
*  We're doing this thing.
*  We've been doing this thing.
*  We've been doing these like democratic input governance grants where we're giving different
*  research teams money to go off and set different proposals.
*  There's some very interesting ideas in there about how to, how to kind of fairly decide
*  that.
*  The naive approach to this that I have always been interested in, maybe we'll try at some
*  point is what if you had hundreds of millions of chat GPT users spend an hour, few hours
*  a year answering questions about what they thought the default settings should be, what
*  the wide bounds should be.
*  Eventually you need more than just chat GPT users.
*  You need the whole world represented in some way.
*  Because even if you're not using it, you're still impacted by it.
*  But to start, what if you literally just had chat GPT chat with its users?
*  I think it's very important.
*  It would be very important in this case to let the users make final decisions, of course.
*  But you could imagine it saying like, hey, you answered this question this way.
*  Here's how this would impact other users in a way you might not have thought of.
*  If you want to stick with your answer, that's totally up to you.
*  But are you sure given this new data?
*  And then you could imagine like GPT five or whatever, just learning that collective preference
*  set.
*  And I think that's interesting to consider.
*  Better than the Apple terms of service, let's say.
*  I want to ask you about this feeling.
*  Kevin and I call it AI vertigo.
*  Is this a widespread term?
*  No, I think you invented this.
*  So there is this moment when you contemplate even just kind of the medium AI future.
*  You start to think about what it might mean for the job market, your own job, your daily
*  life, for society.
*  And there is this kind of dizziness that I find sets in.
*  This year I actually had a nightmare about AGI.
*  And then I sort of asked around and I feel like people who work on this stuff, like that's
*  not uncommon.
*  I wonder for you if you have had these moments of AI vertigo, if you continue to have them,
*  or is there at some point where you think about it long enough that you feel like you
*  get your legs underneath you?
*  I used to have, I mean, there were some, I can point to these moments, but there were
*  some very strange, like extreme vertigo moments.
*  Particularly around the launch of GPT-3.
*  But you do get your legs under you.
*  And I think the future will somehow be less different than we think.
*  Like it's this amazing thing to say, right?
*  Like we invent AGI and it matters less than we think.
*  It doesn't sound like a sentence that parses.
*  And yet it's what I expect to happen.
*  Why is that?
*  There's like a lot of inertia in society and humans are remarkably adaptable to any amount
*  of change.
*  One question I get a lot that I imagine you do too is from people who want to know what
*  they can do.
*  You mentioned adaptation as being necessary on the societal level.
*  I think for many years the conventional wisdom was that if you wanted to adapt to a changing
*  world, you should learn how to code, right?
*  That was like the classic advice.
*  It may not be such good advice anymore.
*  Exactly.
*  So now AI systems can code pretty well.
*  The other thing that I think is really interesting about conventional wisdom was that creative
*  work was sort of untouchable by machines.
*  If you were a factory worker, you might get automated out of your job.
*  But if you were an artist or a writer, that was impossible for computers to do.
*  Now we see that's no longer safe.
*  So where is the sort of high ground here?
*  Like where can people focus their energy if they want skills and abilities that AI is
*  not going to be able to replace?
*  My answer is, my meta answer is you always, it's always the right bet to just get good
*  at the most powerful new tools, most capable new tools.
*  And so when computer programming was that, you did want to become a programmer.
*  And now that AI tools like totally change what one person can do, you want to get really
*  good at using AI tools.
*  And so like having a sense for how to work with Tri-GPT and other things, that is the
*  high ground.
*  And that's like, we're not going back.
*  Like that's going to be part of the world.
*  And you can use it in all sorts of ways, but getting fluent at it, I think is really important.
*  I want to challenge that because I think you're partially right in that I think there is an
*  opportunity for people to embrace AI and sort of become more resilient to disruption that
*  way.
*  But I also think if you look back through history, it's not like we learn how to do
*  something new and then the old way just goes away, right?
*  We still make things by hand.
*  There's still an artisanal market.
*  So do you think there's going to be people who just decide, you know what, I don't want
*  to use this stuff.
*  And there's going to be something valuable in their sort of, I don't know, non-AI assisted
*  work.
*  I expect that if we look forward to the future, things that we want to be cheap can get much
*  cheaper.
*  And things that we want to be expensive are going to be astronomically expensive.
*  Like what?
*  Handmade goods, art.
*  And so totally, like there will be a huge premium on things like that.
*  And there will be many people who like really, you know, there's always been like a, even
*  when machine-made products have been much better, there has always been a premium on
*  handmade products.
*  And I'd expect that to intensify.
*  This is also a bit of a curve ball.
*  Very curious to get your thoughts.
*  Where do you come down on the idea of AI romances?
*  Are these net good for society?
*  I don't want one, personally.
*  But it's clear that there is a huge demand for this, right?
*  Yeah.
*  Like I think that, I mean, you know, Replica is building these.
*  They seem like they're doing very well.
*  I would be shocked if this is not a multi-billion dollar company, right?
*  Someone will make a multi-billion dollar company.
*  Yeah, for sure.
*  Do you, like I just personally think we're going to have a big culture war.
*  Like I think Box News is going to be doing segments about the generation lost to AI girlfriends
*  and boyfriends, like at some point within the next few years.
*  But at the same time, you look at all the data on loneliness and it seems like, well, if we
*  make them happy during the day, it could be a net good thing.
*  It's complicated.
*  Yeah.
*  I have misgivings, but I don't, this is not a place where I think I get to like,
*  impose what I think is good on other people.
*  Totally.
*  But it sounds like this is not at the top of your product roadmap, is building the boyfriend API.
*  No.
*  All right.
*  You recently posted on X that you expect AI to be capable of superhuman persuasion
*  well before it is superhuman at general intelligence,
*  which may lead to some very strange outcomes.
*  Can you expand on that?
*  Like what are some things that AI might become very good at persuading us to do?
*  And what are some of those strange outcomes you're worried about?
*  The thing I was thinking about at that moment was the upcoming election.
*  There's a huge focus on the U.S. 2024 election.
*  There's a huge focus on deep fakes and the impact of AI there.
*  And I think that's reasonable to worry about, good to worry about.
*  But we already have some societal antibodies towards people seeing like,
*  like, the world is changing,
*  but we're not seeing any of that.
*  I think that's a good thing.
*  I think that's a good thing.
*  But I think that's a good thing.
*  I think that's a good thing.
*  Because people seeing doctored photos or whatever,
*  and yeah, they're going to get more compelling,
*  it's going to be more, but we kind of know those are there.
*  There's a lot of discussion about that.
*  There's almost no discussion about what are the new things AI can do
*  to influence an election.
*  AI tools can do to influence an election.
*  And one of those is to like carefully,
*  you know, one-on-one persuade individual people.
*  Tailored messages.
*  That's like a new thing that the content farms couldn't quite do.
*  Right.
*  still be pretty harmful.
*  I think so, yeah.
*  I know we are running out of time,
*  but I do want to push us a little bit further
*  into the future than the sort of, I don't know,
*  maybe five-year horizon we've been talking about.
*  If you can imagine a good post-AGI world,
*  a world in which we have reached this threshold,
*  whatever it is, what does that world look like?
*  Does it have a government?
*  Does it have companies?
*  What do people do all day?
*  Like a lot of material abundance.
*  People continue to be very busy,
*  but the way we define work always moves.
*  Like if you, our jobs would not have seemed like real jobs
*  to people several hundred years ago, right?
*  This would have seemed like incredibly silly entertainment.
*  It's important to me, it's important to you.
*  And hopefully it has some value to other people as well.
*  There will be, and the jobs of the future may seem,
*  I hope they seem even sillier to us,
*  but I hope the people get even more fulfillment,
*  I hope society gets even more fulfillment out of them.
*  But everybody can have a really great quality of life,
*  like to a degree that I think we probably
*  just can't imagine now.
*  Of course we'll still have governments.
*  Of course people will still squabble over
*  whatever they squabble over.
*  Less different in all of these ways than someone would think.
*  And then like unbelievably different in terms of what
*  you can get a computer to do for you.
*  One fun thing about becoming a very prominent person
*  in the tech industry as you are,
*  is that people have all kinds of theories about you.
*  One fun one that I heard the other day
*  is that you have a secret Twitter account
*  where you are way less measured and careful.
*  I don't anymore.
*  I did for a while.
*  I decided I just couldn't keep up with the opsec.
*  It's so hard to lead a double life.
*  What was your secret Twitter account?
*  Obviously I can't.
*  I mean, I had a good alt, a lot of people have good alts,
*  but you know.
*  Your name is literally Sam Altman.
*  I mean, it would have been weird if you didn't have one.
*  But I think I just got, yeah,
*  like too well known or something to be doing that.
*  Yeah.
*  Well, and the sort of theory that I heard attached to this
*  was that you are secretly an accelerationist.
*  A person who wants AI to go as fast as possible.
*  And then all this careful diplomacy that you're doing
*  and asking for regulation.
*  This is really just the sort of polite face
*  that you put on for society.
*  But deep down, you just think we should go
*  all gas, no brakes toward the future.
*  No, I certainly don't think all gas, no brakes to the future.
*  But I do think we should go to the future.
*  And that probably is what differentiates me
*  than like most of the AI companies is I think AI is good.
*  Like I don't secretly hate what I do all day.
*  I think it's gonna be awesome.
*  Like I want to see this get built.
*  I want people to benefit from this.
*  So all gas, no brake, certainly not.
*  And I don't even think like most people who say it mean it,
*  but I am a believer that this is a tremendously
*  beneficial technology and that we have got to find a way
*  safely and responsibly to get into the hands of the people,
*  to confront the risk so that we get to enjoy
*  the huge rewards and like maybe relative to the prior
*  of most people who work on AI,
*  that does make me an accelerationist.
*  But compared to those like accelerationist people,
*  I'm clearly not them.
*  So I'm like somewhere,
*  I think you like want the CEO of this company to be somewhere.
*  Your acceleration is adjacent.
*  Your gas and brakes.
*  I believe that this will be the most important
*  and beneficial technology humanity has ever
*  has yet invented.
*  And I also believe that if we're not careful about it,
*  it can be quite disastrous.
*  And so we have to navigate it carefully.
*  Yeah.
*  Sam, thanks so much for coming on Hard Fork.
*  Thank you guys.
*  Hard Fork is produced by Davis Land and Rachel Cohn.
*  We're edited by Jen Poyant.
*  Today's show was engineered by Alyssa Moxley.
*  Original music by Marian Lozano and Dan Powell.
*  Our audience editor is Nelka Lokely.
*  Video production by Ryan Manning and Dylan Bergeson.
*  Special thanks to Paula Schumann, Pui Wing Tam,
*  Kate Lepresty and Jeffrey Miranda.
*  You can email us at hardforknytimes.com.
