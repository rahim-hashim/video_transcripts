# The Sunday Read: ‘Elon Musk’s Appetite for Destruction’
**New York Times Podcasts:** [February 26, 2023](https://rr3---sn-ab5l6nrk.googlevideo.com/videoplayback?expire=1711235620&ei=xA3_ZevYAqy5_9EPsOmAkAM&ip=2603%3A7000%3A3200%3Ade9e%3A1c32%3Adff1%3A8836%3A2e08&id=o-AC4AmmQ9vQd44gDlfEnSXnZxAidFq2pgZwvYFvguBEXJ&itag=139&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&mh=2Y&mm=31%2C29&mn=sn-ab5l6nrk%2Csn-ab5sznly&ms=au%2Crdu&mv=m&mvi=3&pl=37&pcm2=no&initcwndbps=1498750&vprv=1&mime=audio%2Fmp4&gir=yes&clen=17599300&dur=2885.951&lmt=1677409979015294&mt=1711213743&fvip=2&keepalive=yes&c=ANDROID_EMBEDDED_PLAYER&txp=6218224&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cpcm2%2Cvprv%2Cmime%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRQIgJEoJg0rBv52-5w__eueXr0w9BksqYBZa9yHWpQlwrowCIQDiBRoICYhpPfZaSGo4rXWPdzOWjC0dGJRuKa6RwhVJSw%3D%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Cinitcwndbps&lsig=ALClDIEwRQIgdxLtFGzI16gsS1pSMLwQnWa9LcV9ITJ52JqWb8QKw-YCIQDEHZ4fb3wCj-l-ejc0OnFueN-RJlybsma14_TyhJqQHg%3D%3D)
*  Hi, this is Christopher Cox. I'm a contributor to the New York Times magazine, and this week's
*  Sunday read features a recent story of mine about Elon Musk and Tesla. So, Tesla is facing
*  a difficult few months. As I speak, five separate lawsuits against Musk's electric car company
*  are set to go to trial for crashes involving autopilot, one of its self-driving products. In
*  four of those cases, the crashes were fatal. I've been tracking these lawsuits for about
*  a year now, and in the beginning, I was mainly interested in the ethical questions raised
*  by self-driving technology. How do we assign blame when a computer is involved in a car crash?
*  As I learned more about autopilot and has this full self-driving program, I discovered that
*  there's a more basic question that hasn't yet been answered. Are these cars actually ready
*  for the road? I went on test drives with Tesla owners who were convinced the answer is yes,
*  but then the cars nearly steered themselves into serious crashes while I was in the passenger seat.
*  Recently, Tesla announced that it was recalling more than 360,000 vehicles because of problems
*  with its full self-driving system. Eventually, I realized that the key to understanding Tesla's
*  headlong pursuit of self-driving and the resulting court cases could be found in Elon Musk's personal
*  philosophy of risk, which he's revealed in his public statements and in previously unpublished
*  correspondence that I uncovered. The good news is that Musk seems to be motivated by an earnest
*  desire to save lives, but whether the experiment he's running on our roads and highways will produce
*  that outcome, that remains an open question. So here's my article, Elon Musk's appetite for
*  destruction, read by James Patrick Cronin. This was recorded by Autumn. To listen to more
*  stories from the New York Times, the New Yorker, Vanity Fair, the Atlantic, and other publications on
*  your smartphone, download Autumn on the App Store or the Play Store. Visit Autumn. That's
*  AUDM.com for more details.
*  Early on, the software had the regrettable habit of hitting police cruisers.
*  No one knew why, though Tesla's engineers had some good guesses. Stationary objects and flashing
*  lights seemed to trick the AI. The car would be driving along normally, the computer well in control,
*  and suddenly it would veer to the right or left and smash, at least 10 times in just over three years.
*  For a company that depended on an unbounded sense of optimism among investors to maintain
*  its high stock price, Tesla was at one point worth more than Toyota, Honda, Volkswagen, Mercedes,
*  BMW, Ford, and General Motors combined. These crashes might seem like a problem,
*  but to Elon Musk, Tesla's chief executive, they presented an opportunity.
*  Each collision generated data, and with enough data, the company could speed the development of
*  the world's first truly self-driving car. He believed in this vision so strongly that it led him
*  to make wild predictions. My guess as to when we would think it is safe for somebody to essentially
*  fall asleep and wake up at their destination, probably toward the end of next year? Musk said in 2019
*  I would say I am certain of that, that is not a question mark.
*  The future of Tesla may rest on whether drivers knew that they were engaged in this data-gathering
*  experiment, and if so, whether their appetite for risk matched Musk's. I wanted to hear from the
*  victims of some of the more minor accidents, but they tended to fall into two categories,
*  neither of which predisposed them to talk. They either loved Tesla and Musk and didn't want to say
*  anything negative to the press, or they were suing the company and remaining silent on the advice
*  of council. Umar Ali, whose Tesla steered into a highway barrier in 2017, had a different excuse.
*  Put me down as declined interview because I don't want to piss off the richest man in the world.
*  Then I found Dave Key. On May 29th, 2018, Key's 2015 Tesla Model S was driving him home from the
*  dentist in autopilot mode. It was a route that Key had followed countless times before,
*  a two-lane highway leading up into the hills above Laguna Beach, California.
*  But on this trip, while Key was distracted, the car drifted out of its lane and slammed into the
*  back of a parked police SUV, spinning the car around and pushing the SUV up onto the sidewalk.
*  No one was hurt. Key, a 69-year-old former software entrepreneur, took a dispassionate engineer's
*  eye view of his own accident. The problem with stationary objects, I'm sorry, the sounds stupid,
*  is that they don't move, he said. For years, Tesla's artificial intelligence had trouble separating
*  immobile objects from the background. Rather than feeling frustrated that the computer hadn't
*  figured out such a seemingly elementary problem, Key took comfort in learning that there was a reason
*  behind the crash, a known software limitation, rather than some kind of black swan event.
*  Last fall, I asked Key to visit the scene of the accident with me. He said he would do me one
*  better. He would take me there using Tesla's new full self-driving mode, which was still in beta.
*  I told Key that I was surprised he was still driving a Tesla, much less paying extra,
*  FSD now costs $15,000 for new autonomous features. If my car had tried to kill me, I would have
*  switched brands. But in the months and years after his Model S was totalled, he bought three more.
*  We met for breakfast at a cafe in Laguna Beach, about three miles from the crash site.
*  Key was wearing a black V-neck T-shirt, khaki shorts and sandals, Southern California semi-retirement
*  chic. As we walked to our table, he locked the doors of his red 2022 Model S, and the side mirrors
*  folded up like a dog's ears when it's being petted. Key had brought along a four-page memo he
*  drafted for our interview, listing facts about the accident organized under subheadings like
*  Tesla full self-driving technology discussion. He's the sort of man who walks around with a
*  battery of fully formed opinions on life's most important subjects, computers, software,
*  exercise, money, and a willingness to share them. He was particularly concerned that I understand
*  that autopilot and FSD were saving lives. The data shows that their accident rate while on beta is
*  far less than other cars, one bullet point red in 11 point Calibri. Slowing down the FSD beta will
*  result in more accidents and loss of life based on hard statistical data.
*  Accidents like his and even the deadly ones are unfortunate, he argued, but they couldn't distract
*  society from the larger goal of widespread adoption of autonomous vehicles. Key drew an analogy
*  to the coronavirus vaccines, which prevented hundreds of thousands of deaths but also caused
*  rare deaths and injuries from adverse reactions. As a society, he concluded, we choose the path
*  to save the most lives. We finished breakfast and walked to the car. Key had hoped to show off
*  the newest version of FSD, but his system hadn't updated yet. Elon said it would be released at the end
*  of the week, he said. Well, it's Sunday. Musk had been hinting for weeks that the update would be
*  a drastic improvement over FSD 10.13, which had been released over the summer. Because Musk liked to
*  make little jokes out of the names and numbers in his life, the version number would jump to 10.69
*  with this release. The four available Tesla models are S, 3, X, and Y, presumably because that
*  spells the word sexy. Key didn't want to talk about Musk, but the executives' reputational collapse
*  had become impossible to ignore. He was in the middle of his bizarre, on-again, off-again campaign
*  to take over Twitter to the dismay of Tesla loyalists. And though he hadn't yet attacked Anthony
*  Fauci or spread conspiracy theories about Nancy Pelosi's husband or gone on a journalist banning
*  spree on the platform, the question was already suggesting itself. How do you explain Elon Musk?
*  People are flawed, Key said cautiously, before repeating a sentiment that Musk often said about himself.
*  If partisans on both sides hated him, he must be doing something right. No matter what trouble
*  Musk got himself into, Key said he was honest, truthful to his detriment.
*  As we drove, Key compared FSD and the version of autopilot on his 2015 Tesla.
*  Autopilot, he said, was like fancy cruise control, speed, steering, crash avoidance.
*  Though in his case, he said, I guess it didn't do crash avoidance. He had been far more
*  impressed by FSD. It was able to handle just about any situation he threw at it.
*  My only real complaint is it doesn't always select the lane that I would.
*  After a minute, the car warned Key to keep his hands on the wheel and eyes on the road.
*  Tesla now is kind of a nanny about that, he complained. If autopilot was once dangerously
*  permissive of inattentive drivers, allowing them to nod off behind the wheel even,
*  that flaw, like the stationary object bug, had been fixed.
*  Between the steering wheel and the eye tracking, that's just a solved problem, Key said.
*  Soon we were close to the scene of the crash. Scrub covered hills with mountain biking trails,
*  lacing through them rows on either side of us. That was what got key into trouble on the day of
*  the accident. He was looking at a favorite trail and ignoring the road. I looked up to the left and
*  the car went off to the right, he said. I was in this false sense of security. We parked at the
*  spot where he hit the police SUV four years earlier. There was nothing special about the road here.
*  No strange lines, no confusing lane shift, no merge, just a single lane of traffic running along
*  a row of parked cars. Why the Tesla failed at that moment was a mystery. Eventually, Key told FSD
*  to take us back to the cafe. As we started our left turn though, the steering wheel spasimed and
*  the brake pedal juttered. Key muttered a nervous, okay. After another moment, the car pulled halfway
*  across the road and stopped. A line of cars was bearing down on our broadside. Key hesitated a
*  second but then quickly took over and completed the turn. It probably could have then accelerated but
*  I wasn't willing to cut it that close, he said. If he was wrong, of course, there was a good chance
*  that he would have had his second AI-caused accident on the same one mile stretch of road.
*  Three weeks before Key hit the police SUV, Musk wrote an email to Jim Riley, whose son Barrett
*  died after his Tesla crashed while speeding. Musk sent Riley his condolences and the grieving
*  father wrote back to ask whether Tesla's software could be updated to allow an owner to set a
*  maximum speed for the car, along with other restrictions on acceleration, access to the radio and
*  the trunk and distance the car could drive from home. Musk, while sympathetic, replied,
*  if there are a large number of settings, it will be too complex for most people to use. I want to
*  make sure that we get this right, most good for most number of people. It was a stark demonstration
*  of what makes Musk so unusual as a chief executive. First, he reached out directly to someone who was
*  harmed by one of his products, something it's hard to imagine the head of GM or Ford contemplating
*  if only for legal reasons. Indeed, this email was entered into evidence after Riley sued Tesla.
*  And then Musk rebuffed Riley. No vague, I'll look into it or we'll see what we can do.
*  Riley receives a hard no. Like Key, I want to resist Musk's tendency to make every story about him.
*  Tesla is a big car company with thousands of employees. It existed before Elon Musk. It might
*  exist after Elon Musk. But if you want a parsimonious explanation for the challenges the company faces,
*  in the form of the lawsuits, a crashing stock price, and an AI that still seems all too capable of
*  catastrophic failure, you should look to its mercurial, brilliant, soft-mouric chief executive.
*  Perhaps there's no mystery here. Musk is simply a narcissist and every reckless
*  swerve he makes is meant solely to draw the world's attention. He seemed to endorse this theory
*  in a tongue-in-cheek way during a recent deposition. When a lawyer asked him,
*  do you have some kind of unique ability to identify narcissistic sociopaths? And he replied,
*  you mean by looking in the mirror? But what looks like self-obsession and poor impulse control
*  might instead be the fruits of a coherent philosophy, one that Musk has detailed on many occasions.
*  It's there in the email to Riley, the greatest good for the greatest number of people.
*  That dictum, as part of an ad hoc system of utilitarian ethics, can explain all sorts of
*  mystifying decisions that Musk has made, not least his breakneck pursuit of AI, which in the long
*  term he believes will save countless lives. Unfortunately for Musk, the short term comes first,
*  and his company faces a rough few months. In February, the first lawsuit against Tesla for a crash
*  involving autopilot will go to trial. Four more will follow in quick succession. Donald Slavic,
*  who will represent plaintiffs in as many as three of those cases, says that a normal car company
*  would have settled by now. They look at it as a cost of doing business. Musk has vowed to fight it
*  out in court, no matter the dangers this might present for Tesla. The dollars can add up, Slavic said,
*  especially if there's any finding of punitive damages. Slavic sent me one of the complaints he
*  filed against Tesla, which lists prominent autopilot crashes from A to Z, in fact from A to W.
*  In China, a Tesla slammed into the back of a street sweeper. In Florida, a Tesla hit a tractor
*  trailer that was stretched across two lanes of a highway. During a downpour in Indiana, a Tesla
*  Model 3 hydroplaimed off the road and burst into flames. In the Florida Keys, a Model S drove through
*  an intersection and killed a pedestrian. In New York, a Model Y struck a man who was changing his
*  tire on the shoulder of the Long Island Expressway. In Montana, a Tesla steered unexpectedly into a
*  highway barrier. Then the same thing happened in Dallas, and in Mountain View, and in San Jose.
*  The arrival of self-driving vehicles wasn't meant to be like this. Day in, day out, we scare and
*  maim and kill ourselves in cars. In the United States last year, there were around 11 million
*  road accidents, nearly 5 million injuries, and more than 40,000 deaths. Tesla's AI was meant to put
*  an end to this bloodbath. Instead, on average, there is at least one autopilot-related crash in the
*  United States every day, and Tesla is under investigation by the National Highway Traffic Safety
*  Administration. Ever since autopilot was released in October 2015, Musk has encouraged drivers to
*  think of it as more advanced than it was, stating in January 2016 that it was probably better than a
*  human driver. That November, the company released a video of a Tesla navigating the roads of the
*  Bay Area with the disclaimer, the person in the driver's seat is only there for legal reasons.
*  He is not doing anything. The car is driving itself. Musk also rejected the name Copilot in favor of
*  autopilot. The fine print made clear that the technology was for driver assistance only,
*  but that message received a fraction of the attention of Musk's announcements. A large number of
*  drivers seemed genuinely confused about autopilot's capabilities. Tesla also declined to disclose that
*  the car in the 2016 video crashed in the company's parking lot. Slavic's legal complaint doesn't hold
*  back. Tesla's conduct was despicable, and so contemptible that it would be looked down upon and
*  despised by ordinary decent people. The many claims of the pending lawsuits come back to a single
*  theme. Tesla consistently inflated consumer expectations and played down the dangers involved.
*  The cars didn't have sufficient driver monitoring because Musk didn't want drivers to think
*  that the car needed human supervision. Musk in April 2019, if you have a system that's at or
*  below human level reliability then driver monitoring makes sense, but if your system is dramatically
*  better, more reliable than a human, then monitoring does not help much.
*  Drivers weren't warned about problems with automatic braking or uncommanned lane changes.
*  The company would admit to the technology's limitations in the user manual, but publish viral
*  videos of a Tesla driving a complicated route with no human intervention. Musk's ideal customer was
*  someone like Key, willing to accept the blame when something went wrong, but possessing almost
*  limitless faith in the next update. In a deposition, an engineer at Tesla made this all but explicit.
*  We want to let the customer know that number one, you should have confidence in your vehicle.
*  Everything is working just as it should. And secondly, the reason for your accident or reason
*  for your incident always falls back on you. After our failed left turn in Laguna Beach,
*  Key quickly diagnosed the problem. If only the system had upgraded to FSD 10.69, he argued,
*  the car surely would have managed the turn safely. Unfortunately for Musk, not every Tesla owner is like
*  Dave Key. The plaintiffs in the autopilot lawsuits might agree that the AI is improving,
*  but only on the backs of the early adopters and bystanders who might be killed along the way.
*  Online, there's a battle between pro-Musk and anti-musk factions about autopilot and FSD.
*  Reddit has a forum called R-Real Tesla that showcases the most embarrassing AI screw-ups,
*  along with more generic complaints, squeaky steering wheels, leaky roofs, haywire electronics,
*  noisy cabins, stiff suspensions, wrinkled leather seats, broken door handles.
*  The Musk stands tend to sequester themselves in R-Tesla Motors, where they post Tesla sightings,
*  cheer on the company's latest factory openings, and await the next big announcement from the boss.
*  I found David Alfred on YouTube, where he posted a video called Tesla Full Self-Driving Running a Red Light.
*  In it, we see the view through the windshield as Alfred's car approaches an intersection with a
*  left turn lane that has a dedicated traffic signal. With a few hundred yards remaining,
*  the light shifts from green to red. But the car doesn't stop. Instead, it rolls into the intersection,
*  where it's on track to collide with oncoming traffic, until Alfred takes over.
*  In the comments, Tesla fans grow angry with Alfred for posting the video, but he pushes back.
*  How does it help put pressure on Tesla to improve their systems if you are scared to post their faults?
*  Replying to one comment, he writes that FSD is unethical in the context they are using it.
*  When I called Alfred, I was expecting someone suited for R-Real Tesla,
*  but he ended up having more of an R-Tesla Motors vibe. He told me that he would be willing to take
*  me to the site of his video and demonstrate the failure, but first I had to make a promise.
*  The only thing I ask is try not to put me in a bad light toward Tesla, he said.
*  I don't want anybody to think that I hate the company or whatnot, because I'm a very, very big supporter of them.
*  Alfred lives in Fresno, California, and before I went to meet him one day last fall,
*  he told me some exciting news. He had just received the FSD 10.69 update.
*  Our drive would be his first attempt to navigate the intersection from the YouTube video with the new
*  system. The morning I met him, he was wearing a black t-shirt that showed off his tattoos,
*  black sunglasses, and faded black jeans with holes in the knees. Hollywood would typecast him as a
*  white hat hacker, and indeed he's a software guy like Key. He is a product engineer for a Bay Area
*  Tech Company. His white 2020 Tesla Model 3 had a magnetic bumper sticker he found on Etsy,
*  caution, full self-driving testing in progress. He said he drives an FSD mode 90% of the time,
*  so his car is always acting a bit strange. The sticker helped keep some of the honking from
*  other cars at Bay. He seemed to be, like Key, an ideal FSD beta tester, interested in the software,
*  alert to its flaws, dogged in his accumulation of autonomous miles. I climbed into the passenger
*  seat and Alfred punched in our first destination, a spot a few blocks away in downtown Fresno.
*  We were lucky it was overcast, he said, because the car behaved well in these conditions.
*  On days when it was sunny out and there was a lot of glare, the car could be moody,
*  and when it was foggy, and it was often foggy in Fresno, it freaks out.
*  After a few minutes, we approached a crosswalk, just as two parents pulling a child in a wagon began
*  across. A screen next to the steering wheel showed that the AI had registered the two pedestrians,
*  but not the wagon. Alfred said he was hovering his foot over the brake, but the car stopped on its own.
*  After the wagon came a woman in a wheelchair, the car stayed put. Alfred told me that the automotive
*  jargon for anyone on the street who is not in a car or a truck is a VRU, a vulnerable road user.
*  And it's true, pedestrians and cyclists and children and strollers and women and wheelchairs,
*  they are so fragile compared with these giant machines we've stuffed into our cities and
*  onto our highways. One wrong move, and a car will crush them.
*  We turned on to Van S Avenue, which cuts through downtown. It had been newly paved and instead
*  of lines on the street, there were little yellow tabs indicating where the lines would eventually go.
*  The Tesla hated this and dodged wordedly right and left, looking for something to anchor it.
*  There were no other cars around, so Alfred let it get that out of its system and eventually find
*  a lane line to follow. You build a tolerance to the risks it takes, he said. Yes, it's
*  swirving all over the place, but I know it's not going to crash into something.
*  Still, the experience of the beta had changed the way he approached his own work.
*  It's actually made me, as a software developer, more hesitant to put my software in the hands of
*  people before it's fully ready, he said, even though it's not dangerous.
*  Seconds later, we drove through an intersection as two VRUs, a man walking a dog, entered the
*  crosswalk. They were a safe distance away, but the dog started to strain against its leash in our
*  direction. Alfred and I knew that the pet wasn't in peril because the leash would stop it, but all
*  the Tesla saw was a dog about to jump in front of us and it came to an abrupt stop.
*  It was a good outcome, all things considered, no injuries to any life form, but it was far from
*  a seamless self-driving experience. Alfred nudged the steering wheel just often enough that the car
*  never warned him to pay attention. He didn't mind the strict driver monitoring. He never tired of
*  studying the car's behavior so he was never tempted to tune out. Still, he knew people who abused
*  the system. One driver tied an ankle weight to the steering wheel to kick back and do whatever
*  during long road trips. I know a couple of people with Teslas that have FSD beta, he said,
*  and they have it to drink and drive instead of having to call an Uber.
*  We left downtown and got on the highway, headed toward an area northeast of the city called
*  Clovis, where the tricky intersection was. Alfred pulled up his FSD settings. His default driver mode
*  was average, but he said he has found that the two other options, chill and assertive,
*  aren't much different. The car is just really aggressive anyway. For highway driving though,
*  he had the car set to something called Mad Max Mode, which meant it would overtake any vehicle
*  in front of him if it was going even a few miles per hour slower than his preferred speed.
*  We exited the highway and quickly came to a nod of cars, something had gone wrong with the
*  traffic light, which was flashing red, and drivers in all four directions across eight lanes had to
*  figure out when to go and when to yield. The choreography here was delicate. There were too many cars
*  to interviewee without some allowances being made for mercy and confusion and expediency.
*  Among the humans, there was a good deal of waving others on and attempted eye contact to see
*  whether someone was going to yield or not. We crept toward the intersection, car by car,
*  until it was our turn. If we were expecting nuance, there was none. Once we had come to a complete
*  stop, the Tesla accelerated quickly, cutting off one car turning across us and veering around another.
*  It was not so much inhuman as the behavior of a human who was determined to be a jerk.
*  That was bad, Alfred said. Normally, I would disengage once it makes a mistake like that.
*  He clicked a button to send a snapshot of the incident to Tesla.
*  Later, at a four-way stop, the car was too cautious. It waited too long and the other two cars
*  that the intersection drove off before we did. We talked about the old saying about safe driving.
*  Don't be nice, be predictable. For a computer, Tesla's AI was surprisingly erratic.
*  It's not nice or predictable, Alfred said. A few miles down the road, we reached the intersection
*  from the video. A left turn onto East Shepherd Avenue from State Route 168.
*  The traffic light sits right at the point where the city's newest developments end and open land
*  begins. If we drove straight, we would immediately find ourselves surrounded by sagebrush
*  on the way up into the Sierra. To replicate the error that Alfred uncovered, we needed to approach
*  the intersection with a red left turn arrow and a green light to continue straight.
*  On our first pass, the arrow turned green at the last second. On the second pass,
*  though, on an empty road, the timing was right. A red for our turn and green for everyone else.
*  As we got closer, the car moved into the turning lane and started to slow.
*  It seized the red, I said. No, Alfred said. It always slows down a little here before plowing through.
*  But this time it kept slowing. Alfred couldn't believe it. It's still going to run the light, he said.
*  But he was wrong. We came to a tidy stop right at the line. Alfred was shocked.
*  They fixed it, he said. That one I've been giving them an issue about for two years.
*  We waited patiently until the light turned green and the Tesla drove smoothly onto Shepherd Avenue.
*  No problem. It was as clear a demonstration of Musk's hypothesis as one could hope for.
*  There was a situation that kept stumping the AI until after enough data had been collected
*  by dedicated drivers like Alfred, the neural net figured it out. Repeat this risk-reward conversion
*  X number of times and maybe Tesla will solve self-driving. Maybe even next year.
*  On the drive back to the center of Fresno, Alfred was buoyant, delighted with the possibility that he
*  had changed the Tesla world for the better. I asked him whether the FSD 10.69 release met the hype
*  that preceded it. To be honest, yeah, I think so, he said. He was even more enthusiastic about the
*  version of FSD released in December, which he described as nearly flawless. A few minutes later,
*  we reached a rundown part of town. Alfred said that in general Tesla's AI does better in
*  hiring come areas, maybe because those areas have more Tesla owners in them. Are there data biases
*  for hiring come areas because that's where the Teslas are? He wondered. We approached an intersection
*  and tried to make a left in what turned out to be a repeat of the Laguna Beach scenario. The Tesla
*  started creeping out, trying to get a clearer look at the cars coming from our left. It
*  inched forward, inched forward, until once again we were fully in the lane of traffic. There was
*  nothing stopping the Tesla from accelerating and completing the turn, but instead it just sat there.
*  At the same time, a tricked out Honda Accord sped toward us, about three seconds away from
*  hitting the driver's side door. Alfred quickly took over and punched the accelerator, and we escaped
*  safely. This time he didn't say anything. It was a rough ride home from there. At a standard left
*  turn at a traffic light, the system freaked out and tried to go right. Alfred had to take over.
*  And then as we approached a clover leaf on ramp to the highway, the car started to accelerate.
*  To stay on the ramp, we needed to make an arcing right turn. In front of us was a steep drop off
*  into a construction site with no guard rails. The car showed no sign of turning. We crossed a
*  solid white line, milliseconds away from jumping off the road, when at last the wheel jerked sharply
*  to the right and we hugged the road again. This time, FSD had corrected itself, but if it hadn't,
*  the crash would have surely killed us.
*  Peter Teal, Musk's former business partner at PayPal once said that if he wrote a book,
*  the chapter about Musk would be called, the man who knew nothing about risk.
*  But that's a misunderstanding of Musk's attitude. If you parse his statements, he presents himself as
*  a man who simply embraces astonishing amounts of present-day risk in the rational assumption of
*  future gains. Musk's clearest articulation of his philosophy has come, of course, on Twitter.
*  We should take the set of actions that maximized total public happiness he wrote to one user who asked
*  him how to save the planet. In August, he called the writings of William McCaskill, a Scottish
*  utilitarian ethicist, a close match for my philosophy. McCaskill, notably, was also the
*  intellectual muse of Sam Bankman Fried, though he cut ties with him after the FTX scandal came to light.
*  Musk's embrace of risk has produced true breakthroughs. SpaceX can land reusable rockets on
*  remote controlled landing pads in the ocean. Starlink is providing internet service to Ukrainians on
*  the front lines, open AI creeps ever closer to passing the turtling test. As for Tesla,
*  even Musk's harshest critics, and I talked to many of them while reporting this article,
*  would pause unbidden to give him credit for creating the now robust market in electric vehicles in
*  the United States and around the world. And yet, as Robert Lowell wrote, no rocket goes as far
*  as tray as man. In recent months, as the outrageous Twitter and elsewhere began to multiply,
*  Musk seemed determined to squander much of the goodwill he had built up over his career.
*  I asked Slavic, the plaintiff's attorney, whether the recent shift in public sentiment against Musk
*  made his job in the courtroom any easier. I think at least there are more people who are skeptical
*  of his judgment at this point than were before, he said. If I were on the other side, I'd be worried
*  about it. Some of Musk's most questionable decisions, though, begin to make sense if seen as a result
*  of a blunt utilitarian calculus. Last month, Reuters reported that Neuralink, Musk's medical device
*  company, had caused the needless deaths of dozens of laboratory animals through rushed experiments.
*  Internal messages from Musk made it clear that the urgency came from the top.
*  We are simply not moving fast enough, he wrote, it is driving me nuts. The cost-benefit analysis
*  must have seemed clear to him. Neuralink had the potential to cure paralysis, he believed,
*  which would improve the lives of millions of future humans. The suffering of a smaller number
*  of animals was worth it. This form of crude long-termism in which the sheer size of future
*  generations gives them added ethical weight, even shows up in Musk's statements about buying Twitter.
*  He called Twitter a digital town square that was responsible for nothing less than preventing a
*  new American civil war. I didn't do it to make more money, he wrote. I did it to try to help humanity,
*  whom I love. Autopilot and FSD represent the culmination of this approach. The overarching
*  goal of Tesla engineering, Musk wrote, is maximize area under user happiness curve.
*  Unlike with Twitter or even Neuralink, people were dying as a result of his decisions.
*  But no matter. In 2019, in a testy exchange of email with the activist investor and
*  steadfast Tesla critic, Aaron Greenspan, Musk bristled that the suggestion that Autopilot was
*  anything other than life-saving technology. The data is unequivocal that Autopilot is safer
*  than human driving by a significant margin, he wrote. It is unethical and false of you to claim
*  otherwise. In doing so, you are endangering the public. I wanted to ask Musk to elaborate on his
*  philosophy of risk, but he didn't reply to my interview requests. So instead, I spoke with
*  Peter Singer, a prominent utilitarian philosopher, to sort through some of the ethical issues involved.
*  Was Musk right when he claimed that anything that delays the development and adoption of autonomous
*  vehicles was inherently unethical? I think he has a point, Singer said, if he is right about the facts.
*  Musk rarely talks about Autopilot or FSD without mentioning how superior it is to a human driver.
*  At a shareholders meeting in August, he said that Tesla was
*  solving a very important part of AI, and one that can ultimately save millions of lives and
*  prevent tens of millions of serious injuries by driving just an order of magnitude safer than
*  people. Musk does have data to back this up. Starting in 2018, Tesla has released quarterly
*  safety reports to the public, which show a consistent advantage to using Autopilot.
*  The most recent one from late 2022 said that Tesla's with Autopilot engaged were one-tenth
*  as likely to crash as a regular car. That is the argument that Tesla has to make to the public
*  and to juries this spring. In the words of the company's safety report, while no car can prevent
*  all accidents, we work every day to try to make them much less likely to occur. Autopilot may
*  cause a crash, WW times, but without that technology, we'd be at...
*  Singer told me that even if Autopilot and human drivers were equally deadly, we should prefer
*  the AI, provided that the next software update, based on data from crash reports and near misses,
*  would make the system even safer. That's a little bit like surgeons doing experimental surgery,
*  he said. Probably the first few times they do the surgery, they're going to lose patience,
*  but the argument for that is they will save more patients in the long run.
*  It was important, however, Singer added that the surgeons get the informed consent of the patients.
*  Does Tesla have the informed consent of its drivers? The answer might be different for
*  different car owners. It would probably be different for Dave Key in 2018 than it is in 2022.
*  But most customers are not aware of how flawed Autopilot is, said Philip Copman, the author of
*  How Safe is Safe enough, measuring and predicting autonomous vehicle safety. The cars keep making
*  really crazy, crazy surprising mistakes, he said. Tesla's practice of using untrained civilians as
*  test drivers for an immature technology is really egregious. Copman also objects to musks,
*  supposed facts. One obvious problem with the data the company puts out in its quarterly safety
*  report is that it directly compares Autopilot miles, which are mainly driven on limited access
*  highways with all vehicle miles. You're using Autopilot on the Safe miles, Copman said. So of course,
*  it looks great, and then you're comparing it to not Autopilot on the hard miles.
*  In the third quarter of 2022, Tesla claimed that there was one crash for every 6.26 million
*  miles driven using Autopilot. Indeed, almost 10 times better than the US baseline of one crash
*  for every 652,000 miles. Crashes, however, are far more likely on surface streets than on the highway.
*  One study from the Pennsylvania Department of Transportation showed that crashes were five times
*  as common on local roads as on turnpikes. When comparing Autopilot numbers to highway numbers,
*  Tesla's advantage drops significantly. Tesla's safety claims look even shakier when you try to
*  control for the age of the car and the age of the driver. Most Tesla owners are middle aged or
*  older, which eliminates one risky pool of drivers, teenagers, and simply having a new car
*  decreases your chance of an accident significantly. It's even possible that the number of Tesla's in
*  California, with its generally mild dry weather, has skewed the numbers in its favor. An independent
*  study that tried to correct for some of these biases suggested that Tesla's crashed just as
*  often one Autopilot was on as when it was off. That's always been a problem for utilitarians,
*  singer told me. Because it doesn't have strict moral rules, people might think they can get away
*  with doing the sums in ways that suit their purposes. Utilitarian thinking has led individuals to
*  perform acts of breathtaking virtue, but putting this ethical framework in the hands of an industrialist
*  presents certain dangers. True utilitarianism requires a careful balancing of all harms and
*  benefits in the present and the future, with the patience to do this assessment and the patience
*  to refrain from acting if the amount of suffering and death caused by pushing forward wasn't clear.
*  Musk is using utilitarianism in a more limited way, arguing that as long as he's sure something
*  will have a net benefit, he's permitted to do it right now. In the past two decades,
*  Musk has somehow maneuvered himself into running multiple companies where he can plausibly
*  claim to be working to preserve the future of humanity. SpaceX can't just deliver satellites
*  into low orbit, it's also going to send us to Mars. Tesla can't just build a solid electric car,
*  it's going to solve the problem of self-driving. Twitter can't just be one more place where we
*  gather to argue, it's one of the props holding up civilization. With the stakes suitably raised,
*  all sorts of questionable behavior begin to look almost reasonable.
*  True believers, the novelist Jeanette Winterson, wrote, would rather see government's topple and
*  history rewritten than scuff the cover of their faith. Musk seems unshakable in his conviction that
*  his approach is right, but for all his urgency, he still might lose the AI race.
*  Right now, in San Francisco, and Austin, Texas, and coming soon to cities all over the world,
*  you can hail a robot taxi operated by cruise or Weimo. If there's one moment in time where we go
*  from fiction to reality, it's now. Sebastian Thrunn, who founded Google's self-driving car team, told me.
*  I didn't say this last year, by the way, he added. Thrunn was no R-real Tesla lurker, he was on
*  his fifth Tesla, and he said he admired the company. What Tesla has is really beautiful,
*  they have a fleet of vehicles in the field. But at this point, Tesla's competitors are closer to
*  achieving full self-driving than any vehicle equipped with FSD. In recent months, Musk has stopped
*  promising that autonomous Teslas are just around the corner. I thought the self-driving problem would
*  be hard, he said, but it was harder than I thought. It's not like I thought it'd be easy,
*  I thought it would be very hard, but it was actually way harder than even that.
*  On December 29, 2019, the same day, a Tesla in Indiana got into a deadly crash with a parked
*  fire truck. An off-duty chauffeur named Kevin George Aziz Riyadh was driving his grey 2016
*  Tesla Model S down the Gardina-freeway in suburban Los Angeles. It had been a long drive back from
*  a visit to Orange County, and Riyadh had autopilot turned on. Shortly after midnight, the car passed
*  under a giant sign that said, end-freeway signal ahead in flashing yellow lights. The autopilot kept
*  Riyadh's Tesla at a steady speed as it approached the stoplight that marked the end of the freeway
*  and the beginning of Artigia Boulevard. According to a witness, the light was red, but the car drove
*  straight through the intersection, striking a Honda Civic. Riyadh had only minor injuries,
*  but the two people in the Civic, Hilberto Alcas Arlopez and Maria Guadalupe Niewes, died at the scene.
*  Their families said that they were on a first date.
*  Who was responsible for this accident? State officials have charged Riyadh with manslaughter and
*  planned to prosecute him as if he were the sole actor behind the two deaths. The victim's families,
*  meanwhile, have filed civil suits against both Riyadh and Tesla. Depending on the outcomes of the
*  criminal and civil cases, the autopilot system could be judged in effect legally responsible,
*  not legally responsible, or both simultaneously. Not long ago, I went to see the spot where Riyadh's
*  Tesla reportedly ran the red light. I had rented a Tesla for the day to find out first hand,
*  finally, what it felt like to drive with autopilot in control. I drove east on surface streets until
*  I reached a ramp where I could merge on the state route 91, the Gardina Freeway. It was late at night
*  when Riyadh crashed. I was taking my ride in the middle of the day. As soon as I was on the highway,
*  I engaged autopilot and the car took over. I had the road mostly to myself. This Tesla was
*  programmed to go 15% above the speed limit whenever autopilot was in use and the car accelerated
*  quickly to 74 miles per hour, which was Riyadh's speed when he crashed. Were his autopilot speed
*  settings the same? The car did a good job of staying in its lane, better than any other traffic
*  aware cruise control I've used. I tried taking my hands off the wheel but the Tesla beeped at me after
*  a few seconds. As I got closer to the crash site, I passed under the giant end-freeway signal ahead
*  sign. The autopilot drove on blively. After another 500 feet, the same sign appeared again, flashing
*  urgently. There was only a few hundred feet of divided highway left and then route 91 turned
*  into a surface street right at the intersection with Vermont Avenue. I hovered my foot over the break.
*  What was I doing? Seeing if the car truly would just blaze through a red light? Of course it would.
*  I suppose I was trying to imagine how easy it would be to do such a thing. At the end of a long night,
*  on a road empty of cars, with something called autopilot in control, my guess is that Riyadh didn't
*  even notice that he had left the highway. The car sped under the warning lights 74 miles an hour.
*  The crash data shows that before the Tesla hit Lopez and Yves, the brakes hadn't been used for
*  six minutes. My Tesla bore down on the intersection. I got closer and closer to the light. No brakes.
*  And then, just before I was about to take over, a pickup truck swung out of the far right lane
*  and cut me off. The Tesla sensed it immediately and break tart. If only that truck, as undeniable as any
*  giant chunk of hardware can be, had been there in December 2019. Lopez and Yves would still be alive.
