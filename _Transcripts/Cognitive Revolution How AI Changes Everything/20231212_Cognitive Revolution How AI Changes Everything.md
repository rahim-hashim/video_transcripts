---
Date Generated: April 02, 2024
Transcription Model: whisper medium 20231117
Length: 8187s
Video Keywords: []
Video Views: 15156
Video Rating: None
---

# Balaji Srinivasan on Polytheistic AI, Human-AI Symbiosis, and Prospects for AI Control
**Cognitive Revolution "How AI Changes Everything":** [December 12, 2023](https://www.youtube.com/watch?v=-9ROlCeB5FQ)
*  They don't care about AI safety. What they care about is AI control. Do I think we eventually
*  get to a configuration like that? Maybe where you have an AI brain is at the center of civilization
*  and it's coordinating all the people around it. And every civilization that makes it is capable
*  of crowdfunding and operating its own AI. You know, our background culture influences things
*  in ways we don't even think about. So much of the paperclip thinking is like a vengeful God will
*  turn you into pillars of salt. The polytheistic model of many gods as opposed to one God is
*  we're all going to have our own AI gods and there'll be war of the gods. Man-machine symbiosis is not
*  some new thing. It's actually the old thing that broke us away from other primate lineages that
*  weren't using tools. Then the question is, what's the next step? Which is AI is amplified intelligence.
*  It is that the AI human fusion means there's another 20 Elon Musk's or whatever the number is.
*  That's good. Hello and welcome to the cognitive revolution where we interview visionary researchers,
*  entrepreneurs and builders working on the frontier of artificial intelligence.
*  Each week we'll explore their revolutionary ideas and together we'll build a picture of
*  how AI technology will transform work, life and society in the coming years. I'm Nathan Labenz
*  joined by my co-host Eric Torenberg. Hello and welcome back to the cognitive revolution.
*  Today my guest is Balaji Srinivasan. In tech circles Balaji needs no introduction but for
*  folks from other backgrounds Balaji is a serial startup entrepreneur who's founded and ultimately
*  sold highly dissimilar technology companies including Teleport which helped people move
*  around the world to realize opportunities, Council which provided genetic testing for couples planning
*  to have children and earn.com a paid email on the blockchain startup which ultimately sold to
*  Coinbase where Balaji became CTO. Along the way he's also taught statistics at Stanford and been
*  a general partner at Andreessen Horowitz as well. Today as an independent thinker, investor and
*  author of the network state Balaji is extremely prolific in both text and audio formats and as
*  you'll hear whether for the first time or the 50th he is an incredibly creative thinker who
*  relentlessly develops and iterates on new paradigms for understanding a fast-changing
*  often chaotic world. He's also a very associative and interdisciplinary thinker who constantly adds
*  dimensions to any analysis. Such horsepower can be hard for a podcast host to rein in but I
*  personally find it extremely stimulating. So in this conversation I tried to strike a balance
*  between letting Balaji go off as only he can do, contributing what I hope are worthy versions of
*  core AI safety arguments and supporting results from recent research and occasionally steering
*  us back toward what I see as the most critical questions for the AI big picture. If there's one
*  area where Balaji and I disagree most consequentially it's on the question of how independent AI systems
*  are likely to become over the next five to ten years. Balaji thinks that AI systems need to be
*  at least symbiotic with humans because physical computers can't replicate themselves without human
*  support. Well I think there's at least a significant chance that we get AIs that are so independent of
*  humans that their behaviors and interactions become the primary drivers of world history.
*  In Balaji's own words he does expect massive economic and social disruption from AI but doesn't
*  think that quote unquote can't turn the killer AI off scenarios are likely at least for a long
*  while due to factors like the existence of adversarial inputs that can paralyze AIs particularly
*  those with open model weights. The observation that even decentralized programs like the Bitcoin
*  network can't run independently without continuous human support. And he also thinks that
*  and the premise that to control the physical world AIs will need to direct either large numbers of
*  humans who are notoriously difficult to control or highly agile robots which don't yet exist.
*  With all that in mind in the first half of this conversation you'll hear Balaji's analysis of the
*  likely impact of AI in a world where powerful AI systems do come to exist but humans retain control
*  resulting in a human AI symbiosis similar to how believers relate to their gods or citizens relate
*  to their governments. Then in the second half we really dig into the question of just how confident
*  we should be that AI won't prove to be even more revolutionary than that. After more than two hours
*  of recording I was the one who ran out of time today but I really enjoyed this conversation with
*  Balaji. He is as good-natured and curious as he is opinionated and we have continued to exchange
*  links and arguments offline such that I hope we'll have another episode to share with you
*  in the future as well. As always if you're enjoying the show we'd ask that you take a
*  moment to share it with a friend and with that here's part one of an all angles look
*  at how AI will shape the future with Balaji Srinivasan. Balaji Srinivasan welcome to the
*  Cognitive Revolution. All right I feel welcome. Well we've got a ton to talk about you know
*  obviously you bring a lot of different perspectives to everything that you think about and work on
*  and today I want to just try to muster all those different perspectives onto this you know what I
*  see is really the defining question of our time which is like what's up with AI and you know how
*  is it going to turn out. I thought maybe for starters I would love to just get your baseline
*  kind of table setting on how much more AI progress do you expect us to see over the next few years
*  like how powerful are AI systems going to become in again kind of a relatively short timeline and
*  then maybe if you want to take a you know bigger stab at it you could answer that same question
*  for a longer timeline like the rest of our lives or whatever. Sure let me give an abstract answer
*  then let me give a technical answer. You know if you look at evolution we've seen something as
*  complex as flight evolve independently in birds bats and bees and even intelligence we've seen
*  fairly high intelligence in dolphins in whales in octopuses you know octopus in particular can
*  do like tool manipulation they've got things that are a lot like hands you know with tentacles
*  and so that indicates that it is plausible that you could have multiple pathways to intelligence
*  whether you know we have carbon based intelligence so we could have silicon based intelligence that
*  just has a totally different form where the fundamental thing is an electromagnetic wave
*  and data storage as opposed to you know DNA and so on right so that's like a plausibility argument
*  in terms of evolution is being so resourceful that it's invented really complicated things
*  in different ways okay. Then in terms of the technical point I think as of like right now I
*  should probably date it as like December 11, 2023 because this field moves so fast right
*  my view is and maybe you'll have a different view is that the breakthroughs that are really needed
*  for something that's like true artificial intelligence that is is human independent
*  right maybe the next step after the Turing test I've got an article that you know we're writing
*  called the Turing thresholds which tries to generalize the Turing test to like the Kardashev
*  scale you know have you got energy thresholds like what are useful scales beyond that and right now
*  I think that what we call AI is absolutely amazing for environments that are not time
*  varying or rule varying and what I mean by that is so you kind of have let's say two large schools
*  of AI and obviously there's overlap in terms of the personnel and so on but there's like the deep
*  mind school which has gotten less press recently but got more press you know a few years ago
*  and that is game playing right it is you know superhuman playing about of go with alf go it is
*  you know all the video game stuff they've done where they learn at the pixel level and they don't
*  they just teach the very basic rules and it figures it out from there and it's also you know the
*  protein folding stuff and what have you right but in general I think they're known for reinforcement
*  learning and and those kinds of approaches I mean they're good at a lot of things but that's what
*  I think deep mind is known for of course they put out this new model recently the the Gemini model
*  so I'm not saying that they're not good at everything but that's just kind of what they're
*  maybe most known for and then you have the open AI chat gbt school of generative AI and it include
*  stable diffusion and just as a pioneer even if you know they're not I don't know how much they're
*  used right now but basically you know you have the diffusion models for images and you have
*  large language models and now you have the multimodals that integrate them and so the
*  difference I think with these is the the reinforcement learning approaches are based on
*  an assumption of static rules like the rules of chess the rules that go the rules of a video game
*  are not changing with time they are discoverable they're like the laws of physics and similarly
*  like the body of language where you're learning it English is not rapidly time varying that is
*  say the rules of grammar that are implicit aren't changing the meanings of words aren't changing very
*  rapidly you can argue they're changing over the span of decades or centuries but not extremely
*  rapidly right so therefore when you generate a new result training data from five years ago for
*  English is actually still fairly valuable and the same input roughly gives the same output now of
*  course there are facts that change with time like who is the the ruler of England right the queen
*  of England is passed away now it's the king of England right just facts that change with time
*  but I think more fundamentally is when there's rules that change with time you know you have
*  for example changes in law in countries right but most interestingly perhaps changes in markets
*  because the same input does not give the same output in a market if you try that then what
*  will happen is adversarial behavior on the other side and once people see it enough times they'll
*  see your strategy and they're going to trade against you on that right and I can get to other
*  technical examples on that but I think and probably people in the space are aware of this but I think
*  that is the true frontier is dealing with time varying rule varying systems as opposed to systems
*  where the implicit rules are static let me pause there yeah I think that makes sense I think the
*  you know in the very practical you know just trying to get as Zvi calls it mundane utility from AI
*  that is often kind of cashed out to AI is good at tasks but it's not good at whole jobs you know
*  it can handle these kind of small things where you can define you know what good looks like and
*  tell it exactly what to do but in the sort of broader context of you know handling things that
*  come up as they come up it's definitely not there yet and I agree that there's likely to be some
*  synthesis you know which is a kind of the the subject of all the q star rumors recently I would
*  say is kind of the the prospect that there could be already you know within the the labs a beginning
*  of a synthesis between the I kind of think of it as like harder edged reinforcement learning systems
*  you know that are like small efficient and deadly versus the like language model systems that are
*  like kind of slow and soft and you know but have a sense of our values which is really a remarkable
*  accomplishment that that they're able to have even you know an approximation of our values that seems
*  like reasonably good so yeah I think I agree with that framing but I guess I would you know still
*  wonder like how far do you think this goes in the near term because I have a lot of uncertainty
*  about that and I think the field has a lot of uncertainty you'll hear people say well you know
*  it's never going to get smarter than its training data you know it'll kind of level out where humans
*  are but we certainly don't see that in the reinforcement learning side right like once
*  it usually don't take too long at human level of these games and then it like blows past human
*  level interestingly you do still see some adversarial vulnerability like there's a great
*  paper from the team at far ai and I'm planning to have Adam cleave the head of that organization
*  on soon to talk about that and other things where they found like a basically a hack where a really
*  simple but unexpected attack on the superhuman go player can defeat it so you do have these like very
*  interesting vulnerabilities or kind of lack of adversarial robustness it's still a kind of
*  wondering like where do you think that leaves us in say a three to five years time obviously huge
*  and uncertainty on that it's really hard to predict something like this just to your point
*  generative ai is generic ai right it's like generically smart but doesn't have specific
*  intelligence or creativity or facts and as you're saying just like we have you know adversarial
*  images back in fool programs that are trained on a certain set of data and they just give some weird
*  you know pattern that looks like a giraffe but the algorithm thinks it's a dog you can do the same
*  thing for game playing and you can have out of sample input that can beat you know these these
*  very sophisticated reinforcement learners and an interesting question is whether that is a fundamental
*  thing or whether it is a work around able thing and you'd think it was work around able you know
*  because there's probably some robustification because these pictures look like giraffes
*  you know and yet they're being recognized as dogs so there's you would think that the right proximity
*  metric would group it with giraffes you know but maybe there's some i don't know maybe there's some
*  result there my intuition would be we can probably robustify the system so that they are
*  less vulnerable to adversarial input but if we can't then that leads us in a totally different
*  direction where these systems are fragile in a fundamental way so that's one big branch point
*  is how fragile these systems are because if they're fragile in a certain way then it's almost like
*  you can always kill them which is kind of good right in a sense that there's that you know almost
*  like the you know the 50 iq 100 iq 150 iq thing like the the meme yeah the meme right so the 50
*  iq guys like these machines will never be as creative as humans or whatever 100 iqs look at
*  all the things they can do the 150 iq is like well there's some like equivalent equivalent result
*  you know that's like some impossibility proof that shows that we the dimensional space of a
*  giraffe is too high and we can't actually learn what a true giraffe i don't think that's true
*  but maybe it's true from the perspective of how these learners are working because my understanding
*  is people have been trying and i mean i'm not on the cutting edge of this so you know maybe
*  someone but my understanding is we haven't yet been able to robustify these models against
*  adversarial input am i wrong about that yeah that's definitely right hey we'll continue our
*  interview in a moment after a word from our sponsors real quick what's the easiest choice
*  you can make taking the window instead of the middle seat outsourcing business tasks that you
*  absolutely hate what about selling with shopify shopify is the global commerce platform that helps
*  you sell at every stage of your business shopify powers 10 of all e-commerce in the u.s and shopify
*  is the global force behind all birds rothies and brooklyn and millions of other entrepreneurs of
*  every size across 175 countries whether you're selling security systems or marketing memory
*  modules shopify helps you sell everywhere from their all-in-one e-commerce platform to their
*  in-person p.o.s system wherever and whatever you're selling shopify's got you covered i've used it in
*  the past at the companies i've founded and when we launch merch here at turpentine shopify will be
*  our go-to shopify helps turn browsers into buyers with the internet's best converting checkout up to
*  36 percent better compared to other leading commerce platforms and shopify helps you sell more with
*  less effort thanks to shopify magic your ai-powered all-star with shopify magic whip up captivating
*  content that converts from blog posts to product descriptions generate instant faq answers pick the
*  perfect email send time plus shopify magic is free for every shopify seller businesses that grow grow
*  with shopify sign up for a one dollar per month trial period at shopify.com slash cognitive go to
*  shopify.com slash cognitive now to grow your business no matter what stage you're in shopify.com
*  slash cognitive
*  omni-key uses generative ai to enable you to launch hundreds of thousands of ad iterations
*  that actually work customized across all platforms with a click of a button i believe in omni-key so
*  much that i invested in it and i recommend you use it too use cog rev to get a 10 discount
*  there's no single architecture as far as i know that is demonstrably robust and on the contrary
*  with language models there's a we did a whole episode on the universal jailbreak
*  where especially if you have access to the weights not to change the weights but just to kind of
*  probe around in the weights then you have a really hard time you know guaranteeing any sort of
*  robustness the conjecture is see for humans you can't like mirror their brain and analyze it
*  okay but we have enough humans that we've got things like optical illusions stuff like that
*  that works on enough humans and our brains aren't changing enough right a conjecture is if you had
*  as you said open weights open weights mean safety because if you have open weights you can always
*  reverse engineer adversarial input and then you can always break the system conjecture yeah there's
*  i that's again with adam from far ai i'm really interested to get get into that because they are
*  starting to study as i understand it kind of proto scaling laws for adversarial robustness
*  and i think a huge question there is what are the kind of frontiers of possibility there like do you
*  need you know how do the orders of magnitude work right do you need another 10x as much adversarial
*  training to half the rate of your adversarial failures and if so you know can we generate that
*  many it may always sort of be fleeting so far ai and they are so they're working on
*  cutting edge of adversarial input yeah they're the group that did the attack on the alpha go model
*  and found that like you know and what was really interesting about that i mean multiple things
*  right first that they could beat a superhuman go player at all but second that the technique
*  that they used would not work at all if playing a quality human or is you know it's a strategy that
*  is trivial to beat if you're a quality human go player but the alpha go is just totally blind to
*  it you know that's why i say the conjecture is if you have the model then you can generate the
*  adversarial input and then so if that is true and that itself is an important conjecture about ai
*  safety right because if open weights are inherently something where you can generate adversarial input
*  from that and break or crash or defeat the ai then that ai is not omnipotent right you have some
*  power words you can speak to it almost like magical words that'll just make it power down
*  so to speak right it's like those movies where the monsters can't see you if you stand really still
*  or if you you don't make a noise or something like that right they're very powerful on dimension x
*  but they're very weak on dimension one a kind of an obvious point but you know i'm not sure how
*  important it's going to be in the future your next question was on like you know humanoid robots and
*  so on before we get to that maybe obviously but all of these models are trained on things that
*  we can easily record which are sights and sounds right but touch and taste and smell we don't have
*  amazing data sets on those well i mean there's some haptic stuff right uh there's there's probably
*  some you know some work on taste and smell and so on but there's there's five senses right i wonder
*  if there's something like that where uh you might be like okay how are you going to out smell uh you
*  know a robot or something like that well dogs actually have a very powerful sense of smell
*  that's being very important for them you know and it may turn out that there's maybe it's just that
*  we just haven't collected the data and it could become a much better smeller or whatever or you
*  know taster than anything else i wouldn't be surprised it could be a much better wine taster
*  because you can do molecular diagnostics but it's just kind of i just use that as an analogy to
*  say there's areas of the human experience that we haven't yet quantified and maybe it's just the
*  the operative term is yet okay but there's areas of the human experience we haven't yet quantified
*  which are also an area that ai's at least are not yet capable of yeah i guess maybe my expectation
*  boils down to i think the really powerful systems are probably likely to mix architectures
*  in some sort of ensemble you know you think about just the structure of the brain it's not i mean
*  there certainly are aspects of it that are repeated right you look at the the frontal cortex and it's
*  like there is kind of this you know unit that gets repeated over and over again in a sense that's
*  kind of analogous to say the transformer block that just gets you know stacked layer on layer
*  but it is striking in a transformer that it's basically the same exact mechanism at every
*  layer that's doing kind of all the different kinds of processing and so whatever weaknesses
*  that structure has and you know with the transformer and the attention mechanism there's
*  like some pretty profound ones like finite context window you know you kind of need i would think a
*  different sort of architecture with a little bit of a different strength and weakness profile to
*  complement that in such a way that you know kind of more similar to like a biological system where
*  you kind of have this like dynamic feedback where you know if we have obviously you know thinking
*  fast and slow and all sorts of different modules in the brain and they kind of cross-regulate each
*  other and don't let any one system you know go totally you know down the wrong path on its own
*  right without something kind of coming back and trying to override that it seems to me like that's
*  a big part of what is missing from the current crop of ais in terms of their robustness and
*  i don't know how long that takes to show up but we are starting to see some you know possibly you
*  know i think people are maybe thinking about this a little bit the wrong way they're just in the last
*  couple weeks there's been a number of papers that are really looking at the state space model
*  kind of alternative it's being framed as an alternative to the transformer but when i see
*  that i'm much more like it's probably a complement to the transformer or you know these two things
*  probably get integrated in some form because to the degree that they do have very different
*  strengths and weaknesses ultimately you're going to want the best of both in a in a robust system
*  certainly if you're trying to make an agent certainly if you're trying to make you know a
*  humanoid robot that can go around your house and like do useful work but also be robust enough
*  that it doesn't uh you get tricked into attacking your kid or your dog or you know whatever you're
*  going to want to have more checks and balances than just kind of a single stack of you know the same
*  block over and over again well so i know boston dynamics with their legged robots is all control
*  theory and it's not classical ml it's really interesting to see how they've accomplished it
*  and they do have essentially a state space model where they have a big position vector that's got
*  all the coordinates of all the joints and then a bunch of matrix algebra to figure out how this
*  thing is moving and all the feedback control and so on there and it's more complicated than that
*  but that's you know i think the v1 of it sorry it was there i wasn't following this though are you
*  saying that there's papers that are integrating that with the kind of generative transformer
*  model um you know what like what's a good citation for me to look at yeah starting to um we did an
*  episode for example with one of the technology leads at skydio the you know the us's champion
*  drone maker and they have kind of a similar thing where they have built over you know a decade right
*  a fully explicit multiple orders of you know spanning multiple orders of magnitude control
*  stack and now over the top of that they're starting to layer this kind of you know it's
*  not exactly generative ai in their case because they're not like generating content but it's kind
*  of the high level you know can i give the thing verbal instructions have it go out and kind of
*  understand okay like this is a bridge i'm supposed to kind of you know survey the bridge and translate
*  those high level instructions to a plan and then use the the lower level explicit code that is
*  is fully deterministic and you know runs on control theory and all that kind of stuff
*  to actually execute the plan at a low level but also you know at times like surface errors up to
*  the top and say like hey we've got a problem you know whatever i'm not able to do it you know
*  can you now at the higher level the semantic layer adjust the plan that stuff is starting to happen
*  in in multiple domains i would say yeah and so i think that makes sense is basically it's like
*  generative ai is almost the front end and then you have almost like an assembly like
*  you give instructions to figma and the objects there are their shapes and their images and so
*  there's not it's not text you give instructions to a drone and the objects are like gps coordinates
*  and paths and so on and so you are generating structures that are in a different domain or
*  it's like in vr you're generating 3d structures again as opposed to text and then that compute
*  engine takes those three structures and does something with them in a much more rules-based
*  way so you have like a statistical user-friendly front end with a generative ai and then you have a
*  more deterministic or usually totally deterministic almost like assembly language
*  back end that actually takes that and does something that's what you're saying right
*  yeah pretty much and i would say there's another analogy to just again our biological experience
*  where it's like i'm you know sort of in a semi-conscious level right i kind of think
*  about what i want to do but the low level movements of the hand you know are both like not conscious
*  and also you know if i do encounter some pain or you know hit some you know hot item or whatever
*  like there's a quick reaction that's sort of mediated by a lower level control system and
*  then that fires back up to the brain and is like hey you know we need a new plan here so that is
*  only starting to come into focus i think with you know because obviously these i mean it's amazing
*  as you said it's all moving so fast what is always striking to me i just and i kind of like
*  recite timelines to myself almost as like a mantra right like the first instruction following ai that
*  hit the public was just january 2022 that was open ai's text avinci 002 was the first one where you
*  could say like do x and it would do x as opposed to having you know an elaborate prompt engineering
*  type of setup gbt4 you know just a little over a year ago finished training not even a year that
*  it's been in the public and you know it's it has been amazing to see how quickly this kind of
*  technology is being integrated into those systems but it's definitely still very much a work in
*  progress yeah i mean the tricky part is um like the training data and so on with like a large
*  existing scale company like a figma or dji that has millions or billions of user sessions will
*  have a much easier time training and and they have a unique data set and then everybody else
*  will not be able to do that so there is actually almost like i mean a return on scale where the
*  massive data set if you've got a massive clean data set in a unique domain that lots of people
*  are using then you can you can crush it um and if you don't i suppose i mean there's lots of people
*  who work on zero shot stuff and and sort of sort of but it still strikes me that there'll probably
*  be an advantage to see those sessions you know i i find it hard to believe that you could you
*  know generate a really good uh like drone command language without lots of drone flight paths but
*  you know you can see and where it doesn't exist people are you know obviously you need deep
*  pockets for this but the likes of google are starting to just grind out the generation of
*  that right they've got their kind of test kitchen which is a literal you know physical kitchen
*  at google where the robots go around and do tasks and when they get stuck my understanding of their
*  kind of critical path as as i understand they understand it is robots going to get stuck
*  we'll have a human operator remotely operate the robot to show what to do and then that data
*  becomes the bridge from what the robot can't do to what it's supposed to learn to do next time
*  and they're going to need a lot of that you know for sure but they increasingly have you know i
*  don't know exactly how many robots they have now but last i talked to someone there it was like
*  into the dozens and you know presumably they're continuing to scale that i i think they just view
*  that they can probably brute force it to the point where it's like good enough to put out into the
*  world and then very much like a waymo or a cruise or whatever they probably still have kind of remote
*  operators even when the robot is like in your home you know when it encounters something that
*  it doesn't know what to do about raise that alarm get the human supervision to help it over the hump
*  and then you know obviously that's where you really get the scale that you're talking about
*  this raises a couple questions i wanted to to ask that are conceptual so you know obviously there's
*  huge questions around like again highest level how is all this going to play out one big debate is
*  to what degree does ai favor the incumbents to what degree you know does it enable startups
*  obviously it's both but you know i'm interested in your perspective on that also really interested
*  in your perspective on like offense versus defense that's something that a lot of people
*  now and in the future right that seems like it probably really matters a lot whether it's a more
*  offense enabling or defense enabling technology so what's your take on on those two dimensions
*  hey we'll continue our interview in a moment after a word from our sponsors
*  omni-key uses generative ai to enable you to launch hundreds of thousands of ad iterations
*  that actually work customized across all platforms with a click of a button i believe in omni-key so
*  much that i invested in it and i recommend you use it too use cog rev to get a 10 discount if
*  you're a startup founder or executive running a growing business you know that as you scale your
*  systems break down and the cracks start to show if this resonates with you there are three numbers
*  you need to know 36 000 25 and 1 36 000 that's the number of businesses which have upgraded to
*  net sweep by oracle net suite is the number one cloud financial system streamline accounting
*  financial management inventory hr and more 25 net suite turns 25 this year that's 25 years of helping
*  businesses do more with less close their books in days not weeks and drive down costs one because
*  your business is one of a kind so you get a customized solution for all your kpis in one
*  efficient system with one source of truth manage risk get reliable forecasts and improve margins
*  everything you need all in one place right now download net suite's popular kpi checklist
*  designed to give you consistently excellent performance absolutely free and net suite
*  dot com slash cognitive that's net suite dot com slash cognitive to get your own kpi checklist
*  net suite dot com slash cognitive so like offense or defense in the sense of disenable disruptors
*  or incumbents both in business and in like you know potentially outright conflict i'd be uh
*  interested to hear your analysis on both all right a lot of views on this so obviously if you've got
*  a competent existing tech ceo you know like who's still in their prime like omjad of replet or uh
*  you know dylan field of figma um or you know those are two who have thought of who are very good and
*  you know will be on top of it i'm just very early on integrating ai into replet and it's basically
*  built that into an ai first company which is really impressive those are folks who
*  cleanly made a pivot it's as big or bigger than comparable to i would say the pivot from uh
*  desktop to mobile that broke a bunch of companies in the late 2000s and early 2010s like facebook
*  in 2012 had no mobile revenue roughly at the time of their ipo and then they had to like
*  redo the whole thing and it's hard to turn a company 90 degrees when something new like that
*  hits you know those that are run by kind of tech ceos in their prime uh will will adapt and will
*  aiify their existing services and the question is obviously there's new things that are coming out
*  like pica and character.ai there's some like really good stuff that's that's out there the question is
*  uh you know will the disruption be allowed to happen in the u.s regulatory environment
*  and so my view is actually that uh you know so this is from like the network state book right
*  i talk about you know people talk about a multipolar world or unipolar world the political
*  axis is actually really important in my view for thinking about whether ai will be allowed to
*  disrupt okay because i will get to this probably later but the 640k of compute is enough for everyone
*  executive order you know 640k of memory the apocryphal he didn't delegates and actually say it
*  but that that quote kind of gives a certain mindset about computing that should be enough
*  for everybody so that 10 to the 26 of compute should be enough for everyone bill um i actually
*  think it's very bad and i think it's just the beginning of their attempts to build like a
*  software fda okay to decelerate control regulate red tape the entire space just like how you know
*  the threat of nuclear terrorism got turned into the tsa the threat of you know terminators and
*  hgi gets turned into a million rules on whether you can set up servers and this last free sector
*  the economy is strangled or at least controlled within the the territory controlled by washington dc
*  now why is why does this relate to the political well obviously this you know you can just spend
*  your entire life just tracking ai papers and that's moving like at the speed of light like this right
*  what's also happening as you can kind of see in your peripheral vision is
*  there's political developments that are happening at the speed of light much faster than they've
*  happened in our lifespans like there's more you just noticed more wars more serious online
*  conflicts like you know there's a sovereign debt crisis all of those things i can show graph after
*  graph of things looking like their own types of singularities you know like military debts are way
*  up you know the long piece that sevin pinker showed it's looking like a u that suddenly way up after
*  ukraine and some of these other wars are happening unfortunately right interest payments whoosh way up
*  to the side what's my point point is i i think that uh the world is going to become from the
*  pax americana world of just like basically one superpower hyper power that we grew up in from
*  91 to 2021 roughly that we're going to get a specifically tripolar world not unipolar not
*  bipolar not multipolar but tripolar and those three poles i kind of think of as nycp btc or you can
*  think of them as and those are just certain labels that are associated with them but they're roughly
*  us tech in the u.s environment china tech and china environment and global tech and the global
*  environment and why do i identify btc and crypto and so on with global tech because that's a tech
*  that decentralized out of the u.s and right now people think of crypto as finance but it's also
*  finance years okay and in this next run-up it is i think quite likely about depending on how you
*  count between a third to a half of the world's billionaires will be crypto okay around you know
*  i calculate this a while back around bitcoin at a few hundred thousands around a third to a half
*  the world's billionaires for crypto that's the unlocked pool of capital and those are the people
*  who do not bow to dc or beijing and they might by the way be indians or israelis or every other
*  demographic world or they could be american libertarians or they could be chinese liberals
*  like jack maw who are pushed out of beijing sphere okay or the next jack maw you know jack maw himself
*  may may not be able to do too much okay that group of people who are let's say the dissident
*  technologists who are not gonna just kneel to anything that comes out of washington z or beijing
*  that is the that's decentralized ai that's crypto that's decentralized social media so you can think
*  of it as you know where we talked about in the recent pirate wires podcast freedom to speak with
*  decentralized censorship resistant social media freedom to transact with cryptocurrency freedom
*  to compute with open source ai and no compute limits okay that's a freedom movement and that's
*  like the same spirit as a pirate bay the same spirit as bit torrent the same spirit as bitcoin
*  the same spirit as peer-to-peer and end to an encryption that's a very different spirit than
*  having comal harris regulate a super intelligence or signing it over to zhixian ping thot and the
*  reason i say this is i think that that group of people of which i think indians and israelis will
*  be a very prominent maybe a plurality right just because the sheer quantity of indians are like
*  the third sort of big group that's kind of coming up and they're relatively under priced you know
*  china is i don't say it's priced to perfection but it's something that people when i say priced i
*  mean people were dismissive of china even up until 2019 and then it was after 2020 if you look that
*  people started to take china seriously and i mean that is the west coast tech people knew that china
*  actually had a plus tech companies and was a very strong competitor but the east coast still thought
*  of them as a third world country until after covid when now you know the east coast was
*  sort of threatened by them politically and it wasn't just blue collars but blue america that
*  was threatened by china and so that's why the reaction to china went from oh who cares it's
*  just taking some manufacturing jobs to this is an empire that can contend with us for control of
*  the world that's why the hostility is ramped up in my view there's a lot of other dimensions
*  here but that's a big part of it so india is also kind of there but it's like the third and india is
*  not going to play for number one or number two but india and israel if you look at like tech
*  founders depending on how you count especially if you include the diasporas it's on the order of
*  30 to 50 percent of tech founders right and it's obviously some you know very good tech ceos and
*  you know satya and sundar and investors and whatnot those are folks indians do not want to
*  bow to dc or debauché neither do israelis for all kinds of reasons even if israel has to you know
*  take some direction from the u.s now they're bristling at it right and and then a bunch of
*  other countries don't so the question is who breaks away and and now we get to your point on
*  the reason i had to say that is that that's preface the political environment this tripolar
*  thing of u.s tech and u.s regulated chinese tech and china regulated and global tech that's free
*  okay of course there's even though i identify those three poles there's of course boundary
*  regions iac is actually on the boundary of of u.s tech and decentralized tech you know and i'm sure
*  there'll be some chinese thing that comes out that is also on the boundary there for example
*  binance is on the boundary of chinese tech and global and decentralized tech if that makes any
*  sense right there's probably others apple is actually on the boundary of u.s tech and chinese
*  tech because they make all of their stuff in china right so these are not totally disjoint groups but
*  there's boundary areas but but you can think about why is this third group so important in my view
*  both the chinese group and the decentralized group will be very strong competition for the
*  american group for totally different reasons china has things like wechat these super apps
*  i mean obviously not likely but like super wechat is a super app but they also have for example
*  their digital yuan right they have the largest cleanest data sets in the world that are constantly
*  updated in real time that they can mandate their entire population opt into and most of the chinese
*  language speaking people are under their ambit right so that doesn't include taiwan doesn't
*  include singapore doesn't include um you know some of the chinese diaspora but basically anything
*  that's happening in chinese for 99 percent of it 95 whatever the ratio is they can see it and they
*  can coerce it and they can control it so they can tell all of their people okay here's five bucks in
*  um you know digital yuan do this microtask okay all of these digital blue collar jobs both china
*  and india i think can do quite a lot with that and i'll come back to it so you can make their
*  people do immense amounts of training data clean up lots of data sets once it's clear that you have
*  to build this and do this they can just kind of execute on that and they can also deploy i mean
*  in many ways the u.s is still very strong in digital technology but in the physical world
*  it's terrible because of all the regulations cause all the nimbias um and so on it's not like
*  that in china so anything which kind of works in the u.s at a physical level like the boston
*  dynamic stuff they're already cloning it in china and they can scale it out in the physical world
*  you already have drones little little sidewalk drone things that come to your hotel room and
*  drop things off that's already like very common in china in many ways it's already ahead if you
*  go to the chinese cities so the chinese version of ai is ultra centralized more centralized
*  more monitoring less privacy and so on than the american version and therefore they will have
*  potentially better data sets at least for the chinese population and so we chat ai i don't
*  even know what it's going to be but it will be probably really good okay it'll also be really
*  dangerous in other ways okay then the decentralized sphere has power for a different reason because
*  the decentralized sphere can train on full hollywood movies it can train on all books all mp3s
*  and just say screw all this copyright stuff right like what psi hub and you know libchen are doing
*  because all the copyright first of all it's not it's like disney lobbying politicians to put like
*  another 60 or 70 or 90 i don't even know what it is some crazy amount on copyright so you can
*  keep milking this stuff and it doesn't go into public domain number one and second you know how
*  hollywood was built in the first place it was all patent copyright and ip violation essentially
*  edison had all the patents he's in new jersey ish okay that east coast area and um neil gabbler has
*  this great book called an empire of their own where he talks about how uh immigrant populations
*  you know the jewish community in particular also others went to southern california in part so they
*  could just make movies that edison coming and suing them for all the patents and so on and so
*  forth and they made enough money that they could fight those battles in court and that's how they
*  built hollywood okay so you know one of my big theses is history is running in reverse
*  and i can get to why but it's like 1950s a mirror moment you go more decentralized backwards and
*  forwards in time is like these you have these huge centralized states like the u.s and ussr and
*  china you know all these things exist and their fist relaxes as you go forwards and backwards in
*  time for example back to the time the western frontier closed uh and forwards in time the
*  internet frontier opens backwards the time you have the robber barons forwards the time you have the
*  tech billionaires back to the time you have spanish flu forwards the time you have covet 19
*  and i've got dozens of examples of this in the book the point is that if you go backwards in time
*  the ability to enforce patents and copyrights and so on starts dropping off right you have much more
*  of a grand theft auto environment and you go forwards in time and that's happening again
*  so india in particular for many years basically just didn't obey western patent protections and
*  all these stupid rules basically you know it's a combination of artificial scarcity on the patent
*  side and artificial regulation on the fdi side that's a big part of a jackson drug cost where
*  these things cost you know only cents to manufacture they sell them for so much money um all the delays
*  of course that are imposed on the process the only way they can pay for the manufacturers to take it
*  out of your height what india did is they just said we're not going to obey any of that stuff
*  so they have a whole massive generic drugs and biotech industry that arose because they built
*  all the skills for that that's why they could do their own vaccine during covet and they're one of
*  the biggest biotech industries in the world because they said screw western restrictive ips and other
*  stuff right so i was actually talking with the uh the founder of flipkart that's india's largest
*  exit and we were talking about this a few months ago and what we want is for india and other
*  countries like it do something similar not just generic drugs but generic ai meaning just let
*  people train on hollywood movies let them train on full songs let them train on every book let them
*  train on anything and you know what sue sue them in india right and have the servers in india and
*  let people also train models in india because that's something that can build up a a domestic
*  industry with skills that the rest of the world uh you know people will want the model output they'll
*  want to use the the software service there and they'll be fighting in court on the back end this
*  is similar to how all of the record companies fought uh napster and kaza and so on but they
*  couldn't take down spotify do you know that story do you remember that basically because spotify was
*  legitimately you know a european company and that a combination of execution and you know negotiation
*  they couldn't take them down they did take down napster they took down limewire they took down
*  groove shark and kaza had estonians i don't know exactly how it was incorporated but it's probably
*  too us proximal and that's what they were able to get them but spotify was far enough away that
*  they couldn't just sue them and they actually genuinely had european traction that's why the
*  ra had to negotiate so being far away from san francisco may also be an advantage in ai because
*  it means you're far away from the blue city in the blue state in the union this relates to another
*  really important point when you actually think about deploying ai there's those jobs that can
*  disrupt that are not regulated jobs like you know obviously programmers are not thank god you don't
*  need a license to be a programmer but programmers adopt this kind of stuff naturally right so get
*  up copilot replit we just boom use it and now it's amplified intelligence okay but a lot of other
*  jobs there's some that are unionized and then some that are licensed right so hollywood screenwriters
*  are complaining right journalists are complaining artists are complaining this is a good chunk of
*  blue america if you add in licensed jobs like lawyers and doctors and bureaucrats right um you
*  know especially lawyers and doctors very politically powerful mds and jd's they have strong lobbying
*  organizations ama and you know aba and so on basically ai is part of the economic apocalypse
*  for blue america okay it just attacks these overpriced jobs when i say overpriced relative to
*  an indian could do with an android phone what a south american could do with an android phone
*  what someone in the middle east or the midwest could do with an android phone now those folks
*  have uh you know been armed with generative ai they can do way more they're ready to work they're
*  ready to work for much less money and they're a massive threat to blue america blue america is
*  now feeling like the blue collars of 10 or 20 years ago where um the blue collars had their jobs
*  you know going to china and other places right and they were mad about that factories got shut down
*  and so on that's about to happen to blue america already happening okay and so that's going to mean
*  a political backlash by blue america of protectionism again already happening and the ai safety stuff
*  that's a whole separate thing but it's going to be used i'm going to use a phrase and i hope you
*  won't be offended by this have you heard the phrase useful idiots like by lennon or whatever
*  okay it basically means like okay those guys uh you know they're useful idiots for communism and
*  so so there's let me put it like naive people who think that the u.s government is interested in ai
*  safety are trying to give a lot of power to the u.s government and the reason is they haven't
*  actually thought through from first principles what is the most powerful action in the world
*  i come back to trying to get power to the u.s government to regulate ai safety but the government
*  doesn't care about safety of anything they literally funded the covid virus in wuhan
*  credibly alleged right there's at least it is a reasonable hypothesis based on a lot of the data
*  matt matt ridley wrote a whole book on this there's a lot of data that indicates a lot of
*  scientists believe it i'm i'm actually like a bioinformatics genomics guy if you look at the
*  sequences there is a gap and a jump where it looks like this thing could have been engineered
*  or partially engineered or evolved there's the you know peter dazak there's zengli shi there's
*  actually a lot of evidence here so the u.s government and the chinese government are
*  responsible for an existential risk you know by studying it they created it okay they're
*  responsible for risking nuclear war with russia over this you know a piece of land in eastern
*  ukraine which you know probably is going to get wound down okay so they don't care about your safety
*  at all they're not like these are immediate things where we can show and there's nobody who's punished
*  for this nobody's fired for this you know literally rolling the dice on millions hundreds of millions
*  of people's lives has not been punished in fact it's like it's not even talked about we're past
*  the pandemic and you know this these institutions can't be punished so they don't care about ai
*  safety what they care about is ai control and so the people in tech who are like well the government
*  will guarantee ai safety that's actually what we're going to actually get is something on the
*  current path like what happened with nuclear technology where you got nuclear weapons but not
*  nuclear power or at least not to the scale that we could have had it right we could have had much
*  cheaper energy for everything instead we got the militarization and the regulation and the deceleration
*  worst of all worlds where you can blow people up but you can't build nuclear power plants and like
*  even getting into nuclear technology forget about just nuclear power plants we don't have nuclear
*  submarines we don't have nuclear planes all that kind of stuff i don't know if nuclear planes are
*  possible but i do know nuclear submarines are possible you can do a lot more cruise ships a lot
*  more stuff like that you could probably have nuclear trains you know you have to look at exactly
*  how big those are you know not i don't know exactly how big those engines are and what the
*  supply is but i wouldn't be surprised if you could we don't have that why don't we have that because
*  we had the wrong fear-driven regulation in the early 70s putting it all together i think that
*  the current ai safety stuff is similar to nuclear safety stuff that the u.s government has a terrible
*  track record on safety in general it doesn't care about it it funded the covid virus incredibly
*  alleged it definitely risked nuclear war with with russia recently hot war with russia was the red
*  line we were not supposed to cross and we're now like way into that so it doesn't care about ai
*  safety doesn't care about your safety and it's also not even good at regulating and so what it
*  cares about is control and we are going to have potentially a bad outcome where silicon valley in
*  san francisco is the xerox park of ai may i think that's too strong okay but basically it develops
*  it and there's a lot of things it can't do because it lobbied for this regulation that is going to
*  come back and choke it and then the other two spheres will push ahead because it's not about
*  the technology it's also about the political layer you know the steve job saying actually alan k by
*  way of steve jobs if you're really serious about software you need your own hardware right so if
*  you're if you're really serious about technology you need your own sovereignty because like what
*  the ai people haven't thought about is there's a platform beneath you which is not just compute
*  it is regulate it's a law okay and if the law doesn't allow you to compute so much for all of
*  your stuff above that and i know you're saying oh it's only a 10 to 26 compute ban and so and so
*  forth have you seen the first irs tax form it's always always super simple it's only the super
*  super super rich who's we're going to get in first doesn't matter to you so that's called
*  boiling the frog slowly there's a million you know slippery slope slippery slope isn't a fallacy it's
*  literally how things work right you know apple one of the reasons they you know they talk about not
*  setting a precedent zuck starts a is a very hard line on setting precedents because he
*  understands the long-term equivalent of setting a precedent right the precedent setting is that
*  they're setting up a software fda and they're going to and and dc is so energized on this
*  because they know how much social media disrupted them that's why they're on the attack on crypto
*  and ai that's why they're on the attack on self-driving cars they want to freeze the
*  current social order in amber domestically and globally so they think they can sanction china
*  and stop it from developing chips they think they can impose regulations on the u.s. and stop it
*  from developing ai but they can't and also by the way they're they're totally schizophrenic on this
*  where when they're talking about china they're like we're gonna stop their chips to make sure
*  america is a global leader this is jina raimondo's angles and then domestically they're like we're
*  gonna regulate you so you stop accelerating ai we're not about ai acceleration eac is weird over
*  okay so think about how schizophrenic that is okay you're going to be far ahead of china
*  we're also going to be make sure to control the u.s. so they want to try and slow what they
*  actually want is to freeze the current system in amber try to go back to pre 2007 before all
*  these tech guys disrupted everything but that's not what's going to happen so but they're going
*  to try to do it and so everybody who's still loyal to the dc sphere which includes an enormous chunk
*  of ai people and because they're all in a lot of them in san francisco right and the political
*  chaos of the last few years was not sufficient for them to relocate yet not all of them i mean
*  elon is in texas and that it may turn out that grok for example and what they're doing there
*  because he's a very legit i mean you know he's elon so he's capable of doing a lot he's very
*  early on opening i he understood he understands you know the right it may turn out that grok
*  becomes red ai or the community around that you know and opening i and deep mind are still blue ai
*  and we have chinese and we're gonna have decentralized yeah okay let me pause there i
*  know there's a big download well i for starters i would say broadly i have a pretty similar
*  intellectual you know tendency as you i would broadly describe myself as a techno optimist
*  libertarian i'm just about every issue and i think your analysis of the dynamics is super
*  interesting and i think it you know a lot of it sounds pretty plausible although i'll kind of
*  float a couple things that i think may be bucking the trend but i think it's maybe useful to kind
*  of try to separate this into scenarios because the all the analysis that you're describing
*  here seem if i understand it correctly it seems to have the implicit assumption that the ai itself
*  is not going to get super powerful or hard to control it's like if we assume that it's kind of
*  a normal technology then you're off to the races on this analysis and then we can get into the
*  fine points but i do want to take at least one moment and say are you know how confident are you
*  on that because if it's if it's a totally different kind of technology from other
*  technologies that we've seen if it's more you know you raise the gain of function research
*  uh you know example if it's if it's that sort of technology that you know has these sort of
*  non-local possible impacts or you know self-reinforcing kind of dynamics which need
*  not be like a you know ellie eiser style snap of the fingers foom but even over say a decade let's
*  imagine that you know over the next 10 years that ai is kind of you know multiple architectures
*  develop and they sort of get integrated and we have something that kind of looks like
*  robust silicon based intelligence you know maybe not totally robust but like as robust or more
*  robust than us and running faster and you know the kind of thing that can like do lots of full jobs
*  or maybe even be tech ceos then it kind of feels like a lot of this analysis probably doesn't hold
*  right because we're just in a totally different regime that is just like extremely hard to predict
*  and i guess i wonder like first of all do you agree with that kind of just like there seems to be a
*  big fork in the road there that's like just how fast and how powerful do the how fast do these
*  ai's become super powerful or do they not and if they don't then like yeah i think we're much more
*  into like real politic type of analysis but i'm not at all confident in that to me it feels like
*  there's a very real chance that you know ai of 10 years from now is and by the way this is like
*  what the leaders are saying right i mean open ai is saying this anthropic is saying this demis
*  you know and shane lag are certainly you know saying things like this it seems like they expect
*  that we will have ai's that are more powerful than any individual human and that you know that that
*  becomes like the bigger question than anything else so do you agree with that kind of division
*  of scenarios first of all and then maybe you could kind of say like how likely you think each one is
*  and obviously that one where it takes off is like super hard to analyze and i also definitely think
*  it is worth analyzing this scenario where it doesn't take off but i just wanted to flag that
*  it seems like there's a you know there's a big if you talk to the ai safety people any world in which
*  it's like you know we're suing indian ai firms in indian court over like ip is like a normal world
*  in their mind right and that's not the kind of world that they're most worried about i think
*  that there have been some plausible sounding things that have been said but i want to just kind of
*  talk about a few technical counter arguments mathematical or physical that constrain what is
*  possible okay and actually martin cassato and vijay and i are working on a long thing on this
*  where you know vijay did folding at home he's a physicist martin sold in the sierra for you know
*  a billion dollars and and knows a lot about how a stuxnet like thing could work at the systems level
*  and i've thought about it from other angles and you know um and some of the math stuff as we'll
*  get to so for example one thing and i'm going to give a bunch of different technical arguments and
*  then let's kind of combine them okay one thing that's been talked about is if you have a
*  super intelligence it can double it right for a million years and then it can make one move and
*  it's going to outthink you all the time and so and so okay well if you're familiar with the math of
*  chaos or the math of turbulence there are limits to even very simple systems that you can set up
*  where they can become very unpredictable quite quickly okay and so you can if you want to
*  engineer a system where you have very rapid diversions of predictability so that i don't
*  know it's like the heat depth of the universe before you can predict out in timestamps do you
*  understand i'm saying right this is sort of akin to like a wolf from like simple even simple rules
*  can generate patterns such that you can't know them without literally computing them yeah exactly
*  right so at least right now with chaos and turbulence you can get things that are extremely
*  provably difficult to forecast without actually doing it okay you know i can make that argument
*  quantitative but it's just something to to look at right it's almost like a delta epsilon challenge
*  from calculus like okay how hard do you want me to make this to predict okay i can set up a problem
*  that is that is like that right it's basically extreme sensitivity to initial conditions lead
*  to extreme divergence in outcomes so you could design systems to be chaotic that might be ai
*  immune because they can't be forecasted that well you have to kind of react to them in real time
*  the ultimate version of this is not even a chaotic system it's a cryptographic system
*  where i've got a whole slide deck on this how ai makes everything fake easy to fake crypto makes it
*  hard to fake again right because crypto in the broader sense of cryptography but also in the
*  narrower sense i think crypto is to cryptography as uh the internet is to computer science it's
*  like the primary place where all this stuff is applied but obviously it's not the equivalent
*  okay and ai can fake an image but it can't fake a digital signature unless it can break certain
*  math you know and and so sort of like a you know solve factors problem or something like that
*  so cryptography is another mathematical thing that constrains ai similar to chaos and turbulence
*  it constrains how much an ai can infer things you can't statistically infer it okay you need
*  to actually have the private key to solve that equation so that is another math so i'm going to
*  rules of math right math is very powerful because you can make proofs that will work no matter what
*  devices we come up with okay you start to put an ai in a cage you can't predict beyond a certain
*  amount because of chaos and turbulence math it cannot uh solve certain equations unless it has
*  a private key is because of what we know about cryptography math okay again if somebody proves
*  p equals np some of this stuff breaks down but this is within the bounds of our mathematical
*  knowledge right now physics-wise physical friction exists a lot of physical friction exists and
*  a huge amount of the writing on ai assumes by guys like elizabeth who i like i don't i don't
*  dislike it you know but it is extremely uh it's there's two things that really stick out to me
*  about it first is extremely theoretical and not empirical and second extremely abrahamic rather
*  than dharmic or signing okay why theoretical and not empirical it's not trivial to turn something
*  from the computer into a real world thing okay one of the biggest gaps in all of this thinking
*  is what are the sensors and actuators okay because like if you actually build you know i've built
*  industrial robot systems that you know 10 years ago i you know a genome sequencing lab with robots
*  that's hard that's physical friction okay and a lot of the ai scenarios seem to basically say oh
*  it's going to be a self-programming stuxnet that's going to escape and live off the land
*  and hypnotize people into doing things okay now each of those is actually really really difficult
*  steps first is self-programming stuxnet like this would have to be a computer virus that can live
*  on any device despite the fact that apple or google can push a software update to a billion
*  devices right a few executives coordinating almost certainly can i mean the off switch exists right
*  like this is actually like the core thing lots of ai safety guys get themselves into the mind state
*  that the off switch doesn't exist but guess what there's almost nothing living that we haven't been
*  able to kill right like can we kill it this thing exists and this is getting back to living off land
*  even if you had like something that could solve some other technical problems that i'll get to
*  it exists as an electromagnetic wave kind of thing on on a certain you know on chips and so
*  and so forth it's taking it out in the environment is like putting a really smart human into outer
*  space right your body just explodes and you die it doesn't matter how smart you are that that strength
*  on this axis but you're weak on this axis and you know it's your strength on the x axis not strength
*  on the y or the z axis an ai outside you know pour water on it you know this is why i mean the 50 iq
*  150 iq thing you know 150 iq way of saying it is it's strong on this x and weak on this x and the
*  50 iq way is pour water on it disconnect it you know turn the power off okay right like it'll it'll
*  be very difficult to build a system where you literally cannot turn it off the closest thing
*  we have to that is actually not stuxnet it's bitcoin and bitcoin only exists because millions
*  of humans keep it going so you you need so that gets the second point living off the land
*  for an ai to live off land meaning without human cooperation okay that's the next touring
*  threshold an ai to live without human cooperation it would need to be able to control robots
*  sufficient to dig ore out of the ground set up data centers and generators and connect them
*  and defend that against human attack literally a terminator scenario okay that's a big leap in
*  terms i mean is it completely impossible i can't say it's completely impossible but it's not
*  happening tomorrow no matter what your ai timelines are you would need to have like a billion or
*  hundreds of millions of internet connected autonomous robots that this stuxnet ai could
*  hijack they were sufficient to carve ore out of the earth and you know set up data centers and make
*  the ai duplicate we're not there that's a huge amount of physical friction that's ai operating
*  without a human to make itself propagate right a human doesn't need the cooperation of a of a lizard
*  to to self replicate for an ai to replicate right now it would need the cooperation of a human
*  in some sense because otherwise those humans can kill it because there's not that many different
*  pieces of you know operating systems around the world i'm just talking about the practical
*  constraints of our current world right you know actually existing reality not ai safety guys you
*  know you know reality where all these things don't exist there's just a few operating systems just a
*  few countries if everybody's going with torches and searchlights through the internet it's very
*  hard for a virus to continue okay so a on the practicalities that there's a technical stuff with
*  with you know with the with the chaos and turbulence and with cryptography itself or ai can't predict
*  and it can't solve certain equations b on the physical difficulties it probably i mean like to
*  be a stuxnet microsoft and google and so on could kill it the off switch exists can it live off the
*  land no it cannot because it doesn't have you know drones to mine war and stuff out of the ground and
*  can it like exist without humans can it be this hypnotizing thing okay so the hypnotizing thing
*  by the way this is one of the things that's the most hilarious self-fulfilling prophecy in my view
*  okay in my and no offense anybody listening to this podcast but i think the absolutely dumbest
*  kind of tweet that i've seen on ai is i typed this in and oh my god it told me this
*  like i asked it how to make sarin gas and it told me x or whatever right that's just a search engine
*  okay what what basically a lot of these people are doing is they're saying what if there were
*  people out there that were so impressionable that they would type things into an ai and and follow
*  it as if they were hearing voices and that's actually not the the the model or whatever that's
*  doing it that's like this ai cult that has evolved around the world like a om shunrikyo you know that
*  that hears voices and does like the sarin gas the point is an ai can't just like hypnotize people
*  those people have to like participate in it they're typing things into the machine or whatever
*  okay now you might say all right let's project out a few years in a few years where you have is
*  you have an ai that is not just text but it appears as jesus what would what would ai jesus
*  do what would ai lee kwon you do what ai george washington do so it appears as 3d okay so it's
*  generating that it speaks in your language and in a voice it knows the history of your whole culture
*  okay that would be very convincing absolutely be very convincing but it still can't exist without
*  human programmers who are like the priests tending this ai god whether it's ai jesus or ai lee kwon
*  you or something like that the thing about the hypnotization thing that i really want to poke
*  on that are you familiar with the concept of principal agent problem basically in every
*  every time you've got like a ceo and a and a worker or you have a lp and a vc or you have
*  you know an employer and a contractor every edge there there are four possibilities in a two by
*  matrix win win win lose lose win lose lose okay and so for example win win is you know when
*  somebody joins a tech startup the the ceo makes a lot of money and so does a worker okay that's win
*  win lose loses they both lose money when lose is the ceo makes money and the employee doesn't
*  lose win is the company fails but the employee got paid a very high salary so what equity does
*  is it aligns people that's where the top concept alignment comes from it aligns people to the upper
*  left corner of win win that's when you have one one ceo and one employee when you have one ceo
*  and two employees you don't have two squared outcomes you have two cubed outcomes because
*  you have win-win-win win-win-lose win-lose-lose etc right because all three people can be win or
*  lose okay ceo can be win or lose employee can be win or lose employee number two can
*  be win or lose if you have n people rather than three people you have two to the end possible
*  outcomes and you have essentially a two by two by two by two by two by n hypercube of possibilities
*  Okay, it's all literally just two dimensions on the axis. There's tons of possible defecting
*  kinds of things that happen there. So that's why in a large company, there's lose win coalitions
*  that happen, where m people gang up on the other k people and they win with other people lose.
*  That's how politics happens. When you've got a startup that's driven by equity, and the biggest
*  payoff people don't have to try to think, Okay, well, I make more money by politics, they'll make
*  them money by the win, win, win, win, win column, because the exit makes everybody make the most
*  money. That's actually how the opening I people were able to coordinate around, we want an 80
*  billion dollar company, the economics help find the cell that was actually the most beneficial to
*  all of them helped them coordinate. Okay, so you search that hyper view. Okay, that's a point of
*  equity as lining. Still, despite all of this, that that's one of our best mechanisms for coordinating
*  large numbers of people in the principal agent problem. Despite all this, the possibility exists
*  for any of these people to win while the others lose, right with me so far, and I'll explain why
*  this important. But that means is those 1000 employees of the CEO are their own agents with
*  their own payoff functions that are not perfectly aligned with the CEOs payoff function. As such,
*  there are scenarios under which they will defect and do other things. Okay, the only way they become
*  like actual limbs, see my hand does not is not an agent of its own. It lives or dies with me.
*  Therefore, it does exactly what I'm saying at this time, I tell it to go up, it goes up,
*  tells you go down, it goes down sideways, sideways, right? An employee is not like that they will do
*  this and this and sideways, sideways, up to a certain point. And if you if you have them do
*  something that's extremely against their interests, they will not do your action. You understand my
*  point? Okay, that is the difference between an AI hypnotizing humans, versus an AI controlling drones.
*  Now controlling drones is like your hands, they're actually pieces of your body, there's no
*  defecting, there's no loose wind, they have no mind of their own, they're literally taking
*  instructions, okay, they have no payoff function, they will kill themselves for the hoard, right?
*  An AI hypnotizing humans has 1000 principal Asian problems for every 1000 humans. And it has to
*  incentivize them to continue and ask to generate huge payoffs. It's like an AI CEO, that's really
*  hard to do. Right? The history of evolution shows us how hard it is to coordinate multicellular
*  organisms, you have to make them all live or die as one, then you get something along these lines,
*  like an ant colony can coordinate like that. Because if the queen doesn't reproduce all the
*  ants, it doesn't matter what they're having sort of genetic material, okay? We are not currently
*  set up for those humans to not be able to reproduce unless the AI reproduces. Do I think we eventually
*  get to a configuration like that? Maybe. Where you have an AI brain is at the center of civilization,
*  and it's coordinating all the people around it. And every civilization that makes it is capable
*  of crowdfunding and operating its own AI. That gets me to my other critique of the AI safety
*  guys. I mentioned that the first critique is very theoretical rather than empirical. The second
*  critique is Abrahamic rather than Dharmic or Sineq. Okay. And you know, our background culture
*  influences things in ways we don't even think about. So much of the paperclip thinking is like
*  a vengeful God will turn you into pillars of salt, except it's a vengeful, you know, AI God will
*  turn you into paperclips. Okay. The polytheistic model of many gods as opposed to one God is
*  we're all going to have our own AI gods and there'll be war of the gods, like Zeus and Hera and so
*  on. That's the closest Western version. You know, the paganism that predated, you know, Abrahamic
*  religions, but that's still there in India. That's still how Indians think. That's why India is sort
*  of people have gotten so woke that they don't even make large scale cultural generalizations anymore.
*  But it's true that India is just culturally more amenable to decentralization to, you know,
*  multiple gods rather than one God and one state. Okay. And then the Chinese model is yet the
*  opposite. Like they have like, I mean, of course they have their tech entrepreneurs and so on,
*  but they're, if India is more decentralized, China is more centralized. They have like one government
*  and one leader for the entire civilization. Okay. And, uh, and that the biggest thing that China has
*  done over the last 20 or 30 years is they've taken various, you know, us things and they've made sure
*  that they have their own Chinese version where they have root. So they take us social media and
*  they made sure they had root over Sina Weibo. Okay. Uh, they make sure they have their own Chinese
*  version of electric cars and most Chinese version. So the private keys in a sense are with G. So that
*  means that they also at a minimum, you combine these two things, you're at a minimum going to
*  get polytheistic AI of the U S and Chinese varieties. And then you add the Indian version on it and
*  you're going to get quite a few of these different AIs around there. And then you have war of the
*  gods where maybe they are good at coordinating humans who, who, uh, you know, take instructions
*  from them, but they can't live without the humans and the humans are giving input to them.
*  That's a series of things. I could probably make that clearer if I just laid it out in bullets
*  in an essay, but just to recap it, a technical reasons like chaos, turbulence, um, cryptography,
*  why AI is limited in its ability to predict timeframes and to solve equations, be practical
*  limits. And AI cannot easily be a Stuxnet because Microsoft and Google and Apple can install software
*  on a billion devices and just kill it. Right. Like basically guys with torches come, all right.
*  It can't easily live off the land without humans because they would need hundreds of millions of
*  autonomous robots out there to control, to mind the or, and, and set up the data centers.
*  It can't just hypnotize humans like it can control drones because the principal agent problem and the
*  degree of human defection to make those humans do that. You'd have to have such massive alignment
*  between the AI and humans that the humans all know they'll die if the AI dies and vice versa.
*  We're not there. Maybe we'll be there in like, I don't know, N number of years, but not for a while.
*  That's a total change in like how states are organized. Okay. Finally, let me just talk about
*  the physics a little bit more. There's a lot of stuff which is talked about at a very sci-fi book
*  level of it'll just invent nanomedicine and nanotech and kill us all. And so and so forth.
*  Now, look, I like Robert Freitas, obviously Richard Feynman's a genius and so to so forth,
*  but nanotech somehow hasn't been invented yet. Okay. Meaning that, you know, there's a lot of
*  chemists that have worked in this area. Okay. And a lot of quote nanotech is like rebranded
*  chemistry because those are the molecular machines, you know, for example, DNA polymerase
*  or ribosome. Those are molecular machines that we can get to work at that scale, the evolved ones.
*  To my knowledge, and I may be wrong about this. I haven't looked at it very, very recently.
*  We haven't actually been able to make artificial, you know, replicators of the stuff that they're
*  talking about, which means it's possible that there's some practical difficulty that intervened
*  between Feynman and Freitas and so on to calculations, right? Just the sheer fact that
*  those books have came out decades ago and no progress has been made indicates that maybe
*  there's a roadblock that wasn't contemplated, right? So you can't just click your fingers and say,
*  boom, nanomass. And it's sort of like clicking your fingers and saying, boom, time travel,
*  right? Nanomass exists. That was, that was a good poke that I had a while ago in a conversation
*  like this, where the AI guy, AI safety guy on their side was like, well, time travel,
*  that's too implausible. I'm like, yeah, but you're waiting on, on the nanotech thing you're thinking
*  is like here. And you're making so many assumptions there that I want to actually see some more work
*  there. I want to actually see that nanotech is actually more possible than you think it is.
*  As for, oh, we just need to mix things in a beaker and make a virus and so it's over.
*  You know, what is really, really good at defending against novel viruses, like the human immune,
*  that's something that's within envelope, right? Like you have evolved to not die and to fight off
*  viruses. Is it possible that maybe you can make some super virus? I mean, maybe, but again, like,
*  humans are really good. And the immune system is really good at that kind of thing. That is what
*  we're set up to do right to adapt to that billions of years of evolution being set up. Physical
*  constraints are not really contemplated when people talk about these super powerful as
*  mathematical constraints, practical constraints are not contemplated. And I could give more,
*  but I think that was a lot right there. Let me pause here. Yeah, let me try to steal man a few
*  things. And then I do think it's before too long, I want to kind of get back to the
*  somewhat less radically transformative scenarios and ask a few follow up questions on that too.
*  But I think for starters, I would say the sort of Eliezer, he's updated his thinking over time as
*  well. And I would say probably doesn't get quite enough credit for it because he's definitely on
*  record repeatedly saying, yeah, I was kind of expecting more something from like the deep mind
*  school to pop out and be wildly overpowered very quickly. And on the contrary, it seems like we're
*  in more of a slow takeoff type of scenario where we've got these again, like super high surface
*  area kind of suck up all the knowledge, gradually get better at everything. Some surprises in there,
*  certainly some emergent properties, if you will accept that term, surprises to the developers,
*  if nothing else, right, that are definitely things we don't fully understand. But it does seem to be
*  a more gradual turning up of capability versus some like, you know, super sudden surprise.
*  But okay, so then what is the alternative? I'm going to try to kind of give you the what I
*  think of as the most consensus strongest scenario where humans lose track of the future and or lose
*  control of the future, maybe starting by kind of losing track of the present and then having that
*  kind of, you know, give way to losing control of the future. And I think within that, by the way,
*  I'm not really one who cares that much about like whether AIs say something offensive today.
*  I'm not easily offended and like whatever. That's not world ending. I understand your point. That's
*  not like who cares, whatever. That's within scope. That's within envelope. Within this bigger kind of,
*  you know, what is the real, you know, most likely path to like AI disaster, as understood, I think,
*  by the smartest people today, I think that is still a useful leading indicator, because it's like,
*  okay, the developers, you know, whether you agree with their politics, whether you agree with their
*  whether you think their commercial reasons are their sincere reasons or not, they have made it
*  a goal to get the AI to not say certain things, right, they don't want it to be offensive.
*  The most naive, you know, kind of down the fairway interpretation of that is like, hey,
*  they want to sell it to corporate customers. They know that their corporate customers don't want,
*  you know, to have their AI saying offensive things. So they don't want to say offensive things.
*  And yet, they can't really control it. It's like still pretty easy to break. So I view that as just
*  kind of a leading indicator of, okay, we've seen GPT two, three and four over the last four years.
*  And that's, you know, a big delta in capability. How much control have we seen developed in that
*  time? And does it seem to be keeping pace? And my answer would be on the face of it, it seems like
*  the answer is no, you know, we we don't have the ability to really dial in the behavior, such that
*  we can say, okay, you're gonna, you know, you can expect you can trust that these AIs will like not
*  do, you know, A, B and C. On the contrary, it's like, if you're a little clever, you know, you can
*  get them to do it. You can break out of the sandbox on it. Yeah. And it's not even like, I mean,
*  we've talked about, you know, things where you have access to the weights and you're doing like
*  counter optimizations, but you don't even need that. You know, the kind of stuff I do in like my
*  red teaming in public is literally just like, feed the AI a couple of words, put a couple words in
*  its mouth, you know, and it will kind of carry on from there. So with that in mind is just the
*  leading indicator. You know, I don't know how powerful the most powerful AI systems get over
*  the next few years, but it seems very plausible to me that it might be as powerful as like an Elon
*  Musk type figure, you know, somebody who's like really good at thinking from first principles,
*  really smart, you know, really dynamic across a wide range of different contexts. And, you know,
*  he's not powerful enough to like, in and of himself take over the world, but he is kind of
*  becoming transformative. Now imagine that you have that kind of system and it's trivial to
*  replicate it. So, you know, if you have like one Elon Musk, all of a sudden you can have arbitrary,
*  you know, functionally arbitrary numbers of Elon Musk power things that are clones of each other.
*  Maybe I can pause you there. So that's my polytheistic AI scenario. But here's the
*  thing that is this is background, but I want to push it to foreground. You still have a human
*  typing in things into that thing. The human is doing the jailbreak, right? What we're talking
*  about is not artificial intelligence in the sense of something separate from a human,
*  but amplified intelligence. Amplified intelligence, I very much believe in. The reason is amplified
*  intelligence. So here's something that people may not know about humans. There's this great book,
*  Cooking Made Us Human. Okay, tool use has shifted your biology in the following way. For example,
*  I know I'll map it to the present day. This book by Richard Ringham Cooking Made Us Human, where
*  the fact that we started cooking and using fire meant that we could do metabolism outside the body,
*  which meant it freed up energy for more brain development. Okay, similarly developing clothes
*  meant that we didn't have to evolve as much fur. Again, more energy for brain development,
*  evolving tools meant we didn't have as much fangs and claws and muscles. Again, more energy
*  for brain development, right? So encephalization quotient rose as tool use meant that we didn't
*  have to do as much natively and we could push more to the machines. In a very real sense,
*  we have been a man machine symbiosis since the invention of fire and the stone axe and clothes,
*  right? You do not exist as a human being on your own, like the entire Ted Kaczynski
*  concept of living in nature by itself. Humans are social organisms that are adapted to working
*  with other humans and using tools. And we have been for millennia, okay? This goes back,
*  not just human history, but like hundreds of thousands of years before, hundred gatherers
*  were using tools, okay? So what that means is man machine symbiosis is not some new thing.
*  It's actually the old thing that broke us away from other primate lineages that weren't using
*  tools, okay? This is the fundamental difference between what I call Uncle Ted and Uncle Fred.
*  Uncle Ted is Ted Kaczynski. It's a Unabomber. It's a Doomer. It's a decelerator, the de-groather,
*  who thinks we need to go back to Gaia and Eden and become monkeys and live in the jungle like
*  Ted Kaczynski, right? The Unabomber style. Uncle Fred is Friedrich Nietzsche, right? Nietzschean,
*  we must get to the stars and become Ubermen and so on and so forth. This I think is going to become,
*  and I actually tweeted about this years ago before current AI debates, that between anarcho-primitivism,
*  de-growth, deceleration, okay, on the one hand, and transhumanism and acceleration and human 2.0 and
*  human self-improvement and make it to the stars on the other hand, this is the future political
*  axis, the current one. And roughly speaking, it's not really left and right because you'll have both
*  left status and right conservatives go over here. You know, left states will say it's against the
*  state and the right states will say, the right conservatives say it's against God, okay? And
*  you'll have left libertarians and right libertarians over here, where left libertarians say it's my
*  body and the right libertarians say it's my money, right? And so that is a re-architecting of the
*  political axis where, you know, Uncle Ted and Uncle Fred, which is kind of clever way of putting it,
*  okay? And the problem with the Uncle Ted guys in my view is, as I said, yeah, if they go and want
*  to live in the, you know, the woods, fine, go get them. But once you start having even like a thousand,
*  forget a thousand, a hundred people doing that, your trees will very quickly get exfoliated,
*  you know, the leaves are going to get all picked off of them. Humans are not set up to just literally
*  live in the jungle right now. You've had hundreds of thousands of years of evolution that have
*  driven you in the direction of tool use, social organisms, farming, etc., etc. The man-machine
*  symbiosis is not today, it's yesterday and the day before and 10,000 years ago and 100,000 years
*  ago. And how do we know we've got man-machine symbiosis? Can you live without, even if you're
*  not living, even if you're not using the stove, somebody's using a stove to make you food, right?
*  Can you live without the tractors that are digging up the grains? Can you live without
*  indoor heating? Can you live without your clothes? Frankly, can you do your work without your phone,
*  without your computer? No, you can't. You are already a man-machine symbiosis. Once we accept
*  that, then the question is, what's the next step? And right now we're in the middle of that next step,
*  which is AI is amplified intelligence. So what you're talking about is not that the AI is Elon
*  Musk. It is that the AI-human fusion means there's another 20 Elon Musk's or whatever the number is.
*  Okay. And that's good. That's fine. That's within envelope. That's just a bunch of smarter humans
*  on the planet that is amplified intelligence. That is more like, you know, I mentioned the tool thing,
*  okay, the third analogy would be like a dog, you know, dog is man's best friend. Right? So that AI
*  does not live without you. Humans can turn it off. They have to power it. They have to give it
*  substance, right? Eventually that might become like a ceremonial thing. Like this is our God that we
*  pray to, right? Because it's wiser and smarter than us and it appears in an image, but the priests
*  maintain it. You know, just like you go to a Hindu temple or something like that and the priest will
*  pour out the ghee, you know, for the fires and so on and so forth. And then everybody comes in and
*  prays. Okay. The priests believe in the whole thing, but they also maintain the back of the
*  house. They do the system administration for the temple. Same, you know, in a Christian church,
*  right? The, you know, like it's not like it appears out of nowhere. Somebody, you know,
*  went and assembled this cathedral, right? They saw the back of the house, the fact that it was just
*  woods and rocks and so on that came together. But then when people come there, it feels like a
*  spiritual experience. You see what I'm saying? Okay. So the equivalent of that, the priests or
*  the, you know, the people maintaining temples, cathedrals, mosques, whatever, is engineers who
*  are maintaining these future AIs, which appear to you as Jesus. They appear to you, maybe even a
*  hologram. Okay. You come there, you ask him for guidance as an oracle. You've also got the personal
*  version on your phone. You ask him for guidance, but guess what? You're still a human AI symbiosis
*  until and unless that AI actually has the Terminator scenario where it's got lots of
*  robots and can live on its own. I'm not saying that's physically impossible. I did give some
*  constraints on it earlier, but for a while we're not going to be there. So that alone means it's
*  not FOOM because we don't have lots of drones running around. The AI has to be with the human.
*  It's a human AI symbiosis. It's not AI Elon Musk. It is human AI fusion that becomes Elon Musk. And
*  frankly, that's not that different from what Elon Musk himself is. Elon Musk would not be Elon Musk
*  without the internet. Without the internet, he can't tweet and reach 150 million people.
*  The internet itself made Elon what he is. Right. And so this is like the next version of that.
*  Maybe there's now 30 Elans because the AI makes the next 30 Elans.
*  Yeah. I mean, again, I think I'm largely with you with just this one very important nagging worry
*  that's like, what if this time is different? Because what if these systems are getting so
*  powerful so quickly that we don't really have time for that techno-human fusion to really work out?
*  I'll just give you kind of a couple data points on that. You said it's still somebody putting
*  something into the AI. Well, sort of. I mean, already we have these proto-agents and the
*  super simple scaffolding of an agent is just run it in a loop, give it a goal, and have it kind of
*  pursue some plan, act, get feedback, and loop type of structure. It doesn't seem to take a lot.
*  Now, they're not smart enough yet to accomplish big things in the world, but it seems like the
*  language model to agent switch is less one right now that is gated by the structure or the
*  architecture and more one that's just gated by the fact that the language models, when framed as
*  agents, just aren't that successful at doing practical things and getting overhumps. So they
*  tend to get stuck. But it doesn't seem that hard to imagine that if you had something that is
*  that next level that you put it into a loop, you say, okay, you're Elon Musk, LLM, and your job is
*  to make us, whatever us exactly is, a multi-planetary species. And then you just kind of
*  keep updating your status, keep updating your plans, keep trying stuff, keep getting feedback.
*  And what really limits that? There may be a really good program,
*  but the whole AI kills everyone thing is so it's like, where's the actuator? Okay, I hit enter.
*  What kills me? Is it a hypnotized human who's been hypnotized by an AI that he's typed into
*  and he's radicalized himself by typing into a computer? Okay, that's not that different
*  from a lot of other things that have happened in the past. So who is actually striking me?
*  Who's striking the human? It's another human with an axe that he's been radicalized by an AI?
*  Okay, actually that's not even the right term. We're giving agency to the AI when it's not
*  really an agent. It is a human who's self-radicalized by typing into a computer screen
*  and has hit another human. That's one scenario. The other scenario is it's literally a Skynet drone
*  that's hitting you. How else is it going to be physical? The actuation step is a part that is
*  skipped over and it's a non-trivial step. Well, I think it could be lots of things.
*  If it's not one of those two, if it's not another human or a drone hitting you, what is it?
*  Just habitat degradation. I mean, how do we kill most of the other species that we drive to
*  extinction? We don't go out and hunt them down with axes one by one. We just change the environment
*  more broadly to the point where it's not suitable for them anymore and they don't have enough space
*  and they kind of die out. So we did hunt down some of the megafauna literally one by one with
*  spears and stuff. But most of the recent loss of species is just like we're out there just
*  extracting resources for our own purposes. And in the course of doing that, whatever bird or
*  whatever thing just kind of loses its place and then it's no more. And I don't think that's
*  totally implausible. Wait, so that is though, I think within normal world, right? What does that
*  mean? That means that some people, some amplified intelligence and maybe might call it HAI. Okay,
*  human plus AI combination, right? Some HAIs out compete others economically and they lose their
*  jobs. Is that what you're talking about? I think also the humans potentially become
*  unnecessary in a lot of the configurations, like just a recent paper from DeepMind.
*  Zero marginal product workers. Or negative. Yeah. I mean, so the last,
*  DeepMind has been on Google, Google DeepMind has been on a tear of increasingly impressive medical
*  AIs. Their most recent one takes a bunch of difficult case studies from the literature.
*  I mean, case studies, you know, this is like rare diseases, hard to diagnose stuff,
*  and asks an AI to do the differential diagnosis, compares that to human and compares it to human
*  plus AI. And they phrase their results like in a very understated way. But the headline is the AI
*  blows away the human plus AI. The human makes the AI worse. So here's the thing. I'll say something
*  provocative maybe. Okay, like I have an array. Fine. I do think that the ABCs of economic
*  apocalypse for blue America are AI, Bitcoin and China, where AI takes away their a lot of
*  the revenue streams, the licensures that have made medical and legal costs and other things so high.
*  Bitcoin takes away the power over money and China takes with their military power. So I
*  foresee total meltdown for blue America in the years and you know, maybe decade to come.
*  Already kind of happening. But that's different than being at the end of the world.
*  Right? Like blue America had a really great time for a long time. And they've got these licensure
*  locks. But because of that, they've hyperinflated the cost of medicine. It's like how much how so
*  what you're talking about is, wow, we have infinite free medicine, man, doctor billing
*  events are going to get ahead. That's the point. Yeah. And to be clear, I'm really with you on
*  that too. Like I want to see one of the things when people say like, what is good about AI,
*  you know, why should we why should we pursue this? This, my standard answer is high quality
*  medical advice for everyone at pennies, you know, per visit, right? It is orders of magnitude
*  cheaper. We're already starting to see that in some ways, it's better. People prefer it,
*  you know, that AI is more patient, it has better bedside manner. I wouldn't say, you know, if I
*  was giving my, you know, my own family advice today, I would say use both a human doctor and
*  an AI, but definitely use the AI as part of your mix. Absolutely. That's right. That's right. But
*  you're prompting it still, right? The smarter you are, the smarter the AI is, you notice this
*  immediately with your vocabulary, right? The more sophisticated your vocabulary, the finer the
*  distinctions you can have, the better your own ability to spot errors. You can generate a basic
*  program with it, right? But really amplified intelligence is I think a much better way of
*  thinking about it, because whatever your IQ is, it surges it upward by a factor of three or
*  whatever the number and maybe the amplifier increases with your intelligence. But that that
*  internal intelligence difference still exists. It's just like what a computer is, a computer is an
*  amplifier for intelligence. If you're smart, you can hit enter and programs can go to like, like
*  think about the Minecraft guy, right? Or Satoshi. One person built a billion or so she gets trillion
*  dollar thing, you know, obviously, other people continued Bitcoin and so on and so forth, right?
*  So what I feel though, is this is what I mean by going from nuclear terrorism to the TSA. Okay,
*  we went from AI will kill everyone. And I'm like, what's the actuator to? Okay, it'll gradually
*  degrade our environment. What does that mean? Okay, some people lose their jobs, but then we're back
*  in normal world. Well, hold on, let me paint a little bit more complete picture, because I
*  don't think we're quite there yet. So I think the differential diagnosis, recent paper, that's just
*  a data point where it's kind of like chess, this, you know, this came long before, right? There was
*  a period where humans are the best chess players, then there was a period where the best were the
*  hybrid human AI systems. And now as far as I understand it, we're in a regime where the human
*  can't really help the AI anymore. And so the AIs are, you know, the best chess players are just pure
*  AIs. We're not there in medicine, but we're starting to see examples where, hey, in a pretty
*  defined study, differential diagnosis, the AI is beating, not just beating the humans, but also
*  beating the AI human hybrid, or the human with access to AI. So, okay, that's not it, right?
*  There's a paper recently called Eureka out of Nvidia. This is Jim Fan's lab where they
*  use GPT-4 to write the reward functions to train a robot. So you want to train a robot to like twirl
*  a pencil in fingers, hard, you know, hard for me to do, robots, you definitely can't do it. How do
*  you train that? Well, you need a reward function. The reward function, basically, while you're in
*  the early process of learning and failing all the time, the reward function gives you encouragement
*  when you're on the right track, right? So you, there are people who, you know, have developed this
*  skill and you might do something like, well, if the pencil has angular momentum, you know,
*  then that seems like you're on maybe sort of the right track. So give that, you know, a reward,
*  even though at the beginning you're just failing all the time. Turns out GPT-4 is way better than
*  humans at this, right? So it's better at training robots. So all of that is awesome and it's great.
*  But here's the thing is there's a huge difference between AI is going to kill everybody and turn
*  everybody into paper clips. Okay. Versus some humans with some AI are going to make a lot more
*  money and some people are going to lose their jobs. Yeah, I'm not scared of that. I'm not scared of
*  that scenario. I mean, it could be disruptive. It could be disruptive, but it's not existential
*  unto itself. Bingo. Okay. So that's why I went right. There's the, the, the bait to me, it comes,
*  if I, if I asked just one question is what is the actuator, right? You know, sensors and actuators,
*  right? What is the thing that's actually going to plunge a knife or a bullet into you and kill you?
*  It is either a human who has hypnotized themselves by typing into a computer,
*  like basically an AI terrorist, you know, which is kind of where some of the EAs are going in my view,
*  or it is like an autonomous drone that is controlled in a StarCraft or Terminator-like way.
*  We are not there yet in terms of having enough humanoid or autonomous drones that are internet
*  connected and programmable. That won't be there for some time. Okay. So that alone means fast
*  takeoff is, and what I think by the time we get there, you will have a cryptographic control over
*  them. That's a crucial thing. Cryptography fragments the whole space in a very fundamental way. If you
*  don't have the private keys, you do not have control over so long as that piece of hardware,
*  the cryptographic controller, you've nailed the equations on that. And frankly, you can use AI to
*  attack that as well to make sure the code is perfect, right? Remember you talk about attack
*  and defense? AI's attack crypto is defense, right? Because one of the things that crypto has done,
*  do you know what the PKI problem is, public key infrastructure? I'll say no on behalf of the
*  audience. This is good. We should do more of these actually. I feel it's a good fusion of things or
*  whatever, right? But the public key infrastructure problem, the public key infrastructure problem is
*  something that was sort of lots of cryptography papers and computer science papers in the
*  nineties and two thousands assumed that this could exist and essentially meant if you could assume
*  that everybody in the internet had a public key that was public and a private key that was kept
*  both secure and available at all times, then there's like all kinds of amazing things you
*  can do with privacy preserving, messaging and authentication and so on. The problem is that
*  for many years, what cryptographers try to do is they try to nag people into keeping their private
*  keys secure and available. And the issue is it's trivial to keep it secure and unavailable where
*  you write it down, you put it into a lock box and you lose the lock box. It's trivial to keep it
*  available and not secure. Okay. Where you put it on your public website and it's available all the
*  time. You never lose it, but it's not secure because anybody can see it. When you actually ask,
*  what does it mean to keep something secure and available? That's actually a very high cost. It's
*  precious space because it's basically your wallet, right? Your wallet is on your person at all times,
*  so it's available, but it's not available to everybody else. So it's secure. So you actually
*  have to like touch it constantly. Yes. Right. So it turns out that the crypto wallet, by adding a
*  literal incentive to keep your private keys secure and available, because if they're not available,
*  you've lost your money. If they're not secure, you've lost your money. Okay. To have both of them,
*  that was what solved the PKI problem. Now we have hundreds of millions of people with public,
*  private key pairs where the private keys are secure and available. That means all kinds of
*  cryptographic schemes, zero knowledge stuff. There's this amazing universe of things that
*  is happening now. Zero knowledge in particular has made cryptography much more programmable.
*  There's a whole topic, which is if you want something that's kind of, you know, like AI was
*  creeping for a while and people, specialists were paying attention to it and then just burst out on
*  the scene. Zero knowledge is kind of like that for cryptography. Thanks to the, you know, you've
*  probably heard of zero knowledge before. Yeah. We did one episode with Daniel Kang on the use of
*  zero knowledge proofs to basically get to prove without revealing like the weights
*  that you actually ran the model. You said you were going to run and things like that,
*  I think are super interesting. Exactly. Right. So what kinds of stuff, why is that useful in
*  the ice face? Well, first is you can use it, for example, for training on medical records while
*  keeping them both private, but also getting the data you want out of, for example, let's say you've
*  got a collection of genomes. Okay. And you want to ask, okay, how many G's were in this data set?
*  How many C's, how many A's, how many T's? Okay. Like you just say, like that's a very simple
*  down, let's say ACGT content of this, you know, the sequence data set, you could get those numbers,
*  you could prove they were correct without giving any information about the individual sequences,
*  right? Or more specifically, you do it at one locus and you say how many G's and how many C's
*  are at this particular locus and you get the SNP distribution. Okay. So it's useful for what you
*  just said, which is like showing that you ran a particular model without giving anything else away.
*  It's useful for certain kinds of data analysis. There's a lot of overhead on compute on this
*  right now. So it's not something that you do trivially. Okay. But it'll probably come down
*  with time. But what is perhaps most interestingly useful for is in the context of AI is coming up
*  with things in AI can't fake. So what we talked about earlier, right? Like an AI can come up with
*  all kinds of plausible sounding images, but if it wasn't cryptographically signed by the sender,
*  then you know, it should be signed by sender and put on chain. And then at least you know
*  that this person or this entity with this private key asserted that this object existed at this time
*  in a way that'd be extremely expensive to falsify because it's either on the Bitcoin blockchain or
*  another blockchain. It's very expensive to rewind. Okay. This starts to be a bunch of facts that an AI
*  can't fake. You know, so the going back to the kind of big picture loss of control story, I was just
*  kind of trying to build up a few of these data points that like, Hey, look at this differential
*  diagnosis. We already see like humans are not really adding value to AIs anymore. That's kind
*  of striking. And like similarly with training robot hands, GPT-4 is outperforming human experts.
*  And by the way, all of the sort of latent spaces are like totally bridgeable, right? I mean, one
*  of the most striking observations of the last couple years of study is that AIs can talk to
*  each other in high dimensional space, which we don't really have a way of understanding natively,
*  right? We, it takes a lot of work for us to decode. This is like the language thing. It, we're starting
*  to see AIs kind of develop not obviously totally on their own as of now, but we are, there is
*  becoming an increasingly reliable go-to set of techniques. If you want to bridge different
*  modalities with like a pretty small parameter adapter. That's interesting. I actually, what's
*  a good paper on that? I actually hadn't seen that. The blip family of models out of Salesforce
*  research is really interesting. And I've used that in production at Salesforce, really Salesforce
*  research. They have a crack team that has open-sourced a ton of stuff in the language
*  model, computer vision, joint space. And this, this, you see this all over the place now, but
*  basically what they did in a paper called blip two, and they've had like five of these with a
*  bunch of different techniques, but in blip two, they took a pre-trained language model and then
*  a pre-trained computer vision model. And they were able to train just a very small model that kind of
*  connects the two. So you could take an image, put it into the image space, then have their little
*  bridge, bridge that over to language space and that everything else, the two big models are frozen.
*  So they were able to do this on just like a couple days worth of GPU time, which I do think goes to
*  show how it is going to be very difficult to contain proliferation. Which is good. I, in my
*  view, that's really good. As long as it doesn't get out of control. I'm, I am probably with you on that
*  too. But by bridging this vision space into the language space, then the language model would be
*  able to converse with you about the image, even though the language model was never trained on
*  images, but you just had this connector that kind of bridges those modalities. It's just, it's like
*  another layer of the network that just bridges two networks almost. Yeah. It bridges the spaces.
*  They like it bridges the conceptual spaces between something that has only understood images and
*  something that has only understood language. But now you can kind of bring those together.
*  As I think about it, it's not that surprising because that's what, you know, for example,
*  text image models are basically that they're bridging two spaces, you know, in a sense,
*  right? But I'll check this paper out. So that, so on the one hand, it's not that surprising.
*  On their hand, I should see how they implemented it or whatever. So blip two. Okay.
*  Yeah. I think the most striking thing about that is just how small it is. Like you took these two
*  off the shelf models that were trained independently for other purposes and you're able
*  to bridge them with a relatively small connector. And that seems to be kind of happening all over
*  the place. I would also look at the flamingo architecture, which is like a year and a half
*  ago now out of deep mind. That was a one for me where I was like, oh my God. And it's also a
*  language to vision where they keep the language model frozen. And then they kind of, in my mind,
*  it's like, I can see the person in their garage, like tinkering with their soldering iron, you
*  know, cause it's just like, wow, you took this whole language thing that was frozen and you kind
*  of injected some, you know, vision stuff here and you added a couple layers and you kind of Frankenstein
*  it and it works. And it's like, wow, that's not really, it wasn't like super principled, you know,
*  it was just kind of hack a few things together and, you know, try training it. And I don't want to
*  diminish what they did because I'm sure there were more insights to it than that. But it seems
*  like we are kind of seeing a reliable pattern of the key point here being model to model
*  communication through high dimensional space, which is not mediated by human language is,
*  I think one of the reasons that I would expect, and by the way, there's lots of papers too on like,
*  you know, language models are human level or even superhuman prompt engineers, you know,
*  they're self-prompting, like techniques are getting pretty good. So if I'm imagining the big
*  picture of like, and we can, you know, get back to like, okay, well, how do we use any techniques
*  crypto or otherwise to keep this under control? And then I would say this is kind of the newer
*  school of the big picture AI safety worry. Obviously there's a lot of flavors, but if you
*  were to, you know, go look at like a J a contra, for example, I think a really good writer on this,
*  her worldview is less that we're going to have this FOOM and more that over a period of time,
*  and it may not be a long period of time, maybe it's like a generation, maybe it's 10 years,
*  maybe it's a hundred years, but obviously those are all small in the sort of, you know,
*  grand scheme of the future. We have in all likelihood, the development of AI centric
*  schemes of production, where you've got kind of your high level executive function is like
*  your language model. You've got all these like lower level models. They're all bridgeable. All
*  the spaces are bridgeable in high dimensional form, where they're not really mediated by language,
*  unless we enforce that. I mean, we could say, you know, it must always be mediated by language so
*  we can read the logs, but there's a tax to that, right? Cause going through language is like highly
*  compressed compared to the high dimensional space to space. All right. So let me see if I can
*  steal man or articulate your case. You're saying AIs are going to get good enough. They're going to
*  be able to communicate with each other good enough and they're able to do enough tasks that more and
*  more humans will be rendered economically marginal or unnecessary. I'm not saying I think that will
*  happen. I'm just saying, I think there's a good enough chance that that will happen, that it's
*  worth taking really seriously. I actually think that will happen something along those lines,
*  in the sense of at least massive economic disruption. Definitely. Okay. But I'll give an
*  answer to that, which is both, you know, maybe fun and not fun. Have you seen the,
*  you've seen the graph of the percentage of America that was involved in farming?
*  Yeah, I tweeted a version of that once. Oh, you did. Okay, great. Good. So you're familiar with
*  this and you're familiar with what I mean by the implication of it, where basically Americans used
*  to identify themselves as farmers, right? And manufacturing rose as agriculture collapsed,
*  right? And here is the graph on that. But from like 40% in the year 1900 to like a total collapse
*  of agriculture, and then also more recently, a collapse of manufacturing into bureaucracy,
*  paperwork, legal work, what is up into the right since then, is, you know, the, the lawyers,
*  what is up into the right, what is replacing that? Starting in around the 1970s, we used to be adding
*  energy production and energy production flatlined once people got angry about nuclear power. So this
*  is a future that could have been, we could be on Mars by now, but we got flatlined, right? What did
*  go up into the right. So construction costs, this is the bad scenario where the miracle energy got
*  destroyed because regulations, the cost was flat. And then when vertical, when regulations were
*  imposed, all the progress was stopped by decels and degrowthers. And then a Lara was implemented,
*  which said nuclear energy has to be as low risk as, as reasonably necessary, as reasonably
*  achievable. And that meant that you just keep adding quote safety to it until it's as same as
*  cost as everything else, which means you destroyed the value of it, right? But you know what was up
*  into the right, what replaces agriculture and manufacturing jobs? Look at this, you see this
*  graph for the audio only, we will put this on YouTube. So if you want to see the graph,
*  do the YouTube version of this for the audio only group, it's an exponential curve in the number of
*  lawyers in the United States from looks like maybe two thirds of a million to 13 million over
*  the last 140 years. Yeah. And in 1880, it was like, like sub 100,000 or something like that,
*  right? And then it's just like, especially that 1970 point, that's when it went totally vertical,
*  okay. And it's probably even more since. So, you know, if you add paperwork jobs, bureaucratic
*  jobs, you know, every lawyer is like, you know, sorry, lawyers, but you basically negative value
*  add right, because it should, the fact that you have a lawyer means that you couldn't just
*  self serve a form, right? Basically, government is platform is where you can just self serve and
*  you fill it out. And you don't have to have somebody like code something for you custom,
*  you know, lawyers that's doing custom code is because the legal code is so complicated.
*  So, you know, the whole Shakespeare thing, like first thing we do, let's, you know, kill all the
*  lawyers. First thing we do, let's automate all the lawyers, right? Only something that's the hammer
*  blow of AI can break the backbone. And it will. That's, it's going to break the backbone of blue
*  America, right? It's going to cause that's why the political layer and the sovereignty layer is not
*  what AI people think about. But it's like crucial for thinking about AI, because what tribes does AI
*  benefit? And again, we got away from why is AI kill everybody? Well, it's going to need actuators,
*  who's going to stab you, who's going to shoot you, it's got to be a human hypnotized by AI or a drone
*  that AI controls. A human hypnotized by AI is actually a conventional threat. It looks like a
*  terrorist cell. We know how to deal with that, right? It's just like radicalized humans that
*  worship some AI that stab you. It's like the pause AI people are one step, I think, away from that.
*  All right. But that's just like Om Shunriko. That's like al-Qaeda. That's like, basically,
*  terrorists who think that the AI is telling them what to do. Fine. If it's not a human that's
*  stabbing you, it is a drone. And that's like a very different future where like five or 10 or 15 years
*  out, maybe we have enough internet connected drones out there, but even then they'll have private
*  keys. So there's going to be fragmentation of address space, not all drones be controllable by
*  everybody, in my view. Okay, that's what AI safety is. AI safety is, can you turn it off? Can you kill
*  it? Can you stop it from controlling drones? That's what AI safety is. Can you also open the
*  model weights, you can generate adversarial inputs. Can you open the model weights and proliferate it?
*  You're saying, oh, proliferation is bad. I'm saying proliferation is good. Because if everybody
*  has one, then nobody has an advantage on it. Right? Not relatively speaking. Okay.
*  I have very few super confident positions. So I wouldn't necessarily say I think that proliferation
*  is bad. I'd say so far, it's good. It has, and even most of the AI safety people,
*  I would say, if I could speak on the behalf of the AI safety consensus, I would say most
*  people would say even that the llama to release has proven good for AI safety,
*  for the reasons that you're saying. But they opposed it. Well, some didn't, some didn't.
*  I would say the main posture that I see AI safety people taking is that we're getting really close
*  to or we might be getting really close. Certainly, if we just kind of naively extrapolate out recent
*  progress, it would seem that we're getting really close to systems that are sufficiently powerful,
*  that it's very hard to predict what happens if they proliferate. Lama to not there. And so,
*  you know, yes, it has enabled a lot of interpretability work. It has enabled things
*  like representation engineering, which there is a lot of good stuff that has come from it.
*  The big thing that I want to kind of establish is you agree with me on the actuation point or not.
*  Like, the thing is this thing like, oh, llama to proliferates. And so businesses are disrupted.
*  And people, you know, maybe maybe they, they paid a lot of money for their MD degree, and they can't
*  make as much money. That's within the realm of what I call conventional warfare. You know what
*  I mean? That's like we're still in normal world as we were talking about. Okay. Unconventional warfare
*  is, you know, Skynet arises and kills everybody. Okay. And that is what is being sold over here.
*  And when you think about the actuators, we don't have the drones out there. We don't have the
*  humanoid robots that control and hypnotize humans are very tiny subset of humans, probably. And if
*  they aren't, that just looks like a religion or a cult or a terrorist cell. And we know how to deal
*  with that as well. The super intelligent AI with, you know, lots of robots that control in a Star
*  Craft form, I would agree is something that humans haven't faced yet. But by the time we get that
*  many robots out there, you won't be able to control all of them at once because of the private
*  keys things I mentioned. So that's why I'm like, okay, everything else we're talking about is in
*  normal world. That's that is the single biggest thing that I wanted to get like economic disruption,
*  people losing jobs, proliferation, so that the balance of power is redistributed, all this fine.
*  The reason I say this is people keep trying to link AI to existential risk. A great example is
*  one of the things you actually had in here. This is similar to the AI policy and so you think
*  it's totally reasonable question, but then I'm going to, in my view, deconstruct the question.
*  What would you think about putting the limit on the right to compute or their capabilities
*  and AI system might demonstrate that we make you think open access no longer wise. Most common
*  nurture man sir here to be seems to be related to risk a pandemic via novel pathogen engineering.
*  So guess what? You know who the novel pathogen engineers are? The US and Chinese governments,
*  right? They did it, or probably did it credibly did it credibly been accused of doing it. They
*  haven't been punished for COVID-19. In fact, they covered up their culpability and pointed everywhere
*  other than themselves. They use it to gain more power in both the US and China with both
*  lockdown in China and in the US and all kinds of COVID era, trillions of dollars was printed and
*  spent and so on and so forth. They did everything other than actually solve the problem that was
*  actually getting, you know, the vaccines in the private sector. And they studied the existential
*  risk only to generate it. And they were even paid to generate pandemic prevention and failed.
*  So this would be the ultimate Fox guarding the henhouse. Okay, the only read the two
*  organizations responsible for killing millions of people novel pathogen are going to
*  prevent people from doing this by restricting compute. No, it you know what it is, actually,
*  what's happening here is one of the concepts I have in the network state is this idea of God,
*  state and network. Okay, meaning, what do you think is the most powerful force in the world?
*  Is it almighty God? Is it the US government? Or is it encryption? Right? Or eventually,
*  maybe an AGI, right? If what was happening here is a lot of people are implicitly,
*  without realizing it, even if they're a secular atheist, they're treating GOV as God. Okay,
*  they treat the US government as God as the final mover.
*  No, I appreciate your little I take inspiration from you, actually, in terms of
*  trying to come up with these little quips that, you know, that are memorable. So I was just smiling
*  at that, because I think you do a great job of that. And I try to incorporate I have less success
*  coining terms than you have, but certainly try to follow your example on that front.
*  It's like a helpful name. If you can compress it down, it's like more memorable. So that's what
*  I try to do. Right. So exactly, a lot of these people who are secular, think of themselves as
*  atheists have just replaced God with GOV. They worship the US government as God. And there's two
*  versions of this, you know, how like God has both the male and female version, right? The female
*  version is the democrat God within the USA that has infinite money and can take care of everybody
*  and care for everybody. And the republican God is the US military that can blow up anybody and it's
*  the biggest and strongest and most powerful America. F. Yeah. Okay. And everybody who thinks of the US
*  government as being able to stop something is praying to a dead God. Okay, when you see this,
*  you actually get an interesting reaction from AI safety people, where you've actually hit their
*  true solar plexus. All right, the true solar plexus is not that they believe in AI, it's that they
*  believe in the US government. That's a true solar plexus, because they are appealing to their praying
*  to this dead God that can't even clean the poop off the streets in San Francisco, right? That is
*  losing wars or fighting them to sell me it says lost all these wars around the world that spent
*  trillions of dollars has been through financial crisis, Coronavirus, Iraq war, you know, total
*  meltdown politically, okay, that has interest that is now has interest payments more than the
*  defense budget. That is, you know, that spent $100 billion on the California train without
*  laying a single track. It's like that, you know, that Morgan Freeman thing, you know, the clip
*  from Batman, who's like, so this man has a billionaire, blah, blah, blah, this and that,
*  and your plan is to threaten him, right? And so you're going to create this super intelligence
*  and have Kamala Harris regulated. Come on, man, so to speak, right? Like, these people are praying
*  to a blind, deaf and dumb God that was powerful in 1945. Right? That's why, by the way, all the
*  popular movies, what are that? It's Barbie, it's Oppenheimer, right? It's, it's Top Gun. They're
*  all throwbacks, the 80s or the 50s, when the USA was really big and strong. And the future is a
*  black mirror. Yeah, I think that's tragic. I one of the projects that I do like, and you might
*  appreciate this. I don't know if you've seen it is the from the future of life Institute,
*  a project called Imagine a World, I think is the name of it. And they basically challenged,
*  you know, their audience and the public to come up with positive visions of a future, you know,
*  where technology changes a lot. And obviously, AI pretty central to a lot of those stories.
*  And, you know, one of the challenges that people go through, and how do we get there and whatever,
*  but a purposeful effort to a man to imagine positive futures, super under provided, and I
*  really liked the the investment that they made in that, you know, something, one of the things I've
*  got in the never said book is there's certain mega trends that are happening, right? And mega
*  trends, I mean, it's possible for, like one miraculous human maybe to reverse them. Okay.
*  So I think both the impersonal force of history theory and the great man theory of history have
*  some truth to them. But the mega trends are the decline of Washington, DC, the rise of the
*  internet, the rise of India, the rise of China. That is like my worldview. And I can give 1000
*  graphs and charts and so on for that. But that's basically the last 30 years. And maybe the next
*  x, right? I'm not saying there can't be trend reversal, of course, it can be trend reversal,
*  as I just mentioned, some hammer blow could hit it, but that's what's happening. And so because of
*  that, the people who are optimistic about the future are aligned with either the internet,
*  India or China. And the people who are not optimistic about the future are blue Americans
*  or left out red Americans, okay, or Westerners in general, who are not tech tech people. Okay,
*  if they're not tech people, they're not up into the right, basically, because the internet's,
*  if you I mean, one of the things is we have a misnomer, as I was saying earlier, calling it
*  the United States, because the dis United States, it's, it's like talking about, you know, talking
*  about America is like talking about Korea, there's North Korea and South Korea, they're totally
*  different populations. And, you know, communism and capitalism are totally different systems.
*  And the thing that is good for one is bad for another and vice versa. And so like America
*  doesn't exist, there's only just like there's no Korea, there's only North Korea and South Korea,
*  there's no America, there's blue America and red America and also gray America, tech America,
*  and blue America is harmed, or they think they're harmed, or they've gotten themselves into a spot
*  where they're harmed by every technological development, which is why they hate it so much,
*  right? AI versus journalist jobs, crypto takes away banking jobs, you know, everything, you know,
*  self driving cars, they just take away regulator control, right? Anything that reduces their power,
*  they hate, and they're just trying to freeze in amber with regulations. Red America got crushed
*  a long time ago by offshoring to China and so on there, they're making, you know, inroads ally with
*  tech America or gray America. Tech America is like the one piece of America that's actually still
*  functional and globally competitive. And people always do this fallacy of aggregation, where they
*  talk about the USA. And it's really this component that's up into the right, and the others that are
*  down into the right or at best flat like red, but they're like down, right? Like red is like,
*  okay, functional blues down. Point is, tech America, I think we're going to find is not even
*  truly or how American is tech America, because it's like 50% immigrants, right? And like a lot
*  of children immigrants and most of their customers are overseas, and their users are overseas. And
*  their vantage point is global, right? And they're basically not. I know we're in this ultra
*  nationalist kick right now. And I know that there's going to be there's a degree of a fork
*  here, where you fork technology into Silicon Valley and the internet. Okay, where Silicon
*  Valley is American, and they'll be making like American military equipment, and so on and so
*  forth, and they're signaling USA, which is fine. Okay. And then the internet is international,
*  global capitalism. And the difference is Silicon Valley, or let's say US tech, let me less, you
*  know, US tech says, ban tik tok, build military equipment, etc. It's really identifying itself
*  as American. And it's thinking of being anti China. Okay, but there's US and China only 20% of the
*  world 80% of the world is either American or Chinese. So the internet is for everybody else
*  who wants actual global rule of law, right? When as the US decays as a rules based order,
*  and people don't want to be under China, people want to be under something like blockchains,
*  where you've got like property rights, contract law across borders that are enforced by an impartial
*  authority. Okay, that's also kind of laws that can bind a eyes like a eyes across borders, if you
*  want to make sure they're going to do something cryptography can bind an AI in such a way that
*  it can't fake it, it can't an AI can't mint more Bitcoin, you know, my here's my last question for
*  you. AI discourse right now does seem to be polarizing into camps. Obviously, a big way that
*  you think about the world is by trying to figure out, you know, what are the different camps? How
*  do they relate to each other? So on and so forth. I have the view that AI is so weird. And so unlike
*  other things that we've encountered in the past, including just like, unlike humans, right, I always
*  say AI alien intelligence, that I feel like it's really important to, to borrow a phrase from Paul
*  Graham, keep our identities small, and try to have a scout mindset to really just take things on their
*  own terms, right, and not necessarily put them through a prism of like, who's team am I on? Or,
*  you know, is this benefit my team or hurt the other team or whatever? But, you know, just try to be as
*  kind of directly engaged with the things themselves as we can without mediating it through all these
*  lenses. You know, I think about you mentioned, like, the gain of function, right? And I don't
*  know for sure what happened. But it certainly does seem like there's a very significant chance
*  that it was a lab leak. Certainly, there's a long history of lab leaks. But it would be like,
*  you know, it would seem to me a failure to say, Okay, well, what's the what's the opposite of
*  just having like a couple of government labs, like, everybody gets their own gain of function lab,
*  right? Like, if we could, and this is kind of what we're doing with AI, we're like, let's compress
*  this power down to as small as we can, let's make a kit that can run in everybody's home.
*  Would we want to send out these like gain of function, you know, wet lab research kits to like,
*  every home in the world and be like, hope you find something interesting, you know, like,
*  let us know if you find any new pathogens or hey, maybe you'll find life saving drugs, like,
*  whatever, we'll see what you find, you know, all 8 billion of you. That to me seems like it would be
*  definitely a big misstep. And that's the kind of thing that I see coming out of ideologically
*  motivated reasoning, or like, you know, tribal reasoning. And so I guess, I wonder how you think
*  about the role that tribalism and ideology is playing and should or shouldn't play as we try
*  to understand AI. Okay, so first is, you're absolutely right that just because a is bad
*  does not mean that B is good, right? So a could be a bad option, B could be a bad option, C could
*  be a bad option. There might be we have to go down to option G before you find a good option,
*  or there might be three good options and seven bad options, for example, right. So to map that here,
*  in my view, an extremely bad option is to ask the US and Chinese governments to do something,
*  anything the US government does at the federal level, at the state level in blue states at the
*  city level has been a failure. And the way here's a here's a meta way of thinking about it, you
*  invest in companies, right. So as an investor, here's a really important thing. You might have
*  10 people who come to you with the same words in their pitch. They're all for example, building
*  social networks. But one of them is Facebook, and the others are Friendster and whatever. Okay,
*  and no offense to Friendster, you know, those guys were like, you know, pioneers in their own way.
*  But they just got outmatched by Facebook. So point is that the words were the same
*  on each of these packages, but the execution was completely different.
*  So could I imagine a highly competent government that could execute? And that actually did,
*  you know, like, you know, make the right balance of things and so on. I can't say it's impossible,
*  but I can say that it wouldn't be this government. Okay. And so you are talking about the words,
*  and I'm talking about the substance. The words are we will protect you from AI, right? In my view,
*  the substances, they aren't protecting you from anything, right? You're basically giving money
*  and power to a completely incompetent and in fact, malicious organization, which is Washington, DC,
*  which is the US government that has basically over the last 30 years, gone from a hyper power
*  that wins everywhere without fighting to a declining power that fights everywhere without women.
*  Okay, like just literally burn trillions of dollars doing this, take maybe the greatest
*  decline in fortunes in 30 years and maybe human history, not even the Roman Empire went down this
*  fast on this many power dimensions this quickly, right? So giving that guy, let's trust him. That's
*  just people running an old script in their heads that they inherited. They're not thinking about
*  it from first principles that this state is a failure. Okay. And like how much of a failure
*  it is, you have to look at the sovereign debt crest, you look at look at graphs that other
*  people aren't looking at. But like, you know, the the the domain of what blue America can regulate
*  is already collapsing. Because it can't regulate Russia anymore, it can't regulate China anymore,
*  it's less able to regulate India, it's less able even to regulate Florida and Texas,
*  states are breaking away from it domestically. So this gets to your other point, why is the tribal
*  lens not something that we can put in the back, we have to put in the absolute front, because the
*  world is retribalizing. Like, basically, your tribe determines what law you're bound by. If you think
*  you can pass some policy that binds the whole world, well, there have to be guys with guns
*  who enforce that policy. And if I have guys with guns on their side that say we're not enforcing
*  their policy, then you have no policy, you've only bound your own people. Does that make sense? Right?
*  And so blue America will probably succeed in choking the life out of AI within blue America.
*  But blue America controls less and less of the world. So have more power or fewer people.
*  I can go into why this is but essentially, you know, a financial Berlin wall is rising.
*  There's a lot of taxation and regulation, and effectively financial repression, de facto
*  confiscation that will have to happen for the level of debt service that the US has been taking on.
*  Okay, just there's there's one graph, just to make the point. And if you want to dig into this,
*  you can. All right. But the reason this impacts things is, when you're talking about AI safety,
*  you're talking about AI regulation, you're talking about the US government, right. And you have to
*  ask, what does that actually mean? And it's like, in my view, it's like asking the Soviet Union in
*  1989, to regulate the internet, right? That's going to outlive, you know, the country, US interest
*  payment on federal debt versus defense spending. The white line is defense spending, look at the
*  red line, that's just gone absolutely vertical, that's interest. And it's going to go more
*  vertical next year, because all of the status getting refinanced, and much higher interest rates.
*  This is why the look at this, you want you want AI timelines, right? The question for me is DC's
*  timeline. What is DC's time left to live? Okay, this is the kind of thing that kills empires.
*  And, and you either have this just go to the absolute moon, or they cut rates, and they print
*  a lot. And either way, you know, the the fundamental assumption underpinning all the AI safety,
*  all the AI regulation work is that they have a functional golem in Washington, DC, where if
*  they convince it to do something, it has enough power to control enough of the world. When that
*  assumption is broken, then a lot of assumptions are broken. Right. And so in my view, you have to,
*  you must think about a polytheistic AI world, because other tribes are already into this,
*  they're already funding their own, right, the proliferation is already happening.
*  And they're not going to about a blue tribe. So I, that's why I think the tribal lens is not
*  secondary. It's not some, you know, totally separate thing, it is an absolute primary way
*  in which to look at this. And in a sense, it's almost like a, you know, in a well done movie,
*  all the plot lines come together at the end. Okay. And all the disruptions that are happening,
*  the China disruption, the rise of India, the rise of the internet, the rise of crypto, the rise of AI,
*  and the decline of DC, and the internal political conflict. And, you know, various other theaters,
*  like what's happening in Europe, and you know, and, and Middle East, all of those come together
*  into a crescendo of ah, there's a lot of those graphs are all having the same time.
*  And it's not something you can analyze by just, I think, looking at one of these curves on its own.
*  I think that's a great note to wrap on. I am always lamenting the fact that so many people
*  are thinking about this AI moment in just fundamentally too small of terms. But I don't
*  think you're one that will easily be accused of that. So with an invitation to come back and
*  continue in the not too distant future for now, I will say Balaji Srinivasan, thank you for being
*  part of the cognitive revolution. Thank you, Nathan. Good to be here. It is both energizing
*  and enlightening to hear why people listen and learn what they value about the show.
*  So please don't hesitate to reach out via email at tcr at turpentine.co or you can DM me on the
*  social media platform of your choice. Amnaki uses generative AI to enable you to launch hundreds of
*  thousands of ad iterations that actually work customized across all platforms with a click of
*  a button. I believe in Amnaki so much that I invested in it and I recommend you use it too.
*  Use Cogrev to get a 10% discount.
