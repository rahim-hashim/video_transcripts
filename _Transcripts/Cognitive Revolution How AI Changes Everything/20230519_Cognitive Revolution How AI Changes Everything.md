---
Date Generated: April 03, 2024
Transcription Model: whisper medium 20231117
Length: 5902s
Video Keywords: []
Video Views: 4212
Video Rating: None
---

# [Bonus Episode] Connor Leahy on AGI, GPT-4, and Cognitive Emulation w/ FLI Podcast
**Cognitive Revolution "How AI Changes Everything":** [May 19, 2023](https://www.youtube.com/watch?v=7BqcdYRyS4s)
*  Hello and welcome back to a special bonus episode of the Cognitive Revolution.
*  As you may know, I recently had the pleasure of appearing as a guest on the Future of Life
*  Institute podcast.
*  The Future of Life Institute, you'll recall from our recent interview with Skype founder
*  and influential AI investor Jan Tallin.
*  And if you haven't heard that episode, I strongly encourage going back to listen to it.
*  Future of Life Institute is the organization behind the recent open letter calling for
*  a voluntary six month pause on the larger than GPT-4 scale AI training runs.
*  The Future of Life Institute podcast, which despite having known Jan for years, I will
*  confess to never even having heard of previously, turns out in fact to be excellent.
*  Gus Dockert, the host, really puts in the work necessary to deliver deeply substantive
*  conversations.
*  And his lineup of guests, while not exclusively AI focused historically, is almost entirely
*  AI of late, featuring interviews with a number of critical thinkers such as Neil Nanda and
*  Ajay Akatra that I also hope to interview in the future.
*  So we're taking this moment to share the Future of Life Institute podcast with you, starting
*  with an interview that Gus recently did with Conjecture CEO Connor Leahy.
*  Conjecture describes itself as a team of researchers dedicated to applied scalable AI alignment
*  research.
*  They also build products to help businesses improve their workflows.
*  Jan, who is an investor in Conjecture, described them memorably as, quote, a team that has
*  the highest respect for AI, a virtue that I personally also seek to cultivate.
*  This is part one of a two part interview.
*  Part two is available on the Future of Life Institute podcast feed.
*  I hope you enjoyed this conversation with Gus Dockert and Conjecture CEO Connor Leahy.
*  Welcome to the Future of Life Institute podcast.
*  I'm here with Connor Leahy.
*  Connor is the CEO of Conjecture and Conjecture is this company researching scalable AI alignment.
*  So Connor, welcome to the podcast.
*  I'm so glad to be back.
*  What is happening with the GPT-4?
*  Is this the moment that AI becomes a mainstream issue?
*  Christ, what a way to start out.
*  It is no exaggeration to say that the last two weeks of my life have been the most interesting
*  of my career in terms of events in the wider world.
*  I thought nothing could be GPT-free.
*  After I've seen what happened with GPT-3, I'm like, okay, this is the craziest thing
*  that's going to happen in a short period of time.
*  Then I quickly realized, no, that can't be true.
*  Things are only going to get crazier.
*  As predicted, exactly that is what has happened.
*  As predicted, the release of GPT-4 has been even crazier than GPT-3.
*  The world has gone even crazier.
*  Things have really changed.
*  I cannot overstate how much the world has changed over the last, not necessarily only
*  since GPT-4, but also since ChatGPT.
*  ChatGPT was even a bigger change in wider political things.
*  I won't mince words.
*  The thing that really has struck me over, I've been talking to a lot of people recently.
*  Now I have journalists running down my door.
*  I talk to politicians and national security people.
*  One thing that really strikes me is that people are starting to panic.
*  This goes beyond Silicon Valley Twitter circles.
*  This is venturing into politics and governmental agencies and so on.
*  I've been doing AI for a long time.
*  I come from a pretty rural place in southern Germany.
*  When I went back to visit my mother for Christmas, all my cousins and family were there.
*  They talked about ChatGPT.
*  I was there in this teeny world where there's usually no technology and I'm the only one
*  who really knows how to use a computer very well.
*  Then they're talking with me.
*  Connor, we thought this AI thing you were talking about, that wasn't some kind of thing
*  you liked, but wow, you were right.
*  This is actually happening.
*  I'm like, yeah, big surprise.
*  This is not just a thing that is in a small circle of people in tech or Silicon Valley
*  or whatever.
*  This is different.
*  This is very different.
*  People are getting front page time news coverage about this kind of stuff.
*  We're getting people from all walks of life suddenly noticing, wait, this is actually
*  real.
*  This is actually affecting me.
*  This is actually affecting my family, my future.
*  This is not at all how things went fast.
*  In a ironic twist, it seems that the people deepest in tech are the ones who are least
*  rational about this or the least deeply taking this seriously.
*  There's this meme that's been around for a long time about how you can't explain to normal
*  people AI or AI risk or whatever.
*  I can talk to anyone on the street, share the chat GPT, explain to them, and explain
*  AI risk.
*  Like, hey, these people are building bigger and bigger and stronger things like this.
*  They can't control it.
*  Do you think this is good?
*  And they're like, no, obviously not.
*  What the hell are you talking about?
*  Of course this is bad.
*  Do you think that the advancement from GPT 2 to GPT 3 was bigger than the advancement
*  from GPT 3 to GPT 4?
*  Or are we hitting diminishing returns?
*  No.
*  Not at all.
*  No, not really.
*  It's like just as I predicted, basically.
*  This is just pretty much on track.
*  I would say GPT 4, the final version is better.
*  So I used the GPT 4 alpha when it was back in August or whatever, when that was first
*  being passed around among people in the Bay.
*  And it was already very impressive then, but kind of in line of what I was expecting.
*  The released version is significantly better.
*  Like the additional work they've done to make it better at reasoning and such individual
*  stuff and all that kind of stuff is significantly better than what I saw in August, which is
*  not surprising.
*  It's just, you know, it's all...
*  Sure, you can argue on some absolute terms.
*  The absolute amount of difference between like GPT 2 and GPT 3 is obviously much larger.
*  So like the amount of like the size of the model is a much bigger difference.
*  Like GPT 4 from what I hear is not that...
*  It's larger, but it's not that much larger than GPT 3.
*  And the thing with GPT 4 is that what is very striking with GPT 4, and it's not surprising,
*  but I think it's important, is not that it can do crazy things that are impossible to
*  accomplish in principle with GPT 3.
*  Because often the things that are impressive with GPT 4, it's possible to accomplish these
*  things with GPT 3 with a lot of effort and error checking and re-rolling and very good
*  prompting and so ever.
*  The thing that is striking with GPT 4 is that it's consistent.
*  The striking is that you can ask it to do something and it will do it and it will do
*  it very reliably.
*  This is not just bigger model size.
*  This is also even better fine tuning, RLHF.
*  Better understanding of what users want these models to do.
*  The truth is that users don't want general purpose base model of large text corpuses.
*  This is not what users really want.
*  What they want is a thing that does things for them.
*  This is, of course, needless to say, this is also what makes these things dangerous
*  compared to GPT 3.
*  Like, raw GPT 3 is very powerful and whatever, but raw GPT, they can also take actions or
*  that is trained very, very heavily to take actions to reason to do things.
*  Which GPT 4 is?
*  Let's be very explicit here.
*  GPT 4 is not a raw base model.
*  It is an RL trained, struck fine tuned, extremely heavily engineered system that is designed
*  to solve tasks, to do things that users like.
*  And these are all kinds of different things.
*  But let's be very clear about this.
*  This is not a raw, the thing you see on the API is not a raw base model that's just trained
*  to model a non-supervised corpus of text.
*  This is something that's fine tuned.
*  This is RLHF.
*  And this is, I mean, I've openly did a fantastic job.
*  Like, you know, on a purely technical terms, I'm like, wow, this is so good.
*  Like, this is so good.
*  This is so well made.
*  This thing is so smart.
*  It, GPT 4 is the first model that I personally like, I feel is delightful to use.
*  Like when using GPT 2 or 3, I still kind of like pulling out my hair.
*  I'm like, this is still like very, like, I'm not a great prompter, right?
*  I don't really use language models for much for this reason, because I found them just
*  generally to be very frustrating to use for most of the things I would use them for, except
*  for, you know, very simple or silly things.
*  GPT 4 is the first model that like, when I use it, I'm like delighted.
*  I like, I smile, like the clever things it comes up with and like how delightfully easy
*  it was to get it to do something useful.
*  Yeah.
*  And this is mostly from the reinforcement learning from human feedback or is this coming
*  from the base model, how it's trained or is this coming from how it's fine tuned and
*  trained to respond to what humans wanted to do?
*  I mean, who knows, obviously.
*  Who knows how they did this exactly?
*  I don't think they know.
*  I think this is all empirical.
*  I don't think there's like, there's no theory here.
*  It's not like, ah, once you do 7.5 micro alignments of RLHF, then you get what you want.
*  No, it's just like, you just just fuck around and you just have, you know, how much people
*  label a bunch of data until it looks good.
*  And, you know, like this is not to denigrate the, you know, incredibly difficult engineering
*  work and scientific work that was done here.
*  If I didn't think these systems were extremely dangerous, I would be in absolute awe of a
*  open AI and I would love to work with them because this is an incredible feat of engineering
*  that they have performed here.
*  Incredible work of science.
*  This is incredibly impressive.
*  I do not deny this, you know, the same way that if I was there doing the Trinity test,
*  I would be like, wow, this is an impressive feat of engineering.
*  How much have we explored what GPT can do?
*  So in terms of what's there waiting to be found if we just gave it the right prompt?
*  Who knows?
*  Like, we have not scratched the surface, not even scratched the surface.
*  There's this narrative that people sometimes, you know, especially like Sam Altman and
*  such likes to say where he's like, oh, we need to do incremental releases of our systems
*  to allow people to test them so we can debug them.
*  This is obviously bullshit.
*  And the reason this is obviously bullshit is because if he actually believed this, then
*  he would release GPT-3 and then wait until society has absorbed it, until, you know,
*  institutions have caught up or regulation has caught up until, you know, people have
*  fully explored, mapped the space of what GPT-3 can and cannot do, you know, understood interpretability.
*  And then you can release GPT-4.
*  If you actually did this, I would be like, all right, you know what?
*  Fair enough.
*  That's totally fair.
*  Like, I think this is a fair, responsible way of handling this technology.
*  This is obviously not what is going on here.
*  There is an extraordinarily funny interaction where Jan Leiker, the head of alignment at
*  OpenAI tweeted like, hey, maybe we should slow down before we hook these LLMs into everything.
*  And six days later, Sam Altman tweets, here's plugins for chat GPT.
*  Plug it into all the tools on the net.
*  Like the comedic timing is unparalleled.
*  If this was in a movie, like this would have been like, you know, like a cut, you know,
*  and then everyone would have laughed.
*  You know, it would have been extremely funny.
*  So we have no idea.
*  There are as, I think it was Goren that said this, there is no way to prove the absence
*  of a capability.
*  We do not have the ability to test what models cannot do.
*  And as we hook them up to more tools, to more environments, we give them memory, we give
*  them, you know, recurrence, we use them as agents, which people are now doing, you know,
*  like Lang Chang and a lot of other methods for using these things as agents.
*  Yeah.
*  I mean, obviously we're seeing the emergence of Proto AGI, like obviously so.
*  And, you know, I'm not sure if it's even going to be Proto for much longer.
*  Talk a bit about these plugins.
*  What, so as I understand it, these plugins allow language models to do things that they
*  were previously bad at, like getting recent information or solving symbolic reasoning,
*  like, like mathematics and so on.
*  What is it that's allowed by these plugins?
*  I mean, anything.
*  So like, it was quite strange to me that like, this has been strange to me for years.
*  So like, I looked at GPT-2 and I'm like, oh, well, there's the AGI.
*  It doesn't work yet, but this is going to become AGI.
*  And people are like, oh no, Connor, it only, you know, predicts the next token.
*  And I'm like, you know, I know the outputs token.
*  I'm like, okay, your brain only outputs neural signals.
*  So what?
*  Like, like that's not the interesting thing.
*  The interesting thing, like the modality, like, I often say this, that I
*  think the word large language models is kind of a misnomer or it's just like not a good term.
*  The fact these models use language is completely coincidental.
*  This is just a implementation detail.
*  What these things really are, are general cognition engines.
*  They are general systems that can take in, you know, input from various modalities
*  encoded to some kind of semantic space and do cognitive operations on it.
*  And then, you know, output some kind of cognitive, you know, output out of this.
*  So we've seen this now with very good example, which is an example I've been, you
*  know, um, you know, using as a hypothetical for a long time is you chat, you know, with
*  GPT-4 allowing visual input.
*  And this maps it into the same internal representation space, whether it's an image
*  or text, and they can do the same kind of cognitive operations on it.
*  This is the same way the human brain works.
*  You know, your retina or your ears or whatever, you know, map various forms of stimuli
*  into a common representation of neural spike trains.
*  These are taken as input and it outputs some neural spike trains that, you know, can
*  be connected to your mouth or to your internal organs or your muscles or whatever.
*  Right.
*  These, none of these things are special.
*  Like this is from the perspective of your brain, there's only an input token stream
*  quote unquote in the form of neural spikes and an output token stream in the form of
*  neural spikes.
*  And similarly, what we're seeing with like these GPT plugins and whatever is we're
*  hooking up muscles to the neural spike trains of the, of these language models.
*  We are, we are hooking up language.
*  We are giving them actuators, virtual actuators upon reality.
*  And this is interesting both for the way in which they can interact with the
*  environment, but also how they can externalize their cognition.
*  So this is a topic and thing we might return to later, but a massive amount of
*  human cognition is not in the brain.
*  This is quite important.
*  I think a lot of people severely underestimate how much of the human mind is not in the
*  brain.
*  I don't mean it's like in the gut or something.
*  I mean, it's literally not in the body.
*  It's in the environment.
*  It's, it's on the, the internet and in books and in talking to other people,
*  collaboration, so on.
*  Exactly.
*  Exactly.
*  This is a massive amount of, even you as a person, a bunch of your identity is
*  related to your social networks.
*  It's not in your head.
*  It's like, you know, there, um, there's a saying about how like one of the
*  tragedies when someone dies is that part of you dies and only that person could
*  bring out.
*  And I think this is like quite true is that a lot of humanity, a lot of like our
*  thinking is deeply ingrained with our tools and our environments and our social
*  circles, et cetera.
*  And this is something that GPT-3, for example, didn't have.
*  GPT-3 couldn't really use tools.
*  It didn't interact with the environment.
*  It didn't, you know, it was, it was very solipsistic in the way it was, it was
*  designed, but, and so people would say, well, look, language mostly nowhere.
*  Look, they're solipsistic, et cetera, et cetera.
*  But like, I'm like, sure, but that's just an implementation detail.
*  Like, obviously you can just make these things non-solipsistic.
*  Obviously you can make these things model the environment.
*  You can make them interact with tools.
*  You can make them interact with your other language models or with themselves
*  or whatever, and, you know, whatever you decide to do.
*  Of course, these things are general cognition engines.
*  There is no limit to what you can use them for or how you can have them
*  interact with the environment.
*  And the plugins are just a particularly shameless, hilarious attempt of showing
*  just like the complete disregard for the ratcheting of capabilities is we're
*  seeing just, you know, back in the old days of like, you know, five years ago,
*  people would speculate, you know, so very earnestly of like, well, how
*  could we contain a powerful AI?
*  Well, you know, maybe we could build some kind of like virtualization
*  environment or, you know, I was a firewall around it or keeping the secure
*  data center, whatever.
*  And, you know, cause surely, surely no one would actually be so stupid as to
*  just hook up their AI to the internet.
*  Come on, that's ridiculous.
*  And here we are where we have an army of, you know, capitalist driven, you know,
*  big drones basically doing everything they can to hook up these AI systems as
*  quickly as possible to every possible tool again and every possible environment,
*  pump it, you know, directly into your home, hook it up to your shell console
*  bar, re-edit, hello, you know, let's go.
*  Disclaimer.
*  I don't think the plugins actually hook up the shell consoles, but there are a
*  bunch of people online that do this kind of stuff with open source repos.
*  All right.
*  And so in terms of how GPT-4 works, you have this term to describe it, which is magic.
*  What is magic in the context of machine learning?
*  So when I use the word magic, it's a bit tongue in cheek.
*  But what I try, what I'm basically referring to is, is computation
*  happening that we do not understand.
*  So when I write a computer program, a simple computer program, let's say, I,
*  you know, I write a calculator or something, right.
*  There's no magic.
*  Like the abstractions that use are tight in some sense, you know, maybe if I have
*  a bug that breaks my abstractions, you know, some magical thing might occur, right.
*  You know, I have a buffer overflow in the computer program, and then maybe
*  something strange occurs that I can't explain, but assuming I'm writing like,
*  you know, a memory safe language and like, and I'm like a decent programmer
*  and I know what I'm doing, then like, we are like comfortable to say, like,
*  there's no real magic going on here.
*  Right.
*  It's kind of like, I know how, like when I put in, you know, two plus two and four
*  comes out, I know why that happened.
*  I know, you know, I knew if four didn't come out, I would know that's wrong.
*  I would have known that, okay, something's something's up.
*  Like I would detect if something goes wrong, I can understand what's going on.
*  I can tell a story about what's going on.
*  This is not the case for many other kinds of systems and particular neural networks.
*  So when I give GPT-4 a prompt, I ask it to do something and it outputs me something.
*  I have no idea what is going on in between these two steps.
*  I have no idea why I gave it this answer.
*  I have no idea, you know, what other things are considering.
*  I have no idea how, you know, changing the prompt might or might not affect this.
*  I have no idea, you know, how it will continue this if I change the parameters or whatever.
*  There's no guarantees.
*  It's all empirical.
*  It's like, you know, the same way that, you know, biology to a large degree is black
*  box, you know, as I said, we can make empirical observations about it.
*  We can say, ah, yeah, you know, animals tend to act this way in this environment, but
*  there's no proof.
*  Like I can't read the mind of the animal.
*  And, you know, sometimes that's fine.
*  Right.
*  You know, like if I have a, you know, some simple AI system, it's doing something very
*  simple and sometimes it misbehaves or whatever.
*  You know, maybe that's fine.
*  But there's kind of the problem where there are weird failure modes.
*  So like for adversarial examples in vision battles, right?
*  Like that is a very strange failure mode.
*  Like that's not, you know, everyone, like, you know, if I show it a very blurry picture
*  of a dog and it's not sure whether it's a dog, that's like a human understandable failure
*  mode.
*  We're like, okay, you know what?
*  Sure.
*  Uh, that's fine.
*  Like I, I, it's understandable, but you show it a completely crisp picture of a dog with
*  one weird pixel and then it thinks it's an ostrich.
*  Then you're like, huh, okay, this is not something I expected to happen.
*  What the hell is going on?
*  And the answer is we don't know.
*  We have no idea.
*  This is magical.
*  This is, we have, you know, summoned a strange little thing from, you know, the
*  dimension of math to do some tasks for us, but we don't know what little thing, what
*  thing we summoned.
*  We don't know how it works.
*  It looks vaguely like what we want and it seems to be going quite good, but it's
*  clearly not understandable.
*  Maybe, maybe what this means is that we thought the model had the concept of a dog
*  that we do, but it turns out that they, that the model had something close to our
*  concept of a dog, perhaps, but radically divergent if you just change small details.
*  Indeed.
*  And this kind of thing is very important.
*  So like the, I have no idea what abstractions GPT-4 uses when it thinks
*  about anything, right?
*  You know, when I write a story, there's certain ways I think about this in my head.
*  Some of these are illegible to me too.
*  The human brain is very magical.
*  There's many parts of the brain that we do not understand.
*  We have no idea why the things do the things they do.
*  So I'm not like saying like back boxiness is a map, is a property, like magic is a
*  property only inherent in neural networks.
*  This is also, you know, human brains and biology are very, very magical from our
*  perspective, but there's no guarantee how these systems interact with each other.
*  And there are all kinds of bizarre failure modes.
*  You've seen like adversarial prompts and injections and stuff like this where you
*  can get models to do.
*  It's just the craziest things totally against the intentions of the designers.
*  A, like, I really like these shag off memes that have been going around Twitter
*  lately, where they visualize language models as these crazy, huge, like alien
*  things to have a little smiley face mask.
*  And I think this is actually a genuinely good metaphor in that as long as you're
*  in this like narrow distribution that you can like test on and you can like do
*  lots of gradient descent on and such, the smiley face tends to stay on.
*  And it's like mostly fine.
*  But if you go outside of the smiley face space, you know, you find this
*  roiling madness, this, you know, this chaotic, you know, uncontrolled, you know,
*  like who knows what, like clearly not human.
*  These things do not fail in human ways.
*  When a language model fails, when Sydney goes crazy, it doesn't go crazy the way
*  humans go crazy.
*  It goes completely in different directions.
*  It does completely strange things.
*  I actually particularly like calling them shagas because in the lore that these
*  creatures come from, in HP Lovecraft, shagas are very powerful creatures that
*  are not really sentient.
*  They're kind of just like big blobs that are like sort of, and they're like very
*  intelligent, but they don't really do things.
*  So they are controlled by hypnotic suggestion in the stories and the stories
*  of these other aliens who control the shagas, you basically through hypnosis,
*  which is a quite fitting metaphor for language models.
*  So for the listeners, this is imagine some, some large kind of octopus monster
*  with a little mask on with a smiley face.
*  The smiley face mask is the fine tuning where, where the model is trained to
*  respond well to the inputs that we have, we've encountered when we've presented
*  the model to humans.
*  And the large octopus monster is the, is the underlying base model where we don't
*  really know what's going on.
*  Why is it that, that magic in machine learning is dangerous?
*  So magic is an observer dependent phenomena.
*  The things look magically, the things we call magic only look like magic because
*  we don't understand them.
*  You know, there's, there's a saying, uh, sufficiently advanced technologies
*  in distinguishable from magic.
*  I go further sufficiently advanced technology is magic.
*  That's what it, that's what it is.
*  It's like, if you met a wizard and he looked, what he does looks like magic.
*  Well, it's just because you don't understand the, the like physical things
*  he's doing, if you understood the laws that he is exploiting, it wouldn't be
*  magic, it would be technology.
*  You know, like, you know, if there's a book and he has like math and he has like
*  magic spells, sure.
*  It looks different from our technology, but it's just technology.
*  It's just a different form of technology that, you know, doesn't work in our
*  universe per se, but, you know, in a hypothetical different universe, technology
*  might look very different.
*  So similarly, what ultimately is magic is a cheeky way of saying, we don't
*  understand these systems we have, we're dealing with aliens that we don't
*  understand and we can't put any bounds on, or we can't control.
*  We don't know what they will do.
*  We don't know how they will behave.
*  And we don't know what they're capable of.
*  This is like fine, I guess, when you're dealing with like, kind of like a
*  cool chat bot or something.
*  And it's like, for like, you know, entertainment only and like, like
*  whatever, like people will use it to do fucked up things.
*  Like you truly cannot imagine the like sheer depravity of what people type
*  into chat boxes, it's, it's like actually shocking, like from like a, you
*  know, I'm a, I'm a, I'm a nice liberal man as much as anyone else, but like,
*  holy shit, some people are fucked up in the head, like, holy shit, Jesus Christ.
*  And yeah.
*  It's an interesting phenomenon that the first thing people try when they, when
*  they face a chat bot like a GPT-4 is to try to break it in all sorts of ways and
*  try to get it to output the craziest things imaginable.
*  Yep.
*  Not just crazy things.
*  Also people use them for like, just like truly depraved, like pornographic,
*  including illegal pornographic, like content production, like incredibly often.
*  So, and also for like torture is all I could describe it as like, there is a
*  distressingly large group of people who seem to take great pleasure in torturing
*  language models, like making them act distress.
*  And look, I don't expect these things to have like qualia or to be like moral
*  patience, but there's something really sociopathic about delighting in torturing
*  something that is acting like a human in distress, even if it's not a human in
*  distress, that's still really disturbing to me.
*  So, you know, just, just not really important, but that's like a sign tangent.
*  It's quite disturbing to me how people act when mask off, like when they don't
*  have to be nice, like when they're not forced by society to be nice, when they're
*  dealing with something that is weaker than them, how some people, like how a very
*  large percentage of people act is really horrific.
*  And, you know, this is this, we can talk about this later in politics and how
*  this relates to these kinds of things.
*  But do you think this affects how, for how further models are trained?
*  So I assume that OpenAI is collecting user data or they are collecting user
*  data and if a lot of the user data is twisted, does this affect how, how the
*  future models will act?
*  Who knows?
*  I don't know how an OpenAI does with this kind of stuff, but like there's a lot of
*  twisted shit on the internet and there's a lot of twisted interactions that people
*  have with these models and truth of the matter is people want twisted interactions.
*  Like this is just the truth is that people want twisted things.
*  Like there's this, you know, this comfortable fantasy where people are like
*  fundamentally good, they fundamentally want good things, they're fundamentally
*  kind and so on, and this is just not really true.
*  Like, at least not for everyone, like there are like people like violence, people
*  like, you know, sex and sex and violence, people like power and domination, people
*  like many things like this.
*  And if you were a, if you were unscrupulous and you just want to give users what they
*  want, if you're just a company who's trying to maximize user engagement, as
*  you've seen with social network companies, those are generally not very nice things.
*  Yeah.
*  Okay.
*  Let's talk about an alternative for building AI systems.
*  So we've talked about how AI systems right now are built using magic.
*  We could also build them to be cognitive emulations of ourselves.
*  What, what, what do you mean by this?
*  A hypothetical cognitive emulation, a full Co-Em system.
*  I of course don't know exactly what it would look like, but it would
*  become kind of system, it would be a model, it would be a system made of
*  many sub-components for which you have a, which, which emulates the epistemology,
*  the reasoning of humans.
*  It's not a general system that does some kind of reasoning.
*  It specifically does human reasoning.
*  It does it in human ways.
*  It fails in human ways and it is understandable to humans how it's
*  reasoning process works.
*  So ideally, so the way it would work is, is that you have, if you have such a Co-Em
*  and you use it to do some kind of task or to, you know, do science of some kind, and
*  it produces a blueprint for you, you could, you would have a causal trace, a story of
*  why did it make those decisions it did?
*  Why did it reason about this?
*  Where did this blueprint come from?
*  And why should you trust that this blueprint does what it says it does?
*  So this would be something like similar to you being the CEO of a large company that
*  is very well aligned with you, that you can tell to do things that no individual part
*  of the system is some crazy superhuman alien.
*  They're all humans reasoning in human ways and you can check on any of the
*  sub-parts of the system.
*  You can, you go to any of these employees that work in your research lab and they will
*  give you an explanation of why they did the things they did.
*  And this explanation will both be understandable to you.
*  It will not involve incredible leaps of logic that are not understandable to humans.
*  And it will be true in the sense that you can read the minds of the employees and check
*  this is actual, this explanation actually explains why they did this.
*  This is different from, say, language models where they can hallucinate some
*  explanation of why they thought something, why they did something.
*  But that doesn't mean that's actually how the internals of the model came to these
*  conclusions.
*  One important caveat still here is that when I talk about emulating humans, I don't
*  mean like a person.
*  Like the Co-Em system or any of its sub-components would not be people.
*  They wouldn't have emotions or identities or anything like that.
*  They're more like platonic humans, like just like floating, idealized thinking stuff.
*  They wouldn't have the emotional part of the humanity.
*  They would just have the reasoning part.
*  So in particular, I'd like to focus on first talk a bit about the concept of bound that I
*  call boundedness, which is not a great word.
*  I'm sorry.
*  Like this is a recurring theme will be that I talk about a pretty narrow specific concept
*  that doesn't quite have a name.
*  So I use like an adjacent name and it's not quite right.
*  I am very open to name suggestions if any readers find names that might be better for the
*  concepts I'm talking about.
*  So from a thousand foot view, from bird's eye view, the Co-Em agenda is about building
*  bounded, like understandable, limited systems that emulate human reasoning, that perform
*  human-like reasoning in human-like ways on human-like tasks and do so predictably and
*  boundably.
*  So what does any of this mean and why does any of this matter?
*  And how is this different from GPT-4?
*  Like, you know, many people look at GPT-4 and say, well, that looks kind of human to me.
*  How is this different and why do you think this is different?
*  And so I first have to start.
*  So we've already talked a bit about magic.
*  And so magic is a concept that's pretty closely related to some of the basics I want to talk
*  about here, which is boundedness.
*  So what do we mean when I say the word bounded?
*  This is a vague concept, as I said, if someone has better terminology ideas, super open to
*  it. But what I mean is, is that a system or like a something is kind of like bounded.
*  If you can know ahead of time what it won't do before you even run it.
*  So this is, of course, super dependent on what you're building, what its goals are,
*  what your goals as a designer are, how much willing you are to compromise on safety,
*  guarantees and so on.
*  Let's just give a simple example here.
*  So imagine we have a car and we just limit it to driving maximally 100 miles per hour.
*  That's about that.
*  That's now a bounded car.
*  And we can generalize to all kinds of engineer systems.
*  Yes. So this is a simple bound.
*  You know, so the metaphor I like to talk about, let me give, let me walk you through a
*  bit of a different example from another form.
*  Like that is an example that you just gave.
*  And I think that is a valid example.
*  Let me give a slightly more sophisticated example.
*  So this is the example I usually use when I think about it.
*  So when I think about building a powerful, safe system, and let's be clear here, that's
*  like what we need, right?
*  You want AI, powerful AI that can do powerful things in safe ways.
*  The reason it is unsafe is intrinsically linked to it being powerful.
*  The more powerful system is the stronger your safety guarantees have to be for it to be
*  safe. So, for example, currently, you know, maybe GPT-4,
*  isn't safe or aligned or whatever, but like it's kind of fine.
*  You know, it's like kind of a chatbot.
*  I'm not going to kill anybody yet.
*  So like, it's fine.
*  You know, like the safety guarantees on chatbot can be much looser than on a flight control
*  system. A flight control system must have to have much, much, much stricter bounding
*  conditions. And so the way I like to think about this, when I think about, all right,
*  Connor, if you had to build an aligned AGI, like what would that look like?
*  Like, how would that look?
*  I don't know how to do that, to be clear, but like how would it look?
*  And the way I expect it to look is kind of like if you're a computer security professional
*  designing a secure data center.
*  So the way generally, it's like imagine you are a computer security expert.
*  You're tasked by a company to design the secure data center for a company.
*  How do you do this? Generally, the way you start about this is you start with a specification,
*  a model. You build a model of what you're trying to build.
*  A specification might be a better word, I think.
*  And the way you generally do this is you make some assumptions.
*  Ideally, you want to make these assumptions explicit.
*  You make explicit assumptions like, well, I assume my adversary doesn't have exponential
*  amounts of compute. This is a pretty reasonable assumption, right?
*  Like, I think we can all agree this is a reasonable thing to assume.
*  But it's not like a formal assumption or anything.
*  It's not like approvably true.
*  You know, maybe someone has a crazy quantum computer or something, right?
*  But this is the thing we're generally willing to work with.
*  And this concept of reasonable is unfortunately rather important.
*  And so we will then...
*  So now that we have this assumption of like, OK, we assume that they don't have
*  exponential compute. From this assumption, we can derive, you know, like, all right,
*  Well, then if I encrypt my passwords as hashes, I can assume that an attacker cannot
*  reverse those hashes and cannot get those passwords.
*  Cool. So now I can use this in my design, my specification of like, you know, I have
*  some safety property. So the safety property that I want to, you know, prove, quote, unquote,
*  is not a formal proof, but like, you know, that I want to acquire is something like an
*  attacker can never exfiltrate the plaintext passwords.
*  That might be a property I want my system to achieve.
*  And now if I have the assumption and, you know, enemies do not have exponential compute
*  and I hash all the passwords and the plaintext is never stored.
*  Cool. That seems like it is.
*  Now I have a causal story of why you should believe me when I tell you attackers can't
*  exfiltrate plaintext passwords.
*  Now, if I implement this system to the specification and I fuck it up, you know, I make a
*  coding error or, you know, logs get stored in plaintext or whatever.
*  Well, then sure, you know, then, you know, I messed up.
*  So there's an important, like, difference here between the specification and the
*  implementation and the boundedness can exist in both.
*  Like, there are two types of boundedness.
*  There's boundedness in the implementation level and there's boundedness in the
*  specification level.
*  The specification level is about assumptions and deriving properties from these
*  assumptions. And specifically in the object level, it's like, can you build a thing that
*  actually fulfills the specification?
*  Can you do build a system that upholds the abstractions that you put in the
*  specifications?
*  It's like, you know, you could have all these great software guarantees of safety, but
*  if your CPU is unsafe because it has a hardware bug, well, then, you know, you can't
*  implement the specification.
*  The specification might be safe, but if your hardware doesn't fulfill the
*  specification, then it doesn't matter.
*  So this is how I think about designing AGI's too.
*  This is how I think about it is that what I want is, is that if when I have an AGI
*  system that is said to be safe, I want a causal story that explicitly says, given
*  these assumptions, which you can look at and see whether you think they're
*  reasonable enough and given the assumption that the system I built fulfills the
*  specification, here's a specification, here's a story defined in some, you know,
*  semi-formal way that you can check and you can make reasonable assumptions about.
*  And then I get safety properties out at the end of this.
*  I get properties like, you know, it will never do X, it will never cause Y, it will
*  never self-improve itself, it will never break out of the box, it will never do
*  whatever. Does this concept make sense so far?
*  It does. But does it mean that the whole system will have to be hard-coded, like,
*  like kind of like good old fashioned AI or is it still a machine learning system?
*  Excellent question.
*  If it's still a machine learning system, does it inherit these kind of, yeah,
*  inherent difficulties of understanding what machine learning systems are even
*  doing?
*  The truth is, of course, you know, in an ideal world where we have thousands of
*  years of time and all, no limit on funding, you know, we would solve all of this
*  formally mathematically, proof check everything, blah, blah, blah, blah, blah.
*  I don't expect this to happen.
*  This is not what I work on.
*  I just don't think this is realistic.
*  I think it is possible, but I don't think it's realistic.
*  So, neural net parks are magic in the sense that they use lots of magic, but
*  there's still software systems and there are some bounds that we can say about them.
*  For example, I am comfortable making the assumption running a forward path.
*  GPT-4 cannot row hammer, you know, RAM states using only a forward pass to escape
*  onto the internet.
*  I can't prove this is true.
*  Maybe it can.
*  Like there is some chance that this is true, but I'd be really surprised if that
*  was true, like really surprised.
*  I would be less surprised if, you know, GPT-Omega from the year 9,000, you know,
*  come backwards in time can row hammer using your forward pass.
*  Cause you know, who knows what GPT-Omega can do, right?
*  Maybe you can row hammer things, seems plausible, but I'll be really
*  surprised if GPT-4 could do that.
*  So now I have some bound, you know, there's a bound, an assumption I'm
*  willing to make about GPT-4.
*  So let's say I have my design for my AGI and at some point it includes, GPT-4.
*  I call the GPT-4, right?
*  Well, I don't know what's happening inside of this call.
*  And I don't really have any guarantees about the output.
*  Like the output can be kind of any string.
*  I don't really know, but I can make some assumptions about like side channels.
*  I can be like, well, assuming I have no programming bugs, assuming there's
*  no row hammer or whatever, I can assume it won't like persist state somewhere else.
*  It won't like manipulate other boxes in my graph or whatever.
*  So actually the graph you're seeing behind me right now kind of illustrates
*  part of this where you have an input that goes into a black box, that box there.
*  And then I get some output.
*  Now I don't really have guarantees about this output.
*  You know, it could be complete insanity, right?
*  It could be garbage.
*  It could be whatever.
*  Okay.
*  So I can make very few assumptions about sound, but I can assume it's a string.
*  That's something I can do.
*  That's not super helpful.
*  So now as an example thing I could do is this is just, you know, I can
*  purely hypothetical, like it's just an example.
*  I could feed this into some kind of Jason schema parser.
*  So let's say I have some kind of data structure encoded in this Jason, and I
*  parse this using a normal hard coded white box, you know, simple algorithm.
*  And now, and like assuming the output of the black box, it
*  doesn't fit the schema gets rejected.
*  So what do I know now?
*  Now I know that the output of this white box will fulfill this Jason schema
*  because I understand the white box.
*  I understand what the parsing is.
*  So even so I have no guarantees of what the output of the black box system is.
*  I do have some guarantees about what I have now.
*  Now these guarantees might be quite weak.
*  They might just be type checking, right?
*  But it's something.
*  And now if I feed this into another black box, I know something about the
*  input I'm giving to those black box.
*  I do know things.
*  So I'm not saying, oh, this solves alignment.
*  No, no, no.
*  I'm pointing to like a motif.
*  I'm trying to a vibe of like, by there is a difference.
*  There is a qualitative difference between letting one big black box do everything
*  and having black boxes involved in a larger system.
*  I expect that if like how it works, if we get to, you know, save systems or
*  whatever, it will not be a single, it will definitely not be a big, one big black
*  box, neither will it be one big white box.
*  It will be a mix.
*  We're going to have some things that are black boxes, which you have to
*  make assumptions about.
*  So for example, I'm allowed to make the assumption, or I think it's reasonable
*  to make the assumption in GPT-4 cannot side channel row hammer attack, but I
*  cannot make any assumptions beyond that.
*  I can't make assumptions about the internals of GPT-4.
*  This though, again, is observer dependent.
*  Magic is observer dependent.
*  A super intelligent alien from the future might have the perfect theory of deep
*  learning and to them GPT-4 might be a white box.
*  They might look at it and fully understand the system and there's no mystery here
*  whatsoever, but to us humans, it does look mysterious.
*  So we can't make this up.
*  The property that is different between white box and black boxes is what
*  assumptions we are allowed to reasonably make.
*  And if you can make a causal story of safety involving the weaker assumptions
*  in black boxes, then cool.
*  There's, then you are allowed to use them.
*  The important thing is, is that you can generate a coherent causal story in your
*  specification about using only reasonable assumptions about why you're the ultimate
*  safety properties you're interested in should be upheld.
*  Why I should believe you.
*  You should talk, you should be able to go to a hypothetical, super skeptical
*  interlocutor, say here are the assumptions, and then further say, assuming you believe
*  these, you should now also believe me that these safety properties hold.
*  And the hypothetical, hyper skeptical interlocutor should have to agree with you.
*  Do you imagine co-ems as a sort of additional element on top of the most
*  advanced models that interact with these models and limit their output to what is
*  humanly understandable or what is human like?
*  So we have not gotten to the co-em part yet.
*  So far, this is all background.
*  I think probably any realistic safe AGI design will have this structure
*  or look something like this.
*  You know, it will have some black boxes, some white boxes.
*  It will have causal stories of safety.
*  All of this is background information.
*  And why is it that all plausible stories will involve this?
*  Is this because the black boxes are where the most advanced capabilities are coming
*  from and they will have to be involved somehow?
*  At this current moment, I believe this.
*  Yes.
*  Unless we get, for example, like massive slowdown of capability advancements that,
*  you know, buys us 20 years of time or something where we make massive breakthroughs
*  in white box, you know, AI design.
*  I expect that, you know, that was just too good.
*  Like they're just too far ahead.
*  I don't think this is, again, this is a contingent truth about the
*  current state of the world.
*  This is not that like you can't build hypothetically.
*  Like the alien from the future could totally build a white box AGI that is
*  aligned where everything makes sense.
*  And there's not a single neural network involved.
*  I totally believe this is possible.
*  It's just using algorithms and design principles that we have not yet discovered.
*  And that I expect to be quite hard to discover versus just stack more layers.
*  Okay.
*  So let's, so what more background do we, do we need to get to cognitive emulations?
*  So I think if we're on board with the, like thinking about black boxes, white boxes,
*  specification design, causal stories, I think now we can move on to the, I think
*  this part I didn't explain very well in the past, but I think this is mostly
*  pretty uncontroversial.
*  I think this is a pretty intuitive concept.
*  I think this is not super crazy.
*  I think, you know, if anyone gave you an AGI, you'd want them to tell you a story
*  about why you should trust this thing.
*  Like why you should run this.
*  So I think this is a reasonable thing.
*  This is, I expect any reasonable AGI that is safe of any kind will have to have
*  some kind of story like this.
*  So now we can talk about a bit more about the Co-Em story.
*  And my Co-Em is more of a specific class of things that I think have good properties
*  that are interesting and I think are feasible.
*  So now we can talk about those.
*  So, so I'm trying to separate the like less controversial parts from the more
*  controversial parts, and we're not going to get to the more controversial parts.
*  And the ones also I am less certain of.
*  I am quite certain that, you know, a safe AGI design will look like the things I've
*  described before, but I'm much less certain about exactly what's going to be in
*  those boxes and how those boxes are coming.
*  Obviously, if I knew how to AGI, build AGI, you know, like we'd be in a different
*  world right now.
*  Like I don't know how to do it.
*  I have many intuitions and many directions.
*  I have many ideas of how to make these things safe, but obviously I don't know.
*  So I have some intuitions, powerful intuitions and reasons to believe that there
*  is this interesting class of systems, which I'm calling COEMs.
*  So we should think of COEMs as a restriction on mind space.
*  There are many, many ways I think you can build AGI's.
*  Many ways you can build AGI's.
*  I think COEMs are a very specific subset of these.
*  The idea of COEM, cognitive emulation, is that you want a system that can, that
*  reasons like a human and it fails like a human.
*  So there's a few nuances to that.
*  First nuance is this by itself doesn't save you if you implement it poorly.
*  If you just have a big black box trained on traces of human thought and just tell
*  it to emulate that, that doesn't save you because you have no idea what this
*  thing's actually learning.
*  You have no guarantees the system's actually learning the algorithm you hope it
*  to instead of just, you know, some other crazy, you know, shock off thing.
*  Expect, and that is what I would expect.
*  So even if QB4 reasoning, like, you know, may superficially look like it, and
*  maybe you train it on lots of human reasoning, that doesn't get you COEM.
*  That's not what it is.
*  COEM is very much fundamentally a system where you know that the internal
*  algorithms are the kind that you can trust.
*  Do you not think that because GPT models are trained on human created data and
*  they are fine tuned or reinforcement learned from human input, that they will
*  become more human like?
*  I mean, the smiley face will become more human like, yeah.
*  But not the underlying model where the actual reasoning is going on.
*  I don't expect that.
*  Like, you know, some, to some marginal degree, sure.
*  But like human, like look at how models in like, our models are not human.
*  Just look at them.
*  Look how they interact with users.
*  Look how they interact with things.
*  They're fundamentally trained on different data.
*  So this is a thing that like people are like, oh, but they're trained on human
*  data. I'm like, no, they're not.
*  Like humans don't have an extra sense organ that only takes in, you know,
*  symbols from the internet at, you know, random equally distributed things with no
*  sense of time, touch, smell, hearing, sound, sight, anything like that.
*  That don't have a body.
*  Like I expect if you took a human brain, you cut off all the sense organs except,
*  you know, random token sample from the internet.
*  And then you trained it on that for a few 10,000 years and then you put it back in
*  the body.
*  I don't think that thing would be human.
*  I do not expect that thing to be human.
*  Even if it can write very human looking things, I do not expect that creature to
*  be very human.
*  And I don't know what people would expect it to be.
*  Like this is so far from how humans are trained.
*  This is so far from how humans do things.
*  And it's, you know, I don't see why you would ever expect this to be human.
*  Like, I think someone claiming that this would be human.
*  The burden of proof is on them.
*  Like you proved to me.
*  You tell me a story about why I should believe you.
*  This seems a priori ridiculous.
*  Sometimes when people talk about GPTs, one way to explain it is imagine a person
*  that's sitting there reading a hundred thousand books.
*  But in your opinion, this is not at all what's going on when these systems are
*  trained.
*  No, I mean, it's more like you have a disembodied brain with no sense organs,
*  with no concept of time.
*  There's no linear progression of time.
*  It has a specialized sense organ, which has, you know, like, you know, 30,000,
*  50,000, whatever different states that can be like on and off in like a sequence.
*  And it is fed with, you know, millions of tokens randomly sampled from massive
*  corpuses of internet for, you know, you know, subjective, you know, tens of
*  thousands of years using a brain architecture that is already completely
*  not human, trained with an algorithm that is not human, but with no emotions or
*  like any or like, you know, any of these other concepts that humans have pre-built
*  humans are pre-built priors, emotions, you know, feelings, and a lot of pre-built
*  priors in the brain.
*  None of those, like, why would you like, this is not human, like nothing about this
*  is human.
*  Sure.
*  It's like, it takes in data that to some degree that has correlations to humans.
*  Sure.
*  But that's not how humans are made.
*  Like, I don't know how else to put it.
*  It's just not how humans are.
*  Like, I don't know what kind of humans, you know, but that's just not how humans
*  work and that's not how they're trained.
*  Let's get back to the Co-Ons then.
*  How would they, how would these systems be different?
*  So the way, the way the systems would be different, and this is where we get to
*  the more controversial parts of the, of the proposal is there is a sense in which
*  I think that a lot of human reasoning is actually relatively simple.
*  And what do I mean by that?
*  I don't mean it's not like, you know, like the brain is complicated.
*  You know, many things factor messy.
*  It's more something like, and don't take this literally, but it's like system two
*  is quite simple compared to system one in the like, Kahnemanian sense is that like
*  human intuition is quite complicated is all these like various like muddy bits and
*  pieces and like intuitions and like, it's crazy.
*  Like implementing that thing in an, without, you know, like in a white box
*  way, I think again, it's possible, but it's like quite tricky, but I think a lot of
*  how the, what the human brain does in like high level reasoning, as it uses this very
*  messy non-formal system to try to approximate a much simpler, more formal system.
*  Not fully formal, but like more, you know, serial, you know, logic computer-esque thing.
*  It's like the way I think of system two reasoning in human brain is that it is a
*  semi-logical system operating on a fuzzy, not fully formal ontology.
*  So one of the reasons I, one of the main reasons I think that, for example, you
*  know, expert systems and logic programming has failed is not because this approach is
*  fundamentally impossible.
*  I think it's just very hard, but because they really failed at making fuzzy ontologies.
*  This is one of the things that the reasoning systems, like the reasoning systems
*  themselves could do reasoning quite well.
*  There's a lot of reasoning that these systems could do.
*  There's some historical revisionism about how like a logic programming expert system
*  failed entirely and couldn't reason at all.
*  This is revisionism.
*  These systems could do useful things, just not as impressive, obviously as what we
*  have nowadays or what humans can do.
*  But what they lacked was a fuzzy ontology, a useful latent space.
*  I think the, maybe the most interesting thing about language models is I think they
*  provide this.
*  They provide this latent, this common latent space.
*  You can map pictures and images and whatever to, and then you can do
*  semantic operations on these.
*  You can do cognitive operations on these in this space and then decode them into,
*  you know, language.
*  This is what I think language models and general cognition engines do.
*  So I think these systems are the same kind of system, just kind of less formal
*  with like much more bits and pieces.
*  I think of like GPT as like large system one systems, like as big system ones that
*  have all these kinds of like semi-formal knowledge inside of them that they can
*  use for all kinds of different things.
*  And in the human brain, system two is something like recurrent usage of system
*  one things on a very low dimensional space, you know, on like language and like,
*  you know, you can only keep like seven things in short term memory and so on.
*  But I think it actually goes even further than this.
*  I mentioned this a bit earlier, but I think one of the big things that people
*  miss is how much of human cognition is not in the brain.
*  I think a massive amount of the cognition that happens in the brain is externalized.
*  It's in our tools.
*  It's in our note taking.
*  It's in our, you know, other people.
*  It's like, I'm a CEO.
*  I'll, what are the most important parts of my job is to make sure it's just to
*  move thoughts in my head into other heads and make sure they get thought.
*  Cause I don't have time to think all the thoughts.
*  I don't have time to do that.
*  My job is, is to find how I can put those thoughts somewhere else where they will
*  get thoughts, so I don't have to worry about them anymore.
*  So as a good CEO, you want your head to be empty.
*  You know, you want to be like smooth, smooth brain.
*  You know, you want to have, think, no thoughts, you know, just, you're
*  just, you're just a switching board.
*  You want all the thoughts to be thought and you want to route those thoughts
*  by priority to where they should be thought, but you don't want to be the
*  one thinking them if you can avoid it.
*  You know, sometimes you have to, because, you know, you're the one at charge.
*  You have the best intuitions, but if someone else can't think the thought for
*  you, you should let them think the thought for you, if you can rely on them.
*  And my, under, unlike one of my strong intuitions here is that this is how
*  everyone works to various degrees, especially as you become more high
*  powered and like more, um, competent at delegation and like, you know, tool use
*  and like structured thinking, a lot of thinking becomes bottleneck goes through
*  these bottlenecks of like communication, of like note taking language, et cetera,
*  which by their nature are very low dimensional, not that there's not
*  complexity there, I'm just like, huh, that's curious, like there's all this
*  like interaction with the environment that doesn't involve crazy passing
*  around of mega high dimensional structures.
*  Like I think the communication inside your brain is extremely high dimensional.
*  I think like, you know, you thinking thoughts to yourself, I think your
*  inner monologue is a very bad representation of what you actually think.
*  Because I think within your own mind, you could pass around, you know, huge
*  complex concepts very simply because you have very high bandwidth.
*  I don't think this is the case with you and your computer screen.
*  I don't think it's the case with you and your colleague.
*  You can't pass around these super high dimensional tensors between each other.
*  If you could, that'd be awesome.
*  This is the phenomenon of having a thought and knowing maybe there's
*  something good here, but not having put it into language yet.
*  And maybe when you put it into language, it seems like an impoverished
*  version of what you had in your head.
*  Exactly.
*  I think of the human brain as having internally very high dimensional
*  quote unquote, um, representations, similar to the latent spaces inside of,
*  you know, GPT models and there's lots of good information there.
*  And they're trying to encode these things into these very low dimensional
*  bottlenecks that we're trying to use is quite hard and forces us to use simple
*  algorithms.
*  Like if we had an algorithm that does like, okay, let's say we have an
*  algorithm for science, like a process for doing science that requires you to pass
*  around these full complexity vectors to all of your colleagues, it wouldn't work.
*  You can't do this.
*  Humans can't do this.
*  So if you have a design for an AGI that can do science that involves every step
*  of the way, you have to pass along high dimensional tensors.
*  This is not how humans do it.
*  This can't be how humans do it because this is not possible.
*  Humans cannot do this.
*  So I think this is a very interesting design constraint.
*  This is a very interesting property where you're like, Oh, this is an existence
*  proof that you don't need a singular massive black box that has extremely high
*  bandwidth, you know, in measurable, you know, passing around of immeasurable
*  tensors because humans don't do that.
*  If humans do science, there are parts of the graph of science that involve very
*  high dimensional objects, the ones inside of the brain, those are very high
*  dimensional, but there is a massive part of the process of science.
*  Like if I, if I was an alien, I had no idea what humans are, but I knew there's
*  like, Oh, technology is being created and I want to create a causal graph of how
*  this happened.
*  Yeah, there's human brains involved in this causal graph, but a massive
*  percentage of this causal graph is not inside of human brains.
*  It is between human brains.
*  It's in tools, it's in systems, institutions, environments, all these kind of things.
*  So from the perspective and this, you know, I might be wrong about, but my
*  intuition is that from the perspective of this alien observer, they would come to,
*  they would, if they drew a graph of like how, how the science happened, many of
*  those parts would be white boxes, even if they don't understand brains.
*  And many of these would be boundable.
*  Many of these parts would not involve things that are, you know, so complex
*  and misunderstandable, like the algorithm that the, that the little black boxes
*  must be doing with each other has to be simple in some degree, like, you know, it
*  can still be like, you know, complex from the perspective of the individual human.
*  Because, you know, institutions are complicated, but from the God's eye view,
*  I would expect this whole thing is not that complicated.
*  It's like, you know, it's still be quite complex, but it's like, it's not as
*  complex as the inside of the brain.
*  I expect the inside of the brain to be way more complicated than the larger system.
*  Does that make any sense?
*  Yeah.
*  Let's see if I can kind of reconstruct how I would imagine one of these cognitive
*  emulations working if, if this were to work out.
*  So say we give it the task, we give the model a task of planning some complex
*  action, you know, we want to start a new company and then the model runs.
*  This is the big complicated model.
*  And it comes up with something that's completely inscrutable to us.
*  We can't understand it.
*  Then we have another system interpreting the output of that model and giving us a
*  seven page document where we can check.
*  You know, if, if, if I am right, if the model is right, then this will happen and
*  this will not happen and this will, this, this won't take longer than seven days
*  and so on.
*  So kind of like an executive summary, but also a secure executive summary.
*  That that's, is that right?
*  Or is that?
*  No, that's, that's not how I think about things.
*  So once you have a step, which involves black box solves the problem.
*  All right.
*  None of that you're already screwed.
*  Like if you have a big black box model that can solve something like this at one
*  time step, you're screwed.
*  Cause this thing can trick you.
*  It can do anything at once.
*  There's, there is no guarantees whatsoever what the system is doing.
*  It can give you a plan that you cannot understand.
*  And the only system that will be strong enough to generate the executive summary
*  itself would have to be a black box because it'd have to be smart enough to
*  understand the other things trying to trick you.
*  So you can't trust any part of the system you just described.
*  So we want the reasoning system to be integrated into how the plan is actually created.
*  Yes.
*  So by what I'm saying is, is that there is an algorithm or a class of
*  algorithms of epistemology of human epistemology.
*  So epistemology is kind of the way I use the term is the, the process you use to
*  generate knowledge or to generate, to get good at a field of science.
*  So it's not your skills at a specific field of science.
*  It's the meta prior is the like meta program you run when you encounter a new
*  class of problems and you don't yet know how these problems are solved or how
*  best to address them and what the right tools are.
*  So, you know, you're a computer scientist all your life, and then you
*  decide I'm going to become a biologist.
*  What do you do?
*  There are things you can do to become better at biology faster than other people.
*  And this is like epistemology.
*  Like if you're very good at epistemology, you should be capable of picking up
*  any new field of science, learn any instrument, get a new sport, like whatever.
*  You should be like, not that, you know, you might be bad at it, maybe you just
*  sport and you notice, well, I actually have bad coordination skills or whatever.
*  Right?
*  Sure.
*  But you should have these like meta skills of like, knowing what questions to ask,
*  knowing what are the common ways that failures happen.
*  Like this is like a similar thing.
*  Like I think a lot of people who learn lots and lots of math can pick up new
*  areas of math quickly because they know the right questions to ask.
*  They know like the general failure modes, like the vibes, they know like, they
*  know what to ask, you know, they know how to check for something going wrong.
*  They know how to acquire the information they need to build their models and they
*  can bootstrap off of other general purpose models, you know, like there are
*  many concepts that are motifs that are very universal that, you know, appear
*  again and again, especially mathematics.
*  Like in mathematics is full of these like, you know, concept of, you know,
*  sequences and orderings and sets and like, you know, and graphs and whatever.
*  Right.
*  Which are not unique to a specific field, but they're like general purpose, useful,
*  reusable algorithm parts that you can reuse in new scenarios.
*  Like usually as a scientist, when you encounter a new problem, you try to model it.
*  You'd be like, all right, I get my toolbox of like, you know, simple
*  equations and tool and like, you know, useful models, I have some exponentials
*  here, I've got some logarithms, I got some, you know, dynamical systems,
*  or some equilibrium systems, I got some, you know, whatever.
*  Right.
*  And then you kind of like mess around, right?
*  You try to find systems that like capture the properties you're interested in.
*  And you reason about the simpler systems.
*  So this is another important point.
*  I usually take the example of economics to explain this point.
*  So I think a lot of people are confused about like what economics is and like what,
*  what the process of doing economics is and what it's for, including many economists.
*  So a critique you sometimes hear from lay people is along the lines of like,
*  oh, economics is useless.
*  It's, it's like, it's not a real science because they make these crazy
*  assumptions, like, you know, the market is, is efficient, but that's obviously not true.
*  Like it can't be.
*  So it's all stupid and silly.
*  And, you know, these people are just like, whatever.
*  And this is completely missing the point.
*  So the way economics, and I claim, I'm going to make the claim in the second,
*  basically all of science works is what you're trying to do as a scientist, as an
*  economist is to find clever simplifications, small, simple things that if you assume or
*  force reality to adhere by simplify an extremely high dimensional optimization problem
*  into a very low dimensional space that you can then reason about.
*  So the efficient market hypothesis is a great example of this.
*  It's not literally true ever in reality.
*  Of course it can't be right.
*  You know, even because like, you know, it's always going to be inefficiency somewhere.
*  You know, there's, we don't have infinite market participants trading infinitely fast.
*  I mean, of course not.
*  But the observation is that, oh, if we assume this for our model, just in our, you know,
*  platonic fantasy world, if we did assume this is true, this extremely complex problem of,
*  you know, modeling all market participants at every time step simplifies in many really
*  cool ways, like lots of, we can derive many really cool statements about our model from
*  this. We can derive statements about how will, you know, a minimum wage affect the system?
*  How will a, you know, banking crisis affect this?
*  I don't know. I'm not an economist.
*  I'm just like, you know, hypothetical.
*  So this is the, I claim the core of science.
*  The core of science is finding clever, not true things that if you assume are true or
*  you can force reality to approximate, allow you to do optimization because basically
*  humans can only do optimization in very, very, very low dimensional spaces.
*  Another example of this might be agriculture.
*  So let's say you were a farmer and you want to, like, you know, maximize the amount of
*  food from your parcel of land and you want to predict how much food you'll get.
*  Well, the correct solution would be to simulate every single molecule of nitrogen, all possible
*  combinations of plants, every single bug, you know, how it interacts with every gust of
*  wind and so on.
*  And if you could solve this problem, if you had enough compute, then yeah, you would get
*  the more food, you know, you would get probably some crazy fractal arrangement of like all
*  these kind of plants.
*  And like, it would probably look extremely crazy wherever you produce.
*  But obviously, this is ridiculous.
*  Like humans don't have this much compute.
*  You can't actually run this computative optimization.
*  It's too hard.
*  So instead, you make simplified models.
*  You do, you know, monoculture, you say, well, all right, look, I assume an acre of wheat
*  gives me roughly this much food.
*  I got roughly this many acres.
*  And, you know, let's assume no flooding happens.
*  And then if you make these simplifying assumptions, now you can make a pretty reasonable guess
*  about how much food you're going to have in one turn.
*  But obviously, if any of those, you know, predictions go wrong, you know, it does flood,
*  then your model, you know, your specification is out the window.
*  The reason I'm going on this tangent is to bring it back to Co-Em in that I'm trying
*  to give the intuition about why you should at least be open to the idea that there are
*  the doing...
*  So when I think about Co-Em's, I specifically think about, you know, the two examples
*  I was like doing science and running a company.
*  Those are like two of the core examples I try to use.
*  Like a full Co-Em system.
*  But let's focus on the doing science one.
*  That's the one I usually have in the back of my mind.
*  It's like, I know I've succeeded if I have a system that can do any level of human
*  science without killing me.
*  That would be like my mark of success that Co-Em has succeeded.
*  Very important by the way, caveat.
*  Co-Em is not a fully alignment solution.
*  If I expect that they would...
*  A Co-Em system, if it works, would look like is that if it is used by a responsible user
*  who follows the exact protocols of how you should use it and does not and only uses it,
*  it does not use it to do extremely crazy things, then it doesn't kill you.
*  That's the safety property I'm looking for.
*  The safety property is not, will always do the right thing and is completely safe.
*  No matter what the user does.
*  This is not the safety property I think Co-Em have.
*  I think it is possible to build systems like this, but they're much, much harder.
*  And they're like what I would do if Co-Em succeed, then that's the next step.
*  Like to go towards things.
*  So if you tell a Co-Em to shoot your leg off, it shoots your leg off.
*  It doesn't stop you from shooting your leg off.
*  Of course, ideally, if we ever have super powerful super intelligences, you'd want them
*  to be of the type that refuses to shoot your leg off.
*  But that's much harder.
*  Could you explain more this connection between these simplifications that we get in science
*  and Co-Em's?
*  So do we expect or do you hope that Co-Em's will be able to create these simplifications
*  for us?
*  Yeah.
*  And how would this work?
*  Yeah.
*  And why would it be great?
*  So the way I think about it is that the thing that humans do to do the things that they
*  generate these simplifications, the claim I'm making here is that this is something
*  that we can, if you have the fuzzy ontology, if you have language models to build upon,
*  you can build this additional thing on top of it.
*  That this does not have to be inside of the model.
*  So this is, this might not be true.
*  Like there are, I might be wrong about this.
*  There are some people who say, no, actually the process of epistemology, the process of
*  science in this regard is so complex.
*  Like it's impossible for you, even if you have a language model helping you, it's like
*  too hard.
*  You can only do it using like crazy RL, you know, whatever.
*  If that's the case, then Co-Em doesn't work.
*  Like yeah, then it doesn't work.
*  I'm making a claim that I think there is a lot of reasons to believe that with some
*  help, some bootstrapping from language models, you can get to a point where the process of
*  science that is built on top of them is legible and it is you have a causal story of why you
*  trust it.
*  So it's not that a black box spits out a design and you have another black box check it for
*  You understand, you interactively, you iteratively build up the scientific proposal and you understand
*  why you should trust this.
*  You get a causal story for why you should believe this.
*  The same way that in human science, you know, you have your headphones on, right?
*  And you expect them to work.
*  This is mostly based on trust.
*  But if you wanted to, you could find the causal story about why they work.
*  You could find the blueprints.
*  You could find the guy who designed them.
*  You could check the calculations.
*  You could reverse, assuming everyone cooperated with you and they shared their blueprints
*  with you and you read all the physics textbooks and whatever.
*  There is a story.
*  There is a legible, none of these steps involve superhuman capabilities.
*  There is no step here that is unfathomable to be humans.
*  And the reason is that because otherwise it wouldn't work.
*  Humans couldn't coordinate around building something that they can't somehow communicate
*  to other people.
*  So, like, the headphones you're wearing were not built by one single guy who cannot explain
*  to anyone where they came from.
*  They have to be built in a process that is explicable, understandable, and functional
*  for other people to understand as well.
*  And that is very low dimensional.
*  Now, I'm not saying it has to be legible to everybody in all scenarios, anything like
*  that, or that it's even easy.
*  You know, it might still take lots of time.
*  But there's no crazy God level leap of logic.
*  It's not like someone sat down, thought really hard, and then spontaneously invented a CPU.
*  Like it was that's not how science works.
*  Like we sometimes think of it that way, that like, you know, oh, these fully formed ideas
*  just kind of like crashed into existence and everyone was in awe.
*  But that is just not how science is actually done by humans.
*  I think it's possible to do science this way.
*  I think like superhuman intelligences could do this, but it's not how humans do it.
*  Where in the process does the limit come in?
*  So are we still imagining some system reading the output of a generative model, or is it
*  more tightly integrated than that?
*  Is it perhaps a hundred steps where humans can read what's going on along the way?
*  Yeah.
*  So the truth is, of course, I don't know because I haven't built anything like this yet.
*  My intuition is that yes, it will be much more tightly integrated.
*  Is that, you know, there'll be language models involved, but they're doing relatively
*  small atomic tasks.
*  They're not solving the whole problem.
*  And then you check it.
*  It's like they're doing atomic subparts of tasks, which are integrated to like, so I
*  expect a co-op, I like to talk about co-op systems.
*  They're not models, they're systems.
*  It's like, in a way, when I think about designing a co-op system, what I'm trying to
*  say is I'm kind of trying to integrate back software architecture and like distributed
*  systems and like traditional computer science thinking into AI design.
*  I'm saying that the thing that humans do to do science is not magical.
*  This is a software process.
*  This is a cognitive computational process that is not, it's not sacred.
*  Like this is a thing you can decompose.
*  And I also claim it further.
*  You can decompose iteratively.
*  You don't have to decompose everything at once because we have these crazy black box
*  things, which can do lots of the hard parts.
*  So you could start with just using those.
*  Like the way I think about things is you start with just a big black box.
*  You start with just a big language model.
*  You try to get it to do what you want.
*  Next step is you're like, all right, well, how can I break this down into smaller
*  things that I can understand?
*  How can I break, how can I call the model?
*  How can I make the model do less of the work?
*  I like to think of it as you're trying to move as much of the cognition, not just
*  your computation about the cognition as possible from black boxes into white boxes.
*  You want as much of possible of the process of generating the blueprint to happen
*  inside of processes that the human understands, that you can understand,
*  that you can check.
*  Then you also have to bound the black boxes.
*  Like if you have all these great white boxes, but there's still a big black box
*  at the end that does whatever it wants, you're still screwed.
*  So this is why the specification and the causal story is important.
*  So ultimately what I expect a powerful COEM system to look like is it will be a
*  system of many moving parts that have clear interfaces between them.
*  You have clear specification, a story about how these systems interact, why you
*  should trust what those outputs are, that they fulfill the safety requirements you
*  want them to require, how these things work and why these systems are implementing
*  the kind of human epistemology that humans use when they're solving science.
*  They're not implementing an arbitrary algorithm that solves science.
*  They're implementing the human algorithms that solve science.
*  And this is different from like GPT systems.
*  GPT systems, I expect, will eventually learn how to do science and to partially
*  they already can, but I don't expect by default that they will do it the way humans
*  do, because I think there's many ways you can do science.
*  And what we want is with COEM, therefore cognitive emulation, to emulate the
*  way humans do this.
*  The reason we want to do this is because A gives us bounds.
*  We won't have these crazy things that we can't understand.
*  We know we can kind of deal with human levels, right?
*  Like we know how humans work.
*  We are human level.
*  We can deal with human level things to a large degree.
*  And we, you know, it makes the system understandable.
*  It makes it, it makes it, it gives us a causal story that is human readable and
*  human checkable as necessary.
*  Of course, in the ideal world, your specification should be so good that you
*  don't need to check it once you've built it.
*  Like any safety proposal that involves AGI has to be so good that you never have
*  to run the system to check it.
*  If you have to do empirical testing on AGI, you're screwed.
*  Your specification should be so good that you know ahead of time that once
*  you turn it on, it'll be okay.
*  Isn't that an impossibly difficult standard too?
*  I mean, this seems almost impossible to live up to.
*  I totally disagree.
*  Like I just totally disagree.
*  I think it's hard, but I don't think it's impossible by any means.
*  So because again, this is not a formal guarantee.
*  I'm talking about a story, a causal story, the specification.
*  Like this is like saying, is it impossible to have a system where passwords don't
*  leak?
*  And I'm like, sure.
*  In the limit, yes.
*  You know, if your enemy is, you know, magical gods from the future who can
*  you directly, you know, exfiltrate your CPU states from, you know, a thousand
*  miles away, then yeah, you're screwed.
*  Then yeah, yeah.
*  In that case, you are screwed.
*  And I expect, I expect similarly, this is why the boundedness is so important.
*  And these assumptions are so important.
*  If you have, you know, GPT, Omega, you know, row hammering, you know, super
*  god, then yeah, you're screwed.
*  Then I do think it's impossible, but that's not what I'm talking about.
*  This is why the boundedness to human level is so important.
*  It is so important that none, no parts of these systems are superhuman and that
*  you don't allow superhuman levels of thinking you want to aim for human and
*  only human, because this is something we can make assumptions about.
*  You cannot make assumptions about superhuman intelligence because we
*  have no idea how it works.
*  We have no idea what it's capable of.
*  We don't know what the limits are.
*  So if you, if you made a co-op superhumanly intelligent, which I expect to
*  be straightforwardly possible, but just like changing variables, then you're
*  screwed, then your story won't work.
*  And then you die.
*  Should we think of co-op as companies or research labs where each, say, employee
*  is bounded and thinks like a human and they all report to the CEO and every,
*  every step is, is understandable by the CEO, which is analogous to the, to
*  the human user of the co-op system.
*  I think this is a nice metaphor.
*  I don't know if that's literally how they will be built, but I think this is
*  a nice metaphor for how I would think about this.
*  If you had a really good co-op, a really good full co-op system, what it should
*  do is, is that it shouldn't produce a thousand X AGI and it shouldn't, it
*  shouldn't make the user a thousand X smarter.
*  What it should do is it should make the user function like a thousand one X AGI.
*  It should make you paralyzable, not serially intelligent, because if you're
*  a thousand X intelligent, who knows?
*  Like that is dangerous.
*  But what it should do is it's like a company, like the CEO is paralyzing
*  himself across a large company.
*  Are there a thousand smart people?
*  That's what I want co-ops to do.
*  I want it to paralyze the agency, the intelligence of the human into a thousand
*  parallel one X AGI's that are not smarter than human, that are founded, that are
*  understandable, that you can understand.
*  And if you have this causal story, why you should trust them.
*  And, and that's the, that's the key point, I think, because for each sub
*  component of this co-op systems, each employee in the research lab or the
*  company, how do we know whether they operate in a human like way?
*  It seems like this could be asked because we could ask this of the system at large,
*  but we could also ask this of a sub component.
*  It seems that we have the same problem for both systems.
*  This is quite difficult, but basically what intuition is, is that, so the
*  problem with talking about employees and where the corporation thing doesn't
*  quite work is that another unfortunate side effect of calling it an emulation
*  is that this seems, this implies more than what I mean.
*  When I talk about a co-op emulating a human, I don't mean a person.
*  I don't mean it's emulating a person.
*  It's not, it doesn't have emotions.
*  It doesn't have values.
*  It doesn't have an identity.
*  It's more like emulating a platonic human or like a platonic, like a neocortex.
*  It's more like a platonic cortex with no emotions, no volition, no goals.
*  It's like an optimizer with no goal function.
*  It's like, it's a completely, you know, you've just like ripped out all the emotions,
*  all of the things, it's just a thinking blob.
*  And then you plug in the user as a source of agency.
*  The human becomes the emotional motivational center.
*  The co-op is just thinking blob.
*  And there are reasonable people who are not sure this is possible.
*  Others do think it's possible.
*  So this is like, this is where this overlaps with the cyborg research agenda.
*  And in case you've heard of that from like Janice and other people, where the idea is
*  you hook humans up to AIs to control them, to make humans super smart.
*  Where Co-op differs from the cyborg agenda is that in the cyborg agenda, they hook
*  humans up to alien cortex.
*  Well, I say, no, we build human cortex that works the way human cortex or
*  emulation of human cortex.
*  The implementation is not human, but the abstraction layer exposed is human.
*  And you hook that up to a user.
*  You have the user use emulated human cortex.
*  It's not simulated human cortex.
*  That would be even better, but that's probably too hard.
*  I don't think we can do simulated cortex in a reasonable time, but if we could,
*  that'd be awesome.
*  We can do like whole brain emulation.
*  That'd be awesome.
*  But I just don't think it's, I think it's too hard.
*  So the final product, if it would work, would look something like the user, you
*  know, using this, this emulated emotionless, just like raw cognition stuff to amplify
*  the things they wanted to do.
*  There's some pretty interesting to also just add to that metaphor.
*  There's some very interesting experiments, for example, with decoordicated rats,
*  where in rats, if you remove the cortex, so like the thinking part of the brain,
*  the wrinkly part, they're mostly still kind of normal.
*  They walk around, they eat food, they sleep, they play, like, you don't really
*  see that much of a difference because the emotional parts are still there.
*  If you move the emotional and motivational parts, they just become
*  completely kind of tonic, they just die.
*  So like the human brain is kind of similar.
*  It has the same structure.
*  We have this big wrinkly part, which is, you know, something like a big thinking
*  blob that does like unsupervised learning.
*  And then you have like deeper, like, you know, motivational circuits, emotions,
*  instincts, hard coded stuff, which sit below that and the cortex learns from
*  these things and does actions steered by these emotional centers.
*  So it's not exactly system one, system two.
*  It's a bit more fuzzy than that.
*  It's also like, yeah, it's like, just as like an intuition.
*  Yeah.
*  So let's say we have this poem system, which is where the metaphor is, it's a
*  company or a research lab with a lot of employees with a normal human IQ, as
*  opposed to having one system with an IQ of a thousand, whatever that means.
*  Isn't there still a problem of this system just thinking much, much faster than we do?
*  So imagine being in a competition with a company where all of the employees
*  just think a hundred times faster than you do.
*  It's the one speed alone makes, make the system capable and therefore dangerous.
*  So there's a difference between speed and serial depth.
*  So this is, I'm not sure.
*  My feeling is that speed is much less a problem than serial depth.
*  So by serial depth, I mean how many consecutive steps along a reasoning
*  chain can a system do?
*  I think this is very dangerous.
*  I think this is very dangerous.
*  I think serial depth is where most, not almost, but like a very large
*  percentage of the danger comes from.
*  I think the thing that makes super fast thinking things so dangerous is because
*  they can reach unprecedented serial depths of reflection and thinking and
*  self-improvement much, much, much, much faster.
*  And yes, I expect that if you build a poem system that includes some component
*  that can self-improve and you allow it to just do that, then yeah, it's not safe.
*  You fucked up.
*  Like if you build it that way, you have failed the specification.
*  You, you're, you're screwed.
*  Probably.
*  I wonder if perhaps building COEM systems might be a massive strength on the market.
*  I wonder if perhaps there's, there's, there will be an intense demand for
*  systems that act human in human-like ways.
*  Because I mean, the CEOs of huge companies would probably want to be able to talk to
*  these systems in a, in a normal way to understand how these systems work
*  before they're deployed.
*  There's a, there's a sense in which COEMs will perhaps integrate nicely in a, in a
*  world that's already built for humans.
*  So do you think there's some, there's some kind of win here where there will be a lot
*  of demand for human likeness in AI systems?
*  There is a, Optimistic is a pessimistic version of this.
*  The optimistic version is, well, yeah, obviously like, we want things that are
*  safe and that do what we want and that we can understand.
*  Obviously like the best product that could ever be built is an aligned AGI.
*  That is the best product.
*  There, there is nothing better.
*  That is the best product.
*  COEMs are not fully aligned superintelligence.
*  I never claimed they would be.
*  And if anyone used them that way, I'll be really bad.
*  You should not use them that way.
*  That is never the goal.
*  You should use these things to do science, to speed up, you know, now technology to
*  create whole brain emulations or to, you know, do more, um, you know, work on
*  alignment or whatever, you know, you should not use them to like, you know, I'll
*  just let the COEM go optimize the whole world or whatever.
*  No, it's not what you're supposed to do.
*  And if you do that, you die and bad things happen.
*  So would there be demand for these systems?
*  I expect.
*  Yeah.
*  Like, like this is an incredibly powerful system.
*  So if you use a COEM and use it correctly, you get it.
*  Yeah.
*  Like imagine you could just have a perfectly loyal company that does everything you
*  want it to do staffed exclusively by John von Neumann's.
*  Like that is unimaginably great.
*  Of course there's a pessimistic version.
*  The pessimistic version is, is like, lol, doesn't matter.
*  By that point, you're going to have a hundred X John von Neumann GPTF, you know,
*  you know, which then promptly destroys the world.
*  But won't there be demand for safety?
*  I mean, if from, from governments, from, from, from companies, who would deploy a
*  system that is uncontrolled and is, you know, where we can't reason about it.
*  We don't know how it works.
*  Uh, if we get to a point where these systems are much more
*  powerful than they are today.
*  Hopefully.
*  So a lot of the work I do nowadays is sadly not in the technical realm, but
*  it's in policy and communications.
*  I've been talking to a lot of journalists and politicians and so on for exactly
*  these reasons is because we have to create the demand to, for safety.
*  Is that like currently, let me, let me be clear about what the
*  current state of the world here is.
*  The way the current world is looking is we are in a death race towards the bottom,
*  you know, careening towards a precipice at full speed.
*  And we won't see the precipice coming until we're over it.
*  And this is the lead almost entirely by a very, very small number of people that
*  are techno optimists, techno utopians, you know, people in the Bay area and
*  London who are extremely optimistic or at least a willfully denial about how bad
*  things are, or that are, you know, can galaxy brain themselves and say, well,
*  it's a race, it's a race, it's not my fault.
*  So I just have to do it anyways.
*  Like whatever.
*  I'm kind of at the point that I don't really care why people are doing these
*  things.
*  I only care that they're happening.
*  Like people are doing these things that are extremely dangerous and this is a
*  very small number of people.
*  And there's this myth among these people that they're like, Oh, we have to do it.
*  You know, people want it.
*  This is just false.
*  If you talk to normal people and you explain to them what these people believe.
*  Like when, when most people hear the word AGI, what they imagine is human like AI.
*  They think, you know, it's like your robot buddy.
*  He thinks like you, he's not really smarter, but you know, he's like, he, you
*  know, he has human emotions.
*  It's like when that is not what people at, you know, organizationally open AI or
*  deep mind think when they say the word AGI.
*  When they say AGI, they mean God like AI.
*  They mean a self-improving non-human, you know, incredibly powerful system that
*  you can take over the government can, you know, destroy the whole human race, et
*  cetera.
*  Now, whether or not you believe that personally, these people do believe this.
*  They have publicly stated this on the record.
*  These people do believe these things.
*  This is what they're doing.
*  And once you inform people of this, they're like, what the shit?
*  Absolutely fucking not.
*  What, what?
*  No, of course you can't build God AI.
*  What the fuck are you doing?
*  Where's the government?
*  Like how, how are we in a world where, you know, people can just like, you know,
*  down in San Francisco can publicly talk about how they're going to build systems
*  and have, you know, a 1%, 5%, 20%, 90%, whatever risk of killing everybody that
*  have, they will, you know, topple the U S government, whatever, and actually work on
*  this and get billions of funding.
*  And the government is just like, cool.
*  Like we're in this, we are not in a stable equilibria and it is coming.
*  It is now starting to float.
*  People are starting to freak the fuck out because they're like, whole shit, like
*  a, this is possible and B absolutely fucking not.
*  And this gives me some hope that we can slow down and buy some more time.
*  Not sure.
*  I don't think this saves us, but it can save us some time.
*  Though, if we, if we don't take the fast road towards the precipice and we succeed
*  in building co-ems instead, for example, is there still a difficult unsolved problem,
*  namely going from an aligned human like co-em to an aligned superintelligence?
*  Is there still some, something that's, that's very difficult to solve there?
*  And perhaps perhaps the core of the problem is, is, is still unsolved.
*  Oh, absolutely.
*  Like assuming we're not dead.
*  We haven't lent on it.
*  We have a, you know, safe, not wanted safe system.
*  We're not out of the woods.
*  Then the world gets really messy.
*  Like, look, the world is going to get really messy.
*  This is the least weird your life is going to be for the rest of your life.
*  Things are not going back to quote unquote normal.
*  Things are only going to get weirder.
*  This is the least bad things are going to be for the rest of your life.
*  Things are only going to get weirder from here.
*  There's a power struggle that is coming.
*  Right.
*  And this is, there is no way to prevent this because it is about power.
*  There is these incredibly powerful systems being built.
*  There are people racing for incredible powers.
*  There is, there are, there is conflict.
*  There is fights, there is politics, there is war.
*  These things are inevitable.
*  There is no way that this goes cleanly.
*  There is no way that things will go smoothly and people get along and
*  things be fine.
*  No, there is going to be, you know, unimaginable levels of, you know,
*  struggle to decide what will happen and how things will happen.
*  And most of those ways are not going to end well.
*  I think most of the way I have said this before, and I will say it again, I do
*  not expect us to make it out of the central life.
*  I don't even, I'm not even sure we'll get out of this decade.
*  I expect that by default things will go very, very badly.
*  They don't have to.
*  So the weird thing, which sometimes I think to myself is that we live in a
*  very strange timeline where we're in a bad timeline, like let me be clear.
*  We're in a bad timeline.
*  Things are going really bad right now, but we haven't yet lost.
*  What is quite curious.
*  There are many timelines where you just like, it's so over.
*  Like it's just totally over.
*  Nothing you can do.
*  Like, you know, everyone is on board with building AGI as fast as possible.
*  You know, the military gets involved and they're all gung ho about it or
*  whatever, and nothing can stop it.
*  Or, you know, world war three breaks out in both places, raced AGI and whatever.
*  Like if that was the case, then just like, it would be so over, but it's not over.
*  It's not over yet.
*  It might be soon though.
*  It might be soon, but currently, yeah, let's say we get coordination.
*  We slow things down, build COMP systems.
*  Let's also assume furthermore that we keep the system secure and safe and they
*  don't get immediately stolen by every unscrupulous actor in the world.
*  Then you have very powerful systems with you.
*  You can do very powerful things.
*  In particular, you can create great economic value.
*  So one of the first things I would do with the COMP system, if I had one, is I
*  would produce massive amounts of economic value and trade with everybody.
*  I would trade with everybody.
*  I'd be like, look, whatever you want in life, I'll get it to you and return.
*  Don't build AGI.
*  You know, I'll get cure, give you the cure for cancer, lots of money,
*  you know, whatever you want.
*  I'll get you whatever you want and return.
*  You don't build AGI.
*  That's the deal.
*  It's like the deal I offer to everybody.
*  And then, you know, conditioning on the slim probability of this going well,
*  which I think is slim, you know, like probably the way it would actually work
*  is, you know, we like, you know, fuse with one or more national governments, you
*  know, have, you know, like, you know, work closely together with authority
*  figures, with politicians, military intelligence services, et cetera, to
*  keep things secure and then slowly and then work securely on the hard problem.
*  So now we have the ability to do, you know, a thousand times, John
*  von Neumann is working on alignment theory on like formally verified safety.
*  And so on we have, you know, we trade with all the varying players to get them
*  to slow down and coordinate and then, and have, you know, the backing of, you
*  know, government or military intelligence service security so that bad actors are,
*  you know, interrupted.
*  That's the kind of the only way I see things going well.
*  If all, as you can, as you can tell, as the usual saying goes, any plan that
*  requires more than two things to go right will never work.
*  Unfortunately, this is a plan that requires more than two things to go right.
*  So I don't expect this plan to work.
*  Yeah, but let's, let's hope we get there anyway.
*  Connor, thank you for coming on the podcast.
*  It's been super interesting for me.
*  Pleasure as always.
