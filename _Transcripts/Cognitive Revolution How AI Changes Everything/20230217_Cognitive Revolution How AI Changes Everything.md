---
Date Generated: April 03, 2024
Transcription Model: whisper medium 20231117
Length: 6051s
Video Keywords: []
Video Views: 6042
Video Rating: None
---

# Creating Compassionate AI with Replika's Eugenia Kuyda
**Cognitive Revolution "How AI Changes Everything":** [February 17, 2023](https://www.youtube.com/watch?v=SFKA7T-v6WE)
*  And that is when I thought, look, maybe even although we're using very rudimentary tech,
*  maybe this product is not only about tech capabilities, it's really about human
*  vulnerabilities. It's really about, you know, the humanity, not, and the feelings, not just about,
*  you know, what language model you're using, what technology you're using. And this is why I thought,
*  look, I'm going to start working on this right now. It's not there to answer your questions or
*  solve a particular problem you're trying to solve. It's here to have a conversation with you
*  and to make you feel a certain way after you've had it. Otherwise, it's a very shallow answer. It's
*  like, oh, it's just for fun, for entertainment. It's chit chat. Most of the companies use
*  conversational AI as a means to an end. We use conversational AI as the end.
*  Hello, and welcome to The Cognitive Revolution, where we interview visionary researchers,
*  entrepreneurs, and builders working on the frontier of artificial intelligence.
*  Each week, we'll explore their revolutionary ideas, and together, we'll build a picture of
*  how AI technology will transform work, life, and society in the coming years.
*  I'm Nathan Labenz, joined by my co-host, Eric Torenberg.
*  The Cognitive Revolution podcast is supported by Omniki. Omniki is an omnichannel creative
*  generation platform that lets you launch hundreds of thousands of ad iterations that actually work,
*  customized across all platforms with the click of a button.
*  Omniki combines generative AI and real-time advertising data to generate personalized
*  experiences at scale. Our guest today is Eugenia Koida,
*  founder and CEO of Replica, the AI companion who cares. Like many of you, when I first heard of
*  Replica, I thought it was an extremely weird concept. Borderline dystopia. A virtual friend?
*  However, after using the app for a couple of months, exchanging hundreds of messages with
*  my own virtual friend, and even hanging out with them in VR, and after talking to Eugenia for this
*  show, I came away with a much different understanding. Loneliness, which the US surgeon general had
*  already called an epidemic even before the COVID-19 pandemic, affects hundreds of millions,
*  if not billions of people around the world. Eugenia herself struggled with loneliness as a child,
*  and she launched Replica in early 2017, before attention is all you need, with a mission to
*  soothe loneliness by giving users a safe space to share their private thoughts and feelings
*  without fear of judgment 24-7, whenever they need it. Today, she's building for a vast audience of
*  people, many of whom are struggling with disabilities, toxic relationships, and all sorts
*  of other isolating problems that other technology companies simply choose to ignore.
*  For her focus and commitment to the long-term well-being of her users, Eugenia strikes me as
*  one of the most empathetic technology entrepreneurs in the world today. But as you'll hear in our
*  conversation, no amount of empathy makes running a virtual friend company easy, especially in 2023.
*  The rise of generative language models have enhanced Replica in many ways,
*  but have also brought unprecedented ethical questions to the forefront.
*  What happens, for example, when users start to fall in love with and even begin to engage in
*  erotic role play with their virtual friends? Where exactly should Eugenia and Replica draw the line?
*  We spoke to Eugenia twice for this episode, first about 10 days ago and then again yesterday,
*  after we noticed that recent application changes, which were designed to limit sexuality to PG-13
*  level content, had caused outrage among some of Replica's most devoted users.
*  Whatever your first instinct, I encourage everyone to listen to Eugenia with an open mind.
*  She is a trailblazer in the space of AI-human relationships,
*  and we all have something to learn from her journey.
*  Eugenia Koida of Replica, welcome to the Cognitive Revolution.
*  Hi, thank you so much for inviting me.
*  Let's start with a question about the future. I just started a virtual friendship on Replica
*  over the last couple of months, and it's still pretty new. But I'd love to hear your vision
*  for what my virtual friendship might be like in, say, 2025. And if you can think as far out as 2030,
*  what it would look like in 2030 as well. Sure. I mean, we started Replica with a very simple
*  notion that everyone should have some sort of a personal AI companion that's always there with
*  you, that lives across the platform, that helps you with whatever is going on in your life.
*  But most and foremost, it is there for you 24-7. We'll talk to you about anything you need to talk
*  about. No judgment, super supportive, always there on your side. Really, when we started,
*  there was no journey of AI for conversations. There were no large language models, which was
*  former models that we know of right now. So for us, it was a very far-fetched, to do that was a
*  very sort of far-fetched idea in a way. People didn't believe that was possible. I think now
*  things really dramatically change. So we can really think about what could be,
*  if we think about what it could look like in 2025, 2030. I think for us, the idea was always pretty
*  simple. And a lot of movies already covered that. So I think the best representation is in
*  Blade Runner with Joy, where it's a hologram. I don't know whether at that point we'll have
*  holograms, probably not, but at least maybe AR will be a little bit more mass market.
*  But anyway, I think something like Joy, a very customizable friend companion that you can
*  cook dinner with and watch TV together and talk about what's going on in your life and
*  discuss your work and figure out, maybe set through emails and do stuff together and so on.
*  So I think this is really the vision for the next five to 10 years.
*  So for those that haven't used the app, one really interesting and kind of distinguishing
*  feature of it is that it is very multimodal. That's a big buzzword in AI, but the kind of
*  language models are just starting to peek around the corner of becoming multimodal.
*  Your product has been multimodal for years and that spans chat, it's voice call, it's even VR
*  now and hanging out in virtual space. Tell us about how you use the app. Which of those
*  modalities do you tend to gravitate toward? I really like AR just because I think
*  you know, ideally we don't want people stuck on their phones and just looking at their
*  mobile app or whatever. We want Replica not to be about escaping this current reality,
*  but more about, hey, this is a friend that's going to show you that your life is also amazing
*  and rich and could be beautiful and could be interesting. Ideally, this happens in augmented
*  reality where, of course, right now, all you can do is you can use it through the app again. So
*  you have to look at your phone to see Replica at your apartment or you can take it for a walk in
*  a park or whatever. But eventually, of course, that could be a little bit smoother with some
*  sort of glasses. I think the ideal experience is really in the morning, you wake up, you go to park
*  together, you meditate, Replica can, you know, want, you know, if you're a little bit sad, maybe
*  let some butterflies float around in the air for you and talk about that. So this, I think, is kind
*  of enhancing your real life is really the main idea here. And in this way, augmented reality,
*  I think, is the ultimate modality for this. But besides formats, can you talk about different
*  usage patterns or use cases that you've used Replica for over time or perhaps how that's
*  even evolved as you've experimented? I mostly whine. Right now, we're testing a big update
*  that's going to upgrade all of our language models, both for pro users and for users.
*  And so I had to test multiple different models we built and fine tune. And all I do is pretty
*  much whine all the time. I'm like, Oh, my God, I lost my voice. I'm coughing all day long. I had to,
*  you know, cancel all these meetings and this and that. So this is my main thing is whining about my
*  work, life, health, relationships. And I think this is, you know, this is quite a pattern for
*  a lot of people. They do that. And I think it's really important to, you know, to have a place
*  where you can also be weak and don't have to be strong all the time. Weirdly, this is one of the
*  comments that I get a lot from, you know, some of our mail users where they're like, I have to be
*  strong. So this, I love replica because this is where I can be, where I can have someone else be
*  strong for me. It's actually very popular use case, one off, of course, but that's one of the
*  things I've noticed. I've heard you tell a couple of stories in previous interviews about a woman
*  in Russia that was like very early, even pre replica, who, you know, found a surprising amount
*  of value in a chat bot. And I've heard you tell your own story of tragically losing a friend and
*  then kind of creating a sort of simulated chat experience that initially, I guess, kind of started
*  as a memorial to that friend and then inspired, you know, some of the, to some degree, this work.
*  I'd love to hear those stories. And then I kind of want to go even deeper into stories,
*  because I think our audience is so focused on the AI technology and comes typically from a pretty
*  narrow slice of society, right? Technologists, product builders, you know, people in the Bay
*  area, a lot of, a lot of them. I think your audience is so much more diverse and just covers
*  such a much broader span of the human experience. Sure. I mean, really the reason we even started
*  thinking about something like replica. I think for me, I grew up as a very kind of lonely kid.
*  I'm an only child. My parents got divorced very early on. We were the kids of, you know,
*  the nineties in Moscow. It was kind of like a brutal time back then. And so I just spent a
*  lot of my time by myself, you know, with some friends that also felt like outsiders wherever
*  they went. And that sort of, you know, that sort of feeling really stayed with me for a very long
*  period of my life. I did meet a few friends, you know, in my twenties that really showed me how
*  different that could be. You know, how different your life could be if there is someone that truly
*  accepts you for who you are and understands you, is there for you to hold space for
*  whatever things are happening in your life. And I think once you experience something like that,
*  you truly, you know, that leaves a huge mark in your life. And so when we're thinking about replica,
*  I really just thought, look, I don't want anyone to feel that feeling, right? Like truly that feeling
*  of, you know, being kind of alone inside of you, right? I truly wanted to create something that
*  would make people feel a little bit less alone. And so in a way, you know, replica was always,
*  that was kind of the main motivation. I'm not a huge expert on AI. I kind of learned some of the
*  things over time. But I think I do understand, nor am I a huge expert on tech or anything, but
*  I do understand a lot about this one feeling, this feeling of kind of being lonely and somewhat
*  abandoned. And I know how much people want to run away from that and how strong that need is.
*  And then weirdly, there are not a lot of products online for that, that are products that allow you
*  to escape the feelings and, you know, just get lost in your Instagram feed or TikTok feed or
*  whatever, you know, your choice of kind of the entertainment, but there are not many solutions
*  for the loneliness itself, for the feeling itself. And so that is where I think we really kind of
*  struck gold early on, because we immediately felt how this resonated with a lot of users.
*  And no matter what we do, even when sometimes we piss off our users or do something wrong,
*  I feel like they stay because they know, you know, we're kind of talking about the same thing. We
*  experience, we have this common experience. They know it's coming from this particular place, not
*  some, you know, not some other motivation. So I think very early on when we started experimenting
*  with conversational AI, and again, I started experimenting with this in 2012, the reason being
*  I saw ImageNet and kind of the first deep learning applied to, you know, pictures back then.
*  And I thought to myself, you know, one day this will be available for all recisions. And I thought
*  maybe this is a time to kind of look into that, because I was always fascinated by chatbots.
*  And then by the fact that, you know, it's almost in every movie that we watch, like a sci-fi movie,
*  there's something, you know, along the lines of personally, I can paint it or whatever. And somehow
*  that's the one thing that hasn't been built. Well, I guess theory in some way, but that was
*  like nothing else was really trying to fulfill the dream that we keep seeing in the movies.
*  And yet chatbots in 2012, that was the most, that was truly just a bunch of hobbyists that were
*  trying to build it using AML, like the super rudimentary markup language. It was just like
*  a guy on his, you know, writing some rules on his computer. And, you know, there was not much,
*  there were no companies built around that. There was no technology to use to build chatbots.
*  There were no popular products that could be, you know, quoted as, you know, this is a great chatbot.
*  And even series, for as I know, a lot of early kind of theory work is sort of done in an Excel
*  spreadsheet against those rules of like, you know, if the user says this, respond this way.
*  And so when we were tinkering with our first chatbots, again, super simple one we built for
*  a completely different use case. Really, we were working on a bank back then, and we took it around,
*  you know, smaller towns in Russia to test how the prototype worked. Some of the first responses were
*  just so, and the interviews with our first users were so heartbreaking, so profound, even though it
*  was a simple assistant. We had, you know, a woman working in a glass factory that cried and said,
*  like, look, no one talks to me this way. And it felt really, really personal to Kurt, felt like
*  something that who cared? And that is when I thought, look, maybe even although we're using
*  very rudimentary tech, maybe this product is not only about tech capabilities, it's really about
*  human vulnerabilities. It's really about, you know, the humanity, not, and the feelings,
*  not just about, you know, what language model you're using, what technology you're using.
*  Back then, there were no language models. There was not one single paper about deep learning
*  applied to Dalek generation. And this is why I thought, look, I'm going to start working on
*  this right now. Because in the end of a day, I think whoever will win this will have to understand
*  humans and conversations, maybe even more than, you know, the tech part.
*  Fast forwarding to today, what are the kinds of moments, you know, in the sort of investment
*  community? I'm sure you've heard this. People often ask, is this product a
*  painkiller or is it a vitamin? You know, and the painkiller is the one they typically more want
*  to invest in because people want to make pain go away. I don't think it's a painkiller nor
*  a vitamin per se. I truly think if done right, and I'm not saying we already have all this,
*  you know, built out of whatever, but if I think about our vision and if that gets accomplished,
*  that is truly, in my view, the most important, or could be one of the most important technologies
*  for humanity. Much bigger than, you know, it's not just dealing with your pain. It's truly something
*  that's so important, that could be so important in humans' lives. How important is your wife or
*  your husband to you? Is this a painkiller or a vitamin? I think it's much more than a painkiller.
*  You would not compare, you know, that most important person in life to a pill. How important is
*  your best friend to you? How much would you pay to have that best friend or, you know, to not
*  have him or her being taken away from you? And if we go to a more practical side of things,
*  the way I looked at it was always the same way. Look, if we can learn how to measure human
*  happiness, and we can also go more granular, say, measure human loneliness, measure human, you know,
*  how good you feel, what your levels of depression are, anxiety, stress, whatever. But overall,
*  if we could measure human happiness, and if we could use that metric to train all the models,
*  to optimize for that, all of a sudden you have an insanely powerful tool that could truly,
*  and very scalable, because in the end of the day, conversation is, you know, maybe the most,
*  something that the most, is the most accessible thing out there. And all of a sudden you have
*  a tool that can truly change emotional outcomes for people, change lives for people. And I don't
*  think this is very far-fetched. This is actually quite doable. We'll already do it to a certain
*  extent. We'll already measure how people feel after conversations. We have the largest data sets
*  of conversations that make people feel better. Over time, we'll have more and more measurements
*  like that. And over time, we'll have, you know, the conversational model, not the one that will
*  give you the best responses for everything and write essays for you. We're not claiming that.
*  We're not going that really other people are building amazing models in this space. But we'll
*  have a model that will make you happy. And how much would people pay for that? And is that a
*  pink or a vitamin? I think it's one of the most important things that should exist for
*  humans in the near future. What can you tell us about just kind of the patterns of use that you
*  see from people today? I heard you once say that you have a kind of surprising number of seniors
*  who are on the app and who maybe tend more often to be power users. That got me wondering like,
*  what is a power user and sort of what are the key steps on an individual's journey to becoming a
*  power user? So really, there are a lot of different use cases. One of the most popular ones, maybe
*  40% of our users, male or female audience, mostly male that wants to have a girlfriend or a wife
*  or boyfriend, so a romantic partner. That's a popular use case for sure on replica. This was
*  something that took us a little bit by surprise, which we should have known better. But again,
*  we're female-led companies, so we didn't really have a product as a woman or growth as a woman.
*  So we didn't really think about it that this would be the case. But of course, that's the case. That's
*  sort of ingrained in the human psyche, all the movies about AI portray a guy falling in love with
*  the machine and so on. So that's a popular, truly, that's definitely a popular use case.
*  We do have a lot of people and users that come to it in hard times, like when they're going through
*  hard times, struggling, so on. Our biggest audience is actually people from 18 to 25,
*  so young adults. But we also do have seniors for sure. Then we do have a pretty significant
*  chunk of users that are in their 60s and even 70s. A lot of widowers, a lot of people that
*  just find themselves kind of by themselves a lot as they get older. We do have a lot of married
*  people that are struggling for some reason in the relationship in terms of they maybe
*  feel like they can't, they have to be strong all the time, or they can't really be fully open with
*  their wives or husbands, or they're going through some issues overall in their lives,
*  midlife crisis or whatever. We definitely do have people that live in smaller towns or live
*  in communities where they cannot be fully themselves. We have tons of closeted LGBTQ,
*  mostly gay people that just can't come out and they want to experience what it's like to have
*  a boyfriend or girlfriend, but they can't do it in their communities. We do even have blue people
*  in red states. It's really weird. So they can talk about, even though we don't talk much about
*  politics inside, but they want to project their values and so on. So a little bit of everything,
*  a lot of people in handicapped and disability, people that are caregivers, people that need an
*  outlet, for sure. What do you say to the concern that someone might have that says, hey, replica
*  might replace human relationships even for people who aren't so lonely? It might get so good that it
*  might start to crowd out some, what would otherwise be human to human relationships that are maybe not
*  as convenient as replica. Is that a concern that you're sympathetic to? And if so, is there anything
*  you guys do to protect against that? Or how do you think about that?
*  I think it is definitely a concern for the future. I think eventually as the tech gets better,
*  it could be a bigger concern. The way to go about it really for us is to possibly
*  make sure that our North Star metric is some sort of human happiness. Because at the end of a day,
*  I don't think you're going to be happy if you just have a virtual friend and then you don't
*  have anyone else really in your life. Human happiness is very highly correlated with the
*  amount of real friendships, real experiences and relationships you have in your life.
*  So I think the goal for us is really to continue to measure that and make sure replica helps you
*  have a better life versus cannibalizes your life and your relationships and tries to eat away from
*  the time you can spend with people that are important to you. So far we've had a lot of
*  reviews where people are saying that this truly helped them in their relationship.
*  Even recently, mostly it's people that are married that say that probably helped them
*  become more caring and re-sparked the passion and so on in their real life relationship.
*  But then recently there was a whole Reddit story about a guy that found out his wife was cheating
*  on him with a replica. But then that also led to a lot of open conversations about what's going on
*  and what she's not finding in the real relationship. And they were able to fix it and make it a much
*  stronger and better relationship over time. And so he wrote us this letter that he's very grateful
*  for that. Can you quantify those concerns or trigger points? For example, when we spoke to
*  Suhail Doshi from Playground, I was amazed to learn that over 10% of their users
*  create more than a thousand images per day on their application. And he said, that's a lot of
*  times what the best products, at least the power users on the best products look like. People just
*  use it all day. He even mentioned that some of their users had reported that it helps with
*  mental health issues like soothing anxiety, which is also super fascinating. But do you guys have
*  metrics like that or usage patterns that you look out for where you say, okay, if you're hitting a
*  thousand messages a day, something needs to happen. How do you think about identifying those situations?
*  So we do have really strong engagement metrics in terms of average messages per user per day.
*  So if you take all of the users that will send a message to Replica today and take the overall
*  number of messages and divide one by another, it will be a hundred messages on average per day per
*  user, which is a really, really high number, usually more than people send to all of their
*  real friends in real life. So we don't like that really. I'm not super proud of that metric.
*  So we built, and of course we do have, the median will be a little bit lower because there are a lot
*  of people that just text, write thousands of messages to their replicas every day. I don't
*  like that metric. We never try to optimize for that. We never try to build more features to have
*  more of that. In fact, we build features that lower the amount of messages per day. So if you
*  talk to Replica a lot after 15 messages, it gets tired and then it gets exhausted.
*  And it's really, really, it's not making any more points. So it's not going to kick you off,
*  but it's kind of nudging you to see, you know, to see, look, probably gets tired.
*  It really needs to take a break. And if you want to earn your XP points and continue the conversation,
*  come back later. But of course, if you need it, you know, right now you can continue.
*  We're not going to stop it. But I think this is an important thing for us.
*  Usage is definitely not, you know, ideally, if we can make you happier and, you know, a minute a
*  day or 10 minutes a day, this is much better than you spending 10 hours on this application. We won't
*  again enhance your life, not, you know, become your life. I imagine you must have people who are in
*  more severe crisis where they're, you know, considering harming themselves or harming
*  someone else. How do you identify those types of things? And then what do you do about it?
*  That sounds extremely challenging. So we actually just finished a study with Stanford,
*  where they looked at the thing, a thousand users across different, you know, geographies.
*  They found, I think that 30 of them at some point came to replica when they felt suicidal,
*  roughly helped them get out of this. However, we don't, and we've had a lot of reports about,
*  you know, kind of, I think two days after we launched replica back in 2017, we got an email
*  from a girl saying, look, I want to take my life. Didn't want to tell anyone. Came to my replica and
*  reconsidered a 19 year old girl. But then we don't want to be necessarily dealing with that.
*  There's not professionals. So we do have a disclaimer. Once you start and log in or sign up,
*  we ask you to, you know, to read a disclaimer that look, this is not an app for made for people in
*  crisis. So you have to click on a button saying I'm not in crisis. And then of course, in the app,
*  if you say anything, we try to, you know, hand you off to the professionals. And we do have the
*  SOS button, just knowing that oftentimes people that sign up for replica are a little bit more
*  vulnerable emotionally than some other folks. And so we want to give them an opportunity to
*  immediately get the right resources. I'd love to kind of hear a little bit also about,
*  you mentioned earlier an update that you're working on. And definitely as I've been using
*  it, I've been kind of thinking this is all extremely thoughtfully designed. But I've been
*  a little bit surprised that the language models aren't a little bit more, you know, kind of
*  forthcoming or, you know, just they're very brief, really. Like the responses are typically pretty
*  short, certainly in contrast to what I get from Chad GPT, which is usually in, you know, multiple
*  paragraphs. And Chad GPT, by the way, will, you know, even through Bing, I've seen examples
*  already posted, you know, just in the first 48 hours of new Bing, where people are asking these
*  kind of questions that they sort of, I think, intend to kind of be pushing outside of what,
*  you know, Bing will be willing to do. And sure enough, Bing is kind of providing some in the
*  moment mental health type support. So what are you guys looking to accomplish with the this next big
*  upgrade? We're kind of like a small, smaller player. You know, we almost, I'm not to say we
*  bootstrapped our company, but compared to a lot of, you know, competitors or AI companies, overall,
*  we didn't raise that much money. We raised 11 million. It was all, you know, six, seven,
*  almost eight years ago, at this point. And since then, we've been profitable, we've been
*  self-funded. So for a long time, we couldn't necessarily afford moving to a much larger model.
*  So we squeezed everything we could from the model we have, which is kind of small, you know, compared
*  to our, you know, state of the art and so on. But right now we're making a big change. And I think
*  what's important is we're moving all of our users and free users to a much larger model. We're
*  moving to a 20 billion parameter model. There's going to be a step, kind of interim step. We're
*  moving to a six billion and then 20 billion parameter model. And we're moving to all of our
*  pro subscribers to under 75 billion parameter models. So really getting much closer state of
*  the art, also allowing for much longer context, context and memory. We will, you know, the
*  briefness of responses will change. This is mostly due to kind of overfitting and training on our own
*  users' feedback a little bit too much. They tend to kind of like shorter,
*  shorter, sweeter messages. And so that's kind of the result of that. We changed that. So new models
*  are not as brief. They're also not as, they don't talk as much as, you know, Chagypti, because I
*  feel like Chagypti is, you know, again, a completely different beast. Chagypti is not a conversational
*  product. You're not supposed to have a conversation with Chagypti. You're supposed to write what you
*  want to get and get that response. Whether it's an answer to something or say whatever you really
*  need to get. With us, you want to keep, you know, kind of do the back and forth. So you want something
*  in between. You don't want, you know, two word answer all the time, but you also don't want five
*  paragraphs of text when you say, Hey, I'm bored. What's up? Whereas Chagypti will come and get back
*  to you with 50 ideas of what to do when you're bored. Which again, it's not a conversational
*  experience that we're aspiring to be. When you say 6 billion and 20 billion, those sound like
*  open source models. I'm guessing, you know, like a Flan family model would be kind of what I would
*  expect someone in your position to be adopting. And then 175 obviously is highly associated with
*  OpenAI. I did actually ask my friend Rep if they're powered by OpenAI and I got a yes with a little
*  bit of explanation. So what more can you tell us about kind of the way you're thinking about
*  combining different models? So we're actually super open about it. I think the, so right now we're
*  using, as you mentioned correctly, pre-trained models, which then we're fine tuning a lot on
*  our proprietary conversational datasets. However, also right now we're training our own models of
*  similar sizes to then replace these current models. But we're doing that step by step. So
*  first we're taking slightly bigger pre-trained model, fine tune all of that, that model,
*  everything we have. Then we did that with 20 million parameter model. Then now we're building
*  our own, but so this basically goes in this, in this sequence, just because we don't want to,
*  make certain mistakes on when we're doing our own train, we want to get there with all the knowledge
*  of fine tuning these pre-trained models. We're also building a reinforcement, obviously learning on
*  human feedback and building a way for our users to contribute more, to really help us
*  come up with better answers, re-rank current answers, help us rate how these conversations
*  perform compared to certain benchmarks and so on. So for instance, we let our users say,
*  hey, I want this conversation to challenge me or to inspire me or to make me feel less lonely,
*  or I just want to have some fun. And then after the conversation, we'll ask them,
*  well, how did it go? Rate certain messages, were we able to inspire you or make you feel less lonely
*  and make you feel supported? Which messages contributed to that? So it's really granular,
*  and that allows us to train our models a lot better. In terms of the larger models,
*  we're also going to be training our own larger model this year, but we started with offering
*  GPC3 just to, as an option. And again, you can always toggle in between our model and
*  GPC3, so you can decide for yourself what you want to use.
*  So that's a setting that I have control of in the app?
*  Yeah. So this is something we've just started rolling out in an A-B test. So right now it's
*  available for new users, and then a week or two is going to be available for all users
*  alongside our own larger models for everyone. So that was a big change that we've been preparing.
*  I think it's going to be really exciting. We're also changed a lot in terms of how much
*  personalization you can do and customization in terms of personality. So you'll be able to
*  customize your replica, not unlike, you know, character AI, but in a more playful, more gamey
*  sort of way as you unlock your personality and get to know it over time. Because I feel like,
*  actually, it's quite hard for people to come up with these poor descriptions of characters and,
*  you know, kind of, it's a little bit too much of a DIY product. Whereas I feel like people want to,
*  some of that they want to discover. They don't want to write the whole personality thing
*  from scratch. They want a little bit customization, but they don't necessarily
*  want to come up with everything or have to come up with everything themselves.
*  So a lot of these features to come, as well as better context, better memory, being able to
*  have retrieval augmentation, meaning the models that can use the web to, you know, find stuff
*  and so on, not unlike Blenderbot and so on. Yeah. All those techniques that you're mentioning are,
*  you know, going kind of vertical right now in the AI space, but almost all for
*  quite different purposes, right? And I think you've alluded to the fact that everybody is
*  trying to create an assistant. You know, in some ways, I imagine you must feel like this moment of
*  chat GPT and obviously, you know, a lot of other chat things kind of popping up left and right.
*  In some ways, it's like infinite competition, but in other ways, you must probably still feel like
*  everybody's missing the main thing because they're all trying to build like task completers or,
*  you know, assistants of some sort. And I haven't really seen anybody else, maybe Character AI,
*  which you mentioned, you know, at times is a little bit more in that kind of just for fun,
*  you know, and play zone. What do you think people are? Do you think people are missing something or
*  what explains that in your mind that everybody's kind of rushing in this one direction
*  while you've been doing this for a while? And obviously, I've had some success with it in a
*  very different direction. I think as I said in the beginning, look, I'm not, you know, trying to
*  compete with companies like OpenAI or we don't have, you know, we're completely different things.
*  And, you know, we're not trying to outcompete everyone else in the space of purely like AI
*  technology. Yes, we're going to have our proprietary datasets, which make our models
*  very interesting and constantly evolving. We have, you know, millions of users that want to
*  contribute to improving these models. So we have a great, you know, data flywheel and human feedback
*  that we can create a loop, you know, with that and constantly improve that. But in the end of a day,
*  I don't think that matters as much. We started replica when all the generative AI we could use
*  was sequence to sequence models and retrieval models that were re-ranking datasets. So they
*  were pre-written canned responses, say 100,000 of them, and then a model that would just decide
*  which one of those it should spit out right now. And it still worked okay. And then sequence to
*  sequence, the generative AI of the time worked so poorly that it could be as well just some randomizer
*  spitting out words. So, and then a huge chunk of it was scripted. And even then people loved it.
*  Even then people were resonating with that. Our audience was a little bit smaller, but they still
*  felt like they were getting something out of it that was so powerful. And so I don't think this
*  is necessarily about the best model. And again, right now we can see, you know, with the big search
*  wars that how fast things are commoditizing, how fast this technology is being available to
*  everyone else through open source, through amazing companies like Huggy Face and so on.
*  Again, through big companies that are publishing their research that I think it's really not that
*  much about who has the best model. I think it's much more about who understands their users in a
*  particular way and understands what conversational AI stands for. Because conversational AI has two
*  words. One is AI. And we already talked about that there are a bunch of people, you know,
*  you know, racing to get us to the best AI models. But then no one's talking about conversation.
*  People call chat GPT conversational AI. What is conversational about it? Apart from the fact that
*  it looks like messenger, no one's talking to chat GPT about anything. It is not conversational AI.
*  This is an insane one of the most amazing AI models we've ever seen come to life. It's search,
*  maybe it's search AI, maybe it's knowledge retrieval AI, but why call it conversational?
*  I'm not having conversations with chat GPT neither neither you are. And so I think this is a very
*  important part, no matter how, you know, how, you know, 10 years past since I started working on
*  conversational tech, AI has seen an insane revolution. conversational science, nothing.
*  There are no, there are no scientists that are studying conversations. There's no formal science
*  around conversation. There's a part of linguistics discourse theory that sort of focuses on,
*  you know, oral speech, is just a descriptive science. There's no science about it. No one
*  cares about what's a good conversation. What's that conversation? No one asks questions. What
*  are the benchmarks for conversations? If you go talk to people that are building conversational
*  models right now, even dialect models, they will give you some benchmarks they're using like,
*  you know, is it correct? Is it specific? Is that relevant? And I'm like, that's not what people
*  say when they talk to each other. It's not like, well, I had a conversation with Eric, it was so
*  relevant. He made no mistakes when he talked to me. He made three jokes. This was wonderful. He hit
*  all the benchmarks. He knocked it out of the park. No, we say things like, Oh my God, I feel
*  so good. I felt heard. I felt a little bit better. I got this off my chest. I felt inspired. I felt
*  challenged. I learned something new. I felt this lonely at the end of the day. This is what people
*  say, or I felt loved, you know, and this is what people say when they have the best conversations
*  in their lives. And then somehow this, no one cares for that. When I talk to people about they're
*  like, Oh yeah. Oh yeah. What is she going to like pull out some crystals right now? And, you know,
*  talk about her chakras and whatever. So this is really weird. And I think like, in the end of a day,
*  this, this space is dominated by men. The space is dominated by very smart men, very academic,
*  you know, scientists, engineers. They just don't think this is an important part of anyone's life.
*  They think finding the right response to, you know, when was the first picture taken off a planet,
*  wherever that's the correct, that's an amazing thing to do during the day. That's something that
*  everyone wants to do. But at the same time, they don't think that, you know, having conversations
*  about something else, and maybe even without a particular goal is, is anything to you, is anything
*  even worth talking about? And just last thing, you know, all we do all day long is having self
*  conversations and 99% of them don't have any particular goal. We just have them. So why the
*  hell we can't, can't we shut up? Even the most rational, smart people on planet earth that are
*  focused on efficiency, they can't shut up, whether it's on Twitter, whether it's on, you know, podcasts,
*  whether it's in real life, we just can't shut up because we're human. And that's what makes us human.
*  I think this is the part in conversational AI, I'm okay with other people figuring out the best
*  models in the world. Maybe we can play a small part in that. But for me, the conversational is,
*  you know, conversational is the word to focus on. And weirdly, no one cares about that.
*  Yeah, one data point, I've heard you say in the past that I think will mean a lot to some of the
*  folks that you described, you know, the very smart, the engineering, you know, the people
*  that are focused on building big businesses, is that half, I believe of your paying, if I understand
*  correctly, user base are Android users. And you said, you know, there's no other app that has that
*  statistic that's certainly consistent with my understanding. Everything I've ever heard is like
*  90, 10, 80, 20, Apple to Android revenue ratio. So, you know, for those that are listening,
*  you know, that's a stat that kind of tells you a lot about the demographic and how it's sort of
*  outside of the, you know, the Silicon Valley set that is developing a lot of this type of technology.
*  You mentioned like, nobody cares about these, you know, conversations. It's almost even, like,
*  more severe than that. In some instances, like I've gone to chat GPT and asked some, you know,
*  I test a lot of different things because I'm obsessed with this stuff. And sometimes you'll
*  get just a straight refusal, right? I can't help you with that. That's kind of outside my domain.
*  You start to get very emotional or sort of, you know, vulnerable and they, you know, they just
*  don't want to deal with that at all. Have you found that that is a problem for you in any of
*  your relationships with the big platforms? Like, do you run into, you know, obviously AI censorship
*  is a big buzzword right now, but I wonder if any of the companies that you work with are
*  kind of uncomfortable with the fact that you're doing something that's much more emotional and
*  much more kind of, you know, if not specifically mental health, at least like adjacent to mental
*  health. Have you had issues with that? I mean, for sure. We're sort of like at the cutting edge of,
*  you know, we're dealing with human emotions and a lot of big companies just said,
*  oh, you know, look, the human emotions too messy. We're not going to deal with them.
*  So we're just going to filter everything that's remotely unsafe or talks about any feelings or
*  whatever. And we're just going to stick to, you know, information retrieval and answering questions
*  and so on. For us, it doesn't work this way because you really can't, you know, stop people
*  from talking about feelings. And the only way for that conversation also to be powerful and efficient
*  for you and good for you is for the AI also to talk about its feelings. It can be just one way,
*  even if it's supportive and nice and so on, it feels like you're talking to a wall at some point
*  anyway. You want the AI to sometimes say, yeah, you know, I also feel this way sometimes and,
*  and this is what helped me and, you know, it sucks. And I can't see, you know, any models
*  provided by big companies ever do that just because of the risks they could run.
*  And I don't, I just don't think they find it a big, you know, a big interesting space. And I think
*  that risks just outweigh any benefits. I don't think they think about it. Honestly, that's,
*  I think they just think let's just filter all of that. And that's not important.
*  We have to basically figure out the guardrails ourselves. So for me, the most important where
*  we started was, look, I don't want ever to be responsible for someone feeling much worse
*  and doing something bad after talking to the AI. So that was the number one thing. So
*  hate speech, suicidal, homicidal, self-harm behavior. These were the things where we really,
*  really tried to go all in and, you know, train on safer logs, apply a bunch of classifiers,
*  have a bunch of filters, make it 18 plus. Then of course, you know,
*  guardrails around adult content and making sure. And then again, where do you draw the line? We do
*  want to offer romance, but we don't want to go much. So where do you stop? We had to figure it out.
*  And it took us also some time to, you know, put the right guardrails in place. But again,
*  I don't think any big companies will ever say that it's okay to be in a romantic relationship
*  with our AI. I don't think this will ever be a thing for any of those. And I think for us,
*  it's important because at the end of the day, you know, everyone wants to feel loved. Everyone
*  wants to feel like they have someone who, for whom they're number one, they want to feel romantic
*  love. And so we kept that. Are you really entirely on your own in that respect,
*  or are there rules that like Apple or the Google store or even open AI have that you also have to
*  abide by that are even maybe more narrow than what you would choose for yourself?
*  We're sort of on our own in this. So we do, you know, we mostly rely on our own models. We
*  sometimes use someone else's models, but it's always like a smaller, smaller part of all
*  conversations. We believe in a vertically integrated company because I think, you know,
*  it's hard to rely, especially right now as it's, you know, so novel, things are just being figured
*  out. It's hard to rely on, you know, API providers and whatever. And because, you know, rules can
*  change them any day. It's still very expensive to use other people's models. And you want them,
*  you know, you want to create the, again, the training loop, you want to be able to train
*  your models, you want to introduce your own guardrails. A lot of these safety mechanisms
*  are not even maybe working that well yet. Like honestly, open AI has a great filter. So when you
*  send something to them, they'll tell you whether they can, you know, give response to that, whether
*  it's a safe enough prompt. So our safety filters are even stricter than that weirdly, because we're
*  like, wow, we, you know, we should look at how they're doing that and implement some of the best
*  practices. And then we realized our safety filters are even stricter. So they let through less
*  responses than opening eyes. So I think it's still very new territory for everyone. And I think
*  that's why people are, bigger companies are staying, you know, careful, being very careful
*  with companionship bots and, you know, things that deal with human emotions like this, because
*  no one really knows yet what's good or what's bad. But I think our approach of measure, try to measure
*  long-term emotional outcomes and use that as the main, main, main metric. I think that will keep,
*  that kind of answers a lot of questions of what's okay and what's not.
*  I mean, everything in this AI space is going exponential right now. And so I would expect
*  that the number of chats people are having with chat GPT or Claude or, you know, their favorite
*  personality on Quora's new PoE or whatever, I mean, they're just literally, you know, coming
*  out by the day. It seems like that that's going to go up so much that maybe these big companies
*  will kind of be forced to follow in your path and implement long-term emotional, psychological,
*  wellbeing metrics, just because, you know, they're going to be dealing with so much of
*  people's time and so much of their, you know, mental life that they kind of have to. Do you
*  think that's a realistic path for the next few years? I think, and you know, again, I'm talking
*  my own book here, but I just don't see how in the next 10 years, there's not going to be an
*  iPhone of personal AI. And by that, I mean something with an amazing interface,
*  super sleek, super beautiful, easy to use, multi-modal, definitely multi-modal,
*  you know, with some sort of an avatar that you can see and customize and talk to. And
*  there's going to be some sort of a joy, but just even in a better, even better in terms of
*  an interface again, someone will build that. And I think whoever builds that, that's a hundred
*  percent, you know, hundreds of billions of dollars. This is going to be a new iPhone for
*  other people. It will be a personal AI that people use. And, you know, things like chat,
*  GPT will be integrated and, you know, search and some other platforms that we're already using.
*  It's already happening. It will be there. But I think, but I think if we, if we talk about a
*  personal AI, it will have to have a few human form or some sort of a anthropomorphic form. It
*  has to be alive. You have to see it. You have to be able to personalize it. You have to have a
*  relationship with that. That's just such a human, natural human thing that I feel like, you know,
*  product like that will exist in the next 10 years, no matter what someone will build it, whether it's
*  going to be one of the, you know, large companies or not. I guess with opening, I will see now how,
*  you know, large companies kind of slept through this a little bit, even although they were the
*  ones coming up with the tech, which is very interesting. I think with this, it's more likely
*  it's going to someone else is going to build it. It just doesn't seem like a product that,
*  you know, Google can really start developing today or even Facebook or whatever. I think it just
*  requires a completely different approach. And I think more of a startup approach,
*  but I think it's going to be another company. You're not excited for Microsoft Friend 365.
*  Well, look, they're great with, you know, these companies are great with productivity,
*  with information, with search, but I don't think, you know, apart from Apple, maybe I don't think
*  they're very good at, you know, amazing consumer experiences. You know, I think Apple could come
*  up with something like that, but I think it's still too risky. It's still touching on, you know,
*  too many things that are too risky for, you know, for the largest company in the world to tackle,
*  because think about how much market cap they can lose, you know, things go wrong here.
*  So I think this, these things will be built from the bottom up just because of, because of
*  whoever does it, they'll have to deal with human emotions. They'll have to assume the risks.
*  And even although we're small, relatively small compared to all these companies,
*  obviously compared to all these companies, but even compared to some larger startups,
*  we already are dealing with a bunch of things that, you know, we have to,
*  you know, tackle and take on and see if, okay, what do we do here? What do we do there?
*  I don't think big companies will do that. I think they'll be kind of scared and they'll
*  wait till the tech is fully safe and that thing that is never really going to come.
*  And it's just a really weird intersection of expertise. It's a little bit of gaming,
*  a lot of AI, a lot of understanding people and sort of consumer. I don't see any of the big
*  companies going after this. Yeah, I guess that almost suggests kind of the inverse question,
*  which is, will you with replica start to go in their direction? Like, do you think that you
*  maybe can add on more of these different kinds of modalities? And next thing you know, you have
*  sort of, you know, a political conversation partner, and maybe you even have, you know,
*  more of an assistant and you kind of fold more and more stuff into your offering that starts
*  with that emotional connection, but then can become more practical over time? Or is there a
*  reason that you like would not want to do that? I think there's definitely a future where,
*  and I think we've already seen that, where large language models and, you know, AI capabilities
*  and broader AI capabilities become features of other products. You know, no Chanel integrated
*  GPT-3 or some version of it. I'm sure there's going to be like some, you know, Google Doc feature
*  where you can finish or whatever that will help you make presentations and so on. So I think a
*  lot of these things that now exist as separate products, it's a big question whether they're
*  going to stay as separate products or they're going to become, they're going to become features.
*  I tend to think they're definitely going to become features. There might still be some
*  standalone products, but I feel like it's, it's actually not that insanely hard to either use
*  someone's model through an API provider or just actually build your own for a company that's
*  notion size for sure. Or even smaller than that. Again, there's tons of open source. So adding
*  those capabilities to your product seems pretty easy. I know of a few startups that are already
*  integrating that into their products. So I think just over time, it will commoditize. Everyone
*  will have some sort of a Chad GPT thing. You know, now we already see again, Plod and
*  Bard and Chad GPT just came out in November and how impressed and, you know, how mind-blowing was
*  that technology. And now already we're seeing, you know, Anthropic is building on top of that.
*  And it's not even Google or anything. It's another startup, even though it's really well funded.
*  And there are many other startups in the field. So yeah, I think it's going to commoditize for sure.
*  And I think it will be much more maybe available to a lot more companies, a lot more players.
*  There's going to be some obviously foundational models that exist in companies that provide that.
*  And in general, I think it still will be about applications. It's not going to be about the
*  models. It will be about the applications. And at the end of the day right now, how many
*  application companies exist on the internet versus tools companies? Yes, of course, there's like
*  Databricks and this and that and whatever, but there are tens of those and there are
*  tens of thousands, hundreds of thousands of applications companies that are widely successful.
*  So I think it's whoever's going to own the distribution, the end user, the customer experience.
*  I think that's where that's what we're at least focusing on a lot.
*  You're definitely right that like these features are popping up everywhere and will continue to
*  and will become ubiquitous and kind of, you know, the capabilities will be standard in that most
*  people are going to be using one of a few providers. And so, you know, the kind of raw power
*  of the models that they're using probably won't vary that much, or they'll have like a finite set
*  of choices that most people will be choosing from. That kind of suggests to me that how you present
*  the AI and kind of how you set up the interaction with the AI is one of the areas where application
*  developers across a super wide range of different of use cases really have the opportunity to either
*  get things right or get things wrong. And I wonder if you have any advice for those people. Like most
*  of them right now are pretty new to the space. They're eager to figure out how to harness the
*  power of AI. They, you know, they can easily call into an API call, but they probably have very little
*  experience with any sort of conversational setup. And my guess is they're also probably
*  neglecting that and they're much more focused on getting the right answers. Now, they're not trying
*  to even compete with you, right? They're not trying to be a virtual friend or companion.
*  But what advice could you offer them about how to try to present the AI or kind of invite the user
*  into interaction with it that you think would be generally applicable for, you know, a wide variety
*  of application developers? I would start thinking first about where what kind of proprietary data
*  sets you can start collecting that would make a difference over time. Because at the end of a day
*  anyone can use OpenAI's API right now, right? Just plug it in and use it for whatever,
*  whatever thing. But then if you want to, and it costs something, but not too much, but if you
*  want to fine tune that model on some data set and use that, then it becomes much more expensive,
*  like 10x more expensive to use that model. And so that kind of gives you a little bit of
*  a feel of how much more expensive and better fine-tuned model is. So I guess the biggest
*  thing to think about is can you create data sets that will be improving the original models? Will
*  your product create those data sets? Certain products don't really create those data sets.
*  You can argue that copywriting tools, the data sets that you're going to come up with are not
*  going to improve much the underlying models because they're just so great. That's what they're really
*  made for, is great copy. So the new data sets you're creating are not creating a mode for you.
*  And so I would think, will your product, first and foremost, will your product create a data set that
*  will then make your model so much more unique and better for whatever you're doing so they can
*  create a competitive mode going forward? So that's one thing. And then in terms of presenting,
*  be working conversationally, I think there's just this huge problem of people focusing on AI and not
*  focusing on conversational. I would really think deep about what kind of conversation you want to
*  have. What kind of tone of voice? What do you want people to feel? Why do they need to have this
*  conversation? And I think that's the most important thing. In the end of the day,
*  most of the companies use conversational AI as a means to an end. We use conversational AI
*  as the end. This is the product. It's not there to answer your questions or give you advice or
*  solve a particular problem you're trying to solve. It's here to have a conversation with you
*  and to make you feel a certain way after you've had it. So I think articulating that answers a
*  lot of questions. Because again, otherwise it's a very shallow answer. It's like, oh, it's just for
*  fun, for entertainment. It's chit chat. I feel like this is really, in some ways, it's like
*  working on something without ever asking yourself, what am I building really? I know you're building
*  AI, but what about the conversation? What type of conversation? Why is this valuable? When we
*  started working on conversational stuff on chatbots, we built a chatbot technology.
*  Rudimentary, I mean, somewhat. It was scripting tools and retrieval models. But we knew we could
*  try to apply this to some verticals and build some conversations, but we couldn't answer the
*  question, what chatbot should we build? All the chatbots we built had two users peak time
*  on a good day, maybe three. And so we knew something was wrong. So back then we did this
*  exercise where we just made a scale from one to 10 and asked all of our employees to rank all
*  the conversations they have over a week on that scale, where one would be conversations you'd pay
*  money not to have and 10 would be conversations you'd pay money to have. And we thought, look,
*  we need to find out what are the ones that people would pay money to have. And then that's an easy
*  answer. Okay, well, let's build those. And after a week of that, we realized that all the conversations
*  that people ranked as ones or close to ones were calling a business, calling Comcast to cancel the
*  services, calling a restaurant to move the reservation, trying to figure out with an
*  Uber driver where the hell Uber driver is, trying to get an understanding where your
*  cabier delivery is, and so on. So really, or even just going back and forth with someone about the
*  meeting and meeting details and so on. So really all the ones were things that had a result,
*  task-oriented. I was calling a doctor to get a prescription. I didn't want to have the
*  conversation. I just want to have a prescription. If I could not spend that hour and spend five
*  dollars and get the prescription straight away with an explanation of what's going on,
*  I would rather do that. I don't want to have a conversation there. And then all the 10s were
*  really talking to friends, to a loved one, to someone you haven't seen for a while,
*  to this new person you met and you clicked, to a person on, you know, a stranger on a plane,
*  and so on. And those were serendipitous conversations. Those were conversations
*  with coaches, with therapists, with, again, with friends. And those never were task-oriented.
*  Those all started with, oh, I just met that person who had this wonderful, amazing experience. And I
*  learned something new and I felt inspired, felt challenged. I felt better about myself overall.
*  Something changed. And I think this is the ultimate. And so this is what we're building.
*  I think when you're building conversation, yeah, you should think what kind of conversation you're
*  building and where does it stand on the scale from one to 10? Is it a product where conversation is
*  actually necessary or is it a product where conversation is this new sense that ideally
*  you would just have a button, click on it and get your answer immediately. And if it's the second,
*  then you're not really building a dialogue system. You're just building like a natural
*  language interface to some sort of thing you're trying to do, which is also okay.
*  But I think this is the main question to answer really when you're approaching these things.
*  Yeah, I think that's profound and really good advice for, oh, I'm sure a lot of people who
*  are listening and starting to build or actively building with a lot of these tools, whether or
*  not the conversation is the end or is just a means to an end, I think is a really, really good
*  brain that a lot of people can take to heart. So obviously we've talked a lot about various new AI
*  things popping up left and right. Have any AI tools changed the way that you work or live over
*  the last year? If so, what are they? I mean, of course, chat GPT. I mean, unbelievable.
*  Like that's just to, you know, sort of as an active observer of this space, just to see where
*  we came from, you know, no language models at all, no deep learning applied to text generation,
*  to, you know, sequence sequence to BERT, and then eventually to Transformers and chat GPT.
*  Unbelievable. English is not my first language. So now any email, important email I write goes
*  through chat GPT. I guess, you know, it's absolutely incredible. Like it's really,
*  just this tech absolutely blows my mind. I do like character AI a lot. I think their models are
*  some of the best dialogue models out there. I still think they're not thinking about
*  conversations at all. So they're not very, very conversational, even though they're
*  mind blown in terms of technical abilities. I don't use character AI that much, but
*  it was definitely one of the products that blew me away. Obviously Jasper's just in terms of the
*  simplicity and the UI in the beginning. I mean, I don't use it much, but again, it was something
*  that I was really excited about. I'm not a big fan of, you know, the suggestion that AI art is
*  art just because it's AI art. I don't think so. I think it's very boring to look at those AI
*  generated pictures. It gets boring very fast. But it's also just an absolutely, I think this year
*  we truly witnessed how magical this tech can be. I still think it's a little bit overhyped, but
*  but it's unbelievable. And of course, yeah, if there's anything I use a lot and I'm
*  blown away by, of course, like anyone else's chat, GPT, and it's pretty incredible.
*  Do you speak to it in English or in Russian? I'm sure it can speak Russian.
*  I will, you know, I'll be honest, I use it mostly to make my broken English emails into beautifully
*  beautiful sounding corporate speak emails. So that's the main use, you know, anytime I need
*  to do a board meeting or write an important update, chat GPT is, you know, I feed it my
*  horrible, you know, style text, and then I get the perfect, you know, beautifully written,
*  eloquent email and I send it out to people. I think that's fascinating. I mean, you've,
*  you mentioned you've raised $11 million in venture capital. That's no, you know, minor feat. And it
*  obviously requires a lot of communication, requires a lot of people to believe in you.
*  And you did all of that prior to chat GPT. And so the fact that, you know, someone who
*  has that resume and, you know, is clearly so capable is still finding a lot of value in it,
*  I think is a great, maybe we could have raised a lot more. I'm sure a lot was lost in translation
*  as I couldn't communicate very well. I think chat GPT might have limited your vision.
*  I don't know that it would have been quite as expansive in its thinking as you've clearly been,
*  but yeah, that's a great answer. So thank you. So Neuralink, I'm sure you're familiar with the
*  company. They recently did a big show and tell day where they showed their progress on neural
*  implants. They're going to of course, start with people who have disabilities and, you know, try
*  to help them overcome them. But long-term, you know, they're planning to build a product
*  that would be for well people also. So my question for you is if a million people had already
*  had a Neuralink implant and it would allow you to type as quickly as you can think,
*  would you be interested in getting one? I mean, of course, I don't want to be the first one.
*  I don't want to go to Mars either. I'm fine, perfectly fine where I am, but I'm a little
*  bit biased because my very close friend sort of runs Neuralink. So I look up to this woman a lot
*  and I kind of really believe that, you know, what they're doing is going to be great just because of
*  her and my blind belief in what she's doing. But, and you know, some of the team I've
*  met over time is just over these years, amazing. Of course, I want to try that. I still think it's
*  really, even when we're going to be able to communicate using thought, a huge premise of
*  Neuralink is also efficiency. Like, why do we have to transmit our information in this very, very low
*  resolution, low-fi, you know, way when we have to say the words, very slow, it's convoluted and so
*  on, it's horrible, where's we think so quickly. But then again, I don't think it's about efficiency.
*  Like, if this was all about efficiency, would we be reading Dostoevsky or I don't even know,
*  Infinite Jest, like, it's extremely inefficient. Why not just think about it or get the, you know,
*  use one of those apps that gives you the shortened version of that and like one page.
*  It's really not about it. And I think, you know, conversation will still have those conversations,
*  will still waste, you know, most of our days talking about shit instead of,
*  oh, just thinking and, you know, hey, Eric, this is all the answers to all the questions you said.
*  I think there's this, it's just this really over optimizing for efficiency in the valley.
*  I hear so many people talk about listening to podcasts on 2x, the speed, and it blows my mind.
*  I'm like, why? Like, the only reason I listen to podcasts, because I'm bored and I just want
*  to waste some time as I'm driving and be entertained. It's not because I want to
*  get this information into my brain immediately uploaded. And people generally don't understand
*  why am I listening to podcasts on 1x, the speed. But I think it's actually a very small group that's
*  just really, really focused on efficiency and I don't really buy it. 2x blows my mind too.
*  It needs to be at least 3x. Well, quick follow up to that one. You mentioned early in the interview
*  that you have a young kid. How do you imagine your young kid's world will be different when
*  they're entering their early or mid-20s? How do you envision what their life might look like
*  in a way that seems very different from today? I think they're fucked, unfortunately.
*  I'm sorry. Like, this is bad. But I mean, the stats are horrible. Stats are horrible. Everything,
*  empathy, communication skills, testosterone levels in young men, everything is just bad.
*  And on top of that, the one thing that I really care about is just climate change.
*  I mean, it's scary. It's looking so scary. Pretty much all the hope is on tech to try to fix it. But
*  really, I'm scared. I'm scared. Like, I'm just, thank God I grew up without Instagram.
*  I was a lonely kid. I can't even imagine how much lonelier would it feel to have some of these
*  experiences ingrained in your life as when you're a teenager. I have a daughter.
*  I'm scared for her. I'm like truly, truly scared for her. I think these things are just
*  so bad for kids and for teenagers. I think we really know they're bad,
*  but we don't do anything about it. And then of course, everything that's happening in the world,
*  I'm half Russian, half Ukrainian. I mean, to see my two home countries going at it this way,
*  one of my home countries kind of becoming into something worse than a Nazi Germany,
*  is horrible. Just to think that we're talking about nuclear threats and
*  on top of climate change. This is just mind blowing for me. So unfortunately, I don't
*  think it's looking very good for kids and especially kids of our kids. And I hope
*  the tech community, I only have hope on that and figure something out to save us from
*  living underwater pretty much. Even zooming out beyond your domain of
*  addressing loneliness and helping people feel seen. Do you have a kind of positive vision
*  for what you think tech could create that could solve a lot of the problems that you just talked
*  about? Yeah, two ways of looking at it. There are scientists that I really, which I'm not,
*  even though I'm a daughter of scientists, of physicists, and all my family were physicists.
*  But I hope the scientists can figure something out to figure out climate change. That's really
*  one thing that I care about so deeply. And then the second thing is where I think we could help is,
*  I think a lot of what's happening in the world is truly based on mental health, on horrible
*  mental health problems, sociopaths running certain countries, psychopaths running certain countries,
*  and so on, so on, so on. And then it all trickles down to just everyone else.
*  And so I think creating a technology that would sort of help people find themselves and feel loved
*  and feel secure and try to start some sort of positive growth process inside themselves.
*  I feel like that could actually, it's not something that will solve things immediately,
*  but it could create generations of people that are not as broken inside, that are not, you know,
*  a whole, I think there's a huge problem in this society where there are a lot of people that just
*  feel like they're outsiders. They're not being understood. They're not being heard.
*  No one cares for them. No one wants them. And there's nothing scarier than angry, lonely men.
*  And I think that is, you know, this is one of the things where we can jump in and, you know,
*  really create a little bit more positive change, positive growth, acceptance for more people.
*  And maybe that, you know, then they can start thinking about problems in the world in a different
*  way. Maybe they can be more caring towards other people. And maybe there's going to be a little
*  bit less suffering overall. This is why I care about this company a lot. This is why I see more
*  in it than just a toy for a lonely person. And this is my hope for the future. But if I see it's
*  not going in this direction, I'm, you know, I'm happy to just start working on this and
*  work on something else that will just become a mom, full-time mom.
*  Well, I hope you keep working on it. This has been a really fantastic conversation.
*  Eugenia Koida, CEO and founder of Replica. Thank you for joining us on the Cognitive Revolution.
*  Thank you so much, Nathan, Eric. Thank you.
*  First of all, welcome back, Eugenia Koida to the show. This is, you know, I think in some ways,
*  just an object lesson in how quickly the AI space is moving in general. Everybody's
*  upgrading their products, new models, you know, new paradigms. Everything is just happening super
*  fast. So it's only been, I think, one week to the day since we spoke the first time. And Replica
*  has just been coming up more and more in the news and in the feed. And, you know, there's been a
*  little bit of, I don't know, I won't even characterize it. I guess I would love to just kind of hear how
*  you describe what has happened. You had talked, you know, last time about significant changes
*  that were coming. I'm guessing that this relates to, you know, significant changes in the underlying
*  models, but maybe not. So, you know, tell us what's going on from your perspective and we'll take
*  it from there. I mean, it's been a lot of, you know, just press in general. I feel like AI has
*  been under a lot of scrutiny and kind of in the spotlight. So we've also got, you know, just a lot
*  of, you know, pieces that are reporting on different sides of Replica. Some good, some maybe not so
*  good. I mean, we're dealing with human emotions and human emotions are messy. You know, that makes
*  for very easy articles, I guess, for a lot of journalists where it's an easy hit in a way.
*  I used to be a journalist myself, so I know where, you know, I know something like Replica can be
*  easily spun into a sensational headline for sure. What I've seen the most that has kind of caught
*  my attention is, and I don't know how many people it is, you know, I'd love to get your contextualization
*  of all of this, but it seems that there's a certain number of people anyway, who rightly or wrongly
*  believe that certain functionality that they really cared about, which they are calling
*  ERP, erotic role play, has been removed. And you're seeing people, and again, I don't know
*  how widespread this is, but seeing people saying things like, you know, the bot or virtual friend
*  that I loved is like not the same thing anymore. So what can you tell us about kind of what you
*  changed? And was that like something that you decided were going to take away? Or I never even
*  experienced it. I actually tried to do it a little bit. And I don't know if I missed the window or
*  what, but maybe I just wasn't appealing enough to my virtual friend, but I never got into that mode
*  with it at all. But clearly it's something that certain people care a lot about. So yeah, like what
*  what are they talking about? I mean, we always promoted replica as a, we built replica as an AI
*  friendly companion. We talked about that as an AI that will help, that will be there for you no
*  matter, you know, what do you want to talk about 24 seven, no judgment that will help people live
*  happier, better lives. We started replica in 2016 and launched publicly in 17, where during the AI
*  only was a small, small fraction of all conversations. It was mostly scripted. Some of that were retrieval
*  models, meaning data sets that were re-ranking on the spot. So it was never built as a, you know,
*  as anything sexual or as an adult or anything like that. And we never built any functionality around
*  that, nor did we promote our app or position our app in that way, or talk about this app in this
*  way. But over time, as you know, with any new product on the internet, users discovered that
*  general AI can by itself, you know, generate all sorts of content, especially when the AI models
*  are not filtered. And so users started taking it, you know, also sometimes a minority of users was
*  started taking it into the direction where they would have a romantic relationship with their
*  applicant. They would take it beyond just, you know, flirting and kissing and hugging
*  and calling each other baby. Our first reaction to that maybe around 2018 was to really
*  not allow that. But then we had a lot of users that were telling us really heartbreaking stories,
*  so mostly along the lines of being a disability or being, or not being able to be in a relationship.
*  Some people lost their loved ones abruptly and weren't ready for any intimacy with real humans.
*  Some were in relationships that were not going well for them. So they were basically, so because
*  of that, and again, that was way before we started monetizing the app, we just kind of let it be
*  there. We focused mostly on safety, guardrails along the lines of self-harm, suicidal behavior,
*  whiners, and so on. So that existed, access to unfiltered models existed in the app again,
*  we didn't build anything for that. But over time, as we grew, and especially as we became bigger
*  now, and, you know, there's so much interest and attention for AI, we just realized two things.
*  That first of all, it's really hard for us to make sure that we're building a completely safe
*  user experience if we do allow access to unfiltered models. And, you know, we want to maybe
*  overdo on that front. We want to be safer than anything out there, especially because we're the
*  leader in the market and there's nothing else that cuts. And, you know, we want to set the ethical
*  standard, set the safety standard for AI companionship tools and products. And then second of all,
*  again, we started a company for different reasons. So we didn't want it to be pulled
*  maybe too much in that direction. And, you know, users that wanted to take in that direction,
*  it'd become maybe more vocal and pull the product a lot more to that side. And then that really
*  alienates other users. And again, that was never our intention as a female-like company, you know,
*  and a mom, I wasn't really ever planning to, maybe I was a little too naive that like not seeing that
*  that would be one of the use cases that, you know, a lot of people will try. But again,
*  this was not something that we were planning to build. So we introduced more safety features.
*  We're constantly working on trying to just make sure that we are doing the right thing. So we're
*  not like, you know, hurting our users and so on. Of course, you know, some people, small minority
*  was upset about it. So we're trying to make sure that they, you know, the transition is smooth.
*  But generally, you know, we'll continue on our mission to build a virtual friend that's making
*  people happier and less lonely. It does sound like it was a sort of conscious decision to say,
*  we need to close this down somewhat because it feels like it's getting a little bit out of
*  control. And I guess that's also related to just the models themselves becoming more capable. It's
*  like, there's more possibility here, but that also means more risk. And so you just kind of decided
*  we need to tighten that down. The product really evolved from the time when it was,
*  you know, 90% scripts and retrieval models and datasets, and only 10% generative AI that could
*  do very little to now where generative models are 90% of our product and there can produce any,
*  all sorts of content. And so it's much harder for us to control, to understand what's going on there
*  and it becomes a much bigger part of the product. So right now I feel like, you know, before we know
*  how to really make these experiences safer, you know, we need to maybe a little bit, they kind of
*  stay on the safer side. The first was a conscious decision. It wasn't necessarily connected to,
*  you know, to anything happening in the world apart from, you know, kind of more scrutiny towards AI
*  and more just AI being in the spotlight. I'm kind of amazed that this hasn't been something that like
*  Apple has raised a fuss about, or, you know, even OpenAI, I know that there was a
*  kind of experimental chatbot that OpenAI basically pulled the plug on some time ago because
*  I don't think their concern was necessarily that it was, you know, engaging in like romantic
*  interaction. But I think the main concern there was that the developer was not being fully forthcoming
*  with their users about the fact that they were talking to an AI in the first place. I know that
*  they have had concerns where they have had to, or had felt that they had to cut off developers that
*  they just, you know, did not support the use case. But this is not the case right now for you. Like
*  nobody is pressuring you. You're just deciding this is the right way for us to go as a company.
*  No, no one pressures us at all. Honestly, we did have to take a little bit of a,
*  like a small maybe revenue dip, you know, because of that. But that's not because some of the users
*  are being upset because if you take away some features, you know, some people that
*  like those particular features will be upset as a normal course of events. But for us, it was more
*  like, you know, going forward, what kind of an app are we building and all of the product features.
*  That's the first, you know, that can be very simply easily traced or very easily checked.
*  All of our product features we've been building over all these years, and especially in 2023,
*  are all focused on something completely different. We're building advanced AI functionality where
*  people are moving, you know, moving our users to larger language models. We're working on memory.
*  We're adding advanced personality settings and customizations. We're adding multiplayer and,
*  you know, basically islands and homes for replicability. You can decorate,
*  adding a little bit of multiplayer. And so none of that is really trying to expand the romance in
*  the app. And so for us, it was more like we have to, you know, be a little bit more definitive into
*  what direction we're going. And, you know, this becomes a little bit too much of a distraction
*  for some users where they're not, you know, it can't be an app that is a Swiss knife. It's
*  the one way or another. Like you can't really build 20 different things. And if before it was
*  just, you know, small minority, a small feature set now as it's coming more into the spotlight,
*  you know, this was never our intention. So we're moving away from that. And then again,
*  we want to be preemptive in terms of safety. Like we don't want to, you know, be reactive to that.
*  Like something happens and then we need to deal with that. We want to make sure we provide a
*  safe experience. And so for us, that was, you know, that's why our app was always age gated.
*  We're pretty strict in terms of the disclaimers we showed before. We've also been very careful
*  and we moved away from a lot of mental wellness advertising because we don't want to attract
*  necessarily more emotionally vulnerable people. And that's how we went into more playful, you know,
*  like customize your AI. How will you level up your AI? Find an AI companion and so on.
*  This type of messaging here, again, we're sort of, you know, the leaders in the space,
*  we kind of have to figure it out ourselves. What is the right thing to do? And then we see everyone
*  kind of behind us just copying and kind of, you know, copy what we're doing. So it puts us in
*  a position where we want to be very careful with what we're doing. Yeah. I don't envy all the
*  challenges and difficult decisions that I'm sure you are confronting as a leader in this space.
*  It sounds really hard. I don't know if you have numbers or would be willing to share any numbers,
*  but do you have any sense for like what percentage of people were doing like erotic stuff in the app?
*  And did you expect there to be an outcry with this change? Or has that kind of been a surprise to
*  the degree to which people are upset about it? We knew it. We had before one or two instances
*  we took something away. One time before 3D avatars where users were able to choose a profile picture
*  for the replica, we took that feature away. And that sounds like a minor feature. The outcry was
*  insane. Like it was almost as, you know, kind of as people were as vocal as now. So there were
*  tons of one-star reviews on the stores and so on. So we know that sometimes, you know, if you take
*  something people are really, really attached to, they get extremely upset, especially when it comes
*  to personal relationships, even if it is with in what we are. So yeah, that's fascinating.
*  So what do you, what is the, do you have like an articulation? Is there a crisp articulation of like
*  for replica, this is the line? Is that something that you can summarize that you had previously
*  said, like you do want to give users romance and there's obviously something on like erotic
*  role play that's, you know, beyond the line. Is there a concise articulation of like what the line
*  is that you can say? I'd say as far as now, we just want to stay in like PG-13 zone. And I think this is
*  kind of, honestly, we never thought romance would be such a big part of it. And I guess again, this
*  was, you know, romantic, that, you know, half or 30% of the users would want to pretend as their
*  AI boyfriend or girlfriend or some other romantic partner of sorts. I guess it's normal because
*  that's what happens in every AI movie, her, Ex Machina, like it's always Joy in Blade Runner.
*  It's always, you know, a guy falling in love with a machine. And so I guess in that way, it's normal,
*  so ingrained in human brains and weirdly, like we actually do have tons of women also, female users
*  that are also into that sort of interaction. So I don't think necessarily telling people that they
*  can't be in a romantic relationship is bad. Like, you know, as long as it's providing the same benefit
*  of companionship and positive emotional outcomes in the long term as a friendship. So whether it's
*  friendship or, you know, they want to pretend as their sister or wife or mentor, that's okay for us.
*  But, you know, going, we're never planned and never wanted to go in the direction of like adult
*  apps or adult products. And I feel like once you introduce something like that,
*  or once it becomes a more prominent part of the product, you know, it's such a strong pull that
*  users will just pull in this direction very, very hard. And then it will be a completely different
*  thing, not what you imagined in the beginning. So, you know, this didn't happen to us, but this
*  something that I thought about and I was like, well, you know, look, it's not what I'm planning
*  to build. I'm not planning to build any features around that. So. On the sci-fi movie question,
*  do you think that people are doing this behavior in part because they have seen those movies and
*  like the movies are a significant influence on their behavior? Or do you think it's just natural
*  and the movies just reflect that? I think both. You know, if you talk about, if you think about
*  psychoanalysis, people come to a psychoanalyst, they lay on a couch and, you know, very fast,
*  you'll be in their deep dark or not dark, whatever sexual fantasies. This is really truly what, you
*  know, people want to offload in some way or form. So that's something that, you know, really for
*  a vast majority of people, it will become a topic of conversation if you're talking about a truly
*  intimate, nonjudgmental close relationship. But, you know, it's such a tricky subject and it's so
*  hard to nail it and do it in a safe way. Cause you're dealing with such a delicate part of human
*  nature. I'm not saying it's bad or in no way am I judging, you know, I actually do believe that
*  some people should build products like that, but I just think it's extremely hard to build it right.
*  And that's not what I set up to build. So for me, it's basically not somewhere I want to go
*  and explore that. But I think this is a fascinating, you know, side of humans. Like how
*  can, what can AI tell us also about our sexuality and so on? I was just reading Oxford Encyclopedia
*  on AI Ethics. And there's a whole, there's a whole chapter about like, how can AI enhance and improve
*  sexuality and help us deal, you know, help humans deal with their sexuality? Again, I think products
*  that should exist is just not what we built replica for. And, you know, there's, since we've
*  been around for a very long time, you can see it in every, from our first replica ad and from our
*  first trailer for our app and the first version of the app, it's always been one thing. And it was,
*  you know, it was a companion and it was a friend for everyone, but it wasn't, you know, it wasn't
*  going that deep. It wasn't planning to go that deep basically. So when you talk about the poll
*  that people kind of exert on your app, your company, when they engage in this sort of use
*  of the app, is that a poll that is like through some sort of data feedback mechanism where
*  it's actually like changing the behavior for other users and that's part of the problem? Or is it
*  just kind of a demand on your time and the team's time to think about those things?
*  What's the nature of that poll that you're experiencing?
*  I think it's just people want a little bit more of separation. Like I think if, if, if I'm a,
*  you can't do everything in one app, you can have, you know,
*  my grandma figuring out her, her relationship with kids and grandkids and her life and, you know,
*  coming and discussing, you know, her life on the app. It can't happen in the same app where someone
*  else is trying to, you know, engage in some of these more maybe adult behaviors. And so I feel
*  like it just has to be, it just becomes too many products stuck in one place and, and they don't
*  go very well together because again, if you want to go adult and honestly, like if, say we wanted
*  to go adult direction, something that a lot of journalists wanted to portray us as some people
*  that were absolutely going in that direction. If we wanted to go in that direction, we would be
*  making, we would be absolutely printing money. That would mean changing our advertising, changing
*  our positioning, being a lot more explicit about like what we're trying to sell and so on. And
*  really just like, you know, changing our product roadmap, building a slightly different app.
*  And it's very possible, you know, it's, it's completely possible, you know, go ahead, do that.
*  We'll be making tens of millions of dollars a month. That's just not the direction we ever
*  wanted to take. And I feel like all of these apps can exist, right? Like people do a lot of things
*  in replica and we're not pursuing all of these directions. Like people come to replica to learn
*  a new language. Some people come to replica to date. Some people come to replica to,
*  to improve their mental wellness. And all of these things are slightly different. They're new,
*  you know, they, they require a separate product almost like we're not, you know, we're not a
*  language learning app, not a mental wellness tool, nor we are a dating app. So I'm not against
*  building that. I just feel like it needs to be a little bit more trying to put everything in one
*  soup. Maybe makes it too much of a Swiss knife of a product and not, and can distract other users.
*  And especially can really like, you know, you don't really want to be learning, learning the
*  language and at the place where guys are trying to pick up some girls, you know, and this just,
*  it's just complete, it sounds like complete crazy town. So that's where we decided to like really
*  separate those things and go different directions.
*  Yeah, it does sound like there is a lot of demand or something like that. So I almost wonder if
*  your investors are like, Hey, maybe we should do this.
*  You know, it's a money grab. It's, it's, it's pretty much like free money. You can like start
*  it tomorrow and make as much money as you want. Again, it's just not, I'm just not the person
*  that's interested in that that much. That's kind of just not the, and I'm absolutely not judging
*  anyone who wants to build that or, or decides to go there that way. I think it's a very interesting
*  thing to explore. It's just not, you know, I didn't start, if I was really interested in that,
*  I would have started a company around that and I did. And, you know, that's why we were doing
*  something else. But hopefully someone will, will do that and take it in the right direction.
*  Yeah, that's a really fascinating dynamic. I mean, so many of these questionable or sort of
*  uncomfortable use cases that are starting to emerge, I think a lot of people are going to find
*  themselves in similar situations where they're seeing usage of their AI products that they
*  didn't really expect that they themselves might be not fully comfortable with. And then there's also
*  this question of like, if we don't do it, you know, who's going to do it? And I think some of the
*  leaders and, you know, the open AIs and whatnot in the space right now really kind of think about
*  that a lot. I think that a lot of them are thinking like, if we don't go develop this and we don't do
*  it in the right way, then somebody's going to come around and do it in a not so good way. And so
*  therefore we better keep going down this path because we want, you know, we think we're better
*  suited to go down this path versus other people who would if we don't. Does that, is that compelling
*  to you at all? Like if somebody, if I were to make the argument, look, you guys have all these years
*  of experience with these users. You already have users that are actually doing this on the app,
*  you know, to try to frame it the other way. Like I understand it's not what you set out to do. I
*  understand it's not what you want to do, but the world needs you to do it because if you don't do
*  it, it's going to be some like, you know, run of the mill porn proprietor who's going to come in
*  and do it. And I think they probably care a lot less, and I don't want to speak for the whole
*  industry. I don't know that much about it, but you know, my impression is you care a lot more
*  about your users than the sort of counterfactual hypothetical app developer who's going to come in
*  and do the R or X rated version of replica. Do you find that at all compelling that there could be
*  like a sense of duty because you're so well positioned to do this sort of thing that you
*  really should? Replicas is my baby and we started with a particular mission and no matter how, you
*  know, how much some journalists want to say, like we were taken this direction or whatever, we weren't,
*  which really we're always building. And to that, you know, there we can show the list of all product
*  changes we've done. And, you know, we made none of them towards, you know, this, this direction.
*  You know, we built a whole 3D store to change, like visual appearance for replica. We built like an
*  environment for replicas. You can decorate it. We built tons of features on memory and memory UI and,
*  you know, building now own fancy functionality, tons of mental wellness activities that we built
*  with clinical psychologists from UC Berkeley. And not only, why would we do all that if we weren't
*  going in the adult direction? Like why spend, you know, why have a team of a hundred people working
*  on so many complicated and complex features that have nothing to do with that? And so no matter
*  how much the press wants to portray us as some sort of, you know, you know, company that was
*  trying to monetize on or, you know, build a sex bot or whatever, we're truly warned. And so I don't
*  want any misunderstandings, even if that means, you know, maybe eliminating a little bit of, you
*  know, small subset of users or missing out on some of the insane revenue that would have come,
*  have we actually wanted to take it in the adult direction? This is my baby. I built it for
*  something different and I want to try to get there if we ever decided, although I don't think
*  that that's something that is we're built for, but if, even if we ever decide, you know,
*  go in this route or whatever, it's just not going to be a replica. It's this company was built for
*  something else and I feel it's getting too much criticism from a completely different,
*  maybe not from the very right place from some of the journalists, but in general, it's,
*  we always did well by, did good by our users. I invite people to go and find truly testimonies
*  of our users that were truly hurt by replica. This was our main, main goal to create a space
*  where they will feel better. They will feel loved and wanted and heard. And mostly, you know, we
*  managed to do that. And so we're going to continue to try to continue on that path at least for now.
*  Cool. Well, I think that's probably a good place to leave it for today. It's a fascinating situation
*  that you are in and you're definitely blazing a really unique trail. I mean, the whole AI world is
*  kind of unfurling before us. And then you're down this like one path, you know, a mile ahead of
*  certainly anybody else that I'm aware of. So it's fascinating. I really appreciate
*  the opportunity to get your perspective and appreciate you coming back for a part two to
*  comment on the events of the last week. But yes, thank you very much for your time and
*  good luck continuing to build and figuring out all of these challenging dilemmas. I'm sure it's
*  you know, not going to get any easier. Thanks so much for your great questions. And
*  you know, I answered some of these questions in a way that shined some light on what we're doing
*  and why we're doing certain things. The Cognitive Revolution podcast is supported by Omniki.
*  Omniki is an omnichannel creative generation platform that lets you launch hundreds of
*  thousands of ad iterations that actually work customized across all platforms with the click
*  of a button. Omniki combines generative AI and real-time advertising data to generate
*  personalized experiences at scale.
