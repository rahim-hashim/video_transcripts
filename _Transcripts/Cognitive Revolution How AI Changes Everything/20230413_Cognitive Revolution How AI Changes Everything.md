---
Date Generated: April 03, 2024
Transcription Model: whisper medium 20231117
Length: 6168s
Video Keywords: []
Video Views: 1847
Video Rating: None
---

# Pausing the AI Revolution? With Technologist Jaan Tallinn
**Cognitive Revolution "How AI Changes Everything":** [April 13, 2023](https://www.youtube.com/watch?v=R78mbtNeCvM)
*  There is a lot of reasonable discussion that happens in the labs and even between the racing labs
*  about the need to coordinate, need to be careful and people have become public about it, etc.
*  But there's always one missing component. The component is when. If there is this pause and
*  associated realization that these experiments are considered too reckless by society,
*  this will create some kind of incentive gradient for the companies themselves
*  to figure out how to make them in a more responsible manner and more legible manner.
*  But if we survive the life, the world, the universe could be potentially like
*  unfathomably better than it is now. So in a sense, we are living a lottery ticket and it is
*  in some way in our control to improve the odds. So that's what I'm doing.
*  Hello and welcome to the cognitive revolution where we interview visionary researchers,
*  entrepreneurs and builders working on the frontier of artificial intelligence.
*  Each week we'll explore their revolutionary ideas and together we'll build a picture of how
*  AI technology will transform work, life and society in the coming years. I'm Nathan Labenz,
*  joined by my co-host Eric Thornburg. Before we dive into the cognitive revolution,
*  I want to tell you about my new interview show Upstream. Upstream is where I go deeper with some
*  of the world's most interesting thinkers to map the constellation of ideas that matter.
*  On the first season of Upstream, you'll hear from Mark Andreessen, David Sacks,
*  Balaji, Ezra Klein, Joe Lonsdale and more. Make sure to subscribe and check out the first episode
*  with A16Z's Mark Andreessen. The link is in the description.
*  Hi everyone. Our guest today is Jan Tallin. Jan is a technologist, entrepreneur and investor
*  whose unique life journey has intersected with some of the most important social and technological
*  events of our collective lifetime. Born in 1972 in then Soviet Estonia, Jan was 17 years old when
*  the Berlin Wall fell and he quickly became a video game entrepreneur. Years later, he created Kaza,
*  the famous P2P file sharing platform that, at its peak, accounted for half of all internet traffic.
*  From there, he went on to co-found Skype, which eventually sold to eBay in 2005 for $2.5 billion
*  and for years remained the most successful internet company founded outside of the United States.
*  Circa 2009, Jan came across Eliezer Yudkowsky's AI Risk Writing, which he found extremely persuasive
*  and which inspired him to dedicate his time, resources and personal credibility to existential
*  risk mitigation with a particular focus on AI. Jan has since invested in nearly 180 startups,
*  including dozens of AI application layer companies and some half dozen startup labs that focus on
*  fundamental AI research. Those include DeepMind, Anthropic and most recently Conjecture.
*  He's done all this in an effort to support the teams that he believes most likely to lead us
*  to AI safety and to have a seat at the table at organizations that he worries might take on too
*  much risk. He's also founded several philanthropic nonprofits, including the Future of Life Institute,
*  which recently published the open letter calling for a six-month pause on the development of AI
*  systems more powerful than GPT-4. With so much happening in AI right now, I decided to take a
*  look at Jan's personal story and discuss Eliezer's baseline AI safety worldview only briefly in the
*  first part of today's conversation. Instead, we focused on the current state of AI development
*  and safety, including Jan's expectations for possible economic transformation,
*  what catastrophic failure modes worry him most in the near term,
*  how likely he believes next generation systems like GPT-5 are to literally end the world,
*  how big of a bullet we dodged with the training of GPT-4, whether in some sense we are lucky
*  that language models are softer and slower than alternative AI paradigms, which organizations
*  really matter for immediate term pause purposes, to what extent those organizations are currently
*  coordinating or slowing down already, how AI race dynamics are likely to evolve over the next couple
*  of years, what Jan and his team hoped to accomplish by calling for a six-month pause,
*  and finally, how it's gone and how he's feeling about it all now.
*  If nothing else, I hope this conversation makes it clear that the pausers are not merely Luddites
*  who have never built and don't understand technology. On the contrary, Jan's personal
*  achievements, world-class investment portfolio, and evident optimism for an AI-enabled future,
*  should we manage to build one safely, show that at least some of our most sophisticated
*  and accomplished thinkers take existential risks from AI extremely seriously. With that,
*  I hope you enjoy this conversation with Jan Tallin. Jan Tallin, welcome to the Cognitive Revolution.
*  Thanks for having me. Really excited to have you. You have been a, I think, quiet but major player
*  in the development of AI over the last 10 or so years now. I want to give people just a very
*  quick overview of who you are and the role you've played, and then jump to the future,
*  which is the present, and talk about all the things that have happened in the last few months,
*  as well as the call that you recently participated in putting out as part of the
*  Future of Life Institute to call for this six-month pause in the development of large-scale
*  models. So, a lot to cover. The world is moving faster than ever, it seems. But maybe just give
*  us a little bit of an intro to yourself as an investor in AI companies. You can tell a little
*  bit, if you want, about the story of how you came to be in position to invest in AI companies,
*  but really super interested in how you have managed to become an investor in so many leading
*  companies and the philosophy that supports that. Skip over the period of becoming an entrepreneur,
*  running my own games company, then getting into development of peer-to-peer technology
*  that culminated with Skype. Then at the end of my Skype career, I stumbled upon Eliezer
*  Rutkowski's writings and going like, holy hell, what the thing is? What is the world that I've
*  been born into? Having a meeting with Eliezer, like almost exactly 14 years ago, where I tried
*  to poke at his arguments, didn't find any holes. Then I thought, okay, how can I help? Sent them
*  some money, but I think more importantly, started taking those arguments, turning around and
*  presenting those arguments to people who would want to have some brand behind the person who
*  is making the arguments. That's how basically the Cambridge Center for the Study of Extension Risk
*  got started, where I convinced my co-founder Hugh Price there that these topics are important.
*  Max Tegmark, I think he already was very primed to these arguments, but that's how the Future of
*  Life got started, Future of Life Institute. The other strategy that I deployed was, okay,
*  yeah, I already was a bit of an investor and I thought that perhaps I could use my brand to
*  get a foot in the door in various companies who are developing potentially dangerous things.
*  So I did invest in a bunch of AI companies. I always had this dilemma of not wanting to
*  directly accelerate them, so I tried to not be a majority investor or anything,
*  but just enough to have a voice. With DeepMind, I actually had to walk up the demis at the conference
*  and that's how we started talking and eventually we became friends. I still catch up with him every
*  second time I'm in London or so. Once I already was an investor in DeepMind and eventually a board
*  member, getting the air of other AI companies became easier. So it's kind of like, just work
*  my way up, so to speak. Yeah, I think a lot of VCs would be extremely envious of your deal flow.
*  So I want to get back to that a little bit more in a second, but let's just go to the Eliezer
*  moment for a second. So you said this was 14 years ago, so this takes us back to
*  circa 2009. At the time, the deep learning revolution hasn't even really started yet.
*  At that point, a kind of highly... Well, you may object to this, but I would say for me,
*  I read it as a highly speculative yet very compelling thought about what might happen.
*  The arguments were, there was a lot of detail to be filled in where it was like, well, we have this
*  insane amount of compute and we're probably going to figure out how to use it and then that probably
*  goes very bad for us. So how did you understand or what do you think is kind of the strongest
*  version of that original argument and then what have been the biggest changes to that worldview
*  in the intervening time? Yeah, I mean, there are many ways to frame things, frame the problem.
*  Sometimes I've been asking people two questions like A, can you program and B, do you have children?
*  And then I get four different kind of framings or approaches I can explain the situation with AI.
*  One sort of simple argument is that, look, there is a reason why chimpanzees are not determining
*  the future and haven't been determining the future for a long time, if ever. And humans are,
*  but perhaps not for long because we are working furiously to get rid of that advantage that we
*  have over as they're kind of like the apex species on this planet. So once you're going to realize
*  that AI will likely not stop at human level, there is this unfortunate narrative, especially
*  that's very widespread in Asia where a lot of people think that we are going to make AI smarter
*  and smarter up to the point where it becomes conscious and then it's just like us. Then it's
*  just like other people and we need to integrate them, give them voting rights and whatnot.
*  Whereas I think this is just completely illusionary tale. It will probably not be conscious.
*  It will just be very competent and competence and consciousness. They might be related somehow,
*  but probably not. So we will have just control over the future yanked from our hands. So that's,
*  I think, for me, a compelling enough story. Yeah. So the linchpin there is we're the boss of the
*  world because we're the smartest thing around. And if we change that, there's a pretty good chance
*  that we may not be the boss of the world anymore. And not only that, but we really, at this point,
*  as things are starting to come online, we don't have a great understanding of what the new boss
*  would look like or what it might want or even how to conceptualize things like want in the context of
*  its internal working. So anything you would object to in my very brief extension and then,
*  how has your mindset shifted also from the purely theoretical, largely purely theoretical,
*  2009 Eliezer arguments versus today where we're in this world of large language models, obviously,
*  but also increasingly multimodal large language models and agent style systems like Agato
*  that can do all sorts of things. How has the actual development of the technology
*  changed how you think about it? Yeah. So many things to say about that.
*  I mean, first of all, yeah, I think I just agree the way you phrase things. Sometimes I've been
*  saying that, look, we are seeing the tail end, possibly last years of something like a 100,000
*  year period during which humans were the boss on this planet. And it could be even more extreme.
*  It's unclear if evolution will continue, if self-replicators will continue. Once you have AI,
*  just completely taking the solar system down to the atom levels and rebuilding it
*  and the rest of the universe. So it might even be the tail end of a four billion year period.
*  So how my thinking has changed? Yeah, it's, I mean, there has been this abstract argument that,
*  look, if we just continue on this trend, we're going to be accelerating towards a cliff. And
*  I think the current situation is that we seem to be starting to see the
*  shape of the cliff through the fog. It's possible that it is still a mirage and false alarm and
*  things will kind of level out. Then we need some new paradigms. But the current situation is like,
*  it seems more likely than not that this is it. And when it comes to
*  general trend, I think it has been very unfortunate in AI research with some silver linings.
*  The unfortunate trend has been like, we have gone from kind of more transparent, more understandable
*  paradigms to less and less understandable paradigms. We went from things like expert
*  systems. By definition, they were super understandable. People were just interviewing
*  experts and trying to kind of hand code the rules by which experts are making decisions
*  into machine. And that was like 80s. It was a really big thing in 80s. Then we went to
*  supervised learning where people were just labeling data in different domains, trying to
*  distinguish numbers. And this is where deep learning kind of started to shine first.
*  And now we are in unsupervised learning. We don't even care much about what data we throw. We just
*  throw a lot of data at AI and ask it to just figure it out in what kind of universe you are,
*  what kind of heuristics you should apply, what kind of skills you need to learn in order to
*  predict the next token. I call it the summon and tame paradigm, which is like you just
*  use these like multi-hundred million large experiments to summon an uncontrollable mind.
*  And then you look at what it looks like and try to tame it. And this kind of works if the mind is
*  not very powerful, but it might not work for very long. Omniki uses generative AI to enable
*  you to launch hundreds of thousands of ad iterations that actually work, customized across all platforms
*  with a click of a button. I believe in Omniki so much that I invested in it and I recommend you
*  use it too. Use Cogrev to get a 10% discount. It is wild to think about. I have a lot of different
*  follow-up questions I want to ask you as well, but let's go back and just touch on the investment
*  side for a second, because I think this will help people understand the point of view that you have.
*  It started with a series of blog posts in 2009, but now you're really quite the AI insider.
*  You did a recent interview where you kind of ran through your investment portfolio in
*  more detail, but I thought it was interesting how you split it into two categories, one being
*  like the fundamental AI research type company that you've invested in. I believe there's a
*  half a dozen of those. And then there's kind of the application layer companies, and it sounds
*  like there are dozens, maybe 50 plus of those. It seems like the big research companies would
*  be the ones that would kind of give you more insight into what's going on and what matters
*  most right now, but maybe that's wrong. Could you kind of just give a quick run through of
*  some of the highlights of the portfolio and we can get a little sense from that of kind of
*  all the different angles that you have on AI today? Yeah, I'm actually not the best person
*  to talk about my investments because I have mostly delegated the way to a team of a few people.
*  I still make the final decisions, but my focus really is philanthropy. But yeah, when it comes
*  to investments, the fundamental research, AI research companies, I specifically invested
*  not to make money, but to have some kind of influence over what's happening inside those
*  companies. So they are in some ways kind of adjacent to my philanthropy. And when it comes
*  to applied AI, I think the prospects of applied AI companies are much worse now than they used
*  to be before this large LLM paradigm. But of course, the LLM paradigm is very new.
*  So there was no way to know that five, 10 years ago. But currently, I think we have had this
*  discussion with you that the big problem with trying to build an applied AI company using the
*  LLM paradigm is that you have to be ready for the rock being pulled out from your next six months
*  to a year work by the next generation of LLM, which is like a new crop of capabilities that
*  have been bred. So in a way, the more domain specific is the AI competence, the more
*  value there is in building application layers around this competence. Whereas if you just get
*  increasingly generally competent minds, it's much harder to build applications in a stable way.
*  Yeah. One of the interesting things about doing this show and talking to all the people that we
*  have is not to spoil one of our closing questions, but we often ask what AI products
*  people are using today that they recommend to the audience. And I have been really amazed by how few
*  different answers we've heard. Probably two thirds of people have said like, well,
*  basically just use chat GPT. That's it. We get a couple other mentions, but it has led me to
*  believe that the application layer faces some very serious challenges. And it reminds me of
*  other hyperscaling platforms that we've seen over the last couple of decades where you build
*  around the edges of them, but the monopolist power is just so big. I do want to ask a little
*  bit more about competing trends between centralization and decentralization, because
*  I don't think it's obvious at all that it plays out as it did for Google and as it did for Facebook
*  this time around. Well, let's just cover the flagship. Maybe that's the wrong word,
*  but the fundamental research company investments. DeepMind was the first. I know that you're also
*  an investor in Anthropic and have supported Aught. I don't know if that's an investment
*  or if that's just a donation. Conjecture is on that list. Who am I missing on the list? And
*  I'm also really interested in terms of the conversations that you've had with
*  founders as somebody, just given your statement, I'm sure you said the same to them. I'm not really
*  doing this to make money. I'm doing it because I want to have your ear in case something important
*  comes up. How do people react to that? Do they say, yeah, that's great. I want you to be in that
*  position to have my ear or are people sort of like, I don't know what to make of you.
*  Maybe you're only investing in aligned people. Yeah, I found it really in general, my pitch as
*  an investor to deep tech companies is that, look, I'm investing my own money. I don't have a boss.
*  And I have a sizeable philanthropic corporation. So if I can do good by walking away from profits,
*  I can do that in a way that VCs are at least, for them it's harder to do that because they manage
*  other people's money. And for them, it's kind of like, in some ways, LPs are their bosses.
*  So A, I will be on the side of founders if they feel uneasy. I'm not going to push you to take
*  this defense contract or whatnot. And this usually goes down pretty well with founders,
*  because yeah, it's true. So who am I missing on the list? We got DeepMind,
*  Anthropic, Conjecture, OTT. Who else would you put in that fundamentals bucket?
*  I'm actually not an investor. I have sent some philanthropic money their way though.
*  So yeah, Vicarious was a long time investment around the same time than DeepMind and a few
*  other kind of AGI groups that are not as well known like Curious.ai, for example.
*  There is this Improbable.ai, if I remember correctly, in the UK.
*  Yeah, it's just like I have like 180 investments or something like that.
*  So I don't quickly recall all the names from that. But yeah, Conjecture, I think very highly of
*  Conjecture. In fact, whenever I go to London, I try to hang out in their office because they
*  are, they seem to be a company, a group that has the highest respect for AI in a sense that this
*  could be really dangerous. And the danger is the important part here to focus on rather than
*  whatever exciting commercial contracts we can squeeze out of it or something.
*  So let's talk about that kind of emerging paradigm of danger. I mean, this has obviously been all
*  over the discourse lately with the Paul's letter and Eliezer's timepiece. And I think broadly
*  speaking, the public is extremely confused because on the one hand, we have Eliezer and then on the
*  other extreme, we still have people routinely saying like, this is all just hype and it'll
*  never amount to anything, which seems crazy to me at this point, almost self-evidently,
*  this is a big deal. But that is still out there. And for folks like my parents who don't rush to
*  try chat GPT, they're just kind of hearing all these different messages from the media and it's
*  all just very confusing. So let's start with maybe the neutral or ideally even positive side.
*  People are throwing around AGI all over the place, a lot of disagreement or probably mostly
*  implicit disagreement on what does that even mean? Maybe we could just start with, what do you think
*  AI is going to do for us in daily life? We'll then extend to the dangers that it can pose. But
*  what is your kind of expectation for how AI is going to impact our lives over the next few years?
*  I think it's really dependent on how capable the planet is in constraining the large scale
*  experiments. Because if it turns out that we can't constrain them and slow them down, then we're just
*  going to die. That's my fairly confident prediction. If it can pause, then a lot of interesting
*  questions come up because the GPT level crop of AI is that we're going to continue improving,
*  even if we don't do new generational experiments, breathing experiments. Then even those could be
*  super disruptive. For example, I wouldn't want to be an art student in the year of 2023 because
*  it's possible that the skills that you're learning are somehow can be pivoted into something that
*  there will be societal demand for. But the answer could also be no, there won't be any demand for
*  your skills. I personally see that extending to a great many domains. We just did a little episode
*  on the possibilities for economic transformation. One of the things I'm trying to help people
*  understand is I feel like right now we are in this perfect little happy zone. You could call it the
*  Goldilocks time after the... I don't know if you know the Goldilocks story, but this feels like the
*  level of AI power that is just right perhaps in that 90th percentile on the American bar exam.
*  That's a really strong showing. That's base model GPT-4 capability. When you imagine what that can
*  start to power when it is fine-tuned, when it is integrated with other systems, when it's able to
*  take advantage of its ability, which we've seen demonstrated to use tools, and that's not yet
*  broadly deployed, but it certainly has been, I think, compellingly demonstrated. Then you add
*  onto that an even bigger context window that very few have seen. Then on top of that, you've got the
*  multimodal stuff. The latest models will certainly be able to browse around the internet and
*  understand websites and navigate and take actions online. It feels like that is enough on the
*  positive side to create transformation, really. Economic transformation is my baseline scenario
*  at this point. We're just at the beginning of the engineering phase of that, the deployment phase,
*  social figuring out of how it's all going to integrate. It feels like that could be really
*  amazing. Yet at the same time, it seems like still pretty safe to say that it's limited enough in
*  power that it won't become an out of control problem at this level. One of the things that
*  frustrates me most is when people who focus on AI risk also dismiss the power because you're
*  undermining your own message there. If you dismiss what it can do, then nobody's going to worry
*  about what you are worried about that it might do. Let's be very clear on just how capable the
*  systems are. I like your comment about conjecture having the highest respect for AI. I think that's
*  something I try to cultivate in myself as well. Do you see that any differently from me? Does it
*  feel to you like what we have is enough for economic transformation? Where do you think
*  we are on that? I have a lot of confusion about how the economy works in the first place because
*  I know that there are jobs whose main purpose is to make the boss feel more important. I don't think
*  these jobs are very vulnerable to AI disruption because boss would feel less important if that
*  underling would be replaced by AI. But I don't know how typical that kind of job is in human economy.
*  Also, I think that Leazer has pointed out that they just don't expect any changes
*  from AI before we all die because the rules and regulations in economy have
*  like sort of constrained everything to the degree where it just can't have like innovation that
*  is going to leave a significant mark on the GDP or have like a big changes in like
*  construction or something like that. Perhaps it's wrong, but I have like a significant uncertainty
*  about I definitely wouldn't be confident that we're going to get massive economic disruption
*  from the current crop of AIs. But it's very plausible that we would. Yes.
*  Yeah. I think what you said about just how much time there is for the transformation to play out
*  definitely makes sense to me. I've started counting time since the official release of
*  GPT-4. So we're at four weeks and one day into the GPT-4 era as of today. And I do think it's
*  really worth just kind of reminding ourselves and grounding ourselves in the fact that
*  no previous system that the public had any access to could really do the
*  sorts of high value tasks that GPT-4 can just do. And so we're literally, there's been a lot of
*  growing awareness. There's been interesting use cases. There's been like copywriting assistance
*  that have made a lot of money, but there was not an AI on the market until a month ago that had any
*  plausible chance of like giving you quality legal advice or quality medical advice. And now, that
*  is there. And again, we're just so early in starting to figure out how to use it. So it does seem
*  like that takes a little while kind of unavoidably. And I just want to remind the listening audience
*  more so than you, that window has just opened. And we have no idea what's about to start coming
*  through it economically, let alone in terms of alien AI overlords. So turning then to the kind
*  of things that you worry about, I think this model of AI strength kind of proceeding through
*  what appears to be a smooth loss curve, but what actually seems to be happening under the hood is
*  all these little thresholds of unlocking different discrete capabilities kind of being passed one by
*  one and all of that kind of aggregating. Yes, I love that paper. All of that kind of aggregating
*  to a smooth curve, but actually being like all these little discrete bits. I think that's a really
*  helpful frame. But I want to ask you, what are the things that you kind of most worry about?
*  If you could try to make this somewhat vivid for people, what are the big thresholds that you're
*  like, man, I don't know when, but an AI crosses that threshold and we're in real trouble. What
*  are those and how does that play out in your mind? So it's possible that there are many such
*  thresholds that we should be worried about. One kind of sort of neutral frame to describe what AI
*  is, is that it's an automated decision-making machine that is A, non-human and B, it is getting
*  increasingly competent by day. So, and like whenever, as every leader knows, whenever you're
*  delegating something, you're also like giving up some control over the outcome. So with that frame,
*  like there could be like many domains where we, in order to remain in charge of what happens next,
*  we should not delegate it to non-humans. So we should have like, as they call it, human in the
*  loop. But like the most obvious one that I can think of, where we are already rushing to delegate
*  things away, is AI development. So like once you have AI, LLMs that are able to develop,
*  AI is better than any human's research or scan. Then basically we have the most capable systems
*  on this planet appearing without any human help and possibly without any human consultation.
*  And then basically, good luck humanity. Yeah, that's interesting. I thought you were maybe
*  going to say the deception threshold, which is one I hear kind of thrown around most. I mean,
*  it's funny, it's striking for one thing that like, that's kind of OpenAI's explicit plan.
*  They're fairly high level, I would say, plan for AI alignment involves ultimately having
*  AIs kind of supervise themselves and refine the data set and hopefully bootstrap into something
*  good. That has never really reassured me that much either. Anthropic also doing something.
*  Yeah, constitutional AI, that is like specificity. Although this is like kind of AI's constraining
*  AIs rather than AIs developing next generations of AIs. I think this is important to distinguish
*  between those two frames. One big threshold is AI designing training the next generation of AIs.
*  Pretty, hopefully intuitive to see for people how that becomes potentially a runaway problem that
*  we don't have great control over. The deception threshold, you know, kind of outer inner alignment
*  mismatch seems like one that a lot of people worry about just as much. Any
*  personal thoughts on that one that you want to share? Yeah, I think like one thing that I,
*  unlike alignment community has sort of like learned over the last decade is that the shape of the
*  alignment problem has become like much clearer. Like for example, indeed this like inner outer
*  alignment dichotomy is something that at least myself that I had no idea about. Just like this
*  idea that deep learning paradigm and machine learning paradigm in general is training AIs by
*  picking essentially random minds out from behavioral classes. So like you're not
*  selecting AIs based on what they want. You're selecting AIs based on how they behave
*  and they could be like many, many motivational structures behind giving certain particular
*  behavior. The most scary one is basically realizing that it is being trained and then just acting out
*  the goal that you're training it for in order to kind of be selected and eventually escape the box.
*  Yeah, I think that one is hard to get around too. Just from the simple observation that
*  we're not super reliable. You know, anybody who's spent a significant amount of time
*  trying to validate language model output, even just for like a, you know, relatively run of the
*  mill application. Like I've done this at Waymark, right? We're making marketing video content for
*  small businesses and you know, really all the stuff we create is with language models. The main thing
*  is write a script for a like short commercial for a small business. So it's a pretty narrow
*  domain of space that we need to evaluate and yet it remains a real challenge to figure out like,
*  is this model better than this one? Or, you know, we do a fine tune. How does it compare to the last
*  fine tune? You know, you're getting all these like different outputs and it's just tough. Like
*  there's the over, the distributions are overlapping. You know, the rate at which the new model is
*  preferred to the previous one is often fairly low. I've seen published results as low as like 11 to 9
*  ratio, you know, where one is preferred to the other. Even GPT-4 to 3.5 is just 70-30 in terms
*  of preference. So like still a full third of the time people prefer 3.5 in a head-to-head comparison,
*  which kind of blows my mind given how like qualitatively better it seems GPT-4 is. So
*  that's just like the general problem of validation. But then you add into that mix that like we have
*  all these, you know, heuristics and biases that are exploitable. We have these kind of, you know,
*  cognitive gaps that have kind of lingered in our own systems. And, you know, evolution never had
*  a real reason to eliminate all of them or Greener never hasn't got around to it yet.
*  And so we're exploitable, right? And we're, everybody kind of knows that in our daily life,
*  like we know that people at a minimum will tell us like little white lies to make us feel good
*  or just to get through a situation a little bit easier. Do you see any promising route to
*  avoiding that sort of
*  exploitable evaluator problem? I mean, short answer is no. But it's very much like open
*  research question. So on a theoretical level, indeed, you would want to somehow kind of
*  hitch a ride on the increasing capabilities of AI when it comes to somehow making
*  more reliable or more constrained, more predictable in general. I hesitate to say
*  more aligned because like my model of Elias, it goes like, no, no, no, you don't, you don't point
*  point AI towards alignment. That's just like a silly thing to do. So, but yeah, like, like,
*  AI is going to get more capable. Can we somehow get something out of it that is scalable rather
*  than kind of ending in a predictably bad place? It might be worth just spending a little bit more
*  time too on, again, just kind of how these things might play out. I think Elias has spoken very
*  interestingly, compellingly about what happens when you go outside of your distribution of
*  training. And for humans, he just points out that, you know, basically everything in nature is
*  optimized for reproduction and inclusive genetic fitness. And yet the behavior that we observe in
*  ourselves is not at all about in the modern environment, does not appear to be about
*  maximizing our reproduction. And in fact, we didn't even know that that's what we had been
*  optimized for until relatively recently. So we were out here kind of doing whatever we're doing.
*  It took like a few random geniuses to figure out how we had actually kind of been created by nature.
*  And that has had relatively little impact on what anyone has actually done in their day to day
*  lives. So would you add anything to that story or observation?
*  Yeah, I mean, just to be more precise, I think we are selected for
*  inclusive genetic fitness ability to reproduce. And again, because of the same
*  problem that machine learning faces, that we can only select based on behavior or based on results,
*  that selection kind of effectively pulls in like a random instantiation of capabilities and
*  motivations that just happen to give you this particular behavior without having any fine grained
*  control over what these motivations and capabilities actually are. So yes, like
*  evolution ended up pulling us, selecting us in this ancestral environment where we had
*  developed a bunch of heuristics that were very useful for reproduction in that ancestral environment.
*  But much less so in the modern environment without like never actually ingraining in us
*  any fundamental understanding what we're being selected for. So the very same process might just
*  replay it when it comes. The very same process might get replayed as we are selecting AIs based
*  on behavior and without any insight on the inner workings of them.
*  So we could spend hours unpacking all this. I know you have done that many times. So we will
*  bracket that for the moment. We've got all these different failure modes. We've got potentially
*  kind of runaway AI training its own successors in a way that is not clear to us. We've got kind of
*  the deception problem. We've got the fact that we have no reason to believe really at all
*  that the goals that we have for AI will be represented internally. And so with
*  a sudden jump in the domain in which the AI can operate, it can be totally outside of training
*  distribution and who knows how it might act. Just like who would have expected how it would
*  act. Who would have expected how humans might have acted from the ancestral environment.
*  So all these things are pretty big conceptual problems. We don't have good answers to them
*  at the moment. How does that boil down to a simple worldview for you? What are the odds that you see
*  right now of serious catastrophe happening in say the next two, five, ten years? And maybe we
*  could segment that into given the trajectory that we're on versus how we might be able to shift that
*  if for example we've took a pause. My current estimate for kind of life-ending disaster
*  is basically one to fifty percent per generation of per like 10xing of compute that's being thrown
*  at these experiments. So and like currently there are 10xing things in like six to 18 months window
*  so you can kind of calculate from there. I mean at some point we're going to run out of compute
*  because like there's only so many 10xing you can do. So you can't like,
*  maybe probably can't do like thousands of thousands of those but still like
*  let's say something like a geometric mean of one and fifty percent is seven percent. So with seven
*  percent risk to everything. Like if you continue doing those we probably can still do like something
*  like five or six of those and at that point we are like more likely dead than not.
*  Worth taking a second to just let that sink in. Would you have put that same estimate on GPT-4?
*  Like do you think we just survived a seven percent
*  x-risk event with the training of GPT-4? That is a great question. Like with like hindsight is like
*  I'm like super anchored now right. So I really want to say no but again like the range is like
*  like seven percent is like this point estimate but like really my uncertainty range is like from one
*  to fifty percent and like so the interesting question is like would I put like less than one
*  percent confidence in GPT not destroying everything, GPT-4 not destroying everything
*  and probably not. So I think yeah it's I think it's unreasonable to to kind of have at least
*  the thing given the things that I knew with GPT-3 and like everything else and like things that I
*  didn't know like having a very close look at what's what's happening at GPT-4 in GPT-4 training then
*  yeah I think it's it would have been unreasonable for me to be less than one percent or confident
*  in less than one percent do from GPT-4. Honestly I can't really argue with you there when I look
*  at the you know when I got my first look at it it had already finished pre-training and initial
*  reinforcement learning. This was you know the six months ago when they kind of finished the first
*  version before any of the safety work and obviously there was the you know the whole red teaming effort
*  and and everything else. It definitely hit me pretty hard that like wow this is a significant
*  leap and now you look at all the papers that have kind of come out characterizing it
*  in the wake of the the official release. The thing that I kind of keep coming back to is we have
*  these smooth curves but then you have on individual behaviors you have these sudden jumps. So the one
*  they published in the in the technical report which isn't such a big deal obviously but maybe
*  you know indicative of things that could happen in the future on on more problematic dimensions is the
*  hindsight bias failure where it had previously been observed I think by Amthropic that bigger
*  models suffered more from hindsight bias and so it was an example of a you know an inverse scaling
*  law where the behavior is getting worse with bigger models and then all of a sudden with GPT-4
*  that problem is totally fixed and there is no hindsight bias and it basically just scores like
*  100% you know perfection on those hindsight bias problems which by the way are basically like
*  scenarios where you had a good bet available to you you like took the good bet and you lost
*  you know in an unlikely way and so the question then is like you know should you have taken the
*  bet and you know people might say in the hindsight bias we will know if I lost and I shouldn't have
*  done it when in reality like you actually had you know all good reasons to do it. So 3.5 you know was
*  actually getting this wrong more often than like 3 and more often than some smaller models but then
*  again boom some somehow some unlock has happened in the course of training and it was probably
*  never registered on the smooth loss curve which you know mostly looks smooth but all of a sudden
*  this behavior now is like totally strong I would say you know probably safe safe to say superhuman
*  in the sense that obviously we create these measures because some of us struggle with the
*  hindsight bias and so yeah you wonder okay with that's those kinds of things we do see those kind
*  of sudden jumps in capability in the context of GPT-4 like you know another 10, 50, 100x compute
*  scale up you know it's predictable that it will bring more of them but it's very unclear like
*  what exactly they would be. So 1 to 50 percent across these you know big scale up training runs
*  how do you think that plays out across different groups that might be running those processes?
*  I don't know you may or may not want to go you know to kind of specific names but like
*  obviously there's a few leading groups that can plausibly scale up another you know one or two
*  orders of magnitude right now do you think that's like equally reckless for any of them to do or
*  do you think some have a better handle on how to do that responsibly than others?
*  I mean the role is like some differences in various dimensions so yeah I mean just like hanging out
*  in anthropic feels materially different than hanging out in deep mind where I've both hanged out
*  at both places and like a little bit at OpenAI so like there's certainly like a much more kind
*  of safety culture in in anthropic does that justify risking everything like killing everyone
*  it's like I don't think so so it's like in some ways these are like second order it might be
*  that like second order effects so how kind of like safety conscious your group is compared to
*  the fact that you're taking just massive risks with everyone's lives right now?
*  So how do you think about you've been you mentioned two companies that I have serious
*  questions about I guess let's go deep mind first I've been kind of waiting for like a gato 2
*  to drop and it seems like you know as I check my my imaginary watch like it seems like that is
*  probably due right around now unless there's some sort of pause or like somebody's kind of thought
*  better of doing a gato 2 or you know maybe it just didn't work for some reason but that seems
*  unlikely because it seems like almost everything is you know quote-unquote working these days.
*  Do you have a sense for like what is going on at deep mind you know demis
*  published a time article it feels like a long time ago much more you know reserved in moderated tone
*  for a time article than then Elia's there's more recent one but still pretty striking to see
*  founder CEO of deep mind saying you know we need to think about slowing down are they slowing down?
*  I mean I don't know I don't have like that much visibility into deep mind I have heard about them
*  deliberately being more cautious about publishing things which is like empirical
*  I think that I haven't verified is that actually true but it feels that they are more careful now
*  when it comes to publishing in some ways where I kind of in a lucky world that like I mean all
*  the big free labs they are safety conscious to the level at least to the level of not dismissing the
*  risks in a way that for example Yan Le Kun or An Jung are just completely dismissing the risks
*  it's not obvious that the world should be in a way that like for example I mean I've been
*  praising Sam Altman for saying the things that he says about the risks and he's been very explicit
*  about the massive dangers that humanity is facing from AI. So another question is like
*  to what degree does this safety consciousness actually kind of constrain the actions of these
*  companies that have their own incentives as like non-human optimization engines and like necessarily
*  like necessarily some hard to lead like I mean the leaders of AI companies they have like a bunch of
*  conflicting requirements that they want to satisfy especially in like in deep mind where it's like
*  one big constraint is that they're not a company they are like a subsidiary of Google so it's
*  like I in some ways I'm kind of sympathetic to them trying to navigate like a must be complex
*  set of constraints and I think you're right I found myself saying this too like it is easy
*  to imagine people that are a lot more cavalier you know running the frontier projects so I'm
*  thankful that there does seem to be a profound like awareness and you know real seriousness
*  of approach across the the biggest companies. In some ways I also feel like we might be in a
*  lucky scenario in that language models are taking off and yet they're very like kind of soft edged
*  AI they kind of also run slow and I contrast that to what you know I think maybe
*  Eliezer sort of had in mind 14 years ago or what DeepMind was you know seemingly like closest to
*  if I had to say you know five years ago who was closest to AJ I would have said
*  DeepMind with all of their like you know game playing you know agent learning agents all that
*  kind of stuff those notably like achieve drum you know like AlphaZero and all that right they
*  notably achieve like dramatically superhuman performance in obviously like narrow domains
*  they also run really fast and they're trained if anything in like an even more alien way
*  where AlphaZero just plays itself right in in all these games and kind of learns from that and
*  doesn't even need to see the database of human games and and therefore when it shows up with
*  superhuman skill it's also like kind of an alien superhuman skill and you get these like
*  dramatically surprising moves that you know no human would ever have made. In contrast I feel
*  like language models you know everything has pros and cons right they certainly have insane surface
*  area but their kind of softness and slowness does seem like it might be a real advantage
*  relative to a more kind of hardened you know faster agent type of model. How do you think
*  about that do you think we are lucky with LLMs or am I just naive in my optimism? Yeah I think
*  like the big trend has been negative in terms of like going towards more and more black boxy and
*  kind of uncontrollable training regimes like yeah going from like expert system to supervised to
*  unsupervised learning. On the other hand yeah there are a few things definitely that we can sort of
*  got lucky with like one I would say the prime one is the fact that you actually do need a lot of
*  compute to do the pre-training of large language models which means that there are only like
*  a small number of organizations on the planet who can do that and like those training runs are
*  potentially very conspicuous so like only half jokingly say that like the planet is now
*  breeding alien minds in a way that aliens can see because like very plausibly you can see those
*  energy expenditures from space. So like that's like one lucky thing about LLMs and the other thing
*  yeah I agree that the speed at which or the slowness rather at which they're going to like
*  process things is another advantage but this is a temporary advantage I'm pretty sure because like
*  human minds human brains themselves are like offer like a concept proof of concept that like
*  no you don't have to be that slow this is like just like pure inefficiency and the other thing
*  yeah like once you have like some kind of feedback process where the
*  like LLMs will start developing AIs those AIs might no longer be LLMs.
*  What do you make of this current moment and this is something that's really just popped up and gone
*  kind of widespread in just the last two weeks but there's all these kind of projects to create
*  you know some one of them is called Baby AGI another is called Auto GPT and essentially
*  they're taking a language model putting it in a loop and kind of giving it you know the ability
*  to like have a goal you know delegate to itself go through these like thinking reasoning planning
*  steps you know then start to use tools and again like you know getting around the the kind of hard
*  limit of a context window through some sort of self-delegation. I'm struck by that as kind of
*  you know potentially the next convergence between those two paradigms in some ways and
*  it also seems to open up the potential for a kind of auto you know self-play reinforcement
*  learning like these agents are not very good right now and so if you go on Twitter you'll
*  see people being like this is so amazing look what this thing can do and then you'll see other
*  people being like it fails way too much these are not useful it's it's gonna be a long time
*  before they are useful but I kind of think those people are wrong in the sense that this is the
*  first language model paradigm that feels like relatively easy to evaluate in a fairly open
*  domain because you can kind of know like did the thing you know book you the flight or whatever
*  right or did it just get hung up on some API error that it never solved and it seems like
*  they're going to learn pretty quick from this like massive little agentization and you know kind of
*  exploring a paradigm that's just been set up like how worrying of a development is that for you?
*  There are several frames to look at this thing and these frames will kind of like give you
*  some very kind of opposing judgment about the situation for example one very positive frame
*  to look at this is that like it's great that society is like kind of like poking
*  and rattling and poking these current models to see like what are the extremes that you can push
*  them to because like they are not very competent and like by having those kind of experiments
*  with like chaos gpt and whatnot we will we as a civilization will actually learn like how bad
*  things could be if things would be scaled up so if you take like chaos gpt and put like gpt 6
*  I claim you might not be safe at all anymore on the other hand like you can take this frame that
*  that like when I don't know people like Jan Lakoon etc have been saying like like there's nothing to
*  worry about from AI because like it's not gonna be agent and even if it's going to be agent like
*  it would be just stupid to kind of install some like self-preservation and and and just
*  enough like bad goals like we wouldn't be we're not going to do that it's like no we absolutely
*  like a fraction of humanity has a death wish so it's it's like it's it's that is like enough
*  clear empirical demonstrations that like if something really bad can be done with AI it
*  will be done yeah it's a big world and it's pretty easy I mean that's the other thing that's amazing
*  like I think the first commit of the baby AGI project which I believe has been the number one
*  trending project on github over the last couple weeks alongside like a couple other very similar
*  projects the first commit I think was 105 lines of code and that's all it takes you know a couple
*  clever prompts and a you know and kind of a loop and you've got yourself a little agent you know
*  and it might not do much yet but the bear given the model the barrier to you know creating some
*  sort of semi-embodied you know autonomous version of that is proving to be extremely low so yeah I
*  don't think we're gonna we will not be able to rely on the good discretion of users in the long
*  term certainly probably not more more than for a few days with the release of any major new system
*  you mentioned a minute ago you said three kind of leading groups and I wanted to ask you
*  how you think about like who is at the frontier and who is maybe going to be at the frontier over
*  the next few years I assume the three you had in mind you didn't specifically say open AI but
*  obviously they're in that group DeepMind was the other that we were discussing and then I
*  am guessing you're thinking Anthropic would be the third I think DeepMind and Google they're
*  gonna like could be kind of interchangeable I kind of hear that they are kind of email
*  publicly mentioned that they're somehow kind of joining forces when it comes to this LLM race
*  so nobody else you feel like is close enough at this point like if it's a coordination problem of
*  who actually you know who are you calling on to pause I mean you're calling on everyone to pause
*  but it sounds like it's really those three organizations that you're calling on for a pause
*  yeah I think they are like the first so-called like first tier when it comes to doing like the
*  most dangerous experiments but like of course then you have like the second tier I think Aliezer has
*  this kind of related law like the Moore's law of math science that I kind of forget exactly how
*  it was framed it was like every every two years the IQ needed to destroy the world drops by like
*  one or two points so it's as the hardware companies mostly Nvidia at this point will
*  throw in more and more capable and cheap computing cycles at the market the world destroying capability
*  will be in in in a larger number of hands interesting to think about how that evolves
*  over the next few years do you think if you imagine that let's say we do enact a pause and
*  then you know meanwhile Nvidia keeps shipping and people keep kind of doing you know fundamental
*  which notably the letter explicitly goes out of its way to say like we're not saying all AI research
*  should stop or that you know you can't build your small models or fine-tune things for your use
*  cases and so on so if we imagine a world where there is kind of a pause on these high-end
*  experiments but hardware continues to ship and generally speaking like the field is not shut down
*  do you have a guess for how many folks would be in the kind of you know would be able to do a GPT-5
*  type project if they chose to in say five years five years is a super long time
*  I'm with you on that by the way I don't even try to guess things five years out so I shouldn't ask
*  yeah two years I mean yeah probably a dozen is something that kind of like it's just pulled out
*  of thin air if I would think about it then I would probably have like I mean it's probably less than
*  hundred more than ten perhaps closer to ten than ten to hundred is my answer in two years and so
*  some of those we can kind of fill in pretty obviously right like meta seems like it would
*  be a very natural candidate Microsoft I mean they have the open AI partnership they certainly also
*  have their own big research division presumably Apple you know has the resources to get into that
*  game maybe even like a Tesla I mean they're focused on other things at the moment seemingly
*  they also have the bot the Tesla bot is going to need some sort of you know fairly general
*  intelligence to help it walk around and talk to people and pick stuff up any other kind of
*  specific actors that you think would be likely entrance there and then how do you think about
*  kind of the international scene like is there anything coming out of Europe and obviously
*  then everybody starts to think about China too yeah I was going to say that like the obvious
*  like in two-year perspective yeah you should kind of like start also like
*  looking at like non-US actors in Europe where it's kind of yeah compute is much more available
*  and then also China I think they're like one sort of obvious counter argument that we're getting with
*  when it comes to like calling for the pause is that like what about China and like my answer
*  there is that like currently China does not seem to be in the race at least not as intensely
*  as the leading US labs are in a race between themselves and second like sort of almost like
*  culturally Chinese seem to be like much less keen on pulling a bing and just like on unleashing
*  uncontrollable mind on their territory I mean only half joking you say that like in China if you do
*  that as a tech CEO that might get to disappeared but yeah in the longer run of like two or more
*  years it is big will become like more and more important to get some kind of international
*  uh agreements that we already have in nuclear for example in place for also for compute
*  on the China front I totally agree it's like I don't know why we would assume
*  given the posture that the Chinese government has taken toward technology over the last few years
*  that this is the technology that they're just gonna throw caution to the wind on right I mean
*  they've shut down functionally shut down their entire video game industry and limited it to as
*  I understand it just like a few hours on like a couple weekend nights per week is like all that
*  like video game companies I think can even operate in China now we should fact check me on that but
*  that's what I understand to be the case and online online learning I understand is also being mostly
*  and that one is is fascinating too because like who could object to online learning but
*  my sense is that they intervene there on like a state level because they feel like there is an
*  unhealthy market dynamic developing where people are like working too hard on these you know yeah
*  on these like whatever standardized test measures and like putting way too much resources in this
*  and it's like gone past the point of like societal benefit yeah I mean I I'm actually a chairman of
*  a language online language teaching company uh an Estonian Estonian company that has like big
*  linguists yes oh yeah you know it of course and uh and like one thing that they learned in
*  Japanese market is that there's like a massive English teaching market in Japan but people aren't
*  caring don't care about learning the language they just want to pass the tests in order to get like
*  better employment options so like like you're not like this is like as a language teaching company
*  in Japan your job is not to teach the language your job is to get people pass a good get good
*  test scores which is a very different job so I say suspect that something like that was
*  also happening in China I would not take personally I'm not ready to uh say by any means that I want to
*  you know fully subscribe to uh Ji Jinping thought or live under the technology regime there
*  but it does seem like we're jumping to a conclusion way too quickly uh when we say like well if we
*  don't do it they will and you know that'll be worse but leaving that aside for a second just
*  coming back to ourselves the other big thing that has come out this last week or so is apparent and
*  I take that it's probably legitimate a sort of leaked uh fundraising document from Anthropic
*  that says that they are planning to raise five or so billion dollars and kind of see the next two
*  three years as like super critical uh planning to do like next-gen models you know they're kind
*  of immediately moving into a gbt5 you know type uh scaling regime it sounds like the model itself
*  you know supposedly is going to cost like a billion dollars to train and then they say
*  um again this is all according to reporting I haven't seen the deck but those that fall behind
*  in the 25-26 time frame may never be able to catch up so when I heard that I was like man
*  that does not sound like a company that's about to pause um it doesn't sound like I mean it does
*  sound like a company that's kind of in the race now um how are you viewing that news and I don't
*  know if you have any you know inside view I don't obviously wouldn't ask you to um you know to share
*  anything that you shouldn't but what should the public make of that that you know update from
*  supposedly like the most safety-centric leading lab that there is yeah I mean like to that
*  to the degree that this thing was accurate which again I can't comment on uh because I'm uh
*  investor in Anthropic and board of board observer as well I think to the degree that it was accurate
*  it is strong evidence that there is like a massive race happening between the US companies
*  that is going to get us killed so like can we stop that race please what exactly is the thought
*  process knowing that this group came from open AI they you know high level description that
*  I've heard is you know they felt like it was becoming too commercial they wanted to be more
*  focused on a safety first type of approach and that's been you know two years or whatever
*  now this like what the only thing I can come up with is people must be thinking we'll do a better
*  job than they will do so therefore we should do it before they do it because if we don't then
*  they'll do it they'll do it worse um and it seems like maybe everybody's thinking that I kind of
*  model like open AI to some degree that way as well is that how you think about the decision makers
*  or what do you think they're thinking so I think like one very informative uh public uh piece of
*  information is like future flight institute podcast with Dario and Daniela Amadei uh that
*  was like during covid about a year ago um like yeah in early early 22 well basically kind of like
*  they explain like what the approach of Anthropic is and as far as I know it is uh kind of uh it is
*  true what what I said in this podcast is basically I train the frontier models uh and then basically
*  dual alignment in an empirical fashion while having access to frontier models and the claim is that
*  it is exactly because of these emergent capabilities there is only so much uh you can do
*  using uh not state of the art models uh because uh in some ways as the models get more competent
*  uh they also become easier to interact with um and like I mean just the fact that we have language
*  models in the first place like that is in some ways I think you pointed it out this is uh
*  I can also like serve as a good interface uh when it comes to alignment so like yeah that is kind of
*  Anthropics uh uh all tropics thesis uh to be to have like this uh latest models and uh and then
*  basically use those to do like state of the art uh alignment uh that is kind of like empirically
*  tied to the actual objects uh that we have now of course the big question there is like
*  how many generations you can do that for like like because like the pre-training is like largely
*  uncontrolled unsupervised process like how many generations uh we can do the pre-training
*  safely not to mention things like leaks to elsewhere of the of the resulting weights
*  so like I think it's a genuine dilemma it's it's uh in some ways I think Anthropics framing and
*  perspective makes a lot of sense because indeed like the largest like the you can do more get
*  more useful alignment work done with the latest um crop of the of the large language models
*  but on the other hand like each training imposes like some risk again in my estimate one to 50%
*  risk of complete annihilation of the planet how do you navigate that trade-off it's it's not obvious
*  and I don't think there's current enough kind of thinking into into uh into that trade-off
*  um again it feels like there's some sort of you know game theory element here where it's like
*  it seems like they're doing it because they believe someone else is doing it still on some
*  level right if they if they roughly share your view and they're like well the only way we can
*  do the alignment work is if we have access to the latest models then a good current counterargument
*  would be if it were true well nobody else is going to create these if you don't so maybe you
*  should just sit tight too and then we can all kind of you know study what we have we don't need
*  another frontier just yet so it still seems like it is fair to say there that like
*  on some level they feel like their hand is forced like they can't not do it because
*  they're either in the game or they're out of the game but the game will continue
*  regardless it seems to be the the model that's kind of implicit in that decision making
*  yeah I mean they're like a few models that are sort of consistent with evidence
*  but that definitely is uh like one model because of like these race pressures they're gonna feel
*  that like if they don't have access to the latest uh generation of models they like can
*  their prospects of actually doing alignment are significantly hampered
*  so that's like the positive way of framing things the letter even like I think explicitly
*  mentioned that uh one goal of this letter is to like call for this timeout in this race
*  that indeed has like a non-trivial generatic element I think your most recent kind of
*  fundamental AI research investment is in conjecture and if I understand correctly
*  I may be wrong on this but I don't get the sense that they are planning to like try to train a
*  GPT-5 in the short term how are you thinking about kind of their contribution their strategy it seems
*  like they take a different approach where they they don't feel like they have to create the
*  frontier models in order to do something useful yeah I am much more positive about conjecture's
*  approach in terms of like safety capabilities trade-off they still train language models but
*  they do not do like kind of latest language models their goal is to kind of like compose
*  less capable language models in a way that kind of makes a more makes for a more predictable
*  structure so you're like you don't risk the world during the pre-training phase
*  and have like a more in some ways kind of more old-school approach to AI rather than this
*  someone and tame approach yeah interesting we just did an episode with um Andreas and Jung Won from
*  AUT and they have a very similar you know kind of outlook to composition of models you know the sort
*  of traceability of all the logic you know atomization of the different decisions and
*  operations to try to create you know some sense of kind of designed control you know into the
*  system from the beginning uh sounds like conjecture is kind of on a similar line of thinking yep and
*  I think I think we managed to get the pause uh in this like game theoretic race um assuming it is
*  a game theoretic race it does also like some frame that says that no this is like uh just
*  apocalyptic death cults uh trying to end the world uh this is like the least least charitable
*  frame uh but uh if it could get a pause uh then like uh I do think that like there's like this
*  almost automatic pressure to get like more competence out of the minds that we already
*  have trained and part of it is just like to have like a better understanding a better composition
*  of the capabilities that we already have because like one important bit like as
*  a lot of your listeners probably know is that you have like this training phase
*  that is like much much more expensive than actually the inference phrase so like once you have like
*  spent a lot of compute on training once you finish the training you have a lot of ability to run
*  many many uh instances of the of the minds that you just just trained so let's talk about them
*  later so you guys obviously we've you know we've covered a decade of your uh thinking and investment
*  on this subject and now we get to the point where okay gpt4 is released it is you know closing in on
*  human expert performance in a great many domains it does seem to me like it's quite unclear like
*  what the next generation of that would bring obviously you guys are thinking you know something
*  um very similar there so how did this project come about for a letter like how did you guys settle on
*  a six-month pause how what was the process like of like trying to bring a broad coalition together
*  and was this something that you guys like actually thought might happen or you know is it is it kind
*  of a intended to be a conversation starter like how do you think about this project good questions
*  uh i remember we had a fli catch-up call on uh on march 21st so uh i mean less than a month ago
*  and uh we were kind of like we had already observed that there are like significant voices
*  uh like in in the public i mean the gesra clines article in new york times had come come out where
*  he kind of explicitly compared the current ai race to like summoning uh summoning minds
*  and i think harari's article also was published around that time where he kind of was concerned
*  about lms plugging into into the operating system of civilization which is like about language
*  and which operates on the language level so and then like numerous discussions uh
*  so private discussions that are very concerned uh about uh about the current race including
*  with people in the labs themselves uh so we thought that okay perhaps like one valuable thing that i
*  felt i could do uh is to try to create some kind of common knowledge that yes like a bunch of
*  people are worried and i'll basically create a situation where like those people know that
*  other people are also uh worried and and the other people know that that the other people know etc
*  and we thought okay we have some experience with uh with um open letters so perhaps we should like
*  try to draft draft one up and of course like our previous open letters had something like
*  thousand or less less than thousand signatures so like one thing that we got just completely
*  blown away by uh was was just a reaction uh like kind of immediate in the first few days we got
*  like tens of thousands of signatures uh and we had like technical problems because of that
*  so yeah like the when it came came to kind of drafting the the letter uh there were like
*  multiple considerations like one consideration was just speed so like clearly if it would have had
*  like several weeks to work on it uh then uh then it would have been much better but like it was like
*  sort of done much in the spirit of like not let's let's not have the perfect to be the enemy of the
*  good and and let's draft up something that uh that feels uh okay to put out uh indeed like the
*  six month number was like one thing that kind of uh was put in and then put taken back out of
*  from different versions of the draft uh and uh argument finally that kind of one nowhere the
*  six month thing was that like one question we get like so like why six months why what can you
*  do with a six month what can you demonstrate with a six what would six months pause bias like one
*  one important thing that people don't necessarily realize that six month pause would bias is is
*  confidence that we can pause and so and like in that that sense it's uh it's better to have
*  a proposal that calls for six month pause to fail than a than a than a proposal that calls for
*  indefinite pause to fail because like the indefinite pause situation people go oh yeah if it would have
*  been six months of course we could have could have done it but like because indefinite like nobody
*  would pause indefinitely right so that was like the final final kind of like reason that okay
*  let's let's put in six months and see what happens yeah i thought uh zvi had some great analysis of
*  this in the first uh somebody who also works with a speed premium that i appreciate um had some just
*  i thought pretty ultimately simple but but still very wise analysis that's just like
*  if you want if you feel like you're going to need coordination in the future it makes sense to start
*  building it now and if you need if you you know can only get a little bit going at first then
*  you kind of take what you can get and you hope to build on that foundation yeah the other kind of
*  like really big uh consideration that kind of fed into the open letter is that like there's a lot
*  of reasonable discussion uh that happens like in the labs and even between the racing labs
*  about the need to kind of like coordinate the need to pause need to be careful and like people have
*  been kind of public about it etc but like there's like always like one missing component the component
*  is like when they like they always say i think scott alexander wrote a good article about uh
*  about open ai's like a gi and beyond uh statement where he kind of also pointed out that like yeah
*  they're saying a lot of nice things which is great i mean honestly it's good uh but they don't say
*  when and like this is should make a little bit suspicious at least and so therefore like the
*  one of the rationales for the letter is that how about now so what have i want to go through some
*  additional kind of questions that i've seen floating around in the discourse but what is
*  your sense of kind of how the reaction has been obviously there's been a ton of signatures even
*  one notable ceo of you know a company that i would say is like close to a leading lab which is a mod
*  from stability uh signed on to it i thought that was fascinating as far as i know it's been pretty
*  quiet reaction from the kind of three main groups you know that are the ones who are going to either
*  pause or not pause you know for something beyond gpt4 so what do you make of the reaction from them
*  like has there been any there's been no public statements as far as i know and has there been
*  any kind of private or you know sort of confidential reassurance have you i mean has
*  there been any reaction from the the leading labs yeah i mean i i think like sam meltman like said
*  publicly that like he's uh kind of like with the spirit uh of the of the letter uh like appreciates
*  the spirit of the letter or something uh but then there was of course a but i don't even remember
*  what the what exactly was uh but yeah like a deep mind definitely hasn't said anything
*  nor has an tropic although like i just today read uh jack clark's uh newsletter uh import ai
*  where he kind of like mentions the letter and also says like why he didn't mention it last week
*  so uh story there's like some uh some reactions but like
*  uh also kind of like want to be careful here in the sense that i don't want to like create some
*  self-fulfilling prophecies uh i would say that the possibilities are definitely very much open
*  at this stage uh for for the letter somehow catalyzing uh an actual pause uh but it's like
*  double digit uncertainties both ways cool that's actually more positive
*  of a response albeit minimal than um than i had expected or even understood i haven't seen that
*  sam altman tweet i'll have to dig into that and yeah i don't think it was a tweet i think i hope
*  i'm not just like he didn't see it in in my dreams or something uh but i think it was actually a
*  comment in in some news article uh about uh about the letter humans too suffer from hallucination at
*  time so we'll fact check ourselves indeed we do so okay so we you guys put this out there now
*  people start to say all kinds of stuff right um the thing that i think is kind of most like you
*  didn't finish reading the letter uh but which is definitely worth giving you a chance to to kind
*  of respond to is well all the letter says is a pause you know it doesn't ask for anything else
*  so it's stupid because like what are we supposed to do again like you can read the letter for
*  yourself there are definitely some things that it calls for but just kind of you know obviously
*  it's a big tent you know committee sort of document just zeroing in on your own you know
*  priorities what do you think are like the most important tangible concrete things that we could
*  do over say a six month time frame such that maybe even you would be comfortable you know ending the
*  pause like do i guess top priorities and if those top priorities happened would that be enough in
*  your mind to then end the pause and like trade a gpt5 so so one thing that will sort of like
*  happen automatically is that we will get more experience with the crazy situation now that
*  we're in which is that we have i think i think joshua bengio put it or joshua bengio put it
*  that now we have ai's out there in the internet that can pass a turing test
*  and that is a novel situation like and i think we are would be much smarter six months from now
*  than we are now because six months will be a long period long long chunk of time when it comes to
*  living with aliens on on on your planet so like i don't know what we're gonna learn but hopefully
*  we just like are smarter in the in the autumn than we are in the spring but like when it comes to
*  more concrete things yeah there's like i mean nil nanda a researcher in uk who has worked with
*  deep mind as well as anthropic has this blog post called 200 open problems in mechanistic
*  interpretability so like there's like so much work that can be done with even like the previous
*  generation of of models not to say anything about about the latest generation so there's like so
*  much kind of alignment work and just opening up those black boxes and trying to understand
*  what makes them tick how can we how can we get any guarantees about what what the next generation
*  is going to do so again with with research and with kind of lived experience i hope we will be
*  in a much better i mean i hope we will be in a much better but like realistically i just say that
*  we're going to be in a better place six months from now that doesn't sound to me like you would
*  expect just on general kind of improvement to get to a point where you would then say
*  okay let's end the pause and do the next generation is there anything i mean correct me if i'm wrong
*  on that but maybe framing the question slightly differently if we abstracted away from the six
*  month time frame and said like are there concrete structures of some sort that we could put in place
*  that would on any timeline you know kind of give you enough confidence or reassurance that again
*  you can say like okay it seems like we could we're now in a decent enough place that you would
*  personally be comfortable going ahead with a a next generation training i still like the big
*  question is like how much risk are we okay taking and i'm not saying this risk should be zero because
*  there is always like this background risk of existential risk we could be hit by asteroid
*  there's a certain probability that like this call will not end because the plant will be hit by by
*  like some probably not an asteroid because we can see this kind but by by comet that that is kind
*  of harder to see i understand so like we shouldn't get the risk to zero but like perhaps there are
*  like if there is this pause and like associated realization that like these experiments are
*  considered like too reckless by society hopefully we will get this will create some kind of incentive
*  gradient for the companies themselves to figure out how to make them in a more responsible manner
*  and more legible manner so i am like i mean there's this project in arc alignment research
*  center in berkeley led by paul cristiano called evals like evaluating models about like what
*  what are the things that that they could in principle do other things that that they're
*  still kind of incompetent on when you can have like multiple opinions about this but
*  or different opinions about this but like there's like a generalization of this like is it possible
*  to replace the current blind as far as i know training runs with something that you like at
*  every iteration you do some tests that would give you some kind of guarantees about about the alien
*  that you're summoning yeah there's a few other things that i i think give me a little bit of
*  hope that they might be developed on the kind of a technical level during that time there was an
*  interesting paper that came out from i think this was a pretty small scale yeah i believe it was
*  adventropic where they experimented with doing the pre-training on a human preference data set
*  from the beginning as opposed to just kind of you know random decent quality text off the internet
*  and it seemed that the upshot of that was that you never had quite as alien of an alien in the first
*  place you know on measures of like harmfulness you know for example helpfulness it it stayed
*  much closer throughout the training process to kind of that final post rlhf or rl aif state that
*  we know now and never kind of dipped as far into the sort of you know just very strange alien
*  territory of of general pre-training so something like that kind of changing the
*  the data set maybe from the get-go seems interesting another recent thing that jumped out to me was
*  somebody just i think in the last week or two just published a a result where they showed that they
*  were able to increase the size of the model progressively throughout training it was kind
*  of presented to me as like a efficiency thing that does go to show all these things are kind of
*  pros and cons you know everything is dual use but the net savings on the training flops was like 50
*  percent which is obviously significant enough for people to take notice you know even for just
*  purely commercial reasons but then it seemed to me like boy there's something really interesting
*  there where you're creating kind of this seed you know kernel like truly a little baby version of the
*  the model that is actually just a lot smaller in terms of its parameters and then you're able to
*  kind of layer on more and more layers parameters you know as you go through the the whole training
*  process it seemed like maybe there would be a way to you know zoom in on that small thing and get
*  something kind of working right in like the small you know presumably much more interpretable version
*  first before you know growing the model itself and kind of just having one big you know tangled knot of
*  of parameters i guarantee that it will not guarantee like safety in the long run but it
*  might just be enough to reduce the probability of destroying everything in the next generation
*  so we can actually kind of like do one more step in a way that is much more responsible than the
*  current default so one thing that you have not gone too much here is regulation government
*  intervention i mean the letter does call for that if there's if there can be no pause then government
*  should step in and insist on one but you're not putting a lot of you know maybe just haven't got
*  to it yet but i'm not hearing anything from you that's like government can come in and set up a
*  regime that's going to do much for us is that your do you have any hope in a sort of government
*  regulatory approach oh yeah i do i'm glad that you asked this also like reminds me that like google
*  in the in the form of sundar bichai did react to the letter like there was a podcast
*  hard fork new york times podcast where he said that that like the discussion started by the letter
*  is great but he taught that like government intervention is necessary here that is not enough
*  to rely on on the labs kind of self-regulating so like i think like one silver lining that i
*  already mentioned is that like this pre-training is is super expensive and super visible and
*  therefore like like kind of the metaphor that i've been using is like the nuclear
*  control that the nuclear nuclear arms you also have like these two phases one is like the
*  hard step of enriching uranium which is like super energy energy intensive as well as like
*  much more visible than the second step which is harder to control which is the proliferation of
*  nuclear grade material so like here we have the similar situation where the pre-training is
*  actually happens just in a few places and this is visible to governments
*  whereas like the proliferation is already much harder to control so so yeah my model is that
*  and also like it as i know it it kind of resonates with with thinking in the labs themselves
*  that if you want to kind of have some constraints on the AI trajectory then kind of intervening
*  in this compute having some kind of compute governments is is probably great thing to start
*  from and in fact like this chips act and like the export controls to china are already things that
*  are happening and in some ways make the problem much much more easier although it doesn't it's
*  far from solving it i think that makes sense i mean certainly at the moment the compute
*  requirements are are high i do wonder what do you make of things like the diffusion of just language
*  model know-how proliferation we i think we might have a guest coming up on the podcast that is
*  building a decentralized gpu cluster potentially with some sort of blockchain governance where
*  you know my m1 or m2 macbook air can like contribute to a cluster
*  do you think like how long do you think we have before that barrier of just like mega compute
*  resources goes away possibly because of these like virtual clusters or possibly because of like
*  further algorithmic breakthroughs or just model leaks could be another thing how long do you think
*  that holds i mean i gotta refer back to this like eliezer's more slow of math science but yep like
*  with every every year or a couple of years destroying the world becomes easier so there's
*  that but also like if we do not get a pause like i do think that the world will be destroyed by one
*  of the big labs so it's just because yeah the frontier models happen it's much easier to train
*  frontier models in a big data center than in a distributed manner that's how if the pause does
*  happen then yes we have to worry about things like proliferation things like hackers taking the hackers
*  stealing that stealing the weights and then kind of doing experiments like chaos gpt but on with
*  a competent ai stuff like that i think one of the maybe most kind of i don't know if it's an argument
*  or whatever but maybe one of the things that's kind of hardest to argue with in my experience
*  with for those that are kind of objecting to the letter or the to the concept of a pause is the
*  is the sense that like someone might say you have no you're not giving me anything to hope on here
*  really right you're just telling me like every every generation it gets easier for the world
*  to get destroyed like we've talked about buying ourselves some time here and there but we have
*  we haven't really heard much of a sort of here is the path we can take to safety and so like why
*  bother pausing if we don't even have a sense of where we're going do you have any hopes for
*  concrete paths to safety that you would try to inspire that kind of person with yeah i mean sorry
*  humanity you have cancer so it's like you might be cured of it but currently doesn't look good
*  so it's like i'm not gonna lie the prognosis is bad but like it's not hopeless and also like
*  the kind of like silver massive silver lining is that if we do manage to survive the cancer
*  the future is going to be amazing so like in some ways kind of the expected value of the future
*  is not bad it's just like the odds of survival are bad but if we survive
*  the life the world the universe could be potentially like like unfathomably better
*  than it is now so in sense we are living lottery living a lottery ticket and it is in some way
*  in our control to improve dots and so that's what i'm doing well that's probably about as
*  good of a bottom line as we could hope for for this conversation so i want to thank you for
*  spending the time with us i do have a couple real quick hitter just fun questions that i usually
*  end on if you have an extra second for those we touched on this earlier any applications aside
*  from like the obvious you know kind of usage of the core language models that you are personally
*  just finding delightful or useful that you would recommend that people check out no i'm just like
*  way kind of pan with uh limited uh to to tinker uh with with the language models i mean i i've done
*  a little bit uh but then i i'm trying to find coders at this point uh to like delegate bunch
*  of my projects some of them including some of them might involve language models fair enough
*  you're in the majority on that answer most people are just using a few things and then second
*  um let's imagine a world where we're here in a couple years and neurolink has been deployed
*  to one million people in this scenario you are well um so you're you don't you don't need it for
*  restoring any functionality but if you were to get a neurolink implant in your head it would give
*  you the ability to essentially transmit your thoughts to devices so you would have effectively
*  thought to text or thought to ui control uh would that be enough for you to be interested in getting
*  a neurolink implant depends so much on details like how reversible is the is the procedure
*  what are the risks and what is the going to demonstrate that upside like will it become
*  a better dancer as a result well yeah that when i they did show uh in their in their show and tell
*  they did show an animal where they were creating motor control uh through the neurolink but
*  yeah i think it was a long way from improving on your dancing skills so i hope you are dancing for
*  many years to come jan tal and thank you so much for spending this time with us
*  we appreciate you being part of the cognitive revolution thank you very much
*  amnaki uses generative ai to enable you to launch hundreds of thousands of ad iterations that
*  actually work customized across all platforms with a click of a button i believe in amnaki so much
*  that i invested in it and i recommend you use it to use cog rev to get a 10 discount
