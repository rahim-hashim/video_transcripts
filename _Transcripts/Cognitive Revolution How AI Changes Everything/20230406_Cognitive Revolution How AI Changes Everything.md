---
Date Generated: April 03, 2024
Transcription Model: whisper medium 20231117
Length: 6976s
Video Keywords: []
Video Views: 994
Video Rating: None
---

# The AI Reasoning Revolution with Ought's Jungwon Byun and Andreas Stuhlm√ºller
**Cognitive Revolution "How AI Changes Everything":** [April 06, 2023](https://www.youtube.com/watch?v=eZF7tQSxAeQ)
*  there's like kind of a sense of wonder that has been lost. And I think it's easy to lose sight of how much about ourselves and our world still remains to be discovered. So I really want tools like illicit and just more broadly to be able to help us. You know not just kind of really amplify existing research efforts but help us to discover entirely new ways of doing research discover entirely new research methodologies entirely new research domains that weren't possible before about like you know now using kind of this human
*  artifact of text as a data source analyze us more intelligently so that we can better understand who we are and how we relate to each other and what kind of systems we can we can build to help each person and every group of person flourish more and for like AI systems to be able to generate like very decision relevant but really really really simultaneously rigorous and have one pagers policy memos and answers to people in very high stakes decisions so that we can just become way more intelligent about how we can govern this world. Hello and welcome to the Cognitive Revolution. We're
*  we interview visionary researchers entrepreneurs and builders working on the frontier of artificial intelligence each week will explore their revolutionary ideas and together we'll build a picture of how I technology will transform work life and society in the coming years. I'm Nathan LeBenz joined by my co-host Eric Torenberg. Before we dive into the cognitive revolution I want to tell you about my new interview show upstream upstream is where I go deeper with some of the world's most interesting
*  thinkers to map the constellation of ideas that matter on the first season of upstream you'll hear from Mark Andreessen David Sachs biology Ezra Klein Joe Lonsdale and more make sure to subscribe and check out the first episode with a 16 Z's Mark Andreessen the link is in the description.
*  Hi everyone recording this in the hospital maternity ward where my wife Amy and I just welcomed our third child this week. Thankfully both are doing well and we should be home by the time you're listening to this.
*  Today's episode is one I have looked forward to since we launched the show Andreas Stollmuller and Jung Won Byun are the founders of ought a product driven research lab that develops mechanisms for delegating open-ended thinking to advanced machine learning systems.
*  Their flagship product illicit online at illicit.org is an AI research assistant that helps researchers accelerate time consuming workflows starting with literature review.
*  I was particularly excited to have Andreas and Jung Won on the show for two main reasons. First in my experience illicit works well notably better than any comparable product I tried in early 2023 and so much so that it adds real value to research workflows.
*  No small accomplishment given the inherent difficulty of research tasks.
*  Second and likely more importantly in the long run ought has helped lead the AI industry toward a process focused approach which emphasizes task decomposition and the training of high quality reasoning processes rather than focusing solely on correct or incorrect outcomes.
*  We discuss how this process centric approach works in practice today. Why it's likely critical to building AI systems that can help with novel and important questions where little or no training data exists.
*  How it might help us avoid catastrophic AI outcomes and more. I hope you enjoy this conversation with Andreas Stollmuller and Jung Won Byun.
*  And now for the last question of illicit. Welcome to the cognitive revolution. Thank you. Hey it's good to be here. Yeah I'm really excited for this conversation. I have been a fan of what you guys are building at illicit for some time and had some really great product experiences using it in a couple of research questions that have come up in my own everyday life.
*  And so I'm really excited to learn more about the product and also the philosophy that you know that is behind it. I wanted to start with a question drawn from your website. I went and read the mission statement and there's a really interesting claim which I think is true but maybe under appreciated by many.
*  And you say it's especially unclear that ML will help in matters that require substantial thought. I think that's really apt. You know in any sort of the writing or work that I'm doing I find that the best work you know the AI is currently the least helpful.
*  So maybe you guys could just start by unpacking that. Like why do you see that as the case? And then obviously we'll transition into what you're doing about it. Yeah it's actually interesting. We wrote that six years ago and I think it's still true. Like roughly speaking I think there are two things that machine learning can do. One is it can imitate some behavior. Like for example it can write text like a human can write text. Or you can use reinforcement learning and can optimize some reward like win games of go. I think you get reward when you win. I think essentially all cases where AI has
*  substantially exceeded human capability are cases of reinforcement learning because that's what you need to get to unforeseen creative moves. And so then the question is well when does reinforcement learning work? And I think it works best in domains where you have like a pre well defined task like play a game of go. You have a pretty clear objective like win the game. But for helping us think more clearly or arrive at true beliefs or better decisions there isn't an obvious such objective. So there's a
*  risk and the risk is that we'll use like a proxy objective or something that is like kind of similar to the thing we want but not exactly the thing we want. And the most kind of natural such proxy objective is like does it look good? Like is it producing stuff that looks persuasive or impressive or helpful seeming? But that's not the same as actually being well reasoned robustly helpful arriving at really good decisions. So I think actually right now that's not a big problem. I think in terms of behavior these models are still mostly in the
*  imitation regime like they cannot put sprite stuff or brainstorm stuff or write code and I think that's all great and actually is helpful. But if you ultimately want them to be as helpful for kind of better thinking as they are at like playing go, which is like better than the best human, then I think there's like some work to be done to make that work.
*  So there's kind of two at least two issues there, right? There's the sparse reward issue.
*  And there's also the like human unreliability in rating or valuation. Are those kind of the core two issues that you see limiting the current paradigm or are there others too that you would want to highlight?
*  Those are real things, but I think it's more that there's not really a clear task definition. So I think the most natural task is I ask the model, hey, what decision should I make in this like setting? And then the model is like, here are some considerations for these considerations. You make that decision.
*  And then I can be like, well, does that sound good? Does it not sound good? I think like I have like some kind of resolution in judging that. But I think if I want the model to eventually exceed how good I am at making these decisions, then something else needs to happen like either. I mean, I think there are various ideas for how to address that. But I think the most natural paradigm doesn't solve the problem of kind of exceeding human capability at reasoning and decision making in a kind of like really helpful way.
*  So that kind of feeds into then the next part of the mission statement, which is really your take on solving it. So you say our mission is to automate and scale open ended reasoning so that future improvements in ML help as much with thinking and reflection as they do with tasks that have clear short term outcomes. And it's interesting that you say you wrote that six years ago, I hadn't realized that the mission had been established that early. I guess I'd love to hear a little bit of like what you thought you were going to be doing.
*  When you started off and how that has kind of compared to reality, because I'm sure, you know, if you're anything like me, you've had a few surprises and updates over the course of the last six years. So what was kind of, you know, how was that expected to cash out versus how it in fact has cashed out over time?
*  I think the surprising thing is actually how in line with expectations is everything. I think maybe timelines are somewhat shorter than we expected. But roughly speaking, when we started out, we were like, well, machine learning can't actually help with and can't do even the simplest thinking tasks yet. This was before GPT-2, there were no language models. I guess there were a thing called language models, but it's very different from what you would call language model today. And so we were like, well, how can we
*  still get ahead of the situation and study how to use future AI systems to help with that? We're like, well, I guess humans are our closest standard for AI systems. So we mostly run experiments with humans thinking about, well, how can we decompose complex tasks into tasks that kind of humans who don't know the big picture can help with, like break it down and all that. And I think it is kind of crazy how much AI systems now look like these kind of context free humans that we ran simple toy experiments with.
*  Back in the day, I think I've been surprised by how much things played out in line with expectations actually.
*  Yeah, it's crazy when we started the aggressive AGI timelines were like 2100. That's what the crazy people believe. They're like, oh, maybe we'll have AGI in 2100. And then over the span of a few years, that's gotten to like, you know, condensed by so much. So yeah, when we started, we were doing more research experiments. But even then, I don't know, I think this is you, Andreas, but like we were, we were trying to break down
*  tasks to be very context free. So basically giving our human participants a context window where they could only like read so much text to do a specific task. And like even even details of our experiments like that have actually generalized quite a bit to to what language models have actually ended up evolving into.
*  When we started, because we had no line of sight to when ML would be actually good for for helping with reasoning, we ran experiments, it was way more researchy. And then as language model progress happened, we realized like building a product around that and directly trying to help people with reasoning would be way more impactful.
*  Yeah, that's fascinating. So can you describe those experiments in a little bit more detail? The idea of, you know, giving a human a context window, I think is pretty fascinating. Like procedurally, what did that look like in the early days?
*  Yeah, there were a few different versions of those experiments. So I'll describe one simple one. What we call the relay programming, where basically there's a programming task like like a programming puzzle, you might might get one of these programming
*  competitions. And every participant only has, let's say, a minute to kind of make progress on the task and then pass on their notes to the next participant. Where the thought was, if you could make that work such that, like, a person who doesn't like know the big
*  picture can make small contributions that move the state of thinking on that problem forward such that you eventually solve it, then you could get to mechanisms where you can solve like much more complex problems. Like there's a fundamental kind of question of like, how
*  composable is thinking? How much do you need to know, like the big picture, need to like some mental model where you load everything into your head? And to what extent can you like get away with not doing that? And so one of the settings where we studied that was in the setting of
*  programming.
*  I think the high level research question was, in a world where we have super intelligent AI systems who are doing work that are, you know, that are smarter than us, and so can do work that we can't evaluate directly, can we break down complex tasks into smaller
*  tasks that are easier for people to evaluate? So part of the reason why in some in some of our experiments, we gave, you know, people context windows was because if like, you know, the AI system can read an entire book, but you can't, then that's really hard to
*  evaluate. And so can we then break down work into smaller pieces such that if a human want to spot check parts of it, the amount of work that they would have to spot check is like manageable for them to do. That's kind of what we were
*  So how, how productive were people able to be in this kind of one minute context window environment? Were teams or groups able to solve like substantial problems? I would expect that honestly, to be very hard. And I think this is a really interesting
*  comparison now, as well, because we do have, you know, there's obviously a lot of debates around like, what are the latest systems mean, like, just how capable are they? How should we understand them? But I've not previously considered this notion of like, testing the human on the sort of AI's turf. So I'm guessing we didn't do nearly as well as like GPT-4 would currently do. Certainly, if you, you know, give it a minute, you know, as like your hard limit, you know, to make progress. So
*  I think it is very challenging. And I think the challenge is actually somewhat similar between humans and AI systems where one challenge is you need to get rid of errors more quickly than you introduce them. So like, with every like you do your minute task, you probably are like, like, there's like some chance you get it wrong. And so the more such tasks you compose together, the more likely it is that like at least one of them goes wrong. Even if you only have like a 10% probability of getting like a sub test wrong. That's like, if you do 20 tasks, it's like
*  probably going to happen. And so you need some error correction mechanism. And that error correction mechanism itself is probably needs to be composed out of these like one minute pieces. And so if you are not getting rid of errors more quickly than you introduce them, then you're doomed. And I think that is I think a thing that we are currently facing with language models as well. If you compose together many tasks, I think their models still are like somewhat flaky. And so that I think limits
*  scalability of both the humans we work with and also current models to like very complex compositions of tasks.
*  In addition to the programming task, we also did text based tasks. And again, this is like, you know, before a lot of the language models, but we would have tasks where, you know, one person would make would make generate some summary of like a movie review.
*  And then other people would have to decide like, was this summary accurate or inaccurate and everyone can only see little parts. No one except, you know, one maybe like the original generator can like read the whole summary.
*  And yeah, and I think we ran into a lot of similar issues surprisingly there as we are with language models, which is like in a one maybe specific flavor of this error propagation issue is that this interaction can kind of converge in this like suboptimal place where the people might like get derailed by a particular argument line of reasoning and then get stuck there and kind of like miss the overall point of the summary or like not find the most efficient way to decide if the summary was accurate or not.
*  And then I think the other thing that we struggled with there is that like the context windows were really too limited, I think. And there needed to be some better mechanism for passing along state across different participants. And I think we're still not quite, we haven't figured that problem out with language models either.
*  It's like, what is an intelligent way to pass state so that you're not kind of making this starting from scratch with every call called to the language model, basically.
*  Yeah, maybe here it's worth like zooming out and like briefly asking you about why does this sort of context-freedness matter? I think I mentioned earlier a little bit, but I think the key thing is really how can you make it so that humans can evaluate what's going on?
*  I think one world is the world where in the future, I mean, I think these context windows are obviously going to grow, are going to go away eventually. And so one world is the world where AI systems are building complex, like internal mental models.
*  We can see those models, they make decisions. We're like, well, why did it make that decision? It's really hard to tell. Contrast that with the world where all the state that is being passed along is explicit.
*  Then you can be like, okay, well, this is what the model knows at this point. This is the local decision it made. It said like, oh yeah, here's like the next thing we need to check out based on the things we know so far.
*  And that's a thing you can evaluate. And so I think we just, we care a lot about getting AI systems to a setting where what they do is transparent and where humans can evaluate what's going on.
*  And so that has driven like those experiments with humans in the past and also drives a lot of our work with language models now.
*  Omnike uses generative AI to enable you to launch hundreds of thousands of ad iterations that actually work customized across all platforms with a click of a button.
*  I believe in Omnike so much that I invested in it and I recommend you use it to use Cogrev to get a 10% discount.
*  Yeah, it's funny. I would say that's pretty prescient really, because as you're describing the challenges in performing tasks that way, you know, and then you said like the zoom out, why do we care about this?
*  My immediate thought was like, well, having used language models a lot, it's pretty obvious why I would care about it now.
*  It may not have been nearly so obvious five, six years ago, but that is a really interesting, you know, just kind of history as to how you were exploring the cognitive space, I guess I'll say.
*  And that's also like a pretty natural, I guess, way to bridge toward how you're building today.
*  And it seems like this product philosophy of interpretability by construction reads almost like a mantra for what you guys are doing now.
*  And again, so fascinating that you started with that even before, you know, the technology kind of shaped up the way that it has.
*  Yeah. So tell us about this product philosophy of interpretability by construction, how it feeds into illicit and what illicit is today.
*  And then I also want to hear what your vision is for illicit to and maybe end of the decade type of timeframes.
*  We mostly think about it as how can we make language models follow processes that humans can understand as opposed to these kind of latent unobserved, like only in the language models had kind of things that have been optimized end to end to accomplish some goal.
*  But we don't actually know what's going on.
*  So, like, I guess the most common way that we and also others refer to this as is this distinction between processes and outcomes, where I think the most kind of natural framework for most machine learning is to think about it as like outcome based.
*  You have some metric like predict the next token or try to optimize reward in the gameplay setting.
*  And you're trying to literally set all the floating point numbers in your matrices to make it so that like that final number goes up as much as possible.
*  And you don't really care about the meaning of the internal components.
*  And so they might get like inscrutable. Contrast that with kind of more like programming in, I don't know, programming like just natural programming.
*  Like if you're usually if you're programming, you're like, well, I need to write functions.
*  These functions need to have independent meanings that I can like kind of evaluate, like even outside of like a particular context.
*  And if that's not the case, then it's like terrible spaghetti code that no one will understand.
*  And it's hard to debug, hard to kind of like improve.
*  And so we're, I guess, on the spectrum from more outcome based systems to more process based systems.
*  Our philosophy is like always try to push as much as possible for being in the process based setting where you build on human understandable task decompositions.
*  You directly supervise the individual reasoning steps and try to get more to this sort of transparent setting.
*  And then I think the obvious objection is, well, how's that going to scale?
*  That seems like involves a lot of manual labor. Isn't the whole bitter lesson of machine learning that you just want to like scale up your like matrices and optimize them end to end.
*  And I think there is a lot of truth to that. But the one maybe the idea that pushes against that is actually we can use language models and like AI more generally to help with the task decomposition itself.
*  So it doesn't actually have to be a manual thing. The process of decomposing tasks into understandable pieces is itself a thing that AI can help with.
*  Even in the world where end to end optimization is in principle more powerful.
*  We think that the benefits from transparency and understandability are enough that I think it's worth like pushing really hard in that direction.
*  Yeah, the way I think about the interpretability by construction point is I think the current world we're in is like, as kind of Andreas alluded to, do whatever it takes to score well on this feedback metric.
*  And where like we often don't understand why a particular method works really well or we can't kind of like, yeah, it's not it's not human legible, whatever it takes us.
*  And scaling this up to superhuman levels and like scaling this up with a lot of compute is very concerning. We don't really want to do whatever it takes to score well on this metric.
*  And then there's kind of like an interpretability, you know, an approach to interpretability, which is like, OK, cool, I get this result from this answer.
*  Now let me figure out how it arrived at that. But I think that is also still concerning at kind of like superhuman levels.
*  And even if you ask the model to justify why it did what it did, it can rationalize, you know, it can kind of like make up reasons that were not its actual reasons.
*  So interpretability by construction is, you know, it was trained on kind of human legible processes upfront.
*  Like you have this guarantee that this is what it did because that was that was it was programmed to do that.
*  And so, you know, you can always go back and just see like what were the steps you followed to get get this answer.
*  So that's kind of where the that's like what I think there's a distinction like interpretability by construction versus like just trying to understand why the model did what it did after the fact and using the model to generate explanations that way.
*  And so like an example of this that we we have in Elicit is this part, this feature that kind of runs through a trustworthiness checklist for papers.
*  So currently, Elicit broadly is a research assistant. We've been focused on making it useful for literature reviews.
*  And so right now when you run Elicit, you get back a table of papers.
*  We kind of automate the process of finding relevant papers and extracting details and summarizing the papers so you can very efficiently figure out if a paper is relevant to your research question or not.
*  But good information is not just relevant. It's also trustworthy and something you should actually kind of update update in light of.
*  And so once you set up some infrastructure for finding relevant information better with language models, we then wanted to tackle, OK, how can we evaluate of all of these relevant papers, which one should most update the kind of user's question?
*  And we found in talking to a lot of researchers that there are like very good standardized processes that exist for this already in academia.
*  So we learned about things like strength of evidence checklist, risk of bias checklist.
*  There are just these checklists that researchers already make to kind of evaluate lots of different papers on.
*  And so we started to just automatically answer it, run that checklist over every single paper.
*  I think like a tempting thing here would have been to try and like come up with some score for trustworthiness and like probably other other source, you know, tools or whatever.
*  Other attempts have been made like this. How do we think about citation count and what journal it was published in and then kind of give it like a rating?
*  But the ratings can be easily hacked. And so what we're doing is still like kind of giving the researcher all the pieces they would need to evaluate trustworthiness without like distilling a lot of that richness and losing a lot of context by putting it into a score.
*  And then, you know, that checklist currently has things like, what was this straightforward things like? What was the sample size?
*  Are there any conflicts of interest with the funding sources? Did they control for a bunch of different things?
*  Stuff like that. Like what is the actual like number that they reported on and who is it based off of?
*  And so in the future, we could like take this and like condense it into a score if we wanted to.
*  But because we did this in this kind of like compositional way, if we decide to summarize that checklist into a score, it'll be very easy for someone to then expand the score and see the checklist again.
*  And that's again kind of like interpretability by construction. Do it in pieces first. Then you can collapse it.
*  But then once you collapse it, it's also easy to expand it again and like see what went into the score.
*  Whereas if we had just kind of collected a large data set and trained on, I don't know, what people rate papers as, like no user could then go and really get a detailed understanding of why this score in this specific instance.
*  It really does work amazingly well, at least for many of the use cases that I've tried. So I would definitely recommend that our audience go check it out.
*  I've used it for kind of random medical queries that have come up in my personal life.
*  I would not, you know, my standard disclaimer is I wouldn't recommend making it your doctor at this point, but I would recommend getting a second opinion.
*  I say the same thing about, you know, GPT-4 and I would definitely say here, like it can help you surface valuable information.
*  So that's been a really good experience. I've also had some really nice experiences, honestly, just even at the highest level of like it's often hard to search for a particular paper that I kind of recall what the main conceptual takeaway was.
*  But maybe I can't remember the name or I, you know, I'm terrible with the author's names in many cases. So now I'm like, you know, it's my options at this point are often like, well, I usually found it on Twitter so I can go like,
*  search for things that I liked on Twitter and like put one word in at a time and hope that I actually did like it as I, you know, as I should have and that, you know, it pops up.
*  Or I can go to elicit and type like sentence length, you know, could still be a sentence fragment in many cases, but it's like, you know, what is that kind of conceptual thing that I'm recalling that, you know, is drawing me back to this in the first place?
*  I've had a couple of good examples where I could not find the damn paper. And then, you know, with that kind of 10, 12 word, you know, conceptual memory that I was able to to finally spit out then boom, like I was able to get exactly to the paper that I wanted.
*  So that's obviously not the most advanced use case, but honestly, I would say that does put a list ahead of almost all the competition at this point. You know, I've tried some other kind of research focus tools and.
*  Frankly, in many cases just gotten nothing of real value out of it. Even with Bing, it's like very hit or miss, you know, exactly what you're going to get back and how good it is. So, you know, this is not a paid endorsement, although it's starting to sound like one.
*  It actually just has been really useful to me. So go try it out. I always we're big on telling people to go try out the actual tools.
*  So, OK, back to questions, supervising reasoning processes, not just outcomes.
*  I'm feeling a little bit like the line there is kind of blurring in some ways that I'm feeling a little bit confused because on the one hand, I'm like, OK, it makes a ton of sense to try to create this like what was the thing thinking at sort of every step of the way.
*  And certainly if I like rewind the clock a little bit, you know, and you're looking at, you know, very finite context window language models is kind of all you have available.
*  Then it's like, yeah, you can only do so much that you kind of have to chop it up into these micro tasks and make the only only way.
*  But how do you see things like, for example, with like a 32,000 token context window now coming online and with techniques that are kind of like, you know, whatever, least to most or step by step?
*  Is there a way in which we're kind of headed toward some sort of equivalence where you could like spell out all the steps that you want, you know, a GPT-4 32K to take and then like kind of watch it take all those steps, you know, one by one.
*  And then sort of say, here's your score on that, you know, with kind of all of that out loud.
*  Are these paradigms converging in some sense with that type with those kind of language model best practices?
*  Or do you still feel like there are like fundamental differences that I'm not rocking?
*  I think there's like partial convergence. So I think the step by step is definitely kind of move in the right direction.
*  I think it's good to see like the reasoning spelled out. I think it's good for many reasons.
*  I think it's still the case that I think there are some results where you replace the steps just with dots.
*  Like literally you make the model type out dot dot dot dot dot as opposed to writing out steps.
*  And it still leads to its answer improving because there's like some latent computation that can do during those steps.
*  I wouldn't otherwise do. And I think that is maybe a little bit concerning.
*  I think when you write out the reasoning, I think you definitely want it to be the case that the reasoning is causally responsible for the answer you get in the end.
*  Because if you're like, well, yeah, that reasoning all makes sense.
*  And then you get an answer and you're like, well, did the answer really depend on the reasoning?
*  I think you'd prefer not to be in that situation.
*  Maybe it's like not really a big deal for basically all of the use cases we have of language models right now because they're so low stakes.
*  But I think if you think a little bit ahead and in the future, suppose you're running a company or like, well, should we invest in this thing or hire this person or whatever decisions people will try to make in the future?
*  I think they're a real matter more that you can actually trust that the answer, in fact, faithfully depends on the reasoning.
*  And so I think even though like writing out the reasoning steps in the problem, I think is a good step.
*  I'm like very excited for techniques like that.
*  I think you ideally would go a step further and have guarantees that the answer causally depends on the steps, which you don't have for most of those approaches.
*  Yeah. So one one example that I give to kind of contrast these two methods is, you know, maybe you ask a language model.
*  If you're like, I just kind of like one on one pager on this topic, like, you know, what what should the government do about long COVID or something?
*  And the language model produces an answer. And if you're actually going to make a high stakes decision about that question, you want to know why should I trust this answer?
*  And at that point, like, you know, the more trustworthy, like if the language model is able to say to you, well, I went out, I searched over these databases, I read these papers, I ranked these papers by this by these dimensions, I checked to see if they were all trustworthy.
*  I summarize them and then I compared it against this data. If you can like list out all those processes to you and did like actually follow that process, that is like that you can have way more trust in that answer.
*  And you can then go in and you can look at the process itself and say, like, do I believe do I trust this process or not? And you can also kind of inspect like each step and did it actually follow that step?
*  But like, I think with just chain of thought, like, I don't know, the alternative would be like it kind of says incoherently rambles about a bunch of things.
*  And like Andrea said, may not be like actually what it did, even though having more insight into into them, some more insight into the model state is helpful.
*  I think that kind of interpretability by construction piece comes up again where you want you want some guarantee that this is what the actual model did.
*  Seems like a big part of the the advantage there is also the kind of grounding, right?
*  I mean, in a pure chain of thought, you're just relying on whatever the language model thinks it knows, right?
*  And it's it's then another thing to sort of say, OK, we're going to go to sources and evaluate those sources, try to find takeaways and then kind of synthesize that.
*  So if I was going to try to continue to close the gap between the two paradigms where you guys have taken things and kind of made the complex, you know, discrete and comparatively simple.
*  And ultimately that like involves a lot of calls and each one is kind of, you know, a small input output.
*  And then you can, you know, view that whole history.
*  It seems like maybe much closer to that.
*  If I was like, you know, let's say I'm an open AI foundry customer and I have, you know, I just dropped one five on my, you know, 32,000 token robustly fine tunable thing.
*  The main thing that I would really be wise to do, it sounds like from your your counsel would be like ground in sources.
*  That's like, you know, pause at some point, bring in information.
*  Maybe it could all still be in like one context window, but you're going to want to interrupt at certain points and like bring in information.
*  And then, you know, maybe at the end you could sort of have like a summary that kind of goes back through all of that history and is like, okay, here's what I did.
*  And then, you know, I hit some stop token and that, you know, executed a search and then I had some other stop token and that, you know, fetched the abstracts, you know, for all those papers or what have you.
*  And you could in theory kind of layer that on, but you'd be having kind of like layered calls as opposed to like compositional structure, I guess, to the, you know, to the overall process, like linear with these sort of outbound calls might be at least much more, you know, kind of comparable to the composable structure that you guys have built.
*  Does that sound right? I'm really just trying to understand it. So feel free to tell me if I'm off base here.
*  I think the thing you described where you make like you halt, make calls to sources. I think that is a pretty common thing that maybe the simplest instantiation is like the thing that I think Bing does, which is just do a search, like don't do it even like in multiple steps.
*  Just do a search, take the results and then do generation based on that. That's maybe like V zero of that. And then I think that what you described is like one step more complex, which is you can kind of do some generation, halt, do a search, pull in the results, continue.
*  And then I think there's like versions after that, which are like, well, one of the things you could be doing is you generate halt, decide to farm out some sub computation to another model that like is roughly speaking, the same situation the current model is in.
*  It does its thing. It comes back. Then you continue generating. So I think there is a kind of incremental path from just like do everything in one prompt to a more compositional settings. And I think the world where everyone pursues those more compositional path, I think is a pretty good world.
*  I guess another big opportunity for difference, and this is probably something that that does exist in illicit. I know it does from, I don't know exactly what degree, but I know from your Twitter presence that it does exist to some degree is you can have your, if you do truly break down into a compositional approach, then you can have specialized models for all the different subtasks potentially if you really want to.
*  I guess you could also, I'm really contrasting this because I just get the sense that like, if I had to guess, I would say like half of the Fortune 1000 is probably going to try to buy OpenAI's new thing.
*  And they're really going to be striving for reliability in their language model output. They're not going to be generally wanting it for brainstorming.
*  They're going to want to be completing tasks and they're going to want to be completing them reliably. We don't know what kind of fine tuning they're going to offer, but you could sort of fine tune a bunch of different models for discrete subtasks.
*  So you could maybe try to do like kind of 10 tasks in one, you know, giant fine tuning of a giant model as well. And then you're kind of back to maybe a question of like, you know, does the sort of super generalization like help you?
*  It might in some ways, you know, on performance, like certainly there seems to be this kind of, you know, bigger is better. Maybe you could help me unpack a little bit more of the worry of when I'm looking at kind of chain of thought reasoning, you know, you throw in those halts or whatever.
*  And I get to a final answer. You know, you kind of said like, you'd really like to know that the answer was actually the result of the reasoning. And like on some level it is, right? Because like it is incremental token predictions.
*  So like all the, you know, one by one prediction, right? So all those previously generated tokens in some sense are causal for the tokens that follow through obviously a still fairly alien mechanism.
*  So I definitely appreciate the danger of, you know, just we have really no idea what's going on in very profound ways. But how is that? So how is that different? Like if you then like pull things apart into a bunch of different language models and you're like, okay, this one did this task and then that task like, you know, that output feeds into the next one.
*  How do you see the like, where is that extra causality coming in versus just like the one token at a time prediction?
*  Yeah, so I guess it's important to clarify that it doesn't, it doesn't matter whether it's the same model or different models. So like, you definitely don't need to define, you know, different model. Like it's fine if all the subtasks are done by the same model.
*  And so the causality mostly comes through removing information. So in the case where someone is doing a task and you know, the only thing they know is they are supposed to figure out, does this website answer this question?
*  They don't know you're trying to make a decision about whether to take some medicine or not. They're literally just trying to answer, does this website like answer this question or not? You know, they're not influenced in some way by like making it seem more or less relevant or otherwise taking into account context that it shouldn't take into account.
*  So, whereas if it's all happening in a single prompt, then you're like, well, it could be, there could be kind of causal influence that you're not tracking.
*  Right. Everything's just a lot more tangled.
*  Yeah, everything is more tangled, which for better or worse, right? Like sometimes that's really useful because sometimes to do the subtasks, you want to know, oh, which are the things that actually matter for the bigger, bigger question.
*  But also it has to draw back that it's harder. It gets harder for you to analyze like what actually happened and the pieces lose their individual meanings.
*  Yeah. Okay. That's really helpful because it does, especially with these really long, you know, super high coherence now with some of the latest models, like the, you know, the ability to attend effectively to these super long windows.
*  Yeah, it does come. I've thought primarily honestly about the pros, but you're highlighting a pretty good con, which is the fact that now everything is feeding into this one decision.
*  And that just makes, you know, for every incremental decision you have or every incremental, you know, task, whatever you have a just fundamentally noisier situation where you're like, well, geez, you know, would it have done the same?
*  You now have the question of like, okay, for that paper where we wanted to get this one bit, like did that result depend on the original query or did it depend on like the two papers before it, you know, that got analyzed in this kind of linear stream?
*  And I do see how that now would, I now see how that would make things a lot harder to tease apart. So yeah, that's really interesting. Thank you for bearing with me as I try to sort out my own confusion.
*  I took a note also from your, I think it was from your big Twitter thread, which I've recommended previously about how you break down, you know, the task and approach the composability.
*  So your guideline is once language models do well at small tasks, roll up to bigger tasks. That reminds me of the Drexler comprehensive AI services a little bit.
*  Like he's very, you know, let's deploy narrow superhuman AI everywhere before we even attempt to do very general superhuman AI.
*  Have you guys taken inspiration from that or how would you kind of, you know, contrast what you're building to the vision that he's outlined?
*  Yeah, it's definitely related. The maybe dimensional language to contrast it is like, what is the task size where I think Drexler mostly thinks about fairly large tasks.
*  So like there's a, let's say there's a service for evaluating like whether an investment is good or something. And then within that service, as I understand it, it's still fine if like everything is more or less end to end optimized.
*  And I think we'd probably just try to push harder for even smaller, even more understandable components. And I think Eric also has more recent work.
*  I don't know if you've seen like called the open agency model, which I think is also a bit more in that direction, like trying to get language models to do even smaller tasks, like evaluating, like suggesting different proposals for policies and then different language models evaluate those proposals and such.
*  So I think there's only some convergence here. Yeah, that's interesting. So how about the question of task size? Because like I've noticed this in my own work, right?
*  So at Waymark, we use kind of an ensemble of different AI systems, some of which are the core ones being language models, but also like some visual type stuff as well, because we're ultimately trying to create visual content.
*  And I've definitely noticed over the last year and a half that it is getting easier and easier to just kind of throw more into a single call.
*  We used to have like a fine tuned model that would do this. And then we had a different fine tune model to write the voiceover script. And now we're kind of like, okay, well, you could just write the main script and the voiceover script in the same call.
*  Like, you know, that got easy enough that we can just go with it, right? Similarly with like what image, you know, content would complement this text. You know, we used to fine tune on that separately.
*  And now we can kind of bundle it into one thing. That's all still a pretty narrow task, you know, certainly in the grand scheme of human endeavor.
*  But how are you guys shifting the size of tasks, if at all? Like, are you sort of saying, okay, these things are getting more capable, like we can trust them on, you know, somewhat bigger bytes out of the overall task?
*  Is there kind of a pull in that direction?
*  I can give like the general answer and then Jungwon can maybe address it in the context of the product, if that seems helpful. But in general, I think we must think about what are tasks that humans can fairly robustly evaluate.
*  And you kind of want tasks to be as large as possible under that constraint, I think. It's like, it's great if models do more work at once, because for all the usual reasons for why it's useful for models to have more context.
*  But it's bad if you get to a stage where a human can't look at it and be like, okay, this is like good or bad. I think there's like some caveats to this, where you're like, well, maybe humans can use AI assistance to do that evaluation better.
*  But then you still have like that constraint of like, can a human with AI assistance robustly evaluate whether a sub task was done well or not? And you never want to push beyond like the threshold where that's true.
*  Where is that kind of settling? Like, what do you find that people can in fact evaluate effectively? I mean, for context too, for the audience, like, boy is evaluation hard. I mean, I think most people probably know that.
*  I was really amazed to see from the GPT-4 technical report that the GPT-4 versus 3.5 win rate is only like two to one ratio. It's a little bit higher than that. That's like maybe like 70-30. But, you know, it's not like people are preferring this like 99 times out of 100. It's nowhere close to that. It's like two out of three.
*  So that in and of itself is hard. There's just a ton of noise. Again, at Waymark, we've done a lot of experimentation with like, which are the best looking images? Because our users will have these image libraries. And like one of the biggest things we want to do for them is just for convenience, like pull the most beautiful images forward.
*  And like that is so easy for people. You know, it takes it's like quarter of a second, you know, that they can kind of differentiate between which one they like. But the noise and the inter-rater reliability just gives us insane headaches. It's like the R squared on two humans sitting next to each other looking at the same images is like really tough.
*  And so the models struggle as well, right? Because they're trained on that signal and the signal is just insanely noisy. So in your context, where are you kind of finding that it settles in terms of what people can reliably evaluate?
*  Yeah, I feel like this is another place where breaking tasks into smaller tasks really helps with the evaluation. So we have a bunch of kind of, in Elicit, there are a bunch of abilities to kind of ask questions of the paper. And the paper is like, you know, this has lots of text. It is kind of this like long context situation that we were talking about earlier.
*  And Elicit using language models can say, oh, this paper found this result. This paper studied these types of people. This paper had these limitations. And like, if that's all you got, and you wanted to evaluate that, you would have to read the entire paper to figure out if that was true or not.
*  Like maybe you could for certain things you could control of it. But in a lot of cases, you'd have to read the entire paper, which is very expensive and time consuming to do. But because the way we build out these tasks in Elicit is compositional, what we do is we first find relevant parts of the paper, then using that part generate the answer to that question.
*  So when we show you the answer, we can directly link it back to where in the paper we got that. And then all you have to evaluate is kind of the answer and like a small chunk of text, like a few sentences, which is way more manageable. And it becomes very obvious.
*  Sometimes the models still make mistakes, right? But those mistakes become very obvious because you can check it. And then you can always also like scroll around the paper around that area to see like, was that section taken out of context?
*  So yeah, I think that's just like a really another really important feature of like doing things compositionally, making that easier to evaluate really quickly. And like that size is definitely very manageable, but having to read an entire paper to come up with answers like is weight is weightier here.
*  For that type of like source linking, do like I can imagine a couple ways of doing it. One would be, you know, you have a question like, does this part of the is this part of the paper suited to answer this question? And you could kind of page through, you know, to the degree that your context window allows to try to extract the part of interest.
*  Obviously, you could also like pre chunk it and use embeddings and then kind of do like a embedding comparison at runtime. How are you guys approaching that problem of like grounding? And then I'm thinking with the embeddings too, it could be kind of hard to determine like the retrieval might have failed, right?
*  So I feel like you don't have the answer, but the answer was there. But like, you know, you pulled the wrong section. The embeddings feel like not super easy to evaluate. So how are you thinking about that part of the challenge?
*  You can think about embeddings as roughly speaking, cache language model computation. So I think ideally, the ideal thing is probably more like the former thing you described, which is you go through the paper, like maybe paragraph by paragraph.
*  For each paragraph, you ask like the most expensive model, think step by step and you know, make sure you're correct and whatnot. And think really hard is this relevant? So that's like maybe the most expensive solution. And then ideally, you would just do that over all papers ever written by anyone and all their
*  paragraphs. And so that's my and then maybe and then you rank by like the probability that that says it's relevant or something. So that's maybe defines like one kind of ideal. And then you're like, well, we can't do that. So how and then I think you're trying to approximate it. And usually, the way we go approximating it is like in a multi stage process where like, well, first, we want to find like the 1000 papers or paragraphs that might potentially be relevant. And for that, we need to use like,
*  something very cheap, like some embedding based search. And then you're like, okay, now we have 1000 papers, we can use something slightly more expensive, like maybe like some cross encoder or something. And then you have like, maybe one or a couple of papers. And then you're like, okay, at this point, we can use expensive model and actually pick out the things.
*  But I think this is this is a setting where a lot of it is about like, well, what are kind of tractable approximations to the expensive ideal, and hopefully over time, like people will find better ways to both improve what the ideal is and also better ways to distill kind of the ideal into kind of faster cache computations. Yeah, I don't think there's like something super clean. I think this is something that needs a lot of empirical evaluation and like trade offs related to how much you're willing to spend on any one query.
*  From a product perspective, we've thought about showing multiple segments, this at least kind of helps bridge some of the gap. So yeah, if you can show maybe like right now, we're just showing the best, the most relevant segment and the answer from that. But if you can kind of let the user tab through a couple of the highly ranked segments, you know, even if you didn't get it right in the top slot, you can kind of compensate for where the model might miss some things.
*  Cool. That's really fascinating. Again, I think you guys are on the more I learned about what you're doing, the more I think it honestly, really resembles what a lot of corporations are going to be trying to tackle over the next like, year to two, because they're just now getting to the point where, first of all, they're just aware of all this stuff. And second, you know, the like frontline demo is good enough that they're like, well, they could do that, I could probably figure out my stuff. But now they've got these questions of like, okay, I've got to figure out my stuff.
*  But now they've got these questions of like, okay, I've got whatever internal knowledge base that's in whatever form. And then I've got, you know, whatever tasks and processes and how do I, you know, it's kind of like the assembly lining of knowledge work is what I sort of expect.
*  A lot of corporations are going to try to figure out how to do, you know, for at least a lot of the processes that they have, you know, anything that's not like driving alpha is, you know, is like a good candidate for, you know, can you kind of pull it apart and turn it into some sort of knowledge work assembly line.
*  I suspect that, you know, you guys will be in high demand as it becomes clear that this paradigm is like, really what the corporate customers are going to need. You know, they just, they don't want like, they may want some bigger tasks.
*  I think they probably will want bigger tasks than what you guys are doing. But still like, taking a five minute customer service phone call and like resolving an issue, you know, is one or two levels up from where you are, but it's still like pretty small, you know, in the grand scheme of things.
*  And they're going to be, I think, working really hard to figure out how can we structure all this activity that goes on in our business in ways that we can like feed it into, you know, an AI assembly line and kind of reliably get, you know, the desired behavior out the other end.
*  As we start to see like more high impact, high precision, high reliability applications will really need that infrastructure. So for, for kind of like, you know, approximate use cases, I think something like chat GPT will work just fine.
*  Like, you know, like you said, maybe just a quick second opinion or a quick like, what do I need to know? What should I then go and ask more questions about? It'll be very useful. But you know, we're seeing right now, obviously, people know that these models have issues with hallucination.
*  I'm sure a lot of those will get trained out or, you know, they'll be better at citing their sources. But I think it's still even in that world won't be clear that what the model is telling you is the best possible answer.
*  It's not just not incorrect, but like the best possible answer. And I think with these more serious use cases will need systems that can kind of give us the guarantee that this was the best possible answer.
*  And I think people want to apply these models much more systematically and not just kind of like take the first thing that they tell us, but like have them run through every single option and do really detailed checks and double checks and think about everything that could go wrong.
*  And then be like having all consider all of that. This is the best possible answer. So I do think that with more serious use cases, the way we relate to this model, the way they're deployed will change.
*  I also agree, Nathan, with your kind of analogy of kind of assembly lining of cognitive tasks. We've been thinking about that a lot as we build the new version of illicit.
*  So I think let's say right now still looks a lot like kind of quick searches, but we've been thinking more about, I guess we've been calling kind of large scale industrial cognition.
*  You know, where you're like, I'm not just trying to get like some quick answer to a thing. I'm trying to actually define some process where I go through like maybe all papers that I have internally at my organization.
*  And then for each paper, I extract the relevant insights and then I want to do like some systematic ranking at the end of the insights, maybe could connect it to other database.
*  Anyway, there's like some fairly large scale, probably pretty expensive process that I want to run, but at the end I want to have some guarantees about like how good the result is.
*  And so I do think there will be a lot more of that in the future.
*  That kind of also gets a little bit to this question. I think it's ultimately both. Right. So I'm not looking for like an either or answer because I think that would be silly.
*  But there's this debate right now between like the co-pilot model on the one hand and then kind of the assistant model on the other hand, right?
*  The co-pilot would be characterized by you are doing your work and you know, the thing is kind of helping you out along the way, answering your questions, you know, auto completing for you.
*  But I think that like the vision that people have in their minds there is largely like you are doing work in the same way that you do it today and you're getting help, you know, as if you had somebody, you know, sitting next to you, helping you.
*  Then the assistant model is like, I want to tell the system what to do and its job is to do it, come back when it's done and hopefully have done a good job.
*  And it sounds like you guys are maybe more trying to build like an assistant model type of thing where you want the user to be able to like delegate real tasks and, you know, not just be kind of getting the like weaving in and out of like AI commentary on the research process, but like actually get a quality literature review out, you know, and like be able to advance the research process.
*  I think it's like there may be like two dimensions to this.
*  One is what is the most helpful way to build this kind of co-pilot helpful like voice next to you and like the I think our bet is to figure out what the human is actually doing and then to break it down and then and then train it on those tasks.
*  So instead of just like, you know, letting the language model talk and hoping it's helpful, being more opinionated about what is helpful in this workflow, teaching the model to do that and then the model like we know that thing the model does will be helpful.
*  And then I think like, so that's maybe like about how we kind of approach building in the short term. In the long term, I think it's, it kind of like remains to be seen what is how do we how exactly do we want to integrate these models into more workflow.
*  So in the beginning, maybe they'll do yeah, like just how much of that process can be fully automated. I think I think our approach will be like, do the small tasks work up to bigger and bigger tasks.
*  It's very unclear if we'll ever be able to automate the whole thing. We'll just like keep, you know, keep automating models.
*  And maybe there's just like a fuzzy component of reasoning that can't be made explicit and easy to automate. So who knows, maybe we'll we'll encounter that at some point.
*  There is a fundamental constraint with the copilot model, which is everything needs to be really fast, right? Like you want the you want the model to respond immediately to you. You want your code completions to be there.
*  And then you want to be able to automate it. So that's the kind of thing that we're trying to do.
*  And I think, I think that's just a fundamental limit on how much cognition it can do. And so if there are tasks where you're like, well, I really would like the model to think harder about this thing, then either it's going to be an extremely slow copilot, which I think, I guess maybe that's just starts being more complicated.
*  But I think I would be like very surprised if all use cases that we are interested in just look like the copilot model, because I think that would make it easier to automate.
*  And I think that's just a fundamental limit on how much cognition it can do. And so if there are tasks where you're like, well, I really would like the model to think harder about this thing, then either it's going to be an extremely slow copilot, which I think, I guess maybe that's just starts being more equivalent to the assistant.
*  But I think I would be like very surprised if all use cases that we are interested in just look like the copilot model, because I think that would mean accepting that we never do more thinking than some like very small amount for any given task.
*  Yeah, I think plausibly there will just be some types of work that machines are better suited for than people and like it makes sense for machines like to divide and conquer here.
*  Like there will just it will just you know, language models will just be able to read way more text and extract information from them way faster and be like way more comprehensive and like they should do that type of work.
*  So it might not just it might we might not exactly be on a spectrum of like minimal automation of what the person does to maximal automation of what the person does.
*  But like it's more about specific tasks and how to split them up.
*  Yeah, that was a big point in the economic analysis report, the GPT's are GPT's paper that kind of accompanied or you know fast followed on with the GPT for release.
*  An interesting study there of like, okay, if we break down, you know, we know it's been commonly, you know, said and I think it is true that like jobs as they exist today are not really amenable to like AI's showing up and doing the jobs.
*  There's too much context needed, you know, there's physical demands, there's, you know, whatever there's a there's a member of it's pretty easy in any given job and the report says this like you look at any given job.
*  It's very rare to find one where you could feel like the AI could do the whole thing.
*  But then they break it down into tasks and say, okay, well now what are the tasks that make up this job kind of, you know, a composability paradigm right for what is a job.
*  And then they find like well, a lot of those tasks for a lot of jobs, you know, are in fact tasks that language models are now capable of doing.
*  So yeah, I think I come down. I mean, I think we will certainly see both, but it feels to me right now like there's honestly, I think a lot of wishful thinking kind of feeding into the co pilot model that it or there's like a there's a seems like a bias, you know, that is kind of a wishful thinking bias toward the co pilot model.
*  And one thing that's been really interesting in doing these interviews for the show has been, I always ask, I'll ask you guys too, but I always ask like, what AI products are you using regularly.
*  And strikingly, the answers are very few, like there's not that many, you know, products that are cracking into folks daily lives at this point.
*  And I don't think it's really because in most cases, like there aren't tasks that they could delegate, but rather like it's often just not quite helpful enough to go do it totally on the fly in these kind of, you know, only pop over to here and you know, get my social media content written or whatever.
*  Like, yeah, you know, I think what people really want is to like be able to delegate a whole task, you know, and have them done.
*  But then there's like some people who are, you know, either afraid of that or it just feels like uncomfortable to think of kind of building technology or it may just be bad marketing.
*  Right. I mean, that's the other thing. Like people may, they may know on some level that things are headed one way, but they may sort of still be telling a co pilot story because, you know, what else is GitHub going to do?
*  I don't know how they can't really. It's going to be tough for them to reposition around like the AI developer for, you know, lots of reasons.
*  For me, the barrier is when I'm using some of these language models to actually do a job that I care about, I have very low tolerance and patience.
*  Like, I don't want to, I don't want to read this like paragraph. I don't want to hear about how you're this like helpful assistant. I don't want to like give you feedback on this answer that you didn't quite get right.
*  I want just, I just want the best answer right away. And so I wonder, I wonder how many other people have like, some people I think are enjoying this flow with the language model where it suggests something and they give it feedback.
*  But I'm sure there are more people like me who are like, I just want to get this task done. And there's just like a little bit too much hullabaloo right now around interacting with this like assistant.
*  And I think that's, yeah, I think we'll probably start to see more products that kind of apply these language models in these higher reliability, well-defined, discrete tasks.
*  Because when you're trying to get something done, like you don't, I don't know that you want to have a long conversation about it. You just want the thing done.
*  I had one other thought on the marketing point you made, which is I think even if you ultimately want to automate whole tasks, maybe it's like pretty useful to get feedback from people.
*  So the models are still like relatively weak in many ways. And so being able to have millions of users use your product and give feedback on the places where they break down, I think is pretty useful.
*  Even if your long game is to cut the people out of the loop, maybe that's a cynical view, but that I think that's also one view that's compatible with what's going on.
*  Yeah, there definitely isn't an element of that. Like we had Omen named Kirthina from Google Robotics Research on, and she was talking about a paradigm where as they envision deploying robots in the future, you know, there's obviously going to be so many edge users.
*  And I think in some cases and long tail scenarios, as I understand it, their plan is to train the model to know when to stop and call for help and then have like a remote operator in a call center somewhere that will literally like use joysticks to do the task for the robot.
*  And then there's your training data, you know, patching. So yeah, there's definitely going to be a lot of that.
*  But yeah, I do wonder, I mean, Copilot is very good, right? Because if you accept, they have a very natural loop there. If you accept, it must have been at least decent.
*  You know, they're getting a strong signal like from a lot of a lot more suggestions probably than almost like any other product.
*  Anyway, enough Copilot. Let's talk about Illicit and the Evolution. Right now I go to the site, it asks me or it invites me to ask a research question.
*  And what I understand is you're kind of expanding the workflows that it can do. So tell us how you are expanding what new workflows and what we can expect next.
*  Yeah, so I think the way to think about this is last this like kind of initial phase of Illicit was about taking unstructured data and information is kind of like trapped in PDFs and structuring it into a much more organized table where you can kind of efficiently skim across a lot of information at once and automate the process of going into each PDF and figuring out what is it that they did and what should I know.
*  And then the next phase of Illicit will be, OK, cool. Now we have this like structured data model like this table. How can we run interesting and decision relevant queries on top of that?
*  How do we take this information and then make it even more useful and easy to get good answers out of that so that researchers don't have to then just walk out with this long list of PDFs that they kind of still have to read it.
*  They understand better what they are about better than they did before. So specifically the new workflow that we're launching, we're calling list of entities.
*  And the idea is, can we extract concepts that are discussed across multiple papers and then show you the concepts with links to the papers and the parts of the papers where they're discussed?
*  So this could be like a list of data sets that were used in lots of papers, a list of like machine learning benchmarks that a bunch of models were evaluated on, a list of effects of a certain medication, a list of interventions tried.
*  I'm trying to get it to work for like a list of limit like outstanding research directions, for example. So this list form is like it's really it's quite general.
*  I think the core thing here is querying this table in like a much more rich way and like organizing the information in that in a way that's like way more decision relevant.
*  So yeah, that's that's like the big next workflow. And then more generally trying to think about moving from one workflow to supporting many workflows.
*  So some of the other ones we have on the docket are running the kind of current version of illicit, but over like a much larger set of papers.
*  So we have a ton of organizations who have their own databases of like thousands or tens of thousands of paper and want to basically like get an illicit table for their own paper.
*  So thinking about supporting use cases like that. Yeah, those are some of the things that are on the roadmap.
*  You guys are a research nonprofit, but you describe yourselves as a product driven research or product led research nonprofit.
*  What like how do you envision that evolving in the future? It sounds like you're thinking of offering this on a commercial basis to large companies. Is that right?
*  In terms of our long term roadmap, we see this as having three phases. We have a three phase plan.
*  Phase one is discover what is known. Phase two is discover what is unknown.
*  And phase three is decide in the face of the unknowable. So we're currently in phase one. Discover what is known.
*  And the core problem we're trying to solve here is the fact that there's already a tremendous amount of information that exists in the world.
*  It's just trapped in unusable PDFs and books. Right. There's just too much information. It's not structured to be very decision relevant.
*  It's very hard to query. It's really hard to read an entire book to get a very specific answer to a question you're looking for.
*  So a lot of the capabilities we want to build out in this phase are around information retrieval and synthesis.
*  It'll probably mostly focus on text and we'll be trying to take kind of this unstructured text that exists in the world in papers and start structuring it into a tidier data format.
*  That's kind of what you see in Elissa today. We're taking a bunch of these papers. We're converting it into nice tables.
*  And then this next list of entities workflow is a way to query that table in a richer way to reorganize it at a higher level of abstraction where you're kind of looking more at concepts discussed across papers rather than the papers themselves.
*  So I think phase one will look a lot like query and kind of converting text into richer structured data formats and then flexibly querying that underlying data into more decision relevant insights.
*  And then phase two is discover what is unknown. In this phase, we want to help researchers start to contribute to that body of knowledge.
*  So having kind of parsed through all the text that already exists in the world in phase one, we should be able to start finding gaps in that literature and then helping researchers resolve those gaps or identify cruxes that they can resolve.
*  I expect that will relate to language models totally differently in phase two than we did in phase one.
*  And here I think the core kind of like technical capabilities we need to build are around causal modeling. So using these language models to develop good models of the world and how it works.
*  I think you can think of all the work in phase one is kind of collecting a bunch of observations, things that people have, yeah, observations that people have gathered from running experiments.
*  Then we want to collect those observations to build towards a richer causal model that we can generalize into new contexts and then researchers can kind of take that causal model and come up with new hypotheses that they can design experiments around.
*  So it'll be very different where in phase one we might want to apply language models kind of at scale and batch and go broad. In phase two, I think we want to go much deeper.
*  And then in phase three, decide in the face of the ennoble, we want to, we'll probably like switch user groups a little bit and move from research producers to research consumers, people who want to incorporate research into their workflows are making very high stakes decisions based off of research, but are not researchers themselves and who are kind of doing much more than just understanding the state of science.
*  And again here I suspect that we will relate to language models very differently. I think this here the kind of problem we want to address is that for some really high stakes decisions or some parts of those decisions, there is no right answer.
*  It really comes down to your values or your preferences or just certain choices you want to make.
*  And so at this stage, I think we want to use language models totally differently and use them to elicit the values and maybe kind of unresolvable assumptions or beliefs about the world that the individual has or the decision maker has.
*  So for example, if you want to make a decision about your career, part of that might be informed by research, but a lot of the rest of that might just be around your own values or other kind of hard to resolve beliefs about the world that you have.
*  So anyways, those are our three phases.
*  Another important thing to emphasize is that literature reviews is very much a small part of our long term vision. We don't see ourselves being a lit review tool forever. We don't see ourselves just being kind of a search engine forever, but understanding the state of literature and being able to work with that is just a really important building block to everything else we want to do.
*  So that's why we're starting here.
*  So exciting to you. We're hiring for a product manager. So this is one of my biggest priorities right now. So if you are interested in really helping us convert this grand master three phase plan into a concrete roadmap and are excited about all the different things, Alissa will become over time.
*  Please reach out or recommend people you know that seem like it fits.
*  I think sciency is the target market for that versus just like general use corporate, you know, because I could see you like what I was thinking, who would be your number one most natural customer? And I don't have a great answer. But the first thing came to mind was the Broad Institute.
*  And I was like, they probably have a ton of stuff, you know, that they might like to be able to search over. I don't know how much of it would be public versus private, whatever. But OK, there's that's kind of one. Certainly they have like the science pedigree.
*  But then I think like, OK, you know, Coca Cola, like, would that be a target customer? You know, they're doing stuff with AI, I guess. Is that in scope or is that just kind of too sort of commercial to consumery, you know, not sciency enough to be within what you guys would try to do?
*  I think broadly, we we want to prioritize researchers in a broad sense. And that kind of that will get broader over time. So up to now, we you know, most of our users are kind of academic researchers, PhD students or professors.
*  But there are kind of researchers like that in so many different places at many different companies. There are researchers who are trying to study the effects of sugar and Coca Cola or like what happens if you reduce the sugar content? Probably lots of researchers in industry.
*  And I think the activities that they are doing are very similar, even if the domains are slightly different. So we do want to support all of these people who are trying to have good models of how things work in the world and have like a kind of like science backed beliefs and are thinking a lot about this information.
*  And that's really a big part of why we started with literature review. We thinking about scaling up reasoning and research. This is like is a workflow that's general to many different domains and generalizes even beyond research. So so like, yeah, probably in the short term, we'll keep focusing on more academic researchers, but over time want to scale to everyone who's thinking really hard about evidence and trying to make good decisions in light of that.
*  I think it depends less on like, what, like, what is the flavor of the company? Like, do they sell sodas or not? It's more like what is it? What is the task they're doing? Like, are they trying to do serious reasoning? I think we probably do not want to just build like an internal search engine for someone's documents. I think there's going to be plenty of others who are going to be doing that. And but we do want to help if there if there are people are like, well, we're trying to figure out like, how to use AI to make better decisions or, or to make better decisions.
*  Or figure out what's true in some setting. I think then it becomes more interesting to us.
*  Okay, that's really interesting. When you think about going to customers like, you know, any sort of big company, would you imagine allowing them to kind of create their own composable workflows that are like riffs on yours? And then I guess the follow up question would be like, as soon as I imagine doing something like that, then I imagine what kind of work flow is going to be.
*  And then I imagine, well, these people are probably going to struggle with this. And then I go to, well, maybe I could get a language model to give them some good suggestions, which might also be in your plans like separately.
*  So at some point, you guys can't do all the all the decomposing, right? Like, what's the future of the decomposing process?
*  Yes, this is the dream. We tried actually earlier this year to use language models to decompose tasks, and they just weren't quite ready for that somewhat surprisingly. And so that's why with this list of entities workflow, we're still hard coding the workflow.
*  Like we're saying first, you should search over these papers and you should extract these statements and then you should cluster them in this way, etc.
*  I think the evolution here will be and I do think it's like for a lot of users, they don't want to look at a blank screen and think about how do I need to make this tool do what I want it to do.
*  So I think for most people, they'll want kind of like a predefined workflow to start with. And then from our perspective, the evolution will be we'll let users kind of edit parameters of the workflow to start.
*  So now I have this list of entities workflow. Maybe I can change the search query in the first step. Maybe I can get a few more papers. Maybe I can delete some of the papers. Maybe I can like change the way the clustering is done. That'll be like the next phase.
*  The phase on top of that is I can make more meaningful changes to the steps of the process. Maybe I'll search over a different corpus. Maybe I'll add another search step at the end to check if like this entity came up in Google somewhere or whatever.
*  And then I think once you're there, then you can get to a place where people are just kind of creating their own workflows, right? Like editing each steps could lead you to edit the workflow to an unrecognizable state.
*  And then from there, you can just kind of start from scratch and create the workflow from that thing. So and then once we have many more examples of that, presumably we could have language models also automating those steps or kind of co-pilot, suggesting the steps that someone might want to take.
*  And interestingly enough, we already have some researchers who are basically doing this. So in the main workflow in Elicit is LitReview, but there's this like secret page called Tasks, which is like our graveyard of all the things we experimented with before we landed on LitReview.
*  And it's a ton of things like brainstorm research questions, suggest search terms, like generate counter arguments, all these like very well defined little tasks. And we have some researchers who have like basically manually like stitched together the results of those tasks and like created their own workflow in this janky way because they don't currently support it.
*  Or they like, you know, have a question. They like generated a bunch of research questions, looked, asked each of those questions into LitReview, got a bunch of papers, picked the abstracts of those papers, ran the abstract summarization task, and then like, you know, generate a bunch of other ideas from that.
*  So they are like currently manually like stitching together these tasks to do more complex workflows. And that's something we want to like automate and make way more easy to do natively in the platform going forward.
*  It's like Zapier for reasoning.
*  Yeah, exactly.
*  Does the code base get hard to like corral? I mean, when you describe all those different tasks, and especially if they're all kind of explicitly defined, it sounds like a lot of code under the hood. Does that become a challenge?
*  I mean, it's an, I think, as with any fast moving field, I think it's an ongoing thing where we're trying to figure out the right abstractions. But I think ultimately there aren't too many of them. So there are a few core reusable components like search, answer, given results, summarize, rank, cluster filter.
*  Yeah, I often kind of think, and so you have those components, you can compose them, and I often think a little bit about it as functional programming with natural language. In functional programming, you have like some core higher order abstractions. You have like map, fold, filter, and then like most of the, like those are, there are like five of them.
*  And then there are, you can apply those to more atomic tasks, which are either deterministic computation or language model calls. But there isn't, it's not like there is a huge number of like invoked components that you need to add. In some sense, that's the beauty of language models is that you don't have like that many components.
*  It's about a bunch of pieces and you need to be kind of reasonably smart about how you compose them. And I think that gives you a lot of power. So I don't think we'll be in the business of like building out like the resilience of components.
*  I think we are more in the business of, as I think, I guess, I don't know how this relates to the Sapir analogy, but I think we are more in the kind of infrastructure for composing stuff business than in the make lots of small like bits business.
*  So what about when you get to like low level bits where, you know, like if it's medicine, then, you know, as you're validating the trustworthiness of the result, you might ask, like, is it double blind study or not? But then if I'm doing some different question in AI, then it's like there's, you know, generally, that's not really relevant.
*  So it seems like there's got to be some it's almost like, to me, it seems like there'd be this like fractal complexity at the low level. And like, maybe those are different components, but it seems like we're still like, I'm struggling to imagine how you could handle the breadth that you handle without some sort of like, dynamic decision making, you know, to figure out where you want to be on those low level tasks.
*  Yeah, yeah, I think there are different sources for that information. I think one source is, well, language models probably know some of those things. So I think you could just have dynamic task decomposition, where you ask the language model, what are criteria that computer science papers are generally evaluated on when reviewers make decisions or something like that. And then that probably say a bunch of things. And then I think the other the other source that we talked about, like just moments ago is people are using the platform and people have views on what they think is the best way to do it.
*  And what they consider important or less important. So I think that's, I think in some sense that settings we might also be the users of our platform and might also like have views and help people kind of come up with things like this, illicit right now the risk of bias analysis, I think is a thing that we put in.
*  But I think the trajectory over time is probably more of that stuff comes from some combination of users and or language models.
*  Yeah, I think in the, I actually think when you look at it, it is, there is a lot more generalization than than the example you gave might seem Nathan so in both that kind of computer science question and biomedical question, the activity that the model has to do is the same.
*  It's usually given this segment from a text, what is it's a question answering over a piece of text basically like according to this part of the paper what is the answer to that question.
*  And so that the source of that, you know that that text can be a computer science paper or it can be a biomedical paper but like the activity of use this text to answer this question actually generalizes to both of them.
*  So, I think so far in our experience it hasn't. They, those two don't have need to be treated differently. And then maybe in some of these kind of higher level workflows they might be a little bit different so you know like under so saying maybe this risk of bias checklist,
*  or we ask about sample size, whatever like is that that checklist looks a little bit different for medicine than it does for machine learning but we might just want checklists in both cases and then the way we create those checklists might still be pretty similar even if the content of them is different.
*  So today it says like there's a challenge that you guys probably have made it a real art out of.
*  Again, we had something kind of like this with waymark at one point in time where, especially before language models, you know we're trying to generate content for people in this like on a magic way.
*  And so, but you know our customer base is like all small businesses. So the diversity is insane. And we got pretty good definitely that is good as like the language model experience is now we got pretty good for a while at like classifying businesses into one of 20 categories, and then having like, you know, kind of uncanny Valley copy for a lot of those categories where we'd be like all right well if this is a law firm then we're going to classify it as like professional services and then we're going to classify it as professional services.
*  And then, you know, the kind of like marketing messages that they would want to talk about would be like that they're trustworthy and trusted and you know, versus like if you're a restaurant, you know, it's not you're not going to go with messages like that but you'll go with you know what a great taste and what a great, you know, experience and how much fun it was whatever.
*  So, I imagine you have to work pretty hard to kind of think about like, what are the right types of questions, you know, because even in that scenario of like it's just question answering over a text.
*  So the question of like what question right is it, is it answering. And I imagine you have thought pretty long and hard about like what are the questions that are kind of most generally applicable.
*  I wonder if there are any questions that you could, you know, that you could share that are like, yeah, this is where you know yeah we don't ask, is it double blind because that you know doesn't, you know, generalize super well, but instead we ask like, yeah we found this is the magic question to ask for, you know, evaluating a random piece of text from a random paper in a random field.
*  Yeah, I think we, I think generally the things people want to know across all domains or something like so this is maybe for empirical domains and then I do think for theoretical domains.
*  It might be a bit different, but within empirical domains I think the fundamental questions are like, what did they do, what did they find, what did they not find something like that like at a very very high level.
*  Yeah, so right now we've been last year we were focused on kind of helping biomedical researchers and then this year we started trying to help more like people who are working on AI safety or like empirical kind of machine learning research.
*  And I think a lot of the structure generalized, even if the specific questions were a little bit different so now and when we ask, what did they do in medicine we would ask, what was the population, where were they located, what was the intervention.
*  And now we ask like oh what was the data set, what was the model used. What was the word the techniques use or something like that.
*  But yeah we just I don't think we really like run into significant problems with generalization there.
*  Yeah, I think the, what did they not do I feel like is maybe the one magical question to ask about everything. So, I always find it most useful to ask what limitations to the author say their work has like there's like extremely true for machine learning papers because if you just
*  look at the abstract often you're like wow that's amazing this solves everything. And I think it's also true of other work, because knowing what they didn't do really helps you understand the boundaries around the thing they do often better than the literal explanation of what they did.
*  So, yeah, if I had to pick one question, maybe that would be it.
*  I know you guys are like kind of sewers of language models, and I've seen from, you know, some of your published stuff on Twitter that you're using a mix. So I'd love to get a sense for kind of what the ensemble of language models are that you're using I know you've done some in
*  in house training, and you know also using some commercially available stuff to kind of sketch that out, you know, to the degree that you're that you're comfortable doing so I think people would learn a lot from that.
*  And so I think we have tried basically everything that exists.
*  So, including like all API's like topic, open AI co here, because we haven't tried the Google API. Anyone from Google's listening, please give us access.
*  Yeah, and then I think we've tried most of the open source models to guess, Galactica and GPTJ flanty five, the one that we've found most useful for deployment and practice.
*  Right now is like the flanty five XXL model that 11 billion one. I think it's a good kind of trade off between like reasonably small not too hard to deploy and like reasonably powerful.
*  We did, I guess, there's a little bit of a switch from mostly using the open eyes API before, and the reason mostly being that for the switch mostly being that it does get very I guess we have something like 250,000 users now, it does get very expensive.
*  And it's just a lot cheaper to deploy your own thing. I think the space is changing very quickly so I wouldn't be surprised if we talk to you again in three months and we're like, actually we switched everything to some of the model.
*  So it wouldn't make too much of it. I think this is generally something we that we see, we think we have to be really good at and probably a lot of like language model applications will want to be really good at is the ability to very efficiently test and deploy and
*  swap in different models, actually because I think there will continue to be so many of them from commercial providers as well as open source providers, but also because the rate at which these model capabilities are changing is moving so quickly that I really want us as an organization to be very good at taking a new model and then figuring out okay what new workflows are enabled by this and what it's not what is still missing.
*  And this is something that I really hope like we get really good at as an organization I think it's like somewhat researching in nature as well.
*  One of the things we haven't been doing too much of but that we're very excited about is kind of combining, ensembling multiple models, I think, especially if you have models that are trained on different data, you cannot ask and they all agree on the answer I think you can be a lot more confident that the answer is correct than if you just use one model.
*  So I think often people are like, well, you know, aren't the big labs going to do everything and I think that's like maybe one situation where that's not the case. And there is room for other organizations to be like, okay, they're trying to kind of combine the results of different models in ways that lead to overall better performance than you could have gotten from any one model.
*  Yeah, okay. That's really interesting as well. That's another good nugget. So I mean, one thing that I would just highlight here is you have an 11 billion parameter model working roughly as well I guess as like top of the line models once fine tuned for your particular tasks.
*  How narrowly are you fine tuning those models like do you have like a whole set of different blonde XXL 11 B's or whatever exactly it was.
*  No, so it's like fairly broad, although the task still has relatively short answers generally. So the tasks that we've used this for our one.
*  So this it has abstract dependent summaries. So if you ask a question and elicit, it tells you not just here's like you're the papers and here's the abstract of each paper and it also doesn't just say here's like a TLDR for each abstract it says, here's like a TLDR that was computed for your question specifically.
*  So that's one task that we find you in the model for and then another task is just question answering from papers like the task we talked about a bunch earlier, where given these paragraphs from the paper and this question, tell me concisely what the answer is.
*  And I think in both cases, yeah, first of all, I think those are both like not that specific there. I think they apply to lots of papers, but they do share that you kind of want to use a common rubric where the rubric includes things like is your answer kind of concise and truthful and useful to the reader and there are a few other entries in the rubric.
*  So that yeah that's the rough shape of the training we've been doing.
*  What is the role of I mean it started to sound like a constitutional AI approach there for a second. How are you thinking about incorporating AI into the evaluation and feedback cycle that definitely seems to be one of the biggest trends in the space right now.
*  Yeah, yeah. So yeah, that's exactly right. That's the constitutional AI approach where I guess for context. So usually in our LHF reinforcement learning from human feedback, you have human annotators, they look at like two options and ask which of these options is better.
*  And so in constitutional AI, basically what you do is you replace the human annotator with a language model, ideally like as large a language model as you can find, because the evaluation task is pretty tricky.
*  And then you tell the language model, here's the rubric, here are the options, judge the options that are according to the rubric and select the one which is better. This gives you a pairwise judgments, then you can train a reward model on those judgments.
*  And then once you have the reward model, you can then train like a generation model in our case as mentioned, 95 on kind of getting higher rewards from that reward model and yeah that that's a process that like our engineer Charlie has been leading and that has been
*  like working very well for us. You also mentioned like the bit you've used the biggest language model you can find in the evaluation. Obviously, new biggest available language model just dropped in the last week and a half.
*  What are you seeing right now in terms of like the front absolute frontier of language models? In other words, like, is GPT-4 changing the game for you at all? Where is it? What are the tasks that you know are going to be changing?
*  What are the tasks that you know are kind of the ones you wish it could do but it can't yet do?
*  My very brief answer is, I think, yeah, it is a significant step up. I think it still is not great at avoiding hallucination. So I think we keep saying this every time we talk to someone from OpenAI or any of these arcs is like, stop making bigger models, just make them hallucinate less.
*  I think that would really make, I think tying it back to our earlier discussion about kind of error rates and such, I think if you could just get high robustness, I think actually that would often allow you to do more than if you had a bigger model.
*  So that's a that's a thing like if you ask the model, for example, who is Andreas Stuhlmuller? It would it would like probably give you a description that kind of sounds like me but not quite and would have like some made up facts about what universities I went to.
*  That's the thing we wish for. We don't have yet. Just the I guess TB4 nominally also supports image inputs, that sort of thing that is really deployed yet.
*  But I think that will actually be quite transformative because a lot of papers are like communicate substantial information in like their figures and their tables.
*  And I think being able to access those will be very useful. So excited to see how that goes.
*  I think there are just like so many workflows left that we would like to be able to use the models for that we can't.
*  I think one really exciting, I think a lot of it comes down to like having good causal reasoning and like causal reasoning and logical reasoning abilities.
*  One thing that I would really love them to be able to use the models for are like kind of efficient clustering and cutting up of a space.
*  So if you have like a bunch of research ideas, how do you organize them or like a bunch of ways you can kind of set your product roadmap?
*  How do you cluster the space? What are actually the cruxes and the constraints that kind of segment the space up most efficiently?
*  And like another one is I think this came up earlier in this conversation, but like the ability to know when to stop and like trace it's when to realize that it's hit a dead end.
*  Part of why our earlier experiments this year with trying to use the models to choose the reasoning themselves didn't work is because the model is just like really bad at being like, wait, I'm confused.
*  This answer is wrong. I need to go somewhere else. And it just like once stuck on a path just keeps going down the path.
*  I think that that makes sense as like a next token predictor. It's just going to keep predicting tokens and not be like, stop, stop and go do something else.
*  So I think that that is like another capability that I'm that I really hope can like help unlock a lot of good reasoning.
*  Yeah, I've observed that too. I was also thinking about that recent like jailbreak, I guess they call it token smuggling where you can get GPT-4 to generate some like flagrantly toxic or whatever kind of content that it ordinarily wouldn't.
*  If you sort of embed that generation in like a function and you know it sort of reads the whole thing as code and attempts to like execute the function.
*  And it only needs like a few tokens. I almost imagine it as like pulling on a string, you know, pulling a thread out of a sweater or something, you know, and then like once you've got once you've like just pulled a little bit of toxic content out of it.
*  Next thing you know, you know, the rest of it just kind of on schools. That's like almost, you know, like mechanistic insight there somewhere, I think.
*  Yeah, the other thing that I think people have already started to notice is that a lot of the reinforcement learning is kind of causing the models to be incredibly uncalibrated and overconfident in their answers.
*  And so that is preventing us from being able to use these models in particular ways. Like we would really love to be able to use GPT-4 for ranking, but it's just like it's like really not helpful for things like that.
*  So I don't know how we're going to kind of like resolve this trade off.
*  Yeah, they're also not giving the log props anymore either. So you've got kind of a couple barriers on some of those things.
*  You mentioned earlier like a headline number of users, but one thing I'd love to hear if you have any kind of interesting anecdotes or observations on is how people are using your product.
*  Like you said a little bit about who they are. Is this something that they come to like from time to time? Do they sort of sit there all day like rifling through lit reviews?
*  Anything that's like interesting or kind of surprising about your users behavior I'd be really interested to hear about.
*  Yeah, I think there's a pretty wide spectrum of use cases. So maybe something like 60% of our users are academic researchers, but the rest of them work in, you know, I think tanks and government and finance and consulting.
*  Some of them are just like small business owners where they work in like kind of a lot of people actually from like medicine, a lot of clinicians and medical people.
*  There's definitely a core group of people who are using it very regularly. At least 10,000 people use it more than once a week.
*  And some people, when we built Elyssa, we took a lot of inspiration from the existing systematic review process.
*  It's a very common process in mostly in biomedicine.
*  And it's like a pretty structured process for synthesizing a lot of research and trying to understand across many different papers what is the answer and how do we kind of triangulate the results across different papers.
*  So different teams who are kind of working on that are actually using Elyssa as part of their process to find papers and extract information that they need to kind of work on those types of projects.
*  And then, yeah, I think the other cool thing is like finding researchers like the one that I mentioned earlier, who are stitching together many different tasks and kind of like hacking together his own like workflow automation and all of that.
*  When people are hacking your product to get access to things that it can do that you haven't bundled up for them, that's usually a pretty good sign of something.
*  So we mentioned a little bit earlier just briefly that you guys are a nonprofit research company, but you do have some business plans.
*  It seems like, you know, nonprofit AI orgs developing business plans is also kind of a trend.
*  How are you guys thinking about your future as, you know, like a revenue generating, self-sustaining organization, perhaps?
*  We really want to be able to scale Elyssa's impact up quite a lot and be able to have it guide and really help with a lot of very high stakes decisions.
*  It's grown a ton over the last year, so we are kind of hitting the limits of what is feasible with philanthropic funding and also want to, you know, just kind of want to be self-sustaining as an organization.
*  So yeah, that's like a question that's kind of like actively on our minds.
*  We're thinking about what is the right vehicle to house Elyssa in going forward?
*  I wonder if there's a, you know, of all the like language model first companies I've talked to, you guys remind me most of Wolfram.
*  There's definitely some like shared DNA there of kind of highly explicit reasoning.
*  Yeah, there's definitely some like shared aesthetic in terms of like having like very explicit processes that are like robust and trustworthy.
*  I think there's also important differences in aesthetic.
*  I think Wolfram is like very much open to just building a gajillion widgets and then putting them together.
*  And I think we're probably a bit less excited about that.
*  Yeah, I don't know. I like Wolfram. It's a cool company.
*  Obviously, you guys are motivated, you know, to a significant degree by AI safety considerations.
*  And you made one kind of offhand comment earlier about when you talk to the, you know, the biggest leading labs in the space right now, you say you don't need to make bigger models, just make them more reliable.
*  How would you characterize the overall state of the AI safety issue, landscape, you know, and kind of our prospects?
*  Like right now, it seems like things are changing really quick.
*  We're getting signals from like OpenAI that they're going to, you know, try to do something to kind of keep things under control, hopefully.
*  It would be very interesting to see what that turns out to be.
*  But what's your kind of outlook on the whole scene?
*  I think my honest answer is that I have just have like a tremendously wide range of what could happen.
*  That includes like kind of all of the possibilities.
*  And I just have tremendous uncertainty. I feel like it's just an impossible thing to predict.
*  The things that I do feel more confident about is that I really would prefer for this not to just be a technical solution, at least in the community that we grew up.
*  I think there's a lot of focus on the technical challenges and there are real technical challenges.
*  I think they would be very high impact. And so it's really important.
*  I also kind of want to think a little bit about like how, you know, especially with things like process supervision, one of the things that I think we want to see happen is consumers demanding this type of process supervision and transparency from their AI products.
*  And we don't want to see safety just happening at the largest AI labs, but also at these end user applications.
*  I think they will care internally as they're building their systems about being able to supervise the process and control and debug and also for their users enabling a lot of those features.
*  I think there can be a lot of helpful pressure coming from that side. I think they're yeah. At upstart, we were trying to use machine learning.
*  My last company, we were using machine learning for credit within finance, which is an incredibly heavily regulated industry.
*  And I think there I got to see both how regulation could really protect consumers and be very good for the world.
*  I was on the marketing side and I was just like not allowed to make any false statements or any statements that were not very heavily supported by like evidence and data.
*  And so that that seemed like really good for consumers and like people lying about their products and their financial products seems really bad, but also kind of saw how just how much regulation lags relative to the pace of innovation.
*  So I really hope there that's like another kind of mobilizing force on the safety side.
*  I don't I don't I don't I'm not the expert here, so I don't know how exactly we kind of bridge that gap.
*  But I think it's so important that the regulatory side moves at pace with the technology and also that people have examples of where regulation can be really good for the world, especially the tech industry.
*  And like trying to kind of set be setting a lot of those things up. So yeah, I think that the technical challenges are definitely pretty tough.
*  But I think there there are a lot of other ways we can be coordinating in different parts of society to make this go as well as possible.
*  Yeah, I think I roughly agree. I think also things are very uncertain and up in the air, I think.
*  Partially because in the big scheme of things, I think still not that many people are actually working on these issues, both on alignment and also on kind of making a go well more broadly construed.
*  I think in our space specifically, which is like using AI to kind of increase wisdom, improve decision making, et cetera, I think actually basically no one is working on this, which is maybe kind of surprising.
*  I think one of our best hopes to make things go well is probably figure out better ways to use AI to support us as it becomes more capable, help us coordinate better and figure out what kind of good plans look like.
*  So I think things are up in the air because it's really a high lever situation. I think individuals can probably still make a surprising amount of difference by like pushing in the right directions.
*  I think there are many signs for hope. I think big AI labs all have expressed that they do consider process supervision among their core alignment bets, which I think is a great sign.
*  I think we haven't really seen that cash out in real investments yet. I think if we, for example, I don't think we've seen any kind of scaling up of investment in process supervision that looks at all comparable to how much has been invested in scaling up models.
*  I think if you did see that like better tools for writing compositional language model programs, better debuggers, visualizers, compilers, I think there's a lot you could do in principle.
*  And I think people having expressed that they're excited about this should make us somewhat more optimistic. But I think not having seen the kind of investments on the ground yet should still make us feel like, ah, these things could go.
*  We don't quite know yet how things will go. But on more optimistic days like today, I feel like, yeah, probably I will help us kind of better see what the risks are, help us better coordinate.
*  Probably eventually we'll figure out that we'll have to limit large scale training runs to some extent and AI will help us see more to the extent that that is the right decision.
*  It will help us see that that's the right decision to the extent it's not, it will also help us see that. So I think there is reason for hope.
*  What Jungwad said at the beginning really resonates with me around just very extreme uncertainty. Your comment too about like some sort of high end.
*  I mean, that seems to be obviously a big idea that's kind of circulating in the space is like, could we put some sort of, and this seems like it could be a really good idea, put some sort of like high end cap on super large models.
*  Beneath that there doesn't really seem to be any reason to worry too much now. I could change obviously with a new architecture or some sort of conceptual breakthrough.
*  But at least for now, it seems like that limit could be pretty high and not really the kind of thing that like inconveniences, you know, PhD students or startups or whatever, but still would like create some decent visibility into, you know, what is going on that might actually have some
*  tail risk associated with it. I'm thinking a lot about that right now as well. Obviously I'm not, you know, any sort of ultimate decision maker, but to the degree that I can shape the discourse or whatever, that does seem to be a pretty attractive scenario right now that like, again, doesn't hurt people, you know, doesn't doesn't constrain most people too badly.
*  Which is nice. Like you guys can continue to fine tune your blonde XXL is all you want. But, you know, if somebody is going to go 100X past GPT-4, you know, it seems like society has an interest in getting a tip on that before, you know, before the whole thing kicks off.
*  Yeah, I am encouraged by the fact that I think probably compared to other technological revolutions of the past, a lot of the decision makers in this context have been have cared about safety from the very beginning.
*  So obviously there's like some disagreement about what safety should look like and how much, you know, some individual opinions vary. But I think everyone.
*  Yeah, I think I think everyone is trying really hard to be as safe as they can and like think a lot about their, their, the consequences of their actions and trying to be altruistic in that way. So that's a, I think that's very good.
*  And if we think about like, I don't know, tobacco or something like that, I just like, don't think that was the case from the beginning.
*  Yeah, or even just, you know, you go back like the Industrial Revolution. I don't think I've often said this, you know, I don't think James Watt had any idea what was going to happen.
*  There does seem to be just a lot more emphasis on the kind of trying to get ahead of it, you know, than there were in previous things too, right? Or like Gutenberg.
*  I don't think he had any great sense of like, you know, what he was unleashing in that moment. And we don't have a great sense of necessarily what we're unleashing either.
*  But at least we have a sense that we're unleashing something. I guess that's, you know, that's one step ahead of, you know, Gutenberg. So yeah, that's it. That's a reason for some optimism anyway.
*  So guys, this has been fantastic. I really appreciate your time. Thank you for going so long with me and educating me so much along the way. I've got three kind of fun questions that I always end with.
*  You can, you know, go on as long as you want, or you can just give me quick hitters. But number one is, what are the AI products, services, tools, whatever that you guys are using, you know, obviously aside from your own?
*  That you would recommend to others that they should check out?
*  So this might only apply to a small subset of your audience, but I use Emacs a lot and I wrote my own kind of GPT Emacs extension, which is open source. You can search for a GPT.EL.
*  I actually use it a lot, I think, both for coding and for other tasks. I think it's just really nice to have
*  language models accessible in the environment for you, but do basically all your cognitive work. Yeah, maybe Jungwon has some more scalable suggestions, but that's mine.
*  I've honestly been like really amazed and delighted by the chat GPT deep note combination because I'm not, I don't have much of a programming background, but I've just been able to like do way more cool things with Python with that combination.
*  So yeah, I don't know that it's not an AI. It's I don't know if it's a chat GPT, but I think that's the thing that I use most regularly.
*  Those are both good answers. One of the things I'm going to read about later today is the new plugins architecture that OpenAI just launched today.
*  And there may be some, you know, really cool new stuff there as well. I'm sure there is. But it is funny, actually, most people don't have a lot of answers right now.
*  I do keep coming back to that. It's like the I think the application layer is, you know, maybe kind of a mirage right now.
*  There have been literally like thousands of things launched and, you know, sent out in newsletters or whatever.
*  But the number of things that I actually hear back like you guys, you know, gave more original, you know, kind of novel interesting answers than most.
*  Honestly, it's been just a lot of like chat GPT. OK, number two. So let's imagine a future scenario where one million people have a Neuralink implant.
*  If you get one, you are enabled with thought to text. In other words, you can control your computer purely with thought.
*  Would you be interested in getting one yourselves? 100 percent. Absolutely. I've already thought about this.
*  I just have this. I just like have this experience all the time where I'm like, why do I have I just spent so much time converting my thoughts and typing them into my like note taking system.
*  And I don't I just want to be automated and then like transferring them to messages to other people. So, yeah, I would definitely want this.
*  I mean, I would have a few questions about the details of the implant.
*  But I don't know. It seems like if you're paying for it, Nathan, sure. Why not?
*  Yeah, maybe that'll be part of our universal basic intelligence stipend at some point.
*  So, OK, last one, just zooming out, you know, as far as as you can.
*  You've touched on this throughout the conversation, but zooming out big picture rest of the decade.
*  What are your biggest hopes for or positive vision for an AI enabled future?
*  And then what are your biggest fears of what might come as a I permeate society?
*  I think we're like culturally, I don't know how much this is just me being in my bubble, but I think it's a really important part of the process.
*  I think that I have the sense that like a lot of people have a lot of pessimism or there's like kind of a sense of wonder that has been lost.
*  And I think it's easy to lose sight of how much about ourselves and our world still remains to be discovered.
*  So I really want tools like Elicit and just AI more broadly to be able to help us, you know, not just kind of really amplify existing research efforts,
*  but help us to discover entirely new ways of doing research, discover entirely new research methodologies, entirely new research domains that weren't possible before about like,
*  you know, now using kind of this human artifact of text as a data source, analyze us more intelligently so that we can better understand who we are and how we relate to each other and what kind of systems we can we can build to help each person and every group of person flourish more.
*  I'm really excited. Like, you know, I have I've had friends who kind of worked in different layers of government throughout COVID and they tell us about all of the kind of very impactful decisions that were made on complicated email threads.
*  I would really like that to stop and for like AI systems to be able to generate like very decision relevant, but really, really, really simultaneously rigorous kind of one pagers policy memos and answers to people in very high stakes decisions so that we can just become way more intelligent
*  about how we can govern this world. And then I think on a kind of a somewhat like not illicit related dimension, I feel like there's so much that AI could do for us.
*  Like, you know, things like just being able to translate so much more easily, not literally in terms of language, but in terms of like style or tone or trans like helping us kind of communicate with like, you know, different neuro like atypical people or styles or something like that.
*  I feel like there's just a lot it might be able to do that makes us more human, you know, and not just kind of like, even though it's a machine, I think there are a lot of ways that it can help us be better humans as well.
*  My perhaps idealistic view is that like most disagreements are broad beliefs, not values that like, I think people disagree about like how big a deal is AI, like, is it an existential risk? If so, is it like 90% doom or 10% doom? But I like I think that's true much more broadly.
*  Is universal basic income good? Is it bad? Probably a lot of these disagreements in terms of like policy and like what should be done are more about people having seen different kinds of evidence and less about people having fundamentally incompatible values.
*  And I think on the optimistic view, AI helps us better see the totality of the evidence and so be able to better coordinate and be less in the situation where I'm like, well, it's like my faction against your faction.
*  I think this is the right thing because I have those values and more like, well, actually, this relating to the translation comment that Jungwon made also maybe actually AI can help me understand like, why do you think the thing that you think?
*  What evidence have you seen that I haven't seen and vice versa? And so I think I think there is a like very optimistic vision that could happen like this decade or could happen later, where it just becomes much more clear what the right things to do are for a lot of big picture decisions.
*  And that could lead to things going quite well.
*  On the negative side, I think. Yeah, I'm quite worried about chaos. Like I think there could things can get chaotic for a lot of reasons like perhaps even before and kind of unaligned AI takes off just the fears around this like the imagined problems can cause a lot of chaos.
*  I think, you know, like the kind of generative models, the classic fear of like generative models creating disinformation or just like creating a lot of stuff.
*  I feel like we already have so much noise that we need to sort through already in our current society. And I worry that the noise will increase without the signal or the ability to synthesize that information, those capabilities increasing.
*  So I think I think we really need to keep up with like keep up the ability to make sense of the world as it gets more and more complicated.
*  Thanks, Nathan. Great to chat, Nathan.
*  Omnike uses generative AI to enable you to launch hundreds of thousands of ad iterations that actually work customized across all platforms with a click of a button. I believe in Omnike so much that I invested in it, and I recommend you use it to use cog rev to get a 10% discount.
