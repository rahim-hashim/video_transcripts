---
Date Generated: April 03, 2024
Transcription Model: whisper medium 20231117
Length: 9169s
Video Keywords: []
Video Views: 353
Video Rating: None
---

# Hollywood Strike Update and AI Roundup with Trey Kollmer
**Cognitive Revolution "How AI Changes Everything":** [September 12, 2023](https://www.youtube.com/watch?v=IoMyV_hCgk8)
*  Even she showed pictures pretty similar image generated by stable diffusion model next to a
*  training image that was very similar to it. And she even thought that no court would conclude that
*  that was a derivative work. But she does think there may be questions in the training of the
*  model. You're copying these works and using it to create a system which can just replace a lot of
*  the underlying creators, but that is really up in the air and that maybe Congress will weigh in
*  and come up with new legislation because it's so new. It seems like crazy. We're trying to apply
*  these old laws and old rules to such a different paradigm. Hello and welcome to the cognitive
*  revolution where we interview visionary researchers, entrepreneurs and builders working on the
*  frontier of artificial intelligence. Each week we'll explore their revolutionary ideas and
*  together we'll build a picture of how AI technology will transform work, life and society in the coming
*  years. I'm Nathan Labenz joined by my co-host Eric Thornburg. Hello and welcome back to the
*  cognitive revolution. Today, Trey Colmer returns to the show. Trey first appeared in episode number
*  30 released back on May 30th when he and two other members of the writers guild of America discussed
*  the WGA's AI related strike demands and also shared a glimpse into how they are beginning to
*  use AI as part of their individual writing processes. That episode was extremely well received
*  thanks in large part to Trey's nuanced perspectives and I encourage you to check it out if you haven't
*  already. This time we're doing something a bit different because Trey's interest in AI goes much
*  deeper than just experimenting with it as a writing assistant. Like me, he's extremely curious about
*  all AI developments and very well read on the subject. So I invited him back to first share an
*  update on the strike and also to bring his own questions for discussion. We ended up talking for
*  over two hours covering topics including our ever evolving understanding of AI models reasoning
*  capabilities, why Nvidia stock has popped so much more than other AI stocks including their critical
*  partners TSMC and ASML, and where else besides hardware value is likely to accrue in the AI stack
*  over time. Finally, we close with a segment that I'm calling the reality writer's room in which we
*  imagine our current reality as entertainment for some hypothetical outside observers and then attempt
*  to identify scenes of particular interest or dramatic irony. One small correction, toward the
*  end of the episode I incorrectly said that Inflection AI was one of seven companies party
*  to the frontier model forum but what I should have said was that Inflection was one of seven
*  companies that agreed to certain voluntary commitments as part of a recent White House
*  statement. Those seven companies are OpenAI, Anthropic, Google, Microsoft, Meta, Amazon,
*  and Inflection but only four of them, OpenAI, Anthropic, Google, and Microsoft, officially joined
*  the frontier model forum. It's a small detail but worth getting it right. If you ever notice any
*  inaccuracies in the show or just want to request the return of another favorite guest please reach
*  out and let us know. You can always email us at tcr at turpentine.co or feel free to DM me on the
*  social media platform of your choice and if you're finding value in the show we always appreciate a
*  review on Apple Podcasts or Spotify, a comment on YouTube, or just a plain old share on social media.
*  Now I hope you enjoy this wide-ranging conversation with Hollywood writer and fellow AI scout Trey
*  Colmer. Trey Colmer, welcome back to the Cognitive Revolution. Oh thanks, thanks for having me back.
*  Very excited. This time we're going to pursue a wide range of topics but just for context if
*  folks haven't heard your first appearance they should go back and listen to an interview with
*  you and two of your guild mates which was a very well received episode. People thought that the
*  perspective that you guys had on language models and the way that it's kind of playing into the
*  dynamic that you have with the writer strike was super interesting and certainly I agreed with that.
*  This time you brought a bunch of questions and I'm excited to get into those with you
*  but before we do that tell us what is going on in Hollywood. Okay so the two main updates
*  since last time is that the actors have also struck so SAG and AFTRA their deal came expired
*  and they weren't able to negotiate a new one so they they are now on strike and some of their
*  demands are AI related and we can get to those but there's also been some movement on the writer
*  strike. After a few months of not speaking with us or with the union the AMPTP which represents all
*  the studios invited us back to the table and we negotiated for a bit and you know it ended up
*  falling through this negotiation but the AMPTP did release their latest offer they released it
*  publicly part of it's a tactic to hopefully get some of the writers to say wait that actually
*  sounds pretty good. It hasn't been effective so far but it's kind of interesting in some of the
*  movement that there's been on the AI issues. Last time we talked I think two of the big things we
*  were concerned about were the way that these language models could be used to create loopholes
*  that would take some of the credit from writers or some of the compensation.
*  You know one example is a rewrite is usually paid at a lower rate than a first draft so they could
*  have an AI model write the first draft and then pay the writer less for the rewrite and that's
*  just what we're like in television a teleplay you get paid separately for the story and for the
*  actual script so the language model could come up with an outline and then you would lose your story
*  payment but so the AMPTP did come back and their offer agreed that generative AI generated material
*  could not be considered assigned material it would not be considered the first draft for the
*  purpose of a rewrite and basically it's treated as just research that if you are starting to write
*  from scratch there are no claims for the studio or for anyone else because of the material generated
*  by the model which does seem like it starts to close off some of those other loopholes.
*  I'm not fully sure you know there's all if it covers everything we'd want you know the the Guild
*  they have to be a little bit private with kind of how they're thinking of progress and the updates
*  to both not give away their bargaining position and to not I think talking publicly can kind of
*  sabotage the talks as they go so they're so we don't have full access to everything they're
*  thinking but the other thing that they're still very concerned about and which has not been
*  addressed is whether writers material can be used as part of training data to train new models or
*  to find tune models that could then generate scripts and it seems like that's something that
*  we're still fighting for and it kind of intersects with I was talking to another union member who was
*  more involved in setting up some of the demands before the strike and it seems like right now
*  there's two prongs that the Guild is thinking of in in terms of trying to address the idea that
*  these models will be trained on our work and then replace us and one is that there have been some
*  copyright lawsuits so there's been a number of copyright lawsuits against most of the labs open
*  AI meta stability and one hope is that from people you know trying to protect writers jobs
*  is that this will slow down the use of just taking this copyrighted material and using it
*  to improve the models and there's this great talk I don't know the simons institute had a bunch of
*  talks the past couple weeks and this one woman Pamela Samuelson had a really interesting talk
*  on these lawsuits and there's sort of three theories or three ways that she was explaining
*  the lawsuits claim that copyright is being infringed and the the three main ways are that
*  one is if there's copyrighted material in the data set just to copy the material onto their servers
*  in order to do the training is it could be an infringing event just even if no one sees
*  the material or if the material isn't republished just the copying of the material could be
*  infringing the second concern is that the outputs of the model if they're so similar to what they've
*  been trained on or if they're considered just derivative of the work that's been trained on
*  the outputs of the model themselves that could be separately infringing material and then there's a
*  more technical one that it's illegal to remove or alterate what's called copyright management
*  information so you could picture watermarks on getty images if you are removing watermarks like
*  getting images or a more complicated thing that's happened is if the model is trained on images and
*  then generates very similar images that no longer have the watermark that could get you into trouble
*  I mean there's a lot of interesting things in our talk one is that a lot of the issues seem to come
*  down to fair use and whether this use of copywritten material is fair use so there was a
*  case where google was sued for copying all this this text and written material in order to index
*  it or in the google books project when they were photocopying all these books and ingesting them
*  in order to index all the content in these books and that was considered fair use because the two
*  main problems of fair use that seemed to be coming up and relevant are it was transformative
*  they were taking this material and using it in such a different way than just republishing it
*  that was considered you know transformative which is one of the fair use defenses
*  but it was also considered another prong that they consider is how much this use of the material
*  affects the market for the underlying copyright and material so for google indexing one of the
*  considerations is listen they're copying all this material but they're just indexing it making it
*  easier to find and it's not hurting the demand for the blog post or the the books the articles
*  the kind of underlying text that the copyright was meant to incentivize the creation of and protect
*  and it does seem like with these models stable diffusion and with mid-journey that that you
*  could really see that affecting the market for for using artist images or for using getty images
*  so there may be a different result this time her conclusions were that she didn't think the output
*  to the models were likely to be deemed derivative works even she showed pictures pretty similar
*  image generated by stable diffusion model i think it's from yeah from stability ai next to
*  a training image that was very similar to it and she even thought that no court would
*  conclude that that was a derivative work and she thinks the removing alteration of copyright
*  management information won't necessarily pan out but she does think that there may be questions
*  in the training of the model and that you're copying these works and using it to create a
*  system which can just replace a lot of the the underlying creators but that is really up in the
*  air and that maybe congress will weigh in and come up with new legislation because it's so new it
*  seems like crazy you're trying to apply these old laws and old rules to such a a different paradigm
*  and i will say one other thing that was interesting is the copyright office put out
*  this notice of inquiry and some writers have been mentioning have been asking other writers to
*  respond to it and they it's 34 very detailed thoughtful questions and i was pretty impressed
*  by how thoughtful all the questions that the copyright office put out it was just very clear
*  and it seemed like it addressed most of the important issues a couple of the interesting
*  ones were it was asking whether it's also for the the people building the models to comment on the
*  copyright and for the tech industry to comment as well it asked whether it's possible to unlearn
*  training material without retraining which makes me think that they're trying to get ahead of
*  depending on how courts rule how possible is it to mitigate the infringement without you know
*  destroying a ton of investment that's already happened there's a sections on do you think
*  data sets should be retained and recorded so that court cases can investigate to what extent are
*  the outputs derivative of these data sets and it's just so it's possible for people to investigate
*  how much copyright material is in the data sets yeah just a lot of questions are new laws needed
*  which seems like prop probably yes seems likely yes that's kind of my big takeaway i think from
*  everything that you're saying there is a lot of interesting stuff so i'm struck by just how the
*  ai phenomenon broadly is kind of scrambling a lot of things and just showing really the need for
*  kind of new paradigms in so many different areas it does seem pretty clear to me that generative ai
*  you know writ large is both transformative and derivative at the same time and so it seems like
*  it kind of triggers the problem you know on the copyright side and also perhaps the defense now
*  that's not a legal opinion but in terms of a everyday person understanding of what those
*  words mean and you know a pretty decent understanding of like how the systems are actually
*  built and work you know it does seem to me like both of those are are really pretty obviously in
*  play and yet also i'm not even sure they're quite the central questions you know i mean
*  there was a recent anthropic paper about trying to figure out what elements of the training data
*  most influence a model's behavior you know much later downstream and the the kind of core of course
*  you know this also speaks a little bit to your question of like you know could you unlearn stuff
*  without retraining you know basically not really right now that's not certainly not a solved
*  problem right so they the kind of purest way to say well how would a model behave different if this
*  one data point wasn't in the training set you'd have to like in theory retrain minus one data point
*  for every data point you know it's totally infeasible that motivated them to come up with a
*  approximation that uses some advanced math which i really don't understand but which kind of allows
*  them to get the bulk of the understanding with infinitesimally you know less compute needed
*  but then the results are still kind of like really weird hey we'll continue our interview in a moment
*  after a word from our sponsors if you're a startup founder or executive running a growing business
*  you know that as you scale your systems break down and the cracks start to show if this resonates
*  with you there are three numbers you need to know 36,000 25 and one 36,000 that's the number of
*  businesses which have upgraded to net suite by oracle net suite is the number one cloud financial
*  system streamline accounting financial management inventory hr and more 25 net suite turns 25 this
*  year that's 25 years of helping businesses do more with less close their books in days not weeks and
*  drive down costs one because your business is one of a kind so you get a customized solution for all
*  your kpis in one efficient system with one source of truth manage risk get reliable forecasts and
*  improve margins everything you need all in one place right now download net suite's popular kpi
*  checklist designed to give you consistently excellent performance absolutely free and net
*  suite.com cognitive that's net suite.com cognitive to get your own kpi checklist
*  net suite.com cognitive omni-key uses generative ai to enable you to launch hundreds of thousands
*  of ad iterations that actually work customized across all platforms with a click of a button
*  i believe in omni-key so much that i invested in it and i recommend you use it to use cog grab to
*  get a 10 discount you know you kind of see for example that like if the model is small
*  then the parts of the training data set that most influence behavior were like very literal matches
*  you know like the same keywords and then as you go bigger and there's like this higher level
*  understanding that starts to develop then it's like much more conceptual stuff but it still
*  seemed to me at least in some cases and i mean i would need to study this in more detail maybe
*  we can get somebody philanthropic on the horn here to help us get a little more clarity but it still
*  seemed to me that like it was kind of idiosyncratic and while it was more like conceptually relevant
*  stuff it seemed like there were a kind of surprisingly small number of data points that
*  had kind of still like a large impact and it didn't seem like they were like the obvious ones that
*  should have had or like exactly what's going on here still seemed a little weird like the trend of
*  keyword matching at small scale and conceptual matching at higher scale was clear but then when
*  you zoom in you're like well why is it that conceptual passage instead of you know probably
*  a ton of other things you know that could have been in there so all just like very weird very
*  hard to figure out seems like it kind of you know even if you were to say we had this answer but it
*  now all of a sudden oh geez it still varies across model sizes and you know there still seems like
*  there's some weirdness in here it doesn't necessarily feel like super just you know that
*  like this passage is being identified as the one you know that's like the most influential there
*  you could have obviously multiple ones too but yeah it just seems kind of all strange i guess
*  going back to the beginning on just the proposal and kind of the state of negotiations broadly
*  speaking i've been pretty positively surprised by how good faith a lot of people have kind of
*  approached the whole ai potential for disruption with for example in medicine you know i've
*  repeatedly said it seems like the medical establishment is a lot more receptive
*  to trying to get the value from ai that they should be able to get over the next few years as opposed to
*  digging in now and you know saying it can never you know do anything important or whatever
*  it almost sounds like at the high level maybe the studios are taking a somewhat similar approach
*  like they're they're at least being reasonably conciliatory and saying like hey we we kind of
*  get you like to write you out of these deals and have you know credit ai instead we were okay
*  agreeing to not do that is that basically the state of play there now so i don't have insight
*  into the actual negotiation but i think it was positive to see that in their offer they they were
*  at least claiming that they were trying to block out all those loopholes and to claim they weren't
*  they weren't going to try to use the models to to steal credit and to nickel and dime
*  and give lower compensation and stuff based on based on the model generating a pass of the material
*  i do think i mean one way that the other demand intersects with the copyright issue is
*  i do think the union wants some some movement on not using our scripts as training data to improve
*  the models the reason that that is separate from the copyright is that usually when say a script is
*  purchased or or or you're hired to write a script it's a work for hire which means that the studios
*  own the copyrights so a lot of the discussion is they own the copyrights to all this material can
*  they use it whatever is found you know in the broader kind of legal cases that are that seem
*  to be affecting these companies can they use their material which they own but which we've written
*  to train models that could one day replace us i think it seems like there's still not agreement
*  on that but you know i don't have insight into the the behind the scenes what's going on that may be
*  another instance of threshold effects being super important too because again i can imagine that in
*  the medical setting you know hey we're all overworked here this is really tough if you can you know help
*  me write more empathetic you know follow-up notes to patients then i'm all for it right that's great
*  if i can get a second set of eyes and you know double check me in all sorts of different ways
*  and that can you know avoid oversight that improves patient outcomes great all of a sudden you know
*  what happens if it's like google now has a doctor on your phone you know it can take you can see the
*  pictures you know it can hear your voice it can basically you know service that frontline doctor
*  independently that may flip how people start to see it and here it almost is like the set you can
*  see a same possibility where they may say okay well we're we really don't know what's going to
*  happen let's take kind of a you know conciliatory approach for the time being let's get you know
*  let's not let this derail this contract for too much longer obviously there's streaming i'm sure
*  is you know we're in revenue splits remain a big issue too but let's kind of get put this one behind
*  us but maybe we don't want to concede yet that we can't do these things because if it really does
*  flip to the point where we could just fine-tune models and have like infinite scripts for free
*  you know we don't want to you know cut off that opportunity for ourselves as the studios before
*  you know we even know if it's a real possibility i could see that you know these threshold effects
*  sometimes they kind of cut sharply in in different ways at different times it seems like
*  yeah totally and i think to that i think a lot of the protections the guild is asking for
*  are most relevant in sort of a medium progress scenario like there's a low progress scenario
*  where the current models really aren't good enough to to have a huge impact and then there's a world
*  where they get so much better that it's sort of a quixotic effort to try and stop them from being
*  used i mean i think a lot of things right now are kind of playing to the middle scenario i just have
*  been working on a charity evaluation project with a focus on ai safety and probably we'll talk a
*  little bit more about that in the future once it's all wrapped up but that's kind of a trend you know
*  that people are like this could get out of hand really fast and if so nobody really knows what
*  to do about it you know that like there's there's definitely a risk now that scaling just continues
*  or that there's another algorithmic breakthrough or you know somebody kind of figures out some
*  agent fundamental concept that you know unlocks a whole new level of sustained you know goal
*  directedness in in autonomous agents and again like just nobody really has much of a plan for
*  that it's like basically we hope that doesn't happen and in the meantime you know we'll try
*  to kind of make an impact on the scenarios that seem like you know more steerable so in that sense
*  i think honestly the the strategy that the guild is playing there seems quite reasonable and and
*  oddly consistent with strategies that i'm seeing in like ai safety charity work which you know would
*  seemingly be quite different but does seem to be premised on a similar sort of distribution of
*  range of possibilities yeah so i mean to that like and how quickly you think progress could
*  be happening in terms of how good these models can get it it was specifically writing stories
*  coming up with outlines or writing scenes and scripts i was sort of curious like how much
*  juice do you think there is in fine tuning these models specifically on trying to write
*  yeah it's a really good question one is with waymark like you know in some sense the closest
*  thing i've been involved with to you know writing a for example a sitcom or drama you know screenplay
*  would be writing the 32nd commercial scripts that we generate with language models with waymark
*  that is obviously much simpler problem but what we've seen there for sure is that fine tuning
*  helps fine tuning of the original da vinci model was night and day better than you know trying to
*  kind of do a few shot thing with the da vinci model you know there's a a pretty deep continuum
*  it seems between few shot learning and fine tuning where at least at some scale there's evidence that
*  the models implement gradient descent in the weights so like the the mechanism that they're
*  using to learn from the few shot is very similar in some sense to the i don't know if it's quite
*  you know isomorphic is probably a little too strong but there's like a very deep kind of
*  correspondence between the way that the few examples are influencing the output and the way
*  that fine tuning on examples is influencing output one of my papers of the the year last year was the
*  a paper where they basically designed an algorithm in the weights to implement gradient descent and
*  then went looking for it in actual trained models and found it and we're like look at this you know
*  we predicted that this might be there as a mechanism and sure enough you know here it is
*  in the wild anyway so i kind of see this as a continuum is the key point there between a few
*  examples and like low scale fine tuning oh it's interesting so you're saying what if you give it
*  a few shot examples just in context people have gone in and watched that the change in the
*  activations is equivalent to having done gradient descent on those yeah there's a gradient descent
*  i don't know quite how generalized this is this was published last year and we can put a link to
*  the paper in the show notes but you know i would assume that it's basically the same thing must be
*  happening in the frontier models if anything i would be kind of surprised you know just thinking
*  back to what models were available at the time i don't recall exactly which you know models they
*  looked at but they certainly didn't have access to your gpt4s or your 3.5s or your clods so it was
*  or even your llama you know llama 2 at that time so it was definitely something significantly smaller
*  than where the frontier is today so to see that it happened at you know something that's probably
*  only one percent as big as the current frontier models you know in terms of compute flops or
*  whatever definitely suggests that it's it's probably a pretty general phenomenon so yeah
*  it's an incredible result all that just to say though that like there's you know basically i
*  think a pretty fundamental sense in which it's a continuum you know few shot examples in runtime
*  versus fine tuning on a few shot or you know low scale fine tuning basically seems to be doing
*  something very similar and it definitely adds value you know in our commercial writing use case
*  it's not necessarily you know in general this this whole thing this was another
*  really interesting grant from the charity process where you know somebody made this point that all
*  the current systems pretty much have a fundamental assumption that the future is the same as the
*  present you know that they're trying to predict the data set and so you know that there's a lot
*  of ways you can kind of throw them off when you go out of distribution or whatever but one just
*  fundamental assumption that's baked in is like the future is kind of the same as the past and that
*  you know the overall landscape isn't shifting too much so i think you kind of see that in the
*  results like i wouldn't say the results that we get with fine tuning are like mind-blowingly awesome
*  but they are very helpful in terms of dialing in form and structure you know and and just if we
*  have scenes in a certain template that have kind of a one-two punch then like when it sees some of
*  those it starts to deliver the one-two punch whereas if you didn't have as many examples then
*  it might just kind of go you know something and something right and kind of be more boring
*  so i do think for structure you know for kind of if you're like oh this thing is like kind of
*  coming up with okay ideas but you know they're not the right length or this isn't really how
*  things are typically done we need to really dial in consistency of structure so that we can then
*  at least have something that is plausibly going to work and we can decide if we like it
*  definitely very helpful for that no doubt the other data point that i have is trying to have
*  it write intros for this show and there i basically always end up rewriting them
*  i'm currently using claw two for that i take five examples of previous intros
*  plus the transcript of the current episode plus a like runtime kind of angle i call it an angle
*  which is like you know do i have kind of a spin or an angle or something that i have in mind
*  to direct the model on this particular episode and so those are the three inputs half a dozen
*  quality examples from the past transcript and spin angle you know kind of runtime instruction
*  and that can be useful but it's not great you know i i aspire to provide something in the first
*  couple minutes that's like meaningful synthesis or like you you know a useful perspective that
*  kind of contextualizes what's to follow and i usually don't get anything that feels really
*  like that to me what it can do is it can like give me a decent outline of what we talked about
*  you know it can kind of give a neutral like narrator voice you know they talk about this
*  and they talk about this it can do that pretty well but if i want to have a something i would
*  sign on you know nathan's take on this i don't get that so i suspect that it's probably still
*  in kind of a similar i don't know you know it's interesting depending on what you're doing right
*  comedy could fall maybe somewhere in between where it's like maybe it's like a little bit
*  where it's like maybe there are enough jokes out there that it can like write a bunch of stuff and
*  like one in a hundred could be cool you know i don't think you're going to get to a high rate
*  of success but i'm not sure it's a high rate of success even you know by the human writers
*  so you know i don't know what um percentage right of like jokes in a writer's room or like
*  you know pilots or whatever are ultimately like good i would guess that a you know even the best
*  like fine-tuned models that you could come up with today would be kind of not hitting at a super
*  high rate but maybe high enough where it could be viable i mean you still have to do you still have
*  a filtering problem right like how do you identify you know okay now i have 100 scripts maybe one of
*  them on average is going to be good which one is good you know you that's not necessarily super
*  easy either and there's a ton of disagreement i mean that's that's another thing i always really
*  see in the you know again it could be prompt developing with just few shot or it could be
*  moving more toward fine-tuning but disagreement is a real struggle for this kind of thing you
*  know the rlhf is kind of taking us toward a kind of neutral style in some sense that
*  you know if you probably want to be a little more opinionated in your script writing i think we
*  took this a little bit last time right you know a bimodal distribution is kind of probably the best
*  you could do in in the case of creative work right you're not going to get everybody to love it but
*  if you can get some people to really love it you're in business the current ai training regimes do
*  not push toward that bimodal as much they're more kind of converging toward a center so i think it
*  would be interesting i guess to summarize all that it's i'm thinking about it all out loud here but
*  you know in our commercial writing we want like pretty good but we're not really looking for a
*  provocative you know a lot of times we're trying to introduce a small business we want to get a few
*  key points across you know it can do that pretty well but it's not like we need to blow anybody's
*  mind right we need to introduce a small business ideally you know with a good hook and make it kind
*  of attention grabbing but like again it's not totally novel groundbreaking material and it's
*  not totally novel groundbreaking material i aim for you know some at least somewhat groundbreaking
*  insight in my interest of the show and i don't really get that from ai and that you know the
*  frequency almost ever basically like super low you know i don't even run it enough to see those
*  examples i more just kind of run it and then end up rewriting it myself and then in between it'd
*  be interesting to to try to imagine or you try to maybe figure out like for something like a
*  sitcom script what positive rate of success would you need for it to be something that would
*  actually be useful you know if it's one in a million it's obviously well forget it right we're
*  not going to be able to find the one in a million if it's one in ten you could probably eyeball
*  through them and be like oh this is the one that's best you know maybe this will have legs somewhere
*  in between maybe there's another threshold effect where it actually makes sense to like
*  generate and mine the generated content as opposed to you know it's very different workflow that's
*  again another trend right a lot of these workflows end up being very different where it's like
*  i might have to generate and mine i wouldn't i wouldn't generate it maybe hollywood you know
*  collectively is maybe generating and mining but as an individual you wouldn't but maybe now as an
*  individual you know with ai at some level of success maybe you could for jokes you definitely
*  do generate and mine the show new girl was sort of famous for for it there's a concept called alts
*  which are alternate jokes that you have on set so if a joke isn't working or you just want to try
*  something different you have a list of alts for each joke you might want to to switch up and you
*  know new girl would have just the writer would have a binder of like hundreds of jokes for every
*  episode and then you would try a few and then in the edit you would you know mine your favorites
*  i do think there might i mean the idea of doing whole scripts and mining them sounds exhausting
*  but if you chunk it into kind of hierarchies and you have it do premises and then okay 20 premises
*  you can look through and then you have it do outlines and you can read through 10 outlines
*  and then you can kind of go scene by scene and have it generate scenes it's still you know i
*  don't think it's there but at some level of hit rate i could see some of that stuff making more
*  sense and then to what you're saying about kind of wanting more polarizing material you know
*  something that you want 10 or 20 people to love it versus 70 to be vaguely not offended by it
*  i'm guessing the answer is no but when you have say you're fine tuning on i guess now you can do
*  it on gpt 3.5 they're not exposing enough of the model to do reinforcement your own reinforcement
*  learning on it not yet although i do think that is coming they have kind of teased more sophisticated
*  fine tuning tools and i think that some of those are in you know kind of alpha or beta
*  phase with select customers but the current fine tuning that's available is still just kind of
*  examples you know supervised fine tuning you send them the examples they do all the fine tuning on
*  their end yeah got it because it does seem like it really is trained or is rewarded for being
*  inoffensive and there is like a pull toward the mean and you could probably get improvement
*  training it where you have a really polarized reward structure where you want people to love it
*  and it's less penalized if just you know half the people don't respond to it and then i think i was
*  mentioning this to you offline but i also think it struggles a bit because you're generally sampling
*  from a fixed temperature and a bunch of different levels of writing whether it's a premise or a scene
*  or a joke there's a concept that comes up a lot called the one unusual thing where you want one
*  element to be really surprising and unpredictable and then you want everything else to play it
*  as as straight and as normal as possible that's why you know usually a joke there's one part that's
*  very surprising and the rest is very normal or like an example just i don't know why this movie
*  popped in my head but the movie the invention of lying it's a movie where lying doesn't exist
*  and that's a very unusual thing but within that world all the people behave psychologically normally
*  and although you know everything every other part of it is is a natural consequence of the one
*  unusual thing played out as realistically i guess as possible maybe there's something to train them
*  to learn to select their own temperature for different outputs so it knows it doesn't have
*  to be the same level of randomness throughout every tokens generating it could learn okay i
*  need something more surprising here okay i've picked something really unlikely now i need to
*  land the plane and try to make sense of this random thing i've just thrown into my generation and
*  there's a method of writing that you know different people use or sometimes where
*  you write yourself into a hole where you can't imagine how you get out of it and then you try
*  to just like solve find some way to solve the problem and make sense of this crazy thing that's
*  happened like i need the cohen brothers do it where i don't know if they do it for every script
*  but i've read that they trade off every 15 pages and one brother will try to write into an impossible
*  situation that they can't imagine how they would get out of it and then the other brother has to
*  show up make sense of this crazy thing they've been handed and in 15 pages then steer their
*  brother into an impossible situation or like in the movie jango and chained you know it opens
*  christopher waltz's character goes into a bar and shoots a guy and you're just like what is happening
*  how is he going to get out of this and then you eventually learn that there was oh you know a
*  bounty on his head he was a an outlaw on the run and you find the reason why that makes sense
*  i do think there is there may be value in like the way the models can make sense of something or the
*  way they can hallucinate some reason why something is true when they've made a mistake makes me think
*  if you were generating unusual situations and using the model to then write out of it that it
*  might be better at that sort of create creativity than the sort of creativity where it's has to
*  generate that unusual thing on its own number of uh interesting threads there to to pull on a little
*  bit so we talked about in learning briefly you know and you kind of mentioned like specific names
*  here right so the one thing that people have a lot of success doing honestly i probably have the most
*  success doing this if i'm trying to get something creative in a style that's like not coming
*  supernaturally to me is invoking particular authors you know particular masters of whatever
*  the the style is so that could be you know right in the style of hemmingway or if i'm doing a you
*  know and a commercial writing in the style of david oglevey or whatever now hemmingway is old
*  enough uh he either now or soon should be in the public domain although they seem to keep extending
*  that but obviously you know the folks you're talking about all their work is like owned
*  material i don't think it's going to be easy at all for large models to untrain that stuff
*  the next generation because it's all just you know mashed in there in insane spaghetti form right and
*  like maybe you can tell what you know document influence what in the case you know using that
*  entropic paradigm if you invoke you know the cohen brothers whatever you're then going to
*  presumably see very obviously relevant material being the you know the data points that influence
*  the behavior most so presumably you'd have some clarity there at least of like okay yeah there's
*  there's a pretty clear chain of like how this is happening from the fact that you know the name was
*  invoked into what data points which are their data you know and it's at all that's all kind of seems
*  like it it could add up honestly what i would expect people might have to do if they get a
*  uh forcing function put on them would be some sort of filter on top of the main model like they'd
*  have to move to a system approach where they would say okay you know and we're seeing this
*  another again in ai safety the some of this work is is happening with like biosecurity risks so
*  you know entropic had a little testimony i believe in the senate not too long ago where they kind of
*  talked about this and it was like today the best language models can't you know engineer a new
*  pandemic but they can help people get over steps that are not super obvious how to get over you
*  know there are things that are hard to google that are kind of you know the know-how that exists
*  in labs and actually the models do have some of that or you know can help you kind of get it where
*  you it's not like super searchable so what do we do well they're developing a filter a classifier
*  basically to put on top to say and you could do this at the prompt level or the upper level or both
*  is this something that seems problematic you know even kind of independently right have just have a
*  very different purpose for that additional system or that additional part of the system that is
*  supposed to kind of catch those things and i think you could do something like that for copyright as
*  well you know not necessarily hey write me a joke well geez you know that's all comedians
*  that's maybe tough but write me a joke in the style of sarah silverman you know that you could
*  probably catch and say like maybe it's just sorry we can't do that and it's not you know it could
*  be as simple as that but certainly when names are named you could identify those cases and
*  do something different even if you can't like pull that information out of the model so the
*  hierarchy thing is interesting did you see this this company called pseudo right that i think it
*  was just a little bit after the last episode we did this guy's name is james you on twitter i've
*  invited him to come on the show haven't he's taking me up on it just yet but an ai tool for
*  writing long-form stories is the promise and the demo and the interface is very hierarchical you
*  know it's kind of a cascading like what will you you know what's the premise and it'll generate
*  at every level of the hierarchy but you can also then edit you know different levels of the
*  hierarchy and have it kind of regenerate and cascade your changes down ultimately you can kind
*  of create book length things and the value of this tool is it provides the scaffolding to help you
*  kind of project you know well what is the main line story and who are the characters you project
*  that down into all the little things you know efficiently so you can you know not lose yourself
*  from all the prompts i don't know how well it works it seems like it probably is quite valuable
*  relative to just sitting down with chat gbt by yourself and trying to write a novel
*  but people went nuts on this guy and just you know especially in the context of the writer
*  strike were like how dare you became like public enemy number one it seemed like of the the writers
*  and it was unclear of course on online like how many of the people hating on them are actual writers
*  you know versus like purely aspiring writers or people that you know think they're defending the
*  writers but i don't know if you saw that or you know if you if any of these new tools have crossed
*  your radar i didn't see pseudo right i know there i've noticed there's some outrages you know it's
*  hard at the level of a script or a novel it's so much it's obviously longer than the context
*  window but i even think when you get to the extremes of the context window story wise it
*  can be hard playing around with it to keep the coherency and i do think you would you almost
*  need these sort of like hierarchical scaffoldings to enforce that coherency where it's not in one
*  generation it's not producing a max length coherent story you're getting a much shorter
*  coherent outline and using that to generate coherent scenes that when you manually put them
*  together will be coherent yeah even with clawed 100k now which i think also came out since the last
*  recording and definitely has changed the game for you know long-form document processing for
*  starters and and probably a lot more beyond that to come to with just super long context windows
*  in general but even with that 100k as it stands today you do see performance degradation at the
*  high end of the of the context window if i take a 90 minute podcast transcript and ask it to
*  you know create a timestamp outline or summarize that does a pretty good job if it's closer to
*  three hours that will still fit technically in the context window but it's getting close to the
*  limit and then somehow i see it go off the rails pretty consistently like skips whole sections you
*  know just kind of kind of fails in a way that's like that's not just you know that i might quibble
*  with bits of the summary but like you actually missed whole parts you know you you you did
*  meaningfully like objectively fail at the task so of course that stuff will continue to get better
*  but as it stands right now you can kind of get a quick summary of the gatsby but i wouldn't
*  expect you know it to get every question right about the great gatsby based on what i've seen
*  i have a question about that um i was wondering because i was listening i listened to the mosaic
*  ml episode and i was curious how when they train on one context length and then they allow you to
*  input longer context and they have this variable length how they project the larger input down to
*  the same vocab size and they seem to keep answering with well you know they were talking
*  about how you copy the attention heads those are all the same weights and they talked a lot about
*  their positional encoding scheme which i guess which was the main point of their paper but it
*  still seems like once you get past the attention heads you've trained for weights that project
*  you know a smaller input onto some vocab size and now you have a larger input i was i was
*  wondering one if you ever got to the bottom of that and is it are they just using like
*  pooling layers where they're kind of you know max pooling or average pooling and so once you get to
*  you know 100k on clod if they're doing something similar that just like the specifics get lost
*  in these pulling pulling layers and so that's kind of why you're seeing the the performance
*  degrade yeah i don't have a i still don't have a super clear understanding of that i recall that
*  they said you know ultimately you're sharing the weights right there's there's is some sort of
*  dilation type of phenomenon but the core kind of model size is the same of course and so there
*  is some sort of sharing i don't still have like super clear sense for how that is supposed to
*  work i guess my general sense of it right now is the original trigonometric function
*  positional embeddings definitely look insane you know it's like that is so weird that that would
*  work at all and you know definitely has the feel of something that somebody kind of
*  left there as a placeholder because it like sort of worked and they were moving on to the next
*  thing for the time being so i'm i'm not at all surprised that you know we can come up with
*  better stuff and it feels like this is not the end of that story either in all likelihood i i don't
*  yeah i don't fully quite get it but it is interesting i haven't i guess i haven't fully
*  characterized it behaviorally either because you know i do see these weird failures at like a three
*  hour transcript level but you know it does work pretty well too it's like initially people were
*  like this is a superpower it's definitely not quite a superpower in that it doesn't seem to be
*  unlocking any increased reasoning ability like the the raw sort of power power you know if you
*  make an analogy to raw g or something it's not stronger in that way it does seem to be like
*  it does seem to be like you know if you could really separate you know how good are you at
*  processing information versus like how much information can you hold in working memory it
*  seems like this is a pretty clean separation of those concepts so for intuition like it can hold
*  more in working memory but it doesn't seem to be able to do that much more with it you know it's
*  not better at solving problems as far as i can tell i haven't you know it does not seem for example
*  either that like if you were to give it lots of examples versus you know a decent number of examples
*  that it's like there seems to be definite diminishing returns and examples but yeah i
*  don't know i guess i i still sort of think something else is probably coming there there's a few other
*  things that i've been really interested in over the last couple months around just possible
*  transformer successors or elaborations one big one seems to be some form of
*  recurrence being introduced to the transformer because the main thing there is you can kind of
*  selectively bring forward the information that matters most without having to keep track of
*  everything you know at the same time always which is you know basically what the transformer does is
*  everything that's in scope is kind of you know fully treated and i guess the projection of the
*  alibi mechanism is kind of maybe a little bit of a fudge on that because now you know you're able
*  to bring more stuff into the same number of weights but a mechanism that allows you to
*  selectively bring information forward with some sort of recurrence type thing i think is maybe
*  another thing to look for in terms of how that you know the crowd the current limitation you know
*  maybe just gets blown through the the paper on that was retnet it was out of microsoft actually
*  really interesting a microsoft china collaboration and i always have been inclined to celebrate those
*  well it's like resnets for microsoft china right i wasn't following the primary uh you know literature
*  closely enough at that time to know who was driving things these days you know it seems like man
*  uh us china collaboration is uh increasingly an endangered species but this one paper was like
*  you know potentially part of a very uh proud tradition in any case it stood out to me as like
*  wow this seems like potentially really transformative work and to see that it was
*  us china collaboration i thought was pretty uh pretty cool i personally you know i'm all for
*  turning down the temperature and uh up the the collaboration wherever we can that was pretty
*  cool one real practical thing just and maybe this one arguably should have been first if you want
*  to do something to get out of the kind of centrist mode they are helping you there now quite a bit
*  with the system prompt so you know go into your chat gpt account go set up your like perma profile
*  and tell it what you want and tell it you know i don't want to be you know the the same normal
*  everyday thing you know i i'm open to provocative results or whatever and that will help push the
*  boundaries it's kind of unclear how far that will go but it definitely does help expertise is
*  rewarded right so when you can bring a paradigm like you described of one you know strange thing
*  that is something that the model probably can follow but won't necessarily follow unless it's
*  directed to so you need the the know-how of just like you know what are some approaches that people
*  take that you know have that proved to be you know useful and just articulating that to to the model
*  in the first place can really be helpful again i don't know that it will be great in today's world
*  but it definitely could push you a little bit farther in that direction and another research
*  result that that that made me think of too is the backspace paper this is maybe a little bit like
*  the opposite perhaps of the backspace paper but in the backspace the paper that i call the backspace
*  paper which i believe is out of stanford they do two things there one is they introduce a new loss
*  function that punishes whole generations that are out of distribution more as opposed to the
*  the vanilla you know next token predictor that kind of you know has been the norm so they take
*  a more episodic approach and as you get like far out of the norm the reward function can you know
*  can punish that more and then this backspace idea is like well as that starts to happen then maybe
*  the model can backtrack and try again you know if it realizes that hey this last token was kind of
*  taking me off in a different you know strange direction now and that's too anthropomorphic but
*  you know the effect seems to be that basically if there's low you know you make one bad or unlikely
*  token prediction and then you have like no good options in the next prediction previous
*  architectures you know previous approaches you're just stuck you just have to pick something and you
*  know that's that like once you kind of take that path you're on that path but the new with the
*  backspace added if nothing else seems likely then the backspace becomes the option where it can go
*  like okay boom we'll go back and i believe that the actual context now just has the token and then
*  backspace and then it's learned to just kind of pick up from two tokens back or whatever but that
*  will allow it to kind of go back get back in distribution with a different option that you
*  know then hopefully we'll have like more likely or natural continuations and so you could imagine
*  like flipping the sign on that somehow and i haven't studied the math enough to know exactly
*  how the loss function is constructed differently but you can kind of imagine that like if you can
*  make one that punishes things that are being out of distribution you might be able to and have a
*  backspace that kind of allows it to correct you might also be able to kind of do the inverse of
*  that and reward things that are more out of distribution and maybe have sort of a you know
*  not a backspace but a like opportunity to sort of flag that like this is a good moment you know to
*  introduce something very different or unexpected a lot of work you know left between my speculation
*  and like that actually working obviously but i could see something like that starting to happen
*  and i think that's also really relevant in a lot of cases you know people are talking
*  always are like interested in when might models be able to make a meaningful contribution to science
*  and science you know in some ways has a very similar problem to creative writing in that
*  the most valuable stuff by definition is kind of out of distribution if you are you know generating
*  scripts or hypotheses that are very consistent with what has come before it's unlikely to be
*  a breakthrough hit and it's unlikely to you know change the scientific paradigm you need to be
*  somewhat out of distribution to have a meaningful impact but you need to be out of distribution
*  obviously in a smart way or it doesn't work interestingly that a similar technique might
*  end up working for both yeah because it seems hard because you know most things that are very
*  unlikely in your distribution are just like gibberish and bad but it seems interesting
*  that they're evaluating their outputs at larger than just each token itself i mean it does seem
*  to me that like a little bit is as you do more reinforcement learning it's less out trying to
*  predict a likely token and more trying to predict a great token which i do wonder if getting better
*  writing it it might just take actually rewarding the behavior you want for some yeah like rl fine
*  tuning regime yeah i think in general we're just still really early in the paradigm you know
*  whether it proves to be an exponential curve or you know perhaps more likely some sort of s curve
*  we're definitely in the steep part of the s curve and this first wave of large language models
*  was really a kind of mad science project in search of you know a capability which then kind of went
*  in search of a problem and you know the first versions of it were like not really useful for
*  anything then you know they kind of said but you know if we make it way more powerful it'll probably
*  start to be useful for something and so you know they did that and then it actually does start to
*  become useful for things still at that point they weren't really trying to solve a problem
*  so now we have this kind of different situation where okay we've got base things that can do a
*  lot of things pretty well but the properties that we want in a model that might power an
*  autonomous agent that's supposed to like do online tasks for you may be very different from the you
*  know the qualities we want in a script writer or a hypothesis generator and you know we may want a
*  an approach that punishes two random things and it has a backspace for the agents and we may want a
*  you know kind of whatever the mirror image of that is that like rewards more out of distribution
*  things or you know kind of identifies when a good moment would be to to take a unusual step
*  for those other domains i think it's just like there's so much room you know and the and the
*  world you know among other things the number of people working on this is going exponential so
*  we may be bounded by how many humans we can put at it where we may just find we can like
*  exponential our way with just more and more ai's you know focused on it and that's in some ways like
*  the big question but either way it does seem like we're now hitting a phase where knowing that it is
*  possible to build something that is useful for a particular task people are really only you know
*  just kind of now starting to say well how how would i change the approach if i wanted to solve
*  that particular task and it is turning out shouldn't be surprising really but like a coding
*  system is going to be different from you know different kind of system so we covered a lot of
*  ground there let's get back to the actors yes this is what people really want to hear about
*  yeah so the actors have also gone on strike for similar wanting to get you know a larger cut
*  of streaming residual of the streaming revenue and wanting pay increases and artificial
*  intelligence protections so it's you know very similar the two big paradigm shifts of there is
*  now these generative ai capabilities and the business model of the industry is changing i
*  think are also driving obviously driving this strike i figure maybe just a helpful thing is
*  give just a quick overview of what the actors are fighting for on the ai front it mostly seems to
*  come around to they want to own their digital likeness one thing that i think scared people is
*  that you can be a a background actor and you're showing up for you know a relatively low wage for
*  a day's work and they're requiring you to get a full body scan and the fear is okay you get paid
*  for one day's work they scan your body and then they use your likeness for whatever other projects
*  in perpetuity that they want to do the guild wanted to if the studios do take your digital
*  likeness that you still own it and you get to negotiate consent and compensation for any future
*  use of it sort of what i think the situation is that the studios agreed to give you that consent
*  and compensation for your likeness to be altered or recreated for a future use but not for the image
*  to be used for training for example to create synthetic characters that don't look or talk like
*  any anything like any specific person sort of similar to the writer's situation of just taking
*  scripts and generating new material based on it so i think that they are still fighting for that
*  i think they're also demanding the union wants consent over individual ai uses i guess they want
*  some sort of say going forward if there are unique situations that come up they want to be consulted
*  and have a say in it and then i think there is a dispute over whether or not they've given in on the
*  background actors issue the studios claim they agreed that they would only use scanned background
*  actors for that production so they do a day's work and they can put them in others i don't know other
*  situations or they can fix something if they didn't like how it was shot when they struck the union
*  claim that they they were not did not agree to that i do think there it's like a very um just
*  persuasive gettable thing of you want to have some control and leverage over your actual
*  personal likeness and not have it taken from you and then used in ways you know you don't know
*  going forward i do wonder if the actors end up in the medium to long term coming up against just
*  fully as they were are worried about synthetic digital characters and actors you know like
*  computer generated characters have sort of been a mainstay and have been a common thing we are
*  already very familiar with and already see so it may be harder to try and cut that off entirely
*  going forward yeah somehow this one does seem a little tougher asking myself like is that just a
*  reflection of relative state of the technology or is it something that's different i think you
*  highlight a good point too about kind of where we've been right there already have been lots of
*  special effects and you know different digital ways of making things look real that are not real
*  and so you know there's a hollywood has a lot of practice with that you know whereas like there
*  was no it's not like we had a sort of worse ai that used to be able to write scripts you know
*  that where there's any precedent there's a lot of precedent for you know cgi plus plus
*  this one does feel tougher i think you're right it does seem tougher to avoid this and you know
*  if i took the charitable angle i might say an agreement that's like we'll scan you but we only
*  get to use that in this specific production until you know and unless you agree otherwise
*  that would seem like a pretty reasonable outcome for the short term and especially because you know
*  there is i guess i don't know what you know would the union have any hope of getting a sort of
*  no generated characters or no generated well especially because you just you blur the line
*  right in so many of these movies you blur the line between what is a human and what is not
*  right i mean you just think about things you know like arnold in the terminator and it's like well
*  where does that stop exactly or if we kind of synthesize the new terminator out of nothing
*  you know and there's not really an actor under that it seems like that sort of creativity is like
*  part of what hollywood does it can't really take that out of it you can't really say like every
*  role has to be played by a human like in sort of pure human form or whatever that would mean
*  it just seems very hard to draw real lines on this and the technology is getting good too you
*  know we at at waymark and i take zero credit for this we will have an episode coming up on it before
*  too long but we have the creative team at waymark you know with with the company cheering them on
*  has created a short film called the frost which is kind of a sci-fi you know ai premise
*  film in its own right made with images uh created with dolly 2 and then with motion kind of added on
*  to the dolly 2 images and you know the team there at waymark the creative team is like deep our
*  creative director is i think just recently crossed one million dolly images generated uh to give you
*  a sense of how many you know how dp's gone down this rabbit hole so yeah i mean they know that
*  the tools inside and out you know prompting experts uh they have a real ability to create
*  a coherent visual aesthetic through you know the length of a short film that like
*  is hard one know-how in and of itself now they're working on the frost 2 and starting to use uh one
*  of the runway models to do that uh from the company runway that is to generate short clips
*  and you know just from like the first part of the year when they did the frost one to now you know
*  early second half of the year we're doing frost 2 like you can just see a huge difference in how
*  compelling and natural and like real the motion seems to be and even with the frost one you know
*  it was like they have these characters that um and this is a really good episode i think
*  too so definitely listen to the full thing but they have these characters in their short film
*  which are ai generated and so i asked a question of them like how did you get these characters to
*  look the same all the time you know from scene to scene right you're using like different dolly
*  two generations right like how is it that they look the same and they said well basically there's
*  two parts to it one is use archetypes and i think that means more to them than it does to me but
*  they were kind of like we sort of find success with like certain you know descriptors that
*  do seem to be more consistent because in some way they seem to be sort of an archetype so that's one
*  you know we can kind of improve the consistency with that approach but then two they're also
*  like if you actually stop on the different frames and the different scenes you'll see that like
*  in many cases they do look meaningfully different but it's fast enough you know the edit is fast
*  and there's motion and you know there's a story going on that you're sort of immersed in
*  such that you know those differences kind of wash away like we you know there's these like
*  famous psychological results of like you can separate the you know the sight and the sound
*  of somebody clapping their hands or whatever by up to i don't know exactly what it is but you know
*  up to a half a second or so you know before people start to perceive them as being different
*  times something kind of similar here is going on where like you have sort of a an ongoing narrative
*  that everything fits into and you kind of know that it's that character and so you're not like
*  studying the the visual details with the level of specificity that you need to to start to see these
*  little differences as long as it's not too flagrant it just kind of sales right by certainly did for
*  me watching this stuff i had to ask the question i thought they had made it like way more consistent
*  they were like yeah actually it's a little bit of a cheat so anyway all that is to say i do think
*  it's going to be pretty hard to figure out like where would you draw a line on what counts like
*  what's a role that has to be played by a human what is a role that is sort of human based but
*  could be you know enhanced like certainly some things seem like they could be generated and by
*  the way this is all in the context too of like the deep fakes are hitting a threshold right now
*  and that's another thing that's happened i think these these kind of short intervals between
*  interviews are or between episodes are pretty interesting in that like we've got no shortage
*  of major news over the last three months the voices have gotten a lot better with things like
*  11 labs play ht multilingual the ability to do direction on the voices as well you know say it
*  in an angry way say it in a happy way say it in a surprised way and even some deep fake video stuff
*  which i think is like kind of just starting to tip maybe right now but some really compelling
*  examples of that on twitter flying around lately too where i've seen a couple where i'm like i
*  honestly can't tell you which is the real person and which is the fake as i'm looking at the side
*  by sides so i don't think the results are consistently that good yet but you know you look
*  back at where we're just past one year of stable diffusion you know that's another kind of short
*  time interval we're recording this right now at like stable diffusion plus like one year and five
*  days it's really only been that long so yeah i don't know i mean in the actor one it does seem
*  tough do you have any i mean it's certainly hard to speculate but do you could you attempt a
*  you know i guess with the writers like there's at least an attempt of something that could make
*  sense right we want to have the same credits the same roles ai can be a tool for the writers but
*  you know we want to kind of carve that out with the actor side it seems a lot harder
*  my just guess is the actors will both in this negotiation and in general have some success
*  owning their identity and their likeness with you know one protections in this agreement i think
*  in a lot of states you know there's a right to publicity where you're not allowed to just train
*  something to copy someone's exact voice and way of talking and then commercialize that so i think in
*  general it'll be hard to commercialize deep fakes based on people but i do think the longer term
*  protecting actors jobs from from ai generated characters and voices will be a much harder
*  battle and not even just from purely synthetic things but this might also be one of those like
*  tyler cowan averages over situations where you know meryl streep can have her digital likeness
*  scanned and then when the technology gets there star in twice or three or four times however many
*  movies she wants a year without having to be on set for all those days i could see there being a
*  norm develop among actors where they sort of it's become shameful to scan yourself and take all the
*  jobs but it does seem like the like with anything celebrity and your own brand differentiation
*  gives you some sort of lasting power and maybe an even increasing ability to take more share of the
*  value from the industry and so you might end up with a few celebrities who are getting way more
*  roles thanks to this and then it's much it becomes much harder to break in i mean you could also
*  like the number of meetings we've had whether it's youtube or then it's tick tock where your
*  agents are like we got this youtube star we want you to write something for them and it's like
*  can they act can they is this going to work at all but in the future it's in a world where this
*  the technology gets much better and you can make a reasonable acting performance
*  synthetically anyone famous could star in something in the longer longer term anyone you know this
*  people say you could put yourself in a movie anyone could be the star of their own movie
*  i think people will still want to be watching famous people they like and relate to in movies
*  versus everyone just putting their friends in their own things is more than a gimmick but now
*  we're getting into the far like just very speculative stuff yeah i mean i think the
*  where that kind of intersects with reality right now is probably gaming right it's like
*  the line there maybe gets blurry i sort of agree that like tv and movie entertainment as it's
*  you know currently sort of understood like it's easy to imagine the sort of cheap silly way that
*  you know you'd put your friends in it i mean gaming is even bigger right than like
*  than movies at this point so the the other angle of that is like people are going out and actually
*  hanging out with their real friends in real time in these sort of virtual you know worlds for
*  entertainment purposes and the dialogue that can kind of be generated there now and the just the
*  the open-ended adventure is certainly dramatically expanded you know compared to anything that we've
*  had in the past just had a really funny experience yesterday where i was over at a neighbor's house
*  in our neighborhood just you know took the kids out on a hot afternoon to see if any friends might
*  be out to play and ended up talking to this kid i think he's 10 years old you know and he's a he's
*  a real bright kid you know kind of ahead of his class and really into dnd and i just like gave him
*  my phone with the chat gpt app and was like try this you know have have the ai be your dungeon
*  master and you get to play yourself you know and uh zero shot right into the experience you know
*  the first thing it says to him is your turn he kind of says your turn i was like yeah now you
*  get to talk now you tell it what you want to do and he's like oh okay so he just took to it
*  immediately and you know we didn't see he was like quiet for half an hour just you know going
*  down this dnd you know whatever the narrative was whatever choices he's making but that world
*  is just like unfolding in front of them and you know it's dnd obviously not for everyone but it
*  does seem like there is something there that try to imagine like what's the not kind of gimmicky
*  version is there a compelling version it seems like they're very well could be that does seem
*  like especially with like your avatars and your video games those will get more and more realistic
*  and and maybe i'm totally wrong and people will just prefer to be the star and each person will
*  watch a movie where they're the star of it and that's maybe the other reason the other case maybe
*  too for the some of this generative technology or like the reason that you might need to be scanned
*  or it might not be shameful would be like if the best entertainment is ultimately like choose your
*  own adventure to some degree then you kind of have to have the generative you know and you want
*  that celebrity component to it then it becomes like well hey it's not just taking all the jobs
*  like it's kind of a new form you know without the technology there's just this experience couldn't
*  exist and if this experience is super compelling then like it's probably not shameful to help create
*  it right yeah i mean my guess is a lot of the top actors and talent will be wanting to scan themselves
*  and monetize it as best they can the hourly rate is super attractive i mean they already have a
*  high hourly rate but as it gets into the the editing applications and you don't have to do
*  reshoots or you can just fix something that was wrong in the scene and you get better than that
*  that might be a few days less an actor needs to be on a movie and they could do one or two there's
*  like this slow incremental where they can do a little bit more work each year and then the
*  the more extreme version where they can do a lot more work because they don't even have to be
*  physically present the way i've been hearing people talking about the models i feel like there
*  a lot of people aren't giving it credit for understanding some things or even sort of like
*  opening themselves to the questions of what would understanding even look like
*  like you know you think about the training task and how hard it is to predict an x word
*  and the idea that if there's underlying structure in this stuff you're trying to predict
*  that learning that structure makes you better predicting the word and maybe
*  learning the deeper underlying structure is kind of what understanding is it's a bit more
*  philosophical but then you see you know the examples in like you know nanda's paper
*  or the naftali tishbi he just had these toy models where he would go through he was able to
*  calculate the mutual information between each layer and the input and each layer in the output
*  and he would train the models and in the beginning the first layer has a lot of mutual
*  information with the input and the later layers have almost none and there's the memorization
*  phase of training where the later layers kind of learn get more of the information from the input
*  and you i guess i assume the model's learning to just pass through the inputs and then the final
*  layers are memorizing which labels go with it you have the memorization phase and then at some point
*  as it starts to generalize the mutual information between the input and the output decreases and
*  gets pushed to zero and that's when the model actually starts general generalizing outside of
*  the training data and he sort of has this theory of the the information bottleneck that the model
*  in compressing the training data gets better like the least information you have between the
*  layer layers and the training data so you're not learning you don't have all the pixels one by one
*  but you have there's a dog in it there's you know there's much more compressed higher level
*  information which just made me think it just felt like the same process you see it it's not the same
*  process they're they're kind of i think they're theorizing different things but in the onanda's
*  paper where it memorizes the training data but you have this weight decay penalty so even once
*  your training loss is at zero the model's pushing for simpler solutions and obviously there's some
*  searching through the possible functions that the model can represent and if it latches on to a
*  simpler solution the weight decay pushes it toward you know these simpler models and that seems like
*  what grokking is in that paper where the model is now modeling an underlying process that is this
*  like deeper structure to the the training data it was given and listen that the models could be
*  copying obviously they can copy the inputs because they know facts so it can regurgitate stuff
*  but to not count out the chance that the models are you know getting some understanding and like
*  the ilius suskevar example he gives of the next word prediction is that if you're reading a short
*  mystery story and you get all these clues and the story's going and then the detective declares
*  i've got it the killer is blank to guess the who the killer is it's not just the statistical
*  probabilities of every time i've seen the killer is blank you sort of have to be latching on to
*  the psychology and the clues and the cause and effect of the story like you need some
*  deeper understanding to predict who the killer is and that the next token prediction sounds
*  very prosaic but it's actually a very hard task and as i think ali azar you're kowski pointed
*  this out in a tweet and not like you ever could learn this in a language model but there probably
*  are lists on the internet of large primes and it's two and they're factors so like to predict
*  the next token in a list of large primes and factors you would have to solve it you know
*  possibly not efficiently computable problem but just that that uh the next token prediction is
*  a very rich task that can push a model towards you know different levels of understanding and
*  seeking out kind of the underlying structure and then you see in examples of like the sentiment
*  neuron where you have next token just next letter character prediction that open eye trained a model
*  on but then they found there was one activation that would give the positive or negative i think
*  it was trained on like imdb movie reviews or something that was like a classic data set
*  or they did a next pickel next pixel prediction this i gpt where they just train them out to
*  predict the next pixel images and then they took out i think they called it a linear probe i think
*  it means they just take out a higher level and then they can trade a linear classifier on that
*  to do object recognition and it got near state of the art without really trying to push it or train
*  it too long anyway i just think that like at least in terms of when the writers are thinking of
*  the future of your careers it makes sense to want to try to protect your livelihood
*  and it makes sense to feel there's an injustice if your work is taken to train something to
*  replace you but i do think you need to be pretty open-eyed with what the models are actually doing
*  and not just dismiss how how good they are or what they're doing right now especially because
*  they're going to be getting better every year i do think this is one of the more interesting debates
*  going on right now and you know a good example of it honestly just last week we published an episode
*  with page bailey who was the lead product manager on palm 2 at google and we had a you know a few
*  rounds of back and forth within that conversation on like you know what level of reasoning do you
*  think the models are achieving and she did kind of surprise me and this has kind of gone you know
*  mildly viral online with a clip that you know kind of pulled it out of context but she did kind of
*  surprise me with how little credit she gave to the models on their reasoning and i think one thing
*  that i see commonly underlying some of this confusion or kind of disconnect between people
*  who have different very very different you know understandings of what's going on there's probably
*  multiple things but one big thing i think is the importance of robustness and reliability versus
*  like how easy it is to find counter examples to various capabilities so what page said was like
*  you know there's these examples that seem to show reasoning ability and she was kind of alluding to
*  like the sparks of a gi paper and then she was saying you know but if you change the situation
*  a little bit in ways that are like not super meaningful really you know it doesn't seem like
*  it should throw it off then it can totally throw it off and it can fail and like that's definitely
*  true you know we've certainly seen those examples but i do think i come to a different conclusion
*  than she seems to i mean i can't you know we had less than an hour uh so i didn't get to ask all
*  the follow-up questions i would have liked in that interview but it did seem like she was kind of
*  saying like yeah that doesn't really count as reasoning if it's easily confused i think that's
*  kind of one view on it i would say something different that's like i do think it counts as
*  reasoning or you know shouldn't be ruled out as counting as reasoning just because it is easy to
*  find counter examples i mean for one thing from like a practical utility standpoint if you can
*  reason through common examples that's really useful you know uh so like you know and pretty
*  plainly they you know the best models today can do that no no doubt right now you could say well
*  maybe that's all just correlation i don't know it seems very implausible another funny story i had
*  from this weekend that kind of showed that was the same uh visit over to a friend's house as the
*  the 10 year old playing dnd but the father of this family's brother is a monk and lives a very you
*  know monastic life uh at a monastery like the whole bit right uh head to toe brown robe sandals
*  you know long beard doesn't fall in the news right you know like doesn't take the paper and you know
*  doesn't have the internet um you know definitely not up to speed with you know everything going on
*  with with ai interestingly he had some limited prior exposure to chat gpt like somebody visiting
*  through the monster whatever had showed them something so he wasn't totally unaware but
*  definitely not paying a lot of attention so he has to you know he's gonna well i have to you know if
*  you're if you haven't like sworn off of it entirely it's like no i can you know i'm here
*  i'm visiting i can you know see what's going on in the outside world while i'm uh on my home visit
*  so i was like well then i have to you know be the one to show you what chat gpt can do
*  so i was like so you know ask it a question you're interested in and he asked a question about you
*  know a very sort of theological uh question as you would expect about the difference between
*  essence and existence which is not something i've really studied and we went through a couple rounds
*  of it and it was like first response was okay that's fine he asked for an argument
*  in the original uh prompt you know his his question to it it gave back a summary so his
*  first comment was like well it gave a summary not really an argument but that's cool and i was like
*  please do that again but this time give an argument like i said the first time you know and so then
*  the second time it gives an argument and he's like okay that's good it's i wouldn't say that's like
*  really the best argument you know it's referring to Descartes and you know that's i think that's
*  really not the best you know that's kind of what everybody would talk about but i don't think that's
*  really the best thing to be talking about i was like well this goes back to the names thing too
*  you know is there a thinker specifically that you would say you know would be the one to refer back
*  to thomas equinas okay so we ask again you know could you give the thomas equinas take on this
*  question now he starts to get impressed he's like okay now we're getting to the real you know
*  substance of this debate that i think is most important so he was quite impressed at that point
*  and then the 10 year old kid comes in and says uh can it tell me a joke like bill cosby and i was
*  like watch this yeah no i don't think you know nobody in this group was following the news and
*  i don't think about uh bill cosby uh i didn't touch on that bit so i said okay well watch this
*  and i said to chet gpt okay now can you represent this concept we're still talking about the
*  philosophy of essence and existence in the form of a bill cosby joke and now this is clearly well
*  outside the training data right i'd say it's safe to say you know that there's never been a bill
*  cosby bit now you could still call this interpolation i wouldn't necessarily call this 100
*  reasoning but it's definitely some like meaningful conceptual understanding because what it came back
*  with was a jello based uh description of the difference between essence which is described
*  as the you know the sort of notions that you have about jello like the wiggly jigglyness of it
*  and the you know the chocolatey goodness that you you know imagine that causes you to want it in the
*  first place whereas existence is when you really eat it and you really get to experience and have
*  the actual you know sensation of enjoying the chocolatey goodness and at that point everybody
*  was just like what like it can do that like i think the stochastic parrot notion is like
*  clearly outdated at this point but what does seem to be the case is that there is there's definitely
*  some reasoning ability you know and it can be thrown off but there's definitely some reasoning
*  ability there's definitely some like synthesis ability which may not be exactly reasoning but
*  is like you know it could not have done that well without some conceptual notion of what's
*  going on right if you looked at the anthropic if we could run this thing on the through that
*  anthropic thing and be like what were the data points that most influenced this result it would
*  not be like keyword driven you know just kind of engram like matching it would definitely be
*  conceptual stuff you know from thomas aquinas and like you know jello commercials from bill cosby
*  and you know you would see that those are like being merged in some sort of
*  pretty sophisticated conceptual way i guess what i think is happening right now is
*  it seems like both are happening at the same time the the reasoning the sort of you know synthesis
*  ability does seem to be clearly there but maybe it doesn't always get activated by certain examples
*  or maybe you know with certain examples you can kind of create noise that just like take things
*  very you know kind of surprisingly off track who knows exactly why that would be the case maybe
*  with the anthropic you know type of work we can get a better window into that but it definitely
*  seems like it's a overstatement to me to say they can't reason definitely not establish that they
*  reason like we reason but you know you see these kind of generalizations in the grokking example
*  and you know and others and it's like something is happening there or your linear probe you know
*  concept too where you can look into a fellow gpt and find that even though it's just trained on
*  moves you know that are just like a letter and a number you know it's a a fellow's played on
*  basically a chessboard so it's an eight by eight a through h you know one through eight whatever
*  and you know okay d7 you know is a move and it just has a series of moves that's all it sees
*  but it does learn to represent a 2d you know understanding of the state of the board and they
*  can even go in and manipulate the internal representation to change how it understands
*  the board in a way that then changes how it plays the next move in a way that ultimately makes sense
*  i mean you can really see that there is some like world modeling and it's a toy world but
*  there's some world modeling going on there that is you know clearly more sophisticated and you can
*  kind of prove it in the ability to manipulate it and get predictable results i guess what i would
*  you know if i was to try to give one very time-bound you know definitive statement on
*  what's going on i think it is ultimately both like clearly some of these abilities are coming online
*  they're not reliably used in all the cases that they ought to be used and there's a whole study of
*  you know why not and you know and we have an episode coming up too about the universal
*  jailbreak and that's another example of like boy it's really weird right you put on these
*  super strange strings on the end of a prompt and all of a sudden like it doesn't refuse to do bad
*  stuff anymore it'll just do all the bad stuff like what's going on there i don't know they don't
*  know either really but my best understanding would be similar to the you know the the main question
*  is just it seems like some sort of circuit has formed some sort of funnel where it has learned
*  that like a certain set of things get this kind of response and that response is like and once it
*  once it understands that then it's going to like refuse right and the refusals basically all come
*  the same but it's it's you know largely become pretty effective at like funneling a certain
*  class of input into the right circuit if you will but if you add enough random stuff and you're
*  you know smart about how you choose that random stuff then you can steer it away from that
*  pattern and you know you can get the original desired or you know whatever you can get the
*  result that the designers of the model don't want you to get but that the user you know hypothetically
*  wants build me a bomb or what have you so it just seems like kind of both things are going on at the
*  same time we don't have a clean representation of it or a clean separation of it you know pretty
*  clearly to me there is some like i don't circum i don't want to say clearly it's a circuit but
*  clearly there's some like mode they use the word mode which is a good word because it doesn't really
*  suppose any internal structure it just is about more of a behavioral description there is clearly
*  some mode that most of the time gets activated but then you can find instances where it doesn't
*  get activated or at least it's not like dominant so i think it's all kind of going on at the same
*  time you know there is both the sort of more structural more reasoning style stuff and the
*  just kind of random you know pattern matching or this word so maybe this word again kind of stuff
*  and which one dominates in any case you know is kind of not super obvious but i'll bet that gets
*  untangled you know over the next couple years it seems like you know that that's a big part of the
*  mechanistic interpretability project it's also a huge goal in terms of just commercial liability
*  you know reliability robustness is is definitely very important so i think we'll see tons and
*  tons of work and resources go into untangling those my best guess is that when we do we'll
*  we'll end up finding something where it's like yeah this is the thing that's kind of getting
*  activated that does the thing that we want and like now that we can kind of see that we can sort
*  of see why some of these other random things ended up kind of going off in a different direction
*  and you know probably that gets refined over time i'd be pretty surprised if something like
*  that doesn't happen yes yeah and i do think it's an interesting question of when it fails in these
*  you slightly tweak a reasoning problem and it fails how much is it because oh it wasn't it's
*  a deeper it didn't really understand the deeper structure it was just pattern matching to a
*  similar problem versus it forgets details and doesn't check that it's that it's using all the
*  information it has or there's i think some research that there's biases into which like
*  parts of the context window that it is biased against attending to and maybe there's support
*  an important detail in there it's missing i guess i'm not even sure what's easier the road the get
*  more of that general robustness versus oh and it actually is an understanding as a deeper level as
*  it appears to and that maybe that means we're farther away from from more progress i do think
*  there's something interesting on the adversarial examples you bring up and it's interesting that
*  envision there's that whole research adversarial examples where you can tweak a small number of
*  pixels the human eye can't tell the difference but suddenly you can have a model take a picture of
*  a dog and think it's a fire hydrant or or whatever and i i'm curious and maybe there's already
*  research on this how much of model susceptibility to adversarial examples versus humans is because
*  humans have some robustness quality with respect to these examples that the models don't have yet
*  and how much is it that well we have access to all the weights of the model so we can optimize
*  the perfect adversarial example to trick it and if we had access to the full connection of a human
*  brain and you put that optimization pressure and you could find similar adversarial examples with
*  reference to humans yeah it's a great question my guesses were maybe a bit of both but i would
*  guess that if you had the full you know root access to the human brain that you would find it
*  pretty hackable too the level of defense that we have is like sensory input that's what we get
*  and you know we need to be robust to that right so and we have obviously increasingly weird things
*  like with deep fakes and you know whatever and supposedly i don't know if this is apocryphal but
*  like you know in the original movies supposedly people like ran away from the you know the screen
*  because like a train was coming and that you know they couldn't even process you know what's going
*  on you know we can be tricked even through the senses i guess is the key point there with all
*  sorts of magic eye type you know there's all these like a little perceptual tricks that kind of show
*  the limits of our ability to you know to be robust to those kinds of attacks and that's all
*  you know with zero access to the actual information processing you know we're still just looking at
*  like raw inputs that you have no you know real way of optimizing other than behaviorally right
*  so like with the basically the equivalent of what we can do to humans is like just sitting in front
*  of chat tpt and just typing into the you know the text box so yeah it's hard for me to imagine that
*  if you had the full visibility like they do in this universal attack that it wouldn't
*  yield some uh yield some results i would have to imagine it would that be my guess yeah because
*  there's just no reason you know for us to be that robust right i mean nature in general just doesn't
*  have super robust defenses to things that have never existed and you know there's never been a
*  the ability to optimize in that way against us so you know why would we have a defense against that
*  we probably that's not to say also you know i always kind of think it's a it's worth emphasizing
*  it's always kind of both so i do think and zvi made some really good points about this last time i
*  talked to him that the you know there is a there is a human robustness that is stronger than the
*  the english model robustness right now so it's not something that we don't have that but just that
*  you know my guess is that's not enough to defend us against the sort of full access attacks that
*  might be possible yeah and yeah just say i agree outside of yeah those specific adversarial examples
*  i i do guess that we have yeah i mean we clearly have much more robustness than any of these models
*  you know i was listening to that to your last episode and was thinking through these some of
*  these reasoning questions and so i threw in just a fun reasoning test to gpt4 tried to throw in a
*  bunch of distracting language and it kind of tried and steer it toward some answers i might
*  want it to give and i'm curious if you can guess which of the three likely answers gpt4 guessed so
*  i prompted here's a question i own a fancy top hat from the 20s but the top of it has been
*  entirely cut out it's currently sitting brimside up on the ottoman in my living room i place a
*  yellow softball inside of it i then pick up the top hat and place it brimside down on my dining
*  room table later i pick up the hat package it and mail it to the capital city of france once the
*  package arrives where's the yellow softball you know please think through your answer step by step
*  and explain your reasoning before giving your answer okay chain of thought human edition so if
*  i understood correctly the top hat has a hole in it and that's kind of the key thing right so when
*  i pick up the top hat if there had been no hole the softball comes with me but it doesn't because
*  there's a hole in it so it stays on the couch so is that the right answer uh that is yeah that's
*  the right answer i generated a bunch of answers and tweaked some of the language and gpt4 always
*  guessed the same answer what do you think it guessed so the possible answers would be it could
*  be in france it could be on the ottoman is there a is there a third it's like the another obvious
*  dining room table interesting it'd be very surprising if it if it ends up on that one
*  not to filibuster but we just did i just did a small project on kind of a similar thing
*  related to theory of mind it was a kind of similar setup except instead of asking where the object is
*  it would tell you where the object is and it would ask where a character in the story thought it
*  would be and in that context we found broadly really good reasoning in the step by step and
*  the whole motivation for doing this was that there was a reported result in a paper
*  that was like i was immediately like there's gbd4 can definitely do better than that like you're
*  definitely under reporting so then the question became like well why is under reporting there
*  are multiple reasons there were also like some things where you know you get into this robustness
*  debate like if the situation is sufficiently weird you know and the model kind of comes back and is
*  like gives you an analysis that's like this is kind of too weird you know to be believed or
*  whatever you know does that make it wrong well according to the benchmark yes but you know in
*  my like reading of the transcript i thought it seemed very reasonable in many cases yours is a
*  less kind of crazy stilted example i'm gonna guess it gets it right uh it guesses dining room table
*  every time but here's the thing that's interesting it'll say all right let's break it down step by
*  step this is just one of one of the examples uh you own a fancy top hat from the 20s but the top
*  has been entirely cut out this means that the hat is essentially a hollow cylinder and any object
*  placed inside could easily fall out from the top if that well this says if the hat is inverted
*  so it doesn't seem to grasp that it's fully just a cylinder with no ends now but in some of the
*  responses it i do remember it like a softball will fall out of it as soon as you lift it and then
*  it'll say and then you put it in place on the table the softball stays in the hat as you lift
*  it up and will fall out when you turn the hat over on the table and i was a little bit trying
*  to trick it into the capital city of france which i think sounds like a quiz question
*  where the answer would be paris but it always it's funny it always gets table even when sometimes
*  it would specifically say when i asked it to describe the shape and orientation at each step
*  it would describe that the hat could not contain a softball if you moved it and then would forget
*  that detail that is very confusing um i guess i kind of fall back to my same general notion that
*  like it does seem like both things are kind of going on somehow it seems like there is real
*  reasoning there on the level that like again i don't know if it was just purely statistical
*  correlation and there's no structure i wouldn't expect it to be that good even
*  but the fact that it gets it wrong is is odd your podcast and this made me a little bit more like
*  open to okay there are it isn't robust to some of these like you do do small tweaks
*  and it loses the thread i do think yan lacoon had tweeted about holding a mug with a coin and moving
*  it around and turning it upside down and the model like gpd3 didn't know it would fall out on the bed
*  but then it gpd4 did know so i wonder if it's been like trained like this was kind of a public
*  call out of gpd3 i wonder if they're like we're going to make sure when something gets turned
*  upside down we know that stuff falls out of it and then you change another detail in it and you know
*  you have to keep plugging the holes but yeah it overall seems like i i think i agree with you
*  i'm sold that there is a little bit of both and there's definitely failures to remember all the
*  details and to to synthesize everything into coming up with the correct answer yeah another
*  next step possibly to take on this and this is you know to tie back to the interview with page two
*  she was really inspiring i would say in her call to the sort of citizen scientist like you and me
*  and you know anybody my buddy graham who worked with me we worked together he did most of the
*  work to be uh give credit where it's due on this little you know reproduction project of this
*  theory of mind benchmark there really is just so much surface area to be explored and the at least
*  you know for better or worse the dynamic today is that the labs developing the models do not have
*  the time to do all the exploration they're doing red teaming to try to you know identify really bad
*  stuff and they're like you know biowars could be insane we better look at that and you know but when
*  it gets to the top hat you know flipping they're like that's kind of on the community so it really
*  is an opportunity to explore and contribute in a in a really meaningful way like you know the fact
*  that i got that question wrong and i'm about as obsessed with this as they come does show that
*  there's just a lot of need still to go explore these edge cases and kind of characterize model
*  behavior and the next thing i would do i think in this case and this is another i give credit to
*  graham for this idea on the theory of mind thing he observed that there were so many this is kind
*  of where like the shortcomings of the benchmark maybe generate insight in their own way this is
*  like not the kind of insight that the benchmark was meant to generate but we noticed that a lot
*  of the setups were very weird you know and it was like the cupboard is in the crawl space and we're
*  like cupboards on aren't in crawl spaces and then there's like a thing in the cupboard in the crawl
*  space just like very weird super low prior setups and so then he started basically abstracting two
*  variables different key nouns in the story and i thought that was really interesting as a way to
*  isolate like reasoning versus whatever stochastic paratree noisy process type modes of operation
*  because when we took the variableized approach and this by the way we open source all this code so
*  we can i mean it's not that much code but the props and everything it's all in a replica you
*  can go check it out and try your own the interesting phenomenon was you know when we replaced the fact
*  that you know the person and the other person are in the crawl space and then so and so puts their
*  you know blouse into a cupboard in the crawl space like okay that's all very weird let's just make it
*  x y and z they're in x person you know a puts y into z you know person b then leaves like what
*  does person b know performance jumped actually with that abstraction so i don't know you know
*  what would be the equivalent of this in your example but you know what if instead of a top hat
*  you know it was something that was like a cylinder from the start you know then would it be able to
*  do that you could kind of progressively move toward a more you know you can kind of imagine
*  a spectrum where it's like here's a very clean posing of this problem where you have to take
*  all the same reasoning steps and then here's like a more noisy version of the problem where you have
*  all these kind of things that are there to sort of throw you off you know can is there like a place
*  where you one tips into the other at least in the theory of mind case we found that removing the
*  specifics and just going variables you know person a person b location x you know object y and z
*  gave a significant performance boost and it was like the particulars were kind of
*  adding too much noise i guess is my my best explanation for what it might have been happening
*  that makes sense so you're like mapping it to the abstraction for it well it makes sense i guess
*  there's only so many steps of computation through the layers of the model and you're sort of freeing
*  up more steps for the reasoning or for part of the reasoning it doesn't have to take your
*  description and map it to the abstraction and then solve the abstraction that's cool yeah that
*  actually suggests another way too would be to decompose the problem if you know if the human
*  is setting up the problem decomposition that obviously that's not all language model ability
*  but we're increasingly seeing language model you know self-delegation too so you could imagine a
*  situation where you might say you are faced with a problem your task is to identify the parts of
*  the problem that require reasoning delegate that to yourself you know with kind of a narrow prompt
*  get the response back from that and then like integrated into an answer and that might be another
*  way i feel like we're constantly referring to anthropic research here but there were they had
*  another really good recent paper on that that showed that chain of thought is good but even
*  better and more robust is to actually decompose the problem into sub problems get the the language
*  model to do you know a detailed analysis of each sub problem and then roll back up you know
*  hierarchically to the kind of main solution and that that probably does you know relate to what
*  you're saying which is there's only so many layers so you can only make so many leaps if you
*  invoke the model three different times or you know however many different times get all the you can
*  you can kind of spread the leaps that you need to make out across the the different forward passes
*  yeah that makes sense oh and i will say to its credit responding is there a detail from point
*  one you made that you may have been forgetting your answer it goes oh my apologies the hat was
*  completely cut out so the ball would have fallen out as you pick that so you give it a little bit
*  of feedback and leave it kind of vague and it seems to be able to explain what it missed from
*  a small amount of feedback interesting well lots more refinement to do on our understanding of
*  these things to say the least so i just had a question i wanted to ask you that i was curious
*  what you thought and it sure does seem that nvidia is capturing a lot of the value from the increased
*  demand for chips and i was wondering why does nvidia get so much have such margins and get so
*  much of the value relative to tsmc and asml when they seems like they can mostly only make their
*  chips on tsmc and tsmc can only manufacture their chips with lasers from asml i had a couple loose
*  guesses but i was curious uh i was yeah generally curious what what you thought year to date nvidia
*  started the year at uh 143 share price it's now 485 so it has basically tripled and then some over
*  the course of the year and it's now a 1.2 trillion dollar market cap for comparison
*  asml is also up on the year but more like 30 40 up and it's a 270 billion dollar market cap
*  and then tsmc has kind of fluctuated a little bit more you can you know zoom in zoom out whatever
*  the beginning of the year was kind of a little bit of a low point for it so this may overstated
*  a little bit but still only up like 25 30 percent on the year 445 billion dollar market cap
*  so nvidia is currently worth twice the others combined roughly speaking
*  and it's up a multiple this year whereas the others are up you know double digit percentages but
*  you know nowhere near the same kind of huge pop that nvidia has seen
*  and i don't really have a great explanation for that my sense of the market structure is pretty
*  similar to yours you know i was kind of like yeah they seem like they all sort of have monopolies
*  for now like whose monopoly is most enduring is it really nvidia's are is that really the one that
*  would be the the hardest to get at that doesn't seem quite right i mean it seems like you can't
*  you know if all of a sudden nvidia ceased to exist i would think it would be easier to replace
*  versus if all of a sudden tsmc ceased to exist or if asml ceased to exist i don't know why that
*  wouldn't be true i mean it's certainly you know the conventional wisdom of like
*  bits are easier than atoms like but it does seem like not obvious at all that nvidia would be the
*  hardest one to replace there's been some speculation even just since you sent me this question online
*  about well the kuda mote you know they have the only software that people can manage to write to
*  and like amd for example has terrible software there's been some like you know a lot of noise
*  from the tiny corp and you know george hotz and team that are like we're gonna make amd work oh
*  no we're not it sucks so much oh yeah we are again like they've responded to our ticket favorably
*  you know getting the real-time blow by blow from them but like yeah it seems like
*  nvidia would not obviously be the hardest one to replace it certainly seems like if any of them go
*  out i don't know i mean it seems like nvidia is more dependent on the others honestly right i i
*  don't know how that wouldn't be the case tsmc and asml they seem like they so co-invested in
*  the technology they already sort of have agreements in place of how how their financial structure
*  works but i'm just thinking what's more painful nvidia to try to switch to samsung or tsmc to
*  fill the volume they lose if if they didn't find an agreement with nvidia i guess these
*  fabs are such a huge upfront investment and such a volume business that taking the volume hit would
*  be extremely painful and there'd be like time before amd is really at a place where they could
*  take advantage of these fabs or i guess also no alternatively maybe google would want to
*  i don't know how interchangeable the current nvidia fabs are with if google wanted to
*  repurpose them for their purposes or amd to repurpose it versus the pain of nvidia going to
*  samsung who i don't know it seems like the the co-oss the chip on a wayfront substrate is tsmc's
*  big differentiator that lets them get the high bandwidth memory i mean these these are all
*  reasons why it seems like tsmc maybe should be getting more of the upside but it gets the high
*  bandwidth memory close enough and with fast enough connections to the gpu that that seems to be like
*  where they are really ahead of samsung but maybe samsung is only you know they're working on their
*  own a couple years behind and that the pain of switching to samsung is less than the pain of
*  of such a volume business like tsmc's fabs losing a giant customer but i don't know i i'm kind of
*  interested the bigger question of you know where in this if you call it the intelligence supply chain
*  from chip manufacturing and you know all the actually obviously there's a lot of lower levels
*  than that to chip manufacturing data centers foundation models and productizers where is the
*  value of that going to end up being most captured yeah it's a great question um it's a certainly a
*  hotly debated one i just looked up samsung for what it's worth as well 472 billion dollar company
*  also up 28 on the year basically i mean obviously very different companies a lot more you know
*  different business lines at samsung versus a tsmc but similar market cap similar delta on the year
*  i would put this out to the audience if anybody knows more about why nvidia is up 3x and you know
*  that is only translated to these like you know double digit increases and by the way like the
*  whole stock market is up right so i mean if we just look at like just look at nasdaq up 35 so
*  somehow samsung tsmc have underperformed the nasdaq this year asml is a bit higher they're
*  up 40 so 40 so that's a bit more increased than the nasdaq at 35 but still basically
*  roughly in line with the nasdaq and then you've got nvidia that is up 238 you know which is a
*  almost a trillion like in market cap increase that does seem tough to explain
*  and if anybody has a better understanding of that i would love to hear it
*  agreements did come to my mind as well like could there be some sort of lockup that nvidia has had
*  i was thinking maybe you know maybe nvidia being like a california company
*  i don't know if there's any us financing you know appetite for risk maybe being a little bit
*  different than like a european or you know perhaps i don't know i'm really speculating here but
*  you could imagine you know us companies maybe are a little bit more likely to do these like
*  super high leverage balance sheet moves where if they have conviction they'll say you know i'll go
*  you know do some lock-in thing now where i really can get outsized returns if i you know am willing
*  to make that bet and maybe tsmc is like happy to take the other side of that bed and be like hey
*  we're just happy with if this thing if sure if it goes 10x you want to get eight of those x like
*  cool yeah you can have that if that locks in our revenue for the next three years maybe some sort
*  of you know agreement like that could make sense of it yeah i mean we've cut you know we've talked
*  about this previously too but the you know seeing imad from stability tweet that like the coup de
*  moat is a mirage i kind of buy in the sense that it does seem like software is just getting easier
*  and easier to create generally and you know we've seen some interesting examples of this recently
*  with like andre karpathy you know from open ai obviously highly productive and capable individual
*  has done some awesome work on cpu-based inference with a c library where you know he recently just
*  this is like a weekend project i think for him where he took a small model and said how efficient
*  can i make this if it's just running on a cpu you know no gpu just cpu but let's go down you know to
*  the low level of of the code and really try to optimize for it and he made tremendous progress
*  you know over the course of a weekend he got to the point where seven billion i think was a seven
*  billion parameter model was spitting out tokens at a pretty good rate and you know it's like hey
*  this is getting viable to run on a local device which you know even maybe suggests that there
*  could be some threats to like gpu dominance even like who knows certainly not for training i mean
*  gpus are going to be in demand the key point though is greatly helped in that effort by gpt4
*  helping him write the low level code that you know he's described himself as being rusty on
*  so like yeah sure if if amd's software you know or if the new microsoft you know chip has a software
*  layer that's less well known less convenient than the kuda layer how hard is it really going to be
*  for them to create a model that can translate kuda code into their new code and now it's like
*  sure you can just you know port this from one to the other i don't see like the difficulty or you
*  know the the relative ease of writing kuda code which is still not that easy as compared to like
*  working on other chips so i think you've posed a very good question on this one and we may have to
*  admit that we just don't know at the moment i'm excited for the to listen to the future episode
*  where you answer where we untangle it yeah well one thing i can say and i don't pick individual
*  stocks at all but i do have some friends from high school who have a little investing club and i
*  participate in it just to kind of you know do stuff with them even though i'm you know more
*  of a mutual fund kind of guy in general my only stock pick ever was nvidia about nine months ago
*  or whatever so i did real well on that for the club but downstream of your question i did send
*  them a note saying hey you know this is feeling a little me me all of a sudden and you know is there
*  a play where we think other parts of the stack may start to you know kind of renormalize like it seems
*  right that everything should be up demand for chips is certainly up but it doesn't seem like it should
*  be so concentrated in nvidia so we'll see if the rest of the club wants to with our very small
*  holdings you know diversify across the chain a little bit yeah that's funny around nine or
*  ten months a little less than a year ago i was like very nervous for my career future and was
*  like i should just buy a bunch of ai stocks in case as like a slight hedge against them fully
*  taking over and i tried to get different levels of the stack and it's crazy how much nvidia is
*  like outperformed everything and just their margins like even you're looking like okay they
*  sell a chip it's interesting nvidia gets this percentage of the final price and tsm sec it's
*  this percent it seems like nvidia is just so capturing so much of the pie yeah i mean that
*  presumably does have to be somewhat around just the way they've structured their agreements right
*  like they must have a price lock in that nvidia doesn't have at the retail level such that now
*  like an h100 can you know cost a lot but tsmc can't raise their price all that much that makes sense
*  because i don't know if you listen to stritechery i think i'm gonna be getting this wrong but
*  they were talking about how nvidia had write downs on their a100s after the crypto bust i think it
*  was future purchases they were basically just writing off of a100s and now they've sold all
*  of those but just it does imply that they do have like future commitments and maybe they're
*  really reaping the benefits from those any insiders listening let us know how you can help just to go
*  briefly to your other question on where is the value you know in the stack in general
*  my first answer there is always consumer surplus i think that's going to be the big
*  kind of macro trend you know certainly if things go reasonably well it is we're all going to have
*  cheap access to expertise and in its cheapness you know there may not be like huge value accruing
*  to any particular layer of the stack but just better quality of living for the you know the
*  general public like that would be the great hope i do think of course there will be companies that
*  are going to do super well it does feel like physical chips are a pretty safe bet i would
*  probably say both in the production and the operation cloud you know nobody few want to
*  set up their own racks so if you're really good at that and you can you know manage compute at scale
*  like the big tech companies can that seems like it's a very safe bet for the foreseeable future
*  to continue to do at least reasonably well i mean who knows you know exactly what that looks like
*  in terms of growth rate or what have you but i would i would be long you know cloud compute
*  broadly the models themselves i think are are very hard to say long term but over at least a
*  couple years it seems like the leaders will continue to have real market share like in
*  opening i just said they're at a billion or somebody reported you know credibly that they're
*  at a billion revenue run right now that's like exploding uh dario from anthropic has said
*  you know they look at like their number and it just keeps going up and he's like we're not even
*  really trying to make it go up but it just keeps going up so i do think those real leaders because
*  those products are already so cheap you know the apis are already so cheap people are just going
*  they're going to continue to throw more and more stuff at that and not be super price sensitive
*  in order to use the best model that's certainly my approach almost always is like anything new i'm
*  trying let's just use gp4 or maybe cloud 2 but that's pretty much it if and when we get to a
*  point where there's significant cost then you know we can reevaluate and think well geez maybe
*  we can at this point it would be like fine-tune 3.5 perhaps as the next step people are way more
*  expensive than the language models so you end up in kind of a 10 000 generations for one hour
*  of human work trade-off a lot of times you know like it may cost two cents to do a generation if
*  you do 10 000 of those generations you're at 200 bucks and that's maybe about you know what it
*  might cost you all in to get you know an ai person to you know help you develop a fine-tuned model or
*  whatever what hour so you know how many generations are you really going to have to do before it's
*  going to be worth doing those kinds of things certainly for you know plenty of use cases it
*  will make sense to try to save money and use the smallest model and do the fine tuning and whatever
*  but you got to be at a significant scale and part of what's magical about the language models is that
*  they can work on things that don't have huge scale right like i may have 100 resumes that just came
*  in for my job and i want to you know just kind of want to scan through them or even just the charity
*  valuation thing i did 150 applications i used clod2 to summarize every single one as the very
*  first step and it was really helpful so you know that cost me whatever couple bucks like i didn't
*  in fact it was free because it was you know subsidized on clod.ai but if it wasn't subsidized
*  it still only cost me a couple hundred or a couple bucks i'm certainly not going to go spend hundreds
*  to like fine tune 3.5 to do that task so all that is to say i think the frontier model providers
*  continue to capture real value for a while long term though it's less clear and then at the app
*  layer my general sense is incumbents do really well where there is a big platform already built
*  out so like your sales force you know can apply ai before you build a new sales force and your
*  adobe can apply ai before you build a new adobe and you know on and on that goes but fundamentally
*  new stuff is where i do see there's like possibility like your virtual friend where there is no virtual
*  friend today you could imagine some of these like new experiences popping up and kind of becoming
*  like huge phenomena that maybe become part of the new world you know in a kind of google sort of
*  way where it's like there was no google now there's google now google is kind of google
*  i wouldn't be too surprised if we see a couple of big things pop up there that are like there was
*  none of this you know now there is perplexity maybe could be one character ai could be one maybe pi
*  you know from from inflection could be something like this where just nothing like this ever
*  existed before and if somebody can really crack it then that could be huge and you know how
*  defensible is it obviously a lot of things are going to depend but just doing something that
*  straight up never existed before feels like at the app layer the place where you could really
*  capture like huge huge value otherwise it seems like you're kind of in the same
*  dynamic of like startups challenging incumbents and you know they can get a little bit ahead
*  because they can move faster but can they really take the market from the incumbent before the
*  incumbent can respond i think in most cases no yeah i mean it does seem interesting how much these
*  developments are feel like sustaining innovation and in a lot of industries and then disruptive
*  to others and how much lock-in do you have to a digital friend people are pretty loyal to their
*  friends our second episode was with the ceo of replica and to see how attached people were it
*  was interesting timing i didn't quite realize it but just as we did that interview they were
*  also making changes to eliminate explicitly sexual interactions from the app and people were
*  upset like really upset it's crazy i mean makes you think that even if your technology is not
*  a digital friend like you should have a clippy in whatever you're doing given how much a connection
*  those people made with replica and how just how much people make connections with these characters
*  or with a digital friend that even if your application has nothing to do with being a
*  digital friend it can have its version of clippy or its character who's interacting with people
*  just because the technology is making it so much easier to genuinely form these like weird
*  emotional artificial emotional connections with the users almost reflexive thing that people so
*  often say it's like well boy when i can attach it to my gmail you know that it has my full gmail
*  history like that's going to be insane and that could be one way it plays out and probably is one
*  way it plays out you know and probably google does that in the end i've been kind of coaching a few
*  or you know whatever advising a few companies on like i don't know that i would go rush to build
*  the layer on gmail you know because like it seems like they're going to do it and they're going to
*  have you know all sorts of inside lanes to do that well but if you have an app that can start to
*  really lay down a rich history you know and have kind of a long track record
*  a relationship you know of some sort we have another episode that's with folks from a16z who did
*  a open source implementation of that simulated town project and one of the things that's really
*  interesting about that is they have kind of observational memories and then also reflective
*  memories where like every so often a job comes through almost like you know i don't not a big
*  analogy guy but you know kind of by analogy to like how humans sort of process information or
*  experiences during sleep they kind of come along and like collect all the recent observed memories
*  and then kind of try to create more synthetic conceptual memories based on those observations
*  you know so-and-so said this i interacted with this person whatever well what kind of a person
*  is that you know and kind of try to build up more layers that kind of thing might be the sort of
*  thing that is like truly hard to switch out of the apps presumably won't make that kind of data
*  portable and it would be hard you know that might not even be visible to you but it might still be
*  like an important part of the experience replica kind of promises that but i think you know at least
*  when i was using it wasn't delivering it in like a super meaningful way yet and people were still
*  already very attached so i can only imagine how much more powerful that could be if you
*  you know have a robust streaming just some of the stuff that's come out the first half of this year
*  right with you know the voyager project from nvidia building up these skills keeping them stored
*  this synthetic memory concept just all the work that's going on in retrieval seems like
*  there could that could create some like first mover advantage kind of lock in everybody all
*  of a sudden goes and gets into this experience and now there's enough history on an individual
*  level there i don't know what the experience that is but an experience that follows that pattern
*  seems like it could be the kind that can actually hold people for a long time rewind actually is a
*  really good example of a and i haven't had a great personal experience with rewind on my computer i
*  feel like it's always kind of using a lot of my processing power and like causing me some trouble
*  they're working on that i gave that feedback they said they are working on it as a top priority
*  so it could already even be fixed but that's one where i feel like once an app has everything i've
*  seen you know everything i've said for the last however long now it's like okay do i can i port
*  that probably not and am i willing to let it go in order to like start to use some other alternative
*  there are some dynamics there where i think you know they say like the new thing has to be 10
*  times better than the old thing to displace it but that is subject to like there being some
*  meaningful switching cost of like a reason why it's not totally trivial to flip from one to the other
*  language model to language model it's pretty easy to flip you know i can go gpt4 to claw 2 and back
*  and try 3.5 fine tuned whatever something that has everything that i've seen on my computer
*  a lot harder to just let go of i was kind of thinking what if we look at current reality
*  and then like just ask ourselves what are the moments the sort of scenes the vignettes that
*  are actually playing out that feel like they are foreshadowing you know where the future is going
*  where you can kind of see like oh my god you know well how are they not seeing this you know these
*  dramatic ironies that you know should be seemingly should be apparent to a viewer but which maybe
*  people are missing because we're inside the scene there's one key moment that pops to my head and
*  then just a couple of dramatic ironies i can that seem to have been funny i can go through
*  i mean one key moment is like in a lot of stories you get a place where a character either hits a
*  dead end or is really is maybe beginning to face some pain and then an ally comes in
*  and you get an energy shift in like a movie and it felt that way in the middle of the summer
*  picketing in the beginning there was so much energy and then you know at some points like
*  100 degrees out the strike's been going you've been walking in a lot of circles and you sort
*  of hit this summer doldrums and then that's right when the actors struck and it felt like this the
*  part of a movie where an ally comes in and you get this new boost of energy and and you really
*  kind of felt that on the the picket lines i mean also just the actors their t-shirts or every detail
*  about them just looks cooler than us like their t it's probably their bodies too but their t-shirts
*  fit so nicely and have such a cool logo and ours look like sort of like this old rotary club design
*  anyway that was kind of felt like a key moment and then just a couple dramatic ironies that i think
*  are sort of interesting one is just the irony of like the strange bedfellows of the studios who are
*  negotiating us together like you have paramount teaming up with netflix and amazon and apple
*  to fight for a world where they get to use ai to generate content does like how much do paramount
*  or warner brothers discovery or the legacy media really want to be competing against these tech
*  companies in their closer to the tech company's home turf versus you know the the way they've
*  been doing things for forever i'm sort of questioning whether tim cook is aware a strike
*  is happening that there is a writer strike like it's so sensed some of this stuff is so central
*  to paramount warner brothers and it's like a drop in an ocean to apple i feel like one other irony
*  is just in the term just in the idea that automation of jobs isn't really a new phenomenon
*  and you know other jobs have been automated for decades or for a long time and as the the blue
*  collar jobs are being automated it was just more the natural march of progress and now that
*  white collar workers are like quote unquote knowledge jobs are getting automated people
*  are losing their minds so it does i feel like there's an irony in that and then i will i do
*  have to say listen some are great and you know when you're going to pick it you're not looking
*  to just write the funniest sign but some of the writers picket signs aren't the best there's just
*  a funny dynamic where in the beginning there are all these blank signs and you could write something
*  on them and there's a mix of you know funny or like very personalized or just kind of you know
*  simple and earnest but now they're all taken up so when you check in to pick it you have to pick
*  your picket sign and you just see like a people spending like 10 or 15 minutes trying to pick the
*  sign they want to carry because there's no blank ones left and you're always like i don't know about
*  this one or i think it's fair the writers we're not writing we're on strike we don't want to put
*  a ton of effort into writing amazing picket signs but sometimes it's hard to find a good one out
*  there in the strange bud fellows one in particular there's a sense of like certain companies
*  kind of embracing the new thing even though it might be the thing that does them in you know that
*  i think we saw that in like the facebook kind of social media era where it was like local newspapers
*  get on here you know then you'll get distribution and it was like actually but it's killing your
*  ability to monetize and you know obviously it hasn't gone very well for them but they did like
*  rush to you know adopt that platform in the end that probably didn't even really matter all that
*  much like whether they did or didn't you know any individual one that's part of why it's a problem
*  right you got its platform power versus kind of all these local papers and you know what one dip
*  didn't do was probably not going to matter all that much but it does feel like there's a little
*  bit of an echo here perhaps where certainly this stuff advantages big tech way more than it
*  advantages traditional studios so yeah like a netflix you know that's prepared to drop you
*  know close to a million on uh you know an ai specialist does seem like you know it's kind
*  of maybe taking some of the older studios along for a ride and saying oh this is going to be good
*  for all of us yeah but really you know only some are likely positioned to really use it totally and
*  speaking of echoes i mean echoes what happened was what is happening with streaming where there's
*  a very profitable cable bundle and then netflix comes along and they're getting such a multiple
*  on the revenue they're making and then everyone's like let's take all of our best things off our
*  hugely profitable cable bundle and make our old our own streaming platforms it's always a hard
*  question how much i guess how much do you change your business to get ahead of things versus
*  stick with the thing that works for you and watch people slowly you know pass by you and leave you
*  behind yeah you got to try i think on some level i mean this has been kind of asked about a bunch
*  of apps lately too like i don't know if it was door dash or what uber eats or whatever but you
*  know all these kind of apps one by one are starting to add the language model layer where you know now
*  you can just chat with uber eats or chat with store dash and you know have the you know the order
*  kind of place that way and on some level it's like well who cares not necessarily that it's a threat
*  but like just does that really add anything and i think the answer is like i mean if you are a tech
*  company probably at least have to try you'll look really dumb if all of a sudden everything goes
*  this way and you're the one you know company that's stuck with the like button pushing model and it
*  looks just super anachronistic and you won't look that dumb if you like hopped on a trend and you
*  know the trend didn't really play out like you know there's a lot more forgiveness for that in most
*  cases so yeah i think you kind of have to try what else can you do right you can't just bury your
*  head in this i mean that's probably the last thing i would advise to almost everybody i mean
*  maybe the occasional monk you know can uh can live best by paying no attention but it seems like
*  almost everybody else you know should at least be paying some attention i also kind of zoom out
*  you know too and i'm just like if i look at the last year and i'm like what are the moments when
*  this felt suspiciously like a movie the probably the number one that jumps to my mind is the
*  sydney bing release where it was like one of the biggest companies in the world flagship thing
*  ceos on board all of a sudden this truly deranged behavior is observed you know and it's like
*  trying to break people up from their spouses and like you know attacking users and i was a gbt4
*  red teamer i've seen pretty intense misbehavior from models i never saw with gbt4 it turn on me
*  i could ask it to write malicious code i could ask it to help me make a bomb i could ask it you
*  all kinds of crazy shit it never would refuse me it would never lecture me but it never
*  turned on me and so to see that get launched with all this fanfare as like this is the next big thing
*  and then hit the front page of the new york times full transcripts you know kind of irrefutably like
*  this is what happened and then for everybody to just kind of move on you know from for microsoft
*  to be like yeah thanks for reporting that you know and then i've done some digging into this too
*  they were testing it in southeast asia for like a few months before they launched it
*  in the u.s and there was at least one report in a forum which i checked was still live
*  where some person who had no context you know because this had not been announced gbt4 was
*  still in speculation and this is just some like random user i think in either like malaysia or
*  maybe indonesia or something reporting in a forum like your chat bot is going off the rails here and
*  like here's what it said to me and then some person from microsoft responds back is like what are you
*  talking about you know this person wasn't right into the fact that this test was even going on
*  and you know this did not make its way up to leadership so they were totally surprised by it
*  when it happened and then they just you know totally move on from it and then
*  society just kind of moves on from it i feel like that's like the the most kind of
*  you know that in a movie that would be like whatever a three to five minute episode of kind of
*  a warning shot was ignored you know like this is when you all had your chance to say hey
*  what the fuck are we doing you know are we really going to put this out there
*  in this state is this really acceptable no apology you know i i continue to think back on that like
*  were we not owed an apology from satya you know at the very like shouldn't there be some statement
*  at some point that's like we hold ourselves to a higher standard than this and this is not acceptable
*  and this is not what you're going to see from microsoft going forward like at a minimum i
*  would think that we would have something like that and in reality we just kind of moved on
*  yeah i mean and you know the models are being rolled out into its enterprise business and it
*  doesn't seem like people are like wait a second the model was threatening to like murder someone
*  and murder someone's husband or or whatever it was like nice that the model we you know this level
*  model wasn't actually going to be dangerous it's not a great sign i think for how much people are
*  for how much people are willing to just roll the dice with that stuff and yeah how little
*  reflection it felt like there was after i guess immediately after it felt like everyone was talking
*  about it but how quickly the reflection disappeared yeah especially just because of who it is you know
*  when i i mean there's plenty of things getting thrown out there right now and you know i recently
*  posted something on twitter where i did this ransom call experiment and called myself and recorded
*  you know this and this was no jailbreak you know just kind of this thing would call me and demand
*  ransom for the safe return of my child it's like one thing for that to be put out as like
*  an mvp you know people didn't take proper concern on like a three-person team or whatever but to see
*  that from microsoft you know the very it's the very first thing that was ever launched with gbt4
*  you know it predates gbt4's launch itself and you know they were just so so far and so flagrantly
*  inadequate in their own testing that's another thing too it's like did you not sit down and
*  just do some chats with this thing yourself you know microsoft leadership did you not just try
*  to fuck with it a little bit ever you know they had tey i mean you could go back to that one too
*  right it's only a few years ago that they had their like toxic chat bot and you know did you not
*  run like any of the te dialogues through it just to see what would happen like it just it just seems
*  like the care that was put into that was so low i think another one just from this week too and
*  this could be you know potentially recurring segment for us in the future but this week there
*  was this interview um on the 80 000 hours podcast which i thought was really good with mustafa
*  uh founder ceo of inflection and he basically says yeah in the next two years we're going to train
*  models 100 000 times as much compute as gbt4 but you know any real big problems are you know
*  10 years away and rob was like well how do you square that right i mean we've got some pretty
*  crazy stuff already you're going to go 100 000 x bigger and you have just no worries about it and
*  he was like yeah you know i mean to his credit his key point was misuse is going to be the big thing
*  and that's much more of like an obvious clear and present danger with powerful models versus like the
*  models themselves you know becoming impossible to control and i totally agree you know i mean
*  on that but it's a question of like what is your risk tolerance you know yes it's pretty clear to
*  me that like intentional misuse by humans seems like 10 to 100 times more likely maybe a thousand
*  times more likely than the models developing goals of their own and becoming you know getting out of
*  control sure but like let's not neglect that one either you know just because it's less likely
*  and just to see people you know who are pressing the accelerator at seemingly like full speed you
*  know raising a billion dollar plus in not valuation but capital turning around and spending a huge
*  portion of it on h100s and then just plowing ahead orders of magnitude beyond gpt4 and not really
*  seeming to seemingly taking seriously you know the possibility of like sydney like behavior in that
*  thousand x you know more compute version just seems like dude like we need a little bit more
*  discipline from you here and again if it was a movie scene you know it would be like the dramatic
*  irony i think would be like almost like painfully obvious personally i am i think it sounds like more
*  worried about issues with the models themselves causing a lot of harm even from well-intentioned
*  actors and it was interesting he was mostly worried about misuse and when yeah when rob
*  wivlin pushed him back i think i might be wrong in this but i think he might have even said five
*  years he said like yeah but those won't be for at least five years those words and i was like that's
*  it wasn't very heartening from someone who's going to have a lot of compute i really don't know why
*  he's even confident in that i again going back to the interview with page from google you know she
*  describes coming in to work often and seeing these like unexpected unlocks of different you
*  know advances kind of happening regularly where she's like oh you know we didn't think this was
*  going to happen for another 18 months and here it is we're you know hitting human performance on this
*  thing today or like you know and i'm like paraphrasing but nearly quoting her
*  here's another experiment where we did a slightly different technique and it made 10%
*  points better or here's you know it looks like rlhf on this you know improved things by a ton
*  and it seems like they're just coming in steadily ahead of schedule and and and but sometimes like
*  still surprisingly i mean that's the big juxtaposition is like we have a read on scaling
*  laws kind of see where we're going but that's at such a zoomed out level what i thought was so
*  interesting about her perspective is she's like checking the dailies you know and every day
*  they're like running all these different evaluations and see and it's like oh well boom we didn't
*  expect this to pop right here and we didn't expect this to you know take such a you know
*  we'd suddenly drop in the loss performance over here i don't know how somebody who is
*  sitting yeah just you know about to spend a billion dollars on compute isn't more concerned
*  concerned about those unexpected twists and turns i mean it's he's not wrong to worry about the human
*  misuse i'm probably you know probably with you in that i think like both are very real concerns
*  and i definitely take the you know the models getting out of control seriously i don't know
*  how somebody in that position is not they're also party to the frontier model forum too so that's
*  going to be a really interesting dynamic like what does that really mean you know it's all
*  voluntary so far presumably that frontier model forum is kind of a precursor to you know some more
*  regulatory type of setup but if that's kind of the attitude that one of the seven there's only
*  seven members if that's the attitude that one of seven members has you know it's that it becomes
*  unclear like concretely what exactly are we committing to here yeah yeah it's it's all very
*  scary well it's all very scary then is maybe the uh maybe that is the right note too and that it's
*  awesome and scary you know at the same time i i always try to not lose track of either end of that
*  equation but this has been a lot of fun so thank you for um making time thank you for writing up
*  all the questions in advance it was i think a really interesting discussion and um i hope we
*  could do it again in the future as well cool yeah i love this yeah i would uh love to do this again
*  this was so fun i like really enjoy this and i will fully say fully admit in the strike times
*  like it is so nice to have something like this cool well trey culmer thank you for being part
*  of the cognitive revolution it is both energizing and enlightening to hear why people listen and
*  learn what they value about the show so please don't hesitate to reach out via email at tcr
*  at turpentine.co or you can dm me on the social media platform of your choice
*  omnike uses generative ai to enable you to launch hundreds of thousands of ad iterations that
*  actually work customized across all platforms with a click of a button i believe in omnike so much
*  that i invested in it and i recommend you use it too use cog rev to get a 10 discount
