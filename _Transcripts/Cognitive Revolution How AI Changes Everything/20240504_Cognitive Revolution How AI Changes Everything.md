---
Date Generated: May 05, 2024
Transcription Model: whisper medium 20231117
Length: 7200s
Video Keywords: []
Video Views: 837
Video Rating: None
---

# Emergency Pod: Examining SB 1047, California's Proposed AI Legislation
**Cognitive Revolution How AI Changes Everything:** [May 04, 2024](https://www.youtube.com/watch?v=DcQdy88vCYg)
*  Hello and welcome to the Cognitive Revolution, where we interview visionary researchers,
*  entrepreneurs, and builders working on the frontier of artificial intelligence.
*  Each week we'll explore their revolutionary ideas and together we'll build a picture of
*  how AI technology will transform work, life, and society in the coming years.
*  I'm Nathan Labenz, joined by my co-host Eric Torenberg.
*  Hello and welcome back to the Cognitive Revolution.
*  Today we have a timely and important discussion about SB 1047, the proposed AI legislation
*  that's working its way through the California state legislature, also known as the Safe and
*  Secure Innovation for Frontier Artificial Intelligence Models.
*  To understand this complex and all too often misunderstood topic, I've invited Nathan
*  Calvin, Senior Policy Counsel at the Center for AI Safety Action Fund, which is one of
*  the bill's co-sponsors, and Dean Ball, Research Fellow at the Mercatus Center,
*  to share their perspectives. We're also joined by Steve Newman, a self-described recovering
*  entrepreneur who is now working to raise the level of discourse around AI issues.
*  To set the stage, I can say with quite high confidence based on my own close reading of
*  the bill and related analysis, including that of friend of the show Zvi Moshewicz, that contrary
*  to some of the noise being made online, this bill is not some sweeping anti-technology power grab,
*  nor is it the cynical ploy to protect industry incumbents from competition that some have alleged.
*  As some evidence of that, consider that California State Senator Scott Weiner,
*  who is advancing the bill, is probably best known for his pro-housing work.
*  And he's also proposed legislation to hasten the transition to solar energy,
*  to extend alcohol-sailed hours in bars and restaurants to 4am, and even to decriminalize
*  psychedelics. What's arguably most striking about AI discourse in general is how the
*  strangeness of the technology creates space for people to think for themselves, and how that often
*  results in the scrambling of familiar political coalitions. At the same time that we have ostensible
*  political opposites like Tucker Carlson and Snoot Dogg emitting similar soundbites, our discussion
*  today is among people who are generally very pro-technology and very skeptical of government
*  regulation. As you'll hear, while Nathan and Dean do still ultimately come down on different sides
*  of this legislation, they identify many, many points of general agreement along the way.
*  Personally, while the bill may not be perfect, I appreciate that Nathan and his co-sponsors
*  are making a genuine effort to focus on the critical issue of tail risks posed by advanced
*  AI systems, and not placing an undue burden on startups and smaller players in the AI ecosystem,
*  even though that means leaving other important questions, including the risks associated with
*  algorithmic bias, the future of professional licensing, and the general impact on the
*  workforce at large for another day. At the same time, as someone who has long identified as a
*  libertarian and often been frustrated by government overreach, I'm definitely sympathetic to Dean's
*  concerns that a new agency could exploit the bill's ambiguity to gradually expand its scope and power
*  in very problematic ways over time. Ultimately, it seems to me that some form of government
*  regulation of AI is inevitable, and so I feel that it's well worth genuinely engaging with
*  and attempting to improve SB 1047 as it goes through the legislative process. With that in
*  mind, toward the end of this conversation, in search of a productive synthesis, I put forward
*  and we discuss several possible amendments that I hope might make the bill a bit better. Overall,
*  I think you'll find this to be an unusually respectful, nuanced, and insightful discussion
*  on what might be the defining policy issue of our time. If you agree, I'd ask that you take a
*  moment to share it with your friends. And on this topic in particular, I really want to hear your
*  best ideas for how government can help foster a positive AI future. I have an interview scheduled
*  with Senator Weiner for next week, and I will definitely read everything that you send me and
*  attempt to bring the best ideas to him directly in that conversation. You can reach me, as always,
*  via our website, cognitiverevolution.ai, or by DMing me on the social media platform of your choice.
*  For now, I hope you enjoyed this deep dive with Nathan Calvin, Dean Ball, and Steve Newman into
*  California's AI Bill SB 1047. Nathan Calvin, Dean Ball, and Steve Newman, welcome to the Cognitive
*  Revolutions. Thanks so much. Happy to be here. Likewise. So we're gathered together for a
*  discussion about the proposed legislation that is working its way through the California State
*  Legislature, which is known as SB 1047, or as the authors want it to be known, the Safe and Secure
*  Innovation for Frontier Artificial Intelligence Models Act. This legislation would be first of
*  its kind. Obviously, we've seen the executive order, but there hasn't really been anything
*  passed through a legislature that would try to meaningfully govern what's going on at the
*  frontier of AI. And it's predictably been the topic of a lot of discussion, debate,
*  seemingly some miscommunication, some misunderstandings as well. So I wanted to
*  bring this group together just to explore those and see what we can potentially hope to come to
*  some agreement on. Also, it's worth noting too that we're not yet at the point of an up or down
*  vote. So in terms of how to approach this today, I got a bit of outreach from Senator Wiener's
*  office in California. And the guidance there was like, this is probably going to play out over the
*  next several months. There are going to be a lot of opportunities for people to make amendments.
*  So we're very much still in the discussion phase. And potentially, if we come up with anything that
*  we all feel good about, we could even recommend that to the authors. So that would be a big win
*  in my book. For starters, Nathan, you are the Senior Policy Council at the Center for AI Safety
*  Action Fund. You guys are one of three co-sponsors behind the bill. You want to just start off by
*  giving us a little bit of an introduction on who you are, who the organization is, your kind of
*  background worldview and why it's something that you're motivated to work on. Then Dean, I'll give
*  you the chance to give a similar story next. Yeah, absolutely. So to start with the organization,
*  the Center for AI Safety Action Fund is like the 501P4 affiliate of the Center for AI Safety,
*  which listeners might know from the statement we put out about extra risk for AI, as well as a lot
*  of the technical research work that this center does, including by our Executive Director, Dan
*  Hendricks. The 501P4 was partially created because we were getting lots of inquiries and incoming from
*  policymakers, including Senator Renner, and we wanted to have a vehicle that could do more
*  direct quality work, which is something that 501P4s aren't able to do. And yes, specifically for
*  involvement in the bill, Senator Renner put out an intense bill mid last year talking about
*  being interested in doing a bill on these issues. And he approached us and the other co-sponsors
*  to help him really flesh that out and something in this space. But I think Senator Renner became
*  concerned, as a lot of us have, that it seems like the pace of development in AI is really
*  fighting and I think has a ton of benefits. But I think we are starting to see some of these
*  glimpses of these really large models have pretty dangerous capabilities. I think we saw a similar
*  discussion at the AI Executive Order and that's important, but I think there are things that you
*  can only do in statute and California is an important jurisdiction and it seemed like a
*  really great opportunity and I've yet been really impressed with him as an author. He's a really
*  smart guy and has been really into the details on this and it's been really just a pleasure
*  to work with and hopefully our other co-sponsors of the bill, the Economic
*  Security Projects and Encro Justice. Yeah, just speak a little bit about my own worldview about
*  this. I think that I am just very uncertain. I think one of the main things which I think is
*  that I have a lot of excitement about AI. I think these tools are really amazing. I used GPT-4 and
*  Papri Opus on a daily basis and I'm a big fan of these models and think that they have a ton of
*  potential to improve our lives and work and to improve, bring a lot of innovations that we
*  desperately need to solve really hard problems. I'm also someone who in general is very, I think,
*  critical of unnecessary regulation or bureaucracy. I'm a big fan of the kind of remainder housing
*  work and trying to build additional nuclear energy, trying to reform the FDA. A lot of the things
*  that I think you and I on a separate podcast have probably chatted about and not a lot to agree on.
*  I just happen to think that really the largest AI models that in addition to their potential to do
*  a lot of good, I think they might be able to do just really powerful things and that we should be
*  having some amount of precaution and kind of leading it totally up to the companies and
*  voluntary measures about whether they want to be doing testing. I think we're hearing from both in
*  the labs that there's just tremendous commercial pressure to get them down the door as fast as
*  possible. And I think there needs to be a role for governments to say, let's be thoughtful here,
*  let's get the benefits, but let's not make sure that we're raking ahead and causing catastrophe.
*  I think the one thing I want to say is I'm a very big fan of the general structure of the bill and
*  we'll defend that very strongly. But I think a lot of the details about the big point, like
*  the complicated issue of the bill has already been amended. It will continue to be amended and
*  really speaking in general, I'm going to be speaking for myself instead of for any big
*  reaction fund. But on this area, I'm confident the author also shares this that he's really
*  interested in amendments. We want to get this right. There are a lot of different technical
*  terms and definitions. We have our clear intent, but we want to make sure that those definitions
*  reflect what our intent is and really to target some specific very dangerous concerns of models
*  while really generally letting the AI ecosystem bloom and flourish has been continued. So that's
*  my general standing. I think I'm worried about a variety of different risks. I think I'm worried
*  about a super powerful, intrinsic model that doesn't cover identity, but I think more correctly,
*  I'm often worried about the ability for bad actors to use this for malicious biotech,
*  really severe attacks on critical infrastructure. I think some of the risks that were identified
*  in the Biden executive order from Julia's foundational model, I think similarly,
*  novel DDRM developments, attacks on critical infrastructure and models that deceive or self
*  replicate or are otherwise really hard to get back control of. So that's what we're broadly
*  thinking, also care about a lot of other issues also really important within AI and a lot of
*  the things about deepfakes and the future of work and racial bias in government systems and all those
*  are super important that I think are also important to actually have. But this bill is really focused
*  on kind of these catastrophic national security risks from the largest models in criminal and DNA.
*  So what was attempted in the Biden order, but that's probably where I'm tied from. I think that
*  we do need to take some action here. I think that like we really are on an exponential curve and I
*  think that there's some temptation to be like, this is really complicated. We could wait a year,
*  we should wait until we can make everyone incredibly comfortable with it. I think
*  legislation takes a long time. We already have in the legislation and as I think I'm probably noted,
*  some like delayed implementation or even after the bill is passed, it takes a while to get
*  government set up around these things. And I really do think that we're going to get all
*  possible to pass something this session so we can start that process going. And I really think that
*  not being interpreted on Twitter and think that, but I think there are already a lot of
*  confessions in this bill and trying to be a moderate, not maximum local. We can get more
*  into all this. That's probably where I'm coming from. Quick follow up question. When you refer
*  to the author of the bill, is that state Senator Wiener? Senator Wiener, yeah. Terminology is weird
*  because at the federal level you refer to the bill author as a sponsor, which is the opposite
*  to when I say the author, Senator Wiener. We have provided like technical advising and not feeling
*  without amendment to language, but ultimately him and his legislature have been very involved in
*  this and it made the final call on all the ingredients in the direction of the bill.
*  And yeah, the line we're going to the author, I'm referring to Senator Wiener and his team.
*  Cool. Okay. Thank you. Dean, you are research fellow at the Mercatus Center.
*  You've been a previous guest on the podcast when we talked about all things brain computer
*  interface. So obviously a big and pretty deeply technical engagement with technology, AI, all these
*  sorts of things, though your day-to-day focus is on policy analysis. Give us a sense of where you're
*  coming from on this. Sure. So first of all, I am at the Mercatus Center. That is a think tank
*  affiliated with George Mason University. We are a 501C3 and what that means is we're in the same
*  tax status as a university, but we try to educate the public and policymakers about
*  matters of public policy using insights from academic research. So that's where we come from
*  in a broad sense. Specifically on AI, I think that AI is a general purpose technology, quite possibly
*  the most powerful general purpose technology that humans have ever invented. Certainly the most
*  powerful one of sort of the recent wave, at least it could be, of general purpose technologies,
*  computing, the internet, et cetera. As a general matter, we don't tend to regulate downstream use
*  of general purpose technologies with a centralized government regulator. That's just not typically
*  how that's done. There's not a department of electricity. There's not a department of the
*  internet. There is FERC, the Federal Energy Regulatory Commission. They set standards for
*  interstate transmission of electricity and all kinds of stuff like that, but they're not regulating
*  my downstream use of electricity, which surely can be used to do all kinds of dangerous things,
*  nor is there a department of computers. I think that if we want the AI revolution to be successful
*  and to broadly benefit people, then even to speak of AI policy as a discrete area of public policy
*  is probably going to seem a little bit anachronistic in 20 years. That's my hope.
*  I wouldn't necessarily predict that, but that is my hope, because I hope instead that AI is so
*  integrated throughout our economy that it is regulated in all the myriad ways in which
*  economic activity and public policy interact with one another. I think that creating a
*  centralized regulator is just likely to lead to all kinds of political economy problems
*  that will hinder the impact of AI, the good impact of AI, in ways that I can't even predict.
*  I'm not really sure that it gets us that much more safety either in the long term,
*  in particular because most of the concerns we have here, CBRN, cybercrime, these things,
*  certainly they could come from domestic actors. Nathan, Calvin, you referenced national security
*  concerns. I don't believe that the CCP cares about qualifying for a limited duty exemption
*  under the state of California, nor do they care about the NIST AI risk management framework.
*  So I'm not really sure that this gives us safety in any meaningful way from those types of actors.
*  And instead, I think to the extent it hinders AI progress, it actually creates danger, which is
*  what regulation often does of these kinds. It creates a false sense of security, but in fact,
*  as a tendency to lead to long-term danger. I really think that nuclear energy is a good parallel to
*  think about here, except for at least nuclear plants. There were some nuclear incidents that
*  raised a lot of concerns across the world. There is no AI Chernobyl so far. There is no AI Three
*  Mile Island. And we created the Nuclear Regulatory Commission and illegalized nuclear energy
*  effectively after some major incidents. There have been no such incidents here. So if we're
*  going to legalize AI progress, I'd at least rather do it after we have at least one concrete example
*  of a major problem. I just think not only is it premature, but it's the wrong way to go in general.
*  So that is, I just want to be very clear though. I am not opposed to governments passing laws with
*  the word AI in them. I'm not opposed to a vigorous government role in the regulation of AI. I just
*  am skeptical of the idea that there is one big thing you can do, one major bill you can pass
*  that will do what you want it to. Instead, I think the answer is every agency internalizing
*  the risks and benefits of this technology in a serious and considered way and taking steps
*  within their own jurisdictions and their own areas of expertise across the city, state,
*  and federal levels. I think there's not one big thing. There are five million small things.
*  And that is the work that I hope we can do. And I think that frankly,
*  centralized regulators hinder that work. Thank you both. Steve, to introduce you briefly as well,
*  you are a exited entrepreneur. Most people will have interacted from time to time with one of
*  your creations, which is now known as Google Docs. And having said that line of work, you're now
*  focused on raising the standard of discourse related to AI, which is obviously a noble pursuit.
*  I think we're in a pretty good position to hopefully have some elevated discourse here,
*  because for one thing, I don't hear very extreme positions. Like you will sometimes go online and
*  sign people saying things like, oh, who cares if humanity goes extinct, the machines will be more
*  worthy in some sense than us anyway. Not hearing anything along those lines. I'm also not hearing
*  argument from protection of Weaver employment, so to speak. I'm not hearing anybody saying that
*  the current mode of production and the current employment relationships and dynamics are the
*  primary concern. It seems like everybody on this call is broadly pretty enthusiastic about the
*  upside of AI and technology in general and broadly, and certainly this is true for myself as well,
*  skeptical about government overreach into areas that are probably best left to develop on their own.
*  Do you have any other thoughts on how we can make sure we have the most elevated discussion that we
*  can? I think the first step is just bringing the other people who have a positive attitude and are
*  interested in engaging and with an understanding that we're going to have a positive discussion.
*  I think you've done a great job of doing that up. And I think really looking forward to seeing
*  Nathan and Dean dive in. And I think we've already seen just from everyone's introductory remarks
*  that we're here to kind of talk about reality and talk about things in a reasoned way. And
*  that's 90% of the battle right there. Hey, we'll continue our interview in a moment after a word
*  from our sponsors. The Brave Search API brings affordable developer access to the Brave Search
*  index, an independent index of the web with over 20 billion web pages. So what makes the Brave
*  Search index stand out? One, it's entirely independent and built from scratch. That means
*  no big tech biases or extortionate prices. Two, it's built on real page visits from actual humans,
*  collected anonymously, of course, which filters out tons of junk data. And three, the index is
*  refreshed with tens of millions of pages daily. So it always has accurate up to date information.
*  The Brave Search API can be used to assemble a data set to train your AI models and help with
*  retrieval augmentation at the time of inference, all while remaining affordable with developer
*  first pricing. Integrating the Brave Search API into your workflow translates to more ethical
*  data sourcing and more human representative data sets. Try the Brave Search API for free for up to
*  2000 queries per month at brave.com slash API. I'm going to key uses generative AI to enable you
*  to launch hundreds of thousands of ad iterations that actually work customized across all platforms
*  with a click of a button. I believe in Omniki so much that I invested in it and I recommend you
*  use it too. Use Cogrev to get a 10% discount. I want to pick up Nathan Labenz on something you
*  said about protecting Weaver employment. This transformation that is coming will be an economic
*  transformation. I don't think anybody has intelligently modeled the labor market impacts
*  of AI. I know some people who are trying. I don't know how possible it is right now,
*  but it's going to be a struggle because there are a lot of areas of the labor market that are
*  protected with barriers to entry and barriers to entry will be erected to keep AI from from
*  entering, whether it's driverless cars, medicine, law, there's a thousand things
*  where, you know, current industry groups are going to try to continue extracting rents.
*  And that's going to be a real struggle. I think that will be a huge policy struggle over the
*  coming decades. And if you don't believe a centralized regulator will be a weapon that
*  will be used against AI for diffusion, I think that you have not been paying appropriate attention
*  to the way that government has generally conducted itself at the state and federal level for my
*  entire lifetime, at least. So I think that's one thing that's important is that I am very concerned
*  about maximizing the benefits. I want to do that. And I think a centralized regulator presents all
*  kinds of challenges to doing that. That's just one additional point I would make.
*  Yeah, I think probably you'll have agreement in the general sense that we can find plenty of
*  examples of government gradually overgrowing the original intent of the legislation under which it
*  operating. Maybe we can approach that from the standpoint as we get into the details of like,
*  how can we permit that from happening in this case? I often say, would be blastly like, why should
*  we want any of this? Isn't this all just going to be terrible? I'm like AI doctor. And that's my
*  thing. When I first got my early GBT four access, I couldn't stop asking it all these medical
*  questions. I have that many, fortunately, it was really good and so convenient. And that is,
*  in my view, almost the canonical thing where if we end up in a world where we prevent people from
*  getting quality medical advice from AI, whether out of irrational fear or band cost benefit analysis,
*  or protecting doctor employment, it seems like we'll have made like a very bad mistake. So
*  if that's something we can agree on, then we can get into that tendency, which definitely is out
*  there could be mitigated or prevent bear. I guess I just want to say that I agree with you that there
*  are a lot of really large benefit there. And I think it would be quite bad to not get them. And
*  I think some of that will involve pages in the future work. I think among the coalition that
*  we're working with here, there is like a variety of different views with it. And I feel like to
*  come to be like this bill is not necessarily taking a position either way from the like labor
*  market future and whether you could be trying to have the time. I have my own views about that.
*  But I think either way, I just don't think what this bill is about. And if you're someone who is
*  either concerned or optimistic about AI displacing human jobs, that I don't think the bill is
*  necessarily going to have a lot to say in either direction, to be totally honest, just want to
*  clarify clarify that. And then also, and I think we'll dig a lot more, I would push back on the
*  idea that this bill is a centralized regulator. It's not a licensing regime and the only
*  government body that is created a new vision of the California Department of Technology,
*  you just like receiving a certification, it doesn't have a woman in power, it doesn't issue
*  licenses, most of this a liability regime for the attorney general. So that still is probably more
*  centralized than you would like. But I think there are other proposals that really are like
*  centralized AI regulator, we're getting pre approval to be able to do training run,
*  where they really are doing the talk themselves being a lot of that. I agree that there are really
*  big challenges of that of having government capacity to really understand all the things
*  going on and the cutting edge and doing that effectively. And I just want to say I also I
*  think have concerns about the efficacy of a centralized regulator, particularly given the
*  existing technical expertise or lack thereof within governments. And so I'm sure we'll continue to
*  debate about how we're characterizing the bill. But I also have hesitation about a centralized
*  government regulator here. And I don't think that's actually what this bill does.
*  Only thing I would say is the Frontier Model Division in a recent amendment to the bill
*  does have the power to change the compute threshold, it has the power to change the
*  specific standards for models that fall under its jurisdiction. So that already is starting to look
*  a little bit centralized. And the other point that I think is under discussed here is that we talk
*  about repeating the mistakes of nuclear energy. The Frontier Model Division, because of California's
*  current $60 billion budget deficit, the Frontier Model Division is funded by fines and fees that
*  are levied on AI companies. And that's exactly how the Nuclear Regulatory Commission is funded,
*  the regulators, the regulated entities pay for their regulation. This obviously creates regulatory
*  capture potential, right, like on its face, but their salaries are paid for by the people they
*  regulate. But in addition to that, it insulates the Frontier Model Division from the normal
*  appropriations process in legislatures where maybe five years from now, we're not as worried about AI
*  risk, maybe all the concerns of folks like Jeff Hinton don't pan out. And we want to lower the
*  budget for that. But guess what? You can't because the FMD funds itself, or it's inconvenient to,
*  because the Frontier Model Division funds itself from the AI industry. And so not only does that
*  insulate it from the budget process, but in essence, it insulates it from the tradeoffs of
*  the political process of democratic input. And I think it's probably worth California maybe thinking
*  seriously about why they have a $60 billion budget deficit before they create a new centralized
*  regulator for the most promising technology in the world. But also, I think that it would just
*  create all kinds of problems, just the funding alone. So come back to this question of how
*  centralized the regulator is in a minute. I have done my own close reading of the bill and broken
*  it down into basically six things that I would say it does that seem to matter. And we can then
*  dig into the details of each one, either one by one, or you can add categories to the list if you
*  think I missed any. First one, as I think about it, is it sets a standard for what are we talking
*  about here? What is the actual risk that the bill is trying to address? There, it sets what I would
*  say is a pretty high standard of either mass casualties or $500 million or more in damages to
*  things like critical infrastructure. So that, for me, was like, okay, that's a pretty big thing,
*  right? Like mass casualties, $500 million in damages. I think if you were to go ask most
*  people on the street at what level of harm would you think the government should start to get
*  involved in AI? They would probably come up with something that is lower than mass casualties or
*  $500 million. I don't know if, Dean, you would say that you would be worried would be subject to
*  future discretion or lowering of that threshold, but for me, it's a high number. So I don't think,
*  I don't know this for a fact. I don't think that the bill allows the frontier model division to
*  lower the hazardous capability threshold. I don't believe it can do that. However, I would say the
*  following things. Critical infrastructure is an extremely deceptive term. Critical infrastructure
*  is, it's a little bit Soviet in the sense that critical infrastructure is in fact a larger
*  category than infrastructure. So critical infrastructure includes things like hospitals,
*  all financial services, all telecommunications infrastructure. It is the majority of US economic
*  output. So when you say critical infrastructure, don't think that you're talking about a small
*  subset of things like nuclear power plants or something. You're actually talking about
*  most of the US GDP. $500 million is a lot of money. At the same time, cyber crimes worldwide
*  in 2023 were responsible for about $9 trillion of damage. Oh, 500 million not indexed to inflation.
*  That could be not so much money. It already is not so much money. You could probably,
*  depending on how you calculate it, if you knock a wastewater treatment plant out, you don't blow
*  anything up, you don't do anything like that. You just knock its computers out for a couple of days.
*  You've probably done $500 million of damage to critical infrastructure. Obviously, we don't want
*  people knocking out wastewater treatment plants. I'm not saying that's a light thing. I'm just saying
*  that you should think very carefully. Wastewater treatment plants go down. They go down a lot.
*  Cities issue boil water notices on a regular basis in this country. We shouldn't think that's
*  some unprecedented thing. Just because it happens to be caused by A, it's just a little like,
*  I don't know, it's the new thing. But, oh, it was caused by computers. I don't know. I think that
*  it's a big deal and we should do everything we can to protect our critical infrastructure and
*  be very serious about cybersecurity. But I actually think $500 million is not as big as it sounds.
*  So to read the definition from the Bill of Critical Infrastructure, it says,
*  critical infrastructure, quote unquote, means it is pretty broad, assets, systems, and networks,
*  whether physical or virtual, the incapacitation or destruction of which would have a debilitating
*  effect on physical security, anomic security, public health, or safety in the state.
*  I think we are probably really relying on the number there more so than any sort of narrowness
*  of definition because, as you said, a lot of things would have the potential for a debilitating
*  effect on physical, economic, security, public health, safety. This is maybe more of a nitpick
*  than anything. I'm not sure it really matters. But for what it's worth, that 9 trillion cybercrime
*  number, I just came across that and I don't really buy it to be totally frank because that would be
*  like pushing 10% of global GDP. If global GDP is $100 trillion and US is like $20 trillion,
*  where that number has been flying around, I've seen that actually used on me on both sides of
*  the debate now because I've also seen people on the AI safety side being like, look how big the
*  problem is already is. And I'm like, there's no way that we're losing 10% of global output,
*  the cybercrime. That just seems crazy. I actually saw that a fundraising don't
*  strip too long ago for an AI assurance tech company and I was like, I'm going to have to
*  downgrade your market size a little bit. But well, no, but because of I don't know how that
*  number is calculated, but because of the broken windows phenomenon, some of that $9 trillion might
*  be accretive to GDP, right? It's not necessarily that it's a $9 trillion reduction in GDP. Costs
*  of things are accretive to GDP, right? If I break a window and I fix the window, that's GDP. I would
*  just point that out. And I think, but I think that that $9 trillion figure, it might be like
*  totally FACACTA, that's entirely possible. That should tell you something about $500 million
*  in the context of cyber attack damage. How do you calculate it? I don't know. You could,
*  there's probably ways you can fudge those numbers. Yeah.
*  So let me just guess we take a step back for the moment. Dean, I'll address this question to you.
*  Should we be regulating anything? Is anything bad at alert scale,
*  plausible to happen? Or in other words, are we debating exactly what's the best way to write
*  this bill? Or are we actually debating, is there even a problem here?
*  So I would definitely say we should be regulating things. I think I have a rather circumscribed
*  perspective on that which government can effectively regulate through a single statute.
*  I think there are many different things that government should be doing to incorporate AI
*  into various criminal statutes and scale up their enforcement of things and
*  increased defenses and all that kind of stuff. But is your question, what do I think the outcome
*  would be if the bill passes? So the bill doesn't pass if no bill passes. In other words, we're
*  talking about the wastewater plan being knocked offline. And one perspective, like that could be
*  a really big problem, could cause, I don't know, half a billion dollars in damage.
*  From another perspective, it's a big economy and a big world and big problems happen. And maybe if AI
*  was a little bit involved there, it doesn't really mean that was fundamentally an AI issue.
*  Taking a step back from the specifics of half a billion dollars, should we be worrying about AI?
*  Is there a problem that we could be regulating, whether or not this regulation is the right way
*  to go about it? I get the sense you're asking about like catastrophic risk. Am I worried about
*  sort of existential type major, major risks from AI? I'm not trying to go all the way to existential,
*  but just big stuff like fiber tech that brings the whole US grid down for a month or knocks all the
*  wastewater plants offline or things that start happening in a big way that traditionally have
*  not happened in a big way or just a lot easier to happen or rather, should it be half a billion or
*  should it be two billion? Is there a heat there as you see it?
*  So I share Nathan Calvin's and I think probably Nathan LeBenz's general uncertainty,
*  like fairly strong uncertainty about the future trajectory of AI capabilities.
*  So I would say, yeah, like we should take the concept of a cybersecurity attack that takes
*  major electricity grids down for a long period of time. I think that is definitely something
*  that we need to be worried about. I question the wisdom of passing a law that says it is illegal
*  to take down the US electricity grid because I suspect that it already is. And I think that
*  basically if we pass a law that says it's illegal for the AI transformation to not go well,
*  that doesn't do much for me. That doesn't advance the epistemic ball in any important way
*  because we all want it to go well. We all want this transformation to go well. We don't know
*  what that means. Nobody knows what that means. And if the government just passes a law that says it
*  has to go well, then we all have this epistemic uncertainty. And now you've introduced lawyers
*  into the equation. Not that lawyers shouldn't be part of this whole dynamic, but it's, oh, great,
*  cool. We can go to court and talk about this. Sounds fun. I question the wisdom and really the
*  efficacy of laws of this kind, but I would definitely agree the AI existential risk,
*  inherent risk. I'm not all that worried about CLAWD4 itself developing its own motives and
*  trying to kill all of humanity or whatever. I think that's a little bit of a Jijun debate,
*  but I'm definitely worried about future models being used for all kinds of things. And cybercrime
*  is at the top of the list for sure. So yeah, there's definitely a there there in my mind.
*  One thing that I think could be helpful is that I agree with that in a lot of settings, the
*  right way to handle regulation is to focus on kind of the downstream uses of the technology.
*  I think it's fair to say, look, we're not going to place a bunch of burdens on email or the
*  internet. We're going to focus on people who use those tools to do that, then go after them. And
*  that's appropriate. And our laws are able to handle that. I think there are lots of settings
*  where that works. And I think that's appropriate. I think there are settings where that doesn't
*  necessarily make a lot of sense. We have things like in nuclear policy where we don't say, oh,
*  we're going to go after terrorists who use plutonium or something. We know the people who
*  are enriching nuclear materials have some obligation to secure those materials and be thoughtful
*  about it. I think similarly, I'm very enthusiastic about biology. Biology has tons and tons of
*  benefits. And I'm very optimistic about a lot of different factors. I don't think that therefore
*  means like we should not think at all about gain of function research and also doing lots of
*  them in a function research. So therefore our solution should be, hey, let's just go after the
*  individual terrorists or folks who are doing that. I think that we have to have some process also
*  higher up on the train. I agree that I think that this is a difficult this way, but I think in a lot
*  of cases it makes sense to focus on the downstream applications. I just think that fully advanced,
*  the largest AI systems that have a potential of having these pretty devastating impacts are one
*  of the small handful of technologies where we do need to be thoughtful about whether regulations
*  are necessary. I agree with that general framework you've laid out. The fiendishly difficult aspect
*  of this though is that plutonium is not a general purpose technology. It's not a consumer technology
*  that I might use on my phone. So that complicates it considerably. I think I agree with you that
*  there is room for regulation and a need for regulation somewhere between the downstream
*  use and the creation of an AI model. I actually think AI might just be a little too
*  low in the value chain or the production chain of threats one might say. So a concrete example is
*  the Biden administration just a few days ago released what in my mind is a very productive
*  framework for nucleic acid synthesis machines. Yeah, where they basically said if you're going
*  to do federally funded life sciences research you have to use machines, DNA or nucleic acid
*  synthesis machines from specific providers that will comply with these standards and those standards
*  include that sequences that you want to produce, genetic sequences you want to produce have to be
*  screened against a database of known threats. Great, excellent. But I think that sort of going
*  one level removed from that to like the statistical analysis of genetic data sets
*  and regulating that, you know, I'm an American and that does raise constitutional problems in my mind,
*  right? I think I do have a constitutional right to do statistical analysis on genetic data. I don't
*  think I have a constitutional right to synthesize DNA and I think that's like a pretty clear
*  distinction that like at least most Americans can intuitively rock. Constitution as V. Moshowitz has
*  said to me, constitution is not a death pact so I'm not going to rely just on a constitutional
*  defense. I will also just say that it's too low to be a productive regulation. It's like regulating
*  metallurgy rather than missiles. Hey, we'll continue our interview in a moment after a word from our
*  sponsors. Hey all, Eric Torenberg here. I'm hearing more and more that founders want to get profitable
*  and do more with less, especially with engineering. Listen, I love your 30-year-old ex-fang senior
*  software engineer as much as the next guy, but honestly I can't afford them anymore. Founders
*  everywhere are trying to turn to global talent, but boy is it a hassle to do at scale from sourcing
*  to interviewing to on the ground operations and management. That's why I teamed up with Sean
*  Lanahan who's been building engineering teams in Vietnam at a very high level for over five years
*  to help you access global engineering without the headache. Squad, Sean's new company, takes care of
*  sourcing, legal compliance, and local HR for global talent so you don't have to. With teams across
*  Asia and South America, we can cover you no matter which time zone you operate in. Their engineers
*  follow your process and use your tools. They work with React, Next.js, or your favorite front-end
*  frameworks. And on the back end, they're experts at Node, Python, Java, and anything under the sun.
*  Full disclosure, it's going to cost more than the random person you found on Upwork that's doing two
*  hours of work per week but billing you for 40. But you'll get premium quality at a fraction of
*  the typical cost. Our engineers are vetted top 1% talent and actually working hard for you every day.
*  Increase your velocity without amping up burn. Head to choose squad.com and mention Turpentine
*  to skip the wait list. It's a good time for me to interject the second point of the six and maybe I
*  should even just run all six down real quick. We covered point one which is the standard of
*  the scale of harm that would be of concern. That is again, mass casualties, $500 million
*  damage to infrastructure. The second point is how would you reasonably expect that you might be
*  playing in that regime? Here the bill uses the same threshold as the executive order, at least
*  on the first definition which is 10 to the 26th flops. So if you're going to train a model that
*  is going to use 10 to the 26th flops going in, then that is presumed to be a covered model. That
*  is to say, if you can't make a positive assertion or good technical reasons that this thing is going
*  to be safe, meaning it's not going to be capable of causing the sort of downstream mass casualty or
*  $500 million worth of harm, then you have to do certain things. And those things are you have to
*  implement a lot of safety standards during the training process. So the weights can't leak to
*  the public or to the Chinese government or whatever. That includes having the ability to
*  do a full shutdown. I'm not exactly clear on what would happen to do a full shutdown, but the idea
*  is basically you want to have an off switch so if things get crazy you can flip that switch and
*  stop the process. You have to at the end of that training process, again this is assuming you didn't
*  have good technical reasons up front to say that it'll be safe, then at the end of that process you
*  then have to do the testing and say okay now that we've made this thing let's see what it is actually
*  capable of. And at that point you have another chance to say okay actually we did all this testing
*  and now we can say even though we didn't have a theoretical reason up front we now have a
*  practical reason based on testing that we can confidently assert that it's safe. And then if
*  you still can't do that you have to deploy only with surrounding safeguards which are meant to
*  prevent these large-scale harms from happening. Basically you have three tiers. You could say I
*  have a theoretical reason and I really would love to see that emerge. That would be the real silver
*  bullet which doesn't seem like anybody's super betting on at the moment would be like a conceptual
*  breakthrough that hey if we do these things, design the system this way, create the data set
*  in a particular way, whatever, then we can just be confident that this is going to be safe from the
*  get-go. Now everything is awesome downstream. There's you know minimal imposition right from
*  the bill if you can make that kind of assertion. But if you can't do that then you got to hammer
*  it with a bunch of tests, find out what it is in fact capable of. Again you have a forecler hey
*  we tested as much as we could. We determined that it can't do these things so we're or if still not
*  then we have to have guardrails which would be like moderation, monitoring of usage, and potentially
*  also not releasing the weights into the wild because this is probably one of the more contentious
*  pieces of the bill that if you do release something into the wild then modifications of it downstream
*  are still your responsibility. So you have to be able to assert not only that it will do this
*  in the form that we release it but also that it can't be like relatively easily modified to that
*  state. And this is where I would refer listeners back to the FAR AI episode that we did with Adam
*  Gleave because they showed that it really doesn't take much in a lot of cases. If the capability
*  is in the model then the sort of RLHF finishing that they are generally treated with at the end
*  of training is maybe enough as long as that stays in place but very little fine tuning typically
*  downstream can reverse those sorts of safety fine tunings even in a non-malicious way. That was one
*  of their most notable findings is that even just taking a small chat data set and saying hey we
*  want to have you chat in a particular way not training it to do bad things but just omitting
*  the fact that it's supposed to refuse bad things that that final fine tuning step can in a way that
*  the developers didn't even intend reverse that. So it's on the the model trayer to make sure that
*  not only is this thing not exhibited but that it can't be easily unlocked with downstream fine
*  tuning. So that's like the big one and then the one other thing that's worth noting there is
*  and I think this is also where a lot of the debate and the kind of heat is right now is that 10 to
*  the 26 flops is like clear enough but then there's also this other definition that's if you're going
*  to train a model that could reasonably be expected to be similarly powerful to what 10 to the 26
*  would get you then you're also in that regime and that's where like I don't really know if I'm in
*  that regime or not. So anything that you guys would clarify or refine on my description of that
*  obviously a lot of people have offered their summaries but how would you grade mine? I think
*  good summary definitely hear you on the point that I think one part that that is harder to know
*  is saying I know I'm treating a model below 10 to 20 but I'm not sure whether it will be as capable
*  as a model at data but we are at 10 to the 20th and 2024 how can I make sure whether I then have
*  digging things and the author added a vision which says that basically if you are surprised by how
*  good the model is that's fine you're not in trouble just like institute those fake cards
*  after discovering that you were surprised and that's okay and putting just some understanding
*  that like maybe in the future we will have more accuracy about exactly how high performing the
*  model will be prior to training but at least in the current regime we recognize that you might
*  not know that until after you've started until after you train the model and started doing various
*  things about the difficulties. Just to give people a sense of what is 10 to the 26 flops
*  first of all it's not entirely known whether any already trained model has hit that threshold or
*  not if it has it's almost for sure happened that either OpenAI or perhaps Google the biggest meta
*  trained still in training llama 3 is expected to be halfway there it's funny because I think it's
*  been estimated at 4 times 10 to the 25 which is sounds really close but it's still only 40 percent
*  of the way there because orders of magnitude it's the last one is always the the one that
*  matters the most so what meta is spending on this llama 3 400b still in training now is at retail
*  price something like 200 million dollars in compute they have published a bit of guidance on how many
*  h100 hours they've put into the first two models that they put out they put in over six million
*  h100 hours into the 70d version and they're extrapolating andre karpathy had a nice analysis
*  of this which i'm taking as closest thing available to ground truth for these purposes
*  and then just putting a price on what in the market today an h100 hour if it's not dramatically
*  subsidized pushes usually about four dollars an hour for an h100 so roughly speaking you do the
*  back of the envelope math you're getting to about a 200 million dollar compute budget for the current
*  llama 3 so you're in you're in somewhere in the sort of hundreds of millions to half a billion
*  dollars just to buy the 10 to the 26th flaunt now i know dean you may have some thoughts on this
*  because i know you've written a bit about how you think this is not a great way to frame this in part
*  because efficiency is definitely a major driving force here i don't know if there's any
*  alternative that you would propose if we were going to keep the general structure of this is
*  there anything other than a slot number that would make sense to anchor off of but in any event that
*  that's a sense for the budget i i would say a couple things about the compute threshold first
*  of all yes uh it is the case that oftentimes regulation will be triggered by a firm's size the
*  revenue that they have the number of customers that they have whatever that might be firms as a
*  general rule want to get bigger when you impose thresholds like that there's ample evidence in
*  the economic literature that when you impose thresholds like that you create all kinds of
*  weird problems will firms where firms will just skate right both the threshold the difference is
*  that firms as a general rule want to be more compute efficient uh so with a threshold like
*  this you are swimming against the current and i think you're ultimately incentivizing the frontier
*  model division to lower the threshold over time um and they can do that under this bill um and the
*  other thing is i'm just for me it's not entirely clear what 10 to the 26th is based on it is i've
*  never really heard a scientific basis for 10 to the 26 blocks so nathan if you have any answers
*  there i'd love to hear it but that to me is the main thing about the threshold but i think we
*  shouldn't be regulating at the model layer and i think that the fact that it is so hard to come
*  up with good thresholds and make this stuff legible to the state is like evidence in favor of my
*  argument so i don't have a good alternative solution my alternative solution would be not
*  doing this and doing other things instead it would be not doing nothing but doing other things
*  great this just to briefly justify where the 10 to the 25th number comes from i think the idea
*  here is just that this is the next generation of models so not gp4 and floppy or basically really
*  the ones that companies might be releasing potentially later this year or next year and
*  they come from the understanding that like we don't think that any of the current generation
*  model models are capable of causing these really to dear harms but we think that there are like
*  some glimmers or warning signs that future generations that they should be like keeping
*  their minds open to the potential that they could and i think that's where that is coming from this
*  stuff like open ai's study on bio risks from gp4 saying look maybe it helps like a little bit but
*  it's really not very much it is not the stuff that you couldn't find on on google but saying
*  don't look at the crane larger model keep our mind open to the fact that it really might provide
*  quite substantial assistance to a lay person in creating a really nasty biological weapon
*  the idea is we have some sense about what the abilities are of current models i think on the
*  lower threshold thing i think you're talking about it's guidance about what sort of threshold might
*  be done like covered under like b it's not a formal rule it guidance about it there was
*  consideration about giving them rulemaking power to change the threshold and we didn't do that
*  yeah i guess i i'd hear that and i think if we were debating all of this and the current frontier
*  was gpt3 or gpt3.5 we would 100% be saying this about gpt4 we would and and in fact people did
*  people said it about gpt4 and gpt3 and gpt2 and not to say that it's not eventually going to become
*  a thing i i'm not dismissing that entirely but it does seem to me like a vibes-based argument for
*  imposing a threshold like this of it's the next generation and we think that could happen maybe
*  but otherwise we have no empirical or scientific basis for these claims i just think we should not
*  create an unprecedented regulatory regime for software and that is what this is it is an
*  unprecedented we have never done precautionary principle generally applicable precautionary
*  principle or liability regime for software never done it in the history of this country this would
*  be the first time software has given us a ton of economic and social benefits so i think the bar
*  for that is high and i just don't think we think the next version might be dangerous
*  is a sufficiently high bar for something so unprecedented i do think it's fair that if we
*  were having this conversation around when like gpt3 was released that people might say maybe there
*  are some warning signs and i think basically like open AI and then profit and the companies basically
*  did the things we're describing in the bill right like they cut to them it's self-determined like hey
*  that's actually duty for nasty things and we can release in the form of that okay i think it would
*  have been fair then say haters or potential they should do these types of things which i think they
*  acted and then really the model and everyone used to get including myself and it's great
*  because i think there's some level where you should just have uncertainty i think at this level in the
*  next generation we're at that level i think it's debatable and i think from where they were sitting
*  with gpt3 i think they should have kept an open mind to the fact that he formatted obviously
*  turned out it didn't i'm glad it didn't and maybe someone could made a more bulletproof argument for
*  why it definitely wouldn't again i agree that this is a weird situation we find ourselves in
*  different from a lot of the things i don't know we don't have with like email the creators of email
*  and the leaders of all the email companies signing open letters saying it might kill everyone
*  fundamentally different thing here we have these rules of thumb and we think have worked but i
*  think we really are like in videos worth more than saudi aramco or what video like went down a bit
*  this is like a crazy unprecedented wow thing that people are promising might be able to solve all
*  these crazy problems and do all these insane things and i think it's not unfair to say that
*  are like normal intuition for how to regulate general focus technologies or software
*  i think there's reason to think that these really incredibly large models that capability that we
*  don't understand is an area where our intuition should run yeah two important things there from
*  my perspective one is that this is qualitatively an unusual situation all the effort that is going
*  into making the models agentic or capable of mid-range planning and overcoming barriers
*  finding solutions to the first problems that they encounter this is definitely a horse of
*  a different color compared to normal software we're really on the verge of really entering that
*  moment gbt4 can't do much on its own it falls down at the kind of friction point most of the time and
*  yet as zuckerberg recently said in his dracation review like you observe these weaknesses you have
*  this process of building a lot of scaffolding to try to overcome those weaknesses but you basically
*  have a tick-tock development cycle where you build the scaffolding and you also try to build the next
*  generation of the model so you don't have to have all that scaffolding and so it seems pretty clear
*  that the next generation coming out of open ai and this has been broadly reported on is going
*  to be a lot more agentic and i basically define that as saying if you give the model short
*  instructions it can unpack those into a plan and then go pursue that plan until it either succeeds
*  or dies trying or somebody comes and turns it off right but the there is a qualitative difference
*  to the sorts of things that we're starting to get into now that's also measurable just in terms of
*  like how close these things are to expert performance in a lot of different domains but
*  they're already better than the average human they're closing in on human expert and so there's
*  not that much farther to go either progress would have to like totally stall out which definitely
*  doesn't seem like it's a bet anybody really wants to make either commercially or safety wise
*  or something else really weird would have to happen for us not to be entering into very
*  unprecedented territory the other big thing is key point is the leading developers basically are
*  doing these things right if we consider the live players today to be open ai google and entropic
*  those would be like definitely my top three global model developers entropic has led with
*  a responsible scaling policy when i had a pretty robust i would say could have been better but
*  nevertheless non-trivial effort in gpt for safety a year and a half ago clearly they're bringing way
*  more energy to that now with the next generation and google if anything gets criticized for being
*  too slow and conservative so it things like all of the leading companies are basically already
*  doing something like this so then you would say okay who is this bill really for and it seems
*  like it's for meta maybe it's slash like the next half dozen companies that might say hey we do want
*  to participate in this multi-hundred million dollar training gale game and it's not that many
*  companies that could plausibly do that and and expect to get anywhere close to the state of the
*  art but there are a decent number so it seems like it's really for them and maybe also for the
*  decisions that they might make to open source if they can't which again comes really right back to
*  meta the decisions that they might make to open source if they can't make certain assurances about
*  what might happen with light modification downstream fair analysis i think that's fair
*  one thing that has been the subject of a lot of conversation online and i just want to
*  bet the record trade is that there's been a lot of folks saying you're trying to make it so that no
*  entrants can compete with open ai and antropic and google and that this bill is basically like
*  secretly authored by them they're the ones doing it that is just not true they also have like
*  provisions around like whistleblower protection but different things they have to do that they're
*  not happy that want to give them props and recognition so they said that like i do think
*  that these last have folks are the ones who have taken these issues very seriously and have put out
*  these practices that like we think are good and different income and experts with no financial
*  association with them duck and zin and yosra ben joe and and others like are saying are good
*  and we also kind of we have one in view which is like one of the upstart eating with them like
*  to work the bill and the doable i don't think like that crazy of a super radical thing i think these
*  are things that are doable and what we're just saying is you're doing this now but as the rate
*  intensifies don't stop doing it so important and also new entrants coming in and think that you
*  want to not take these pretty basic common sense precautions that's not okay but that's some of
*  what we're getting at and i think it's fair to recognize that i think we really are trying to
*  look at what different companies and folks are doing that being smart and appropriate and saying
*  that look you've made voluntary commitments the white house by doing this and other things like
*  that but we don't think that these will be voluntary commitments we don't think these can be
*  then you just decide you're out going to do that if your investors are really pushing you like
*  we make clear that we might need to do i think that's interesting because this bill doesn't
*  have any this associated with it other than sign a document that allows you to be sued
*  like it doesn't there is no this in this bill there are no because we have no standards for what
*  companies should be doing if we had those standards for either red teaming or alignment standards
*  whatever you want to say i would be so much more in favor of laws that say for example if you want
*  to commercialize an ai model in xyz industry then you have to refer to these standards to do that
*  to get to that point which would actually be like standards like that would actually be moving the
*  epistemic ball forward as opposed to saying you guys figure it all out and if we don't like what
*  you do we're going to sue you or potentially bring perjury charges against you i know the
*  perjury thing is complicated but certainly there's a civil enforcement aspect of this bill it's very
*  european that's very much how like eu regulation works where the eu says do xyz you figure out how
*  to do it and then if we don't like what you do we'll come after you and we'll like we'll let you
*  know we don't like your compliance by suing you or fining you which again the fine structure here
*  how big would the fines from the frontier model division be we have no idea and they can set them
*  to be whatever they want so that's not great from a political economy perspective but i think that
*  we're getting the order of operations for good ai policy or ai regulation entirely backwards with
*  this bill i think that what we need to make good laws are good standards and what we need to make
*  good standards necessitates active scientific inquiry into ai and active scientific inquiry
*  is best supported by open source investing by open source software by open investigation in
*  a decentralized way by many different people and this bill makes all of that much harder so i think
*  that what we instead need to do is take a process of science to build standards to then create laws
*  and we're doing it in the opposite direction right now yeah i guess my immediate reaction to that is
*  that sounds almost like the case for a pause if you feel like we don't know what the standards
*  even should be i don't make the case for things that won't happen so i don't i don't make the
*  case for things that aren't possible so no it is not a case for a pause it is a case for scientific
*  investigation i agree with you i would love to live in a world where we could sit down for five
*  years and figure out what these rules and standards should be and then write those clearly into law
*  i just think that if we sit down and do that process for five years we might have horrible
*  bio weapon attacks and critical infrastructure attacks and like crazy genetic modeled right
*  around the internet committing law crime but in what world is that actually true if agentic ai
*  models are running around the internet committing crime society adapts right there's this model
*  that i think a lot of people that are fixated on ai safety have where society is static and like
*  things happen to society and society is this inert object but that's not true like you're living in
*  the symphony you're part of the orchestra and it's happening it's unfolding our response is unfolding
*  all the time and so i i think that like society will respond the bio risk thing is the ai models
*  are not the enablers doing to bio risk there's bio risk potential on the table right now major
*  bio risk and cbrn risk that exists regardless of ai there are other bottlenecks to that that we need
*  to be more proactive on policing not just in america but across the world and we are right
*  that's part of what the biden administration executive order did that i've very much applaud
*  we are doing those things so it's not that we would do nothing and that society would be inert
*  and that then we would scale we would pursue the scientific investigation while doing many other
*  things to ensure downstream security and resilience i really love that you're taking seriously the
*  idea around safeguarding society from some of the bio risk i feel like there's some fluffiness in
*  oh if you just have the open source models you can defend the bio weapon with your own bio weapon
*  model and like the way to defend it is more like actually having ruvc and gene separators training
*  and like affected that seems and acting platforms and i agree like wish we lived in a world where
*  maybe we just all stopped debating this and just all worked on trying to safeguard the world against
*  horrible bio attacks i think that is very important and good that still no i agree society in general
*  is pretty good at adapting i think the issue is that this rate of change is so incredibly fast
*  it's just like so much faster than the previous levels where we have that expectation and when
*  i'm in sacramento and i'm like talking staffers and legislators from world california like they're
*  just terrified and i'm not necessarily that optimistic about the ability of society to
*  effectively adapt to the just letting it out and acting on speed and i agree that some what's going
*  to happen no matter what and to some degree that you pay to that of the the two but i do think the
*  us and california in particular is really leading the industry right now we do have these export
*  controls on trying to just kind of their own problems with their tech industry that means
*  that i think if they don't have access to cutting that joke on first models it's unfair how much
*  that they can actually be competing at the frontier i think we also need to be adapting and putting in
*  title adaptations and realizing that stuff's gonna slip through the cracks but i also don't
*  think we can just rely on adaptation on its own given just how incredibly fast it's happening
*  relative to other societal adaptations yeah i agree there's definitely policy steps i just think
*  models are too low in the stack for a variety of different both practical and principled reasons
*  but it's a combination of policy and societal responses i will also just say that there's i i
*  hate when people use iq to apply to language models but there is like a 130 iq language model
*  out there who's going around talking about its metacognitive properties quite freely
*  and it's called clod 3 opus and i don't know that that happened and the world the republic has stood
*  has it not nobody seems to care outside of me and four other people no one really seems to care about
*  it i wrote about it the day it came out and i was like oh my god look at this thing but nobody cares
*  it and it seems like it's been fine i'm not saying that that's great and there should be much much
*  more of that but also one of the things that ai safety advocates love to to say is that their
*  regulations only apply to people that are very large companies spending lots of money that's
*  true today obviously that's just not true in five years if the thresholds stay the same everyone
*  knows that and i've always felt like it's a disingenuous thing when anybody says this only
*  applies to big companies because we all know that's not true in the long term that's not true but
*  it is also the case that currently it does only apply to big companies that are spending a lot
*  of money and they're big companies that have shareholders and they're spending a lot of money
*  and so there are certain they have strong incentives to investigate the safety aspects
*  of this and to not release things that do awful stuff and maybe yes do they maybe do it by accident
*  possible i doubt it i doubt not in any irreversible way i think these companies
*  are very capable i really admire all of these companies for those things i think it's great
*  that we're leading right i think it's great that american firms people that are embedded
*  in an intellectual community that has worried about these issues for a long time really worried
*  about these issues before the technology was real right um that's great sometimes i worry though
*  that because these concerns were initially born before the technology was real there may be a
*  little divorced from the technology in some sort of fundamentally reversible way there's there's
*  like a genetic there's like a genetic problem with the concerns that people have that just is like
*  we had all these concerns before deep learning was a thing we were not anticipating a statistical
*  approximation of the internet as being the thing we were anticipating like a bayesian
*  pure reasoner that could infer special that could infer general relativity from two frames of an
*  apple falling from a tree right uh without having seen any other part of the world right that was
*  like the original vision this is quite different and i just think that's generally important to
*  keep in mind we're not on the path that ali azer thought we were that's there one thing i will say
*  there's been some movement in the opposite direction where you had folks like jeff hindy who
*  worked initially particularly worried who then when working with the latest generation of models
*  like holy crap he will have more than i thought it was and it seems like if i thin out the last
*  five years of progress five years forward i am pretty surprised about that we're the top
*  for everything we need to make sure that we're continuing to think about like in the realities
*  of these models and not in the higher level ideas about how intelligence might work and i think
*  that's why i think all of us i think for our credit i think for acknowledging that tremendous
*  amount of uncertainty here i think it's reasonable i'm glad that we agree on that can i ask what
*  from the leading developers they've been strikingly quiet in the public about this i haven't
*  seen any official statements from open ai or google or we're anthropic on this bill i wonder
*  like why that is what what do you think their attitudes are would you characterize industry
*  as like working against this behind the scenes or saying hey this is okay with us we're not like
*  going to be vocal supporters of it for whatever reason i do want to talk a little bit about the
*  whistleblower thing because i do think one of the weirdnesses of the current moment is that this
*  likely society changing and potentially society devastating technology is being developed in a way
*  where it seems like not even everybody at the labs has visibility into it anymore which is weird
*  but i guess for starters how would you characterize what industry is saying or doing behind the scenes
*  on this yeah what i'll say is that the senator's office with the co-sponsors included have had
*  conversations with open ai and anthropic and google and meta and microsoft and i think they
*  have a variety of different views i think there are aspects of the bill that they
*  want changed i think there are aspects of the bill that they do you and hey these are things
*  we're already doing that seems okay this doesn't necessarily seem that crazy we have seen come out
*  in public opposition places like the chamber of progress tech net guy which are tech associations
*  which have microsoft and meta and google as members and so we've seen some of the companies
*  like speak through these tech associations and that's sometimes how it goes where in general
*  the companies are happy to let their like practice speak through them though i don't want to necessarily
*  pay with a super broad breast because i think there is just like actually a lot of
*  heterogeneity even among companies that you might anticipate would have like similar interest here
*  i think i can like very much say is like they did not write this bill they're not pursuing like
*  that is just 100 percent false there's been talk about that and it's just wrong scott weiner got
*  freaked out about ai risk and then came with sponsors saying can i do an ai risk bill it was
*  not like an anthropic approach didn't do a secret thing hey can you share anything more about what
*  industry wants changed yeah i think they are wanting to be clear about some of the
*  specificity of the language and saying like we understand your intent of what you're trying to
*  cover but are you being efficiently precise about that this and is something that like we're
*  confident we can we can follow and understand what it is there's been like some like micro debate
*  bell do the kyc provisions violate the stored communications act there's some like fairly
*  big provisions but their view in general has been that we're not going to support or post publicly
*  but that we want to weigh in and provide technical information and some of the amendments for
*  design is just the technical way that talks about and is that accurate i also want to be
*  careful to the fact that like the center is seeming to be leading these negotiations they're ongoing
*  and again i want to push back very strongly on the idea that the motivations for this bill are
*  to lock in these incumbents that is just like 100% false and i can't empathize enough the degree to
*  which it's false i think ultimately a company to have in support because they think these are
*  doable i think that's great we would love that again you did that other ones did a good um but
*  like i just you know want to work out that it's drawing as possible but it's just fall yeah i
*  don't believe that open ai or whoever from the big companies wrote this bill i think that's true
*  in terms of the motivation in terms of the effect this bill undoubtedly benefits open ai anthropic
*  and deep mind at the expense of meta whose strategy relies upon openness that is the
*  explicit strategy of the company and also most of the startup community which broadly speaking hates
*  this and a lot of the scientific community and academic community which also i know jeff hinton
*  and yasha robenjio exist but there are other people i don't want to betray confidences because
*  i didn't get permission from anyone to to say names but there are very prominent researchers who
*  are just as respected as hinton and benji who don't like this so you know the startup community
*  has a lot of problems with it scientific community has a lot of problems with it those are important
*  and no i definitely don't think that the regulatory capture is the explicit aim of the bill
*  it is however very likely to be the effect and that's how we should judge things not by the
*  motivations but by the effect california is repeat with legislation which should be judged
*  by its effect and not by its motivations it seems like a lot of the discussion has focused around
*  this is going to make open source illegal which is obviously a bit of a hyperbole but does maybe
*  have a kernel of truth to it the main thing that i see is this idea of you release a model and what
*  people do downstream with it is your responsibility that makes sense to me in as much as i think if
*  you have trained this monstrous showgoth llama four or whatever and it's just got a thin veneer that's
*  like causing it to refuse to make your bio weapons when asked through the meta search bar but with
*  light fine tuning that capability can be exposed again that's going to be a big problem if something
*  like that does get released i presume everybody would agree on that i don't think everyone agrees
*  on that side if you get to that team but it's let's say we have a we train llama four we do not have a
*  theoretical breakthrough we can't make a good argument for why this is not going to be a problem
*  the pre-trained version we can even say hey look okay it'll do these various things right it'll
*  find j0 exploits it'll design bio weapons it will maybe break itself off of its server now we rlhf
*  it so that it refuses to do that but we know from plenty of research that capability is still in
*  there and can be re-exposed with downstream fine tuning that to me seems like it would be a big
*  problem if that's released tell me if you think that's either not a problem or
*  i would say the biological weapons thing uh it's not an intelligible concept to say that llama four
*  can design biological weapons because the design and software aspect of the bio risk thing is an
*  important part of the equation but there are other parts of the value chain as i've already
*  production chain as i've already talked about so there's other things you can do about that
*  in a world where llama four or whatever can identify zero day exploits and have that be
*  fine tuned out of it or otherwise removed by downstream open source users i would i i think
*  that the equilibrium we have reached right now is actually a quite positive one where the frontier
*  of capabilities are generally reached by the closed source companies who take these things very
*  seriously and they test it and microsoft owns github and if gpt5 and github haven't already met
*  each other i suspect they will in the future probably before the public knows about it i think
*  that there is definitely a lot of responsibility that meta would be taking and if it's not a
*  responsible decision for them to open source it i guess it's you're asking me what i would want to
*  happen it's not my decision it's ultimately part of this whole thing is that i don't think it should
*  be any single person's decision i think the world is too complicated to make decisions for people on
*  things like that but yeah i would certainly hope that zuckerberg and john lacoon and others involved
*  in that decision would think quite seriously about that and at least want to robustify society much
*  more our digital infrastructure in particular um and i think steps are already being taken along
*  those lines by the way um but yeah would i want that to be released tomorrow no i wouldn't if it
*  were my choice um and i think that most people don't and that probably includes capital allocators
*  and people who are responsible to shareholders and things like that so i think they probably won't
*  and if such a thing is even possible which we don't know i think a problem gets introduced when
*  you legislate things because legislation is different from conduct when you legislate things
*  you lock certain dynamics into place and create sort of a freezing in amber of industry and industry
*  dynamics and that's just problematic for all kinds of downstream reasons that are hard to predict
*  yeah i guess no i wouldn't want it to be released and no i don't suspect it would be
*  yeah i have a reasonable amount of trust in netto leadership these days i think zuckerberg had said
*  some notable comments recently suggesting that he's not purely ideological about this and is not
*  not going to yolo an open source model if he thinks it's like genuinely not safe to put in the public
*  i wonder if there are things that we could do to allay some of these concerns and on the open
*  sourcing one i do think most everybody thinks that open source is good unless there's these crazy
*  capabilities that are getting inadvertently exposed so one proposal that i was noodling
*  on myself and then got a more concrete version of it from z and a blog post that will be published
*  by the time this is released so you can go check it out on these vlog is to create a little bit
*  sharper definition on what counts as a derivative model versus what counts as a non-derivative
*  model the idea right now is that if you open source something and somebody makes a derivative
*  of it it's still on you if they fine tune and re-expose some latent capability that you mask
*  that doesn't get you off the hook as the foundation model developer but there's not like a limit to
*  that and you could imagine an area where somebody does 10 as much additional training as the original
*  model had or 25 or 50 and at some point it's like okay that's no longer just re-exposing some
*  latent capability but you're actually using this base to build something that is fundamentally
*  new and qualities like different and maybe should be considered non-derivative so one idea i might
*  advance would just be maybe we have some sort of reined in notion of how much downstream modification
*  is fill the responsibility of the original foundation model developer versus at what point
*  it becomes like okay now it's really i'm the new developer you put in your own tens of millions of
*  dollars of training and now that's on you another idea would be maybe some sort of sunset to this
*  bill would be a good idea i think everybody agrees that we're in a regime of fast change we would be
*  very surprised if we don't know a lot more in even two years relative to where we are today so
*  again i don't know why this is never seems to be done but one could imagine a hey for the next
*  36 months this is what you have to do and at the end of that this bill will sunset obviously it
*  could be extended but ideally it would be like updated and improved upon and it could maybe
*  make some sort of forcing function into the legislation now to try to create something like
*  that in the future any thoughts on my my fixes i think these are thoughtful suggestions that i
*  think author really appreciates and again i think we agree on more than i think you might think
*  for better i think lamin-3 being open source was good i'm happy about it i think it's great i think
*  it's gonna be able to do lots of awesome things and i think there will be arms but those are best
*  addressed at the application developer and at other stages other than the developer and you think
*  that there is some level of capability where just going it out is not a great idea and mark
*  suckerberg seems to be acknowledging that i do worry that if we're in a regime or like relying
*  on the will of mark suckerberg to avoid catastrophe and a lot of people are uncomfortable with that
*  and i think that possible there will be other people who maybe do think it's great machine
*  takeover and do want to just blow it out at some point i think that is a rough place to be if that's
*  hope we're taking everything on i definitely i think it's fair to distinguish between a
*  situation where you are fine-tuning remove some guardrail rather trivially versus like building
*  your your own model with 10 or 50 percent of the training costs and again i think it's definitely
*  fair to consider amendments that clarify that i think that seems reasonable on the sunsetting
*  cue like there are things within the bill that try to make it future approved all these references
*  to like reasonable and nifts and all of these best practices those will continue to evolve over
*  time but this is us trying to reckon with the urgent nature of the problem and wanting to put
*  up what is a thoughtful and educated push at that but i definitely don't proceed this that she does
*  ask me being the end all be all there being no later revisions or subsequent statute needed i
*  think that super air again i think that open through great and i am i genuinely think that
*  there are like a ton of benefits and that in the vast majority of cases the benefit that way to
*  cost similar to how i work with that like the bio security issues and when you are working around
*  like any restrictions on gain of function research the group that was opposing any restrictions on
*  getting front readers called like scientists for science and they said no if you don't like
*  in a function research therefore you're anti-science you hate all of science and the fact that i don't
*  want there to be models that can shut down wastewater plants or design bio weapons or
*  things like that i hate it but i love open first i just think there's a very call category of very
*  common things where they'll be often and the opportunity for societal adaptation are not
*  there where i think that we should be more cautious and i'm not that different from where
*  marks look over is that i don't think it can just be them entirely making those calls on themselves
*  and what mood mark suckerberg wakes up on a particular day if you just feel like maybe
*  elon put out a big model and i really want to beat him today they'll look important i don't know
*  that's my thought there i'm not sure if we read different newspapers or something but i'm not really
*  sure i i see anybody yoloing right now this isn't like the least yolo technology in the history of
*  technology it is such a profoundly unfun conversation the unfundness and seriousness and fake everything
*  about this conversation uh is a form of regulation i would argue in some ways but i guess i just don't
*  really see where the person is who says i i don't give a crap about humanity i guess i fail to
*  understand who is the person that like is it thinking about these things that doesn't care
*  about the dangers of taking down critical infrastructure or wiping out humanity or whatever
*  it might be but who does care about the california frontier model division i guess i'm like failing
*  to understand what part of the venn diagram do we think we're really moving the needle on here
*  other than just maybe creating new forms of liability and new regulatory risks for the
*  legitimate players that's what i guess i fail to see and by the way my comments should not have all
*  been taken to suggest that we're relying on mark suckerberg's whims we're relying on a lot of
*  people's whims right that's the nature of the world i can't help with that that's just the
*  reality some people have more power than we do but but at the end of the day what i'm relying upon
*  is a system of incentives that tends to work and by the way which regulation often tends to break
*  in weird like extremely high dimensional ways that are like very hard to diagnose or to foresee
*  and i think we all know that about regulation everyone's seen that so it's not like what i'm
*  saying is empirically unobserved everyone's seen that i feel like sometimes there's an
*  idea that currently california has no rules at all about ai and then this is stepping into the void
*  and creating those there's lots of existing liability law there's lots of actually existing
*  uncertainty about like a situation where you did release a model and then someone modifies it
*  does something bad whether you might have liability i think to some degree these are already
*  questions and uncertainties to exist i view this as actually clarifying what those are at some degree
*  given more confidence what they're doing satisfies their burden i think there's actually
*  in some way back there's no formal safe harbor in this legislation like we have a status clause that
*  makes clear that like all other remedies and rights and actions that are lost still exist but
*  i think there is some more confidence that comes with companies about what do i do if i want to
*  prevent these really bad things from happening we're saying these are some things to do these
*  are some good ideas and this is giving some additional clarity to what it means to take
*  reasonable actions as a bunch of developer again i agree so far how people are acting is i think
*  very in line with the bill and with what i would view as like responsible development i guess i'm
*  just worried is that i don't know that necessarily extrapolate in the future there are people who
*  like the deaf cages and richard suddans of the world who i do think have very different world
*  views for myself or the vast majority of people and i agree there are going to be some group of
*  folks that are not going to care about what the law says at all but maybe they're going to have
*  investors who just think that they're making a bunch of money and they they're like oh i want
*  the machine to take over and the investors might not care about that ideological thing if they don't
*  believe that but maybe they do characters violating the law and so i do think there are other people
*  for whom this does have force and i also do think again there's the question of what some of these
*  developers think upon reflected in their better nature it's in everyone's interest versus what
*  is happening when there is incredibly intense competition and egos and i just think we
*  you shape their incentive and like having some pushback against that drive i think is
*  super important and necessary but also not killing the entire industry and running their own company
*  out of business shapes their incentive if open ai releases a model that brings down the internet
*  tomorrow i don't envy sam altman's job it sounds incredibly hard the pressures are there right and
*  certainly there's internal stakeholders within open ai uh that are very worried about these things and
*  that should be the case my point is that safety is the very hard thing to legislate it's integrated
*  into a lot of different economic activity at the frontier of technology it's incentivized by a lot
*  of different things i think everything you're saying is fair i just think that the dynamic
*  we already have is a perfectly fine one the first thing i looked at this bill and said was if someone
*  uses an ai model to do a hazardous capability of the kinds described here you're gonna get sued
*  like for sure a thousand different ways so like what is the marginal utility of this bill other
*  than creating again a regulator whose objective is to ensure safety but which does not actually
*  do any safety work themselves and which does not build ai systems that is a regulator whose
*  incentive is to cover their ass that's the public choice reality of this and so you're creating like
*  what will ultimately i suspect be a quite pernicious addition to the ai industry dynamic
*  with the frontier model division so
*  one idea a simple person like myself might advance would be to say why don't we just make
*  this a lot simpler and just make certain things illegal like what if we said it is illegal to
*  certainly release or open source an ai model that can do x and just have some concrete x's and say
*  you just can't do that boom simple now it's on you to make something if you want to open source
*  something you got to make sure that you comply with that standard of what it can and can't do
*  and there you have it now it's tough because these are general purpose technologies there's a lot
*  going on there but if we have a pretty high threshold for the sorts of things that we are
*  concerned with then it seems like a lot of people would be like okay that reduces a lot of ambiguity
*  i don't have to worry about whether i fit into this or not i just have to be very
*  careful about 10 different things maybe even fewer than that and at least it's clear what i
*  have to do right what would be the downside maybe it would be more if i just support that but what
*  would be the downside of a sort of simplification clarification and just the following types of ai's
*  are illegal booms yeah i think definitely room to make the bill fairer in part than you and others
*  have given some good deductions though i think us and the offer are going to think hard about
*  i think it's hard when you're trying to enumerate those and also recognize that you can't be 100%
*  sure like we've had some time trying to think about like definitions of deceptive or self-replicating
*  systems or different things very hard to define in statute and i think describing more in terms of
*  the severe consequences that we're trying to prevent we thought was clearer it's easier to
*  describe the process of you should do tests for the types of things and you should have some level
*  of reasonable confidence i hear you and i think open to the specific language that tries to make
*  that clear i doubt that addresses the concurrence today but that's my initial thought the bill you've
*  described nathan i'm not sure if it would be like a single piece of legislation but that is the legal
*  outcome that i would most prefer the problem with it is that we don't actually have the knowledge
*  to legislate an outcome like that at this time because of the lack of good technical standards
*  and evaluations and safety best practices and all that kind of stuff and that stuff simply takes time
*  to build and i think we should invest so much into building those things as quickly as we can
*  i think that this bill hinders our efforts to do that not helps it does take time and i mentioned
*  that earlier that it might take five years to do that and nathan calvin you said we don't have five
*  years and in my mind nobody has made a compelling case that is true the only case that i hear on
*  that is pointing at charts that don't tell me anything concretely calls to authority etc we
*  don't know how much time we have and so i don't think just i like this is a general it's not a
*  critique i'm making of you in this conversation i just want to be clear you've been like awesome
*  but it is a critique i have of the ai safety discourse in general i'm not this is a societal
*  negotiation the ai safety people don't just get what you want because you were here first this is
*  a societal negotiation uh and i don't personally like to negotiate with a gun to my head and that's
*  oftentimes how it feels when mustafa suliman last summer goes on a podcast saying we're all going
*  to die if we don't create a brand new regulatory regime from scratch tomorrow that isn't true and
*  we're giving up a lot we're potentially setting up radical precedent here by the way that is an
*  important part of this it's not necessarily specifically what this i mean it is specifically
*  what this bill does that i don't like but it's also the precedent that it sets about government
*  pre-approval of software and killing that whole system that's been quite valuable we're creating
*  and no one's talked about the enforcement infrastructure for this bill also a major thing
*  especially as models of this level of power proliferate good luck enforcing it without
*  creating a policing regime over the internet california state government which is a long-term
*  outcome that that might be inevitable either way but my only point is we're sacrificing a lot
*  and we're setting a pretty radical precedent by doing this and we need really compelling evidence
*  for that and it just it could happen because of exponentials is so unbelievably far from good
*  enough in my mind yeah i think we are honestly in similar places on the epistem it's like when i
*  say five years you know i put money in my retirement account i do like beyond five years i'm
*  not someone who says definitely in five years it's going to be this but when i look at these graphs
*  i have uncertainty they could turn off at any time we could have another air winter we could run into
*  to block all that could totally happen i have talked to a lot of smart people who have different
*  opinions about this there's i think healthy uncertainty there and there's a question of what
*  to do in face of that tremendous uncertainty i hear you saying you think even if those risks do
*  emerge that this will make us in a less good position to handle them but i do think that they
*  are at a high enough level to be worth taking seriously in one direction or the other the other
*  thing is i 100 agree with you just because a bunch of people on less wrong thought about this 15
*  years ago doesn't mean they get to decide what happens i 100 agree with that i also don't think
*  hey that just because mark sucker burr buys x thousand numbers of 8.100s and he gets to decide
*  what the rest of the world looks like and there's no opportunity for democratic input or things in
*  that either and so i think that's what is happening now and we're going through this multi-month tons
*  of open hearing lots of discussion on twitter talking to dozens and dozens of people and i
*  think it's fair to be pessimistic about government and lots of respect and there are things with good
*  intentions no core legal i think this process of trying to figure out what should this look like
*  and taking seriously a variety of views about people inside the ai community and people who
*  are terrified who are out there community i think that is appropriate it's the way we have to take
*  things and i prefer that to like just saying that these leaders are going to make their calls and i
*  hope they woke up on the right side of the bed this morning to a certain extent i actually would
*  dispute the notion that we make decisions about the trajectory of technologies or industries based
*  on democratic input solicited from like anyone right i don't think i have a vote in that i have
*  a vote as a consumer in the marketplace but that's not something i exercise through my government
*  and i think that government thinking too much that it is responsible for setting the trajectory of
*  technology or the destiny of mankind i really anything even remotely adjacent to that gets you
*  into all kinds of dangerous problems it's a bad attitude for policymakers to have i'm not sure that
*  things in california are going so overwhelmingly great that scott wiener has a lot of time to be
*  working on the destiny of mankind i just i think that we make these decisions in more diffuse
*  and complicated ways and we don't do it through the democratic process and history doesn't unfold
*  by show of hands davriel apprenticeship did not ask everybody in europe whether he should kill
*  archduke ferdinand he just did it sometimes that's the way it goes sometimes non-linear things happen
*  as the results of one person history doesn't unfold by show of hands uh and so no i don't think our
*  fate is in mark zuckerberg's hands i don't think our fate is in any one person's hands and i want
*  to keep it that way and actually my opposition to something like the frontier model division existing
*  is partially connected to that instinct i have of not wanting to put all my eggs in a small number
*  of baskets let me ask another kind of hypothetical question it seems like there was openness to this
*  notion of if we could effectively enumerate a bunch of things that would be really bad for ai
*  models to do then we could all potentially get on board with a law saying you can't make an ai
*  that can do any of these bad things we have definitional challenges there one thing that i
*  see missing from the bill or it's not entirely missing but it's very limitedly addressed is
*  independent testing of the models i would love to see a little bit more independent access to models
*  during the training process in particular there's a lot of debate in the literature and community as
*  to what should be considered an emergent property and how fast do these emergent properties
*  pop up in the training process it seems like in general there is time that last order of magnitude
*  is always the biggest order of magnitude of course and like it seems to take an order of magnitude
*  for a lot of these things to come online to the best of my ability to synthesize the literature
*  so what about a scenario where we say it's illegal to distribute an ai that can do these things
*  distribute could be via api or could be open source it's but it's illegal to distribute an ai
*  that can do any of these things and we will determine whether the ai can do those things by
*  giving access to a bunch of third-party testers who don't necessarily get the weights don't
*  necessarily get to know all your trade secrets don't have a sense for what the parameters are
*  or whatever but can just hammer at the thing for a while and satisfy themselves that either they
*  are able to get a certain behavior out of it or finally give up i know a lot of people who would
*  be motivated to do that i have a little uh or all volunteer project red teaming publicly deployed
*  applications and i could tell you any of those people would be very excited to get the opportunity
*  to go do some of this work on stuff that's not yet released there's only one line in the bill that
*  mentions third-party audits as appropriate at least that i was able to find so quickly i think
*  there's more in there but i'm reminded too we had the page bailey from google peat mind on the
*  podcast some months ago and she was talking about the experience of doing these training processes
*  at google and she described it as like coming in in the morning and oh my god we just hit new state
*  of the art over here this thing that we didn't expect to be this good for a while yet suddenly
*  grok something and it's actually hitting these thresholds sooner than we expected and i took
*  away from that that there is not enough internal during training testing happening to really have
*  a sense for what is going on opening eyes not that big right and a thousand people ish and like half
*  of those or i think i'm the non-technical staff side at this point so we're not really able to
*  put enough actual human cycles into trying to figure out and meanwhile you got plenty online
*  who's jailbreaking everything i'm pretty confident with current techniques like if it's in there
*  he'll pull it out he's got the lead speak to to figure out how to to really talk to the models in
*  the way that they want to be talked to so i'm trying to square the circle here by both giving
*  you know the best standard that we could have would be to throw a motivated community at it i
*  think and try to elicit problematic things and then just say hey if we get the crowd on it and
*  we can't find them then we guess they're probably not there that doesn't give us a guarantee but it
*  would give us much better sense i think so what do you think you can't do these things you have
*  to allow testing if the testers can show that the models can do these things you can't distribute
*  over nathan's sb 1048 yeah i am also very enthusiastic about third-party testing and
*  one of the things as a co-poster and the author of thought about we just want to make sure that
*  the incentives are set up right about who those third-party testers are going to be and what that
*  will look like i love the idea of like finding on twitter going in and messing around with people
*  modeled and trying to break them i think realistically you're probably gonna get like deloitte like going
*  for a checklist i think and i think there's some concern that like companies or if they just hire
*  deloitte or russall and hamilton and give them a check or they're like okay i'm good let's do it now
*  and not do that more like deeper pull that searching and sense of responsibility
*  and so i think that there's something that not wanting to roll that out feel like companies can
*  just push the responsibility onto those folks before we're confident that they will actually
*  do a good job of it we do have some language in the road bill around the division being able to like
*  establish a accreditation process for third-party testers to see them would be
*  credited by the fnb be able to do tests but i think requiring them to then use those specific
*  testers i just think we want a little bit more information about like who counts as a third-party
*  tester how are you going to enumerate the provisions by which they're doing it you're
*  going to rely on accreditation process how is that accreditation process judged and not opportunities
*  for capture weird things yeah i'm enthusiastic about it i'm very open to the idea of there being
*  like a role for greater amounts of third-party testing in the bill and i read for important
*  part of it and some of our whistleblower protections also do apply to folks that labs
*  are bringing into to do testing and i think we definitely view a important role for them in this
*  but just like the attitude i think the light figure deal for this is responsible but very
*  humble about their ability i think that they should feel like they have a role an important part of
*  those systems will protect california residents but that i think they should be pretty thoughtful
*  about what to what degree government is going to be able to go to toe-to-toe in an effective manner
*  that is what we have tried to build into the bill saying that like let's look at what companies and
*  dogs are doing what they think they're currently publicly saying they're doing let's say no you
*  actually have to do it and i think that has been trying to take that approach of not having like
*  a pencilized regime that's trying to do pre-approval model relief is which is not anywhere in the bill
*  and trying to go in and like how the government of student testing themselves because like they
*  don't know how to do it they were very far placed in like government being able to pay the
*  70 salaries of folks at these firms be able to stand and go to sell with them i 100 agree with
*  you that they're having lots of problems and lots of legislation about the government you know we're
*  gonna take the world into set government on this and have that go better as part of how we've gotten
*  stuff like t qual which cowiner has spent a decade fighting and i'm very happy to end it fighting i
*  think the last thing we would ever want to do is create an ai i think why i think that would be a
*  disaster and i just don't think that's what this is so anyway i think things like part of your
*  concern is not necessarily like this itself is doing those things but that it will be grown over
*  time to create something more like centralized ai belief in the future rather than that like
*  the specific language and the that she does that today anyway this is the side view for carding
*  things and i think that the author we were quite open to suggestions there and i agree there's a
*  super important role for them it's just tricky to figure out exactly how you want to specify it
*  yeah to make some of my contentions clear contention number one i do not believe that
*  if this bill is passed current open source models become illegal right i would never make that claim
*  there might even be like another generation ish of open source models that are legal
*  um but they become illegal eventually they become essentially unlawful over time and that's how the
*  bill is designed there is some level of capability where i do think that that unless you come up with
*  methods like self-destructing model weight the resistance of tuning yeah if they have those
*  capabilities it's a legislated gradual death of open source ai i think that's just basically the
*  case and it's designed so that it doesn't happen like immediately the opposite of whatever sam
*  altman says iterative deployment it's like iterative destruction of open source i mean like existing
*  open source models in llama 3 will always exist but it's a sort of gradual devolution but to your
*  point nathan i think third-party evaluation is great one of the things that i support at the state
*  level and at the federal level too is actually safe harbor for good faith third party and like
*  truly independent not like i paid deloitte but like truly independent no financial relationship
*  research into ai models right now a lot of that work violates these companies acceptable use
*  policy and so i think that fixing that would be a great move i would note that truly independent
*  again not like we paid you but actually independent research into models is harder
*  without open source right like that's a that's a trade-off that you're willing to make that i am
*  less willing to make i think that's like a fair characterization i just want to say one other thing
*  though to go back to your hypothetical sb 1048 nathan which is just that we have two problems
*  with saying xyz is illegal first of all it's what is xyz how exactly do you define it but number two
*  i actually think the more serious problem is that we don't have a way of actually making that
*  effective right like you could say right now if you make an image generation model that is
*  specifically designed to produce child pornography then that's illegal under current law but if you
*  say it's illegal to make a dna foundation model that can't produce a dangerous bacteria i don't
*  know can we train that out what is a dangerous bacteria dangerous in what circumstances how do
*  you model that how do we know how would you ever actually legislate that in in practice and then on
*  a technical level in as much as you can legislate that can you actually remove that specific set of
*  things from the model's capability without damaging anything else right now the answer to that is no
*  it might always be no maybe it'll be yes probably it'll be some complex combination in between
*  which is why i think that we need to just like accelerate research into all of that kind of
*  stuff we need to be doing as much as possible why i support national compute infrastructure if
*  california wants to do it that's great i don't think california has the money and i would prefer
*  to see california spend the money on like addressing their overwhelming deficit their
*  overwhelming budget deficit i am a fiscal policy guy at heart so i look at that and i'm like
*  i don't know you probably be thinking about that that's probably the primary threat that your
*  citizens face right now probably not so much seaburn from ai but yeah like i'm in favor of
*  national compute infrastructure to give academic researchers all over the country the same at least
*  a modicum of the kind of resources that that they have in the big labs i want lita have a 10 000 b
*  100s to do whatever she wants with uh and stanford's not going to buy that for her i i think that the
*  public should in some sense for all researchers so i again yeah like i i think that's what we need
*  to be thinking about and i think that like this work this kind of a bill just slows down the
*  scientific research so if i had to synthesize this it sounds like general agreement on it would be
*  good to have a discrete list of things that you can't make an ai that can do these things
*  but for the challenge of actually defining that with clarity and there's also agreement on third
*  party testing is good and obviously we don't want it to be a credit bureau type of situation vis-a-vis
*  the mortgage companies from 15 years ago was an example how that can go wrong but
*  that does seem very overcome able to me i think there are there are plenty of people now who would
*  volunteer or who would take a grant from a philanthropist to do that sort of work and really
*  the only barrier to them doing that sort of work right now is access and so that seems like the
*  kind of thing that legislation could like rather easily address and say you have to provide some
*  amount of opportunities for testing to happen during flash after the training process but before
*  deployment that seems like quite easy to do then the other thing i would wonder about transparency
*  on the results of those tests this is a personal hobby horse for me because in the process of doing
*  the gpt4 red teaming i was under an nda where i was not allowed to talk to anyone about anything
*  related to this entire thing and they wouldn't tell me anything about their other safety
*  plan other than the fact that i was in a black channel with a couple dozen people and
*  not many of them were doing much of anything so i was like i feel like i'm like one of a
*  couple people in the world that is doing this testing and i got a little freaked out not so
*  much because of what i was seeing my conclusion to that at the time was that i did think it was a
*  safe thing to deploy but just yeah because if it's just me and like a couple other people here that
*  are active in this one black channel like that doesn't seem to be enough so i went and talked
*  to a few people in the ai safety community to see what they thought about what i was seeing
*  and when it was discovered that i had done that then they kicked me out of the program now that
*  doesn't seem great for a lot of reasons that i do understand that they viewed me as a bit of a
*  wild card potential loose cannon whatever but this is where again i think legislation could really
*  help one thing i've considered in the past is a special whistleblower provision for observed
*  model behavior if you saw an ai do something you should be able to say what you saw the ai do
*  right they sort of what did ilia see now that it could be distinct from what was the technique
*  right i think there's still a place for trade secrets at least i'm not trying to make a case
*  that there should be no trade secret but if you saw an ai do a certain thing and it freaked you
*  out i think you should probably be able to tell the public that and i don't really see what the
*  harm would be to the companies that hey it did x right this thing broke itself out of our test
*  server and got onto our other test server and i'm freaked out about that that kind of thing doesn't
*  seem like it should be swept under the rug so if i was going to take 1048 to 1049 it would be like
*  you have some things that you definitely can't do we could make those maybe like fairly narrow
*  and as concrete as possible for now we might even feel like that leaves something on the table but
*  in the interest of clarity try to make it narrow insist on access for testers to be able to do
*  this sort of thing and also make it clear that those testers can disclose the things that they
*  see and then if we could do all of that i think we're headed for a scenario where i would feel a
*  lot better about it as opposed to nobody really knows what they're baking and they'll show it to
*  us when they're good and ready and who knows what they did in the meantime that didn't get
*  any light of day then again i don't need to know how much visibility there is in the labs at this
*  point there've been some really interesting somewhat cryptic tweets recently from people
*  that have departed open ai where one who's now google said what's really nice about google's
*  culture is that i have visibility into what's coming somebody responded did you work at another
*  place where that wasn't the case like man what is going on there so i don't know i'm building my own
*  bill here but i feel like i'm on the verge also of getting to a point where you guys would both
*  support this and it feels like it might be pretty good i do think there are areas where you and i
*  have some substantial agreement and i think some of them is around trying to figure out these
*  appropriate transparency measures and i agree there's a problem where there are just far too
*  few people actually investigating the fake news models i think one solution to that is just to
*  release the models in an open weight form which then enables anyone to then think around understand
*  that which i think is again good and very beneficial for current generation models
*  i think there becomes some point in the future where that worries me and where i don't think that
*  is sustainable but then when we do enter that regime where it is maybe not safe to
*  really in an open way form certain models then i do think you need some way of like actually allowing
*  other experts and people outside of it not even just because the companies are like
*  selfish and profit motivated because there's not enough well and not enough like brain there and
*  both actually evaluating it i think you need ways to take the wisdom of the crowd and important
*  root respects i think that is an area where there just is a lot of tension among companies of giving
*  access and of disclosing trade secrets and what exactly those things look like in the bill is
*  written we have some provisions around the companies reporting guy safety incidents like
*  true to fmd and there's some mechanism by which they can then be like made available to the public
*  but i think fair to say that should be cleaner i really don't want some of this just stuffed in a
*  desk drawer somewhere when it really would be vital for the broader public and technical community
*  to be aware of we talked about this briefly but i just do think it is super important one area of
*  agreement that i've seen among thoughtful people who are taking both sides of the open source
*  for frontier models debate is thinking seriously that potential that advanced models are going to
*  lower the ability for bad actors to make bioweapons and i think some of the open
*  source folks then say that risk is going to exist anyway and exist today which i agree that it does
*  and that would be not an effective way of combating it i feel differently but i think you can go back
*  and forth them clearly there are good faiths and thoughtful people who disagree well i just like
*  really want to say there are people listening to this podcast on either side of the debate are you
*  seeing genes that this is creating just dealing with the potential of the future we're going to
*  have words of the littered bio incidents we let's all agree on this and do something about this i
*  worked in congress in 2020 a bit after covid and there was just no energy to do anything on
*  pandemic prevention because it was like all like everyone's covid fatigue dude doesn't want to talk
*  about this these are just like incredibly wise and good investments do you think we are pretty
*  likely to see some intentional bad news to biology in the next 10 years that could just be really
*  horrific and i just think that we could make just incredibly wide investments now for that and it's
*  good to see that scenario working i agree just as a response to your idea nathan i think the
*  whistleblower provisions are great safe harbor for researchers is i think a really smart move to
*  make i don't know if it's actually possible i in a world where it's possible to legislate the kind
*  of things you're talking about and actually technically possible to do to say oh you can't
*  make a model that does xyz and xyz is a thing that everyone can agree is bad sure if that exists as a
*  thing then it will be an example of a technological free lunch and i'll be very happy for that
*  i'll be very happy if that's the outcome that we get i think we also need to be prepared for a world
*  in which it's really not epistemically or technically possible to talk coherently about
*  that kind of thing uh to say a biological foundation model that can't make a bad
*  dna sequence i think we have to be ready for that world and we have to be ready yeah for a world
*  that will be radically better in so many ways that we can understand and it will also have
*  higher stakes bigger dangers more capable actors that are smaller where intelligence and agility
*  and speed and flexibility matter matter greatly uh that's gonna be an adaptation for all of us
*  we're transitioning into that and i think it will change the nature of statecraft
*  i think it will change the nature of business i think it will change many things and it's just i
*  look at bills like this what i see frankly is a rather desperate attempt to to freeze the present
*  in amber and to not go into that new future and i don't know about you but i'm not all that happy
*  with the current status quo in the world i'm open to changing the status quo in quite radical ways
*  and the trade-off to that is that it might change in some ways that i don't like
*  so i think that contending with the reality that we have rather than casting an anchor
*  into a seabed that's not actually there into a seabed that is of our imagination and exists
*  purely in a legislative fiction is i think that's ultimately going to put us in a lot more danger
*  than simply contending with the reality and building capabilities to be able to thrive in
*  that world and that latter thing that building is what i work on primarily every day it sounds like
*  a good closing statement for you do you want to give any final high-level thoughts Nathan sure
*  i guess what i want to say is that i think the proposition that this bill is standing for it is
*  like a lot more intuitive and common sense than i think i've seen it be framed i think that we're
*  saying that like companies have to test for extremely dangerous and hazardous capabilities
*  i think we welcome continued iteration on how exactly those are defined and making sure that
*  they're not overbought in import respects but like i think we can talk about there being really
*  dangerous things and models shutting down race water plants and doing bioweapons and like the
*  people in going around do crazy things like the very bad things companies should test for them
*  and they should make like appropriate and reasonable mitigations not asking them things
*  that are impossible but asking them to take seriously what is possible that is happening
*  in other companies that is happening at nist and other places and to do that i think that
*  seems like pretty fair and i think the thing from the open source community is that if you're saying
*  at some point in the future you think it might be the cake that there's going to be a model of
*  llama 5 or whatever that can do really dangerous things and just say it's just completely impossible
*  for us to prevent it from doing those incredibly dangerous things if we open source it and just
*  everyone else you guys got to deal with it now sorry this is the mutable like march of progress
*  and you're just going to be crushed under its wheels i don't know i just think that's like
*  pretty intolerable and i really welcome i think other solutions i also like really enthusiastic
*  about trying to invest in a method that like to send things around ways to open source that still
*  allow additional open inquiry and transparency in those models again i think we don't know about
*  what those research directions are going to look like but i think this will be subtle promise
*  in ways to open source that are resistant to adversarial crime tuning i think there's a long
*  way to go but i don't think like fundamentally against the laws of the universe for that to be
*  possible and i guess i just want to say that so the large number of people who are like
*  worried about this bill and worried that we're going in the wrong direction but who do believe
*  that dangerous capabilities of future models are real like i genuinely say reach out to the author
*  reach out to myself we genuinely want to get this right it's complicated in a perfect world i would
*  love to just like freeze where we're at for a few years and have a really long deliberative period
*  so we can get this exactly right i think that's not the world we live in and we have to be setting
*  these processes up and having this conversation now and i think they're just like sufficient enough
*  fundamental disagreements that i don't think dean will ever be able to get to the point where he
*  supports this bill but like that doesn't mean that he still can have valuable insights about
*  ways that the bill could be better around like that we should try to fence and but my message
*  that the appropriate action in face of uncertainty and in face of trade-offs is not to just freeze
*  and just accept that's what's coming is coming and which i know is not what you're advocating for
*  one other thing as well but i think that like even on the developer level that i think that we are
*  like we need to be thoughtful and we need to make sure that it's not creating perverse incentives
*  and not causing harms but i don't think it's fundamentally impossible and just the fact that
*  like government has been bad at things before doesn't mean that like we shouldn't weigh in here
*  over anything dates are quite enormous anyway it was a really pleasure chatting i really
*  appreciate it i feel like i've been just spending too much time on twitter watching things go by
*  and wanting to actually have a conversation with that person and realize that they're engaging with
*  a caricature of that we know myself we're opening out of shelves to just want to lock in the incumbent
*  for evil riddance and instead just make clear that we are both thoughtful people want things to go
*  well and are crying hard in a challenging world to do that and so really appreciate you convening
*  faith and and again oh look at that even if you don't think that you can ever support this like
*  i know you'll be back then with any additional ideas that goes for anyone else listening who
*  has also you know wants to make it better i think what matters is that we had a respectful civil and
*  nuanced conversation i'm glad we did as well uh see if any closing thoughts i just want to express
*  my appreciation to everyone this has been a fascinating conversation i wish we had 50 more
*  hours literally uh thank you guys for being a part of it certainly the conversation will continue
*  but for now i will say nathan kelvin from the center for ai safety action fund and deem ball
*  research fellow at the mercatus center thank you both for being part of the cognitive revolution
*  thank you it is both energizing and enlightening to hear why people listen and learn what they
*  value about the show so please don't hesitate to reach out via email at tcr at turpentine.co
*  or you can dm me on the social media platform of your choice
