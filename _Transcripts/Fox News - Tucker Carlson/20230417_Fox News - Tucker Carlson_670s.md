---
Date Generated: February 22, 2025
Transcription Model: whisper medium 20231117
Length: 670s
Video Keywords: ['fnc', 'fox news', 'fox news channel', 'fox news media', 'fox news network', 'fox news today', 'fox news voices', 'tucker', 'tucker carlson', 'tucker carlson monologue', 'tucker carlson monologue tonight', 'tucker carlson tonight', 'tucker monologue', 'tucker monologue tonight', 'tucker reacts', 'tucker tonight', 'elon musk', 'elon musk ai', 'artificial intelligence', 'elon musk news', 'elon musk interview', 'elon musk ai interview', 'elon musk ai warning', 'elon musk tucker carlson interview', 'elon musk interview tucker']
Video Views: 3184225
Video Rating: None
Video Description: Tesla and Twitter CEO Elon Musk joins 'Tucker Carlson Tonight' for exclusive, wide-ranging interview. #foxnews #fox #tucker 

Subscribe to Fox News! https://bit.ly/2vaBUvAS
Watch more Fox News Video: http://video.foxnews.com
Watch Fox News Channel Live: http://www.foxnewsgo.com/

FOX News Channel (FNC) is a 24-hour all-encompassing news service delivering breaking news as well as political and business news. The number one network in cable, FNC has been the most-watched television news channel for 18 consecutive years. According to a 2020 Brand Keys Consumer Loyalty Engagement Index report, FOX News is the top brand in the country for morning and evening news coverage. A 2019 Suffolk University poll named FOX News as the most trusted source for television news or commentary, while a 2019 Brand Keys Emotion Engagement Analysis survey found that FOX News was the most trusted cable news brand. A 2017 Gallup/Knight Foundation survey also found that among Americans who could name an objective news source, FOX News was the top-cited outlet. Owned by FOX Corporation, FNC is available in nearly 90 million homes and dominates the cable news landscape, routinely notching the top ten programs in the genre.

Watch full episodes of your favorite shows
The Five: http://video.foxnews.com/playlist/longform-the-five/
Special Report with Bret Baier: http://video.foxnews.com/playlist/longform-special-report/
Fox News Primetime: https://video.foxnews.com/playlist/on-air-fox-news-primetime/
Tucker Carlson Tonight: http://video.foxnews.com/playlist/longform-tucker-carlson-tonight/
Hannity:  http://video.foxnews.com/playlist/longform-hannity/
The Ingraham Angle: http://video.foxnews.com/playlist/longform-the-ingraham-angle/
Fox News @ Night: http://video.foxnews.com/playlist/longform-fox-news-night/

Follow Fox News on Facebook: https://www.facebook.com/FoxNews/
Follow Fox News on Twitter: https://twitter.com/FoxNews/
Follow Fox News on Instagram: https://www.instagram.com/foxnews/
---

# Elon Musk tells Tucker potential dangers of hyper-intelligent AI
**Fox News - Tucker Carlson:** [April 17, 2023](https://www.youtube.com/watch?v=a2ZBEC16yH4)
*  So all of a sudden AI is everywhere, people who weren't quite sure what it was are playing [[00:00:00](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=0.0s)]
*  with it on their phones. [[00:00:04](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=4.32s)]
*  Is that good or bad? [[00:00:05](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=5.84s)]
*  Yes, so I've been thinking about AI for a long time since I was in college really. [[00:00:06](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=6.84s)]
*  It's one of the things that, just sort of four or five things I thought would really [[00:00:12](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=12.76s)]
*  affect the future dramatically. [[00:00:15](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=15.48s)]
*  It is fundamentally profound in that the smartest creatures, as far as we know, on this earth [[00:00:18](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=18.64s)]
*  are humans, is our defining characteristic. [[00:00:23](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=23.84s)]
*  We obviously are weaker than say chimpanzees, less agile, but real smarter. [[00:00:28](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=28.16s)]
*  So now what happens when something vastly smarter than the smartest person comes along [[00:00:35](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=35.56s)]
*  in silicon form? [[00:00:44](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=44.0s)]
*  It's very difficult to predict what will happen in that circumstance. [[00:00:45](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=45.8s)]
*  It's called the singularity. [[00:00:48](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=48.72s)]
*  It's a singularity like a black hole because you don't know what happens after that. [[00:00:49](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=49.72s)]
*  It's hard to predict. [[00:00:55](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=55.2s)]
*  So I think we should be cautious with AI. [[00:00:56](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=56.86s)]
*  I think there should be some government oversight because it's a danger to the public. [[00:01:02](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=62.58s)]
*  So when you have things that are a danger to the public, like let's say food and drugs, [[00:01:09](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=69.78s)]
*  that's why we have the Food and Drug Administration and the Federal Aviation Administration, the [[00:01:18](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=78.42s)]
*  FCC. [[00:01:25](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=85.22s)]
*  We have these agencies to oversee things that affect the public where there could be public [[00:01:27](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=87.02000000000001s)]
*  harm. [[00:01:33](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=93.26s)]
*  And you don't want companies cutting corners on safety and then having people suffer as [[00:01:35](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=95.18s)]
*  a result. [[00:01:42](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=102.82000000000001s)]
*  So that's why I've actually for a long time been a strong advocate of AI regulation. [[00:01:43](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=103.82000000000001s)]
*  So that I think regulation is, it's not fun to be regulated. [[00:01:51](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=111.94s)]
*  It's somewhat of a somewhat arduous to be regulated. [[00:01:57](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=117.94s)]
*  I have a lot of experience with regulated industries because obviously automotive is [[00:02:03](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=123.46s)]
*  highly regulated. [[00:02:08](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=128.3s)]
*  You could fill this room with all the regulations that are required for a production car just [[00:02:09](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=129.3s)]
*  in the United States. [[00:02:14](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=134.5s)]
*  And then there's a whole different set of regulations in Europe and China and the rest [[00:02:15](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=135.5s)]
*  of the world. [[00:02:18](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=138.66s)]
*  So very familiar with being overseen by a lot of regulators. [[00:02:19](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=139.66s)]
*  And the same thing is true with rockets. [[00:02:25](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=145.18s)]
*  You can't just willy-nilly shoot rockets off, not big ones anyway, because the FAA oversees [[00:02:27](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=147.38s)]
*  that. [[00:02:33](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=153.22s)]
*  And then even to get a launch license, there are probably half a dozen or more federal [[00:02:34](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=154.22s)]
*  agencies that need to approve it, plus state agencies. [[00:02:41](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=161.46s)]
*  So I've been through so many regulatory situations, it's insane. [[00:02:44](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=164.86s)]
*  Sometimes people think I'm some sort of regulatory maverick that defies regulators on a regular [[00:02:51](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=171.74s)]
*  basis, but this is actually not the case. [[00:02:58](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=178.42000000000002s)]
*  So once in a blue moon, rarely I will disagree with regulators. [[00:03:01](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=181.34s)]
*  But the vast majority of the time, my companies agree with the regulations and comply. [[00:03:08](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=188.18s)]
*  So I think we should take this seriously and we should have a regulatory agency. [[00:03:14](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=194.42s)]
*  I think it needs to start with a group that initially seeks insight into AI, then solicits [[00:03:21](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=201.26s)]
*  opinion from industry, and then has proposed rulemaking. [[00:03:30](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=210.22s)]
*  And then those rules will probably, hopefully grudgingly be accepted by the major players [[00:03:34](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=214.82s)]
*  in AI. [[00:03:44](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=224.17999999999998s)]
*  And I think we'll have a better chance of advanced AI being beneficial to humanity in [[00:03:45](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=225.5s)]
*  that circumstance. [[00:03:54](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=234.05999999999997s)]
*  But all regulations start with a perceived danger, planes fall out of the sky or food [[00:03:55](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=235.05999999999997s)]
*  causes botulism. [[00:03:59](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=239.33999999999997s)]
*  Yes. [[00:04:00](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=240.33999999999997s)]
*  I don't think the average person playing with AI on his iPhone perceives any danger. [[00:04:01](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=241.33999999999997s)]
*  Can you just roughly explain what you think the dangers might be? [[00:04:05](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=245.1s)]
*  Yeah, so the danger, really, AI is perhaps more dangerous than, say, mismanaged aircraft [[00:04:09](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=249.1s)]
*  design or production maintenance or bad car production in the sense that it has the potential. [[00:04:23](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=263.18s)]
*  However small one may regard that probability, but it is non-trivial. [[00:04:30](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=270.18s)]
*  It has the potential of civilizational destruction. [[00:04:34](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=274.3s)]
*  There's movies like Terminator, but it wouldn't quite happen like Terminator because the intelligence [[00:04:37](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=277.74s)]
*  would be in the data centers. [[00:04:42](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=282.78000000000003s)]
*  The robot's just the end effector. [[00:04:45](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=285.1s)]
*  But I think perhaps what you may be alluding to here is that regulations are really only [[00:04:47](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=287.46s)]
*  put into effect after something terrible has happened. [[00:04:53](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=293.42s)]
*  That's correct. [[00:04:55](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=295.86s)]
*  If that's the case for AI and we're only putting regulations after something terrible has happened, [[00:04:56](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=296.86s)]
*  it may be too late to actually put the regulations in place. [[00:05:00](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=300.62s)]
*  The AI may be in control at that point. [[00:05:03](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=303.46000000000004s)]
*  Do you think that's real? [[00:05:06](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=306.1s)]
*  It is conceivable that AI could take control and reach a point where you couldn't turn [[00:05:08](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=308.70000000000005s)]
*  it off and it would be making the decisions for people. [[00:05:13](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=313.06s)]
*  Yeah, absolutely. [[00:05:16](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=316.70000000000005s)]
*  Absolutely. [[00:05:17](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=317.70000000000005s)]
*  No, that's definitely where things are headed, for sure. [[00:05:18](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=318.70000000000005s)]
*  Things like, say, Chatch-EVT, which is based on JPD-4 from OpenAI, which is a company that [[00:05:27](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=327.70000000000005s)]
*  I played a critical role in creating, unfortunately. [[00:05:32](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=332.62s)]
*  Back when it was a nonprofit? [[00:05:38](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=338.34s)]
*  Yes. [[00:05:41](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=341.34s)]
*  The reason OpenAI exists at all is that Larry Page and I used to be close friends and I [[00:05:44](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=344.62s)]
*  would stay at his house in Palo Alto and I would talk to him late into the night about [[00:05:50](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=350.58s)]
*  AI safety. [[00:05:54](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=354.86s)]
*  My perception was that Larry was not taking AI safety seriously enough. [[00:05:58](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=358.34000000000003s)]
*  What did he say about it? [[00:06:05](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=365.06s)]
*  He really seemed to be one sort of digital superintelligence, basically digital god, [[00:06:06](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=366.34000000000003s)]
*  if you will, as soon as possible. [[00:06:13](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=373.74s)]
*  He wanted that? [[00:06:16](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=376.82000000000005s)]
*  Yes. [[00:06:18](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=378.14000000000004s)]
*  He's made many public statements over the years that the whole goal of Google is what's [[00:06:19](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=379.14000000000004s)]
*  called AGI, artificial general intelligence, or artificial superintelligence. [[00:06:24](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=384.66s)]
*  I agree with him that there's great potential for good, but there's also potential for bad. [[00:06:29](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=389.02000000000004s)]
*  If you've got some radical new technology, you want to try to take the set of actions [[00:06:35](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=395.06s)]
*  that maximize probably it will do good and minimize probably it will do bad things. [[00:06:40](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=400.1s)]
*  It can't just be helpful others. [[00:06:45](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=405.94000000000005s)]
*  Just go barreling forward and hope for the best. [[00:06:47](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=407.38s)]
*  Then at one point I said, what about, we've got to make sure humanity is okay here. [[00:06:52](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=412.38s)]
*  Then he called me a speciest. [[00:07:02](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=422.5s)]
*  Did he use that term? [[00:07:04](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=424.5s)]
*  Yes. [[00:07:08](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=428.5s)]
*  There were witnesses. [[00:07:09](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=429.5s)]
*  I wasn't the only one there when he called me a speciest. [[00:07:10](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=430.5s)]
*  I was like, okay, that's it. [[00:07:13](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=433.5s)]
*  Yes, I'm a speciest. [[00:07:16](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=436.5s)]
*  You got me. [[00:07:18](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=438.5s)]
*  What are you? [[00:07:19](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=439.5s)]
*  Yes, I'm fully a speciest. [[00:07:22](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=442.5s)]
*  Busted. [[00:07:25](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=445.5s)]
*  That was his last straw. [[00:07:27](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=447.5s)]
*  At the time, Google had acquired DeepMind. [[00:07:30](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=450.5s)]
*  Google and DeepMind together had about three-quarters of all the AI talent in the world. [[00:07:34](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=454.5s)]
*  They obviously had a tremendous amount of money and more computers than anyone else. [[00:07:38](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=458.5s)]
*  I'm like, okay, we have a unipolar world here where there's just one company that has close [[00:07:42](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=462.5s)]
*  to a monopoly on AI talent and computers, like scaled computing. [[00:07:49](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=469.66s)]
*  The person who's in charge doesn't seem to care about safety. [[00:07:56](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=476.66s)]
*  This is not good. [[00:08:00](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=480.66s)]
*  Then I thought, what's the furthest thing from Google? [[00:08:03](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=483.66s)]
*  It would be a nonprofit that is fully open because Google was closed for profit. [[00:08:06](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=486.66s)]
*  That's why the open and open AI refers to open source transparency so people know what's [[00:08:11](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=491.66s)]
*  going on. [[00:08:17](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=497.66s)]
*  While I'm normally in favor of full profit, we don't want this to be a profit maximizing [[00:08:18](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=498.82000000000005s)]
*  demon from hell. [[00:08:26](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=506.82000000000005s)]
*  That just never stops. [[00:08:29](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=509.82000000000005s)]
*  That's how open AI works. [[00:08:32](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=512.82s)]
*  You want speciest incentives here. [[00:08:33](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=513.82s)]
*  Yes, I think we want pro-human, like the future good for the humans. [[00:08:36](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=516.82s)]
*  Yes. [[00:08:41](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=521.82s)]
*  Because we're humans. [[00:08:42](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=522.82s)]
*  Can you just put it, I keep pressing you, but just for people who haven't thought this [[00:08:43](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=523.82s)]
*  through and aren't familiar with it and the cool parts of artificial intelligence are [[00:08:46](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=526.98s)]
*  so obvious, write your college paper for you, write a limerick about yourself. [[00:08:51](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=531.98s)]
*  There's a lot there that's fun and useful. [[00:08:56](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=536.98s)]
*  Can you be more precise about what's potentially dangerous and scary? [[00:09:00](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=540.98s)]
*  What could it do? [[00:09:05](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=545.98s)]
*  What specifically are you worried about? [[00:09:06](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=546.98s)]
*  Going with old saying, the pen is mightier than the sword. [[00:09:09](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=549.98s)]
*  If you have a super intelligent AI that is capable of writing incredibly well and in [[00:09:15](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=555.14s)]
*  a way that is very influential, convincing, and is constantly figuring out what is more [[00:09:22](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=562.14s)]
*  convincing to people over time and then enters social media, for example Twitter, but also [[00:09:33](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=573.14s)]
*  Facebook and others, and potentially manipulates public opinion in a way that is very bad, [[00:09:39](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=579.1800000000001s)]
*  how would we even know? [[00:09:46](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=586.1800000000001s)]
*  How do we even know? [[00:09:49](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=589.1800000000001s)]
*  To sum up, in the words of Elon Musk, for all human history, human beings have been [[00:09:52](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=592.1800000000001s)]
*  the smartest beings on the planet. [[00:09:57](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=597.1800000000001s)]
*  Now, human beings have created something that is far smarter than they are, and the consequences [[00:10:00](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=600.1800000000001s)]
*  of that are impossible to predict. [[00:10:05](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=605.1800000000001s)]
*  The people who created it don't care. [[00:10:08](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=608.78s)]
*  In fact, as he put it, Google founder Larry Page, a former friend of his, is looking to [[00:10:11](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=611.78s)]
*  build a quote, digital god, and believes that anybody who's worried about that is a speciesist. [[00:10:16](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=616.78s)]
*  In other words, is looking out for human beings first. [[00:10:21](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=621.78s)]
*  Elon Musk responded, as a human being, it's okay to look out for human beings first. [[00:10:25](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=625.78s)]
*  Then at the end, he said the real problem with AI is not simply that it will jump the [[00:10:31](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=631.78s)]
*  button and become autonomous and you can't turn it off. [[00:10:36](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=636.78s)]
*  In the short term, the problem with AI is that it might control your brain through words. [[00:10:39](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=639.78s)]
*  And this is the application that we need to worry about now, particularly going into the [[00:10:44](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=644.78s)]
*  next presidential election. [[00:10:48](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=648.78s)]
*  The Democratic Party, as usual, was ahead of the curve on this. [[00:10:50](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=650.78s)]
*  They've been thinking about how to harness AI for political power. [[00:10:53](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=653.78s)]
*  More on that next. [[00:10:57](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=657.78s)]
*  This has been your life from Tucker Carlson tonight. [[00:11:06](https://www.youtube.com/watch?v=a2ZBEC16yH4&t=666.78s)]
