---
Date Generated: May 24, 2025
Transcription Model: whisper medium 20231117
Length: 924s
Video Keywords: []
Video Views: 39816
Video Rating: None
Video Description: In 2011, Marc Andreessen said, “software is eating the world.” And in the last year, we’ve seen a new wave of generative AI, with some apps becoming some of the most swiftly adopted software products of all time. 

In this first part of our three-part series – we explore the terminology and technology that is now the backbone of the AI models taking the world by storm. We explore what GPUs are, how they work, and the key players like Nvidia competing for chip dominance.

Look out for the rest of our series, where we dive even deeper; covering supply and demand mechanics, where open source plays a role, and of course… how much all of this truly costs!

Topics Covered: 
00:00 – AI terminology and technology
03:54 – Chips, semiconductors, servers, and compute
05:07 – CPUs and GPUs
06:16 – Future architecture and performance
07:12 –The hardware ecosystem
09:20 – Software optimizations
11:45 –What do we expect for the future?
14:25 – Upcoming episodes on market dynamics and cost 

Resources: 
Find Guido on LinkedIn: https://www.linkedin.com/in/appenz/
Find Guido on Twitter: https://twitter.com/appenz
 
Find a16z on Twitter: https://twitter.com/a16z 
Find a16z on LinkedIn: https://www.linkedin.com/company/a16z 
Subscribe on your favorite podcast app: https://a16z.simplecast.com/ 
Follow our host: https://twitter.com/stephsmithio 

Please note that the content here is for informational purposes only; should NOT be taken as legal, business, tax, or investment advice or be used to evaluate any investment or security; and is not directed at any investors or potential investors in any a16z fund. a16z and its affiliates may maintain investments in the companies discussed. For more details please see a16z.com/disclosures.
---

# AI Hardware, Explained.
**The a16z Podcast:** [August 16, 2023](https://www.youtube.com/watch?v=-s_Ui5j0Guw)
*  The most commonly used chips today are AI accelerators. [[00:00:00](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=0.0s)]
*  You would have thought that my gaming PC and my Bitcoin miner would eventually become a good AI engineer. [[00:00:04](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=4.4s)]
*  How do you see this industry moving forward? [[00:00:09](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=9.8s)]
*  That's a great question. Moore's law is actually still, as of today, alive and kicking, right? [[00:00:12](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=12.44s)]
*  Power is becoming an issue, heat is becoming an issue, and we need to rely more and more on parallel processing. [[00:00:17](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=17.36s)]
*  In 2011, Mark Andreessen said, software is eating the world. [[00:00:22](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=22.08s)]
*  And the decade that followed just solidified this notion, with software infiltrating nearly every aspect of our lives. [[00:00:33](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=33.2s)]
*  The last year in particular introduced a new wave of generative AI, [[00:00:41](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=41.239999999999995s)]
*  with some apps becoming some of the most swiftly adopted software products of all time. [[00:00:45](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=45.519999999999996s)]
*  And just like all the other software that came before it, [[00:00:51](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=51.8s)]
*  AI software is fundamentally underpinned by the hardware that runs the underlying computation. [[00:00:55](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=55.0s)]
*  So if software is becoming more important than ever, then hardware is following suit. [[00:01:01](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=61.64s)]
*  Plus, the world is constantly generating more data, [[00:01:07](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=67.36s)]
*  and unlocking the full potential of these technologies, from longer context windows to multimodality, [[00:01:11](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=71.08s)]
*  means a constant need for faster and more resilient hardware. [[00:01:16](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=76.8s)]
*  And it's equally important for us to understand who builds and controls the supply of this resource, [[00:01:21](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=81.60000000000001s)]
*  especially since many of even the most established AI companies are now hardware constrained, [[00:01:27](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=87.04s)]
*  with some reputable sources indicating that demand for AI hardware outstrips supply by a factor of 10. [[00:01:33](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=93.32000000000001s)]
*  That is exactly why we've created this mini-series on AI hardware. [[00:01:40](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=100.48s)]
*  We'll take you on a journey through understanding the hardware that has long powered our computers, [[00:01:45](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=105.28s)]
*  but is now the backbone of these AI models absolutely taking the world by storm. [[00:01:49](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=109.80000000000001s)]
*  And in this first segment, we dive into the terminology and technology from GPU to TPU, [[00:01:55](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=115.12s)]
*  including what they are, how they work, the key players like Nvidia competing for chip dominance, [[00:02:00](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=120.84s)]
*  and also we address the question, is Moore's Law dead? [[00:02:07](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=127.16000000000001s)]
*  But make sure to look out for the rest of our series, where we dive even deeper, [[00:02:11](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=131.72s)]
*  covering supply and demand mechanics, including why we can't just print our way over shortage, [[00:02:16](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=136.23999999999998s)]
*  how founders can get access to inventory, whether they should think about owning or renting, [[00:02:21](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=141.32s)]
*  where open source plays a role, and of course, how much all of this truly costs. [[00:02:26](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=146.28s)]
*  And across all three videos, we explore with the help of a 16Z special advisor, [[00:02:31](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=151.95999999999998s)]
*  Guido Appenzeller, someone who is truly uniquely suited for this deep dive as a storied infrastructure expert. [[00:02:37](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=157.16s)]
*  I spent my last couple of years mostly in software, but most recently before joining Andres Norowitz. [[00:02:43](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=163.88s)]
*  I actually was CTO for Intel's data center group dealing a lot with hardware and the low level components. [[00:02:50](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=170.44s)]
*  It's given me sort of, I think, a good insight how large data centers work, [[00:02:55](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=175.72s)]
*  what the basic components are that make all of this AI boom possible today [[00:02:59](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=179.32s)]
*  and that really underpin this great technological ecosystem. [[00:03:04](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=184.64000000000001s)]
*  Guido has also spent time at Ubico, VMware, BigSwitch Networks, and more. [[00:03:08](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=188.4s)]
*  But let's get into it. [[00:03:13](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=193.6s)]
*  As a reminder, the content here is for informational purposes only, [[00:03:15](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=195.28s)]
*  should not be taken as legal, business, tax, or investment advice, [[00:03:19](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=199.04s)]
*  or be used to evaluate any investment or security, [[00:03:22](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=202.48s)]
*  and is not directed at any investors or potential investors in any A16Z fund. [[00:03:25](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=205.04s)]
*  Please note that A16Z and its affiliates may also maintain investments in the companies discussed in this podcast. [[00:03:29](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=209.92s)]
*  For more details, including a link to our investments, please see a16z.com slash disclosures. [[00:03:36](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=216.44s)]
*  We are increasingly hearing terms like chips, semiconductors, servers, and compute. [[00:03:43](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=223.6s)]
*  But are all of these the same thing? [[00:03:46](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=226.79999999999998s)]
*  And what role do they play in our AI future? [[00:03:49](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=229.6s)]
*  If you're running any kind of AI algorithm, right, this AI algorithm runs on a chip, right? [[00:03:52](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=232.4s)]
*  And the most commonly used chips today are AI accelerators, [[00:03:57](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=237.04s)]
*  which are, in terms of how they're built, actually very close to the AI algorithm. [[00:04:01](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=241.44s)]
*  And they're very, very similar to the AI algorithm. [[00:04:04](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=244.56s)]
*  So, if you're running a computer, you're running a computer, [[00:04:07](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=247.44s)]
*  AI accelerators, which are, in terms of how they're built, actually very close to graphics chips. [[00:04:10](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=250.32000000000002s)]
*  And so the cards that these chips are on that are in these servers, [[00:04:15](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=255.12s)]
*  often referred to as GPUs, which stands for Graphics Processing Unit, [[00:04:18](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=258.16s)]
*  which is kind of funny, right? [[00:04:21](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=261.68s)]
*  They're not doing graphics, obviously, but it's a very similar type of technology. [[00:04:22](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=262.88s)]
*  If you look inside of them, they basically are very good at processing [[00:04:26](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=266.48s)]
*  very large number of math operations per cycle in a very short period of time. [[00:04:30](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=270.72s)]
*  So, very classically, like an old-fashioned CPU would run one instruction every cycle, [[00:04:36](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=276.16s)]
*  and then they had multiple cores. [[00:04:41](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=281.36s)]
*  So, maybe now modern CPU can do a couple of 10 instructions. [[00:04:42](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=282.40000000000003s)]
*  But these sort of modern AI cards, they can do more than 100,000 instructions per cycle. [[00:04:46](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=286.40000000000003s)]
*  So, they're extremely performant. [[00:04:52](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=292.08000000000004s)]
*  So, this is a GPU. [[00:04:53](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=293.6s)]
*  These GPUs run inside of servers. [[00:04:54](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=294.64000000000004s)]
*  You think of them as big boxes, you know, [[00:04:56](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=296.64000000000004s)]
*  have a power plug on the outside and a networking plug. [[00:04:58](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=298.32000000000005s)]
*  And then these servers sit in data centers where you have racks and racks of them [[00:05:00](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=300.96000000000004s)]
*  that do the actual computing. [[00:05:04](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=304.0s)]
*  Let's quickly recap. [[00:05:06](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=306.16s)]
*  CPU is Central Processing Unit and GPU is Graphics Processing Unit. [[00:05:07](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=307.6s)]
*  And while both CPUs and GPUs today can both perform parallel processing, [[00:05:12](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=312.72s)]
*  the degree of parallelization is what sets GPUs apart for certain workloads. [[00:05:18](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=318.0s)]
*  So, for example, CPUs can actually do tens or even thousands of floating point operations [[00:05:22](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=322.96000000000004s)]
*  per cycle, but a GPU can now do over 100,000. [[00:05:28](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=328.24s)]
*  The basic idea of a GPU is that instead of just working with individual values, [[00:05:32](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=332.72s)]
*  it works with vectors or even matrices or tensors more generally. [[00:05:37](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=337.36s)]
*  The TPU, for example, is Google's name for these kind of chips. [[00:05:41](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=341.04s)]
*  They call them tensor processing units, which is actually a pretty good name for them. [[00:05:44](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=344.08000000000004s)]
*  The cores in these modern GPUs are often called tensor cores. [[00:05:48](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=348.0s)]
*  They operate on tensors. [[00:05:51](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=351.52000000000004s)]
*  And basically, the core of their value propositions is they can do matrix multiplication. [[00:05:53](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=353.76000000000005s)]
*  So, remember, matrix, like the rows and columns of numbers, [[00:05:59](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=359.12s)]
*  they can, for example, multiply two matrices in a single cycle. [[00:06:03](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=363.12s)]
*  So, in a very, very fast operation. [[00:06:06](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=366.32s)]
*  And that's really what gives us the speed that's necessary [[00:06:08](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=368.32s)]
*  to run these incredibly large language and image models that make generative AI today. [[00:06:10](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=370.96000000000004s)]
*  Today's GPUs are far more powerful than their ancestors. [[00:06:16](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=376.32000000000005s)]
*  Whether we're comparing to the earliest graphics cards in arcade gaming days 50 years ago [[00:06:20](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=380.24s)]
*  or the GeForce 256, the first personal computer GPU unveiled by Nvidia in 1999. [[00:06:25](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=385.44s)]
*  But is it surprising that we're seeing this chip design applied so readily to this emerging space [[00:06:32](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=392.96s)]
*  of AI? Or should we expect a new architecture to evolve and be more performant in the future? [[00:06:38](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=398.08s)]
*  In one way, I think it's very surprising, right? [[00:06:44](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=404.24s)]
*  Who would have thought that my gaming PC and my Bitcoin miner would eventually [[00:06:45](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=405.76s)]
*  become a good AI engineer? [[00:06:49](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=409.44s)]
*  At the same time, what all of these problems have in common is that you want to execute many [[00:06:51](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=411.59999999999997s)]
*  operations in parallel. You can think a GPU is something that was built for graphics, [[00:06:59](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=419.2s)]
*  but you can think of them also just as something that's very good in performing the same operation [[00:07:02](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=422.79999999999995s)]
*  and a very large number of parallel inputs, right? A very large vector, a very large matrix. [[00:07:07](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=427.35999999999996s)]
*  All right, so perhaps it's not so surprising that Nvidia's prize GPUs are aligned to this AI wave. [[00:07:11](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=431.68s)]
*  But they're also not the only company participating. [[00:07:17](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=437.68s)]
*  Here is Guido breaking down the hardware ecosystem. [[00:07:21](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=441.52s)]
*  The ecosystem comes in many layers, right? So let's start with the chips at the bottom. [[00:07:24](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=444.56s)]
*  Nvidia is king of the hill at the moment right there. A100 is the workhorse that powers the [[00:07:29](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=449.52s)]
*  current AI revolution. They're coming up with a new one called the H100, which is the next generation. [[00:07:34](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=454.4s)]
*  There's a couple of other vendors in this space. Intel has something called Gaudi, Gaudi 2, right? [[00:07:39](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=459.44s)]
*  That's as well as that graphics card with ARK. They're seeing some usage. AMD has a chip in this [[00:07:44](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=464.56s)]
*  space. And then we have the large clouds that are starting to build or in some cases have been [[00:07:50](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=470.32s)]
*  building for some time their own chips, right? Google with the TPU you mentioned before, right? [[00:07:54](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=474.96s)]
*  That is quite popular. And Amazon has a chip called Tranium for training and Inferencia for [[00:07:58](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=478.56s)]
*  inference. And we'll probably see more of those in the future from some of these vendors. But [[00:08:04](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=484.32s)]
*  at the moment, Nvidia still has a very, very strong position as the vast majority of [[00:08:08](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=488.88s)]
*  training is going on on their chips. When we think about the different chips, [[00:08:13](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=493.03999999999996s)]
*  you mentioned like the A100s are the strongest and maybe there's the most demand for those. [[00:08:16](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=496.56s)]
*  But how do they compare to some of these chips created by other companies? Is it like, [[00:08:20](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=500.96s)]
*  you know, double the performance or is there some other metric or factor that makes them [[00:08:25](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=505.04s)]
*  much more performant? That's a great question. You know, if you look at the pure hardware statistics, [[00:08:30](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=510.16s)]
*  so how many floating point operations per second can these chips do? There's others that are very [[00:08:35](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=515.68s)]
*  competitive with what Nvidia has. Nvidia's big advantage is that they have a very mature software [[00:08:39](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=519.92s)]
*  ecosystem. So imagine you are an artificial intelligence developer or engineer or researcher, [[00:08:45](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=525.28s)]
*  you're often using a model that's open source, you know, somebody else developed. And, you know, [[00:08:50](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=530.0799999999999s)]
*  how fast that model runs, in many cases depends on how well it's optimized for a particular chip. [[00:08:56](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=536.16s)]
*  And so the big advantage Nvidia has today is that their software ecosystem is just so much [[00:09:02](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=542.0799999999999s)]
*  more mature, right? I can grab a model, it has all the necessary optimizations for Nvidia to run out [[00:09:05](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=545.8399999999999s)]
*  of the box, right? I don't have to do anything. But with some of these other chips, I may have to [[00:09:10](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=550.96s)]
*  do a lot more of these optimizations myself, right? And that's what gives them the strategic [[00:09:15](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=555.0400000000001s)]
*  advantage at the moment. So as we've touched on, AI software is heavily dependent on hardware. [[00:09:19](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=559.44s)]
*  But what Guido is pointing towards here is the performance of hardware being heavily integrated [[00:09:25](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=565.44s)]
*  with software. So Nvidia's CUDA system makes it easier for engineers to plug in and make [[00:09:29](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=569.84s)]
*  optimizations like running with lower precision numbers. Here is Guido speaking to the kind of [[00:09:35](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=575.36s)]
*  optimizations that do exist. It happens at all layers of the stack. Some of it is coming from [[00:09:41](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=581.36s)]
*  academia. Some of it is done by the large companies that operate in the space, right? Some of them is [[00:09:45](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=585.84s)]
*  frankly done by enthusiasts that just want to see their model run faster. But to give an idea of [[00:09:51](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=591.2s)]
*  how this works, like for example, you know, typically a floating point number is represented [[00:09:56](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=596.0s)]
*  in 32 bits, right? And some people figured out how to reduce that to 16 bits. And somebody was like, [[00:10:01](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=601.2s)]
*  well, actually, we can do it in eight bits. And you have to be really careful how you do that. You have [[00:10:05](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=605.6s)]
*  to normalize to make sure it doesn't overrun or underrun, right? But if you know, as I think [[00:10:09](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=609.12s)]
*  probably you can use much, much shorter floats or integers for these calculations. There's many [[00:10:14](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=614.5600000000001s)]
*  tricks like that, really good AI developers use to squeeze more performance out of the chips that [[00:10:19](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=619.0400000000001s)]
*  they have. So to reiterate Guido's point, floating point numbers are typically represented in 32 [[00:10:25](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=625.2s)]
*  bits. That's 32 zeros and ones or binary digits, with the first bit being for sign, the next eight [[00:10:31](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=631.04s)]
*  for the exponent, and the next 23 for the fraction. This gives a fairly large range between the [[00:10:37](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=637.8399999999999s)]
*  smallest possible value and the largest possible value, but also allows many steps in between. [[00:10:44](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=644.16s)]
*  Now, when many people think of semiconductors, they naturally think of Moore's law. That's the [[00:10:51](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=651.12s)]
*  term that describes the phenomenon observed by Gordon Moore, by the way, back in 1965, [[00:10:56](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=656.0s)]
*  where the number of transistors in an integrated circuit doubles every two years. But despite our [[00:11:01](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=661.36s)]
*  collective success for decades to continue to push more computation onto smaller chips, [[00:11:08](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=668.24s)]
*  are we now at the limits of lithography? For example, an Apple M1 chip from 2022 has 116 [[00:11:13](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=673.92s)]
*  billion, that's billion with a B, transistors. And if we compare that to the ARM 1 processor from [[00:11:21](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=681.76s)]
*  1985, that had 25,000. And by the way, the Apple M1 chip is not even the highest transistor count [[00:11:27](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=687.12s)]
*  today. I believe that belongs to the wafer scale engine 2 by Cerebris, with 2.6 trillion transistors. [[00:11:34](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=694.56s)]
*  So looking ahead, are we at the point where we really don't see the same kind of advancement [[00:11:42](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=702.4s)]
*  in at least the physical architecture of chips? And if so, where do we see advancements moving [[00:11:48](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=708.4s)]
*  forward? Is it in the software? Is it in the specialization of these chips? How do you see this [[00:11:54](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=714.64s)]
*  industry moving forward? Yeah, great question. So there's still things to tease apart there. [[00:11:59](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=719.68s)]
*  Moore's law is actually still, as of today, alive and kicking. So Moore's law talks about the [[00:12:06](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=726.72s)]
*  density of transistors on a chip, and we're still increasing that. The scale of transistors going [[00:12:12](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=732.72s)]
*  down, I guess it's exactly the same speed, I don't know. But as of today, if you plot the [[00:12:18](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=738.0s)]
*  curve, it seems to be intact. There's a second thing called Dennard scaling, which used to, [[00:12:22](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=742.72s)]
*  as we say, just as the number of transistors I can squeeze onto a chip doubles every 18 months or so, [[00:12:30](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=750.08s)]
*  it essentially meant that the power at the same time would decrease by the same factor. [[00:12:36](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=756.88s)]
*  It says something about frequency, but that's the net out compass is power. And that's for the last [[00:12:42](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=762.0s)]
*  10, 15 years or so, no longer. It's true. If you look at the frequency of a CPU, [[00:12:47](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=767.12s)]
*  it hasn't moved much over the past 10, 12, 15 years. The net result of this is we're getting [[00:12:53](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=773.12s)]
*  chips that have more transistors, but each individual core doesn't actually run faster. [[00:12:58](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=778.48s)]
*  And what this means is we have to have lots and lots more parallel cores. And this is why these [[00:13:05](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=785.2s)]
*  tensor operations are so attractive. I can't add, like on a single core, I can't add numbers [[00:13:09](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=789.28s)]
*  more quickly, but if I can do a matrix operation instead, and specifically do many of them in [[00:13:14](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=794.72s)]
*  parallel at the same time. The second big consequence of that is that our chips are getting [[00:13:19](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=799.2s)]
*  more and more power hungry. If you look at even a graphics card for a gaming PC today, you have [[00:13:23](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=803.6800000000001s)]
*  these graphics cards that are like hundreds of watts of power, another 500 watt card, which is [[00:13:30](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=810.1600000000001s)]
*  much, much more than they used to be. And that trend is going to continue. And we're seeing [[00:13:35](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=815.52s)]
*  what's happening in data centers, seeing more and more things like liquid cooling, [[00:13:40](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=820.64s)]
*  at least being experimented with, or in some cases, getting deployed, where basically the [[00:13:44](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=824.16s)]
*  energy densities for these AI chips is getting so high that we need novel cooling solutions [[00:13:49](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=829.1999999999999s)]
*  to make them happen. Moore's law, yes, but power is becoming an issue, heat is becoming an issue, [[00:13:55](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=835.68s)]
*  and we need to rely more and more on parallel processing. So it sounds like Moore's law is [[00:14:01](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=841.4399999999999s)]
*  indeed not quite dead, but perhaps a little more complex than it once was. Performance increases [[00:14:05](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=845.84s)]
*  continue as we integrate parallel cores, but we're also seeing chips become a lot more power hungry. [[00:14:11](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=851.52s)]
*  All of this will continue being dynamic as demand continues to outpace supply for high performance [[00:14:17](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=857.6s)]
*  chips. So as we look ahead, what does all this mean for competition and cost? You'll learn a lot more [[00:14:22](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=862.8000000000001s)]
*  about that in the rest of our AI hardware series, tackling the questions that everybody is asking, [[00:14:29](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=869.44s)]
*  including... We currently don't have as many AI chips or servers as we'd like to have. How do you [[00:14:35](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=875.04s)]
*  think about the relationship between compute, capital, and then the technology that we have today? [[00:14:41](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=881.68s)]
*  Yeah, that's a million dollar question, or maybe a trillion dollar question. I don't know. [[00:14:47](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=887.68s)]
*  We'll see you there. Thank you so much for listening to the A16Z Podcast. What we're trying [[00:14:52](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=892.0s)]
*  to do here is provide an informed, clear-eyed, but also optimistic take on technology and its future. [[00:14:57](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=897.5999999999999s)]
*  We're trying to do that by featuring some of the most inspiring people in the things that they're [[00:15:05](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=905.04s)]
*  building. So if that is interesting to you and you'd like to join us on this journey, go ahead [[00:15:09](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=909.68s)]
*  and click subscribe and make sure to let us know in the comments below what you'd like to see us [[00:15:15](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=915.04s)]
*  cover next. Thank you so much for listening and we'll see you next time. [[00:15:19](https://www.youtube.com/watch?v=-s_Ui5j0Guw&t=919.28s)]
