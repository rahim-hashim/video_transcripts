---
Date Generated: May 24, 2025
Transcription Model: whisper medium 20231117
Length: 3323s
Video Keywords: []
Video Views: 21640
Video Rating: None
Video Description: The last few months have seen dramatic — almost magical — applications of expert generative AI released to the public. But what does this mean for healthcare and bio? Vijay and Marc sat down for a wide-ranging discussion on AI as an additive superpower…for healthcare as well as screenplays, music, and more.

Stay Updated:
→ Find us on Twitter: https://twitter.com/a16z 
→ Find us on LinkedIn: https://www.linkedin.com/company/a16z 
→ Subscribe on your favorite podcast app: https://a16z.simplecast.com/
→ Follow our host: https://twitter.com/stephsmithio

Please note that the content here is for informational purposes only; should NOT be taken as legal, business, tax, or investment advice or be used to evaluate any investment or security; and is not directed at any investors or potential investors in any a16z fund. For more details please see a16z.com/disclosures.
---

# Expert AI as a Healthcare Superpower
**The a16z Podcast:** [January 09, 2023](https://www.youtube.com/watch?v=c7ScUDYSRYo)
*  Hey, I'm Vijay Pandey. [[00:00:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=0.0s)]
*  I'm the founding general partner of the A16Z Bio and Health Fund. [[00:00:21](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=21.72s)]
*  And I'm Mark Andreessen, co-founder of A16Z. [[00:00:26](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=26.0s)]
*  Mark, thank you so much for joining us. [[00:00:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=29.4s)]
*  Yeah, it's great to be here. [[00:00:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=31.08s)]
*  So you famously wrote about software eating the world. [[00:00:32](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=32.44s)]
*  And that was basically, what, 10 plus years ago? [[00:00:35](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=35.56s)]
*  And actually, that very much seems to have come to fruition. [[00:00:38](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=38.68s)]
*  If you look at all these other industries that software really [[00:00:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=41.24s)]
*  wasn't part of, software has actually become a dominant part. [[00:00:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=43.96s)]
*  But actually, this year's been kind of an amazing year [[00:00:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=47.64s)]
*  for another type of software, for AI. [[00:00:49](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=49.84s)]
*  And I'm curious to sort of talk about the arc of what we think [[00:00:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=52.04s)]
*  is going to happen in the future based on what we've seen in the past. [[00:00:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=55.56s)]
*  And really how this new technology is going to change everything, [[00:00:58](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=58.839999999999996s)]
*  much like we've seen software change the last 10 years. [[00:01:01](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=61.32s)]
*  I'm curious what you think for just this year. [[00:01:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=64.44s)]
*  It's been kind of an amazing year. [[00:01:06](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=66.84s)]
*  We always seem like not much happens in any given year. [[00:01:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=68.28s)]
*  But 2022 seems to have been an amazing year for AI. [[00:01:10](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=70.67999999999999s)]
*  Well, so Vladimir Lenin once said, there are decades in which nothing happens. [[00:01:14](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=74.28s)]
*  And then there are weeks in which decades happen. [[00:01:19](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=79.56s)]
*  And let's not hope that happens politically anymore. [[00:01:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=83.08s)]
*  But it does happen in science and technology. [[00:01:26](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=86.03999999999999s)]
*  It does happen. [[00:01:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=87.96s)]
*  There are moments where things kind of hit critical mass. [[00:01:28](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=88.44s)]
*  And this sort of AI machine learning revolution seems like that's what's happening right now. [[00:01:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=91.96s)]
*  It's been interesting to watch. [[00:01:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=97.47999999999999s)]
*  It feels to me, at least, it was like there was a breakthrough moment in 2012 [[00:01:40](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=100.03999999999999s)]
*  that had to do with images. [[00:01:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=103.24s)]
*  And then there was a lot of work subsequently that led to things like the creation of self-driving [[00:01:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=104.83999999999999s)]
*  cars based on that. [[00:01:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=108.44s)]
*  And then there was some, it feels like some natural language breakthrough maybe three years ago. [[00:01:50](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=110.03999999999999s)]
*  And now that's really catalyzed into this kind of whole thing that we see happening around [[00:01:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=114.44s)]
*  GPT and text generation. [[00:01:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=117.96s)]
*  And then even other applications, transcription, is getting much better. [[00:02:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=120.75999999999999s)]
*  All of a sudden speech synthesis is getting much better. [[00:02:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=124.67999999999999s)]
*  And then now you've got this artistic revolution happening with image creation. [[00:02:07](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=127.72s)]
*  And now video creation is right next, coming up now really fast. [[00:02:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=133.4s)]
*  And so it seems like one of those catalytic moments. [[00:02:18](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=138.44s)]
*  And then it's like every week now, it seems like there's fundamental breakthroughs. [[00:02:20](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=140.35999999999999s)]
*  There's research papers. [[00:02:25](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=145.0s)]
*  There's product releases coming out. [[00:02:25](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=145.8s)]
*  So it seems like a cascading thing. [[00:02:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=147.8s)]
*  The way I think about it as a software person, sort of lifelong programmer, is that there [[00:02:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=149.32000000000002s)]
*  basically in the fullness of time it will appear, I think that there were kind of two [[00:02:34](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=154.84s)]
*  different ways to write software. [[00:02:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=157.96s)]
*  There was the sort of the old way to write software, which is sort of the classic von [[00:02:39](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=159.0s)]
*  Neumann machine, deterministic way. [[00:02:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=163.08s)]
*  And the whole problem with writing software in the old model is like computers are hyper literal. [[00:02:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=165.48000000000002s)]
*  And so they do exactly what you tell them to do. [[00:02:50](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=170.28s)]
*  Every time they do something wrong, it's because you have instructed them in properly. [[00:02:53](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=173.0s)]
*  And it's your very humbling experience to learn as a young programmer that everything [[00:02:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=176.44s)]
*  is your fault. [[00:03:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=180.52s)]
*  And the machine will just sit and wait for you to fix the problem. [[00:03:02](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=182.92s)]
*  Like it's not going to do that on its own. [[00:03:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=184.76s)]
*  And then there's this other way to write software. [[00:03:07](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=187.48s)]
*  And this has to do with having these AI systems and then having training data, training the [[00:03:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=189.56s)]
*  systems, tweaking the systems. [[00:03:14](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=194.36s)]
*  And the sort of capability that that... [[00:03:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=196.68s)]
*  The way I describe it to kind of normies is that sort of unlocks the ability for computers [[00:03:18](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=198.92s)]
*  to more and more interact with the real world. [[00:03:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=202.68s)]
*  And with the messiness of the real world, right? [[00:03:24](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=204.76s)]
*  And the probabilistic nature of the real world. [[00:03:26](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=206.67999999999998s)]
*  Yeah, well, it seems almost less like writing software, almost like training something. [[00:03:28](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=208.76s)]
*  It's like when I think about machine learning and image recognition, you talked about, it [[00:03:32](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=212.76s)]
*  felt like it was almost like training a dog, right? [[00:03:35](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=215.88s)]
*  Like reinforcement learning is like, we'll give treats as it gets better. [[00:03:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=217.95999999999998s)]
*  But there's something different now. [[00:03:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=221.95999999999998s)]
*  It feels like, I don't know, we've gone from like training a dog to recognize a bird versus [[00:03:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=223.16s)]
*  like a hot dog or hot dog or not hot dog or so on. [[00:03:46](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=226.68s)]
*  To actually something where it feels closer like training a person. [[00:03:49](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=229.39999999999998s)]
*  Or I don't know how you feel. [[00:03:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=232.35999999999999s)]
*  When we talk about learning and training and data, what are we training? [[00:03:53](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=233.64s)]
*  Where do you think we are in that arc of getting to like eventually how 9,000 and so on? [[00:03:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=237.79999999999998s)]
*  Well, so while this has been happening, you have kids, like I have a young child. [[00:04:02](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=242.6s)]
*  So I have a seven-year-old now. [[00:04:07](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=247.72s)]
*  So as this stuff has all been popping, I've been simultaneously training now the seven-year-old. [[00:04:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=248.92s)]
*  Yes, yes. [[00:04:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=253.07999999999998s)]
*  Anybody who's had kids will recognize what I'm about to say. [[00:04:15](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=255.56s)]
*  But it is really interesting watching little kids. [[00:04:19](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=259.15999999999997s)]
*  The way I think about it, or at least the look that I have, who's great. [[00:04:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=262.12s)]
*  It's like everything, for the first few years, it's like every single thing he did was like [[00:04:26](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=266.52s)]
*  a little applied physics experiment. [[00:04:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=271.24s)]
*  Which is like, let's see what happens if I drop this. [[00:04:32](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=272.68s)]
*  Let's see what happens if I eat this. [[00:04:34](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=274.68s)]
*  Let's see what happens if I do this to daddy. [[00:04:35](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=275.96s)]
*  Yes. [[00:04:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=277.64s)]
*  Right, and see what the response is. [[00:04:38](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=278.12s)]
*  And they just run experiment after. [[00:04:39](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=279.24s)]
*  And you can see it very clearly when they're learning how to walk. [[00:04:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=281.0s)]
*  Because they're running all these experiments about how to stand up and what to hold on to. [[00:04:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=283.24s)]
*  And they keep falling over. [[00:04:46](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=286.28000000000003s)]
*  And then at some point, the little neural network actually figures it out. [[00:04:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=287.16s)]
*  It does learn. [[00:04:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=291.0s)]
*  Off and away they go, right? [[00:04:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=291.64s)]
*  Yeah. [[00:04:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=292.68s)]
*  And so clearly, it's a little bit eerie. [[00:04:53](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=293.88s)]
*  You can see that, a similar kind of thing happening. [[00:04:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=297.24s)]
*  Having said that, the human brain just keeps developing. [[00:05:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=300.04s)]
*  And then it ultimately clearly has consciousness. [[00:05:03](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=303.8s)]
*  It achieves higher levels of consciousness. [[00:05:07](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=307.0s)]
*  It achieves higher levels of self-knowledge. [[00:05:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=308.36s)]
*  It reaches the Descartes stage where it has self-awareness. [[00:05:10](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=310.68s)]
*  Clearly, it's very creative from an early age. [[00:05:15](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=315.08s)]
*  I'm a little less convinced that the software technologies we have now [[00:05:18](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=318.04s)]
*  are on some linear path towards just quote unquote AGI or just quote unquote consciousness. [[00:05:21](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=321.64000000000004s)]
*  It's hard for me to believe that consciousness is just simply [[00:05:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=327.72s)]
*  emergent from higher scale neural networks. [[00:05:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=330.36s)]
*  That, to me, seems like a hand wave. [[00:05:33](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=333.48s)]
*  Now, having said that, I have a lot of smart friends who are pretty sure that that's what's [[00:05:34](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=334.92s)]
*  going to happen. [[00:05:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=337.8s)]
*  Yeah, actually, I feel that way as well. [[00:05:38](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=338.6s)]
*  So I want to get to AGI in a bit. [[00:05:40](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=340.20000000000005s)]
*  And also, we can debate whether consciousness is an illusion, as it is. [[00:05:42](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=342.44000000000005s)]
*  But where we are now is kind of amazing. [[00:05:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=345.96s)]
*  People can take GPT-3, you can give it SAT exams, it can do OK. [[00:05:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=348.59999999999997s)]
*  Actually, it can do quite well. [[00:05:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=354.28s)]
*  Yeah, it can do it. [[00:05:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=355.23999999999995s)]
*  It scores it like the one I saw that scored it like what, 1,200? [[00:05:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=355.79999999999995s)]
*  Yeah, something like that. [[00:05:58](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=358.67999999999995s)]
*  Yeah, so it's not bad. [[00:05:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=359.71999999999997s)]
*  I can do homework. [[00:06:01](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=361.4s)]
*  That would get you in a lot of challenges. [[00:06:02](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=362.28s)]
*  Yeah, I actually gave it like acid questions to explain the derivation for the source trial [[00:06:03](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=363.71999999999997s)]
*  radius, the black hole radius, to write a code for, let's say, 8 by 8 tic tac toe. [[00:06:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=369.08s)]
*  Random things that you should never be able to do because it's not just memorizing it, [[00:06:15](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=375.4s)]
*  it's generalizing. [[00:06:19](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=379.24s)]
*  And it's getting that. [[00:06:20](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=380.84s)]
*  But then also, it actually seems to have some sort of weird hiccups. [[00:06:21](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=381.79999999999995s)]
*  Actually, one thing that really does not seem to get is humor. [[00:06:26](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=386.52s)]
*  So I'm kind of curious where you think it's going to go. [[00:06:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=390.2s)]
*  Because before we get to AGI, there are things that an average human can do pretty well that [[00:06:32](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=392.68s)]
*  GPT-3 can't. [[00:06:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=397.79999999999995s)]
*  But then there's also what experts can do. [[00:06:39](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=399.24s)]
*  And what I'm very curious about is actually we may get to some of the expert stuff first [[00:06:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=401.72s)]
*  before it can do even something like humor. [[00:06:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=407.4s)]
*  The irony is that something like humor that we take for granted might actually be really hard. [[00:06:49](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=409.72s)]
*  And other areas might be easier. [[00:06:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=414.44s)]
*  Well, the ultimate example of the things it can't do, like it can't pack your suitcase. [[00:06:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=416.2s)]
*  Like there's no robot that will pack your suitcase. [[00:07:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=420.2s)]
*  Yeah. [[00:07:01](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=421.64s)]
*  If you try to make an omelet, it'll shred your clothes, it'll shred the eggs. [[00:07:02](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=422.68s)]
*  They're not, you know. [[00:07:06](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=426.76s)]
*  It can drive your car, but it can't pack your suitcase. [[00:07:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=429.64s)]
*  So it can't do your laundry. [[00:07:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=431.32s)]
*  So there are these interesting kind of twists. [[00:07:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=433.96s)]
*  So I would describe a little bit as follows, which is I think that this generation of AI [[00:07:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=436.76s)]
*  that we have is impressive. [[00:07:20](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=440.91999999999996s)]
*  It is a little bit of a sleight of hand, which we'll maybe talk about. [[00:07:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=442.28s)]
*  But I also think actually, to your point, human consciousness or human intelligence [[00:07:25](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=445.4s)]
*  is also a little bit of a sleight of hand. [[00:07:28](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=448.11999999999995s)]
*  Maybe slightly different sleights of hand. [[00:07:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=450.67999999999995s)]
*  So the sleight of hand that you see when you're using GPT or one of these image generation [[00:07:32](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=452.6s)]
*  things is it's not literally creating new information. [[00:07:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=457.40000000000003s)]
*  Like what it's doing is it has no opinion. [[00:07:40](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=460.6s)]
*  It has no point of view. [[00:07:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=465.32000000000005s)]
*  It has no like, you know, it's not sitting there like thinking on its own, coming up [[00:07:46](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=466.44s)]
*  with some new thing. [[00:07:49](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=469.96000000000004s)]
*  What it's doing is it's basically training, you know, ideally what it's doing is it's [[00:07:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=471.24s)]
*  training on the sum total of all existing human knowledge. [[00:07:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=474.12s)]
*  So for text generation, it's training on all existing human text. [[00:07:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=476.36s)]
*  Right. [[00:07:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=479.48s)]
*  And so it plays back at you basically projections from the sort of, you know, assembled [[00:07:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=479.72s)]
*  composite of all human text. [[00:08:03](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=483.72s)]
*  And so when you ask it to do the 8x8 like, probably somebody on the internet at some [[00:08:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=484.76s)]
*  point wrote some paper about how to do that. [[00:08:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=489.56s)]
*  I think it's a little more than that because it because I asked like 56x56 or 101x101. [[00:08:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=491.16s)]
*  It has some sense of generalization. [[00:08:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=497.24s)]
*  Yeah. [[00:08:19](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=499.4s)]
*  But I'll bet we can check this. [[00:08:20](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=500.04s)]
*  I'll bet if we Google long enough, I'll bet we can find a paper that described a [[00:08:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=502.04s)]
*  general purpose algorithm for, you know, multi- [[00:08:24](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=504.76s)]
*  That may be. [[00:08:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=507.0s)]
*  Right. [[00:08:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=507.48s)]
*  Somebody did that. [[00:08:28](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=508.35999999999996s)]
*  Yeah. [[00:08:28](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=508.91999999999996s)]
*  Yeah, I've done the same humor experiment. [[00:08:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=509.56s)]
*  I like have it write Seinfeld scripts and sometimes they're really funny and sometimes [[00:08:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=510.76s)]
*  they're just like, it makes no sense. [[00:08:33](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=513.96s)]
*  I went for curb, but same idea. [[00:08:35](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=515.48s)]
*  Exactly. [[00:08:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=517.48s)]
*  But like, look, there are a lot of jokes on the internet, right? [[00:08:38](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=518.1999999999999s)]
*  And so you'd have to kind of, you could kind of go back and kind of say, okay, it [[00:08:40](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=520.12s)]
*  probably like pluck these jokes. [[00:08:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=523.24s)]
*  By the way, maybe there was a paper somewhere where they articulated a general theory of [[00:08:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=525.24s)]
*  humor, right? [[00:08:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=528.28s)]
*  Because this has been, humor has been studied as a thing and maybe there's like a general [[00:08:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=528.92s)]
*  thing of like humor is like the unexpected or whatever. [[00:08:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=532.92s)]
*  And so it generalizes. [[00:08:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=535.3199999999999s)]
*  Well, it could be too, like all sitcoms might be the same sitcom at some level, right? [[00:08:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=536.84s)]
*  So here being an example, so I also had to do like dramatic screenplays, dramatic cast [[00:09:01](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=541.96s)]
*  stage plays. [[00:09:05](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=545.8000000000001s)]
*  It's quite good if those like three, you can say like write a three act screenplay. [[00:09:06](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=546.2s)]
*  It will do it and it will have the proper like setup and resolution and so forth. [[00:09:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=549.0s)]
*  But yeah, there are systems for like screenwriting in Hollywood where they have like three acts [[00:09:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=551.88s)]
*  and then they have like 15 beats and then they have- [[00:09:15](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=555.24s)]
*  It's all Rocky or it's all Star Wars. [[00:09:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=556.9200000000001s)]
*  Yeah. [[00:09:18](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=558.6s)]
*  Well, so actually it's really interesting that maybe what we think is magical when humans [[00:09:19](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=559.0s)]
*  do it isn't actually all that magical either. [[00:09:24](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=564.2s)]
*  So that's what I was going to say. [[00:09:25](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=565.8s)]
*  So then the human sleight of hand is like, you know, is there actually free will? [[00:09:26](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=566.68s)]
*  Is there actually creativity happening upstairs? [[00:09:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=571.0799999999999s)]
*  By the way, if there is, is it everybody? [[00:09:34](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=574.4399999999999s)]
*  Is there really a thousand types of movies or is there like one latent space of the monomyth? [[00:09:36](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=576.1999999999999s)]
*  And basically what's happening, I think the theory, I'm just, you know, I'm kind of making [[00:09:40](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=580.4399999999999s)]
*  this up, but I think the theory would be the hero with a thousand faces or the idea of [[00:09:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=583.88s)]
*  the Jungian hero's journey, which is sort of the basis for all of these plots, you know, [[00:09:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=587.0799999999999s)]
*  Star Wars and Harry Potter and everything else. [[00:09:50](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=590.28s)]
*  You know, somebody with your background might say that basically it's sort of an algorithm [[00:09:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=592.76s)]
*  for surfing human neurochemistry. [[00:09:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=597.24s)]
*  Yes. [[00:09:58](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=598.5999999999999s)]
*  Yes. [[00:09:58](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=598.92s)]
*  Right. [[00:09:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=599.24s)]
*  And just generating different like, you know, sort of neurochemical responses to like, you [[00:09:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=599.56s)]
*  know, fear and anxiety and, you know, love and all these other things. [[00:10:03](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=603.64s)]
*  I've always been fascinated. [[00:10:07](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=607.64s)]
*  There's this thing in psychology called core affect theory. [[00:10:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=608.52s)]
*  That one I don't know. [[00:10:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=611.24s)]
*  Oh yeah, this is great. [[00:10:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=611.9599999999999s)]
*  So, okay. [[00:10:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=612.68s)]
*  So what do humans have all these love and despair, like we have all these different [[00:10:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=613.8s)]
*  emotions. [[00:10:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=616.92s)]
*  It's all great. [[00:10:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=617.24s)]
*  Core affect theory says, no, we don't. [[00:10:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=617.88s)]
*  Oh yeah. [[00:10:19](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=619.9599999999999s)]
*  Yes or no? [[00:10:20](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=620.52s)]
*  Good bet. [[00:10:20](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=620.92s)]
*  Good or bad. [[00:10:21](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=621.5600000000001s)]
*  Yeah. [[00:10:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=622.0400000000001s)]
*  And then higher or low. [[00:10:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=622.2800000000001s)]
*  And so we either have like a positive, like we either have like a positive neural response [[00:10:24](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=624.36s)]
*  or a negative neural response and then it's either high intensity or low intensity. [[00:10:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=629.0s)]
*  And then you just basically, and so it's like, wistfulness is like, you know, just [[00:10:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=631.8000000000001s)]
*  slightly negative, but like, you know, despair is like extremely negative. [[00:10:35](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=635.6400000000001s)]
*  So it's all two by two. [[00:10:40](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=640.44s)]
*  It's a two by two. [[00:10:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=641.5600000000001s)]
*  And it's, and we're more basic organisms than we think. [[00:10:42](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=642.2800000000001s)]
*  And then we just, we retro, you know, as, and we're very, one of the things that's [[00:10:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=644.7600000000001s)]
*  great known is humans are very good at creating a story to justify whatever happened. [[00:10:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=648.2s)]
*  Right. [[00:10:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=651.6400000000001s)]
*  And so we create these stories, these scripts around this idea of an emotion, but it's [[00:10:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=652.6800000000001s)]
*  basically just justifying the neural response. [[00:10:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=656.5200000000001s)]
*  Yeah. [[00:10:58](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=658.2800000000001s)]
*  And so the, the, the cynical view would be like having an ice cream cone on a hot day [[00:10:58](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=658.76s)]
*  and like falling in love are like the same thing. [[00:11:02](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=662.0400000000001s)]
*  Well, maybe neurochemically maybe they are. [[00:11:05](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=665.48s)]
*  Well, this comes into play in like, you know, drug abuse, right? [[00:11:07](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=667.8000000000001s)]
*  Which is, you know, things that, things that generate an opioid response. [[00:11:10](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=670.76s)]
*  Yeah. [[00:11:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=673.1600000000001s)]
*  Like some people get an opioid response from alcohol. [[00:11:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=673.4000000000001s)]
*  Yeah. [[00:11:15](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=675.5600000000001s)]
*  Right. [[00:11:15](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=675.8000000000001s)]
*  And they're, they're far more prone to alcoholism and people don't get that response. [[00:11:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=676.04s)]
*  And so it's literally a neurochemical thing. [[00:11:18](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=678.76s)]
*  So, so yeah, look, maybe we're, maybe we're bundles of neurochemistry to a much deeper [[00:11:20](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=680.8399999999999s)]
*  extent than, or much simpler extent than we want to believe. [[00:11:24](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=684.68s)]
*  Yeah. [[00:11:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=687.8s)]
*  Having said that, you know, again, oh, and then that, that takes me to the other thing [[00:11:28](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=688.68s)]
*  on AI, which is I do, you know, one of the ways that people are testing AI is with the [[00:11:32](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=692.92s)]
*  so-called Turing test. [[00:11:36](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=696.68s)]
*  Yes. [[00:11:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=697.7199999999999s)]
*  And the simplified form of the Turing test is you're chatting with somebody that may [[00:11:38](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=698.1999999999999s)]
*  be a human or maybe a bot, but you chat for 20 minutes. [[00:11:42](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=702.1999999999999s)]
*  Yeah. [[00:11:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=704.52s)]
*  Can you guess better than random as to if it's a human or a bot? [[00:11:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=705.16s)]
*  Yeah. [[00:11:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=707.4799999999999s)]
*  You know, the, the, my take on that is the Turing, you know, Alan Turing was a genius, [[00:11:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=708.28s)]
*  but the Turing test is malformed. [[00:11:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=711.4799999999999s)]
*  Yes. [[00:11:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=712.92s)]
*  Humans are too easy to trick. [[00:11:53](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=713.24s)]
*  Yeah, yeah, yeah. [[00:11:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=714.4399999999999s)]
*  Yeah. [[00:11:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=715.0799999999999s)]
*  But, but that's too low of a bar because tricking a person is not that hard and does not prove [[00:11:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=715.4799999999999s)]
*  anything other than that you've tricked the person. [[00:11:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=719.9599999999999s)]
*  Yes. [[00:12:01](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=721.4799999999999s)]
*  Like I, I think, and this is relevant because I think, you know, things like GPT are about [[00:12:01](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=721.8s)]
*  to pass the Turing test. [[00:12:05](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=725.88s)]
*  Yes. [[00:12:06](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=726.68s)]
*  Because they haven't already. [[00:12:06](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=726.8399999999999s)]
*  Oh, they probably already have. [[00:12:07](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=727.56s)]
*  They probably have in some cases. [[00:12:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=728.4399999999999s)]
*  Yeah, exactly. [[00:12:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=729.56s)]
*  And so I think it's going to turn out that that was too lightweight of a test. [[00:12:10](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=730.76s)]
*  Yes. [[00:12:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=733.96s)]
*  Well, here's, here's my favorite example for why I know GPT is not self-aware. [[00:12:14](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=734.52s)]
*  If you ask it if it's self-aware and you ask it to elaborate on how it became self-aware, [[00:12:18](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=738.6800000000001s)]
*  it will happily tell you. [[00:12:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=742.9200000000001s)]
*  Yes. [[00:12:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=743.96s)]
*  And by the way, if you ask it if, well, how it's going to feel if you, if you turn it off, [[00:12:24](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=744.44s)]
*  it's going to tell you, please don't turn it off. [[00:12:28](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=748.0400000000001s)]
*  Yeah, yeah, yeah, yeah. [[00:12:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=749.24s)]
*  If you ask it to explain to you why it's not self-aware. [[00:12:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=750.44s)]
*  Yes. [[00:12:32](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=752.84s)]
*  It will do that too. [[00:12:33](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=753.32s)]
*  It will very happily do that too. [[00:12:34](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=754.2s)]
*  It does not have a differential opinion about those two outcomes. [[00:12:36](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=756.2s)]
*  Yeah. [[00:12:39](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=759.0s)]
*  Whereas every living, you know, every conscious, well, every even, even non-conscious living [[00:12:39](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=759.4000000000001s)]
*  or any, any, any living organism has a very different response to those two scenarios. [[00:12:42](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=762.76s)]
*  It's been amazing because in some ways I feel like it's as much been interesting to study [[00:12:46](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=766.4399999999999s)]
*  the AI as the AI has reflected for us to study ourselves, you know, and I think we are sort [[00:12:50](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=770.68s)]
*  of seeing that the magician has certain tricks, whether it's the AI magician or the human [[00:12:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=775.4s)]
*  magician, and it's going through this education process. [[00:12:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=779.24s)]
*  Curious though, like it feels like, you know, so, so like GPT can get into high school, [[00:13:03](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=783.4s)]
*  get into college, let's say, but like what would it take for it to get its PhD, you know, [[00:13:07](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=787.72s)]
*  and like we're, I think that's where the sort of dramatic stuff is to come. [[00:13:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=792.12s)]
*  Yeah. [[00:13:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=796.68s)]
*  Well, so again, exactly your point is that stressed, I would ask the question the other [[00:13:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=797.08s)]
*  way, which is like, well, okay, what does it take to get a PhD? [[00:13:20](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=800.68s)]
*  Yeah. [[00:13:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=802.92s)]
*  What does it take for a human to get a, like, how are the universities doing? [[00:13:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=803.16s)]
*  Yes, yes, yes, yes. [[00:13:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=807.16s)]
*  How are they doing on quality control of their own programs? [[00:13:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=809.16s)]
*  Yes. [[00:13:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=811.24s)]
*  How many people are getting PhDs today that we would say are like actually valid, [[00:13:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=811.5600000000001s)]
*  like scientific, you know, whatever, actual accomplishments? [[00:13:35](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=815.32s)]
*  Yeah. [[00:13:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=817.8s)]
*  By the way, people who got, you know, professors a hundred years ago, like how would they score [[00:13:38](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=818.6800000000001s)]
*  the PhDs that are being granted today? [[00:13:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=823.16s)]
*  Yeah. [[00:13:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=824.6800000000001s)]
*  Would they say the bar is higher? [[00:13:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=824.84s)]
*  I don't know the answer to that. [[00:13:46](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=826.12s)]
*  Or would they say the bar is lower? [[00:13:46](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=826.6800000000001s)]
*  Lower. [[00:13:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=827.88s)]
*  I think they would say the bar is dramatically lower. [[00:13:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=828.2s)]
*  Yes. [[00:13:49](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=829.72s)]
*  Right. And so, you know, the answer might be we have lowered the bar, but the same thing [[00:13:50](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=830.44s)]
*  for college admissions, like, you know, what does it take to get into college? [[00:13:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=834.0400000000001s)]
*  Well, what does it take to finish college? [[00:13:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=836.12s)]
*  Yeah. [[00:13:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=837.4s)]
*  And, you know, the education says, well, this is coming up a lot right now because it's [[00:13:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=837.88s)]
*  like, okay, GPT can auto-generate like, you know, essays, right? [[00:14:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=840.52s)]
*  And so student essays. [[00:14:03](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=843.8000000000001s)]
*  And so it's like, okay, the grading method of assign an essay and grade the result, like, [[00:14:05](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=845.72s)]
*  is probably not going to work anymore. [[00:14:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=849.0s)]
*  But it's like, wait, was that ever actually, like, just because we thought that that was [[00:14:10](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=850.6s)]
*  education, was that actually education? [[00:14:14](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=854.28s)]
*  Like, was that actually teaching anybody anything? [[00:14:15](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=855.72s)]
*  Like, actually, I'm sure someone's going to take that to apply to colleges. [[00:14:18](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=858.04s)]
*  Oh, yeah, yeah. [[00:14:21](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=861.3199999999999s)]
*  Yeah. [[00:14:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=862.4399999999999s)]
*  Absolutely. [[00:14:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=862.68s)]
*  Yeah. [[00:14:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=863.16s)]
*  College applications are basically done. [[00:14:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=863.4s)]
*  I mean, at least to the extent that you believe that college applications were a legitimate [[00:14:24](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=864.68s)]
*  way to evaluate anybody in the first place. [[00:14:28](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=868.92s)]
*  Yes. [[00:14:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=870.6s)]
*  Like, that's now over. [[00:14:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=870.76s)]
*  It's done. [[00:14:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=871.56s)]
*  I'd be more skeptical that they were ever useful in the first place, right? [[00:14:32](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=872.36s)]
*  Yeah. [[00:14:35](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=875.5600000000001s)]
*  Well, so in the PhD, let's talk about, like, at least in that old school mentality of a [[00:14:35](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=875.88s)]
*  PhD of some advanced learning where you become an expert in something. [[00:14:39](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=879.96s)]
*  Right. [[00:14:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=883.48s)]
*  You know, I think that's the thing where... [[00:14:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=883.88s)]
*  Expert. [[00:14:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=885.24s)]
*  What do you mean by expert? [[00:14:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=885.64s)]
*  Let's say the ability to be in the top 0.1% of humanity of, let's say, designing a drug [[00:14:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=887.4s)]
*  or building something. [[00:14:53](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=893.24s)]
*  Oh, interesting. [[00:14:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=895.16s)]
*  Is that what they teach? [[00:14:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=895.96s)]
*  Yeah. [[00:14:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=897.08s)]
*  Well, that's... [[00:14:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=897.4s)]
*  Is that what they teach in universities? [[00:14:58](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=898.04s)]
*  That is my goal. [[00:14:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=899.48s)]
*  I wasn't aware that was part of the curriculum. [[00:15:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=900.92s)]
*  I think it is. [[00:15:02](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=902.8399999999999s)]
*  Is it? [[00:15:03](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=903.4s)]
*  Sometimes. [[00:15:03](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=903.64s)]
*  Or at least that's what you have to do eventually when you get out. [[00:15:05](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=905.72s)]
*  Right. [[00:15:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=908.04s)]
*  Okay. [[00:15:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=908.28s)]
*  Yeah. [[00:15:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=908.6s)]
*  You know? [[00:15:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=908.68s)]
*  And you have to apply it. [[00:15:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=909.16s)]
*  I think it's... [[00:15:10](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=910.1999999999999s)]
*  One of the things about being an expert in my mind is that something that is the difference [[00:15:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=912.76s)]
*  between bad, good, and great can be really close. [[00:15:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=917.72s)]
*  Like, I could probably write a piece of music, but no one would think it's all that great. [[00:15:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=922.12s)]
*  You know? [[00:15:26](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=926.1999999999999s)]
*  And then you could have someone who's a good musician, but not a great one. [[00:15:26](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=926.68s)]
*  Then you have a genius, like a Mozart or Led Zeppelin or whatever you pick your genre. [[00:15:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=929.8s)]
*  And I think where we aren't there yet is that when the difference between good and great [[00:15:36](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=936.28s)]
*  is so close, or like I don't know if you remember from Spinal Tap, there's a fine line between [[00:15:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=941.16s)]
*  brilliant and stupid. [[00:15:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=945.16s)]
*  I think that is where I think it hasn't really hit yet. [[00:15:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=947.0799999999999s)]
*  In that if you look at the jokes, the jokes are just kind of, okay, [[00:15:50](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=950.76s)]
*  the screenplays it makes are not like brilliant screenplays. [[00:15:53](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=953.4s)]
*  I think it could get into college, but could it win best screenplay? [[00:15:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=956.12s)]
*  You know? [[00:16:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=960.1199999999999s)]
*  And so that's this part where I think we're not there yet. [[00:16:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=960.68s)]
*  But I think we're getting there. [[00:16:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=964.68s)]
*  So name a great music composer generated by a music PhD program in the university system [[00:16:06](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=966.28s)]
*  in the last hundred years. [[00:16:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=972.04s)]
*  Yeah. [[00:16:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=973.16s)]
*  Name one. [[00:16:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=973.7199999999999s)]
*  Yeah, I'm thinking more in the scientific side of things, but yeah, I don't think, [[00:16:14](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=974.4399999999999s)]
*  probably the PhD program in that space is probably not intended to generate music. [[00:16:18](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=978.52s)]
*  Okay. [[00:16:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=982.68s)]
*  Yeah. [[00:16:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=983.2399999999999s)]
*  Name one great screenplay written by a PhD in drama. [[00:16:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=983.3199999999999s)]
*  Yeah. [[00:16:25](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=985.64s)]
*  Yeah. [[00:16:26](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=986.04s)]
*  So that's an interesting point. [[00:16:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=987.18s)]
*  But I think what I'm getting at is still like the ability to do something. [[00:16:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=990.38s)]
*  And so the education part, we can talk about how they learn. [[00:16:34](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=994.3s)]
*  Because I think in the case of the screenplay or the music you're talking about, [[00:16:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=997.9s)]
*  they still have to learn something. [[00:16:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1001.66s)]
*  Right. [[00:16:42](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1002.78s)]
*  Or do you think they just innately sort of knew how to write a screenplay? [[00:16:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1004.2199999999999s)]
*  I don't know. [[00:16:46](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1006.78s)]
*  Yeah. [[00:16:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1007.42s)]
*  I assume there's a process where they write a screenplay, it's kind of mediocre. [[00:16:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1007.8199999999999s)]
*  Oh, yeah, yeah. [[00:16:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1011.18s)]
*  Yeah. [[00:16:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1011.74s)]
*  And then they get critiqued or they critique themselves and then [[00:16:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1012.06s)]
*  and then it improves and improves and improves. [[00:16:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1015.58s)]
*  Well, the screenplay, okay, so the divorce, divorce and the education. [[00:16:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1017.4200000000001s)]
*  Yeah, yeah, yeah. [[00:16:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1019.98s)]
*  Pre-education. [[00:17:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1020.5400000000001s)]
*  Yeah, yeah, yeah. [[00:17:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1020.86s)]
*  Behind. [[00:17:01](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1021.34s)]
*  Yeah. [[00:17:01](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1021.58s)]
*  Look, the screen, the test of the screenplay, the test for screenplay is does it sell? [[00:17:02](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1022.14s)]
*  Yeah. [[00:17:05](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1025.42s)]
*  So screenplays are subject to market discipline. [[00:17:05](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1025.9s)]
*  Yes. [[00:17:07](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1027.58s)]
*  Yeah. [[00:17:07](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1027.98s)]
*  Right. [[00:17:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1028.14s)]
*  And so question number one for a screenplay is does it sell to the studio? [[00:17:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1028.7s)]
*  Will they buy it? [[00:17:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1031.58s)]
*  Yeah. [[00:17:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1032.46s)]
*  And then test number two is when the movie comes out, [[00:17:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1032.54s)]
*  or the TV show comes out, does anybody watch it? [[00:17:14](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1034.78s)]
*  Yes. [[00:17:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1036.38s)]
*  Do they like it? [[00:17:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1036.94s)]
*  Do they finish it? [[00:17:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1037.5s)]
*  Yeah. [[00:17:18](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1038.38s)]
*  One of the fun things that Netflix will now tell people who make film and TV is they [[00:17:18](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1038.94s)]
*  actually tell them for the first time whether anybody's actually finishing their movie. [[00:17:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1042.14s)]
*  Yeah. [[00:17:24](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1044.46s)]
*  Yeah. [[00:17:24](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1044.8600000000001s)]
*  Yeah, just all those stats are kind of mind-boggling. [[00:17:26](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1046.3000000000002s)]
*  Right. [[00:17:28](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1048.3000000000002s)]
*  Yeah. [[00:17:28](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1048.5400000000002s)]
*  Yeah. [[00:17:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1049.0200000000002s)]
*  A lot of movies, and you know, people go to the theater and they feel invested in and they [[00:17:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1049.26s)]
*  don't want to leave in the middle, but Netflix, it's very easy to punch out or it turns out a [[00:17:32](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1052.94s)]
*  lot of screenplays. [[00:17:36](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1056.38s)]
*  You know, this is something that professional screenwriters will tell you, like it can't [[00:17:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1057.26s)]
*  ever sag. [[00:17:40](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1060.7800000000002s)]
*  Yeah. [[00:17:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1061.5s)]
*  Yeah. [[00:17:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1061.8200000000002s)]
*  Just as one example because people will stop watching. [[00:17:42](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1062.0600000000002s)]
*  So yeah, so screenwriting is subject to market test, popular music is subject to market test. [[00:17:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1064.8600000000001s)]
*  By the way, classical music, which I'm a huge fan of, is no longer subject to market test. [[00:17:50](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1070.54s)]
*  It's thoroughly subsidized. [[00:17:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1075.02s)]
*  Yes. [[00:17:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1076.06s)]
*  That's interesting. [[00:17:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1076.46s)]
*  Right? [[00:17:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1077.1s)]
*  It's not in the free market anymore. [[00:17:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1077.42s)]
*  Yeah. [[00:17:58](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1078.7s)]
*  Or maybe the equivalence of movie music is, you know. [[00:17:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1079.42s)]
*  Yeah, so movie music is subject to market test. [[00:18:02](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1082.22s)]
*  Right. [[00:18:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1084.22s)]
*  And it's probably the modern classical. [[00:18:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1084.46s)]
*  It is the modern classical. [[00:18:05](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1085.98s)]
*  Yeah, for that reason. [[00:18:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1088.06s)]
*  So yeah, like the market test is real, but yeah, let me grant your point. [[00:18:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1088.86s)]
*  So let's build on what you said. [[00:18:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1092.1399999999999s)]
*  Let me grant your point. [[00:18:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1093.5s)]
*  Like, let's use, could we use the term paste? [[00:18:14](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1094.22s)]
*  Yeah. [[00:18:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1097.6599999999999s)]
*  Like- [[00:18:18](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1098.06s)]
*  Yeah. [[00:18:18](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1098.1399999999999s)]
*  Or just ability to do something hard. [[00:18:18](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1098.7s)]
*  Well, ability, okay, so ability to do something hard and let's say create something hard. [[00:18:20](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1100.54s)]
*  Yes. [[00:18:26](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1106.46s)]
*  Create something complicated and then also the ability to judge. [[00:18:26](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1106.8600000000001s)]
*  Yeah. [[00:18:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1110.14s)]
*  Right. [[00:18:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1110.6200000000001s)]
*  Critically, like to start with judging your own work. [[00:18:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1111.98s)]
*  Yeah, and probably therefore the ability to prove. [[00:18:33](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1113.9s)]
*  And then therefore the ability to prove, right. [[00:18:36](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1116.06s)]
*  So yeah, I think that there's, yeah, so there is something about taste. [[00:18:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1117.5800000000002s)]
*  Yeah. [[00:18:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1121.9s)]
*  Like I tend to think this stuff all has like aesthetic. [[00:18:42](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1122.22s)]
*  Yes. [[00:18:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1124.54s)]
*  Like a properly constructed mathematical formula or software program has aesthetic. [[00:18:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1125.1000000000001s)]
*  100%. [[00:18:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1128.7s)]
*  Molecule design has aesthetic properties. [[00:18:50](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1130.06s)]
*  Physics. [[00:18:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1131.58s)]
*  Right. [[00:18:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1132.22s)]
*  Physics has aesthetic properties. [[00:18:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1132.46s)]
*  All of it. [[00:18:53](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1133.58s)]
*  Yeah. [[00:18:53](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1133.9s)]
*  So there's something about taste that's like some combination of quantitative, qualitative. [[00:18:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1134.3s)]
*  Yeah, like a great startup is like from a mediocre one, this taste. [[00:18:58](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1138.54s)]
*  Right. [[00:19:02](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1142.46s)]
*  Yeah, exactly. [[00:19:02](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1142.7s)]
*  And like there's certain signals, like there's certain methods and certain signals, [[00:19:03](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1143.3400000000001s)]
*  but it's not necessarily reducible to an algorithm. [[00:19:05](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1145.82s)]
*  It's more of like a composite, you know, it's sort of the foundational knowledge combined [[00:19:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1148.22s)]
*  with some scope of experience combined with some kind of ineffable characteristic of judgment. [[00:19:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1152.3s)]
*  Well, we associate an aesthetic with it, but I wonder whether that's also just our emotional [[00:19:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1156.86s)]
*  connection to it. [[00:19:21](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1161.5s)]
*  Right. [[00:19:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1162.3799999999999s)]
*  You know, because I think we have this good right or wrong or more right or more wrong. [[00:19:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1162.6999999999998s)]
*  Right. [[00:19:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1167.02s)]
*  Like a gradient, like yeah, that's the right direction. [[00:19:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1167.1s)]
*  Right. [[00:19:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1169.26s)]
*  And but a lot of it is also whether something is elegant versus like just a hack. [[00:19:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1169.58s)]
*  Right. [[00:19:32](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1172.86s)]
*  You can tell whether these great things are just simple and powerful rather than like some, [[00:19:33](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1173.26s)]
*  some complicated machine to do something that, you know, you know, this is going to [[00:19:38](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1178.4599999999998s)]
*  eventually fall apart or that. [[00:19:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1181.82s)]
*  You think about that's true in physics or in a go to market or in a or in music. [[00:19:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1184.14s)]
*  It has all that both that sort of complexity and simplicity at the same time. [[00:19:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1188.3s)]
*  But so but I'm curious, like so when I guess that point, which I think that's a when not an if. [[00:19:53](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1193.9s)]
*  Okay. [[00:20:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1200.46s)]
*  Yeah. [[00:20:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1200.94s)]
*  Yeah. [[00:20:01](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1201.18s)]
*  Yeah. [[00:20:01](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1201.3400000000001s)]
*  Or so why would you say why wouldn't it get there? [[00:20:01](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1201.74s)]
*  Because like, do we even understand how it works in people? [[00:20:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1204.8600000000001s)]
*  Maybe we don't have to. [[00:20:07](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1207.9s)]
*  Well, maybe we don't have to. [[00:20:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1208.8600000000001s)]
*  So this is where I describe this. [[00:20:10](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1210.0600000000002s)]
*  This is like the AGI question. [[00:20:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1211.18s)]
*  This is where I call the hand wave. [[00:20:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1212.38s)]
*  The embedded assumption that it's a when is that it will be an emergent process [[00:20:15](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1215.42s)]
*  that will sort of unlock as a consequence of greater and greater levels of scale. [[00:20:18](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1218.5400000000002s)]
*  Yeah. [[00:20:21](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1221.3400000000001s)]
*  Maybe. [[00:20:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1222.22s)]
*  Yeah. [[00:20:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1222.7800000000002s)]
*  One way of looking at that is yes, that is just what's going to happen. [[00:20:24](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1224.3000000000002s)]
*  That's the human consciousness emerged. [[00:20:26](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1226.38s)]
*  Like that's obviously what's going to happen. [[00:20:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1227.66s)]
*  The other impression that is it's just a mess. [[00:20:28](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1228.7800000000002s)]
*  And it's a hand wave. [[00:20:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1230.38s)]
*  It's a hand wave and it's what the kids would call cope. [[00:20:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1231.0200000000002s)]
*  And the cope would be okay. [[00:20:33](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1233.74s)]
*  So here let me ask you a question in return. [[00:20:34](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1234.6200000000001s)]
*  What is the sub specialty of human biology and medicine that best understands [[00:20:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1237.5s)]
*  the nature of human consciousness today? [[00:20:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1241.98s)]
*  Oh, I don't think there is one. [[00:20:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1243.42s)]
*  There is one. [[00:20:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1244.78s)]
*  Anesthesiology. [[00:20:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1245.98s)]
*  Okay. [[00:20:46](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1246.94s)]
*  Which is poorly understood. [[00:20:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1247.98s)]
*  Well, it's poorly understood. [[00:20:49](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1249.02s)]
*  But they know how to turn it off. [[00:20:49](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1249.9s)]
*  Yeah. [[00:20:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1251.98s)]
*  And they know how to turn it back on. [[00:20:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1252.3s)]
*  Yes. [[00:20:53](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1253.82s)]
*  They've got the on-off switch. [[00:20:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1254.7s)]
*  That's all we got. [[00:20:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1256.22s)]
*  That's all we have. [[00:20:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1256.94s)]
*  Like we have been we collectively have been studying this question of human [[00:20:58](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1258.38s)]
*  consciousness for a very long time. [[00:21:02](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1262.14s)]
*  We have very advanced technologies today. [[00:21:03](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1263.82s)]
*  Functional MRI and like all this stuff. [[00:21:05](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1265.74s)]
*  That speaks to there's a field I would love to see created, [[00:21:07](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1267.34s)]
*  which is molecular psychology. [[00:21:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1269.98s)]
*  Where you can start to probe this a little more than on-off. [[00:21:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1272.46s)]
*  Okay. [[00:21:14](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1274.78s)]
*  And molecular. [[00:21:15](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1275.02s)]
*  Is it literal or metaphorical? [[00:21:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1277.1799999999998s)]
*  Quite literal. [[00:21:19](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1279.1s)]
*  It's a play like molecular biology was this big thing in the 80s [[00:21:19](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1279.82s)]
*  where we finally can bring like chemistry of small molecules to poke at biology [[00:21:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1283.1799999999998s)]
*  or chemical biology as well. [[00:21:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1287.34s)]
*  And if we could use like small molecules to maybe perturb more than just on-off, [[00:21:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1289.02s)]
*  but like perturbs things, we can start to understand the brain a little bit. [[00:21:34](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1294.3799999999999s)]
*  Because reading is one thing, but like poking and sort of perturbing [[00:21:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1297.34s)]
*  and then seeing the result is usually how we do any sort of experiment. [[00:21:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1301.18s)]
*  Right. [[00:21:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1303.9s)]
*  And would you do that? [[00:21:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1304.3s)]
*  Is that a chemical? [[00:21:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1304.94s)]
*  Would that be a chemical experimentation? [[00:21:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1305.74s)]
*  Or that be electrical? [[00:21:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1307.42s)]
*  It could be either one. [[00:21:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1308.54s)]
*  It could be any of that, but probably some combination of those things. [[00:21:49](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1309.26s)]
*  Like neural links like on a track in theory to enable some of this. [[00:21:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1311.5800000000002s)]
*  Yeah. [[00:21:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1314.46s)]
*  So like look, we just don't. [[00:21:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1314.8600000000001s)]
*  Okay. [[00:21:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1316.14s)]
*  So here would be the counter argument is like we just we don't know how human [[00:21:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1316.3s)]
*  consciousness works. [[00:21:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1319.3400000000001s)]
*  We actually don't. [[00:22:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1320.46s)]
*  If I actually I didn't go in the field, but I didn't go in the field. [[00:22:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1320.94s)]
*  Actually was that was going to be what I was going to study in school 30 years ago. [[00:22:05](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1325.26s)]
*  But I looked at the field at the time and I was like they don't have a clue. [[00:22:07](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1327.9s)]
*  Yeah. [[00:22:10](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1330.3s)]
*  I'm going to spend my entire career. [[00:22:10](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1330.54s)]
*  So you want to go into consciousness. [[00:22:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1331.8999999999999s)]
*  Tilting at windmills. [[00:22:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1332.94s)]
*  Yeah. [[00:22:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1333.6599999999999s)]
*  At the time cognitive science was like the hot thing. [[00:22:14](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1334.06s)]
*  Yeah. [[00:22:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1336.06s)]
*  Building off of AI. [[00:22:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1336.3s)]
*  Yeah. [[00:22:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1337.02s)]
*  But that was like expert systems. [[00:22:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1337.42s)]
*  Expert systems. [[00:22:18](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1338.62s)]
*  Well, early neural networks and then a lot of it got into brain chemistry [[00:22:19](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1339.42s)]
*  and like we're going to figure this stuff out and we're going to learn how to build. [[00:22:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1343.1s)]
*  You know, it's just like they didn't know then as far as I know they don't know now. [[00:22:25](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1345.42s)]
*  So the counter argument would be this is all just like massive cope for the fact [[00:22:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1349.6599999999999s)]
*  that we actually we don't understand that. [[00:22:33](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1353.02s)]
*  So we don't understand how to do it. [[00:22:34](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1354.94s)]
*  And so all we can do is hand wave and kind of just say, well, it's just going to be [[00:22:36](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1356.54s)]
*  emergent. [[00:22:39](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1359.1s)]
*  And it's like, no, it's not. [[00:22:39](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1359.58s)]
*  And we're going to be sitting here 30 years from now and we're still not going to have [[00:22:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1361.1799999999998s)]
*  any more knowledge, you know, barring other scientific breakthroughs of the kind that [[00:22:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1363.6599999999999s)]
*  you're talking about. [[00:22:46](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1366.62s)]
*  What's interesting is if you think about that time, we had neural nets, but they were all [[00:22:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1367.1799999999998s)]
*  single layer basically. [[00:22:50](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1370.62s)]
*  And they couldn't even do XOR. [[00:22:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1371.8999999999999s)]
*  You know, you couldn't even do some simple things because you needed deeper networks [[00:22:53](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1373.5s)]
*  to get at them. [[00:22:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1376.86s)]
*  And you couldn't have deep networks then because we didn't have the computational power. [[00:22:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1377.6599999999999s)]
*  And so the space was pretty dormant for a while, you know, AI until like we started going to [[00:23:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1380.62s)]
*  having basically just the computational power from GPUs and other things to be able to go [[00:23:05](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1385.82s)]
*  deep. [[00:23:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1389.98s)]
*  And then you could feed the data through. [[00:23:10](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1390.38s)]
*  So it is possible that we sort of have a point where we sort of saturate the compute that [[00:23:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1391.82s)]
*  we have now. [[00:23:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1396.86s)]
*  We get to as much as we can get to. [[00:23:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1397.82s)]
*  And that may get to close to AGI, maybe not. [[00:23:19](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1399.9s)]
*  And then it takes another like 30 years to get to the next sort of breakthroughs to get [[00:23:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1402.54s)]
*  there. [[00:23:26](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1406.6200000000001s)]
*  Yeah. [[00:23:26](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1406.86s)]
*  But OK, so I would pull back from this. [[00:23:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1407.58s)]
*  So AGI is the fun thing. [[00:23:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1409.74s)]
*  There is a sort of step back, which is to pick a domain. [[00:23:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1411.82s)]
*  And you know the domains I think a lot about, like life sciences, designing drugs, doing [[00:23:34](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1414.94s)]
*  health care, like seeing if you can pick a diagnosis. [[00:23:40](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1420.54s)]
*  Can you suggest a drug? [[00:23:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1423.8200000000002s)]
*  In those areas, now we're talking about a much more limited domain. [[00:23:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1425.5s)]
*  So we're not talking about we don't need to go all the way to consciousness for that [[00:23:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1428.14s)]
*  necessarily. [[00:23:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1431.42s)]
*  You can have something that's more limited. [[00:23:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1432.38s)]
*  In that limited domain, right now it seems like generative AI isn't quite far enough [[00:23:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1434.54s)]
*  yet to be able to like, yeah, I don't see the examples quite yet. [[00:23:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1439.5800000000002s)]
*  Are you sure? [[00:24:03](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1443.1000000000001s)]
*  Yeah, well, we'll see. [[00:24:03](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1443.98s)]
*  I mean, so what's the counter? [[00:24:05](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1445.5s)]
*  And I know you especially think about health care a lot. [[00:24:07](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1447.18s)]
*  Yeah. [[00:24:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1449.58s)]
*  Yeah. [[00:24:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1449.98s)]
*  Well, so the first thing is whenever you score it, well, let's talk about medical diagnosis, [[00:24:10](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1450.46s)]
*  which is kind of just low hanging fruit question because everybody experiences it. [[00:24:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1453.58s)]
*  So to start up front, you have to ask a question up front, which is like, is the goal, what's [[00:24:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1456.78s)]
*  the threshold? [[00:24:20](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1460.38s)]
*  Is the threshold perfect or is the threshold better than human? [[00:24:20](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1460.94s)]
*  Yeah, that's a great point. [[00:24:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1463.34s)]
*  Right. [[00:24:24](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1464.38s)]
*  Yeah. [[00:24:24](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1464.6200000000001s)]
*  And by the way, this is a topic that comes up all the time with self-driving cars, right, [[00:24:25](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1465.18s)]
*  which is, is it perfect? [[00:24:28](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1468.3s)]
*  It will never make a mistake or is it just going to be better, better than human? [[00:24:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1469.26s)]
*  And the way that self-driving cars score this is accidents per 1,000 miles driven. [[00:24:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1471.98s)]
*  And self-driving cars are already lower than human drivers. [[00:24:35](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1475.58s)]
*  And humans may actually be getting worse with texting. [[00:24:38](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1478.3s)]
*  With texting and other forms of it. [[00:24:40](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1480.54s)]
*  By the way, they increase forms of certain kinds of drug abuse, right? [[00:24:42](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1482.8600000000001s)]
*  And then, of course, the machines have the characteristic they get better universally, [[00:24:46](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1486.7s)]
*  right? [[00:24:49](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1489.74s)]
*  So a car has one mishap in one location. [[00:24:49](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1489.98s)]
*  Every other car gets trained on how to deal with that in the future. [[00:24:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1492.54s)]
*  The learning happens across the entire system. [[00:24:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1495.82s)]
*  And so, like, I think you can make a serious argument that, like, basically, [[00:24:58](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1498.46s)]
*  self-driving cars are already better than people on a relative basis. [[00:25:01](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1501.82s)]
*  And therefore, like, morally, you could even go so far as to say human drivers should be outlawed today. [[00:25:05](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1505.26s)]
*  Yeah. [[00:25:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1509.34s)]
*  Right. [[00:25:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1509.98s)]
*  If you have the alternative, you can have the self-driving car, then, yeah. [[00:25:10](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1510.46s)]
*  Like the utilitarian, I'm not a utilitarian, but the utilitarian argument would be you should obviously [[00:25:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1513.6599999999999s)]
*  ban human drivers today because the machine-driven stuff is already better. [[00:25:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1517.26s)]
*  Probably, by the way, the same is true for airplanes. [[00:25:20](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1520.3799999999999s)]
*  Yeah. [[00:25:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1522.3s)]
*  Right. [[00:25:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1522.86s)]
*  Now, we're not actually going to do that and there are other considerations involved and so forth, [[00:25:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1523.34s)]
*  but, like, you know, logically speaking, you should at least think about that as a possibility. [[00:25:26](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1526.62s)]
*  And I think you should think about that as a possibility, I think, for medical diagnosis, [[00:25:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1530.06s)]
*  which is, you know, and here the test is very simple, which is, well, at least express two tests. [[00:25:33](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1533.4199999999998s)]
*  Test number one is the absolute test, which is if I feed in a set of symptoms, it generates the correct [[00:25:38](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1538.06s)]
*  diagnosis 100% of the time, deterministically guaranteed. [[00:25:42](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1542.3s)]
*  Right. [[00:25:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1545.34s)]
*  That's a high bar. [[00:25:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1545.6599999999999s)]
*  It is very high bar. [[00:25:46](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1546.6999999999998s)]
*  The other is I do that with the algorithm and then I go to 100 doctors, human doctors, [[00:25:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1547.58s)]
*  and I get back 100 different responses, and then let's compare. [[00:25:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1552.3799999999999s)]
*  Yeah. [[00:25:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1554.9399999999998s)]
*  Right. [[00:25:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1555.34s)]
*  And then let's track over time and so. [[00:25:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1555.6599999999999s)]
*  So you compare to the median doctors. [[00:25:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1557.1s)]
*  Yeah. [[00:25:58](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1558.22s)]
*  Yeah. [[00:25:58](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1558.6999999999998s)]
*  Right. [[00:25:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1559.02s)]
*  And, like, how good is the median doctor at doing the diagnosis? [[00:25:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1559.34s)]
*  And, like, I don't know what your experience has been. [[00:26:01](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1561.8999999999999s)]
*  Yes. [[00:26:03](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1563.8999999999999s)]
*  Well, and the median doctor may be smart, but also may be overloaded, may be exhausted, [[00:26:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1564.22s)]
*  may have, like, 12 other patients. [[00:26:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1569.6599999999999s)]
*  15 minutes. [[00:26:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1571.8999999999999s)]
*  Yeah. [[00:26:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1572.62s)]
*  Yeah, yeah, yeah. [[00:26:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1573.02s)]
*  A lot of experiences, 15 minutes. [[00:26:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1573.5s)]
*  Yeah. [[00:26:14](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1574.78s)]
*  You know, there's a thing here where, like, experts in these areas tend to either, like, [[00:26:15](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1575.02s)]
*  be, like, doctors themselves or they, like, know a lot of doctors or they have, like, [[00:26:18](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1578.4599999999998s)]
*  they're, you know, they work in the industry, they make money, they have a concept, they [[00:26:21](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1581.8999999999999s)]
*  have a concierge doctor who spends a lot of time with them and does house calls. [[00:26:24](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1584.7s)]
*  Yeah. [[00:26:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1587.42s)]
*  The median healthcare experience is 15 minutes in somebody's, you know, [[00:26:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1587.9s)]
*  harried schedule with a doctor that may or may not ever see you again and has very limited data. [[00:26:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1591.26s)]
*  Yeah. [[00:26:35](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1595.26s)]
*  Yeah. [[00:26:36](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1596.06s)]
*  And. [[00:26:36](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1596.6200000000001s)]
*  Yeah. [[00:26:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1597.02s)]
*  And there's a well-known algorithm, which is that they come up with a diagnosis, [[00:26:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1597.3400000000001s)]
*  they come up with a treatment, you go with that, that doesn't work, you repeat. [[00:26:40](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1600.54s)]
*  Yeah. [[00:26:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1604.38s)]
*  And while not sick and, while still sick and not dead, you just repeat. [[00:26:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1604.8600000000001s)]
*  Yeah. [[00:26:49](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1609.18s)]
*  And then I think many of us have been through that. [[00:26:49](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1609.66s)]
*  Well, and then there's, and then there's all the other sort of things. [[00:26:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1611.26s)]
*  So then there's like drug interaction, you know, is any one doctor tracking all the interactions of [[00:26:53](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1613.02s)]
*  your drugs? [[00:26:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1616.46s)]
*  Yeah. [[00:26:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1617.1s)]
*  Then there's this other issue, which is, okay, they give the prescription, [[00:26:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1617.5s)]
*  is there actually compliance for taking the prescription? [[00:27:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1620.06s)]
*  Does the doctor actually know whether you're taking the prescription? [[00:27:01](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1621.9s)]
*  Yes. [[00:27:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1624.06s)]
*  Compliance is one of the biggest disasters. [[00:27:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1624.46s)]
*  Right. But that means, like, the ability for a median doctor to even evaluate the [[00:27:06](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1626.22s)]
*  success of a treatment, they may actually may not be able to do it because they may not have the [[00:27:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1629.34s)]
*  data on compliance. [[00:27:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1631.9s)]
*  Yeah. [[00:27:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1632.78s)]
*  And so, like, you look at the existing, I don't know, for me, you look at the existing system by [[00:27:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1633.5s)]
*  which this all happens, it's very similar to looking at the existing system by which people [[00:27:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1636.94s)]
*  actually drive cars, which is like, oh my God, this is not good. [[00:27:19](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1639.98s)]
*  Like, this is really not good. [[00:27:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1643.0200000000002s)]
*  And we kind of fool ourselves into believing that it's good because it kind of feels good, and we [[00:27:24](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1644.7s)]
*  don't really want to look behind the curtain, but we look behind the curtain, and it's pretty [[00:27:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1647.8200000000002s)]
*  horrifying. [[00:27:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1650.46s)]
*  Yeah. [[00:27:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1650.94s)]
*  And so from that standpoint, if you follow that logic, then it says, okay, if the machine could do [[00:27:32](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1652.14s)]
*  a better job, you know, if the machine was twice as good at just, like, listening to symptoms, [[00:27:36](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1656.0600000000002s)]
*  giving the response, doing the prescription, doing the follow-up. [[00:27:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1661.42s)]
*  Yeah. [[00:27:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1664.38s)]
*  I mean, how far, I don't know if you've done this, but you plug in a list of symptoms. [[00:27:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1664.7s)]
*  I've been playing with it too. Yeah, yeah, yeah, yeah. Yeah, yeah. I mean. [[00:27:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1668.3000000000002s)]
*  Because it does have access. I mean, it has access to the collective medical knowledge. [[00:27:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1671.34s)]
*  And if it doesn't now, it can't, right? You know, it could be filled with all the EMRs, [[00:27:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1674.4599999999998s)]
*  all the medical records and so on, and then it could sort of learn from that as well. [[00:27:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1679.4199999999998s)]
*  Right. Well, then the other question I'm sure you thought about, but like, okay, so the medical [[00:28:02](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1682.1399999999999s)]
*  field moves. And so in the existing system, the media and doctor has to, like, read all the papers. [[00:28:06](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1686.22s)]
*  Yeah, yeah, yeah, yeah, which never happens, but no one has time for that. [[00:28:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1691.1s)]
*  Yeah, right. Yeah, and there's continuing education, but still it's not the same. [[00:28:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1693.98s)]
*  Well, here's an example. Do you want your GP? Would you want a MGP or an old GP? [[00:28:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1697.02s)]
*  Probably young, right? Well, presumably the old GP has more experience, and so they have more [[00:28:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1702.22s)]
*  pattern matching over time and more experience with patients. But the young GP is probably more [[00:28:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1707.18s)]
*  up in the current science. Yeah, yeah, yeah. Okay. Yeah. And then it's like, okay, do you really want [[00:28:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1710.94s)]
*  to have to make that trade-off? Or can the machine actually have both of those? Yeah, exactly. Well, [[00:28:35](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1715.26s)]
*  that's the thing is that, like, you talked about how, like, can it beat, let's say, how does it do [[00:28:39](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1719.02s)]
*  compared to 100 doctors? When the 100 doctors collaborate, presumably that's the ideal situation, [[00:28:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1723.82s)]
*  right? I mean, or, or, no, that sounds horrifying. No, no, no, I mean, that's the wisdom of the [[00:28:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1728.7s)]
*  crowd. No, that's a good thing. It could go, well, I guess it could go either way, but usually, [[00:28:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1732.3s)]
*  that's the Soviet method. Usually when you actually, when you pool it, you can, or at least maybe it's [[00:28:58](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1738.46s)]
*  how you collaborate. Have you really found human beings to make better decisions in groups than [[00:29:03](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1743.5800000000002s)]
*  they do as individuals? That's a good question. Yeah. In your entire life? Yeah. Oh, yeah, yeah. [[00:29:07](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1747.1000000000001s)]
*  The full, the serious answer is wisdom of crowds, madness of crowds. Yeah, yeah, yeah, yeah, yeah, [[00:29:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1752.94s)]
*  or flip sides of the same coin, right? And so when are you harnessing the wisdom? When are you [[00:29:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1756.8600000000001s)]
*  descending into madness or even just, you know, mediocrity? Yeah. Yeah, I can, very specific tasks, [[00:29:20](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1760.54s)]
*  groups can do well, but otherwise it's like one big group project from high school. Yeah. Yeah, [[00:29:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1767.98s)]
*  which is like a- Well, so generally, right, generally what happens to people in groups is [[00:29:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1771.98s)]
*  the social conformance kicks in, right? And so people want, there's a well-known, you know, [[00:29:34](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1774.78s)]
*  kind of thing. There's like this law of like group polarization, which is you take a group of people [[00:29:38](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1778.46s)]
*  who are inclined slightly to one side of the political spectrum. Yeah, yeah, yeah. You put [[00:29:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1781.5s)]
*  them together, let them talk for three hours. They all come out much more radical. Yes, yes. [[00:29:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1784.22s)]
*  Because they've self-reinforced. Yes, yes, yes. Right. Well, so maybe that's a really interesting [[00:29:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1787.58s)]
*  thing because you can imagine training AI to do have these different aspects and its collaboration [[00:29:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1791.34s)]
*  with other versions of it would be very different. Yeah, yeah, could be very different. I mean, yeah, [[00:29:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1796.46s)]
*  maybe it should do this effectively in Monte Carlo. Yeah, yeah, yeah, yeah. Right, right, run the same [[00:30:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1800.78s)]
*  inputs a hundred times. Yes, yes. Right. Yeah. Well, okay, so either we'll never get there or [[00:30:05](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1805.42s)]
*  we're already there now. Right. But I think in 10 years, it does seem especially, maybe we hit like [[00:30:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1811.74s)]
*  another winter, but it seems like things are accelerating so much. This seems pretty real. [[00:30:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1817.42s)]
*  It seems pretty real. What do you think society needs to do to change? Because there's like all [[00:30:20](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1820.78s)]
*  these things we were talking about, and this seems bigger than like just the revolution of [[00:30:25](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1825.74s)]
*  software over the last 20 years or internet from the last 20 years. Because we're talking about how [[00:30:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1829.82s)]
*  it changes government, how it changes regulatory, how it changes education. I mean, I don't even [[00:30:33](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1833.34s)]
*  know where you want to start with that, but I think that's something where it may take us 10 [[00:30:38](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1838.46s)]
*  years just culturally to be able to get ready for this thing that may arrive in 10 years or may [[00:30:42](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1842.6200000000001s)]
*  already be here. Right, right. Yeah, I don't know where you want to start. Yeah, so where I would [[00:30:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1847.42s)]
*  start is we've already fallen into, we have deliberately kind of fallen into a trap already, [[00:30:50](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1850.94s)]
*  which is we've only been using a single kind of example, and we've used it both in our discussions [[00:30:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1855.18s)]
*  on medicine and also in education, which is basically something is done today. People are [[00:30:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1859.1000000000001s)]
*  doing something today and then maybe the machine can do it instead. That's an important thing, [[00:31:02](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1862.78s)]
*  and that's what we're thinking about. But the way that technological impact actually plays out in [[00:31:06](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1866.7s)]
*  human society is not just that. The way it plays out is it lets you basically revisit more fundamental [[00:31:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1871.5s)]
*  assumptions. Or what's not being done today. Or what's not being done today that all of a sudden [[00:31:15](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1875.9s)]
*  becomes possible. And this always comes up in any sort of discussion about employment, [[00:31:19](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1879.5s)]
*  whether people doing jobs versus machines doing jobs, so people get worried about technological [[00:31:24](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1884.06s)]
*  displacement of jobs. But technological displacement of jobs, like technology never actually creates [[00:31:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1887.82s)]
*  unemployment. Technology only ever creates jobs in that. And the reason for that is technology [[00:31:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1891.8999999999999s)]
*  makes possible things that were not possible before, which is what leads to growth. And so [[00:31:36](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1896.9399999999998s)]
*  specifically, for example, the role of the doctor. It's like, okay, the doctor of the future is [[00:31:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1901.58s)]
*  probably not going to be doing the same. We have a term in IT, break fix. It's kind of what doctors, [[00:31:46](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1906.62s)]
*  you know, the core motion of a lot of doctors is, as you said, diagnose, prescribe, diagnose, [[00:31:53](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1913.74s)]
*  prescribe. It's debugging. Yeah, debugging, iterative. Exactly. Doctors of the future, [[00:31:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1917.34s)]
*  probably, like the technologically empowered doctor 10 years from now is highly unlikely to be [[00:32:02](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1922.78s)]
*  spending their day doing that. They are probably going to be spending their day doing things that [[00:32:07](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1927.1799999999998s)]
*  are actually much more important than that. Yes. Right. And so, for example, maybe they have more [[00:32:10](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1930.4599999999998s)]
*  time, right, with patients because the machine is a time-saving device. Maybe they have more data [[00:32:14](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1934.78s)]
*  to draw on, you know, to be able to make their decisions. You know, they've got the machine as [[00:32:21](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1941.02s)]
*  a partner in making the decisions. Maybe they're able to spend more time in their conversation [[00:32:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1943.98s)]
*  with the patient talking about psychological issues as compared to just physical issues, [[00:32:28](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1948.14s)]
*  and as, you know, in a lot of medical conditions involve, you know, two sides of that, or behavioral [[00:32:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1951.66s)]
*  issues. Well, as you know, like a lot of primary medical issues today are a consequence of [[00:32:35](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1955.66s)]
*  different behaviors. Yes. And maybe doctors should be spending more time on behaviors. [[00:32:39](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1959.42s)]
*  Yes. And it speaks to compliance as well as other issues. Yeah. Yeah. I mean, compliance is a [[00:32:42](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1962.7s)]
*  behavioral issue. Like, why don't people do this or that? Right. But then also there's all the [[00:32:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1968.38s)]
*  behavioral health issues. Right. Which is probably one of the biggest catastrophes that we have [[00:32:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1972.22s)]
*  coming out of COVID. Yeah. Exactly. Right. Yeah. Exactly. And maybe doctors should be, you know, [[00:32:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1975.74s)]
*  maybe the doctor of the future will be more of a life coach, of which there will be a pharmacological, [[00:32:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1979.26s)]
*  you know, sort of a biological or pharmacological component. Right. But maybe it's like, maybe it's [[00:33:03](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1983.34s)]
*  more of, you know, sort of the dream of sort of holistic medicine. And so, you know, maybe the [[00:33:07](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1987.26s)]
*  doctor of the future is just as much, is actually a much more important and, you know, sort of [[00:33:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1992.6200000000001s)]
*  fundamental figure in your life than he or she is today. Yeah. That sounds fantastic. Yeah. [[00:33:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=1996.94s)]
*  Right. Exactly. So if I'm a doctor, that's where I would want to be, like, angling towards. Right. [[00:33:21](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2001.18s)]
*  And then that's probably a bigger and more important market. Right. And then in terms of, [[00:33:26](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2006.3799999999999s)]
*  like, the size of that industry will probably expand, you know, kind of correspondingly. I think [[00:33:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2011.1s)]
*  the same thing is true in education. Like, you know, the teacher 10 or 20 years from now, I hope, [[00:33:34](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2014.62s)]
*  is not doing the same things the teacher is doing today. I hope they're doing much better things. [[00:33:38](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2018.94s)]
*  Yeah. Right. So for example, one-to-one tutoring. Like, there's basically, let's take education [[00:33:42](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2022.46s)]
*  example. Like, there's only one in the last, like, 50 years, there's basically only one known [[00:33:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2027.18s)]
*  education intervention at scale that actually improves outcomes after, you know, thousands of [[00:33:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2031.18s)]
*  experiments. It's one-to-one tutoring. Yes. Which is very ancient, actually. Which is very ancient, [[00:33:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2034.7s)]
*  right. Which is the original form of education. Yes. Which is literally how people used to get [[00:33:58](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2038.54s)]
*  educated. And so maybe this industrial, you know, the education system we have today is an artifact [[00:34:02](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2042.46s)]
*  of the industrial age. If the industrial age components of it become automated, the teacher [[00:34:06](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2046.94s)]
*  becomes freed up to actually work more one-to-one with students, the result might actually be a [[00:34:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2051.58s)]
*  significant breakthrough in how education works. Although the ways you're describing, [[00:34:15](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2055.18s)]
*  you can imagine also, like, AI doing one-on-one pretty intensively. Well, yeah, there will be [[00:34:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2057.98s)]
*  part of that. But also, yeah, and maybe the AI is the one-on-one, and maybe in that case, the teacher [[00:34:21](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2061.98s)]
*  is supervising the AI. Right. And maybe the teacher is making sure that the AI is, like, on the right [[00:34:25](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2065.8199999999997s)]
*  track and doing the right things and is able to kind of sit at the control panel and watch all [[00:34:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2070.7s)]
*  that happening, right. Well, that speaks to something really interesting because I think [[00:34:33](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2073.5s)]
*  we're probably a little nervous, at least short term, to just unleash this and, like, not pay [[00:34:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2077.5s)]
*  attention to it. And so you'll have the doctor using this as a tool but keeping an eye on it. [[00:34:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2081.98s)]
*  You'll have the teacher maybe scaling dramatically for all this one-on-one but keeping an eye on it. [[00:34:46](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2086.94s)]
*  Do you think that's actually the way it's going to? I mean, this is kind of how all technologies work. [[00:34:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2091.5s)]
*  Yeah. So it's sort of, another way to think about it is you can imagine two acronyms for [[00:34:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2094.78s)]
*  AIs, artificial intelligence, which kind of implies replacement. Yeah. The one I actually [[00:35:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2100.7s)]
*  like much better is augmented intelligence, which is like the old Doug Engelbart idea. [[00:35:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2104.86s)]
*  And augmented intelligence is, you know, another example, the term would be Steve Jobs, [[00:35:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2108.62s)]
*  a bicycle for your mind, right, or a bullet train for your mind. So the augmentation, right. And so [[00:35:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2112.62s)]
*  the way, if you just look at the history of new technologies, the way it plays out is everybody's [[00:35:20](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2120.7799999999997s)]
*  afraid it's going to be a replacement and it turns out it's an augmentation. Yes. So you take a human [[00:35:24](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2124.46s)]
*  being and you give them the technological tools. They therefore are much more productive. Like a [[00:35:28](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2128.06s)]
*  factory versus like an artisan with their tools. Yeah, exactly. Or like, you know, the dream of, [[00:35:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2131.74s)]
*  like, an exoskeleton, you know, any of these things. I mean, look, artists are much more [[00:35:36](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2136.78s)]
*  productive today with digital tools than they were with just, you know, painting canvas. Yeah. [[00:35:42](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2142.46s)]
*  And by the way, even artists that still work on painting canvas are much more productive today [[00:35:46](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2146.3s)]
*  because they can sell their products to a much larger audience online. Or like my favorite thing [[00:35:49](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2149.26s)]
*  for art is like, you know, photography comes online and that dramatically changes art because [[00:35:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2152.86s)]
*  being photorealistic isn't that interesting anymore. But so that creates modern art. [[00:35:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2157.34s)]
*  Extraction, right. Yeah. Which actually is maybe even more expressive than just taking a picture. [[00:36:01](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2161.1800000000003s)]
*  And so now I can make pictures with AI all the time. So where does that shove art? Maybe to more [[00:36:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2164.94s)]
*  interesting places. And the artists in history, the artists were not happy about the introduction [[00:36:10](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2170.14s)]
*  of photography because they viewed it originally as a threat. Of course. Yeah. But it transformed [[00:36:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2173.26s)]
*  things. Yeah, it turned out to be, it turned out, yeah, it turned out. The market for art is much [[00:36:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2177.1s)]
*  larger today than it was before the introduction of photography. I mean, we call it different [[00:36:20](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2180.14s)]
*  things. We call it things like TV shows and so forth. But like the market for creative [[00:36:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2183.7400000000002s)]
*  expression is much, much larger than it used to be. By the way, music, same thing, right? You know, [[00:36:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2187.1s)]
*  recorded music was originally a threat. It used to be musician would compose and perform, [[00:36:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2191.02s)]
*  right? And then, you know, to have music in your home, you'd have to hire a musician to come into [[00:36:35](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2195.5s)]
*  your home. You know, phonographs were a threat to that. But phonographs made the music industry [[00:36:38](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2198.78s)]
*  much, much larger. So people who were good at making music all of a sudden had a much bigger [[00:36:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2203.5s)]
*  market. Yeah. So I think AI is going to play out in a very similar way. Like there are people who [[00:36:46](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2206.62s)]
*  will argue, you know, AI is different because it just keeps climbing this ladder and it will replace [[00:36:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2211.34s)]
*  everything. I actually think it's going to be, basically it's the ultimate superpower. It's the [[00:36:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2214.14s)]
*  ultimate pairing. We were talking about creating screenplays and scripts. A good example, if I'm a [[00:36:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2217.74s)]
*  Hollywood screenwriter today, like GPT is my best friend and I'm just sitting there all day long and [[00:37:02](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2222.14s)]
*  I'm just saying, you know, playing out. It's like, okay, I reached this plot point dot dot dot. Give [[00:37:06](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2226.2999999999997s)]
*  me a list of like 10 ideas for what to do. It's like, oh, okay, that's an interesting one. I'll [[00:37:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2229.2599999999998s)]
*  give you an example of how this could work. So Mad Men is one of my favorite shows. Matthew Wiener, [[00:37:14](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2234.14s)]
*  you know, ran that show and he was always praised. He's like, wow, that show is so unpredictable. [[00:37:18](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2238.3799999999997s)]
*  Like, you know, you never knew where it was going. And he said, yeah, well, the technique we had in [[00:37:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2242.7s)]
*  the writers room was at any given time we had to figure out what happened next in the plot. We would [[00:37:25](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2245.66s)]
*  brainstorm. We would come up with the five sort of things, five obvious things and then we would rule all those out. [[00:37:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2249.74s)]
*  So GPT would be obvious things and you rule those out. Yeah, exactly. So it pushes creativity. All of a [[00:37:36](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2256.14s)]
*  sudden every individual screenwriter can do that without having to have a whole writer's room to [[00:37:40](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2260.2999999999997s)]
*  brainstorm. You just plug that in. It gives it back to you in two seconds. He was like, okay, [[00:37:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2263.8199999999997s)]
*  not those things. I'm going to do something else. And now I am more creative than I was before. [[00:37:46](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2266.7799999999997s)]
*  Well, your comment about music is really interesting because now we've got Spotify, [[00:37:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2271.18s)]
*  so we've got everything in your pocket. Can you imagine like the AI Spotify, which is like the doctor, [[00:37:53](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2273.82s)]
*  the personal trainer, the educator, like all those different things in my pocket available right now [[00:37:58](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2278.3s)]
*  for whatever I need to do. Yeah, that's right. Yeah. And with the human escalation path, right? [[00:38:03](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2283.26s)]
*  Yes. Like the AI therapist or whatever, but yeah, with the thing of like, well, okay, yeah, [[00:38:06](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2286.78s)]
*  I have a thing here. Especially if it gets really serious to escalate immediately. Yeah, that's right. [[00:38:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2292.06s)]
*  Yeah. Okay. So what's going to hold us back? What do we need to change? So I think it's mostly fear. [[00:38:15](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2295.5800000000004s)]
*  This is where maybe I'm a radical on it because this is usually where people start talking about [[00:38:21](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2301.5s)]
*  regulation. I think it's like we have these fear-driven reactions. I always think of it. [[00:38:25](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2305.2599999999998s)]
*  There's this deep-seated myth in human societies, the Prometheus myth, right? Yeah. And the Prometheus [[00:38:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2310.7s)]
*  myth is all about new technology, right? And the Prometheus myth is like basically this new technology [[00:38:36](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2316.22s)]
*  of fire. And fire is one of these classic technologies where it can be used for good. [[00:38:40](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2320.06s)]
*  It can burn the whole of death. Or it can be used very badly, right? And yeah, it can destroy your [[00:38:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2324.7799999999997s)]
*  whole world. And so Prometheus famously goes and retrieves fire from the gods. And his punishment [[00:38:49](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2329.66s)]
*  for it is to be chained to a rock and his liver pecked out every day for the rest of eternity. [[00:38:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2336.14s)]
*  So embedded in there is the anxiety about the new technology and then the arrival of the new [[00:38:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2339.8999999999996s)]
*  technology maybe is like the fear, right? Is that it's not bad and the person who does that should [[00:39:03](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2343.74s)]
*  be punished. And so I always find that myth kind of plays out over and over again in all these [[00:39:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2348.46s)]
*  discussions about regulation that this stuff needs. Especially it's the gods who punish him, [[00:39:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2352.06s)]
*  right? The existing gods. Yes. Well, on behalf of existence. But yeah, so yeah, I think generally [[00:39:15](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2355.82s)]
*  it's this, it's just you get these fears. If you look at the history of, we talked about some of [[00:39:25](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2365.1800000000003s)]
*  this, if you look at the history of new technologies, they generally have had these fears every step [[00:39:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2369.42s)]
*  along the way. Every new technology has been created with some prediction that it's going to [[00:39:33](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2373.5800000000004s)]
*  upend the social order and cause the enormous chaos. Well, it does upend to some degree. [[00:39:36](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2376.3s)]
*  It will do that. But generally speaking, in a positive way, on balance, technology is [[00:39:40](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2380.3s)]
*  why we live much better lives today. Certainly people now would not want what people had 50 years [[00:39:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2385.66s)]
*  ago. Nobody would make that, yes. Right. And you could do, you could go back in time infinitely. [[00:39:50](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2390.3799999999997s)]
*  It would be like that. Nobody would ever make the trade, yes. Nobody would ever make the trade to [[00:39:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2394.3799999999997s)]
*  go back in time. It would never happen. Yeah. And right. That's literally, it's because you would not [[00:39:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2397.02s)]
*  want to lose the technologies and the consequences of the technologies you have today. So I think [[00:40:01](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2401.98s)]
*  that's true. And so I actually think like fear may be the rip off FDR, fear may be the actual [[00:40:06](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2406.14s)]
*  biggest threat. Fear leads to the kind of reach for regulation. I'm a skeptic. I don't, [[00:40:10](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2410.3s)]
*  it's like, I don't know, right. Regulating math. Are we really going to regulate math? [[00:40:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2416.6200000000003s)]
*  Well, but it's not going to look like regulating math, right? It's going to look like regulating [[00:40:21](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2421.5s)]
*  this superpower. That's what they're going to say. Yeah. Right. But then the actual implementation [[00:40:24](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2424.46s)]
*  of this is regulating, regulating, regulating algebra. Yeah. Yeah. Regulating algebra, [[00:40:28](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2428.46s)]
*  regulating linear algebra. Are we really going to regulate linear algebra, matrix multiplication? [[00:40:33](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2433.26s)]
*  Really? Seriously? Yeah. And then even if we do, are we going to possibly do it in a way that makes [[00:40:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2437.98s)]
*  any sense? Yeah. Well, okay. But it won't obviously won't look like that. It will be saying, well, [[00:40:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2443.66s)]
*  we can't have computers drive cars. Right. Or like, what's, how do you give the computer a test? [[00:40:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2448.06s)]
*  Yeah. Or how do you know? Like, okay, you make this, I'll be the cynic. So, okay, you make this [[00:40:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2454.38s)]
*  claim that the computer AI is better than human. Like, how do I know that? Because that, well, [[00:40:58](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2458.2999999999997s)]
*  as it turns out, because the cars are driving. Yeah. So, yeah, it's okay. So here was, okay, [[00:41:02](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2462.54s)]
*  so here's how that played out in self-driving cars. Yeah. Yeah. There was one category company [[00:41:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2468.22s)]
*  that said, we're going to basically wait until it's perfect. Yes. And we're going to basically [[00:41:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2471.34s)]
*  try to validate it. We're going to work with regulators and build this stuff. Yes. Yes. [[00:41:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2473.74s)]
*  There's other- They're not driving. Yeah. And they're not on the road. They're still not on the [[00:41:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2476.46s)]
*  road. Yeah. There's another category company that said, you know what, let's evolve out of basically [[00:41:19](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2479.5s)]
*  the cruise control. Yes. And, you know, it's sort of cruise control and then it's radar-based [[00:41:24](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2484.22s)]
*  And you get humans driving with it and you label data and- Exactly. And you don't expect the car [[00:41:28](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2488.78s)]
*  to drive itself from the very beginning. The car is like an autopilot kind of thing. The expectation [[00:41:33](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2493.26s)]
*  is you pay attention. Like, you know, Tesla is the company I'm alluding to. And if you turn on [[00:41:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2497.5800000000004s)]
*  full self-driving on Tesla, you're still, you know, you're still told, like, you're not supposed to [[00:41:42](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2502.1400000000003s)]
*  be watching a movie. You're supposed to be actually paying attention. And the car will, [[00:41:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2505.1800000000003s)]
*  like, alert you when it's time to pay attention. But, you know, notwithstanding that, Tesla has [[00:41:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2507.5s)]
*  been climbing the ladder on self-driving car functionality capability. They do new software [[00:41:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2511.98s)]
*  releases push live to car at night anytime they want. Those new releases are not being tested by [[00:41:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2515.66s)]
*  any federal, you know, it's a whatever is not. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. [[00:42:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2520.94s)]
*  There's no actual test happening. Yeah. And that has led to incredible progress including, [[00:42:05](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2525.02s)]
*  as you said, clearly in the data, this is now safer. Because you can't make it work just [[00:42:10](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2530.7s)]
*  magically, right? It has to happen gradually. Right. Because it's actually much like medicine. [[00:42:15](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2535.1s)]
*  It's centering into a complex system with a lot of variables in the real world. Like medicine, [[00:42:20](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2540.2999999999997s)]
*  too, it's like life or death. You know, it's just serious. But, yeah, but yeah, we go back to how [[00:42:24](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2544.3s)]
*  we started the conversation. Yeah. The the wait for permission thing, the binary zero or one wait [[00:42:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2549.9s)]
*  for permission, wait for perfection thing versus the incremental let's get better and better and [[00:42:35](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2555.26s)]
*  better. And the threshold is, is it better than humans? Is it is it is it a net improvement? [[00:42:39](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2559.6600000000003s)]
*  I mean, clearly in self-driving cars, that second approach is the approach that's working. [[00:42:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2563.82s)]
*  If you just observe the real world. And you think you get to the tipping point where, look, let's [[00:42:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2567.1800000000003s)]
*  look at the statistics we have because we have all this happening right now. We have the statistics [[00:42:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2571.18s)]
*  and it's like so much better than humans. Why wouldn't we do it? Yeah, exactly. Right. And then [[00:42:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2575.4199999999996s)]
*  at some point the morality tips where it's like, well, obviously we have to go in this direction [[00:42:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2579.2599999999998s)]
*  because it's just obviously better. Yeah. I suspect we're going to get there in medicine pretty quick. [[00:43:01](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2581.8999999999996s)]
*  Yeah, yeah, yeah, yeah. I'm an optimist on that. And again, I'm not an optimist because I think [[00:43:06](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2586.8599999999997s)]
*  the AI is going to be perfect. I'm an optimist because I think the status quo is not that great. [[00:43:10](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2590.46s)]
*  Yeah. Well, that might be like you start empowering doctors, you give them tools, [[00:43:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2593.4199999999996s)]
*  they start using them, you start empowering patients, patients start using them. And actually, [[00:43:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2596.7s)]
*  here I think it's even different than a car because you're not on a road. It's your body [[00:43:20](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2600.94s)]
*  or whatever. And actually, patients are driving their own healthcare more than ever. I think COVID [[00:43:25](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2605.1s)]
*  was another sort of tailwind there. So maybe you start, maybe it's just about developing the tools [[00:43:29](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2609.34s)]
*  and giving them out. Well, here would be an example. So let's use our screenwriting example. [[00:43:33](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2613.42s)]
*  It's applied to medicine, which is, you know, a given set of conditions, there may be many [[00:43:36](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2616.86s)]
*  possible diagnoses. An experience I've had is there's a set of symptoms. One doctor comes up [[00:43:40](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2620.38s)]
*  with one diagnosis, another doctor comes up with a different diagnosis. You read the literature and [[00:43:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2625.82s)]
*  it's like actually both of those diagnoses in theory are, but like for some reason the one guy [[00:43:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2628.94s)]
*  only thought of the one, the other guy only thought of the other. So a way for doctors to start using [[00:43:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2632.2200000000003s)]
*  this technology today would be plug in the symptoms, give me five possible diagnoses. [[00:43:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2636.54s)]
*  Yes. Okay. Oh, I didn't even realize that, you know, because maybe this is a new thing since, [[00:44:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2640.06s)]
*  you know, I went to medical school or something. I didn't realize diagnosis number three was an [[00:44:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2644.54s)]
*  option. I should go look at that. Yes. Yes. Right. And so the doctor is still doing the diagnosis. [[00:44:07](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2647.5s)]
*  It's your screenplay example. Yeah. You're augmented. As a doctor, you're augmented [[00:44:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2651.82s)]
*  because the AI in that case is alerting you to things that you should know, but you don't know. [[00:44:15](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2655.58s)]
*  Yeah. Yeah. I mean, that's interesting. It's almost like having a mentor or, [[00:44:19](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2659.02s)]
*  or just someone to riff with. That's right. Yeah. Yeah. Yeah. And they write, it's a great thing is [[00:44:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2663.66s)]
*  it is a machine. It will riff with you as much as you want. Like it will sit there at three in the [[00:44:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2667.58s)]
*  morning. It's a hundred times for you. It's happy to, it doesn't get bored. It doesn't get tired. [[00:44:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2670.62s)]
*  Yes. By the way. And then it also has the advantage. It has all the up-to-date information. Yes. Yes. [[00:44:34](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2674.14s)]
*  And all the outcomes. And when it makes a mistake, it actually can learn from it rather actually from [[00:44:38](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2678.46s)]
*  other than being like devastated by it or emotionally reacting to it. Right. Right. [[00:44:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2684.2200000000003s)]
*  And like self-driving cars, if it makes, if some other doctor in some other state had a patient [[00:44:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2688.54s)]
*  last week and made a mistake and they fixed the mistake, it will not make the mistake again. [[00:44:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2692.14s)]
*  You're a patient. Yeah. Yeah. Right. Yeah. So, I mean, so you think, and so that is a very [[00:44:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2695.26s)]
*  different regulatory play than we've seen in the history of healthcare. Well, I think that's just, [[00:45:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2700.2200000000003s)]
*  well, you tell me, I think that's just going to happen. So here's what everybody knows. I'll [[00:45:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2704.86s)]
*  give you a couple of things. Everybody knows that patients should not be on Google. Yes. [[00:45:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2708.2200000000003s)]
*  They're on their own symptoms. Everybody knows every patient now does that. It's called Dr. Google. [[00:45:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2711.74s)]
*  Exactly. Literally how it's called in the field. Right. And there's no way like you're not [[00:45:16](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2716.7799999999997s)]
*  practically speaking and I like regulate that out of existence. Yeah. That's going to happen. [[00:45:19](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2719.8199999999997s)]
*  I think doctors using these new tools as an augment is something that they can just do. [[00:45:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2723.3399999999997s)]
*  It doesn't require approval. So the ship has already sailed, do you think? I think so. Yeah. [[00:45:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2727.66s)]
*  And by the way, patients using GPT, if it hasn't started, it's going to start imminently. Yeah. [[00:45:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2731.1s)]
*  Probably. So the patients are going to show up with the results of GPT queries and the doctors [[00:45:34](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2734.3799999999997s)]
*  are going to have to respond to that. And so they're going to end up being in this world [[00:45:38](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2738.4599999999996s)]
*  whether they want to be or not. But that's actually really interesting because as a patient, [[00:45:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2741.1s)]
*  and I probably know just enough about medicine to be dangerous to myself, but like I show up [[00:45:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2744.62s)]
*  with the doctor and I have all of that thought out, basically that might equalize the patients. [[00:45:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2748.38s)]
*  Such that they can actually come much more educated and come from much more thoughtful [[00:45:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2754.22s)]
*  and they become much more in the process as well. Yeah. Okay. So what goes wrong? Double-edged [[00:45:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2757.3399999999997s)]
*  sword. I mean, as a doctor, do you want your patient? Yeah. You want your patient more educated [[00:46:01](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2761.42s)]
*  or less educated? They may just be humoring me, but I think they want them more educated. Maybe [[00:46:05](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2765.9s)]
*  with you they want you more educated. With me they might look a little bit more sideways. [[00:46:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2769.18s)]
*  But if it was really helpful, I think it's just about how good it is, right? Okay. So what goes [[00:46:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2773.3399999999997s)]
*  wrong? I mean, look, the big thing that goes wrong, I mean, look, the big thing, I think two things [[00:46:18](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2778.94s)]
*  go wrong. So one is just the expectation of perfection. And look, it's very easy to generate [[00:46:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2783.4199999999996s)]
*  the negative headline. It's very easy to set off the scare, the moral panic basically. A single [[00:46:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2790.14s)]
*  instance goes wrong and it gets extrapolated. We talk a lot about thalidomide. It'd be very easy [[00:46:35](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2795.26s)]
*  to have that kind of moment. Or like the person on a bike that got hit by a Tesla or something [[00:46:40](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2800.0600000000004s)]
*  like that. I think it was biking across a freeway. Right. Exactly. And so a human probably [[00:46:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2805.1000000000004s)]
*  hit him too. That's right. Oh, well, that's a good point. Yeah. So the trolley problem, [[00:46:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2811.42s)]
*  the trolley problem's been in the press a little bit more recently because it turns out that Sam [[00:46:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2816.6200000000003s)]
*  Beckman Fried was an expert in the trolley problem. So that shows you that maybe that's [[00:46:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2819.5800000000004s)]
*  not the route to ultimate morality as it's been marketed. But yeah, the trolley problem [[00:47:02](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2822.86s)]
*  always gets mooted about for self-driving cars, which is you have a choice between killing, [[00:47:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2828.94s)]
*  it's like, I don't know, five grandmas or one little kid or all these different like, [[00:47:14](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2834.86s)]
*  you have to pull the lever to decide. But human drivers don't. No, no, no. Human drivers never [[00:47:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2837.58s)]
*  make that decision. No, no, they have gas or brake. Yes. Right. And they have, I'm going to hit the [[00:47:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2843.5s)]
*  car in front of me or I'm not going to hit the car in front of me. It's never this elaborate thing. [[00:47:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2847.58s)]
*  It's always a very simple thing. And so it's not a question of whether the machine can ideally [[00:47:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2850.7s)]
*  solve this sort of idealized complex problem. It's going to hit the brakes faster when it's [[00:47:34](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2854.3799999999997s)]
*  about to crash into the car directly in front of it. And so properly, logically kind of containing [[00:47:39](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2859.3399999999997s)]
*  the expectation here to actual real world and not having this spin off into these like, basically, [[00:47:44](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2864.3799999999997s)]
*  fantasy narratives that you can then criticize. Yeah. So that, the absolute limits. And then, [[00:47:49](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2869.2599999999998s)]
*  yeah, look, I think just the generalized fear. And what I always have to remind myself is like, [[00:47:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2875.74s)]
*  like I say, I'm a software developer by background. It's like, okay, I can actually, [[00:48:01](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2881.2599999999998s)]
*  like the algorithms that do this, like, you know, can I tell you every aspect of how they work? No. [[00:48:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2884.7s)]
*  Like, do I understand how they work? Do I understand the basic foundations? Do I understand [[00:48:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2888.7s)]
*  the basic math? Yes. This is why I make the comment about regulating math. Yes. Now, [[00:48:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2891.66s)]
*  to somebody who's not a coder, right, this whole, all this stuff just seems like weird magic. Yeah. [[00:48:15](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2895.98s)]
*  And so there is a, I have to remind myself to be patient and tolerant of people who don't understand [[00:48:21](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2901.74s)]
*  the mechanics of what's happening. That said, I think the people who are going to be, I think [[00:48:27](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2907.02s)]
*  they also have to get into the mechanics and try to understand this. And there's always slippage [[00:48:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2911.1s)]
*  there. Yeah. So what's the antidote to fear? Is it optimism? Is it education? I mean, ideally, [[00:48:34](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2914.38s)]
*  I mean, I think there's, ideally it's, yeah, ideally it's cultural, cultural orientation [[00:48:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2921.18s)]
*  towards new technology. And then ideally it's, it's education and people learning and kind of, [[00:48:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2925.5s)]
*  or, you know, the CP Snow II cultures. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. [[00:48:49](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2929.42s)]
*  Coming together and kind of educating each other. Honestly, a big part of it also, I think, is when [[00:48:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2931.98s)]
*  things become a fated company. And this is what Tesla's done with self-driving cars. Yeah. Like [[00:48:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2936.38s)]
*  if it's just happening. Yeah. Yeah. Right. Cause who would want to go back? Like nobody wants to [[00:49:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2940.1400000000003s)]
*  go back. And like the system adapts. Right. And so there was this famous Uber, Uber fought all these [[00:49:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2944.2200000000003s)]
*  regulatory wars in all the cities that they were in, cause it was not technically allowed under the [[00:49:10](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2950.46s)]
*  taxi limo charters in the beginning. And so one of the things they did early on was they just made [[00:49:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2953.82s)]
*  sure that there were always lots of Uber cars available around state houses and city halls. [[00:49:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2957.98s)]
*  Yeah. And so whenever somebody, you know, so you'd literally have somebody who's like, you know, [[00:49:21](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2961.6600000000003s)]
*  sort of giving this like roaring speech, you know, in city hall about shutting down Uber. Yeah. And [[00:49:25](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2965.5s)]
*  then they would come out and they'd have to get home really fast. And Uber would show up 20 seconds [[00:49:28](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2968.7s)]
*  later. Right. It's like at some point it just was like taken for granted. And then at that point, [[00:49:32](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2972.06s)]
*  if you just said, literally, are we going to take Uber away? People would have said, no, we can't. [[00:49:36](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2976.7s)]
*  It's over. And that's what happened. And then, and then they, and then literally what happened is they [[00:49:40](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2980.46s)]
*  changed the laws to accommodate that behavior. And so I actually think part of it here is just [[00:49:43](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2983.42s)]
*  like having these tools. Okay. Here's the thing. Here's a good news thing. Yeah. These tools are [[00:49:47](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2987.02s)]
*  becoming widely available upfront. Right. So like 50 years ago, a new technology like this would have [[00:49:51](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2991.58s)]
*  been like deployed in the government first and then in big companies. And then years later in the [[00:49:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=2996.46s)]
*  form of something individual people could use the model today is like, it's just online. Yeah. Like [[00:50:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3000.06s)]
*  GPD is online right now. Yes. Well, the, the future of paint is really intriguing because from an [[00:50:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3004.46s)]
*  engineer point of view, it's the engineer's dream that if we make it good enough, such that it can [[00:50:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3009.66s)]
*  get to a point where people just love it and it's helpful and it does what it needs to do, the rest [[00:50:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3013.98s)]
*  will take care of itself. Yeah. I mean, I kind of think that's mostly how things are. I mean, [[00:50:19](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3019.18s)]
*  yeah, yeah. No, it's a beautiful future. Yeah. Yeah. So now look, having said that healthcare is [[00:50:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3023.3399999999997s)]
*  very sophisticated, right? There's lots of regulations, there's lots of payment, right? [[00:50:28](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3028.38s)]
*  All these things. So I saw this thing on Twitter the other night that blew my mind, right? Because [[00:50:32](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3032.14s)]
*  this whole time I've been thinking in terms of like, you know, diagnosis stuff in my life. [[00:50:35](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3035.66s)]
*  So this doctor posted a video and I think I saw that one. So that was a video and he said, look, [[00:50:39](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3039.3399999999997s)]
*  he said, the problem is whatever diagnosis or whatever, like I do the diagnosis, I do the [[00:50:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3045.18s)]
*  prescription. Then it's a question of whether or not I can get the insurance company to reimburse, [[00:50:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3048.8599999999997s)]
*  to actually pay for the thing. To do that for anything even slightly out of the ordinary, [[00:50:53](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3053.74s)]
*  I have to write a letter, I, the doctor has to write a letter to the insurance company and that [[00:50:57](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3057.1s)]
*  letter needs to be in a specific format and it needs to make the case and it needs to have the [[00:51:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3060.46s)]
*  scientific citations. And if I do the letter really well, it's going to get paid for it. If I don't [[00:51:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3064.62s)]
*  do the letter really well, it's not going to get paid for it. It's going to matter, you know, to [[00:51:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3068.3799999999997s)]
*  the life of the patient. And so he's like, it turns out GPT is really good at writing those letters. [[00:51:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3071.9s)]
*  With the references. [[00:51:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3077.0200000000004s)]
*  With the references, yes. With the scientific references, like full on, right? And so you've [[00:51:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3077.82s)]
*  got this, that's another way to think about it is you've got this bureaucratic process, [[00:51:22](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3082.3s)]
*  which is legitimate and required, it needs to exist. And that data needs to be submitted. And [[00:51:26](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3086.1400000000003s)]
*  honestly, it does not matter to that process whether that document is written by human or [[00:51:30](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3090.38s)]
*  machine. But all of a sudden, if every doctor in the world is really good at writing correctly, [[00:51:33](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3093.9s)]
*  those letters, then all of a sudden, it goes to the thing. All of a sudden that doctor now is [[00:51:38](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3098.94s)]
*  another, you know, whatever, four hours a week to actually take care of patients. Like that's the [[00:51:42](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3102.0600000000004s)]
*  kind of thing that I think is going to happen quite quickly. And that was interesting. What was [[00:51:45](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3105.5800000000004s)]
*  interesting about that example is you can imagine that example having a big impact on the efficiency [[00:51:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3108.38s)]
*  of the healthcare system today without any regulatory changes. Yes. Without any changes. [[00:51:52](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3112.1400000000003s)]
*  It's within the system. [[00:51:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3115.82s)]
*  Within the system, working within the system. And so, and that was just, and that was the one [[00:51:56](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3116.46s)]
*  where it's just like, oh, in retrospect, that's obvious. I just haven't thought about it. One guy [[00:52:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3120.78s)]
*  thinks about it. All the other doctors start to do that. The whole system upgrades, stuff function, [[00:52:05](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3125.02s)]
*  one time. That kind of thing, I think, is a real possibility. [[00:52:09](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3129.02s)]
*  Yeah. And that could be because someone's working within the system, you can have the [[00:52:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3132.3s)]
*  transformation immediately. But then eventually someone has to read all those letters. Someone [[00:52:15](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3135.5s)]
*  has to validate them. It's probably, you know, some sort of NLP on the other side. [[00:52:19](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3139.1s)]
*  Well, that's right. It's corresponding. So there's, we have this company, [[00:52:23](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3143.02s)]
*  this company called Do Not Pay, which is this company. It's an app that sort of acts like a bot. [[00:52:26](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3146.86s)]
*  I've used the app. It's pretty nice. [[00:52:32](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3152.46s)]
*  It's for people to try it. And it basically, it'll basically get you, it started to get you [[00:52:33](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3153.58s)]
*  like out of, it was starting to get you out of like basically sort of BS traffic tickets. And then [[00:52:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3157.5s)]
*  it's, he did this thing a while ago where it will unsubscribe you for, you know, all these [[00:52:42](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3162.54s)]
*  consumer subscription services like Comcast or whatever, like they all make it hard to like ever [[00:52:46](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3166.7s)]
*  turn off the subscription. And so he has this way to, the bot will do it for you. And so he just [[00:52:50](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3170.22s)]
*  started using AI in the bot. And so he now, so the way a lot of consumer subscription companies work [[00:52:54](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3174.46s)]
*  is you can't, you can subscribe online. You can't actually unsubscribe online. You have to call an [[00:53:00](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3180.46s)]
*  800 number and you have to argue with the person. And there's actually this thing in these companies [[00:53:04](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3184.46s)]
*  called Save Teams where they're actually paid specifically to prevent you from unsubscribing. [[00:53:08](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3188.3s)]
*  And they'll try to cut special deals with you and they'll try to talk you out of it. [[00:53:12](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3192.94s)]
*  And so he has this thing wired up where now he has the, he has AI generated text with, [[00:53:15](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3195.9s)]
*  with then text to speech. Oh, it's just talking. And it talks, and it talks, it talks to the customer [[00:53:20](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3200.86s)]
*  service person at the end of the line. And basically with infinite patience. And so it will just sit [[00:53:25](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3205.42s)]
*  and it will just argue like, no, I am actually going to unsubscribe for this. No, I'm not going [[00:53:31](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3211.98s)]
*  to accept your special offer. No, no, no, no, no. Right. Yeah, exactly. Until finally the other guy, [[00:53:36](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3216.46s)]
*  finally, you know, the guy gives up and says, okay, fine, I'll take it. I'll stop charging you. [[00:53:41](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3221.66s)]
*  And so it's like, okay, you know, it was, it was a precondition of the system that that worked the [[00:53:46](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3226.54s)]
*  way that it did. It was a burden on people to have to deal with that AI can now step in and equalize [[00:53:50](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3230.78s)]
*  the power imbalance between the customer and the company. And presumably that will change the system. [[00:53:55](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3235.34s)]
*  Yeah. Well, one would think, right? Yeah. Yeah. Yeah. Yeah. And to your point, like step one for [[00:53:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3239.26s)]
*  changing the system might be retaliation, which is all of a sudden the save teams will be bots. [[00:54:02](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3242.46s)]
*  And so maybe the bots will be arguing with the bots, but at least it gets you out of this kind [[00:54:06](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3246.38s)]
*  of Kafka-esque thing you're in today, where when you deal with these big companies, you're dealing [[00:54:10](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3250.06s)]
*  with this giant, you're an individual dealing with a giant bureaucracy. At least it like equalizes [[00:54:13](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3253.34s)]
*  the power. Well, that's kind of amazing. And that will be the spark for changing things. [[00:54:17](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3257.7400000000002s)]
*  Because once you're in that sort of system, like we got to do better than this. Yeah, this is crazy. [[00:54:21](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3261.1000000000004s)]
*  It's having bots argue with each other all day long. It's just clearly stupid. Yeah. Yeah. Yeah. [[00:54:25](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3265.4199999999996s)]
*  And especially when it's bots on both sides, now we can finally say, well, let's do an API on both [[00:54:28](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3268.8599999999997s)]
*  sides. So let's do something smart on both sides. Yeah. Just connected. Yeah. Yeah. Yeah. Well, [[00:54:32](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3272.3799999999997s)]
*  Mark, I mean, that's such a sort of beautiful, optimistic view of how this could go, right? [[00:54:37](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3277.1s)]
*  Because the future we're talking about is actually much more engineer driven that if an engineer can [[00:54:42](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3282.54s)]
*  build this and it really, really works, it really helps patients. It really changes things. It'll [[00:54:48](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3288.54s)]
*  get adopted as it gets adopted cultural work around it and we'll love it and we'll not want [[00:54:53](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3293.98s)]
*  to go back. And then the future will just be right in front of us. Yeah. I mean, patients are going [[00:54:59](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3299.7400000000002s)]
*  to get a vote. Yes. Doctors are going to get a vote. Right. Yeah. And you know, this, you know, [[00:55:03](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3303.82s)]
*  look, it's an, it's an industry made up of people, a world made up of people. People will get a vote. [[00:55:07](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3307.58s)]
*  Yeah. Beautiful. Thank you so much for joining us. Good. Yeah. You bet. [[00:55:11](https://www.youtube.com/watch?v=c7ScUDYSRYo&t=3311.26s)]
