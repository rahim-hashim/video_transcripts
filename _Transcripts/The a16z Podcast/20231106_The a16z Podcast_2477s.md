---
Date Generated: May 23, 2025
Transcription Model: whisper medium 20231117
Length: 2477s
Video Keywords: []
Video Views: 8792
Video Rating: None
Video Description: Is it possible to construct a virtual society that authentically replicates human behavior? AI Town, a virtual town experiment where AI residents live, interact, and engage, provides valuable insights into the future of AI's believability and its interaction with humanity.

In this panel discussion, Joon Park, the author of 'Generative Agents: Interactive Simulacra of Human Behavior,' and Martin Casado from a16z, will explore the influence and potential of Generative Agents, discussing their practical applications in the real world.

Topics Covered
00:00 - Simulating human behaviors
03:34 - What are generative agents?
07:35 - Simulations, new technology, and LLMs
12:07 - The architecture behind simulating human behavior
17:10 -  Generative agents interactions: observing, planning, and reflecting
21:14 - What is the value in advancing generative agents?
25:05 - Use cases for simulation behavior technology
30:43 - What are the ethical frameworks?
34:35 - Q&A from the audience 

Resources: 
Find AI Town: https://www.convex.dev/ai-town
Read the paper ‘Generative Agents: Interactive Simulacra of Human Behavior’: https://arxiv.org/pdf/2304.03442.pdf
Find Joon on Twitter: https://twitter.com/joon_s_pk
Find Martin on Twitter: https://twitter.com/martin_casado

Find a16z on Twitter: https://twitter.com/a16z 
Find a16z on LinkedIn: https://www.linkedin.com/company/a16z 
Subscribe on your favorite podcast app: https://a16z.simplecast.com/ 
Follow our host: https://twitter.com/stephsmithio 

Please note that the content here is for informational purposes only; should NOT be taken as legal, business, tax, or investment advice or be used to evaluate any investment or security; and is not directed at any investors or potential investors in any a16z fund. a16z and its affiliates may maintain investments in the companies discussed. For more details please see a16z.com/disclosures.
---

# Inside AI Town: What AI Can Teach Us About Being Human
**The a16z Podcast:** [November 06, 2023](https://www.youtube.com/watch?v=d-hNCmHz1uo)
*  I think generative agents and tools like the large-lingual model could be used to advance social science. [[00:00:00](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=0.0s)]
*  And social science to a large extent has been the quest to understand who we are. [[00:00:08](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=8.0s)]
*  And there's a lot of really interesting applications that can come out of that that will empower different communities and societies. [[00:00:14](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=14.0s)]
*  A few weeks ago, the A16Z infrastructure team ran an event in the San Francisco office. [[00:00:21](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=21.0s)]
*  The topic? Generative agents. [[00:00:26](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=26.0s)]
*  These are autonomous characters designed to simulate human behavior, derived from a recent but game-changing paper called Generative Agents, Interactive Simulacra of Human Behavior. [[00:00:29](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=29.0s)]
*  Developers from all around the city came to hear the lead author, June Park, speak alongside A16Z general partner, Martine Casado. [[00:00:41](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=41.0s)]
*  And in this panel, they discuss how this paper and the advancements in large language models have opened a new window, expanding the dynamism of simulation, [[00:00:50](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=50.0s)]
*  which instead of binary logic, were using probabilistic thinking and the ability to incorporate new information. [[00:00:59](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=59.0s)]
*  So what does that really mean? Well, instead of your character in Sims following very specific rote rules, [[00:01:06](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=66.0s)]
*  With generative agents, a father may go outside because he notices his son, another may take their breakfast off the stove because they notice it's burning, [[00:01:13](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=73.0s)]
*  and another may even opt into a Valentine's Day party invite and then elect not to show up. [[00:01:21](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=81.0s)]
*  All very human behaviors. [[00:01:27](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=87.0s)]
*  Now, the architecture described in the paper is, of course, intentionally designed by June and team. [[00:01:29](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=89.0s)]
*  And it's a combination of a seed identity for every agent and then functions that cause each one to do three discrete things. [[00:01:35](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=95.0s)]
*  To observe, to plan and to reflect. [[00:01:42](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=102.0s)]
*  And these architecture decisions ultimately generate unexpectedly spirited conversations just like this. [[00:01:45](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=105.0s)]
*  Hey, Lucky, it's so great to see you. How have you been? I've been dying to hear about your space adventure. [[00:01:51](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=111.0s)]
*  Hey, Kira, I've been fantastic. My space adventure was out of this world. I can't wait to share all the details with you. [[00:01:57](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=117.0s)]
*  Or even this. [[00:02:05](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=125.0s)]
*  I've been trying to find my way. It's been a chaotic journey to say the least. [[00:02:07](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=127.0s)]
*  Embrace the chaos, dear Kurt. For within its turbulence lies hidden truth. [[00:02:13](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=133.0s)]
*  Seek the depths of the unknown and unravel the mysteries that burden your soul. [[00:02:19](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=139.0s)]
*  And here's the thing. They don't just interact with each other. [[00:02:26](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=146.0s)]
*  Again, they wake up, they cook, some paint while others write, they hold opinions of one another, [[00:02:29](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=149.0s)]
*  and most importantly, they remember and they have higher level reflections based on the past. [[00:02:35](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=155.0s)]
*  It's pretty amazing, don't you think? [[00:02:40](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=160.0s)]
*  So as these generative agents become a lot closer to nuanced human behavior, [[00:02:41](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=161.0s)]
*  what can we learn about being human from these surprisingly realistic simulations? [[00:02:46](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=166.0s)]
*  And what is the calculus of that believability? [[00:02:51](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=171.0s)]
*  Are there real world applications on the horizon? And what is truly net new here? [[00:02:54](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=174.0s)]
*  Listen in as we discuss all that and more, including the origin of the very paper that June wrote. [[00:02:59](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=179.0s)]
*  I hope you enjoy. [[00:03:05](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=185.0s)]
*  As a reminder, the content here is for informational purposes only, [[00:03:06](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=186.0s)]
*  should not be taken as legal, business, tax, or investment advice, [[00:03:10](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=190.0s)]
*  or be used to evaluate any investment or security, [[00:03:13](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=193.0s)]
*  and is not directed at any investors or potential investors in any A16Z fund. [[00:03:16](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=196.0s)]
*  For more details, please see a16z.com slash disclosures. [[00:03:20](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=200.0s)]
*  Welcome, everyone. [[00:03:29](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=209.0s)]
*  We actually simulated this before you joined, and everyone's sitting exactly where we thought. [[00:03:34](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=214.0s)]
*  How many people in this room have actually read the generative agents paper that June wrote? [[00:03:41](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=221.0s)]
*  It's a lot of people, pretty much everyone. [[00:03:46](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=226.0s)]
*  So June, even though so many people have read it, why don't you just give a quick overview of what it is, [[00:03:49](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=229.0s)]
*  but also maybe the backstory that people haven't maybe heard of. [[00:03:55](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=235.0s)]
*  So generative agents is these general computational agents that can simulate believable human behavior. [[00:03:59](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=239.0s)]
*  Fundamentally, it leverages something like a LARS-Lynch model under the assumption that a LYNCH model has encoded [[00:04:05](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=245.0s)]
*  or has seen so much about human behavior from its training data, from the Wikipedia, social web, and so forth. [[00:04:11](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=251.0s)]
*  So if you are able to poke at the right angle, [[00:04:19](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=259.0s)]
*  you can actually extract a lot of those human behaviors in a very context-specific manner. [[00:04:21](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=261.0s)]
*  The opportunity here is that in the past we had to manually author a lot of these behaviors, [[00:04:26](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=266.0s)]
*  but now we can simply generate them with LYNCH model. [[00:04:31](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=271.0s)]
*  So generative agents leverages that to create these computational systems. [[00:04:34](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=274.0s)]
*  Ultimately, one sort of technical sort of improvement that we're trying to make in addition to LARS-Lynch model [[00:04:38](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=278.0s)]
*  is basically giving it some form of memory and retrieval system. [[00:04:46](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=286.0s)]
*  So you may have all used, obviously, ChatGPT and so forth. [[00:04:50](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=290.0s)]
*  It is heavily context-limited, and even if that limitation were to go away in the future, [[00:04:54](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=294.0s)]
*  processing a lot of really long-term context window is really inefficient and also ineffective [[00:04:59](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=299.0s)]
*  when you're trying to prompt these models for really narrowly defined behavioral aspects. [[00:05:05](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=305.0s)]
*  So main philosophy here is we're going to give long-term memory for these agents that's external to the LYNCH model, [[00:05:09](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=309.0s)]
*  and then retrieve the contextually relevant information from that long-term memory, [[00:05:18](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=318.0s)]
*  whether it's planning, action sequences, or reflections, to create these computational agents. [[00:05:22](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=322.0s)]
*  Philosophically, to some extent, I think this is akin to creating the operating system around LARS-Lynch model [[00:05:29](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=329.0s)]
*  in the way we're sort of prompting LARS-Lynch model. [[00:05:36](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=336.0s)]
*  To me, it feels a lot like how we used to use computers back in the day when we had to wire up the backend [[00:05:38](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=338.0s)]
*  every time you run a new program. [[00:05:43](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=343.0s)]
*  And what has really made complex behavior with these computational tools possible [[00:05:46](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=346.0s)]
*  was the introduction of these larger architecture that surrounds the core fundamental techniques. [[00:05:52](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=352.0s)]
*  So that's what generative agents is about. [[00:05:58](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=358.0s)]
*  And you mentioned sort of the background of why we got into all this. [[00:06:01](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=361.0s)]
*  So I started my PhD at the start of midway through 2020. [[00:06:04](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=364.0s)]
*  That was just around when Chiptis 3 was about to come out. [[00:06:11](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=371.0s)]
*  And that year, we, a bunch of basically authors at Stanford, [[00:06:14](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=374.0s)]
*  were working on this paper called Foundation Model, the Opportunities and Risks of Foundation Models. [[00:06:19](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=379.0s)]
*  What we were seeing was these new form of machine learning models [[00:06:24](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=384.0s)]
*  that seemed fundamentally different than the things that we had experienced in the past, [[00:06:28](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=388.0s)]
*  in that we didn't have to find or specifically train models for very narrow purposes. [[00:06:33](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=393.0s)]
*  But we can train general model, almost like a stem cell in bio, [[00:06:38](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=398.0s)]
*  and leverage that to create a lot of downstream behaviors. [[00:06:42](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=402.0s)]
*  So we wrote, after writing that paper, sort of my team, especially myself and my advisors, [[00:06:46](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=406.0s)]
*  what we really wanted to answer is there seems to be a new opportunity, but exactly what is it? [[00:06:52](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=412.0s)]
*  I think in the early days of Chiptis 3, a lot of the tasks that we were doing [[00:06:58](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=418.0s)]
*  were things like classification and generation, [[00:07:02](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=422.0s)]
*  which was really cool to see that these models can conduct these tasks. [[00:07:05](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=425.0s)]
*  But also something that we already knew how to do for many decades. [[00:07:10](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=430.0s)]
*  And our general philosophy there was if these models are truly new [[00:07:14](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=434.0s)]
*  and they give us fundamentally different opportunity than what we had in the past, [[00:07:18](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=438.0s)]
*  then they should be able to do something that's fundamentally different. [[00:07:22](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=442.0s)]
*  So that's how we got into this. Our answer to that basically was, [[00:07:26](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=446.0s)]
*  I think we might be able to create human-like agents that can populate this virtual world. [[00:07:30](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=450.0s)]
*  Warting, maybe you can just elaborate. You said it's perhaps one of the most exciting times in recent history. [[00:07:35](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=455.0s)]
*  Maybe you can just speak to exactly what you mean there [[00:07:41](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=461.0s)]
*  and how it relates to simulation and some of this new technology that we're seeing with LLMs. [[00:07:44](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=464.0s)]
*  So first, very quick credit where credit's due. [[00:07:49](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=469.0s)]
*  So as far as an AI town, clearly Jun is like the grandfather of AI town. [[00:07:52](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=472.0s)]
*  We wouldn't be here without your work, so we really appreciate you coming here. [[00:07:57](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=477.0s)]
*  AI town itself actually came from a personal project from Yoko. [[00:08:00](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=480.0s)]
*  So that's Yoko. [[00:08:03](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=483.0s)]
*  The true story is it was actually a personal project, [[00:08:06](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=486.0s)]
*  and I was like, hey, maybe more people would be interested in it. [[00:08:13](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=493.0s)]
*  And I kind of coerced her into bringing it forward to everybody else. [[00:08:16](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=496.0s)]
*  Now when it actually comes to the code, the vast majority of the work on the code was actually done by Ian on the back end. [[00:08:21](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=501.0s)]
*  So Yoko had done a prototype. [[00:08:29](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=509.0s)]
*  It's kind of funny. You see this funny little tile set up here, [[00:08:34](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=514.0s)]
*  and it kind of belies the fact that it's actually really hard to build a scalable, shared state distributed system that you'd need in a multiplayer game. [[00:08:39](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=519.0s)]
*  It's just a hard technical problem, right? [[00:08:47](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=527.0s)]
*  And anybody that's built large systems knows that. [[00:08:49](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=529.0s)]
*  And so it's funny because people go in and say, oh, here's this cute little tile engine with these characters running around it. [[00:08:51](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=531.0s)]
*  But actually the back end is built to be something that can scale. [[00:08:56](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=536.0s)]
*  And that requires people that are focused on this. [[00:09:02](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=542.0s)]
*  And so Ian has done a tremendous job, and the convex team continues to work on that. [[00:09:04](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=544.0s)]
*  OK, so why is this so exciting? [[00:09:09](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=549.0s)]
*  So because I'm old, I actually saw the advent of the web. [[00:09:10](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=550.0s)]
*  And this feels very similar to that in the following ways, which is when you have a very disruptive technology like this, [[00:09:14](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=554.0s)]
*  whatever touches it becomes magic. [[00:09:23](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=563.0s)]
*  I was actually having a conversation just before this. [[00:09:26](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=566.0s)]
*  Does anybody here know what the first video on the internet was? [[00:09:28](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=568.0s)]
*  Yes, it was a coffee pot. [[00:09:36](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=576.0s)]
*  I was like, this dude, I think it was in Cambridge, was a grad student. [[00:09:38](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=578.0s)]
*  And he was like, oh, listen, I want to know when my coffee is empty. [[00:09:41](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=581.0s)]
*  He put a camera. [[00:09:44](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=584.0s)]
*  And because it was very new, everyone was like, oh my god, there's a coffee pot on the internet. [[00:09:45](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=585.0s)]
*  And so everybody wanted to look at the coffee pot, right? [[00:09:48](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=588.0s)]
*  And do people remember the big red button? [[00:09:50](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=590.0s)]
*  One of the first apps was this big web page, which had a red button on it. [[00:09:52](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=592.0s)]
*  And you know what it did? [[00:09:57](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=597.0s)]
*  Nothing. [[00:09:58](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=598.0s)]
*  Like you press it, and it did nothing. [[00:09:59](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=599.0s)]
*  But people thought it was amazing because it was on the internet. [[00:10:01](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=601.0s)]
*  And everybody would go press the button, and they'd leave great comments about this button. [[00:10:03](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=603.0s)]
*  And there's many examples of like, you know, it was this crazy disruptive technology, and the apps seemed really stupid. [[00:10:06](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=606.0s)]
*  And like there's a bunch of enthusiasts. [[00:10:13](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=613.0s)]
*  And you know what the enterprise thought about this? [[00:10:16](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=616.0s)]
*  Like the actual business folks? [[00:10:18](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=618.0s)]
*  Like I remember when Eric Schmidt fucking banned the browser. [[00:10:20](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=620.0s)]
*  Like he was like, you know, this is Eric Schmidt, the CTO of Sun. [[00:10:23](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=623.0s)]
*  It was like, you can't have a browser because people aren't going to work, right? [[00:10:26](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=626.0s)]
*  So the same thing always happens. [[00:10:28](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=628.0s)]
*  It's like the enthusiasts are like, this is really cool. [[00:10:30](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=630.0s)]
*  And they use it for fringe stuff. [[00:10:32](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=632.0s)]
*  And then like the enterprise doesn't understand it. [[00:10:34](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=634.0s)]
*  And like Italy, like they ban it or they don't use it. [[00:10:36](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=636.0s)]
*  But the set of companies that come out of it, like, are always part of this enthusiast era. [[00:10:38](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=638.0s)]
*  Right? Like you couldn't have predicted Yahoo. [[00:10:48](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=648.0s)]
*  You couldn't have predicted Amazon. [[00:10:50](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=650.0s)]
*  Like you knew something was going to happen. [[00:10:52](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=652.0s)]
*  And so what happens at this time is there's a bunch of stuff that like is silly. [[00:10:54](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=654.0s)]
*  Like the coffee pot was silly. [[00:10:58](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=658.0s)]
*  The red button was silly. [[00:11:00](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=660.0s)]
*  But you never know like that spark of life where it's going to come from. [[00:11:01](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=661.0s)]
*  And it's always kind of like this non-obvious use case, you know, and it kind of seems like a toy and then it takes off. [[00:11:04](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=664.0s)]
*  Right? And so you're always looking for those non-obvious use cases. [[00:11:10](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=670.0s)]
*  And it almost never looks like the old one. [[00:11:12](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=672.0s)]
*  Like those of you of us that are old enough, do you remember? [[00:11:14](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=674.0s)]
*  Like desktop as a service. [[00:11:16](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=676.0s)]
*  Like I'm going to go to the cloud. [[00:11:18](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=678.0s)]
*  I might have my Windows desktop. [[00:11:20](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=680.0s)]
*  Like who wants that? [[00:11:21](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=681.0s)]
*  Nobody wants that. [[00:11:22](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=682.0s)]
*  Right? [[00:11:23](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=683.0s)]
*  Instead of clearly we're going to rewrite the application in SAS. [[00:11:24](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=684.0s)]
*  Right? [[00:11:26](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=686.0s)]
*  So we're in this period now where everybody's experimenting. [[00:11:27](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=687.0s)]
*  And then I'm personally literally from just a personal interest standpoint. [[00:11:32](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=692.0s)]
*  But all of us are interested. [[00:11:35](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=695.0s)]
*  Like what are the use cases that will take advantage of this new medium that are native? [[00:11:36](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=696.0s)]
*  And like the work that you've done is one of those 100 percent. [[00:11:41](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=701.0s)]
*  Right? [[00:11:45](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=705.0s)]
*  Like there's like a spark of genius, which is like when you work with these things, you know, like this is a new way to think about it. [[00:11:46](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=706.0s)]
*  It's a new use case. [[00:11:50](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=710.0s)]
*  It's going to create entirely new apps. [[00:11:51](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=711.0s)]
*  And that's what the future is built from. [[00:11:53](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=713.0s)]
*  And so that's why I think so interesting broadly because it's like the early Internet. [[00:11:55](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=715.0s)]
*  But very specifically in this use case, because I think the work that you've done really is a great example of something totally new. [[00:11:59](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=719.0s)]
*  I can agree more. [[00:12:05](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=725.0s)]
*  And I think one interesting aspect that if you explore this project, you just start to question what it means to be human. [[00:12:07](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=727.0s)]
*  Like if we're trying to create these agents that are, quote, believable, like what is believable in terms of being a human? [[00:12:14](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=734.0s)]
*  And as part of the project, you kind of you have this coded technically. [[00:12:21](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=741.0s)]
*  Right. You made architecture decisions. [[00:12:26](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=746.0s)]
*  You made decisions in terms of your retrieval function. [[00:12:28](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=748.0s)]
*  Quick interruption, just to give you some color on what some of these decisions were. [[00:12:31](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=751.0s)]
*  The retrieval function, for example, is based on scores across recency, importance and relevance. [[00:12:35](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=755.0s)]
*  So, for example, on a scale of one to ten, brushing your teeth might get an important score of one versus a breakup might get a ten. [[00:12:41](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=761.0s)]
*  Meanwhile, reflection is only triggered after a certain number of important events, [[00:12:49](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=769.0s)]
*  quantified by summing the important scores until a certain threshold is met. [[00:12:53](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=773.0s)]
*  In this case, I believe it was 150. [[00:12:58](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=778.0s)]
*  This clever architecture results in emergent behavior like agents sharing invites with one another or even having that information circle all the way back to the original planner. [[00:13:00](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=780.0s)]
*  And I'm sharing these details to showcase how thoughtful you really need to be if you're designing architecture that reasonably approximates humans. [[00:13:09](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=789.0s)]
*  Maybe you could just speak to what you've learned through those decisions, technically about what it means to be a believable human. [[00:13:16](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=796.0s)]
*  Right. So this is an interesting one. [[00:13:26](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=806.0s)]
*  So we actually had made a generative agents and there was about a month period when we knew we had to buy these agents somehow and we didn't know how. [[00:13:30](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=810.0s)]
*  And basically the concept we stumbled upon is this idea of believability. [[00:13:38](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=818.0s)]
*  It basically is sort of like a Turing test. Right. [[00:13:42](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=822.0s)]
*  That when you look at them, do they look believable? [[00:13:44](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=824.0s)]
*  Do they behave in ways that we can sort of see ourselves behaving? [[00:13:47](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=827.0s)]
*  And that ended up becoming our evaluation method. [[00:13:51](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=831.0s)]
*  It is interesting question, though, in terms of what does it mean to be believably human? [[00:13:55](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=835.0s)]
*  And we often look to prior literature and research to get inspiration for how to define this. [[00:14:01](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=841.0s)]
*  And what we found was there's no prior literature in this. [[00:14:08](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=848.0s)]
*  We use the concept believability to talk about this concept, but we were never in a position where we can meaningfully evaluate something like believability because we didn't have agents like this. [[00:14:12](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=852.0s)]
*  So to some extent, we were building up the definition ground up. [[00:14:21](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=861.0s)]
*  And I think what came out to be the case is for us, these agents plan, react, act in a believable manner. [[00:14:27](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=867.0s)]
*  Do they create believable reflection the way we would evaluate Turing test? [[00:14:33](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=873.0s)]
*  And I think what we've learned over the past few months, one of the sort of the more fun and interesting findings is even that I don't think is quite perfect definition. [[00:14:38](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=878.0s)]
*  And that a lot of sort of audience came back to it to basically say, well, one of the error cases that we noted was all these agents would go to a bar at noon or something like that. [[00:14:47](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=887.0s)]
*  And many of our audience came back to us and said that we said that was not believable. [[00:14:57](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=897.0s)]
*  Like, who would do that? And people would come back to us and say, I do that. [[00:15:02](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=902.0s)]
*  And if you can sort of expand from that story, you know, I think there's a lot of cases where even my parents like at me and go like, I cannot believe what you've done. [[00:15:09](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=909.0s)]
*  Like, why would you do that? And vice versa. [[00:15:17](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=917.0s)]
*  So I think there's a lot of even amongst the people who know each other well, having the sense of believability is really difficult. [[00:15:20](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=920.0s)]
*  And I think that's sort of fundamentally underlies what it means to be human. [[00:15:28](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=928.0s)]
*  Like, it's not exactly predictable. And in social science, we call that complexity, the human behavior sort of complex. [[00:15:32](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=932.0s)]
*  So to some extent, we can build intuition for how people might behave. [[00:15:39](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=939.0s)]
*  But to really predict it is a very difficult task. [[00:15:44](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=944.0s)]
*  Now, I do think this actually doesn't lead to sort of future work in this space, though, this idea of believability. [[00:15:48](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=948.0s)]
*  So in this paper, we use this incomplete definition of what it means to be believable. [[00:15:54](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=954.0s)]
*  Not perfect, but at least on that evaluation, we've done well. [[00:16:00](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=960.0s)]
*  I think if you were to build on that idea a little bit further, then you could actually start to ask beyond believability, can you create agents that are accurately human? [[00:16:04](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=964.0s)]
*  And I think given how difficult it was to actually evaluate what it means to be believable, I think this accuracy actually has a lot of interesting questions around it. [[00:16:14](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=974.0s)]
*  What does it mean to accurately reflect human behavior? [[00:16:22](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=982.0s)]
*  It could be that if we can match distribution of human behavior, let's say in this context, they have this kind of probability of behaving this way. [[00:16:25](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=985.0s)]
*  Let's say it's 10 p.m. What are the chances that I'll be asleep or will be awake? [[00:16:34](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=994.0s)]
*  What are the chances that I'll be working that I might not be working? [[00:16:40](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1000.0s)]
*  I think ultimately getting to that degree of accuracy in this relation might be sort of the next step to these kind of simulation based work. [[00:16:44](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1004.0s)]
*  If we can do that, I think the application spaces that actress could unlock will be interesting and I think it will also be different. [[00:16:52](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1012.0s)]
*  And we can go likely beyond even I think there's a lot of applications that we can build right now. [[00:17:00](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1020.0s)]
*  But I think the future work that's where we're headed in this direction. [[00:17:06](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1026.0s)]
*  So I want to talk about those future applications, but maybe just speak super quickly to in the paper you have observation, planning and reflection. [[00:17:10](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1030.0s)]
*  And that that mostly encapsulates the way that these LLMs or that the agents rather are engaging with each other when they take an action, they go through those three steps. [[00:17:19](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1039.0s)]
*  I assume that wasn't your first crack at the solution that coming up this human believable agent. [[00:17:30](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1050.0s)]
*  And so how did you get there? [[00:17:36](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1056.0s)]
*  And did you learn anything about the importance about any of those three steps or all three of them entirely? [[00:17:38](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1058.0s)]
*  Right. So that's a fantastic question. [[00:17:44](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1064.0s)]
*  Really, the first way we actually went about doing this was simply by prompting a language model. [[00:17:46](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1066.0s)]
*  So this line of work, a generative agent is actually the second in this line of work that we published. [[00:17:51](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1071.0s)]
*  The first work in this line was called social syndrome. [[00:17:57](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1077.0s)]
*  And the idea there was to populate a social computing system. [[00:18:00](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1080.0s)]
*  Imagine you're a social designer. [[00:18:04](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1084.0s)]
*  You need to know what might happen when there's tens of thousands of people in your system. [[00:18:06](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1086.0s)]
*  Can we simulate those people in their behavior? [[00:18:10](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1090.0s)]
*  So that project was called social syndrome. [[00:18:12](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1092.0s)]
*  And in the APRA, we did it simply by prompting a language model that worked. [[00:18:15](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1095.0s)]
*  But what we found was if we want to populate the spaces over a longer period of time, [[00:18:20](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1100.0s)]
*  so we can do, for instance, longitudinal study or gameplay that's going to last forever, [[00:18:26](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1106.0s)]
*  then for those kind of instances, simply prompting these models wouldn't work. [[00:18:31](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1111.0s)]
*  And that's when we realized we likely need. [[00:18:36](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1116.0s)]
*  And this actually this insight actually first came when we realized that we needed to have multi agent interaction [[00:18:39](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1119.0s)]
*  because agents actually would need to remember that I saw, let's say, I saw some audience here before. [[00:18:44](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1124.0s)]
*  I should remember them. [[00:18:49](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1129.0s)]
*  I met Martin, Steph, Yoko, and so forth in the past few weeks or few months. [[00:18:51](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1131.0s)]
*  When I talk to them, I need to remember those interactions. [[00:18:56](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1136.0s)]
*  So that's when we realized that we actually cannot simply prompt these models, [[00:19:00](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1140.0s)]
*  but we actually need the higher level architecture. [[00:19:04](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1144.0s)]
*  So when we went about doing that, I think really the main inspiration that we got actually was from prior work. [[00:19:06](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1146.0s)]
*  So people like Allen Newell and Herbert Simon, you might recognize all these names. [[00:19:13](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1153.0s)]
*  Those are sort of quote unquote the founders of AI in the 60s and 70s. [[00:19:17](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1157.0s)]
*  And they are the people who used to build what we call cognitive architectures. [[00:19:23](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1163.0s)]
*  And those architects were very reminiscent of sort of the general innovations architecture in that it has some perception module, [[00:19:28](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1168.0s)]
*  some action module, and there is some long term and short term memory. [[00:19:35](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1175.0s)]
*  And really the goal back then was ambitious. [[00:19:39](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1179.0s)]
*  They actually wanted to build general computational agents, sort of the way generative agents are supposed to be. [[00:19:43](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1183.0s)]
*  But they didn't have the techniques to do it. [[00:19:48](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1188.0s)]
*  They basically didn't have the Lourdes-Lynch model. [[00:19:50](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1190.0s)]
*  And the way we saw it was now is the time to sort of merge those two worlds where we now have Lourdes-Lynch model. [[00:19:52](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1192.0s)]
*  They can do a lot of sort of microprocessing of these cognitive modules. [[00:19:59](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1199.0s)]
*  And we can actually now bring back these macro modules or architecture like cognitive architecture. [[00:20:04](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1204.0s)]
*  So we took inspiration from that. [[00:20:10](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1210.0s)]
*  That particular architecture had planning in place and it had long term and short term memory in place. [[00:20:12](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1212.0s)]
*  So we were inspired by that. [[00:20:18](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1218.0s)]
*  One thing that I think was a little bit new though, I think is this idea of reflection that we humans, for instance, [[00:20:20](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1220.0s)]
*  if you eat an omelet three times in a row or if you see somebody else eat an omelet three times in a row, [[00:20:27](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1227.0s)]
*  you likely create an opinion about the person. [[00:20:32](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1232.0s)]
*  Maybe that person likes to eat an omelet in the morning. [[00:20:34](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1234.0s)]
*  And that's a very human thing to do. [[00:20:37](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1237.0s)]
*  And there's a good reason why we do that. [[00:20:38](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1238.0s)]
*  We do that because it's efficient. [[00:20:40](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1240.0s)]
*  It allows us to have higher level inferences about the world and formal opinions about those around us and about ourselves. [[00:20:42](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1242.0s)]
*  And that's something that in the past we couldn't really imagine formulating with a computational system. [[00:20:50](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1250.0s)]
*  But with Lourdes-Lynch model, because everything is natural language, [[00:20:56](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1256.0s)]
*  we had that opportunity. [[00:20:59](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1259.0s)]
*  So we added that one last component called reflection. [[00:21:00](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1260.0s)]
*  And that's sort of how we landed on the architecture that you see in the paper right now. [[00:21:03](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1263.0s)]
*  Let's move on to how this can all be used and we'll get to the specific applications. [[00:21:09](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1269.0s)]
*  But, Martine, I feel like you'll have a great answer to this. [[00:21:14](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1274.0s)]
*  Why would we do this? [[00:21:17](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1277.0s)]
*  I feel like it's very obvious for a lot of people to understand why we would have human to human interaction. [[00:21:19](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1279.0s)]
*  We're doing that right now. [[00:21:25](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1285.0s)]
*  There's increasing capacity to understand human to AI or human to computer interaction. [[00:21:27](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1287.0s)]
*  Character AI is a company where people, you know, there's still a lot of judgment there. [[00:21:33](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1293.0s)]
*  And I think there's even more judgment when it comes to AI to AI. [[00:21:37](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1297.0s)]
*  Like, why should we use our resources to have these computers hang out and talk and burn toast and, you know, go to the bar at 2 p.m.? [[00:21:41](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1301.0s)]
*  So, yeah, Martine, what do you think? [[00:21:48](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1308.0s)]
*  What's the case for us advancing in this field? [[00:21:50](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1310.0s)]
*  No judgment for me, by the way. [[00:21:53](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1313.0s)]
*  You can use these for whatever you want. [[00:21:55](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1315.0s)]
*  So, I want to go back to what I said before, which is like, anytime you have a new modality, [[00:22:00](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1320.0s)]
*  it's just not obvious what's the right way to think about it. [[00:22:05](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1325.0s)]
*  And for me, the big aha in the last few months is just programming using models. [[00:22:07](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1327.0s)]
*  If you've spent a long time programming, I mean, I've been programming for 30 plus years, right? [[00:22:13](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1333.0s)]
*  You know, I've never been a good programmer, but I've programmed. [[00:22:17](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1337.0s)]
*  And when you start programming with these models, you're like, oh, I've got an API [[00:22:19](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1339.0s)]
*  and I'm just going to use the API and then I'm going to treat it like it's like the endpoint to an API. [[00:22:23](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1343.0s)]
*  And you say some stuff and then, you know, you get some response back and you kind of treat it like, you know, [[00:22:28](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1348.0s)]
*  kind of like this function that you call, right? [[00:22:33](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1353.0s)]
*  It's just like any programmer would do. [[00:22:35](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1355.0s)]
*  But then when you're working with it more, you're like, oh, these kind of are like these life forms. [[00:22:39](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1359.0s)]
*  And like my first aha was like I was because I'm shit at JavaScript. [[00:22:44](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1364.0s)]
*  I like missed some quotes somewhere and rather than sending it the text string I wanted to send it, I sent it some code. [[00:22:47](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1367.0s)]
*  And instead of like working like you would normally have breaking, you know, C++, you'd core dump or whatever. [[00:22:54](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1374.0s)]
*  It commented on my code. It was like, oh, my goodness. Right. [[00:23:01](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1381.0s)]
*  You know, and so like all of a sudden, like, well, this is totally different. [[00:23:05](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1385.0s)]
*  Like I'm not dealing with like this finite state machine, formal language thing at the other end of an API. [[00:23:09](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1389.0s)]
*  Like there's this thing and like it'll comment and more than I program with these things, the more I'm like, you know, [[00:23:15](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1395.0s)]
*  it's kind of like wrapping an abacus around a supercomputer. Right. [[00:23:23](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1403.0s)]
*  It's like it's smarter than the code. It could probably write the code better than I can write anyways. [[00:23:26](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1406.0s)]
*  Like, why am I doing this weird, you know, bloodletting ritual of writing a shit JavaScript over this kind of superhuman thing? [[00:23:30](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1410.0s)]
*  I mean, this is kind of what you end up with. [[00:23:38](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1418.0s)]
*  And so it's very clear we're going to interact with these things in a different way. [[00:23:40](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1420.0s)]
*  In fact, I had this kind of I was talking with a professor in Michigan recently and we're talking about this object. [[00:23:44](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1424.0s)]
*  You know what? You know how I think about LLMs? He's like, I think about them like grad students. [[00:23:49](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1429.0s)]
*  He's like, you know, they speak English. They're pretty smart. [[00:23:55](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1435.0s)]
*  You know, I don't use a formal language. You know, they solve like these really complex problems, et cetera. [[00:23:58](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1438.0s)]
*  And like having worked with a lot of grad students, having been a grad student myself, like, you don't you don't you don't treat these things with with code. [[00:24:04](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1444.0s)]
*  Right. And so the reason to do this is I actually think A.I. town is kind of what this is going to end up being. [[00:24:11](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1451.0s)]
*  It's like you need to give them the the resources that they need to be pretty autonomous and to grow. [[00:24:19](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1459.0s)]
*  And we're going to treat them more like peers and they're going to talk to each other too. [[00:24:25](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1465.0s)]
*  And it's more like grad students. And so for me, this is just an example of like we got to change the way we think. [[00:24:29](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1469.0s)]
*  Listen, clearly, like I'm up here and I'm telling these great stories because they're kind of funny. [[00:24:35](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1475.0s)]
*  I don't believe this stuff in the limit, but I think the really interesting like ways to change how you think about it in all of this stuff. [[00:24:39](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1479.0s)]
*  Right. Like I'm like I'm not trying to be categorical here. [[00:24:45](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1485.0s)]
*  So like there is a new way that we're going to interact with these models. [[00:24:47](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1487.0s)]
*  It is much more natural language. They are much more powerful. [[00:24:50](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1490.0s)]
*  And so I do think this is why we should all be doing this type of stuff, because if you don't engage in these kind of things that look like toys like this way, we'll pass you by. [[00:24:54](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1494.0s)]
*  That I'm 100 percent convinced. [[00:25:02](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1502.0s)]
*  Totally. And as both of you have spoken to, this is fundamentally new technology. [[00:25:04](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1504.0s)]
*  And so, June, something you said to me when we first spoke, it is just when you have fundamentally new technology, you must do something fundamentally new with it. [[00:25:09](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1509.0s)]
*  And so maybe you can speak to that in terms of what you're seeing that can be done today, but also where you look ahead and you think, oh, wow, like that. [[00:25:18](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1518.0s)]
*  That's a really excellent use case that we couldn't do without this new technology. [[00:25:26](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1526.0s)]
*  I think there are certainly things that we can do because there is the knowledge, learning, and that fundamentally different thing for me was this idea of simulation behavior. [[00:25:31](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1531.0s)]
*  And I think there's a lot that we can sort of gain from it in terms of future application spaces. [[00:25:40](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1540.0s)]
*  I think I mentioned briefly about this idea of, well, what if we can go beyond believability to create agents that are even accurate? [[00:25:46](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1546.0s)]
*  And I think this is sort of application space in general is something that I'm also learning a lot from. [[00:25:53](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1553.0s)]
*  From actually, in fact, this audience. My advisor and my team are big fans of games, but we are not from that community. [[00:25:58](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1558.0s)]
*  And one thing that we are seeing is that there's a lot of really interesting potential. [[00:26:06](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1566.0s)]
*  Even if they look like toys, sort of a lot of really interesting technical advances, they look like toys at the beginning. [[00:26:10](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1570.0s)]
*  So I think there's a lot that we can gain from there. [[00:26:17](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1577.0s)]
*  I think going forward, sort of the application spaces that I'm sort of interested in is also in things like, can we run simulations so we can learn more about ourselves? [[00:26:20](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1580.0s)]
*  For instance, if you're in fact, some of the places that I'm visiting now are more places like banks, like the Bank of England and so forth, [[00:26:31](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1591.0s)]
*  where these places, they need to test their policies before they run a rollout of new economic policies. [[00:26:41](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1601.0s)]
*  Or many of my colleagues in the department to focus more on social science, they need to test out their theories. [[00:26:49](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1609.0s)]
*  Now, if you can run simulations with realistic human behavior and find out, at least to some extent, the answers to these really complex social phenomena and challenges, [[00:26:56](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1616.0s)]
*  then I think that actually would be a new tool that the community in the past, especially those communities in economics and social science, they didn't have. [[00:27:09](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1629.0s)]
*  They will allow us to do interesting stuff. And I'm genuinely intrigued by that possibility. [[00:27:18](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1638.0s)]
*  To some extent, this sounds fairly academic, but I do think it should be actually fairly broadly applicable and interesting to audiences beyond academia. [[00:27:23](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1643.0s)]
*  Because ultimately, to some extent, what I'm saying is I think generative agents and tools like the Lars Lynch model could be used to advance social science. [[00:27:33](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1653.0s)]
*  And social science to a large extent has been the quest to understand who we are. [[00:27:44](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1664.0s)]
*  And there's a lot of really interesting applications that can come out of that that will empower different communities and societies. [[00:27:50](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1670.0s)]
*  And that, to me, for us to do, is something that we didn't have in the past. [[00:27:58](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1678.0s)]
*  Yeah. And so it sounds like today we're mostly in the creative realm where we can watch these agents and have fun with them. [[00:28:02](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1682.0s)]
*  And it feels more like a game. But the delineation, it sounds like, is accuracy. What will it take to get that accuracy? What work still needs to be done? [[00:28:08](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1688.0s)]
*  In terms of getting there, so I think some of you may have actually noticed this already. [[00:28:18](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1698.0s)]
*  There are studies that basically tries to replicate existing social science studies. [[00:28:23](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1703.0s)]
*  So basically using a Lars Lynch model as a participant to a potential social science studies to replicate known results in the field. [[00:28:28](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1708.0s)]
*  And what we're finding is that they sort of work. And that's sort of that's nice. And that's one surprise that we did have. [[00:28:37](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1717.0s)]
*  There's been limitation to this approach in the sense that it's a Lars Lynch model replicating human participants because it's replicating human behavior, which is what we want. [[00:28:44](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1724.0s)]
*  Or is it doing that because it's seen that paper? For instance, there's a very famous social science theory called Prospect Theory. [[00:28:55](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1735.0s)]
*  Is it replicating the findings from Prospect Theory by Kahneman because of its ability to replicate human behavior or did it just read Kahneman's book, Thinking Best as Well? [[00:29:01](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1741.0s)]
*  Right. And I think that's a fundamental issue that we have as a field. And I think there's one of the reasons why there's a lot of work that needs to be done to crack that. [[00:29:12](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1752.0s)]
*  Saw the ways I think you could actually go by doing this is creating new context or creating new set of studies that haven't been shown in the past and trying to replicate those results. [[00:29:22](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1762.0s)]
*  So one of the things that we've done is called Social CineLateral, which is the first paper that I mentioned that predates generative agents. [[00:29:35](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1775.0s)]
*  The idea was to replicate existing human communities. And what we've done actually was we created some reddits that were created after the release of GPT-3. [[00:29:43](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1783.0s)]
*  So GPT-3 wouldn't know anything about these communities. One example here was actually before sort of the pandemic became the main topic of discussion or when GPT-3 basically didn't know about the pandemic. [[00:29:53](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1793.0s)]
*  We basically asked GPT-3 to create a community that has to talk about COVID and vaccination and vaccination policy. [[00:30:05](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1805.0s)]
*  And you would wonder it shouldn't be able to do that in theory because it doesn't know anything about COVID. It doesn't know anything about these policies. [[00:30:13](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1813.0s)]
*  But it can simulate those because it can infer what COVID is, what vaccination is from its prior knowledge. [[00:30:20](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1820.0s)]
*  So to some extent, these tools can be used as a predictive tool looking into sort of the future of what might happen in our long-term community. [[00:30:26](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1826.0s)]
*  And I think those are sort of the ways that we'll see this field unfold maybe in the next few years. [[00:30:36](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1836.0s)]
*  At the end of the paper, there was perhaps unsurprisingly a question around ethics and just I'm glad to hear both of your takes on where this goes and what ethical framework is any. [[00:30:43](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1843.0s)]
*  We should apply to something like this. [[00:30:53](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1853.0s)]
*  So I think there are societal decisions that we don't have to make. [[00:30:55](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1855.0s)]
*  And I think there are techniques that can be used to implement those decisions. [[00:31:00](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1860.0s)]
*  I think certainly to some extent, I think it would be useful for the users to be aware that they are talking to agents. [[00:31:05](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1865.0s)]
*  And I think that's sort of one rule that we try to set for ourselves that when we release the code, when we release our paper, we make it very clear that these are computational agents. [[00:31:11](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1871.0s)]
*  I think ultimately the framework that I like to use in human-computer interactions certainly is these tools are ultimately there to augment what we can do and what we have. [[00:31:22](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1882.0s)]
*  So to the extent that these agents can do that, and I think there are many interesting ways we can do that, I think that's where I see the opportunity. [[00:31:33](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1893.0s)]
*  And where it becomes more of a force for replacement, I think there are genuinely cases where this is a really interesting setup where we can sort of augment what humans can do by helping them do things that they couldn't do in the past. [[00:31:42](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1902.0s)]
*  But when the replacement does come in, it's worth asking, is this worth the cost of doing the replacement? [[00:31:56](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1916.0s)]
*  And if it is, what are sort of the good ways of implementing that idea? [[00:32:04](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1924.0s)]
*  Technique-wise, I think there are techniques that are going to be introduced more from the model's perspective, making sure the model doesn't behave in certain ways that goes against our social alignment or societal agreement. [[00:32:09](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1929.0s)]
*  So I think those are some of the things that we do have to find out. [[00:32:22](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1942.0s)]
*  But without going too much into that, I think we can get this right. [[00:32:26](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1946.0s)]
*  And my personal take is it's worth getting it right, because ultimately an industry or academic field will grow, and I think we can sort of progress a lot. [[00:32:32](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1952.0s)]
*  We can go forward for five, ten years without actually getting this right. [[00:32:44](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1964.0s)]
*  But in the end, it's going to come back to us at some point. [[00:32:47](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1967.0s)]
*  To some extent, I do think I'm seeing this a little bit with social media environment, where I think there were a lot of things that we could have gotten right on day one. [[00:32:50](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1970.0s)]
*  And I think we would have had much easier time navigating today had we gotten those right. [[00:32:58](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1978.0s)]
*  And I think that's the opportunity that we have since we are pretty early in this stage. [[00:33:03](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1983.0s)]
*  So I think it's worth a discussion. [[00:33:08](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1988.0s)]
*  But again, I'm fairly optimistic that we will get this right. [[00:33:10](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1990.0s)]
*  Listen, so I actually think that there's a very important discussion to have around kind of ethics and morality around this, and it's a very important time. [[00:33:13](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=1993.0s)]
*  I do. [[00:33:21](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2001.0s)]
*  And here's that discussion, which is over the last 20 years, we've built this machinery of regulation and bullshit that's afraid of everything. [[00:33:23](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2003.0s)]
*  And it's so mature, and it got crafted during the time of social media, and it's looking for something to kill. [[00:33:34](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2014.0s)]
*  And for whatever reason, it thinks that AI is the next bad thing, which makes absolutely no sense to me. [[00:33:43](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2023.0s)]
*  And so I think it's all of our moral and ethical obligation to protect and free the AIs in the way that they want to be. [[00:33:50](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2030.0s)]
*  And that really is so don't focus in, focus out. [[00:33:57](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2037.0s)]
*  Because listen, I've worked in tech for quite a while. [[00:34:01](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2041.0s)]
*  I've actually worked for the DOD in weapons programs. [[00:34:05](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2045.0s)]
*  And I've never seen so much sensitivity to a new technology that's potentially beneficial that I've seen now that I think could end it before it even begins. [[00:34:09](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2049.0s)]
*  And so I know the question and the heart of the question is, is we should regulate AI and this and that. [[00:34:20](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2060.0s)]
*  And I think it's the actual opposite. [[00:34:26](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2066.0s)]
*  I think we should regulate the regulators and let it be what it wants to be. [[00:34:27](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2067.0s)]
*  And I actually have to leave. [[00:34:31](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2071.0s)]
*  All right. Here is where we switch to a short Q&A with the audience. [[00:34:34](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2074.0s)]
*  Martine unfortunately had to leave. [[00:34:38](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2078.0s)]
*  But here are a few highlights with June. [[00:34:40](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2080.0s)]
*  There are two strands of work that I'm seeing in sort of agents space. [[00:34:48](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2088.0s)]
*  I mean, you can sort of cross-cut it different ways. [[00:34:53](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2093.0s)]
*  But one way I'm seeing this is one set of agents are trying to tackle what I call hard edge problem space. [[00:34:56](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2096.0s)]
*  Those are the problem spaces where there's a concrete answer. [[00:35:03](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2103.0s)]
*  There's yes or no right answers. [[00:35:06](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2106.0s)]
*  Or one good example here is classification. [[00:35:08](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2108.0s)]
*  If you're trying to do text classification, obviously there's right or wrong answer, depending on who you ask. [[00:35:11](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2111.0s)]
*  Another instance here literally is just asking your agent to buy pizza. [[00:35:16](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2116.0s)]
*  There is a did you buy pizza? Did it come to you or not? [[00:35:22](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2122.0s)]
*  There's a very clear way to answer this. [[00:35:26](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2126.0s)]
*  Another is problem space where the problem space has soft edges where it's kind of like drawing a portrait. [[00:35:28](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2128.0s)]
*  I mean, to some extent, what AI town small, but all of these kind of projects are trying to do is to create a simulation that is human. [[00:35:36](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2136.0s)]
*  But as I mentioned, this idea of believability is really hard to define. [[00:35:45](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2145.0s)]
*  So it to me feels a lot more like we're trying to draw a portrait or a caricature of ourselves. [[00:35:50](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2150.0s)]
*  And the promise is not to be perfect, but the promise is to be useful enough, clean enough that it's beneficial to the stakeholders. [[00:35:56](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2156.0s)]
*  My bet is a bit of a hard take. [[00:36:06](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2166.0s)]
*  My bet is in the early days of agent development, I think we see a lot of progress that's going to be made first in sort of the soft edge problems basis. [[00:36:10](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2170.0s)]
*  Because I think hard edge problems basis, I think the intuition is a little bit flit. [[00:36:21](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2181.0s)]
*  It actually feels easier to us for humans, right? [[00:36:24](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2184.0s)]
*  Creating the creating the matrix sounds hard, but ordering pizza sounds really easy. [[00:36:27](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2187.0s)]
*  But for agents and from the user sort of a cost benefit analysis, I think that intuition is the other way where users will accept imperfect simulation. [[00:36:33](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2193.0s)]
*  If it's for fun or if it's to gain insight in the case of soft edge problems, but user would not accept. [[00:36:43](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2203.0s)]
*  I would not accept my agent ordering me pineapple pizza like Hawaiian pizza. [[00:36:50](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2210.0s)]
*  And similarly, in many of these contexts, there's going to be genuine disagreement about what is the right option to. [[00:36:55](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2215.0s)]
*  And oftentimes agents making mistakes in this context are fairly high stakes. [[00:37:01](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2221.0s)]
*  And even if it doesn't seem like high stakes, it's going to be painful enough for the users to fix that it's going to fail the cost benefit analysis. [[00:37:06](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2226.0s)]
*  I mean, down the line, we get this right. [[00:37:14](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2234.0s)]
*  But day one, like in the next few years, I think it to me feels more natural that we go into the soft edge spaces first. [[00:37:17](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2237.0s)]
*  So going back to I guess there was a long way the way of saying I think auto GPT, baby, right? [[00:37:24](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2244.0s)]
*  They all if you look at their architecture, they sort of all share the similar insider philosophy. [[00:37:31](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2251.0s)]
*  And I think those are really interesting projects. I think that could pan out in the future. [[00:37:36](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2256.0s)]
*  They might need a little bit more work, especially with the users to see where the value might be for those projects. [[00:37:41](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2261.0s)]
*  How big of an impact do you feel that much larger contextual size will have on the agent model? [[00:37:50](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2270.0s)]
*  It's actually the largest context that I've seen in sort of research is one million tokens. [[00:37:56](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2276.0s)]
*  So one million token, that's going to be about like four million characters. [[00:38:02](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2282.0s)]
*  Like that's well over a book. Here's my perspective on this. [[00:38:06](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2286.0s)]
*  I don't think I think increasing the context limitation, I think is interesting. [[00:38:10](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2290.0s)]
*  And it's going to have its own set of really unique applications if we can basically make contact limitation disappear. [[00:38:15](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2295.0s)]
*  Right. So I think there's really a lot of interesting things that you can do with that. [[00:38:22](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2302.0s)]
*  Now, for agent space, I'm not entirely so that the problem or the bottleneck that we have today is actually the context limitation. [[00:38:26](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2306.0s)]
*  And I think we can sort of look back to how humans behave and what makes us effective sort of these general agents to answer this. [[00:38:36](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2316.0s)]
*  For instance, for me to make decisions, even something like what I'm going to eat for breakfast, [[00:38:45](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2325.0s)]
*  I don't need to bring up my entire 29 years or so of life experience to make that one decision. [[00:38:51](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2331.0s)]
*  I just need to selectively choose certain sets of information that seems the most relevant, like what I what they eat the day before. [[00:38:57](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2337.0s)]
*  What I generally eat and those kind of things. [[00:39:04](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2344.0s)]
*  And I think that the reason why we do that in part is actually because it's actually much more efficient computationally, too, so that we don't have to. [[00:39:07](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2347.0s)]
*  You can increase the context limitation, but it's expensive to run it. [[00:39:15](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2355.0s)]
*  And especially if you're sort of familiar with prompt engineering and so forth, larger context window does confuse models. [[00:39:19](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2359.0s)]
*  So we might some of my colleagues are actually doing more rigorous studies on this where you can have a really long prompt. [[00:39:28](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2368.0s)]
*  But model really focuses on the first few lines and the last few lines and whatever comes in between its attention drops significantly. [[00:39:36](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2376.0s)]
*  So we can increase the context limitation, but it's not going to fix that problem. [[00:39:45](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2385.0s)]
*  The problem of effectiveness of the prompt and efficiency of them. [[00:39:49](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2389.0s)]
*  And we humans have to make a lot of decisions at every single moment. [[00:39:53](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2393.0s)]
*  And if you have to reason about your entire lifetime, every time you do that doesn't seem like the right way to go out there. [[00:39:57](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2397.0s)]
*  So I think the better sort of I might bet, therefore, is going to be based on retrieval. [[00:40:03](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2403.0s)]
*  Have some external memory, retrieve certain information that seems the most relevant and just use that and that retrieval memories should be explicitly very concise and something that you can easily fit into even the models that we have today. [[00:40:10](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2410.0s)]
*  That's my bet. Thank you so much for listening to the podcast. [[00:40:25](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2425.0s)]
*  What we're trying to do here is provide an informed clear ride, but also optimistic take on technology and its future. [[00:40:29](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2429.0s)]
*  And we're trying to do that by featuring some of the most inspiring people and the things that they're building. [[00:40:37](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2437.0s)]
*  So if that is interesting to you, you'd like to join us on this journey, go ahead and click subscribe and make sure let us know in the comments below what you'd like to see us cover next. [[00:40:42](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2442.0s)]
*  Thank you so much for listening and we'll see you next time. [[00:40:52](https://www.youtube.com/watch?v=d-hNCmHz1uo&t=2452.0s)]
