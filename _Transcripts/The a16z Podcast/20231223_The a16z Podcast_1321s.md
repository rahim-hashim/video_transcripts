---
Date Generated: May 23, 2025
Transcription Model: whisper medium 20231117
Length: 1321s
Video Keywords: []
Video Views: 2331
Video Rating: None
Video Description: Anjney Midha, General Partner at a16z, believes that mechanistic interpretability (a fancy term for "reverse engineering" AI models) will take center stage in 2024.

In this discussion, we move beyond the black box and explore pivotal questions: Why do AI models make specific statements? What influences the success of certain prompts? Most crucially, how can we control these models in real-world scenarios?

Topics Covered: 
00:00 - Big Ideas in Tech 2024
01:39: AI Interpretability: From Black Box to Clear Box
02:21: What do we and donâ€™t understand about LLM black boxes and interpretability
04:23 - Research in interpretability
06:43 - Features represented in the outputs from LLMs 
08:16 - Unlocks in interpretability
11:49 - The engineering challenges 
14:10 - Scaling mechanistic interpretability research
17:27 - A new focus on explainability

Resources: 
View all 40+ big ideas: https://a16z.com/bigideas2024
Find Anish on Twitter: https://twitter.com/anjneymidha

Stay Updated: 
Find a16z on Twitter: https://twitter.com/a16z 
Find a16z on LinkedIn: https://www.linkedin.com/company/a16z 
Subscribe on your favorite podcast app: https://a16z.simplecast.com/ 
Follow our host: https://twitter.com/stephsmithio 

Please note that the content here is for informational purposes only; should NOT be taken as legal, business, tax, or investment advice or be used to evaluate any investment or security; and is not directed at any investors or potential investors in any a16z fund. a16z and its affiliates may maintain investments in the companies discussed. For more details please see a16z.com/disclosures.
---

# Big Ideas 2024: AI Interpretability: From Black Box to Clear Box with Anjney Midha
**The a16z Podcast:** [December 23, 2023](https://www.youtube.com/watch?v=yTZVcOmhmlw)
*  Precision delivery of medicine. Entertainment franchise games absolutely exploding. Small [[00:00:00](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=0.0s)]
*  modular reactors and the nuclear renaissance. Plus, AI moving into very complex workflows. [[00:00:06](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=6.4s)]
*  Now these were just a few of the major tech innovations that partners at A16Z predicted [[00:00:13](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=13.44s)]
*  last year. And our partners are back. We just dropped our list of over 40 plus big ideas for [[00:00:18](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=18.88s)]
*  2024. A compilation of critical advancements across all our verticals. From smart energy [[00:00:25](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=25.439999999999998s)]
*  grids to crime detecting computer vision to democratizing miracle drugs like GLP-1s [[00:00:31](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=31.04s)]
*  or even AI moving from black box to clear box. You can find the full list of 40 plus [[00:00:36](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=36.56s)]
*  builder-worthy pursuits at a16z.com slash big ideas 2024. Or you can click the link in our [[00:00:43](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=43.519999999999996s)]
*  description below. But on deck today, you will hear directly from one of our partners as we dive [[00:00:51](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=51.2s)]
*  even more deeply into their big idea. What's the why now? What opportunities and what challenges [[00:00:57](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=57.52s)]
*  are on the horizon? And how can you get involved? Let's dive in. As a reminder, the content here is [[00:01:02](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=62.96s)]
*  for informational purposes only. Should not be taken as legal, business, tax or investment advice [[00:01:10](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=70.56s)]
*  or be used to evaluate any investment or security and is not directed at any investors or potential [[00:01:15](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=75.92s)]
*  investors in any A16Z fund. Please note that A16Z and its affiliates may also maintain investments [[00:01:20](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=80.72s)]
*  in the companies discussed in this podcast. For more details, including a link to our investments, [[00:01:27](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=87.28s)]
*  please see a16z.com slash disclosures. My name is Anjane Mitta. I'm a general partner here at A16Z [[00:01:32](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=92.56s)]
*  and I'm talking to you today about AI interpretability, which is just a complex [[00:01:43](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=103.2s)]
*  way of saying reverse engineering AI models. Over the last few years, AI has been dominated [[00:01:48](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=108.56s)]
*  by scaling, which is a quest to see what was possible if you threw a ton of compute and data [[00:01:54](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=114.64s)]
*  at training these large models. But now, as these models begin to be deployed in real world [[00:02:00](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=120.0s)]
*  situations, the big question on everyone's mind is why? Why do these models say the things they do? [[00:02:05](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=125.92s)]
*  Why do some prompts produce better results than others? And perhaps most importantly, [[00:02:13](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=133.6s)]
*  how do we control them? Anjane, I feel like most people don't need convincing that this is a [[00:02:19](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=139.2s)]
*  worthwhile endeavor for us to understand these models a little better, but maybe you could share [[00:02:25](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=145.2s)]
*  where we're at in that journey. What do we or don't we understand about these LLM black boxes [[00:02:30](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=150.07999999999998s)]
*  and their interpretability? It might help to reason by analogy here. If you pretend one of these AI [[00:02:35](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=155.92s)]
*  models is like a big kitchen with hundreds of cooks, and when you ask the kitchen to make [[00:02:42](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=162.08s)]
*  something, each cook knows how to make certain foods. And when you give the kitchen ingredients [[00:02:50](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=170.08s)]
*  and you say, take a cook a meal, all the different cooks debate about what to make. And eventually, [[00:02:56](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=176.88000000000002s)]
*  they come to an agreement on a meal to prepare based on these ingredients. Now, the problem is [[00:03:01](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=181.60000000000002s)]
*  where we are in the industry right now is that from the outside, we can't really see what's [[00:03:06](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=186.8s)]
*  happening in these kitchens. So you have no idea how they made that decision on the meal. [[00:03:11](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=191.68s)]
*  You just get the cake or the taco or whatever it might be. [[00:03:16](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=196.96s)]
*  Right. So if you ask the kitchen, hey, why did you choose to make lasagna? It's really hard to get a [[00:03:21](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=201.68s)]
*  straight answer because the individual cooks don't actually represent a clear concept like a dish or [[00:03:27](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=207.44s)]
*  cuisine. And so the big idea here is what if you could train a team of head chefs to oversee [[00:03:33](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=213.36s)]
*  these groups of cooks and each head chef would specialize in one cuisine. So you'd have the [[00:03:39](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=219.68s)]
*  Italian head chef who controls all the pasta and pizza cooks. And then you have the baking head [[00:03:43](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=223.36s)]
*  chef in charge of cakes and pies. And now when you ask why lasagna, the Italian head chef raises [[00:03:47](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=227.44s)]
*  his hand and says, I instructed the cooks to make a hearty Italian meal. And these head chefs [[00:03:52](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=232.48000000000002s)]
*  represent clear, interpretable concepts inside the neural network. And so this breakthrough is [[00:03:56](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=236.88s)]
*  finally understanding all the cooks in that messy kitchen by training these head chefs to organize [[00:04:03](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=243.92s)]
*  them into tidy sort of cuisine categories. And we can't control every individual cook, but now we [[00:04:09](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=249.28s)]
*  can get insights into the bigger, more meaningful decisions that determine what meal the AI chooses [[00:04:15](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=255.11999999999998s)]
*  to make. Does that make sense? It does. But are you saying that we do actually have a sense now [[00:04:20](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=260.4s)]
*  of those like head chefs or the people responsible for parts of what might be happening within the AI? [[00:04:26](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=266.88s)]
*  Obviously, it's not people in this case. But have we actually unlocked some of that information [[00:04:32](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=272.8s)]
*  with some of the new releases or new papers that have come out? We have we have and you can kind [[00:04:37](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=277.6s)]
*  of you can you can break the world of interpretability down into a pre 2023 and a post 2023 world in my [[00:04:42](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=282.24s)]
*  opinion, because there's been such a massive breakthrough in that specific domain of understanding [[00:04:48](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=288.56s)]
*  which cooks doing what, you know, more specifically, what's happening is that these these [[00:04:54](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=294.8s)]
*  models are made up of neurons, right? A neuron refers to an individual node in the neural network. [[00:05:02](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=302.08000000000004s)]
*  And it's just a single computational unit. And historically, the industry sort of tried to analyze [[00:05:07](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=307.6s)]
*  and interpret and explain these models by by trying to understand what each neuron was doing, [[00:05:14](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=314.72s)]
*  what each cook was doing in that in that situation of each on the other hand, which is this new [[00:05:19](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=319.12s)]
*  atomic unit that the industry is proposing now as an alternative to neuron refers to a specific [[00:05:26](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=326.0s)]
*  pattern of activations across multiple neurons. And so while a single neuron might activate in [[00:05:31](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=331.76s)]
*  all kinds of unrelated contexts, like whether you're asking for lasagna, or you're asking for a [[00:05:36](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=336.96s)]
*  pastry, a feature which is this new atomic unit represents a specific concept that consistently [[00:05:41](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=341.28s)]
*  activates a particular set of neurons. And so to explain the difference using the cooking analogy, [[00:05:46](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=346.96s)]
*  a neuron is like an individual cook in the kitchen, each one knows how to make certain dishes, [[00:05:53](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=353.52s)]
*  but doesn't represent a clear concept. A feature would be like a cuisine specialty [[00:05:57](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=357.52s)]
*  controlled by a head chef. So for example, the Italian cuisine feature is active whenever the [[00:06:02](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=362.4s)]
*  Italian head chef and all the cooks they oversee are working on an Italian dish. And that feature [[00:06:07](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=367.2s)]
*  has a consistent interpretive interpretation, which in this case is Italian food, while individual [[00:06:12](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=372.47999999999996s)]
*  cooks do not. And so in summary, these neurons are individual computational units that don't map [[00:06:18](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=378.32s)]
*  neatly to concepts. These features are patterns of activations across multiple neurons that do [[00:06:24](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=384.32s)]
*  represent clear interpretable concepts. And so the breakthrough here was that now we've learned how [[00:06:29](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=389.04s)]
*  to decompose a neural network into these interpretable features when previous approaches [[00:06:34](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=394.32s)]
*  focused on interpreting single neurons. And so the short answer is yes, we have a massive [[00:06:39](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=399.36s)]
*  breakthrough where we actually now know how to trace what was happening in the kitchen. [[00:06:44](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=404.08s)]
*  And maybe could you give an example that's specific to these LLMs when we're talking about [[00:06:48](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=408.8s)]
*  a feature? I know there's still so much research to be done, but like what's an example of a feature [[00:06:52](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=412.64s)]
*  that you actually see represented in the outputs from an LLM? Yeah, that's a great question. So I [[00:06:58](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=418.16s)]
*  think if you actually look at the paper that moved the industry forward a bunch earlier this year, [[00:07:03](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=423.92s)]
*  there's a paper called Decomposing Language Models with Dictionary Learning. This came out of [[00:07:10](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=430.24s)]
*  Anthropic. Interpretability is a large field, but this paper, I think, took a specific approach [[00:07:15](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=435.68s)]
*  called Mechanistic Interpretability. And the paper has a number of examples of features that [[00:07:21](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=441.52s)]
*  they discovered in a very small, almost toy-like model because smaller models proved to be very [[00:07:28](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=448.96000000000004s)]
*  useful petri dishes for these experiments. And I think an example of one of these features was [[00:07:35](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=455.36s)]
*  a god feature where when you talk to the model about religious concepts, then a specific god [[00:07:41](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=461.28000000000003s)]
*  feature fired over and over again. And they found when they talked to the model about a different [[00:07:48](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=468.64s)]
*  type of concept like biology or DNA, a different feature that was unrelated to the god feature [[00:07:54](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=474.48s)]
*  fired, whereas the same neurons were firing for both those concepts. And so there was a [[00:08:01](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=481.84000000000003s)]
*  feature level analysis allowed them to decompose and break apart the idea or the concept of religion [[00:08:11](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=491.35999999999996s)]
*  from biology, which is something that wasn't possible to tease apart in the neuron world. [[00:08:18](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=498.0s)]
*  Yeah. Yeah. And maybe you could speak a little bit more to why this is helpful. I mean, maybe [[00:08:24](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=504.32s)]
*  it's obvious for folks listening, but now that we have these concepts that we see and maybe can also [[00:08:29](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=509.76s)]
*  link pretty intuitively like, oh, okay, I understand biology. Oh, I understand religion [[00:08:35](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=515.36s)]
*  as a concept that's coming out of these LLMs. Now that we understand these linkages a little more, [[00:08:39](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=519.44s)]
*  what does that mean? Like, why does this now open things up? What are we in a new environment? You [[00:08:46](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=526.4s)]
*  kind of said pre some of these unlocks and now we're post. What does post look like? [[00:08:51](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=531.6s)]
*  Yeah, this is a great question. So I think there's three big things that are sort of so what's from [[00:08:57](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=537.84s)]
*  this breakthrough. The first is that that interpretability is now an engineering problem [[00:09:03](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=543.92s)]
*  as opposed to an open-ended research problem. And that's a huge sort of sea change for the industry [[00:09:11](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=551.04s)]
*  because up until now there were a number of hypotheses on how to interpret how these models [[00:09:16](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=556.64s)]
*  were behaving and explain why, but it wasn't quite concrete. It wasn't quite understood [[00:09:21](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=561.92s)]
*  which one of those approaches would work best to actually explain how these models work at very [[00:09:27](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=567.5999999999999s)]
*  large scale at frontier model scale. But I think this approach, this mechanistic interpretability [[00:09:32](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=572.32s)]
*  approach and this paper that came out earlier this year shows that actually the relationships [[00:09:37](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=577.2s)]
*  are so easily observable at a small scale that the bulk of the challenge now is to scale up this [[00:09:42](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=582.0s)]
*  approach, which is an engineering challenge. And I think that's massive because the engineering is [[00:09:48](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=588.5600000000001s)]
*  largely a function of the resources and the investment that goes into scaling these models, [[00:09:52](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=592.08s)]
*  whereas research can be fairly open-ended. And so I think one big conclusion or takeaway from [[00:09:57](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=597.6s)]
*  2023 is that interpretability is gone from being a research area to being an engineering area. [[00:10:05](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=605.52s)]
*  I think the second is that if we actually can get this approach to work at scale, then we can control [[00:10:11](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=611.2s)]
*  these models in the same way that if you understood how a kitchen made a dish and you wanted a [[00:10:19](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=619.12s)]
*  different outcome, now you can go to the Italian chef and say, could you please make that change [[00:10:24](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=624.0s)]
*  next time around? And so that allows controllability. And that's really important [[00:10:28](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=628.8s)]
*  because as these models get deployed in really important sort of mission critical situations like [[00:10:33](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=633.6s)]
*  healthcare and finance and in defense applications, you need to be able to control [[00:10:39](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=639.28s)]
*  these models very, very precisely, which unfortunately today just isn't the case. [[00:10:45](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=645.6s)]
*  We have very blunt tools to control these models, but nothing precise enough for those mission [[00:10:48](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=648.64s)]
*  critical situations. So I think controllability is a big piece that this unlocks. And the third is [[00:10:52](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=652.32s)]
*  sort of a byproduct of having controllability, which is once you can control these models, [[00:10:59](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=659.2s)]
*  you can rely on them more. And I think that's a huge increase. Reliability means not only good [[00:11:08](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=668.72s)]
*  things for the customers and the users and developers using these models, but also from a [[00:11:15](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=675.84s)]
*  policy and regulatory perspective, we can now have a very concrete grounded debate about [[00:11:20](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=680.7199999999999s)]
*  what models are safe and not, how to govern them, how to make sure that the space develops in a [[00:11:27](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=687.52s)]
*  concrete empirically grounded way, as opposed to reasoning about these models in the abstract [[00:11:34](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=694.3199999999999s)]
*  without a lot of evidence. I think one of the problems we've had as an industry is that because [[00:11:40](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=700.64s)]
*  there hasn't been a concrete way to show or demonstrate that we understand these black boxes [[00:11:45](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=705.76s)]
*  and how they work, that a lot of the policy work around and policy thinking is sort of worst case [[00:11:50](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=710.08s)]
*  analysis. And worst case analysis can be fairly open to fear mongering and a ton of FUD. And I [[00:11:55](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=715.2s)]
*  think instead now we have an empirical basis to say, here are the real risks of these models and [[00:12:02](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=722.3199999999999s)]
*  here's how policy should address them. And I think that's a big improvement or big advance as well [[00:12:06](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=726.4s)]
*  for us all. Totally. I mean, it's huge and it's kind of interesting because we don't know every [[00:12:11](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=731.52s)]
*  little piece of physics, but we're able to deploy that in extremely effective ways and build [[00:12:18](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=738.16s)]
*  all of the things around us through that understanding that has grown over time. [[00:12:24](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=744.48s)]
*  And so it's really exciting that these early building blocks are getting in place. [[00:12:28](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=748.56s)]
*  Maybe you can just speak to that engineering challenge or the flippening that you said [[00:12:32](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=752.16s)]
*  happened where we previously had a research challenge, which was somewhat TBD. When is this [[00:12:36](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=756.56s)]
*  going to be unlocked? How is it going to be unlocked? And now we have, again, those early [[00:12:42](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=762.88s)]
*  building blocks where we're now talking about scale. And I'll just read out a quick tweet [[00:12:46](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=766.7199999999999s)]
*  from Chris, who I believe is on the Anthropic team. And he said, if you asked me a year ago, [[00:12:52](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=772.7199999999999s)]
*  superposition would have been by far the reason I was most worried that mechanistic interpretability [[00:12:57](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=777.3599999999999s)]
*  would hit a dead end. I'm now very optimistic. I go as far as saying it's now primarily an [[00:13:02](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=782.8s)]
*  engineering problem, hard, but less fundamental risks. I think it captures what you were just [[00:13:09](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=789.12s)]
*  mentioning, but maybe you can speak a little bit more to the papers and the size of, or the scope [[00:13:14](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=794.3199999999999s)]
*  that they've done this feature analysis within and what the steps would be to do this when we're [[00:13:20](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=800.24s)]
*  talking about those much, much larger foundational models. Yeah, that's a great question. So I think [[00:13:27](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=807.5999999999999s)]
*  stepping back the way science in this space is done often is, you start with a small, almost [[00:13:33](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=813.76s)]
*  toy-like model of your problem, see if some solution is promising, and then you decide to scale it up [[00:13:42](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=822.0799999999999s)]
*  to a bigger and bigger level. Because if you can't get it to work at a really small scale, [[00:13:49](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=829.3599999999999s)]
*  rarely do these systems work at large scale. And so I think, [[00:13:55](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=835.28s)]
*  while of course the Holy Grail challenge with interpretability is explaining how frontier models [[00:14:00](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=840.08s)]
*  that are the GPT-4s and CLAWD-2s and BARDS of the world, which are several hundred billion [[00:14:08](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=848.8000000000001s)]
*  parameter in their scale, I think that one of the challenges with trying to attack interpretability [[00:14:14](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=854.8800000000001s)]
*  of those models directly is that they're so large and such complex systems, it is very, [[00:14:24](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=864.24s)]
*  very untractable to try to tease apart all the different neurons in these models at that scale. [[00:14:29](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=869.04s)]
*  Now I should be clear, it's not easy. And there are a ton of unsolved problems in the scaling [[00:14:36](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=876.24s)]
*  part of this journey as well. Yeah, if I could just interrupt real quick, [[00:14:43](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=883.76s)]
*  I mean, you mentioned the scaling laws, and those have continued to scale, but we didn't [[00:14:48](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=888.88s)]
*  necessarily know if that would be the case. It has of course proven to be the case as we move [[00:14:53](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=893.04s)]
*  forward. But what are the challenges that you see that might be outstanding as we look to scale up [[00:14:58](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=898.0799999999999s)]
*  some of this mechanistic interpretability research? What open challenges do you see on that path? [[00:15:03](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=903.5999999999999s)]
*  Okay, so yes, so to borrow our analogy earlier of the kitchen, I think we as an industry now have [[00:15:10](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=910.56s)]
*  a model of what's going on and some proof of what's going on with these features with a kitchen [[00:15:18](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=918.8s)]
*  which has, let's say, three or four chefs. And so to figure out if this would work at frontier scale, [[00:15:27](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=927.12s)]
*  where you have thousands and thousands of chefs in each kitchen, and in the case of a model, [[00:15:32](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=932.3199999999999s)]
*  you have billions of parameters, I think there are two big open problems that need to be solved in [[00:15:36](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=936.88s)]
*  order for this approach to work at scale. The first is increasing the autoencoder, which is [[00:15:44](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=944.56s)]
*  conceptually you can kind of think about as the model that makes sense of what's going on [[00:15:52](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=952.3199999999999s)]
*  with each feature. And the autoencoder here is pretty small in the paper that came out in October. [[00:15:58](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=958.0s)]
*  And so I think there's a big challenge where the researchers in the space have to figure out how to [[00:16:04](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=964.8s)]
*  scale up the autoencoder in the order of magnitude of almost 100x expansion factor. And so that's a [[00:16:10](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=970.32s)]
*  lot. And that's pretty difficult because training the underlying, the base model itself often requires [[00:16:17](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=977.36s)]
*  hundreds and often billions of dollars worth of compute. And so I do think it's a fairly difficult [[00:16:27](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=987.12s)]
*  and compute intensive challenge to scale the autoencoder. Now, I think there's a ton of promising [[00:16:34](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=994.4000000000001s)]
*  approaches on how to do that scaling without needing tons and tons of compute, but those are [[00:16:39](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=999.52s)]
*  pretty open-ended engineering problems. I think the second is to actually scale the interpretation [[00:16:43](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1003.84s)]
*  of these networks. And so as an example, if you find all the neurons and all the features related [[00:16:50](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1010.48s)]
*  to, let's say pasta or Italian cuisine, and then you have a separate set of features that map to [[00:16:59](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1019.4399999999999s)]
*  pastries, right? Now the question is, how do you answer a complex query? And you ask the AI, [[00:17:07](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1027.3600000000001s)]
*  hey, if I asked you a provocative question about whether people of a certain ethnicity enjoy [[00:17:16](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1036.5600000000002s)]
*  Italian cuisine or not, right? You need to figure out how those two features actually interact with [[00:17:24](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1044.96s)]
*  each other at some meaningful scale. And that is a pretty difficult challenge to reason about too. [[00:17:31](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1051.04s)]
*  And I think that's the second big open problem that the researchers call out in their work. [[00:17:37](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1057.28s)]
*  And so I think the combinatorial complexity of each of those sets of features interacting with [[00:17:41](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1061.52s)]
*  each other at increasing scales is a nonlinear increase in complexity that has to be interpreted. [[00:17:49](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1069.92s)]
*  And so these are sort of the two big, at least at the moment, these are the two clear engineering [[00:17:55](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1075.68s)]
*  problems that need to be solved, scaling up the autoencoder and scaling up the interpretation. [[00:17:59](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1079.44s)]
*  But there probably are a list of long tail questions as well that I'm not addressing here, [[00:18:03](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1083.28s)]
*  but those are sort of the two big ones. How does this change the game? And maybe you [[00:18:09](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1089.44s)]
*  could speak to what you're excited for specifically coming into 2024 as it relates to mechanistic [[00:18:12](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1092.64s)]
*  interpretability. Yeah. So to be clear, I'm excited about all kinds of interpretability, [[00:18:18](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1098.8s)]
*  or at explainability. I'm broadly very excited about 2024 as the first time, or [[00:18:24](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1104.88s)]
*  at least the year where the most amount of interest and attention is being paid to explainability. [[00:18:33](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1113.92s)]
*  The last few years, the attention was all on the how and the what. People are just [[00:18:39](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1119.92s)]
*  incredulous at the capabilities of these models. Can we get them to be smarter? Can we get them [[00:18:44](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1124.56s)]
*  to reason about entirely new topics that maybe weren't in the original pre-training data set? [[00:18:49](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1129.12s)]
*  And that's been totally reasonable. But I think more attention on the why of these models and to [[00:18:55](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1135.04s)]
*  explain how they work has been the big blocker on these models getting deployed outside of just [[00:19:03](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1143.1999999999998s)]
*  a few consumer use cases where the costs of the model not being as reliable or steerable [[00:19:12](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1152.6399999999999s)]
*  are low. And so low precision environments, consumer use cases where people are more [[00:19:21](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1161.9199999999998s)]
*  forgiving and tolerant of mistakes by the model and so on is largely where the bulk of the value [[00:19:26](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1166.9599999999998s)]
*  has been generated in AI today. But I think if you want to see these models take over some of [[00:19:32](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1172.8799999999999s)]
*  the most impactful parts of our lives that they currently aren't deployed in, things like [[00:19:37](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1177.92s)]
*  healthcare, I think that those mission critical situations require a lot more reliability [[00:19:42](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1182.48s)]
*  and predictability. And that's what interpretability ultimately unlocks. If you can explain why [[00:19:48](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1188.48s)]
*  the kitchen does something, then you can control what it does. And that makes it much more reliable. [[00:19:53](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1193.6000000000001s)]
*  And therefore, it's going to be used in more and more situations and in more use cases and in more [[00:19:57](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1197.04s)]
*  and more impactful customer journeys where today a lot of the models don't actually make the cut. [[00:20:01](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1201.52s)]
*  Yeah. No, it's so true. Actually, something that also just dawned on me as you were talking is [[00:20:09](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1209.84s)]
*  almost everything in this world has margin for error. There is error inherently in most things. [[00:20:14](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1214.8s)]
*  However, if you can understand, if you can explain that error and constrain it to something that [[00:20:21](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1221.6s)]
*  other people can get behind, it's just much more likely that people will want to engage with that [[00:20:28](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1228.64s)]
*  thing because they can at least understand what is coming out of it. And so yeah, I feel like that [[00:20:33](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1233.0400000000002s)]
*  picture is very compelling and I hope we can get there. I hope so too. I think, to be clear, [[00:20:39](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1239.2800000000002s)]
*  we're not there yet, but we've got the glimmers now of approaches that might work. And I think [[00:20:45](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1245.44s)]
*  2020, what I'm excited about 2024 is a lot more investment, a lot more energy, a lot more of the [[00:20:50](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1250.0s)]
*  best researchers in this space spending their time on interpretability. Yeah. Well, we have some of [[00:20:55](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1255.12s)]
*  the smartest people in the world working on AI and we saw how quickly things moved in 2022, 2023. So [[00:20:59](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1259.9199999999998s)]
*  hopefully in 2024, some of this interpretability work moves just as quickly. I hope so. I've got [[00:21:05](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1265.76s)]
*  my fingers crossed. All right. I hope you enjoyed this big idea. We do have a lot more on the way, [[00:21:11](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1271.28s)]
*  including a new age of maritime exploration that takes advantage of AI and computer vision, [[00:21:16](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1276.6399999999999s)]
*  plus AI first games that never end and whether voice first apps may finally be having their [[00:21:22](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1282.48s)]
*  moment. By the way, if you want to see our full list of 40 plus big ideas today, you can head on [[00:21:28](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1288.08s)]
*  over to a16z.com slash big ideas 2024. It's time to build. [[00:21:34](https://www.youtube.com/watch?v=yTZVcOmhmlw&t=1294.24s)]
