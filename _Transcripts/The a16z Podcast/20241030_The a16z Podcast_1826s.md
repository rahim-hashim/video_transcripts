---
Date Generated: May 23, 2025
Transcription Model: whisper medium 20231117
Length: 1826s
Video Keywords: ['a16z', 'andreessen horowitz']
Video Views: 3508
Video Rating: None
Video Description: Deepfakes. A portmanteau of deep learning and fake, that started making their way into public consciousness in 2018. The rapidly evolving technology is now fully in the zeitgeist, impacting industries ranging from politics to commerce, and of course, social media.

At the rate that they’re appearing, deep fakes might sound like an impossible problem to tackle. But it turns out that despite the lower barrier to entry, advancements in detection tools are keeping pace.

In today’s video we’ll discuss the technology, policy, and economy behind deepfakes with someone who has been thinking about voice security long before it became popular: Vijay Balasubramaniyan, cofounder and CEO of Pindrop.

Topics Covered: 
00:00 - The Rise of Deepfakes
00:49 - The Evolution of Fake Media
02:23 - The Technology Behind Deepfakes
07:34 - AI and Deepfakes in Elections
09:50 - Statistics in Commerce and Banking Deepfakes
11:24 - Detection Challenges: Can We Spot a Deepfake?
13:09 - Watermarking and Cryptography
15:40 - The Economics of Deepfake Creation and Detection
19:15 - What Can Spam Teach Us About Deepfakes?
21:11 - What Are the Defenses?
24:30 - Policy, Platforms, and Accountability
28:54 - Preparing for a Deepfake-Proof Future

Resources: 
Find Vijay on Twitter: https://x.com/vijay_voice
Find Martin on Twitter: https://x.com/martin_casado

Let us know what you think: https://ratethispodcast.com/a16z 
Find a16z on Twitter: https://twitter.com/a16z 
Find a16z on LinkedIn: https://www.linkedin.com/company/a16z 
Subscribe on your favorite podcast app: https://a16z.simplecast.com/ 
Follow our host: https://twitter.com/stephsmithio 

Please note that the content here is for informational purposes only; should NOT be taken as legal, business, tax, or investment advice or be used to evaluate any investment or security; and is not directed at any investors or potential investors in any a16z fund. a16z and its affiliates may maintain investments in the companies discussed. For more details please see a16z.com/disclosures.
---

# Can We Detect a Deepfake?
**The a16z Podcast:** [October 30, 2024](https://www.youtube.com/watch?v=l-p9xZI3w_I)
*  There has been a 1400% increase in the amount of deepfakes we've seen this year in the first [[00:00:00](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=0.0s)]
*  six months compared to all of last year. [[00:00:07](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=7.5200000000000005s)]
*  So my dad jumps on the line and he's like, I just talked to you, you were in prison and [[00:00:09](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=9.84s)]
*  I'm leaving to go bring $10,000 of bail money to you. [[00:00:14](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=14.4s)]
*  I'm like, what are you talking about? [[00:00:19](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=19.12s)]
*  At the end of last year, there were 120 tools with which you can clone someone's voice. [[00:00:21](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=21.2s)]
*  And by March of this year, it's become 350. [[00:00:27](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=27.6s)]
*  Being able to identify what is real is going to become really important, [[00:00:31](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=31.44s)]
*  especially because now you can do all of these things at scale. [[00:00:37](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=37.44s)]
*  We've been doing deepfake detection for like now seven years. [[00:00:50](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=50.08s)]
*  Even before that, you have people manipulating audio and people manipulating video. [[00:00:54](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=54.64s)]
*  And you saw that Nancy Pelosi slurring in her speech. [[00:01:00](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=60.32s)]
*  It's in the Rose Garden with all this sort of visual. [[00:01:03](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=63.68s)]
*  All they did was slow down the audio. [[00:01:09](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=69.84s)]
*  And it wasn't a deepfake, it was actually a cheapfake. [[00:01:12](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=72.56s)]
*  What changed is the ability to use what are known as generative adversarial networks [[00:01:17](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=77.44s)]
*  to constantly improve things like voice cloning or video cloning or [[00:01:23](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=83.6s)]
*  essentially try to get the likeness of a person really close. [[00:01:30](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=90.72s)]
*  So it's essentially two systems competing against each other. [[00:01:34](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=94.72s)]
*  And the objective function is I'm going to get really close to [[00:01:38](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=98.56s)]
*  Martin's voice and Martin's face. [[00:01:43](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=103.19999999999999s)]
*  And then the other system is trying to figure out, okay, what are the anomalies? [[00:01:45](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=105.67999999999999s)]
*  How do I mean, you know, can I still detect that it's a machine as opposed to a human? [[00:01:49](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=109.44s)]
*  So it's almost like a reverse Turing test. [[00:01:55](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=115.03999999999999s)]
*  And so what ended up happening is once you start creating these GANs, [[00:01:57](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=117.36s)]
*  which are used in a lot of these spaces, when you run them across multiple iterations, [[00:02:01](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=121.2s)]
*  the system becomes really, really good because you train a deep learning neural network. [[00:02:06](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=126.4s)]
*  And that's where the deepfake comes from. [[00:02:12](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=132.07999999999998s)]
*  And they became so good that lots of people have extreme difficulty differentiating between [[00:02:14](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=134.48000000000002s)]
*  what is human and what is machine. [[00:02:20](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=140.88s)]
*  Deepfakes are more talked about now than they were in the past, right? [[00:02:23](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=143.44s)]
*  And so clearly this seems to have coincided with the generative AI wave, right? [[00:02:28](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=148.32s)]
*  And so do you think it's fair to say that there's a new [[00:02:34](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=154.0s)]
*  type of deepfake that is drafted on the generative AI wave? [[00:02:37](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=157.76s)]
*  Yeah, you know, generative AI has allowed for combinations of wonderful things. [[00:02:42](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=162.0s)]
*  But when we started, there was just one tool that could clone your voice, right? [[00:02:46](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=166.96s)]
*  Like it was called Liar Bird, incredible tool, was used for lots of great applications. [[00:02:52](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=172.8s)]
*  At the end of last year, there were 120 tools with which you can clone someone's voice. [[00:02:58](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=178.64s)]
*  And by March of this year, it's become 350. [[00:03:05](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=185.04s)]
*  And there's a lot of open source tools that you can use to essentially mimic someone's voice or [[00:03:09](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=189.52s)]
*  to mimic someone's likeness. [[00:03:16](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=196.56s)]
*  And that's the ease with which this has happened. [[00:03:18](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=198.4s)]
*  Essentially, the cost of doing this has become close to zero because all it requires for me to [[00:03:23](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=203.12s)]
*  clone your voice, Martin now requires about three to five seconds of your audio. [[00:03:29](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=209.92s)]
*  And if I want a really high quality deepfake, it requires about 15 seconds of audio. [[00:03:35](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=215.52s)]
*  Compare this to before the generative AI boom, where John Legend wanted to become the voice [[00:03:41](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=221.12s)]
*  of Google Home. [[00:03:48](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=228.16000000000003s)]
*  And he spent like, close to 20 hours recording him saying a whole bunch of things so that Google [[00:03:50](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=230.08s)]
*  Home could say in San Francisco, the weather is 37 degrees or whatever, right? [[00:03:56](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=236.56s)]
*  So the fact is that he had to go into a studio, spend 20 odd hours recording his voice in order [[00:04:02](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=242.16s)]
*  for you to do that compared to 15 seconds and 300 different tools available to do it. [[00:04:09](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=249.04s)]
*  I don't know if you remember this, Vijay, but this wasn't too long ago when I was in Japan. [[00:04:15](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=255.92s)]
*  And I got this call from my parents, which I never do. [[00:04:21](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=261.59999999999997s)]
*  And my mom's like, where are you right now? [[00:04:25](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=265.59999999999997s)]
*  And I'm like, I'm in Japan. [[00:04:27](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=267.76s)]
*  And my mom's like, no, you're not. [[00:04:29](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=269.28s)]
*  And I'm like, yes, I am. [[00:04:30](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=270.48s)]
*  She says, hold on. [[00:04:31](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=271.84000000000003s)]
*  Let me get your father. [[00:04:32](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=272.56s)]
*  So my dad jumps on the line. [[00:04:34](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=274.0s)]
*  And he's like, where are you? [[00:04:36](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=276.64000000000004s)]
*  I'm in Japan. [[00:04:38](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=278.40000000000003s)]
*  He's like, I just talked to you. [[00:04:39](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=279.12s)]
*  You were in prison. [[00:04:40](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=280.32s)]
*  And I'm leaving to go bring $10,000 of bail money to you. [[00:04:41](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=281.6s)]
*  I'm like, what are you talking about? [[00:04:46](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=286.56s)]
*  And he's like, listen, someone called and said that you had a car accident. [[00:04:48](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=288.40000000000003s)]
*  And you were a bit muffled because you were hurt. [[00:04:52](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=292.16s)]
*  And that I needed to bring cash to a certain area. [[00:05:00](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=300.32000000000005s)]
*  And your mom just thought to call you while I was heading out the door. [[00:05:04](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=304.16s)]
*  So of course, we called the police after this. [[00:05:08](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=308.0s)]
*  And they said, this is a well-known scam that's been going on for a very long time. [[00:05:10](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=310.0s)]
*  And it's probably just someone that tried to sound like you and muffling their voice. [[00:05:15](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=315.20000000000005s)]
*  And so it seems that calling somebody and obfuscating the voice to trick people has [[00:05:21](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=321.52000000000004s)]
*  been around for a very long time. [[00:05:28](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=328.16s)]
*  So maybe just from your perspective, do we need a new term for these generative AI [[00:05:30](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=330.48s)]
*  fakes because they're somehow fundamentally different? [[00:05:37](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=337.12s)]
*  Or is this just more of the same? [[00:05:39](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=339.76000000000005s)]
*  And we shouldn't really worry too much about it because we've been dealing with it for a long time. [[00:05:42](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=342.24s)]
*  Yeah. [[00:05:46](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=346.72s)]
*  So it's interesting it happened to you in Japan, man, because the origin of that scam, [[00:05:47](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=347.04s)]
*  this was close to eight, nine years back when I was talking about voice fraud. [[00:05:52](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=352.56s)]
*  The Japanese audience talked to me about Oriori Sagi, which has helped me grandma. [[00:05:58](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=358.16s)]
*  So it's exactly that. [[00:06:03](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=363.84000000000003s)]
*  But at that point in time, it had started costing Japan close to half a billion dollars [[00:06:05](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=365.36s)]
*  in people losing their life savings to the scam. [[00:06:12](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=372.24s)]
*  What's changed is the scale and the ability to actually mimic your voice. [[00:06:16](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=376.16s)]
*  The fact is that now you have so many tools that anyone can do it super easily [[00:06:22](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=382.48s)]
*  to before if you had some sort of an accent and things like that, they couldn't quite mimic your [[00:06:27](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=387.84s)]
*  real voice. [[00:06:35](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=395.59999999999997s)]
*  But now because it's 15 seconds, your grandson could have a 15 second TikTok video. [[00:06:36](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=396.88s)]
*  And that's all that's required. [[00:06:43](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=403.44s)]
*  Not even 15 seconds with five seconds. [[00:06:44](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=404.64s)]
*  And if depending upon the demographic, you can get a pretty good clone. [[00:06:46](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=406.79999999999995s)]
*  So what's changed is the ability to scale this. [[00:06:50](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=410.23999999999995s)]
*  And then these fraudsters are combining these systems, these text to speech systems with LLM [[00:06:53](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=413.68s)]
*  models. [[00:07:00](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=420.16s)]
*  So now you have a system that you're saying, okay, when the person says something, [[00:07:01](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=421.04s)]
*  respond back in a particular way crafted by the LLM. [[00:07:07](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=427.04s)]
*  And here is the crazy thing, right? [[00:07:11](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=431.68s)]
*  Like in LLM's hallucination is a problem. [[00:07:13](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=433.28000000000003s)]
*  So the fact that you're making shit up is a bad idea. [[00:07:16](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=436.08s)]
*  But if you have to make shit up to convince someone, [[00:07:19](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=439.6s)]
*  perfect. [[00:07:23](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=443.68s)]
*  That's right. [[00:07:24](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=444.18s)]
*  Yeah, and it's crazy. [[00:07:25](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=445.12s)]
*  We see fraud where the LLM is coming up with crazy, crazy ways to convince you that something [[00:07:26](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=446.40000000000003s)]
*  bad is happening. [[00:07:34](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=454.0s)]
*  You probably are the world's expert on voice fraud. [[00:07:35](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=455.36s)]
*  You've probably seen more types of voice fraud than any single person on the planet. [[00:07:37](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=457.52s)]
*  So I'm just wondering if you could like we know of the Odi Odi Sagi, which is basically what I [[00:07:42](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=462.56s)]
*  got hit with. [[00:07:46](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=466.96000000000004s)]
*  Can you maybe talk through some other kind of use cases for or uses of deepfakes that [[00:07:48](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=468.08s)]
*  are prevalent today? [[00:07:54](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=474.4s)]
*  Deepfakes right now you can see right in the political spectrum that they're right. [[00:07:55](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=475.2s)]
*  Like so election misinformation with President Biden's campaign happened. [[00:07:59](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=479.28s)]
*  We were the ones who caught it and identified it and things like that. [[00:08:04](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=484.24s)]
*  What was the specifics? [[00:08:07](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=487.44s)]
*  Are you allowed to talk? [[00:08:08](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=488.72s)]
*  Yeah. [[00:08:09](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=489.68s)]
*  Yeah, no, no, for sure. [[00:08:10](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=490.48s)]
*  Right. [[00:08:11](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=491.76s)]
*  So what happened is early on this year and you know, if you think about deepfakes, they [[00:08:12](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=492.0s)]
*  affect three big areas, commerce, media and communication. [[00:08:16](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=496.8s)]
*  Right. [[00:08:21](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=501.84000000000003s)]
*  And so this is news media, social media. [[00:08:22](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=502.08s)]
*  So what happened is at the beginning of an election year, you had the first case of [[00:08:25](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=505.12s)]
*  election interference with everyone during the Republican primary in New Hampshire got [[00:08:30](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=510.32s)]
*  a phone call that said, what a bunch of Malawki. [[00:08:36](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=516.16s)]
*  You know, the value of voting democratic on our votes count. [[00:08:40](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=520.4s)]
*  It's important that you save your vote for the November election. [[00:08:43](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=523.52s)]
*  And this was made in the voice of the president of the free world, right? [[00:08:47](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=527.12s)]
*  President Biden, right? [[00:08:51](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=531.76s)]
*  Like that's the craziness. [[00:08:52](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=532.7199999999999s)]
*  They went for the highest profile target that came out and you know, and people were like, [[00:08:54](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=534.16s)]
*  okay, is this really President Biden? [[00:08:59](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=539.92s)]
*  So not only did we come in and say this was a deep fake, but we identified the, we have [[00:09:01](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=541.52s)]
*  something called source tracing, which tells us which AI application was used to create [[00:09:06](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=546.8s)]
*  this deepfake. [[00:09:11](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=551.28s)]
*  So we identified the deepfake and then we worked with that AI application. [[00:09:12](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=552.0s)]
*  They're an incredible company. [[00:09:15](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=555.84s)]
*  We worked with them and they immediately found the person who used that script and shut them [[00:09:17](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=557.6800000000001s)]
*  down so they couldn't create any other problem. [[00:09:23](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=563.2800000000001s)]
*  And then later on regulation kicked in and they find the telco providers who distributed [[00:09:25](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=565.84s)]
*  these calls. [[00:09:32](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=572.32s)]
*  They find the political analyst who intentionally created these deepfakes. [[00:09:33](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=573.36s)]
*  But that was, you know, the first case of, you know, political misinformation. [[00:09:40](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=580.24s)]
*  You see this a lot. [[00:09:44](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=584.72s)]
*  Was that this year? [[00:09:45](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=585.6800000000001s)]
*  Yeah, it was this year. [[00:09:46](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=586.96s)]
*  It was in January of this year. [[00:09:48](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=588.0s)]
*  That's amazing. [[00:09:49](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=589.44s)]
*  Okay. [[00:09:50](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=590.48s)]
*  We've got politics. [[00:09:50](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=590.72s)]
*  We've got bilking old people. [[00:09:52](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=592.24s)]
*  Maybe one more good anecdote before we get into, you know, whether we can detect these [[00:09:55](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=595.52s)]
*  things. [[00:09:59](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=599.76s)]
*  The one thing that's really close home is in commerce, right? [[00:10:00](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=600.1600000000001s)]
*  Like financial institutions. [[00:10:03](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=603.6s)]
*  Generative AI came out in 2022. [[00:10:05](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=605.84s)]
*  In 2023, we were seeing essentially one deepfake a month in some customer, right? [[00:10:08](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=608.72s)]
*  So it was just one deepfake a month and some customer would face it. [[00:10:15](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=615.6s)]
*  It wasn't a widespread problem. [[00:10:18](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=618.88s)]
*  But this year, we've now seen one deepfake per customer per day. [[00:10:21](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=621.28s)]
*  So it is rapidly exploded. [[00:10:27](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=627.9200000000001s)]
*  And we have certain customers like really big banks who are getting a deepfake every [[00:10:31](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=631.6800000000001s)]
*  three hours. [[00:10:37](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=637.44s)]
*  Like it's insane the speed with it. [[00:10:38](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=638.8800000000001s)]
*  So there has been a 1400% increase in the amount of deepfakes we've seen this year in [[00:10:41](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=641.0400000000001s)]
*  the first six months compared to all of last year. [[00:10:48](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=648.4000000000001s)]
*  And the year is not even over. [[00:10:51](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=651.2800000000001s)]
*  Can you talk to like whether these things are detectable at all? [[00:10:52](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=652.5600000000001s)]
*  Like is this, you know, is this the beginning of the end or where are we? [[00:10:55](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=655.7600000000001s)]
*  Martin, you've lived through many such cycles where initially it feels like the sky is [[00:11:01](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=661.2s)]
*  falling, you know, online fraud, email spam. [[00:11:06](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=666.4s)]
*  There's a whole bunch of them. [[00:11:10](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=670.3199999999999s)]
*  But the situation is the same. [[00:11:12](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=672.3199999999999s)]
*  They're completely detectable. [[00:11:13](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=673.84s)]
*  Right now, we're detecting them with 99% detection rate with a 1% false positive rate. [[00:11:15](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=675.68s)]
*  So extremely high accuracy on being able to detect them. [[00:11:21](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=681.28s)]
*  Just to put this in context, what are numbers for like detecting like identifying voice? [[00:11:24](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=684.72s)]
*  Not fraud, just like whether it's my voice. [[00:11:29](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=689.12s)]
*  So it's roughly about one in every 100,000 to one in every million, right? [[00:11:32](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=692.0s)]
*  Like that's the ratio. [[00:11:35](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=695.92s)]
*  It's much higher precision for sure and much higher specificity. [[00:11:38](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=698.0799999999999s)]
*  But yeah, the deepfakes you're detecting with a 99% accuracy. [[00:11:43](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=703.36s)]
*  Because when you think about even something like voice, [[00:11:47](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=707.92s)]
*  you have 8,000 samples of your voice every single second, [[00:11:51](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=711.68s)]
*  even in the lowest fidelity channel, which is the contact center. [[00:11:57](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=717.12s)]
*  And so you can actually see how the voice changes over time 8,000 times a second. [[00:12:01](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=721.68s)]
*  And what we find is these deepfakes systems either on the frequency domain, [[00:12:08](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=728.48s)]
*  so spectrally, or on the time domain make mistakes. [[00:12:14](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=734.7199999999999s)]
*  And they make a lot of mistakes. [[00:12:19](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=739.4399999999999s)]
*  And the reason they make mistakes and still it's very clear is because think about it. [[00:12:21](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=741.28s)]
*  Your human ear can't look at anomalies 8,000 times a second. [[00:12:25](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=745.4399999999999s)]
*  If it did, you'd go mad, right? [[00:12:30](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=750.08s)]
*  Like you'd have some serious problems. [[00:12:31](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=751.84s)]
*  So that's the reason you don't, I mean, like it's beautiful to your ear. [[00:12:34](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=754.32s)]
*  You think it's Martin speaking on the other end. [[00:12:38](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=758.5600000000001s)]
*  But that's where you can use good AI, [[00:12:41](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=761.6800000000001s)]
*  which can actually look at things 8,000 times a second. [[00:12:45](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=765.12s)]
*  Or in this, like when we're doing most online conferencing, [[00:12:48](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=768.8000000000001s)]
*  like this podcast, it's usually 16,000. [[00:12:52](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=772.72s)]
*  So then you have 16,000 samples of your voice. [[00:12:55](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=775.6s)]
*  And if you're doing music, you have 44,000 samples [[00:12:57](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=777.92s)]
*  of the musician's voice every single second. [[00:13:01](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=781.04s)]
*  So there's so much data and so many anomalies [[00:13:03](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=783.36s)]
*  that you can actually detect these pretty comfortably. [[00:13:07](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=787.12s)]
*  I see a lot of proposals, particularly from policy circles of [[00:13:10](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=790.3199999999999s)]
*  using things like watermarking or cryptography, [[00:13:16](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=796.56s)]
*  which has always seemed kind of a strange idea to me [[00:13:21](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=801.28s)]
*  because you're kind of asking criminals to comply by something or, you know, [[00:13:23](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=803.76s)]
*  and some, you know, so I don't know. [[00:13:30](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=810.16s)]
*  I like how do you view more active measures to like self-identify [[00:13:32](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=812.64s)]
*  either legit or illegitimate traffic? [[00:13:40](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=820.16s)]
*  Yeah, see, this is why you're in security, Martin. [[00:13:42](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=822.8s)]
*  Almost immediately you realize that most attackers will not comply [[00:13:46](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=826.08s)]
*  to you putting in a watermark. [[00:13:50](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=830.96s)]
*  But even without putting in a watermark, right? [[00:13:53](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=833.36s)]
*  Like even if you didn't have an active adversary, [[00:13:55](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=835.92s)]
*  like the President Biden robocall that I referenced before, [[00:13:58](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=838.48s)]
*  when it finally showed up, the system that actually generated it had a watermark in it. [[00:14:02](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=842.64s)]
*  But when they tested it against that watermark, [[00:14:08](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=848.88s)]
*  they only were able to extract 2%. [[00:14:12](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=852.08s)]
*  Oh, interesting. [[00:14:14](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=854.16s)]
*  So you mean the original Biden call had? [[00:14:14](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=854.96s)]
*  A watermark because it was generated by an AI app that included a watermark. [[00:14:17](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=857.44s)]
*  And then they copied it. [[00:14:21](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=861.92s)]
*  And 98% of that watermark went away, [[00:14:25](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=865.12s)]
*  largely because when you take that audio, play it across air, [[00:14:27](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=867.92s)]
*  play it across telephony channels, they're bits and bytes, they get stripped away. [[00:14:31](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=871.92s)]
*  And so once they get stripped away, and audio is a very sparse channel, [[00:14:36](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=876.7199999999999s)]
*  so even if you add it over and over again, it's not possible to do it. [[00:14:40](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=880.56s)]
*  So these watermarking techniques, I mean, they're a great technique. [[00:14:45](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=885.1999999999999s)]
*  You always think about defense in depth where they're present. [[00:14:49](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=889.36s)]
*  You will be able to identify a whole lot more genuine stuff with as a result of these watermarks. [[00:14:53](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=893.28s)]
*  But attackers are not going to comply. [[00:14:59](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=899.36s)]
*  When you get videos, like, you know, we are now working with news media organizations, [[00:15:01](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=901.6s)]
*  and 90% of the videos and audios they get from, for example, the Israel Hamas war are fake. [[00:15:06](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=906.32s)]
*  So how many? [[00:15:14](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=914.96s)]
*  How many? [[00:15:16](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=916.24s)]
*  90% of them are fake. [[00:15:17](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=917.2s)]
*  What? [[00:15:18](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=918.64s)]
*  Yeah, I guess I should be so surprised. [[00:15:19](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=919.52s)]
*  They're all made up. [[00:15:22](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=922.64s)]
*  They're a different war. [[00:15:23](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=923.44s)]
*  Some of them are cheap fake. [[00:15:25](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=925.2s)]
*  Some of them are actually deep fake. [[00:15:26](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=926.4s)]
*  Some of them are clutched together stuff. [[00:15:28](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=928.0s)]
*  And so being able to identify what is real is going to become really important, [[00:15:31](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=931.76s)]
*  especially because now you can do all of these things at scale. [[00:15:38](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=938.96s)]
*  Can you draw out how the maturation in AI technology impacts this? [[00:15:42](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=942.4s)]
*  Because clearly something happened in the last year to make this economic for attackers, [[00:15:47](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=947.5200000000001s)]
*  which we're seeing a rise. [[00:15:53](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=953.6s)]
*  And clearly it's going to keep getting better. [[00:15:55](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=955.36s)]
*  Yeah, you know, so one of the things that we talk about is any deep fake system [[00:15:58](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=958.6400000000001s)]
*  should have strong resilience built in it. [[00:16:04](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=964.8800000000001s)]
*  So it should not just be good about detecting deep fakes right now. [[00:16:07](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=967.44s)]
*  It should be able to detect what we call zero-day deep fakes. [[00:16:10](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=970.8800000000001s)]
*  New system gets created. [[00:16:15](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=975.12s)]
*  How do you detect that deep fake? [[00:16:16](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=976.56s)]
*  And there is essentially the mental model is the following. [[00:16:18](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=978.0799999999999s)]
*  One, deep fake architectures are not simple monolithic systems. [[00:16:21](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=981.92s)]
*  They have like several components within them. [[00:16:26](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=986.7199999999999s)]
*  And what ends up happening is each of these components tend to leave behind artifacts. [[00:16:30](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=990.0s)]
*  We call this a fake print. [[00:16:34](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=994.64s)]
*  So they all leave behind things that they do poorly, right? [[00:16:36](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=996.0s)]
*  And so when you actually create a new system, you often find they've [[00:16:39](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=999.5999999999999s)]
*  pulled together pieces of other systems. [[00:16:44](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1004.2399999999999s)]
*  And those leave behind their older fake prints. [[00:16:46](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1006.48s)]
*  And so you can actually detect newer systems because they usually only improvise on one component. [[00:16:49](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1009.28s)]
*  The second is we actually run GANs. [[00:16:56](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1016.64s)]
*  So you get these GANs to compete. [[00:16:59](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1019.84s)]
*  Like we create our own deep fake detection system. [[00:17:01](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1021.6800000000001s)]
*  Now we say, how do you beat that? [[00:17:03](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1023.9200000000001s)]
*  And we have multiple iterations of them running and we're constantly running them. [[00:17:05](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1025.44s)]
*  Oh, wait, wait, wait. [[00:17:09](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1029.2s)]
*  I see. [[00:17:09](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1029.76s)]
*  Sorry. [[00:17:10](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1030.08s)]
*  I just want to make sure that I understand here. [[00:17:10](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1030.32s)]
*  Please. [[00:17:11](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1031.92s)]
*  So you're creating your own deep fake system using the approach you talked about before, [[00:17:12](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1032.64s)]
*  which is the general for serial networks. [[00:17:17](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1037.1200000000001s)]
*  So then you can create a good deep fake and then you can create a detection for that. [[00:17:19](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1039.2s)]
*  Is that right? [[00:17:24](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1044.0s)]
*  Exactly. [[00:17:24](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1044.64s)]
*  And then you beat that detection system and you run that iteration, iteration, iteration. [[00:17:25](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1045.6000000000001s)]
*  And then what you find is actually something really interesting, which is [[00:17:30](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1050.96s)]
*  if a deep fake system has to serve two masters, [[00:17:35](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1055.92s)]
*  that is one, I need to make the speech legible and sound as much like Martin. [[00:17:41](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1061.04s)]
*  And two, I need to deceive a deep fake detection system. [[00:17:46](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1066.56s)]
*  Those two objective functions start diverging. [[00:17:52](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1072.8799999999999s)]
*  So for example, I could start adding noise and noise is a great way to avoid you from [[00:17:55](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1075.52s)]
*  understanding my limitations. [[00:18:01](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1081.6s)]
*  But if I start adding too much noise, it stops. [[00:18:03](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1083.36s)]
*  I can't hear it. [[00:18:06](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1086.8s)]
*  So for example, we were called into one of these deep fakes where LeBron James apparently was [[00:18:07](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1087.84s)]
*  saying bad things about the coach during the Paris Olympics. [[00:18:14](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1094.56s)]
*  It wasn't LeBron James. [[00:18:18](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1098.48s)]
*  It was a deep fake. [[00:18:19](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1099.68s)]
*  We actually provided his management team the necessary detail so that in X, it could be [[00:18:20](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1100.8799999999999s)]
*  labeled as AI generated content. [[00:18:28](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1108.72s)]
*  But if you look at the audio, there was a lot of noise introduced into it, right? [[00:18:31](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1111.2s)]
*  To try and avoid detection. [[00:18:35](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1115.84s)]
*  Lots of people couldn't even hear the audio. [[00:18:48](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1128.72s)]
*  They were like, is this really? [[00:18:50](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1130.56s)]
*  And so that's where you start seeing these systems diverge. [[00:18:53](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1133.12s)]
*  And this is where I have confidence in our ability to detect it, right? [[00:18:56](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1136.8s)]
*  Which is you run these GANs, you know the architectures that these deep fake generation [[00:19:00](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1140.32s)]
*  systems are created, and ultimately you start seeing divergences in one of the objective [[00:19:05](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1145.6s)]
*  functions. [[00:19:10](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1150.3999999999999s)]
*  So either you as a human will be able to detect somethings off or we as a system will be able [[00:19:11](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1151.04s)]
*  to detect somethings off. [[00:19:15](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1155.4399999999998s)]
*  I mean, one of the reasons that spam works and deep fakes work is just the marginal cost [[00:19:17](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1157.04s)]
*  of the next call is so low that you can do these things in mass, right? [[00:19:24](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1164.3999999999999s)]
*  Like the marginal cost of the next spam email or whatever. [[00:19:28](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1168.32s)]
*  If it takes me a dollar to generate and deep fakes, how much does it cost to detect and [[00:19:32](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1172.1599999999999s)]
*  deep fakes? [[00:19:39](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1179.04s)]
*  Is it one to one? [[00:19:39](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1179.52s)]
*  Is it 10 to one? [[00:19:40](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1180.3999999999999s)]
*  Is it 100 to one? [[00:19:41](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1181.28s)]
*  Yeah, it's way cheaper to detect deep fakes, right? [[00:19:42](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1182.7199999999998s)]
*  Because if you think about it, the closest example is Apple released its model that could [[00:19:46](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1186.3999999999999s)]
*  run on device. [[00:19:52](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1192.7199999999998s)]
*  And even that model, which is a small model in order to do lots of things like voice, [[00:19:54](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1194.56s)]
*  to text and things like that, our model is about 100 times smaller than that. [[00:20:01](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1201.36s)]
*  So it's so much faster in detecting deep fakes. [[00:20:06](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1206.56s)]
*  So the ratio is about one hundredth right now. [[00:20:09](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1209.84s)]
*  And we're constantly figuring out ways to make it even cheaper. [[00:20:14](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1214.1599999999999s)]
*  But it's one hundredth that of generation. [[00:20:18](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1218.24s)]
*  Wow, I see. [[00:20:21](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1221.4399999999998s)]
*  So to detect it is two orders of magnitude cheaper than creation, which means in order [[00:20:22](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1222.56s)]
*  for anybody to economically get, listen, if there is no defense, there's no defense. [[00:20:28](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1228.88s)]
*  But if there is a defense, it requires the bad guys to have two orders of magnitude more [[00:20:35](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1235.2s)]
*  resources, which is actually pretty dramatic, given normally you go for parity on these [[00:20:40](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1240.48s)]
*  things because there tends to be a lot more good people than bad people. [[00:20:46](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1246.4s)]
*  Yeah. [[00:20:50](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1250.16s)]
*  And that's the thing. [[00:20:50](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1250.88s)]
*  You have two orders of magnitude. [[00:20:51](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1251.7600000000002s)]
*  And then the fact is that once you know what a deep fake looks like, unless they re-architect [[00:20:53](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1253.2s)]
*  the entire system and the only companies that re-architect full pipelines. [[00:20:57](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1257.36s)]
*  And the last time this was done is back when Google released [[00:21:02](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1262.8s)]
*  Tachotron, where they re-architected several pieces of the pipeline. [[00:21:06](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1266.56s)]
*  It's a very expensive proposition. [[00:21:10](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1270.3999999999999s)]
*  Is the intuitive reason that the cost is so much cheaper to detect is that you just have [[00:21:12](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1272.08s)]
*  to do less stuff. [[00:21:16](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1276.0s)]
*  Like the person generated the deep fake has to like sound like a human, be passable to [[00:21:16](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1276.9599999999998s)]
*  a human and evade this. [[00:21:22](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1282.24s)]
*  And so like that's just more things than detecting it, which just can be much more [[00:21:24](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1284.0s)]
*  narrow focus. [[00:21:27](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1287.12s)]
*  So it'll always be cheaper to detect. [[00:21:27](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1287.6799999999998s)]
*  And then you don't see a period in time where the AI is so good, no deep fake [[00:21:29](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1289.84s)]
*  mechanism can detect it. [[00:21:35](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1295.6799999999998s)]
*  You don't see that. [[00:21:36](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1296.8s)]
*  We don't see that because either you become so good at avoiding detection that you actually [[00:21:38](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1298.1599999999999s)]
*  start becoming worse at producing human generated speech or you're producing human generated [[00:21:44](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1304.9599999999998s)]
*  speech. [[00:21:51](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1311.36s)]
*  And unless, you know, you actually create a physical representation of a human, because [[00:21:52](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1312.56s)]
*  we've had 10,000 years of evolution and the way we produce speech has vocal cords, has [[00:21:58](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1318.7199999999998s)]
*  the diaphragm, has your lips and your mouth and your nasal cavity, all of that physical [[00:22:04](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1324.24s)]
*  attributes. [[00:22:09](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1329.76s)]
*  It's really hard for these systems to replicate all of that. [[00:22:10](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1330.7199999999998s)]
*  They have generic models and those generic models are good. [[00:22:14](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1334.24s)]
*  You can also think about the more we learn about your voice, Martin, the better we can [[00:22:17](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1337.84s)]
*  get at knowing where your voice is deviating. [[00:22:22](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1342.48s)]
*  And I have an incentive as a good guy to work with you on that, right? [[00:22:26](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1346.48s)]
*  So like you'll have access to data where like the bad people may not have access to data [[00:22:29](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1349.2s)]
*  and it totally makes sense. [[00:22:32](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1352.56s)]
*  So it's interesting. [[00:22:34](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1354.32s)]
*  So it seems to me like almost like the spam, the spam lessons learned apply here, which [[00:22:34](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1354.96s)]
*  is spam can be very effective for attackers, very effective. [[00:22:40](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1360.3999999999999s)]
*  Defenses can also be incredibly effective, however you have to put them in place. [[00:22:45](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1365.04s)]
*  And so is it this the same situation here, which is like, be sure you have a strategy [[00:22:49](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1369.6799999999998s)]
*  for deepfake detection, but if you do, you'll be okay. [[00:22:53](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1373.6799999999998s)]
*  That's exactly right. [[00:22:56](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1376.48s)]
*  And I think, you know, it has to be in each of the areas, right? [[00:22:57](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1377.4399999999998s)]
*  Like when you think about deepfakes, you know, you have incredible AI applications that are [[00:23:00](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1380.08s)]
*  doing wonderful things in each of these spaces. [[00:23:04](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1384.8s)]
*  Like, you know, the voice cloning apps, they've actually given voices to people who have [[00:23:07](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1387.28s)]
*  throat cancer and things like that, right? [[00:23:12](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1392.88s)]
*  But in each of those situations, it was with the consent of the user who, you know, who [[00:23:15](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1395.6000000000001s)]
*  wanted their voice recreated, right? [[00:23:21](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1401.8400000000001s)]
*  And so that notion that the source AI applications need to make sure that the people using their [[00:23:24](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1404.64s)]
*  platform actually are the people who want to use their platform. [[00:23:30](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1410.8000000000002s)]
*  That's part A. [[00:23:34](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1414.96s)]
*  And this is where the partnerships that you talked about with the actual generation [[00:23:36](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1416.5600000000002s)]
*  companies comes in so that you can help them for like the legitimate use cases as well [[00:23:41](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1421.04s)]
*  as sniffing out the illegitimate one. [[00:23:45](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1425.52s)]
*  Is that right? [[00:23:46](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1426.96s)]
*  Yeah, absolutely. [[00:23:47](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1427.52s)]
*  11 labs. [[00:23:48](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1428.8799999999999s)]
*  Incredible. [[00:23:50](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1430.3999999999999s)]
*  The amount of work they're doing to create voices ethically and safely and carefully [[00:23:51](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1431.28s)]
*  is incredible. [[00:23:58](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1438.6399999999999s)]
*  They're trying to get lots of great tools out there. [[00:23:59](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1439.52s)]
*  We're partnering with them. [[00:24:02](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1442.3999999999999s)]
*  They're making their data sets accessible to us. [[00:24:04](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1444.1599999999999s)]
*  There are companies like that, right? [[00:24:07](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1447.04s)]
*  Another company called Respeacher that's doing that did a lot of the Hollywood movies. [[00:24:08](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1448.88s)]
*  So all of these companies are starting to partner in order to be able to do this in [[00:24:14](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1454.72s)]
*  the right way. [[00:24:20](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1460.48s)]
*  And it's similar to, you know, a lot of what happened in the, you know, fraud situation [[00:24:21](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1461.7600000000002s)]
*  back in the 2000s or the email spam situation back in the 2000s. [[00:24:27](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1467.92s)]
*  I want to shift over to policy. [[00:24:31](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1471.7600000000002s)]
*  So I've heard a lot of policy discussions lately, you know, in the California, in California [[00:24:34](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1474.8s)]
*  as well as at the federal level. [[00:24:41](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1481.6799999999998s)]
*  And so have you given thought to what guidance you would give to policymakers, many of who [[00:24:42](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1482.8799999999999s)]
*  listen to this podcast and how they should think about, you know, any regulations or [[00:24:48](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1488.56s)]
*  rules around this and maybe how it intersects with things like innovation and free speech, [[00:24:53](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1493.9199999999998s)]
*  et cetera. [[00:24:57](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1497.84s)]
*  It's a complicated topic. [[00:24:58](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1498.32s)]
*  I think the simple one-liner answer is they should make it really difficult for threat [[00:24:59](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1499.76s)]
*  actors and really flexible for creators, right? [[00:25:06](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1506.64s)]
*  Like that's the ultimate difference. [[00:25:10](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1510.24s)]
*  And so, and, you know, history is rife with a lot of great ways, right? [[00:25:12](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1512.72s)]
*  Like you live through the email days where the CanSpan Act was a great way, but it came [[00:25:17](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1517.04s)]
*  in combination with better ML technologies, right? [[00:25:23](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1523.6s)]
*  Maybe just walk through how CanSpan works. [[00:25:27](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1527.2s)]
*  I think it's a good analog. [[00:25:29](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1529.28s)]
*  The CanSpan Act is one where, you know, anyone who's providing unsolicited marketing has [[00:25:31](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1531.44s)]
*  to be clear on its headers, has to allow you to opt out, all of those things. [[00:25:38](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1538.32s)]
*  And if you don't follow this very strict set of policies, you can be fined. [[00:25:43](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1543.52s)]
*  And you also have great detection technologies that allow you to detect these spams, right? [[00:25:49](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1549.2s)]
*  Now that you follow a particular standard, especially when you're doing unsolicited [[00:25:54](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1554.72s)]
*  marketing or you're trying to do, you know, bad things like pornography or things like that, [[00:25:58](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1558.72s)]
*  you have detection, AI ML technologies that can detect you well. [[00:26:03](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1563.9199999999998s)]
*  The same thing happened when, you know, lots of when banks went online, right? [[00:26:07](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1567.6799999999998s)]
*  Like you had a lot of online fraud or e-commerce, right? [[00:26:12](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1572.6399999999999s)]
*  You had a lot of online fraud. [[00:26:16](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1576.0s)]
*  And if you remember the Know Your Customer Act and the Anti-Monthly Laundering Act came [[00:26:17](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1577.9199999999998s)]
*  in there. [[00:26:23](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1583.84s)]
*  So the onus was you as a organization have to know your customer. [[00:26:24](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1584.6399999999999s)]
*  You have to put in technology that knows your customer. [[00:26:30](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1590.8799999999999s)]
*  That's the guarantee. [[00:26:34](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1594.24s)]
*  And so you need technology. [[00:26:35](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1595.76s)]
*  After that, you can do what you want. [[00:26:37](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1597.6799999999998s)]
*  And so I think what was really good about both of those cases is they got really specific [[00:26:39](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1599.4399999999998s)]
*  on one, what can the technology detect? [[00:26:45](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1605.76s)]
*  Because if the technology can't detect it, you can't litigate. [[00:26:48](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1608.3999999999999s)]
*  You can't like find the people who are misusing it and so on. [[00:26:51](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1611.6s)]
*  So what can the technology detect? [[00:26:54](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1614.8799999999999s)]
*  And two, how do I make it really specific on what you can and cannot do in order to [[00:26:56](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1616.7199999999998s)]
*  be able to do this? [[00:27:03](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1623.1999999999998s)]
*  And so I think those two were great examples of how we should think about litigation. [[00:27:04](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1624.6399999999999s)]
*  And in deep fake, there is this very clear thing, right? [[00:27:10](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1630.24s)]
*  Like you have free speech, but for the longest time, anytime you used free speech for fraud [[00:27:13](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1633.28s)]
*  or you were trying to incite violence or you were trying to do obscene things, these are [[00:27:19](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1639.6s)]
*  clear places where the free speech guarantees go away. [[00:27:25](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1645.12s)]
*  So as long as you're not doing that, right? [[00:27:29](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1649.68s)]
*  Like I think if you're doing that, you should be fined, right? [[00:27:31](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1651.84s)]
*  And that's and you should have laws that protect you against that. [[00:27:36](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1656.32s)]
*  And that's where that's the model I like to think of. [[00:27:39](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1659.68s)]
*  But then there's this kind of gray area of unwanted stuff, right? [[00:27:43](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1663.36s)]
*  And the unwanted stuff is like, you know, you didn't ask for it. [[00:27:46](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1666.72s)]
*  It may not be illegal, but it's super annoying and it's unwanted and it can fill your inbox [[00:27:51](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1671.52s)]
*  and it can fill your whatever. [[00:27:56](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1676.56s)]
*  And for those, you can put in rules because if somebody crosses those rules, you can litigate [[00:27:58](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1678.64s)]
*  them or you can opt out of it. [[00:28:03](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1683.52s)]
*  And so it kind of unregulates the unwanted. [[00:28:05](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1685.44s)]
*  I could see that definitely happening here. [[00:28:07](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1687.6000000000001s)]
*  And then, of course, there's the wanted stuff, which, you know, like doesn't require any [[00:28:09](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1689.3600000000001s)]
*  regulation. [[00:28:12](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1692.56s)]
*  And the only other thing that I'll say is right now, because we consume things through [[00:28:13](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1693.36s)]
*  a lot of platforms, platforms should be held accountable at some level to, you know, clearly [[00:28:17](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1697.04s)]
*  demarcating what is real and what is not right, because otherwise it's going to be really [[00:28:24](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1704.32s)]
*  hard for the average consumer to know that this is AI generated versus this is not. [[00:28:29](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1709.84s)]
*  So I think, you know, there's a certain amount of accountability there. [[00:28:35](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1715.84s)]
*  Because the technology is where it is, putting the onus on the platforms to do best practices, [[00:28:39](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1719.84s)]
*  just like we did for spam, right? [[00:28:45](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1725.4399999999998s)]
*  Like I rely on Microsoft and Google for the spam detection, doing the same type of thing [[00:28:47](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1727.04s)]
*  for the platform. [[00:28:52](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1732.32s)]
*  It sounds like a very, very sensible recommendation. [[00:28:52](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1732.8s)]
*  So key point number one is, you know, deepfakes have been around for a long time. [[00:28:56](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1736.08s)]
*  We probably need a new name for this new generation. [[00:29:00](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1740.8s)]
*  And this isn't just like some hypothetical thing, but you're seeing a massive increase. [[00:29:04](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1744.6399999999999s)]
*  You said as much as one per day. [[00:29:09](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1749.28s)]
*  And the cost to generate has gone way down. [[00:29:11](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1751.92s)]
*  Good news is that these things are imminently detectable. [[00:29:15](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1755.76s)]
*  And in your opinion, will always be detectable if you have a solution in place. [[00:29:19](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1759.92s)]
*  And then as a result, I think, you know, any policy should provide the guidance and maybe [[00:29:24](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1764.8s)]
*  accountability for the platforms to detect it because we can actually detect it. [[00:29:30](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1770.96s)]
*  And so like, listen, it's something for people to know about. [[00:29:35](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1775.6000000000001s)]
*  But, you know, it's not the end of the world. [[00:29:38](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1778.64s)]
*  And, you know, policymakers don't have to regulate all of AI for this one specific use [[00:29:40](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1780.72s)]
*  case. [[00:29:45](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1785.5200000000002s)]
*  Is this a fair synopsis? [[00:29:45](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1785.76s)]
*  This is a beautiful synopsis, Martin. [[00:29:47](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1787.44s)]
*  You've captured it really, really well. [[00:29:49](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1789.3600000000001s)]
*  Thank you so much for listening to the A16Z podcast. [[00:29:51](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1791.6000000000001s)]
*  If you've made it this far, don't forget to subscribe so that you are the first to get [[00:29:55](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1795.2800000000002s)]
*  our exclusive video content. [[00:29:59](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1799.5200000000002s)]
*  Or you can check out this video that we've hand selected for you. [[00:30:01](https://www.youtube.com/watch?v=l-p9xZI3w_I&t=1801.92s)]
