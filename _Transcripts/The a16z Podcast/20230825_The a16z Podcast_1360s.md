---
Date Generated: May 24, 2025
Transcription Model: whisper medium 20231117
Length: 1360s
Video Keywords: []
Video Views: 11036
Video Rating: None
Video Description: With the world constantly generating more data, unlocking the full potential of AI means a constant need for faster and more resilient hardware.

In this episode – the second in our three-part series – we explore the challenges for founders trying to build AI companies. We dive into the delta between supply and demand, whether to own or rent, where moats can be found, and even where open source comes into play.

Look out for the rest of our series, where we dive into terminology and technology that is the backbone of the AI, how much the cost of compute truly costs!

Topics Covered:
00:00 – Supply and demand
03:03 –  Competition for AI hardware
04:46 – Who gets access to the supply available
06:28 – How to select which hardware to use
08:59 – Cloud versus bringing infrastructure in house
13:26– What role does open source play? 
16:48 – Cheaper and decentralized compute
20:16– Rebuilding the stack
21:31– Upcoming episodes on cost of compute

Resources: 
Find Guido on LinkedIn: https://www.linkedin.com/in/appenz/
Find Guido on Twitter: https://twitter.com/appenz
 
Find a16z on Twitter: https://twitter.com/a16z 
Find a16z on LinkedIn: https://www.linkedin.com/company/a16z 
Subscribe on your favorite podcast app: https://a16z.simplecast.com/ 
Follow our host: https://twitter.com/stephsmithio 

Please note that the content here is for informational purposes only; should NOT be taken as legal, business, tax, or investment advice or be used to evaluate any investment or security; and is not directed at any investors or potential investors in any a16z fund. a16z and its affiliates may maintain investments in the companies discussed. For more details please see a16z.com/disclosures.
---

# Chasing Silicon: The Race for GPUs
**The a16z Podcast:** [August 25, 2023](https://www.youtube.com/watch?v=IPre5287P3I)
*  Finding the compute capacity to run their applications is actually a real challenge. [[00:00:00](https://www.youtube.com/watch?v=IPre5287P3I&t=0.0s)]
*  What really is stopping companies from going in like 10xing their production? [[00:00:04](https://www.youtube.com/watch?v=IPre5287P3I&t=4.5600000000000005s)]
*  The crazy exponential growth of AI at the moment. [[00:00:08](https://www.youtube.com/watch?v=IPre5287P3I&t=8.96s)]
*  How do I get access to the compute that I need? [[00:00:11](https://www.youtube.com/watch?v=IPre5287P3I&t=11.96s)]
*  I think my number one advice would be to shop around. [[00:00:14](https://www.youtube.com/watch?v=IPre5287P3I&t=14.76s)]
*  For a certain process, which you don't want to use, there's capacity. [[00:00:16](https://www.youtube.com/watch?v=IPre5287P3I&t=16.92s)]
*  But for another one that you do want to use, they don't have the capacity. [[00:00:19](https://www.youtube.com/watch?v=IPre5287P3I&t=19.12s)]
*  How much should founders know about hardware? [[00:00:21](https://www.youtube.com/watch?v=IPre5287P3I&t=21.56s)]
*  There's probably a certain scale where it makes sense for you. [[00:00:25](https://www.youtube.com/watch?v=IPre5287P3I&t=25.0s)]
*  It's a whole new ecosystem forming and I think there's a ton of opportunities to build right now. [[00:00:27](https://www.youtube.com/watch?v=IPre5287P3I&t=27.6s)]
*  With software becoming more important than ever, hardware is following suit. [[00:00:37](https://www.youtube.com/watch?v=IPre5287P3I&t=37.04s)]
*  And with the world constantly generating more data, [[00:00:42](https://www.youtube.com/watch?v=IPre5287P3I&t=42.400000000000006s)]
*  unlocking the full potential of AI means a constant need for faster and more resilient hardware. [[00:00:45](https://www.youtube.com/watch?v=IPre5287P3I&t=45.6s)]
*  That is exactly why we've created this mini-series on AI hardware. [[00:00:51](https://www.youtube.com/watch?v=IPre5287P3I&t=51.68000000000001s)]
*  In part one, we took you through the emerging architecture powering LLMs from GPU to TPU, [[00:00:56](https://www.youtube.com/watch?v=IPre5287P3I&t=56.08s)]
*  including how they work, who's creating them, and also whether we can expect Moore's Law to continue. [[00:01:02](https://www.youtube.com/watch?v=IPre5287P3I&t=62.16s)]
*  But part two is for the founders trying to build AI companies. [[00:01:09](https://www.youtube.com/watch?v=IPre5287P3I&t=69.44s)]
*  And here we dive into the delta between supply and demand, [[00:01:13](https://www.youtube.com/watch?v=IPre5287P3I&t=73.52s)]
*  why we can't just print our way out of a shortage, how founders can get access to inventory, [[00:01:16](https://www.youtube.com/watch?v=IPre5287P3I&t=76.8s)]
*  whether they should think about renting or owning, where moats can be found, [[00:01:21](https://www.youtube.com/watch?v=IPre5287P3I&t=81.44s)]
*  and even where open source comes into play. [[00:01:25](https://www.youtube.com/watch?v=IPre5287P3I&t=85.76s)]
*  You should also look out for part three coming very soon, [[00:01:28](https://www.youtube.com/watch?v=IPre5287P3I&t=88.24000000000001s)]
*  where we break down exactly how much all of this costs, from training to inference. [[00:01:30](https://www.youtube.com/watch?v=IPre5287P3I&t=90.96000000000001s)]
*  And today we're joined again by A16Z special advisor, Guido Appenzeller, [[00:01:36](https://www.youtube.com/watch?v=IPre5287P3I&t=96.16000000000001s)]
*  someone who is truly uniquely suited for this deep dive as a storied infrastructure expert [[00:01:40](https://www.youtube.com/watch?v=IPre5287P3I&t=100.88000000000001s)]
*  with experience like... [[00:01:46](https://www.youtube.com/watch?v=IPre5287P3I&t=106.24000000000001s)]
*  CTO for Intel's data center group dealing a lot with hardware and the low-level components. [[00:01:47](https://www.youtube.com/watch?v=IPre5287P3I&t=107.44s)]
*  It's given me a good insight into how large data centers work, [[00:01:51](https://www.youtube.com/watch?v=IPre5287P3I&t=111.92s)]
*  the basic components are that make all of this AI boom possible today. [[00:01:56](https://www.youtube.com/watch?v=IPre5287P3I&t=116.96000000000001s)]
*  Despite working with infrastructure for quite some time, [[00:02:02](https://www.youtube.com/watch?v=IPre5287P3I&t=122.8s)]
*  here's Guido commenting on how the momentum of the recent AI wave is shifting supply and demand dynamics. [[00:02:05](https://www.youtube.com/watch?v=IPre5287P3I&t=125.84s)]
*  The biggest thing that is strange is the crazy exponential growth of AI at the moment. [[00:02:12](https://www.youtube.com/watch?v=IPre5287P3I&t=132.0s)]
*  AI has been booming since mid last year. I think nobody expected how quickly it would move. [[00:02:17](https://www.youtube.com/watch?v=IPre5287P3I&t=137.76000000000002s)]
*  That is just a credited demand, which at the moment the market can't fulfill. [[00:02:22](https://www.youtube.com/watch?v=IPre5287P3I&t=142.4s)]
*  As a reminder, the content here is for informational purposes only, [[00:02:26](https://www.youtube.com/watch?v=IPre5287P3I&t=146.8s)]
*  should not be taken as legal, business, tax or investment advice, [[00:02:30](https://www.youtube.com/watch?v=IPre5287P3I&t=150.48000000000002s)]
*  or be used to evaluate any investment or security, [[00:02:33](https://www.youtube.com/watch?v=IPre5287P3I&t=153.92000000000002s)]
*  and is not directed at any investors or potential investors in any A16Z fund. [[00:02:36](https://www.youtube.com/watch?v=IPre5287P3I&t=156.48000000000002s)]
*  Please note that A16Z and its affiliates may also maintain investments [[00:02:41](https://www.youtube.com/watch?v=IPre5287P3I&t=161.36s)]
*  in the companies discussed in this podcast. [[00:02:45](https://www.youtube.com/watch?v=IPre5287P3I&t=165.28s)]
*  For more details, including a link to our investments, please see a16z.com slash disclosures. [[00:02:47](https://www.youtube.com/watch?v=IPre5287P3I&t=167.84s)]
*  In a recent article, Guido even stated that some reputable sources [[00:03:03](https://www.youtube.com/watch?v=IPre5287P3I&t=183.04000000000002s)]
*  indicate that demand for AI hardware outstrips supply by a factor of 10. [[00:03:07](https://www.youtube.com/watch?v=IPre5287P3I&t=187.76000000000002s)]
*  Here's him commenting on how that dynamic is impacting competition. [[00:03:13](https://www.youtube.com/watch?v=IPre5287P3I&t=193.60000000000002s)]
*  We currently don't have as many AI chips or servers as we'd like to have. [[00:03:17](https://www.youtube.com/watch?v=IPre5287P3I&t=197.76000000000002s)]
*  For some of our portfolio companies, finding the compute capacity that they need to run [[00:03:24](https://www.youtube.com/watch?v=IPre5287P3I&t=204.48000000000002s)]
*  their applications is actually a real challenge. There's a whole value chain behind that. [[00:03:29](https://www.youtube.com/watch?v=IPre5287P3I&t=209.12s)]
*  It's a combination of many things. We have some bottlenecks on the chip manufacturing side. [[00:03:33](https://www.youtube.com/watch?v=IPre5287P3I&t=213.44s)]
*  We have some bottlenecks on building the actual cards. These development cycles take some time. [[00:03:37](https://www.youtube.com/watch?v=IPre5287P3I&t=217.52s)]
*  Maybe this is a silly question, but what really is stopping companies like Intel, like [[00:03:43](https://www.youtube.com/watch?v=IPre5287P3I&t=223.12s)]
*  Nvidia, from going and 10Xing their production? Is that on the road map where we're just going [[00:03:47](https://www.youtube.com/watch?v=IPre5287P3I&t=227.44s)]
*  to see a lot more chips and we won't see this discrepancy between supply and demand, [[00:03:52](https://www.youtube.com/watch?v=IPre5287P3I&t=232.39999999999998s)]
*  or is there something more complex at play? It's a bit more complex because if you want [[00:03:57](https://www.youtube.com/watch?v=IPre5287P3I&t=237.04s)]
*  to make a chip, the way you do it is you make it in a foundry, which are extremely large, [[00:04:01](https://www.youtube.com/watch?v=IPre5287P3I&t=241.04s)]
*  extremely complex. Intel makes chips on their own foundries, but most companies manufacture [[00:04:07](https://www.youtube.com/watch?v=IPre5287P3I&t=247.2s)]
*  with Taiwan Semiconductor, TSMC. They are capacity constrained. You often have to reserve [[00:04:12](https://www.youtube.com/watch?v=IPre5287P3I&t=252.64s)]
*  capacity long in advance. There's different processes. It might be for a certain process, [[00:04:18](https://www.youtube.com/watch?v=IPre5287P3I&t=258.96s)]
*  which you don't want to use, there's capacity. But for another one that you do want to use, [[00:04:23](https://www.youtube.com/watch?v=IPre5287P3I&t=263.36s)]
*  they don't have the capacity. You could just say, well, in that case, let's just build more fabs. [[00:04:26](https://www.youtube.com/watch?v=IPre5287P3I&t=266.0s)]
*  But building a fab takes you a couple of years and probably a couple of billion or 10 billion [[00:04:30](https://www.youtube.com/watch?v=IPre5287P3I&t=270.64s)]
*  of investment. You're looking at some very large investment projects that take some time to adjust. [[00:04:37](https://www.youtube.com/watch?v=IPre5287P3I&t=277.03999999999996s)]
*  And that's sort of what prevents us from reacting more quickly at the moment. [[00:04:43](https://www.youtube.com/watch?v=IPre5287P3I&t=283.2s)]
*  And while some countries are indeed making major multi-billion dollar investments in new [[00:04:46](https://www.youtube.com/watch?v=IPre5287P3I&t=286.4s)]
*  semiconductor production plants, aka fabs, these will take time to scale. And there are also no [[00:04:51](https://www.youtube.com/watch?v=IPre5287P3I&t=291.59999999999997s)]
*  promises given that expertise is concentrated in a few companies. So with demand not subsiding, [[00:04:57](https://www.youtube.com/watch?v=IPre5287P3I&t=297.52s)]
*  what does this mean for who gets access to the supply available? I mean, it doesn't sound like [[00:05:04](https://www.youtube.com/watch?v=IPre5287P3I&t=304.47999999999996s)]
*  the demand is going to subside, especially because we see this really what seems like [[00:05:09](https://www.youtube.com/watch?v=IPre5287P3I&t=309.12s)]
*  intrinsic relationship between the power of these models and then the compute that's thrown at them. [[00:05:13](https://www.youtube.com/watch?v=IPre5287P3I&t=313.36s)]
*  So how does a company, let's say if I'm a founder today, how do I get access to the compute that I [[00:05:18](https://www.youtube.com/watch?v=IPre5287P3I&t=318.0s)]
*  need? And who decides, is it just who's willing to pay the most or how is that supply being [[00:05:24](https://www.youtube.com/watch?v=IPre5287P3I&t=324.0s)]
*  distributed? Yeah, there's some of that, right? At the moment, capacity is expensive wherever you [[00:05:29](https://www.youtube.com/watch?v=IPre5287P3I&t=329.84000000000003s)]
*  go. I tried just to run some personal experiments, try to reserve an instance [[00:05:35](https://www.youtube.com/watch?v=IPre5287P3I&t=335.52s)]
*  with the cloud service providers a few days ago and they just didn't have any. I was like, [[00:05:40](https://www.youtube.com/watch?v=IPre5287P3I&t=340.47999999999996s)]
*  no, not available. What we're seeing is that often in order to get access to the newer cards and newer [[00:05:43](https://www.youtube.com/watch?v=IPre5287P3I&t=343.68s)]
*  chips, if you want that at scale, you have to pre-reserve capacity. So often these are negotiations [[00:05:50](https://www.youtube.com/watch?v=IPre5287P3I&t=350.56s)]
*  between a company and the large cloud where you say, okay, I need this many chips for this amount [[00:05:55](https://www.youtube.com/watch?v=IPre5287P3I&t=355.59999999999997s)]
*  of time. What they'll often ask for is they ask for a certain time commitment. So I'll be like, [[00:06:00](https://www.youtube.com/watch?v=IPre5287P3I&t=360.4s)]
*  okay, we can give you this many chips, but we want you to sign basically that you get them [[00:06:04](https://www.youtube.com/watch?v=IPre5287P3I&t=364.4s)]
*  exclusively for two years and you pay for that amount. But we've seen, I mean, I think OpenAI [[00:06:08](https://www.youtube.com/watch?v=IPre5287P3I&t=368.23999999999995s)]
*  was in the news with that, right? Where you have investment deals where, for example, a cloud [[00:06:13](https://www.youtube.com/watch?v=IPre5287P3I&t=373.67999999999995s)]
*  provider comes in and invests in a company as a result of the company gets capacity. So we're [[00:06:17](https://www.youtube.com/watch?v=IPre5287P3I&t=377.28s)]
*  seeing all kinds of deals being struck as with any scarce resource, right? There's a lot of [[00:06:22](https://www.youtube.com/watch?v=IPre5287P3I&t=382.23999999999995s)]
*  deal making going on. And it's not just a matter of getting access to compute. It's about ensuring [[00:06:27](https://www.youtube.com/watch?v=IPre5287P3I&t=387.03999999999996s)]
*  you get access to the compute tailored to your needs. And cost is not the only factor. What would [[00:06:32](https://www.youtube.com/watch?v=IPre5287P3I&t=392.16s)]
*  you say in terms of the considerations that they should be keeping in mind? Really, how much should [[00:06:37](https://www.youtube.com/watch?v=IPre5287P3I&t=397.84000000000003s)]
*  founders know about hardware? And again, selecting which hardware to use? Fantastic question. I mean, [[00:06:43](https://www.youtube.com/watch?v=IPre5287P3I&t=403.44s)]
*  I think the first question honestly, I would ask is, do you really need to consume the hardware [[00:06:50](https://www.youtube.com/watch?v=IPre5287P3I&t=410.72s)]
*  directly? Or do you really just want to consume something that runs on top of the hardware? [[00:06:55](https://www.youtube.com/watch?v=IPre5287P3I&t=415.76000000000005s)]
*  Right. Today, let's take an example. If I want to generate images with stable diffusion for my, [[00:06:59](https://www.youtube.com/watch?v=IPre5287P3I&t=419.84000000000003s)]
*  for example, for my mobile phone app or something like that, it might be easier to go to a SaaS [[00:07:04](https://www.youtube.com/watch?v=IPre5287P3I&t=424.96s)]
*  company like Replicate, for example, that essentially will host the model for you, [[00:07:11](https://www.youtube.com/watch?v=IPre5287P3I&t=431.12s)]
*  where you just pay for access to the model and they send you the generated images. And they will [[00:07:14](https://www.youtube.com/watch?v=IPre5287P3I&t=434.48s)]
*  manage all the provision of computer infrastructure and will find the GPUs for you. If you do want to [[00:07:21](https://www.youtube.com/watch?v=IPre5287P3I&t=441.28000000000003s)]
*  run your own model, I think my number one advice would be to shop around, right? There's a fair [[00:07:26](https://www.youtube.com/watch?v=IPre5287P3I&t=446.56s)]
*  number of providers. The large clouds, in my experience, are not always the best option, [[00:07:30](https://www.youtube.com/watch?v=IPre5287P3I&t=450.96s)]
*  right? If you price it out, you know, we've seen that the startups typically are more likely to go [[00:07:35](https://www.youtube.com/watch?v=IPre5287P3I&t=455.84s)]
*  with specialized clouds, you know, like CoreWeave or Lambda, right? That they're basically specialized [[00:07:41](https://www.youtube.com/watch?v=IPre5287P3I&t=461.28000000000003s)]
*  in providing AI infrastructure to startups. Shop around, look at the different offers, compare prices. [[00:07:45](https://www.youtube.com/watch?v=IPre5287P3I&t=465.6s)]
*  Yeah. And when you're shopping around, in addition to price, which I feel like is a [[00:07:51](https://www.youtube.com/watch?v=IPre5287P3I&t=471.84000000000003s)]
*  major motivating factor, what other factors are there in terms of these other companies who maybe [[00:07:56](https://www.youtube.com/watch?v=IPre5287P3I&t=476.32s)]
*  aren't the big clouds? How are they differentiating relative to one other? How are they standing out [[00:08:02](https://www.youtube.com/watch?v=IPre5287P3I&t=482.24s)]
*  in that market? Yeah, there's a whole sort of decision tree there. You know, I mean, the first [[00:08:07](https://www.youtube.com/watch?v=IPre5287P3I&t=487.20000000000005s)]
*  thing is one thing that often drives the decision is how much memory do I need in my cards, right? [[00:08:11](https://www.youtube.com/watch?v=IPre5287P3I&t=491.28000000000003s)]
*  If I have a small image model, right, I might be able to work with a more consumer grade card, [[00:08:16](https://www.youtube.com/watch?v=IPre5287P3I&t=496.96s)]
*  which is much cheaper, right, per hour if I have it in a cloud. Versus if I, for example, train [[00:08:22](https://www.youtube.com/watch?v=IPre5287P3I&t=502.15999999999997s)]
*  a large language model, I not only need a card with the most memory I can find, but I probably [[00:08:27](https://www.youtube.com/watch?v=IPre5287P3I&t=507.28s)]
*  want to have as many cards as possible in one server because communication between them matters. [[00:08:32](https://www.youtube.com/watch?v=IPre5287P3I&t=512.64s)]
*  And I may even care about the networking fabric behind it, right? Some of the very large models, [[00:08:37](https://www.youtube.com/watch?v=IPre5287P3I&t=517.4399999999999s)]
*  you actually network constraints in terms of how quickly you can train them. So it really becomes [[00:08:40](https://www.youtube.com/watch?v=IPre5287P3I&t=520.88s)]
*  a question of what's your objective? Is it inference? Is it training? If it's training, [[00:08:46](https://www.youtube.com/watch?v=IPre5287P3I&t=526.4s)]
*  how big is your model, right? And based on that, you figure out what the card is, [[00:08:49](https://www.youtube.com/watch?v=IPre5287P3I&t=529.6s)]
*  what kind of server you need, what kind of fabric you need between those servers. And then you sort [[00:08:52](https://www.youtube.com/watch?v=IPre5287P3I&t=532.8s)]
*  of can decide what the right fit is for your application. Even prior to this AI wave, [[00:08:56](https://www.youtube.com/watch?v=IPre5287P3I&t=536.88s)]
*  compute was a major line item for many software companies. And the calculus of leaning on the [[00:09:01](https://www.youtube.com/watch?v=IPre5287P3I&t=541.76s)]
*  easily accessible cloud versus bringing infrastructure in-house was becoming an [[00:09:07](https://www.youtube.com/watch?v=IPre5287P3I&t=547.04s)]
*  increasingly important consideration. Here is Guido touching further on that very calculus [[00:09:11](https://www.youtube.com/watch?v=IPre5287P3I&t=551.52s)]
*  in today's era and where scale comes into play. Compute is expensive. It's a major line item for [[00:09:16](https://www.youtube.com/watch?v=IPre5287P3I&t=556.88s)]
*  many companies. And this is even before the AI revolution. Yeah. Even more so today. Yeah. So [[00:09:22](https://www.youtube.com/watch?v=IPre5287P3I&t=562.56s)]
*  do you think, how do you think about, again, like how that impacts different companies' bottom lines [[00:09:28](https://www.youtube.com/watch?v=IPre5287P3I&t=568.48s)]
*  and whether they really factor that in to having their own, I guess, like allocated GPUs versus [[00:09:33](https://www.youtube.com/watch?v=IPre5287P3I&t=573.04s)]
*  using something more like replicate? You really have to figure out, you know, what is the right [[00:09:39](https://www.youtube.com/watch?v=IPre5287P3I&t=579.68s)]
*  fit for you. And it probably depends a lot on the scale at which you need them, right? If you need [[00:09:44](https://www.youtube.com/watch?v=IPre5287P3I&t=584.4s)]
*  a lot, you frankly, you have to pre-reserve them, right? You have to have your own. There's just no [[00:09:48](https://www.youtube.com/watch?v=IPre5287P3I&t=588.7199999999999s)]
*  way around that. You need a smaller quantity. You may be able to reserve them on a more short-term [[00:09:52](https://www.youtube.com/watch?v=IPre5287P3I&t=592.7199999999999s)]
*  basis. Or, you know, you have various models where it can consume only while your application runs, [[00:09:58](https://www.youtube.com/watch?v=IPre5287P3I&t=598.4799999999999s)]
*  but at a higher price, right? And so this really comes down to what kind of load do you have, [[00:10:04](https://www.youtube.com/watch?v=IPre5287P3I&t=604.96s)]
*  right? What we're typically seeing if somebody is training, they're more likely to do a long-term [[00:10:09](https://www.youtube.com/watch?v=IPre5287P3I&t=609.44s)]
*  reservation for a GPU because you want to make sure you have access to it. If somebody has, [[00:10:13](https://www.youtube.com/watch?v=IPre5287P3I&t=613.76s)]
*  there's a more continuous workloads where availability is important. Like if I just do [[00:10:18](https://www.youtube.com/watch?v=IPre5287P3I&t=618.88s)]
*  inference, but, you know, I want to make 100% sure that if a request comes in, I can service it. I [[00:10:22](https://www.youtube.com/watch?v=IPre5287P3I&t=622.72s)]
*  can never be down, right? I probably need to reserve capacity as well. On the other hand, [[00:10:26](https://www.youtube.com/watch?v=IPre5287P3I&t=626.5600000000001s)]
*  if I have more batch jobs where it's like, ah, this job runs an hour later, that's not the end [[00:10:30](https://www.youtube.com/watch?v=IPre5287P3I&t=630.72s)]
*  of the world, then you probably can go with variable capacity and, you know, just reserve it [[00:10:34](https://www.youtube.com/watch?v=IPre5287P3I&t=634.8s)]
*  ad hoc. But it's really a conversation of what is your usage pattern, what is your demand pattern, [[00:10:38](https://www.youtube.com/watch?v=IPre5287P3I&t=638.88s)]
*  and from that comes, you know, the best pick for the partners that you work with. [[00:10:43](https://www.youtube.com/watch?v=IPre5287P3I&t=643.92s)]
*  We've seen that companies even prior to AI have benefited from building their own infrastructure [[00:10:48](https://www.youtube.com/watch?v=IPre5287P3I&t=648.4s)]
*  by basically bringing that in-house because before that they were renting and they were [[00:10:55](https://www.youtube.com/watch?v=IPre5287P3I&t=655.12s)]
*  paying a lot to rent that compute. Do you think that will be a differentiator for companies [[00:11:00](https://www.youtube.com/watch?v=IPre5287P3I&t=660.08s)]
*  moving forward or how should founders be thinking about that relationship between [[00:11:05](https://www.youtube.com/watch?v=IPre5287P3I&t=665.6s)]
*  owning the infrastructure and renting it? Owning the infrastructure comes with cost as well, [[00:11:10](https://www.youtube.com/watch?v=IPre5287P3I&t=670.08s)]
*  right? Because you need to, you know, now hire people that run it, right? You need to [[00:11:15](https://www.youtube.com/watch?v=IPre5287P3I&t=675.92s)]
*  get money for the CapEx and so on. So my guess is that most early stage founders and probably [[00:11:19](https://www.youtube.com/watch?v=IPre5287P3I&t=679.84s)]
*  even most mid-stage and late stage founders, you know, are better off by renting capacity, [[00:11:26](https://www.youtube.com/watch?v=IPre5287P3I&t=686.32s)]
*  renting a cloud or using a SaaS service, right? There's a couple of exceptions. If you have [[00:11:31](https://www.youtube.com/watch?v=IPre5287P3I&t=691.7600000000001s)]
*  really, really specialized needs, right, you may just not find anybody who has exactly the kind [[00:11:38](https://www.youtube.com/watch?v=IPre5287P3I&t=698.4000000000001s)]
*  of hardware that you need, right? There might be some cases where you have geopolitical concerns, [[00:11:43](https://www.youtube.com/watch?v=IPre5287P3I&t=703.0400000000001s)]
*  you know, your data is just too sensitive, you need to run your own data center, [[00:11:47](https://www.youtube.com/watch?v=IPre5287P3I&t=707.2800000000001s)]
*  and there's probably a certain scale where it makes sense for you to run your own data center, [[00:11:52](https://www.youtube.com/watch?v=IPre5287P3I&t=712.08s)]
*  but it's a pretty large scale, right? If you're spending $10 million a year, [[00:11:55](https://www.youtube.com/watch?v=IPre5287P3I&t=715.76s)]
*  you're probably still under critical, right? If you're spending $100 million a year on [[00:11:59](https://www.youtube.com/watch?v=IPre5287P3I&t=719.04s)]
*  infrastructure, that may be a reason to look into options for your own data center. [[00:12:02](https://www.youtube.com/watch?v=IPre5287P3I&t=722.24s)]
*  But if everyone is competing for the same compute, are there other ways to stand out? [[00:12:06](https://www.youtube.com/watch?v=IPre5287P3I&t=726.8s)]
*  Where's the moat here? Or you could say a moat is getting access to different training data, [[00:12:11](https://www.youtube.com/watch?v=IPre5287P3I&t=731.92s)]
*  but that actually doesn't necessarily have to do with compute or money being thrown at the problem, [[00:12:17](https://www.youtube.com/watch?v=IPre5287P3I&t=737.04s)]
*  it's getting access to differentiated data. If you have access to differentiated data, [[00:12:24](https://www.youtube.com/watch?v=IPre5287P3I&t=744.08s)]
*  that could be a moat. I mean, it's a bit more subtle because, look, if you had an area where [[00:12:28](https://www.youtube.com/watch?v=IPre5287P3I&t=748.4000000000001s)]
*  there's just not much public training data, that's probably right, right? You know, and there might [[00:12:36](https://www.youtube.com/watch?v=IPre5287P3I&t=756.48s)]
*  be areas like in finance or so where that's the case. But for, for example, for a large language [[00:12:40](https://www.youtube.com/watch?v=IPre5287P3I&t=760.5600000000001s)]
*  model, it turns out that just making a larger model and training on more data has more benefits [[00:12:45](https://www.youtube.com/watch?v=IPre5287P3I&t=765.9200000000001s)]
*  than just absorbing more knowledge. It also means that it's better in reasoning and understanding [[00:12:51](https://www.youtube.com/watch?v=IPre5287P3I&t=771.76s)]
*  abstract context and then answering really complex multi-stage questions and so on. [[00:12:56](https://www.youtube.com/watch?v=IPre5287P3I&t=776.88s)]
*  So probably, if I have to guess, I think the future will be that we'll still train on all the data we [[00:13:02](https://www.youtube.com/watch?v=IPre5287P3I&t=782.16s)]
*  can find, right? And then maybe you fine tune, meaning you yourself do some additional training [[00:13:07](https://www.youtube.com/watch?v=IPre5287P3I&t=787.52s)]
*  on a particular problem domain with your private data, if that makes sense. So you first go to [[00:13:12](https://www.youtube.com/watch?v=IPre5287P3I&t=792.24s)]
*  elementary school to learn reading and writing, and then you go to your vocational training for [[00:13:17](https://www.youtube.com/watch?v=IPre5287P3I&t=797.44s)]
*  the specialized job that you're going to do in the future. Another important question worth [[00:13:23](https://www.youtube.com/watch?v=IPre5287P3I&t=803.12s)]
*  addressing is who can realistically compete? If compute is expensive, will all the largest, [[00:13:28](https://www.youtube.com/watch?v=IPre5287P3I&t=808.32s)]
*  most heavily capitalized companies win since they can build the largest models with the most data? [[00:13:34](https://www.youtube.com/watch?v=IPre5287P3I&t=814.72s)]
*  Or what role does open source play? As one of many emerging examples, Vicuña was created by fine [[00:13:39](https://www.youtube.com/watch?v=IPre5287P3I&t=819.9200000000001s)]
*  tuning Meta's LamaOne model for chat. The cost of fine tuning added only an additional $300, [[00:13:46](https://www.youtube.com/watch?v=IPre5287P3I&t=826.24s)]
*  but the result is competitive with much larger models like chat gbt or bard. So what might this [[00:13:53](https://www.youtube.com/watch?v=IPre5287P3I&t=833.92s)]
*  example and a growing number of open source projects tell us about the future of open LLMs? [[00:14:00](https://www.youtube.com/watch?v=IPre5287P3I&t=840.96s)]
*  So first of all, the in general larger models, if they're, you know, everything else being equal [[00:14:07](https://www.youtube.com/watch?v=IPre5287P3I&t=847.52s)]
*  perform better. Right. So the really small open source models that we're seeing out there today, [[00:14:13](https://www.youtube.com/watch?v=IPre5287P3I&t=853.36s)]
*  they're not yet at a level of a gbt 3.5 or gbt4. And there's actually a website that runs sort of [[00:14:19](https://www.youtube.com/watch?v=IPre5287P3I&t=859.28s)]
*  regular bake offs where they basically ask users to prepare answers. And you know, it seems to be [[00:14:25](https://www.youtube.com/watch?v=IPre5287P3I&t=865.84s)]
*  pretty clear that the large ones are still a little bit ahead. That said, we're making big advances [[00:14:30](https://www.youtube.com/watch?v=IPre5287P3I&t=870.5600000000001s)]
*  there. We're figuring out a couple of things. So one thing we've learned is there's something [[00:14:36](https://www.youtube.com/watch?v=IPre5287P3I&t=876.48s)]
*  called the chinchilla scaling loss that basically gives us an idea how does data [[00:14:41](https://www.youtube.com/watch?v=IPre5287P3I&t=881.2s)]
*  correspond to model size. And if we over train, so don't train as efficiently as we could, [[00:14:45](https://www.youtube.com/watch?v=IPre5287P3I&t=885.36s)]
*  we can actually get potentially a smaller and better model. So you can match the performance [[00:14:50](https://www.youtube.com/watch?v=IPre5287P3I&t=890.96s)]
*  of a large model with a smaller model if you train it more. So that's interesting. That reduces model [[00:14:55](https://www.youtube.com/watch?v=IPre5287P3I&t=895.6s)]
*  sizes and the trend at the moment is to make slightly smaller models and train them more to [[00:14:59](https://www.youtube.com/watch?v=IPre5287P3I&t=899.76s)]
*  get equal performance. The second thing is that when we talk about models for slightly [[00:15:05](https://www.youtube.com/watch?v=IPre5287P3I&t=905.52s)]
*  different purposes, you have the base large language models. All they're trained and [[00:15:13](https://www.youtube.com/watch?v=IPre5287P3I&t=913.6s)]
*  practically speaking is completing text. Literally how you train them is you give them text and say, [[00:15:17](https://www.youtube.com/watch?v=IPre5287P3I&t=917.92s)]
*  yes, the next letter. And then you tell them, nope, that was wrong or yes, that was right. [[00:15:22](https://www.youtube.com/watch?v=IPre5287P3I&t=922.0s)]
*  And does it back propagate how they predict? And they're really good at that completing text. [[00:15:25](https://www.youtube.com/watch?v=IPre5287P3I&t=925.04s)]
*  That's not quite the same that you want from a chat bot or from a model that you can tell to [[00:15:33](https://www.youtube.com/watch?v=IPre5287P3I&t=933.36s)]
*  do something. So there's usually another step afterwards, which is called fine-tuning for [[00:15:38](https://www.youtube.com/watch?v=IPre5287P3I&t=938.3199999999999s)]
*  instruction following or for chat specifically. Where basically I tell a model, look, if somebody [[00:15:43](https://www.youtube.com/watch?v=IPre5287P3I&t=943.6s)]
*  asks you to come up with a list of steps how to make pizza, this is roughly what I expect you to [[00:15:49](https://www.youtube.com/watch?v=IPre5287P3I&t=949.6s)]
*  answer. These models are very good in learning these things. So whether you first train them, [[00:15:56](https://www.youtube.com/watch?v=IPre5287P3I&t=956.96s)]
*  just complete text, and then you train them how to react to human requests and instructions. [[00:16:01](https://www.youtube.com/watch?v=IPre5287P3I&t=961.52s)]
*  That's called instruction fine-tuning. Lama, for example, that was a Facebook model where they [[00:16:09](https://www.youtube.com/watch?v=IPre5287P3I&t=969.4399999999999s)]
*  published the weights for researchers. And then some people took that and they fine-tuned it, [[00:16:14](https://www.youtube.com/watch?v=IPre5287P3I&t=974.48s)]
*  meaning they took a bunch of instructions for things to turn it into a taco or vicuña, [[00:16:18](https://www.youtube.com/watch?v=IPre5287P3I&t=978.24s)]
*  which is a much, much nicer model in terms of interacting with it. For humans, it's much, [[00:16:23](https://www.youtube.com/watch?v=IPre5287P3I&t=983.6s)]
*  much more useful. And so the biggest challenge at the moment we have in the open source side is [[00:16:27](https://www.youtube.com/watch?v=IPre5287P3I&t=987.2s)]
*  there's currently no large open source LLM out there. GPT-3 was 175 billion parameters. [[00:16:31](https://www.youtube.com/watch?v=IPre5287P3I&t=991.6800000000001s)]
*  There's currently nothing in that weight class that's open source and that people could use to [[00:16:42](https://www.youtube.com/watch?v=IPre5287P3I&t=1002.08s)]
*  fine-tune or to play with or to modify. It's worth noting that since this recording, [[00:16:46](https://www.youtube.com/watch?v=IPre5287P3I&t=1006.4000000000001s)]
*  several more open source models have been released, including Lama 2 with 70 billion parameters [[00:16:51](https://www.youtube.com/watch?v=IPre5287P3I&t=1011.44s)]
*  and an open license, unlike its predecessor, Lama 1. Another 40 billion parameter open source [[00:16:57](https://www.youtube.com/watch?v=IPre5287P3I&t=1017.7600000000001s)]
*  model, Falcon, was released as well. But both of these are still dwarfed in parameters compared [[00:17:03](https://www.youtube.com/watch?v=IPre5287P3I&t=1023.84s)]
*  to closed models like OpenAI's GPT-3 at 175 billion parameters or GPT-4 at an estimated 1.8 [[00:17:09](https://www.youtube.com/watch?v=IPre5287P3I&t=1029.52s)]
*  trillion parameters, although the latter is speculated to be a collection of multiple smaller [[00:17:19](https://www.youtube.com/watch?v=IPre5287P3I&t=1039.68s)]
*  models. However, parameter count isn't the only driver of performance. For example, while Lama 2 [[00:17:25](https://www.youtube.com/watch?v=IPre5287P3I&t=1045.28s)]
*  has fewer parameters than GPT-3, its performance is actually much better due to being trained on [[00:17:31](https://www.youtube.com/watch?v=IPre5287P3I&t=1051.92s)]
*  more data. In fact, Lama 2 is currently comparable to GPT-3's successor, GPT-3 3.5, the current [[00:17:38](https://www.youtube.com/watch?v=IPre5287P3I&t=1058.0800000000002s)]
*  default for chat GPT. And as these models continue to get larger, we may actually see some models [[00:17:46](https://www.youtube.com/watch?v=IPre5287P3I&t=1066.48s)]
*  compress, becoming more efficient in enabling inference on your device. Stable diffusion can [[00:17:53](https://www.youtube.com/watch?v=IPre5287P3I&t=1073.28s)]
*  run on your computer's GPU. Do we expect to see more of that? Because right now they are all [[00:17:59](https://www.youtube.com/watch?v=IPre5287P3I&t=1079.76s)]
*  hosted by these companies, right? They're trained by these companies on their dedicated servers. And [[00:18:04](https://www.youtube.com/watch?v=IPre5287P3I&t=1084.48s)]
*  then even if you interface with chat GPT, it's running that inference for you. Do we expect to [[00:18:09](https://www.youtube.com/watch?v=IPre5287P3I&t=1089.1200000000001s)]
*  see that change at all as compute becomes cheaper, maybe more decentralized? Or how would you think [[00:18:15](https://www.youtube.com/watch?v=IPre5287P3I&t=1095.44s)]
*  about that? That's a really good question. And we're speculating a little bit here, but my guess is [[00:18:20](https://www.youtube.com/watch?v=IPre5287P3I&t=1100.16s)]
*  we will, right? And we're seeing some of these smaller models getting pretty good. They run on [[00:18:25](https://www.youtube.com/watch?v=IPre5287P3I&t=1105.28s)]
*  your laptop or even your phone. We're starting to see stable diffusion implementations that run [[00:18:30](https://www.youtube.com/watch?v=IPre5287P3I&t=1110.64s)]
*  well on phones, which I would have never thought. And they take a couple of 10 seconds to create an [[00:18:36](https://www.youtube.com/watch?v=IPre5287P3I&t=1116.48s)]
*  image, which is comparatively slow. But there's certain applications where that's [[00:18:40](https://www.youtube.com/watch?v=IPre5287P3I&t=1120.88s)]
*  acceptable. So my guess is as both the devices get faster and the models get more optimized, [[00:18:47](https://www.youtube.com/watch?v=IPre5287P3I&t=1127.44s)]
*  right? This will be a trend that we see more and more. And in the future, it might just be part of [[00:18:54](https://www.youtube.com/watch?v=IPre5287P3I&t=1134.3200000000002s)]
*  the operating system to have a large language model, a basic image generation model. We've [[00:18:58](https://www.youtube.com/watch?v=IPre5287P3I&t=1138.72s)]
*  talked about how expensive compute can be and how ultimately that can be a major line item for [[00:19:03](https://www.youtube.com/watch?v=IPre5287P3I&t=1143.1200000000001s)]
*  companies. And I guess probably the model training will remain with those companies and not [[00:19:07](https://www.youtube.com/watch?v=IPre5287P3I&t=1147.52s)]
*  necessarily on folks' devices. But in terms of the inference, I assume that's still a pretty [[00:19:13](https://www.youtube.com/watch?v=IPre5287P3I&t=1153.84s)]
*  significant cost. And in a way, if someone is able to run that locally, doesn't that disjoint the [[00:19:19](https://www.youtube.com/watch?v=IPre5287P3I&t=1159.84s)]
*  company from having to pay for that compute because it's running on, let's say, someone's [[00:19:25](https://www.youtube.com/watch?v=IPre5287P3I&t=1165.6s)]
*  MacBook GPU? Oh, yeah, totally. I mean, look, if I can generate an image of my phone directly, [[00:19:30](https://www.youtube.com/watch?v=IPre5287P3I&t=1170.48s)]
*  all it takes is some battery power and it gets a little warm, right? And that's it, right? So [[00:19:35](https://www.youtube.com/watch?v=IPre5287P3I&t=1175.6799999999998s)]
*  that's a huge advantage. At the same time, there's probably going to be a little bit bifurcation there [[00:19:40](https://www.youtube.com/watch?v=IPre5287P3I&t=1180.56s)]
*  around quality and parameters, right? You can run things locally, but you can probably run them a [[00:19:44](https://www.youtube.com/watch?v=IPre5287P3I&t=1184.8799999999999s)]
*  lot better in the cloud, right? Because you have a much bigger server there. So it probably depends [[00:19:49](https://www.youtube.com/watch?v=IPre5287P3I&t=1189.9199999999998s)]
*  a little bit on what you want to do. If I just want to have a better spell checker that checks [[00:19:53](https://www.youtube.com/watch?v=IPre5287P3I&t=1193.76s)]
*  my email or maybe just some simple completion, that's perfectly fine. I can run that on my phone. [[00:19:58](https://www.youtube.com/watch?v=IPre5287P3I&t=1198.32s)]
*  On the other hand, if I want something that is more, you know, write a good speech or, you know, [[00:20:04](https://www.youtube.com/watch?v=IPre5287P3I&t=1204.16s)]
*  summarize a complex text, they might be like, oh, that I'm going to run the cloud because it takes [[00:20:10](https://www.youtube.com/watch?v=IPre5287P3I&t=1210.5600000000002s)]
*  so many more operations. Hopefully, this is getting your wheels spinning in terms of what [[00:20:14](https://www.youtube.com/watch?v=IPre5287P3I&t=1214.8000000000002s)]
*  can be built. And here is Guido speaking to how this presents a fundamentally new stack [[00:20:18](https://www.youtube.com/watch?v=IPre5287P3I&t=1218.5600000000002s)]
*  and what that means in terms of opportunity. It feels like this really is like this massive wave, [[00:20:24](https://www.youtube.com/watch?v=IPre5287P3I&t=1224.4s)]
*  this renaissance of innovation. It's full of opportunities, right? I mean, we're rebuilding [[00:20:29](https://www.youtube.com/watch?v=IPre5287P3I&t=1229.6s)]
*  a stack. You know, you can look at AI just as a new application, but honestly, I think it's probably [[00:20:35](https://www.youtube.com/watch?v=IPre5287P3I&t=1235.12s)]
*  a better way to look at it as a different type of compute, right? We traditionally build software by [[00:20:40](https://www.youtube.com/watch?v=IPre5287P3I&t=1240.48s)]
*  composing algorithms in a way that we understand well and where, you know, the end result was, [[00:20:46](https://www.youtube.com/watch?v=IPre5287P3I&t=1246.1599999999999s)]
*  you know, a program, those bottoms up constructed. Now we have a second type compute, you know, [[00:20:52](https://www.youtube.com/watch?v=IPre5287P3I&t=1252.24s)]
*  where we just trained a large neural network. And the big advantage is we don't actually need [[00:20:56](https://www.youtube.com/watch?v=IPre5287P3I&t=1256.48s)]
*  to know how to solve a problem. As long as the network can figure it out, right, the neural [[00:20:59](https://www.youtube.com/watch?v=IPre5287P3I&t=1259.6s)]
*  network can figure it out. We're fine. And that opens up a bunch of new applications, [[00:21:03](https://www.youtube.com/watch?v=IPre5287P3I&t=1263.44s)]
*  but it also means, you know, you need a completely different stack in terms of all the different [[00:21:08](https://www.youtube.com/watch?v=IPre5287P3I&t=1268.48s)]
*  pieces, right? You probably want vector DBs to retrieve context. You know, you want different [[00:21:12](https://www.youtube.com/watch?v=IPre5287P3I&t=1272.24s)]
*  types of hosting providers that are good in hosting these models and providing them to you as a [[00:21:17](https://www.youtube.com/watch?v=IPre5287P3I&t=1277.1200000000001s)]
*  service. It's a whole, like, you know, Cambrian explosion of creativity as a whole new ecosystem [[00:21:22](https://www.youtube.com/watch?v=IPre5287P3I&t=1282.4s)]
*  forming. And I think there's a ton of opportunities to build right companies. [[00:21:28](https://www.youtube.com/watch?v=IPre5287P3I&t=1288.5600000000002s)]
*  I think that paints a pretty incredible picture of opportunity across the stack. [[00:21:31](https://www.youtube.com/watch?v=IPre5287P3I&t=1291.52s)]
*  And as many of these trends continue to progress, like supply and demand, the calculus of renting [[00:21:36](https://www.youtube.com/watch?v=IPre5287P3I&t=1296.48s)]
*  versus owning compute, closed versus open source models, we looked at part three of the series to [[00:21:41](https://www.youtube.com/watch?v=IPre5287P3I&t=1301.8400000000001s)]
*  answer a very important question. How much does all of this cost? We'll explore all this in depth, [[00:21:47](https://www.youtube.com/watch?v=IPre5287P3I&t=1307.2s)]
*  including how much startups are really spending on AI compute and whether that's sustainable, [[00:21:54](https://www.youtube.com/watch?v=IPre5287P3I&t=1314.0s)]
*  how much it really costs to train a model like GPT-3, the difference in cost between training [[00:21:58](https://www.youtube.com/watch?v=IPre5287P3I&t=1318.64s)]
*  and inference, and how all of this will change with time. We will see you there. Thank you so [[00:22:03](https://www.youtube.com/watch?v=IPre5287P3I&t=1323.68s)]
*  much for listening to the A16Z podcast. What we're trying to do here is provide an informed, [[00:22:10](https://www.youtube.com/watch?v=IPre5287P3I&t=1330.4s)]
*  clear-eyed, but also optimistic take on technology and its future. And we're trying to do that by [[00:22:16](https://www.youtube.com/watch?v=IPre5287P3I&t=1336.08s)]
*  featuring some of the most inspiring people in the things that they're building. So if that is [[00:22:22](https://www.youtube.com/watch?v=IPre5287P3I&t=1342.1599999999999s)]
*  interesting to you and you'd like to join us on this journey, go ahead and click subscribe and make [[00:22:28](https://www.youtube.com/watch?v=IPre5287P3I&t=1348.0s)]
*  sure to let us know in the comments below what you'd like to see us cover next. Thank you so [[00:22:32](https://www.youtube.com/watch?v=IPre5287P3I&t=1352.8799999999999s)]
*  much for listening and we'll see you next time. [[00:22:37](https://www.youtube.com/watch?v=IPre5287P3I&t=1357.4399999999998s)]
