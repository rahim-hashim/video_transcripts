---
Date Generated: May 24, 2025
Transcription Model: whisper medium 20231117
Length: 929s
Video Keywords: []
Video Views: 8086
Video Rating: None
Video Description: As the world generates more data, unlocking the full potential of AI means a constant need for faster and more resilient hardware. 

But how much does this all really cost? In this final segment of our AI hardware series, we tackle that question head on. 

Be sure to check part 1 and 2, where we explore the emerging architectures and the momentous competition for AI hardware.

Topics Covered:
00:00 – The cost of compute
02:30 – Is this sustainable?
03:46 – The cost to train a model
05:58 – Computation requirements
09:58  – The relationship between compute, capital, and technology
13:31 – GPT4 commenting on the technology with help from ElevenLabs

Resources: 
Find Guido on LinkedIn: https://www.linkedin.com/in/appenz/
Find Guido on Twitter: https://twitter.com/appenz
 
Find a16z on Twitter: https://twitter.com/a16z 
Find a16z on LinkedIn: https://www.linkedin.com/company/a16z 
Subscribe on your favorite podcast app: https://a16z.simplecast.com/ 
Follow our host: https://twitter.com/stephsmithio 

Please note that the content here is for informational purposes only; should NOT be taken as legal, business, tax, or investment advice or be used to evaluate any investment or security; and is not directed at any investors or potential investors in any a16z fund. a16z and its affiliates may maintain investments in the companies discussed. For more details please see a16z.com/disclosures.
---

# The True Cost of Compute
**The a16z Podcast:** [September 01, 2023](https://www.youtube.com/watch?v=MNFeJNUu074)
*  There's very few computational problems that complex that mankind has undertaken. [[00:00:00](https://www.youtube.com/watch?v=MNFeJNUu074&t=0.0s)]
*  So if napkin math, 175 billion parameters, 350 billion floating point operations, [[00:00:04](https://www.youtube.com/watch?v=MNFeJNUu074&t=4.64s)]
*  three times 10 to the 23. That's a completely crazy number. [[00:00:09](https://www.youtube.com/watch?v=MNFeJNUu074&t=9.92s)]
*  Got it. Got it. [[00:00:13](https://www.youtube.com/watch?v=MNFeJNUu074&t=13.84s)]
*  Expectation at the moment is that the cost for training these models [[00:00:14](https://www.youtube.com/watch?v=MNFeJNUu074&t=14.96s)]
*  may actually sort of top out or even go down a little bit. [[00:00:19](https://www.youtube.com/watch?v=MNFeJNUu074&t=19.28s)]
*  How do you think about the relationship between compute, capital, [[00:00:22](https://www.youtube.com/watch?v=MNFeJNUu074&t=22.8s)]
*  and then the technology that we have today? [[00:00:26](https://www.youtube.com/watch?v=MNFeJNUu074&t=26.64s)]
*  Yeah, that's a million dollar question or maybe trillion dollar question. I don't know. [[00:00:28](https://www.youtube.com/watch?v=MNFeJNUu074&t=28.96s)]
*  With software becoming more important than ever, hardware is following suit. [[00:00:35](https://www.youtube.com/watch?v=MNFeJNUu074&t=35.52s)]
*  And with the world constantly generating more data, unlocking the full potential of AI [[00:00:40](https://www.youtube.com/watch?v=MNFeJNUu074&t=40.88s)]
*  means a constant need for faster and more resilient hardware. [[00:00:46](https://www.youtube.com/watch?v=MNFeJNUu074&t=46.24s)]
*  But how much does all of this really cost? [[00:00:50](https://www.youtube.com/watch?v=MNFeJNUu074&t=50.480000000000004s)]
*  In this final segment of our AI hardware series, we tackle that question, [[00:00:54](https://www.youtube.com/watch?v=MNFeJNUu074&t=54.08s)]
*  head on. But if you're just catching up, be sure to check out part one and part two, [[00:00:58](https://www.youtube.com/watch?v=MNFeJNUu074&t=58.8s)]
*  where we explored the emerging architectures and the momentous competition for AI hardware. [[00:01:04](https://www.youtube.com/watch?v=MNFeJNUu074&t=64.08s)]
*  And today we're joined again by A16Z special advisor, [[00:01:10](https://www.youtube.com/watch?v=MNFeJNUu074&t=70.0s)]
*  Guido Appenzeller, someone who is uniquely suited for this deep dive [[00:01:13](https://www.youtube.com/watch?v=MNFeJNUu074&t=73.67999999999999s)]
*  as a storied infrastructure expert. [[00:01:17](https://www.youtube.com/watch?v=MNFeJNUu074&t=77.52s)]
*  CTO for Intel's data center group dealing a lot with hardware and the low level components. [[00:01:19](https://www.youtube.com/watch?v=MNFeJNUu074&t=79.67999999999999s)]
*  So it's given me sort of, I think, a good insight how large data centers work, [[00:01:24](https://www.youtube.com/watch?v=MNFeJNUu074&t=84.08s)]
*  you know, what the basic components are that make all of this AI boom possible today. [[00:01:28](https://www.youtube.com/watch?v=MNFeJNUu074&t=88.08s)]
*  Here is Guido touching on the reality of these models and how much they cost today. [[00:01:34](https://www.youtube.com/watch?v=MNFeJNUu074&t=94.96s)]
*  Training one of these large language models today is it's not a hundred thousand dollar thing, [[00:01:39](https://www.youtube.com/watch?v=MNFeJNUu074&t=99.76s)]
*  it's probably millions of dollars thing. Practically speaking, what we're seeing in industry [[00:01:44](https://www.youtube.com/watch?v=MNFeJNUu074&t=104.0s)]
*  is that it's actually more for tens of millions of dollars thing. [[00:01:50](https://www.youtube.com/watch?v=MNFeJNUu074&t=110.64s)]
*  As a reminder, the content here is for informational purposes only. [[00:01:54](https://www.youtube.com/watch?v=MNFeJNUu074&t=114.08s)]
*  Should not be taken as legal, business, tax or investment advice, [[00:01:57](https://www.youtube.com/watch?v=MNFeJNUu074&t=117.84s)]
*  or be used to evaluate any investment or security, [[00:02:01](https://www.youtube.com/watch?v=MNFeJNUu074&t=121.2s)]
*  and is not directed at any investors or potential investors in any A16Z fund. [[00:02:03](https://www.youtube.com/watch?v=MNFeJNUu074&t=123.76s)]
*  Please note that A16Z and its affiliates may also maintain investments [[00:02:08](https://www.youtube.com/watch?v=MNFeJNUu074&t=128.72s)]
*  in the companies discussed in this podcast. [[00:02:12](https://www.youtube.com/watch?v=MNFeJNUu074&t=132.64000000000001s)]
*  For more details, including a link to our investments, please see a16z.com slash disclosures. [[00:02:15](https://www.youtube.com/watch?v=MNFeJNUu074&t=135.2s)]
*  In Guido's recent article, Navigating the High Cost of AI Compute, Guido even noted that access [[00:02:27](https://www.youtube.com/watch?v=MNFeJNUu074&t=147.84s)]
*  to compute resources has become a determining factor for the success of AI companies. [[00:02:36](https://www.youtube.com/watch?v=MNFeJNUu074&t=156.8s)]
*  And this is not just true for the largest companies building the largest models. [[00:02:42](https://www.youtube.com/watch?v=MNFeJNUu074&t=162.64000000000001s)]
*  In fact, many companies are spending more than 80% of their total capital raised on compute resources. [[00:02:46](https://www.youtube.com/watch?v=MNFeJNUu074&t=166.72s)]
*  We've specifically seen that with founders that want to train their own models, right, [[00:02:53](https://www.youtube.com/watch?v=MNFeJNUu074&t=173.68s)]
*  which is extremely expensive. [[00:02:56](https://www.youtube.com/watch?v=MNFeJNUu074&t=176.88s)]
*  You have to spend a large chunk of your funding just on compute capacity, right? [[00:02:58](https://www.youtube.com/watch?v=MNFeJNUu074&t=178.07999999999998s)]
*  And they often run with very small lean teams, right? [[00:03:03](https://www.youtube.com/watch?v=MNFeJNUu074&t=183.2s)]
*  That's the plus side. [[00:03:05](https://www.youtube.com/watch?v=MNFeJNUu074&t=185.44s)]
*  Over time, I would expect this to normalize a little bit. [[00:03:06](https://www.youtube.com/watch?v=MNFeJNUu074&t=186.96s)]
*  And it's mostly as you go more from the core technology that you're building over early days [[00:03:09](https://www.youtube.com/watch?v=MNFeJNUu074&t=189.68s)]
*  towards more complete product offering, right? [[00:03:15](https://www.youtube.com/watch?v=MNFeJNUu074&t=195.28s)]
*  There's just a lot more boxes to check and, you know, features to implement [[00:03:18](https://www.youtube.com/watch?v=MNFeJNUu074&t=198.48s)]
*  and all the administrative parts of your application if you're getting to the enterprise. [[00:03:22](https://www.youtube.com/watch?v=MNFeJNUu074&t=202.4s)]
*  So probably you'll have more normal software development that's not AI, [[00:03:26](https://www.youtube.com/watch?v=MNFeJNUu074&t=206.08s)]
*  right, or classic software development happening. [[00:03:29](https://www.youtube.com/watch?v=MNFeJNUu074&t=209.20000000000002s)]
*  You'll probably also have a larger headcount of people that they have to pay. [[00:03:31](https://www.youtube.com/watch?v=MNFeJNUu074&t=211.44000000000003s)]
*  So at the end of the day, I would expect as a percentage that they'll go down over time, right? [[00:03:36](https://www.youtube.com/watch?v=MNFeJNUu074&t=216.08s)]
*  As an absolute amount, I think it'll be going up for some time, [[00:03:41](https://www.youtube.com/watch?v=MNFeJNUu074&t=221.28s)]
*  just because this AI boom is still just in its infancy. [[00:03:43](https://www.youtube.com/watch?v=MNFeJNUu074&t=223.84s)]
*  The AI boom has just begun. [[00:03:46](https://www.youtube.com/watch?v=MNFeJNUu074&t=226.72000000000003s)]
*  And in part two, we discussed how it's unlikely for compute demand to subside anytime soon. [[00:03:48](https://www.youtube.com/watch?v=MNFeJNUu074&t=228.64000000000001s)]
*  There, we also discussed how the decision to own or rent infrastructure [[00:03:54](https://www.youtube.com/watch?v=MNFeJNUu074&t=234.64s)]
*  can make a non-trivial difference to a company's bottom line. [[00:03:58](https://www.youtube.com/watch?v=MNFeJNUu074&t=238.79999999999998s)]
*  But there are other considerations when it comes to cost. [[00:04:02](https://www.youtube.com/watch?v=MNFeJNUu074&t=242.95999999999998s)]
*  Batch size, learning rate, and the duration of the training process [[00:04:06](https://www.youtube.com/watch?v=MNFeJNUu074&t=246.95999999999998s)]
*  all contribute to the final price tag. [[00:04:10](https://www.youtube.com/watch?v=MNFeJNUu074&t=250.72s)]
*  How much does it cost to train a model depends on a myriad of factors, right? [[00:04:13](https://www.youtube.com/watch?v=MNFeJNUu074&t=253.67999999999998s)]
*  Now, the good news is we can simplify this a little bit, [[00:04:17](https://www.youtube.com/watch?v=MNFeJNUu074&t=257.84s)]
*  because the vast majority of models that are being used today are transformer models, [[00:04:20](https://www.youtube.com/watch?v=MNFeJNUu074&t=260.08s)]
*  that was the transformer architecture. [[00:04:24](https://www.youtube.com/watch?v=MNFeJNUu074&t=264.15999999999997s)]
*  Huge breakthrough in AI. [[00:04:25](https://www.youtube.com/watch?v=MNFeJNUu074&t=265.76s)]
*  They've proven to be incredibly versatile. [[00:04:27](https://www.youtube.com/watch?v=MNFeJNUu074&t=267.28s)]
*  They're easier to train because they parallelize a little bit better than previous models. [[00:04:28](https://www.youtube.com/watch?v=MNFeJNUu074&t=268.88s)]
*  And so in a transformer, you can sort of approximate the inference time [[00:04:33](https://www.youtube.com/watch?v=MNFeJNUu074&t=273.44s)]
*  as twice the number of parameters at floating point operations, right? [[00:04:38](https://www.youtube.com/watch?v=MNFeJNUu074&t=278.0s)]
*  And the training time is about six times the number of parameters. [[00:04:41](https://www.youtube.com/watch?v=MNFeJNUu074&t=281.52s)]
*  So if you take something like GPT-3, which is the open AI-spec model, [[00:04:45](https://www.youtube.com/watch?v=MNFeJNUu074&t=285.2s)]
*  they have 175 billion parameters. [[00:04:49](https://www.youtube.com/watch?v=MNFeJNUu074&t=289.52s)]
*  So you need twice as much, so 350 billion floating point operations, two to one inference. [[00:04:52](https://www.youtube.com/watch?v=MNFeJNUu074&t=292.88s)]
*  And so based on that, you can sort of figure out how much compute capacity you need, [[00:04:59](https://www.youtube.com/watch?v=MNFeJNUu074&t=299.6s)]
*  how this is going to scale, how you should price it, [[00:05:05](https://www.youtube.com/watch?v=MNFeJNUu074&t=305.04s)]
*  you know how much it'll cost you at the end of the day. [[00:05:08](https://www.youtube.com/watch?v=MNFeJNUu074&t=308.56s)]
*  This also gives you for model training an idea how long the training is going to take. [[00:05:10](https://www.youtube.com/watch?v=MNFeJNUu074&t=310.88s)]
*  You know how much your AI accelerator can do in terms of [[00:05:15](https://www.youtube.com/watch?v=MNFeJNUu074&t=315.12s)]
*  floating point operations per second, right? [[00:05:19](https://www.youtube.com/watch?v=MNFeJNUu074&t=319.52s)]
*  You can sort of theoretically calculate how many operations it is to train your model. [[00:05:21](https://www.youtube.com/watch?v=MNFeJNUu074&t=321.52s)]
*  In practice, the math is more complicated because, you know, [[00:05:26](https://www.youtube.com/watch?v=MNFeJNUu074&t=326.47999999999996s)]
*  there are certain ways to accelerate that. [[00:05:29](https://www.youtube.com/watch?v=MNFeJNUu074&t=329.84s)]
*  So maybe you can train with a reduced precision. [[00:05:31](https://www.youtube.com/watch?v=MNFeJNUu074&t=331.28s)]
*  But it's also very hard to achieve 100% utilization on these cards. [[00:05:33](https://www.youtube.com/watch?v=MNFeJNUu074&t=333.84s)]
*  If you naively implement it, you're probably going to be below 10% utilization. [[00:05:38](https://www.youtube.com/watch?v=MNFeJNUu074&t=338.71999999999997s)]
*  But you know, you can probably get into the tens of percent with a little bit of work. [[00:05:42](https://www.youtube.com/watch?v=MNFeJNUu074&t=342.32s)]
*  This gives you a rough swag, how much capacity you need for training and for inference. [[00:05:46](https://www.youtube.com/watch?v=MNFeJNUu074&t=346.47999999999996s)]
*  But at the end, you probably do want to test it before you make any final decisions on these things. [[00:05:50](https://www.youtube.com/watch?v=MNFeJNUu074&t=350.8s)]
*  And make sure that your assumptions hold on how much you need. [[00:05:55](https://www.youtube.com/watch?v=MNFeJNUu074&t=355.36s)]
*  Now, if all those numbers confused you, that's okay. [[00:05:58](https://www.youtube.com/watch?v=MNFeJNUu074&t=358.88s)]
*  We'll walk through a very specific example. [[00:06:01](https://www.youtube.com/watch?v=MNFeJNUu074&t=361.68s)]
*  GPT-3. [[00:06:04](https://www.youtube.com/watch?v=MNFeJNUu074&t=364.24s)]
*  GPT-3 has about 175 billion parameters. [[00:06:05](https://www.youtube.com/watch?v=MNFeJNUu074&t=365.52000000000004s)]
*  And here's Guido on the computation requirements for training the model and ultimately inference. [[00:06:09](https://www.youtube.com/watch?v=MNFeJNUu074&t=369.68s)]
*  That's when you're prompting the already trained model to elicit a response. [[00:06:14](https://www.youtube.com/watch?v=MNFeJNUu074&t=374.96000000000004s)]
*  So if you just do very naively the math, let's start with training. [[00:06:19](https://www.youtube.com/watch?v=MNFeJNUu074&t=379.28s)]
*  We know how many tokens it was trained on. [[00:06:23](https://www.youtube.com/watch?v=MNFeJNUu074&t=383.2s)]
*  We know how many parameters the model has. [[00:06:24](https://www.youtube.com/watch?v=MNFeJNUu074&t=384.96s)]
*  So we can do a sort of napkin math. [[00:06:27](https://www.youtube.com/watch?v=MNFeJNUu074&t=387.12s)]
*  And you end up with something like 3 times 10 to the 23 floating point operations. [[00:06:29](https://www.youtube.com/watch?v=MNFeJNUu074&t=389.2s)]
*  That's a completely crazy number. [[00:06:34](https://www.youtube.com/watch?v=MNFeJNUu074&t=394.24s)]
*  It's like a number with 23 digits. [[00:06:36](https://www.youtube.com/watch?v=MNFeJNUu074&t=396.08s)]
*  It's like hard to write down. [[00:06:37](https://www.youtube.com/watch?v=MNFeJNUu074&t=397.91999999999996s)]
*  There's very few computational problems that complex that mankind has actually undertaken. [[00:06:40](https://www.youtube.com/watch?v=MNFeJNUu074&t=400.48s)]
*  It's a huge effort. [[00:06:47](https://www.youtube.com/watch?v=MNFeJNUu074&t=407.36s)]
*  Then you can be like, okay, so let's take, say, an A100, the most commonly used card. [[00:06:48](https://www.youtube.com/watch?v=MNFeJNUu074&t=408.32000000000005s)]
*  We know how many floating point operations it can do per second. [[00:06:53](https://www.youtube.com/watch?v=MNFeJNUu074&t=413.04s)]
*  We can divide that. [[00:06:55](https://www.youtube.com/watch?v=MNFeJNUu074&t=415.6s)]
*  That gives us an order of magnitude estimation, like how much time it would take. [[00:06:56](https://www.youtube.com/watch?v=MNFeJNUu074&t=416.8s)]
*  Then we know how much one of these cards costs. [[00:07:01](https://www.youtube.com/watch?v=MNFeJNUu074&t=421.84000000000003s)]
*  Renting an A100 costs you between, I want to say, between $1 and $4, probably, depending on who you [[00:07:04](https://www.youtube.com/watch?v=MNFeJNUu074&t=424.56s)]
*  rent it from. [[00:07:11](https://www.youtube.com/watch?v=MNFeJNUu074&t=431.52000000000004s)]
*  And you end up with something in the order of half a million dollars [[00:07:12](https://www.youtube.com/watch?v=MNFeJNUu074&t=432.56s)]
*  with this very, very naive analysis. [[00:07:16](https://www.youtube.com/watch?v=MNFeJNUu074&t=436.0s)]
*  Now, there's a couple of things there. [[00:07:18](https://www.youtube.com/watch?v=MNFeJNUu074&t=438.64s)]
*  We didn't take into account optimization. [[00:07:20](https://www.youtube.com/watch?v=MNFeJNUu074&t=440.16s)]
*  We also didn't take into account that you probably cannot run this at full capacity [[00:07:21](https://www.youtube.com/watch?v=MNFeJNUu074&t=441.76s)]
*  because of memory bandwidth limitations and network limitations. [[00:07:26](https://www.youtube.com/watch?v=MNFeJNUu074&t=446.8s)]
*  And last but not least, you probably need more than one run to get this right. [[00:07:30](https://www.youtube.com/watch?v=MNFeJNUu074&t=450.08s)]
*  You probably need a bunch of test runs. [[00:07:34](https://www.youtube.com/watch?v=MNFeJNUu074&t=454.32s)]
*  They're probably not going to be full runs and so on. [[00:07:35](https://www.youtube.com/watch?v=MNFeJNUu074&t=455.68s)]
*  But this gives you an idea that sort of training one of these large language models today [[00:07:37](https://www.youtube.com/watch?v=MNFeJNUu074&t=457.92s)]
*  is it's not a $100,000 thing. [[00:07:42](https://www.youtube.com/watch?v=MNFeJNUu074&t=462.16s)]
*  It's probably millions of dollars thing. [[00:07:43](https://www.youtube.com/watch?v=MNFeJNUu074&t=463.92s)]
*  Practically speaking, what we're seeing in industry is that it's actually more of a tens [[00:07:47](https://www.youtube.com/watch?v=MNFeJNUu074&t=467.28000000000003s)]
*  of millions of dollars thing. [[00:07:51](https://www.youtube.com/watch?v=MNFeJNUu074&t=471.84000000000003s)]
*  And that's because you need the reserve capacity. [[00:07:53](https://www.youtube.com/watch?v=MNFeJNUu074&t=473.68s)]
*  So if I could get all my cards for the next two months, it would only cost me a million dollars. [[00:07:56](https://www.youtube.com/watch?v=MNFeJNUu074&t=476.16s)]
*  But the problem is they want a two-year reservation. [[00:08:02](https://www.youtube.com/watch?v=MNFeJNUu074&t=482.24s)]
*  So really, the cost is 12 times as high. [[00:08:04](https://www.youtube.com/watch?v=MNFeJNUu074&t=484.64s)]
*  And so that basically adds a zero to my training cost. [[00:08:08](https://www.youtube.com/watch?v=MNFeJNUu074&t=488.48s)]
*  Right. And how does that compare to inference? [[00:08:12](https://www.youtube.com/watch?v=MNFeJNUu074&t=492.8s)]
*  So inference is much, much, much cheaper. [[00:08:16](https://www.youtube.com/watch?v=MNFeJNUu074&t=496.08s)]
*  Basically, for a modern text model, for example, the training set is about a trillion tokens. [[00:08:18](https://www.youtube.com/watch?v=MNFeJNUu074&t=498.24s)]
*  And if I run inference, each letter, each word that comes out is one token. [[00:08:22](https://www.youtube.com/watch?v=MNFeJNUu074&t=502.96s)]
*  So it's a factor of a trillion or so faster on the inference part. [[00:08:27](https://www.youtube.com/watch?v=MNFeJNUu074&t=507.92s)]
*  You know, if you run the numbers like a large language model, you actually add a fraction [[00:08:32](https://www.youtube.com/watch?v=MNFeJNUu074&t=512.96s)]
*  of a cent, like a tenth of a cent or hundreds of a cent somewhere in that ballpark. [[00:08:38](https://www.youtube.com/watch?v=MNFeJNUu074&t=518.08s)]
*  For the inference, again, if you just naively look at this, for inference, your problem is usually [[00:08:42](https://www.youtube.com/watch?v=MNFeJNUu074&t=522.4000000000001s)]
*  that you have to provision for peak capacity. [[00:08:46](https://www.youtube.com/watch?v=MNFeJNUu074&t=526.72s)]
*  Right. So if everybody wants to use your model at 9 a.m. on a Monday, you still have to pay for [[00:08:48](https://www.youtube.com/watch?v=MNFeJNUu074&t=528.88s)]
*  Saturday night at midnight, but nobody's using it. [[00:08:54](https://www.youtube.com/watch?v=MNFeJNUu074&t=534.5600000000001s)]
*  That increases your cost substantially there. [[00:08:57](https://www.youtube.com/watch?v=MNFeJNUu074&t=537.36s)]
*  For some of them, specifically image models, what you can do for inference is that you [[00:08:59](https://www.youtube.com/watch?v=MNFeJNUu074&t=539.6800000000001s)]
*  use much, much cheaper cards because the model is small enough that you can run it on [[00:09:03](https://www.youtube.com/watch?v=MNFeJNUu074&t=543.44s)]
*  essentially the server version of a consumer graphics card. [[00:09:08](https://www.youtube.com/watch?v=MNFeJNUu074&t=548.4000000000001s)]
*  And that can save you a lot of cost. [[00:09:11](https://www.youtube.com/watch?v=MNFeJNUu074&t=551.04s)]
*  And unfortunately, as we discussed in part one, you can't just make up for these inefficiencies [[00:09:13](https://www.youtube.com/watch?v=MNFeJNUu074&t=553.1999999999999s)]
*  by piecing together a bunch of less performant chips, at least for model training. [[00:09:18](https://www.youtube.com/watch?v=MNFeJNUu074&t=558.16s)]
*  At least you need some very sophisticated software for that. [[00:09:22](https://www.youtube.com/watch?v=MNFeJNUu074&t=562.7199999999999s)]
*  Because the overhead of distributing the data between these cards would probably [[00:09:26](https://www.youtube.com/watch?v=MNFeJNUu074&t=566.0s)]
*  outweigh any saving you get from cheaper cards. [[00:09:30](https://www.youtube.com/watch?v=MNFeJNUu074&t=570.24s)]
*  Inference, on the other hand. [[00:09:32](https://www.youtube.com/watch?v=MNFeJNUu074&t=572.9599999999999s)]
*  For inference, you can often do the inference on a single card. [[00:09:34](https://www.youtube.com/watch?v=MNFeJNUu074&t=574.7199999999999s)]
*  So that's not really a problem. [[00:09:37](https://www.youtube.com/watch?v=MNFeJNUu074&t=577.1999999999999s)]
*  If you take something like stable diffusion, a very popular model for image generation, [[00:09:38](https://www.youtube.com/watch?v=MNFeJNUu074&t=578.4s)]
*  that runs on a MacBook, for example, that has enough memory and enough compute power [[00:09:44](https://www.youtube.com/watch?v=MNFeJNUu074&t=584.24s)]
*  so it can generate an image locally. [[00:09:49](https://www.youtube.com/watch?v=MNFeJNUu074&t=589.04s)]
*  So that'll run on a relatively cheap consumer card. [[00:09:50](https://www.youtube.com/watch?v=MNFeJNUu074&t=590.88s)]
*  And you don't have to use an A100 for it to do inference. [[00:09:53](https://www.youtube.com/watch?v=MNFeJNUu074&t=593.84s)]
*  So when we're talking about the training of the models, clearly the amount of compute is [[00:09:58](https://www.youtube.com/watch?v=MNFeJNUu074&t=598.24s)]
*  just drastically more than the inference. [[00:10:03](https://www.youtube.com/watch?v=MNFeJNUu074&t=603.84s)]
*  And something else that we've already talked about is the more compute, [[00:10:06](https://www.youtube.com/watch?v=MNFeJNUu074&t=606.0s)]
*  often, not always, but often the better model. [[00:10:09](https://www.youtube.com/watch?v=MNFeJNUu074&t=609.44s)]
*  And so does this ultimately, these factors all ladder up to the idea that [[00:10:12](https://www.youtube.com/watch?v=MNFeJNUu074&t=612.32s)]
*  heavily capitalized incumbents win this race or this competition? [[00:10:16](https://www.youtube.com/watch?v=MNFeJNUu074&t=616.4s)]
*  Or how do you think about the relationship between compute, capital, [[00:10:21](https://www.youtube.com/watch?v=MNFeJNUu074&t=621.12s)]
*  and then the technology that we have today? [[00:10:25](https://www.youtube.com/watch?v=MNFeJNUu074&t=625.76s)]
*  Yeah, that's a million dollar question or maybe trillion dollar question. [[00:10:28](https://www.youtube.com/watch?v=MNFeJNUu074&t=628.4s)]
*  I don't know. [[00:10:31](https://www.youtube.com/watch?v=MNFeJNUu074&t=631.6s)]
*  So first of all, training these models is expensive. [[00:10:32](https://www.youtube.com/watch?v=MNFeJNUu074&t=632.16s)]
*  For example, we haven't seen yet a really good open source large language model. [[00:10:35](https://www.youtube.com/watch?v=MNFeJNUu074&t=635.68s)]
*  I'm sure part of the reason is that training these models is just really, really expensive. [[00:10:43](https://www.youtube.com/watch?v=MNFeJNUu074&t=643.1999999999999s)]
*  There's a bunch of enthusiasts out there that would love to do this. [[00:10:47](https://www.youtube.com/watch?v=MNFeJNUu074&t=647.52s)]
*  But you need to find a couple of million or 10 million dollars of compute capacity to do it. [[00:10:50](https://www.youtube.com/watch?v=MNFeJNUu074&t=650.16s)]
*  And that makes it so much harder and means you need to create a substantial effort before [[00:10:55](https://www.youtube.com/watch?v=MNFeJNUu074&t=655.1999999999999s)]
*  something like that can happen. [[00:11:00](https://www.youtube.com/watch?v=MNFeJNUu074&t=660.56s)]
*  All that said, the cost for training these models overall seems to be coming down. [[00:11:01](https://www.youtube.com/watch?v=MNFeJNUu074&t=661.76s)]
*  And in part, I think it is because it seems to me like we're becoming data limited. [[00:11:11](https://www.youtube.com/watch?v=MNFeJNUu074&t=671.04s)]
*  So it turns out there is a correspondence between how big your model is and what the [[00:11:15](https://www.youtube.com/watch?v=MNFeJNUu074&t=675.76s)]
*  optimal amount of training data is for the model. [[00:11:22](https://www.youtube.com/watch?v=MNFeJNUu074&t=682.0s)]
*  That's having a super large model with very few data doesn't get you anything. [[00:11:23](https://www.youtube.com/watch?v=MNFeJNUu074&t=683.84s)]
*  Or having a ton of data with a small model also doesn't get you anything. [[00:11:27](https://www.youtube.com/watch?v=MNFeJNUu074&t=687.6s)]
*  The size of your brain needs to roughly correspond to the length of your university [[00:11:31](https://www.youtube.com/watch?v=MNFeJNUu074&t=691.36s)]
*  education here. Otherwise, it doesn't work. [[00:11:35](https://www.youtube.com/watch?v=MNFeJNUu074&t=695.36s)]
*  What this means is that because some of the large models today already leverage a good [[00:11:38](https://www.youtube.com/watch?v=MNFeJNUu074&t=698.72s)]
*  percentage of all human knowledge in a particular area. [[00:11:45](https://www.youtube.com/watch?v=MNFeJNUu074&t=705.28s)]
*  If you look at GPT, it was probably trained on something like 10% of the internet and all of [[00:11:49](https://www.youtube.com/watch?v=MNFeJNUu074&t=709.84s)]
*  Wikipedia and many books, like a good chunk of all books. [[00:11:55](https://www.youtube.com/watch?v=MNFeJNUu074&t=715.76s)]
*  Right. So going up by a factor of 10, yeah, that's quite possible. [[00:11:58](https://www.youtube.com/watch?v=MNFeJNUu074&t=718.64s)]
*  Going up by a factor of 100, that's not clear if that's possible. [[00:12:02](https://www.youtube.com/watch?v=MNFeJNUu074&t=722.4s)]
*  We as mankind just haven't produced enough knowledge yet that you could absorb all of [[00:12:07](https://www.youtube.com/watch?v=MNFeJNUu074&t=727.76s)]
*  that into one of these large models. [[00:12:10](https://www.youtube.com/watch?v=MNFeJNUu074&t=730.96s)]
*  And so I think the expectation at the moment is that the cost for training these models [[00:12:13](https://www.youtube.com/watch?v=MNFeJNUu074&t=733.76s)]
*  may actually sort of top out or even go down a little bit. [[00:12:21](https://www.youtube.com/watch?v=MNFeJNUu074&t=741.4399999999999s)]
*  The chips get faster, but we don't discover new training material as quickly. [[00:12:25](https://www.youtube.com/watch?v=MNFeJNUu074&t=745.76s)]
*  Unless somebody comes up with a new idea to generate training material. [[00:12:30](https://www.youtube.com/watch?v=MNFeJNUu074&t=750.88s)]
*  If that assumption is true, I think this means that the moat that's created by these large [[00:12:34](https://www.youtube.com/watch?v=MNFeJNUu074&t=754.4799999999999s)]
*  capital investments is actually not particularly deep. [[00:12:39](https://www.youtube.com/watch?v=MNFeJNUu074&t=759.5999999999999s)]
*  It's more of a speed bump than something that prevents new entrants. [[00:12:42](https://www.youtube.com/watch?v=MNFeJNUu074&t=762.7199999999999s)]
*  Today, training a large language model is something that is definitely within reach [[00:12:46](https://www.youtube.com/watch?v=MNFeJNUu074&t=766.9599999999999s)]
*  for a well-funded startup. [[00:12:52](https://www.youtube.com/watch?v=MNFeJNUu074&t=772.3199999999999s)]
*  And for that reason, we expect to see more innovation in that area in the future. [[00:12:54](https://www.youtube.com/watch?v=MNFeJNUu074&t=774.08s)]
*  All right. That is a wrap for our AI hardware series. [[00:12:58](https://www.youtube.com/watch?v=MNFeJNUu074&t=778.1600000000001s)]
*  We genuinely hope you came away with a little more knowledge about this increasingly [[00:13:01](https://www.youtube.com/watch?v=MNFeJNUu074&t=781.36s)]
*  important space. [[00:13:05](https://www.youtube.com/watch?v=MNFeJNUu074&t=785.6s)]
*  Because if software is indeed eating the world, well, hardware is coming along for that ride. [[00:13:07](https://www.youtube.com/watch?v=MNFeJNUu074&t=787.12s)]
*  And as a reminder, if you haven't yet listened to part one, where we explore the emerging [[00:13:13](https://www.youtube.com/watch?v=MNFeJNUu074&t=793.2800000000001s)]
*  architectures and who's creating them, or part two, where we dive into the future AI stack [[00:13:18](https://www.youtube.com/watch?v=MNFeJNUu074&t=798.0s)]
*  and how founders can participate, well, those are already live and ready for consumption. [[00:13:23](https://www.youtube.com/watch?v=MNFeJNUu074&t=803.68s)]
*  And as always, thank you so much for listening. [[00:13:28](https://www.youtube.com/watch?v=MNFeJNUu074&t=808.9599999999999s)]
*  We'd actually like to leave you with a fun fact from GPT-4 itself commenting on the technology [[00:13:31](https://www.youtube.com/watch?v=MNFeJNUu074&t=811.8399999999999s)]
*  that created it. [[00:13:37](https://www.youtube.com/watch?v=MNFeJNUu074&t=817.1999999999999s)]
*  And yes, we did fact check this. [[00:13:38](https://www.youtube.com/watch?v=MNFeJNUu074&t=818.4s)]
*  And this is also AI generated audio from 11 Labs. [[00:13:40](https://www.youtube.com/watch?v=MNFeJNUu074&t=820.0799999999999s)]
*  ChatGPT and its sibling models are trained on diverse internet text. [[00:13:44](https://www.youtube.com/watch?v=MNFeJNUu074&t=824.0s)]
*  However, the exact amount of data used can be hard to comprehend. [[00:13:48](https://www.youtube.com/watch?v=MNFeJNUu074&t=828.88s)]
*  If we were to print all of the data used to train these models, it could fill a large library. [[00:13:53](https://www.youtube.com/watch?v=MNFeJNUu074&t=833.2s)]
*  Consider that one single book may contain around one million characters. [[00:13:59](https://www.youtube.com/watch?v=MNFeJNUu074&t=839.04s)]
*  If we estimate that the training data is hundreds of gigabytes of text data, [[00:14:03](https://www.youtube.com/watch?v=MNFeJNUu074&t=843.28s)]
*  let's take a conservative estimate and say it's 100 gigabytes. [[00:14:07](https://www.youtube.com/watch?v=MNFeJNUu074&t=847.44s)]
*  Considering that one character is approximately one byte, this would mean the model was [[00:14:11](https://www.youtube.com/watch?v=MNFeJNUu074&t=851.76s)]
*  trained on approximately 100 billion characters. [[00:14:16](https://www.youtube.com/watch?v=MNFeJNUu074&t=856.32s)]
*  If each book has one million characters, then the data used to train ChatGPT is equivalent to the [[00:14:19](https://www.youtube.com/watch?v=MNFeJNUu074&t=859.8399999999999s)]
*  text in approximately 100 million books. [[00:14:25](https://www.youtube.com/watch?v=MNFeJNUu074&t=865.6s)]
*  If we take the size of a large library, such as the New York Public Library, which has around [[00:14:28](https://www.youtube.com/watch?v=MNFeJNUu074&t=868.9599999999999s)]
*  53 million items, not just books, the training data is equivalent to the text in almost twice [[00:14:34](https://www.youtube.com/watch?v=MNFeJNUu074&t=874.16s)]
*  the number of items in that library. [[00:14:40](https://www.youtube.com/watch?v=MNFeJNUu074&t=880.56s)]
*  Thanks ChatGPT. [[00:14:42](https://www.youtube.com/watch?v=MNFeJNUu074&t=882.4s)]
*  A quick note to close out that many models today are even bigger. [[00:14:44](https://www.youtube.com/watch?v=MNFeJNUu074&t=884.24s)]
*  With Llama 2, for example, being trained on two trillion tokens or about eight trillion characters. [[00:14:48](https://www.youtube.com/watch?v=MNFeJNUu074&t=888.48s)]
*  Now that is a lot of libraries. [[00:14:54](https://www.youtube.com/watch?v=MNFeJNUu074&t=894.24s)]
*  We'll see you next time. [[00:14:56](https://www.youtube.com/watch?v=MNFeJNUu074&t=896.48s)]
*  Thank you so much for listening to the A16Z podcast. [[00:14:58](https://www.youtube.com/watch?v=MNFeJNUu074&t=898.24s)]
*  What we're trying to do here is provide an informed, clear-eyed, but also optimistic take [[00:15:01](https://www.youtube.com/watch?v=MNFeJNUu074&t=901.6s)]
*  on technology and its future. [[00:15:07](https://www.youtube.com/watch?v=MNFeJNUu074&t=907.36s)]
*  And we're trying to do that by featuring some of the most inspiring people in the things [[00:15:09](https://www.youtube.com/watch?v=MNFeJNUu074&t=909.6s)]
*  that they're building. [[00:15:14](https://www.youtube.com/watch?v=MNFeJNUu074&t=914.08s)]
*  So if that is interesting to you, and you'd like to join us on this journey, go ahead and click [[00:15:15](https://www.youtube.com/watch?v=MNFeJNUu074&t=915.36s)]
*  subscribe and make sure to let us know in the comments below what you'd like to see us cover [[00:15:20](https://www.youtube.com/watch?v=MNFeJNUu074&t=920.32s)]
*  next. [[00:15:24](https://www.youtube.com/watch?v=MNFeJNUu074&t=924.5600000000001s)]
*  Thank you so much for listening and we'll see you next time. [[00:15:25](https://www.youtube.com/watch?v=MNFeJNUu074&t=925.44s)]
