---
Date Generated: May 23, 2025
Transcription Model: whisper medium 20231117
Length: 2346s
Video Keywords: []
Video Views: 3471
Video Rating: None
Video Description: General Partner Anjney Midha explores the cutting-edge world of text-to-video AI with AI researchers Andreas Blattmann and Robin Rombach. 

Released in November, Stable Video Diffusion is their latest open-source generative video model, overcoming challenges in size and dynamic representation.

In this episode Robin, and Andreas share why translating text to video is complex, the key role of datasets, current applications, and the future of video editing.

Topics Covered: 
00:00 - Text to Video: The Next Leap in AI Generation
02:41 - The Stable Diffusion backstory
04:35 - Diffusion vs autoregressive models
07:17 - The benefits of single step sampling
10:55 - Why generative video?
13:10 - Understanding physics through AI video
14:53 - The challenge of creating generative video
18:43 - Data set selection and training
21:24 - Structural consistency and 3D objects
23:51 - Incorporating LoRAs
28:47 - How should creators think about these tools?
31:41 - Open challenges in video generation 
32:35 - Infrastructure challenges and future research

Resources: 
Find Robin on Twitter: https://twitter.com/robrombach
Find Andreas on Twitter: https://twitter.com/andi_blatt
Find Anjney on Twitter: https://twitter.com/anjneymidha
Stay Updated: 
Find a16z on Twitter: https://twitter.com/a16z 
Find a16z on LinkedIn: https://www.linkedin.com/company/a16z 
Subscribe on your favorite podcast app: https://a16z.simplecast.com/ 
Follow our host: https://twitter.com/stephsmithio 

Please note that the content here is for informational purposes only; should NOT be taken as legal, business, tax, or investment advice or be used to evaluate any investment or security; and is not directed at any investors or potential investors in any a16z fund. a16z and its affiliates may maintain investments in the companies discussed. For more details please see a16z.com/disclosures.
---

# Text to Video: The Next Leap in AI Generation
**The a16z Podcast:** [February 17, 2024](https://www.youtube.com/watch?v=0tsIbarSPLk)
*  This is a conversation I've been super excited about. [[00:00:00](https://www.youtube.com/watch?v=0tsIbarSPLk&t=0.0s)]
*  What is stable diffusion? [[00:00:02](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2.6s)]
*  You type in a text prompt and it generates an image. [[00:00:03](https://www.youtube.com/watch?v=0tsIbarSPLk&t=3.6s)]
*  Why was stable video the thing that you guys decided to prioritize? [[00:00:06](https://www.youtube.com/watch?v=0tsIbarSPLk&t=6.08s)]
*  I think we like challenges. [[00:00:09](https://www.youtube.com/watch?v=0tsIbarSPLk&t=9.6s)]
*  I think compute constraints can drive innovation. [[00:00:10](https://www.youtube.com/watch?v=0tsIbarSPLk&t=10.8s)]
*  You always want to try to improve your efficiency. [[00:00:13](https://www.youtube.com/watch?v=0tsIbarSPLk&t=13.6s)]
*  It's already such an incredible leap forward, right? [[00:00:17](https://www.youtube.com/watch?v=0tsIbarSPLk&t=17.48s)]
*  We're really excited to see what will happen. [[00:00:20](https://www.youtube.com/watch?v=0tsIbarSPLk&t=20.72s)]
*  Today, many people are familiar with text to text or text to image AI models. [[00:00:23](https://www.youtube.com/watch?v=0tsIbarSPLk&t=23.4s)]
*  Just think chat GBT or mid journey. [[00:00:31](https://www.youtube.com/watch?v=0tsIbarSPLk&t=31.279999999999998s)]
*  But what about text to video? [[00:00:33](https://www.youtube.com/watch?v=0tsIbarSPLk&t=33.68s)]
*  Well, several companies are working hard to make that a reality. [[00:00:36](https://www.youtube.com/watch?v=0tsIbarSPLk&t=36.239999999999995s)]
*  But for many reasons, it's a lot harder. [[00:00:40](https://www.youtube.com/watch?v=0tsIbarSPLk&t=40.92s)]
*  For one, think about size. [[00:00:43](https://www.youtube.com/watch?v=0tsIbarSPLk&t=43.28s)]
*  You'll often find text files in the kilobytes, images, maybe in the megabytes. [[00:00:45](https://www.youtube.com/watch?v=0tsIbarSPLk&t=45.12s)]
*  But it's not uncommon to find high quality video content in the gigabytes. [[00:00:50](https://www.youtube.com/watch?v=0tsIbarSPLk&t=50.68s)]
*  Plus, video requires a much more dynamic representation of the world, [[00:00:56](https://www.youtube.com/watch?v=0tsIbarSPLk&t=56.120000000000005s)]
*  incorporating the physics of movement, 3D objects and more. [[00:01:00](https://www.youtube.com/watch?v=0tsIbarSPLk&t=60.120000000000005s)]
*  I mean, imagine the hand challenge in text to image. [[00:01:03](https://www.youtube.com/watch?v=0tsIbarSPLk&t=63.800000000000004s)]
*  But in this case, it's hands squared. [[00:01:06](https://www.youtube.com/watch?v=0tsIbarSPLk&t=66.28s)]
*  But this is not stopping the researchers behind stable video diffusion, [[00:01:08](https://www.youtube.com/watch?v=0tsIbarSPLk&t=68.68s)]
*  which as of November 21st was released as a state of the art open source generative video model. [[00:01:12](https://www.youtube.com/watch?v=0tsIbarSPLk&t=72.84s)]
*  So today you'll get to hear from two of those very researchers, [[00:01:19](https://www.youtube.com/watch?v=0tsIbarSPLk&t=79.16s)]
*  Andreas Blattman and Robin Robach. [[00:01:22](https://www.youtube.com/watch?v=0tsIbarSPLk&t=82.12s)]
*  Robin, by the way, is also the co inventor of stable diffusion, [[00:01:24](https://www.youtube.com/watch?v=0tsIbarSPLk&t=84.6s)]
*  one of the most popular open source text to image models. [[00:01:27](https://www.youtube.com/watch?v=0tsIbarSPLk&t=87.96s)]
*  So in today's episode, together with a 16 Z general partner on shame, [[00:01:31](https://www.youtube.com/watch?v=0tsIbarSPLk&t=91.4s)]
*  Mida, you'll get to hear firsthand what really makes text to video so much harder. [[00:01:35](https://www.youtube.com/watch?v=0tsIbarSPLk&t=95.08s)]
*  The challenges like selecting the right data sets that enable realistic representations of the world, [[00:01:40](https://www.youtube.com/watch?v=0tsIbarSPLk&t=100.03999999999999s)]
*  applications where this technology is already being deployed and put to use, [[00:01:45](https://www.youtube.com/watch?v=0tsIbarSPLk&t=105.32s)]
*  plus what the video editor of the future might look like [[00:01:48](https://www.youtube.com/watch?v=0tsIbarSPLk&t=108.92s)]
*  and how constraints continue to spur innovation and ultimately keep this field moving. [[00:01:51](https://www.youtube.com/watch?v=0tsIbarSPLk&t=111.96000000000001s)]
*  Finally, if you like this episode, our infrastructure team at a 16 Z [[00:01:57](https://www.youtube.com/watch?v=0tsIbarSPLk&t=117.32000000000001s)]
*  is coming out with a lot more AI content in the new year. [[00:02:01](https://www.youtube.com/watch?v=0tsIbarSPLk&t=121.24000000000001s)]
*  But in the meantime, you can go to a 16 z.com slash AI for all our previous coverage. [[00:02:04](https://www.youtube.com/watch?v=0tsIbarSPLk&t=124.2s)]
*  All right, enjoy. [[00:02:10](https://www.youtube.com/watch?v=0tsIbarSPLk&t=130.68s)]
*  As a reminder, the content here is for informational purposes only, [[00:02:12](https://www.youtube.com/watch?v=0tsIbarSPLk&t=132.68s)]
*  should not be taken as legal business tax or investment advice, [[00:02:16](https://www.youtube.com/watch?v=0tsIbarSPLk&t=136.44000000000003s)]
*  or be used to evaluate any investment or security, [[00:02:19](https://www.youtube.com/watch?v=0tsIbarSPLk&t=139.8s)]
*  and is not directed at any investors or potential investors in any a 16 z fund. [[00:02:22](https://www.youtube.com/watch?v=0tsIbarSPLk&t=142.44000000000003s)]
*  Please note that a 16 z and its affiliates may also maintain investments [[00:02:27](https://www.youtube.com/watch?v=0tsIbarSPLk&t=147.32000000000002s)]
*  in the companies discussed in this podcast. [[00:02:31](https://www.youtube.com/watch?v=0tsIbarSPLk&t=151.24s)]
*  For more details, including a link to our investments, please see a 16 z.com slash disclosures. [[00:02:33](https://www.youtube.com/watch?v=0tsIbarSPLk&t=153.8s)]
*  This is a conversation I've been super excited about for a while. [[00:02:41](https://www.youtube.com/watch?v=0tsIbarSPLk&t=161.08s)]
*  Thank you so much for finding the time to join us on the podcast. [[00:02:44](https://www.youtube.com/watch?v=0tsIbarSPLk&t=164.52s)]
*  Maybe we can start with just for first time folks, [[00:02:48](https://www.youtube.com/watch?v=0tsIbarSPLk&t=168.76s)]
*  a brief overview of your team, your research lab, [[00:02:53](https://www.youtube.com/watch?v=0tsIbarSPLk&t=173.72s)]
*  and for listeners who are unfamiliar, [[00:02:58](https://www.youtube.com/watch?v=0tsIbarSPLk&t=178.44s)]
*  maybe spend just a couple minutes talking about what stable diffusion is [[00:03:00](https://www.youtube.com/watch?v=0tsIbarSPLk&t=180.68s)]
*  and what stable video diffusion is. [[00:03:05](https://www.youtube.com/watch?v=0tsIbarSPLk&t=185.64s)]
*  What is stable diffusion? [[00:03:07](https://www.youtube.com/watch?v=0tsIbarSPLk&t=187.48s)]
*  Stable diffusion is a text to image model, generative model, [[00:03:08](https://www.youtube.com/watch?v=0tsIbarSPLk&t=188.44s)]
*  that means you type in a text prompt and it generates an image based on that. [[00:03:12](https://www.youtube.com/watch?v=0tsIbarSPLk&t=192.76s)]
*  In particular, stable diffusion is, as the name suggests, a diffusion model. [[00:03:17](https://www.youtube.com/watch?v=0tsIbarSPLk&t=197.32s)]
*  Diffusion models are a type of generative models, [[00:03:22](https://www.youtube.com/watch?v=0tsIbarSPLk&t=202.6s)]
*  which has been super successful recently for image generation. [[00:03:24](https://www.youtube.com/watch?v=0tsIbarSPLk&t=204.92s)]
*  And it's based on a technique that we developed while we were still at the university. [[00:03:28](https://www.youtube.com/watch?v=0tsIbarSPLk&t=208.04s)]
*  So me and Andreas and Patrick and Dominic, all in the same team now at Stability. [[00:03:33](https://www.youtube.com/watch?v=0tsIbarSPLk&t=213.16s)]
*  We are a multimodal company and our specialty is to produce and publish models, [[00:03:40](https://www.youtube.com/watch?v=0tsIbarSPLk&t=220.20000000000002s)]
*  try to make them as accessible as possible. [[00:03:47](https://www.youtube.com/watch?v=0tsIbarSPLk&t=227.96s)]
*  That includes publishing weights and kind of making foundation models [[00:03:50](https://www.youtube.com/watch?v=0tsIbarSPLk&t=230.84s)]
*  for all kinds of modalities, not only images, but also video available [[00:03:56](https://www.youtube.com/watch?v=0tsIbarSPLk&t=236.52s)]
*  and enabling research on top of that. [[00:04:02](https://www.youtube.com/watch?v=0tsIbarSPLk&t=242.04000000000002s)]
*  So we have seen that stable diffusion was super successful, [[00:04:05](https://www.youtube.com/watch?v=0tsIbarSPLk&t=245.16s)]
*  I would say much more successful than we initially anticipated. [[00:04:08](https://www.youtube.com/watch?v=0tsIbarSPLk&t=248.6s)]
*  And there are like hundreds, if not thousands of papers that are building on top of that. [[00:04:12](https://www.youtube.com/watch?v=0tsIbarSPLk&t=252.12s)]
*  We in particular, our group is focused on visual media. [[00:04:16](https://www.youtube.com/watch?v=0tsIbarSPLk&t=256.44s)]
*  So that is images, that is videos, and stable video diffusion that you just introduced [[00:04:20](https://www.youtube.com/watch?v=0tsIbarSPLk&t=260.12s)]
*  is kind of the next iteration. [[00:04:26](https://www.youtube.com/watch?v=0tsIbarSPLk&t=266.12s)]
*  It's our first step into the video domain. [[00:04:27](https://www.youtube.com/watch?v=0tsIbarSPLk&t=267.32s)]
*  We published a model that can take in an image and turn that into a short video clip. [[00:04:29](https://www.youtube.com/watch?v=0tsIbarSPLk&t=269.8s)]
*  Maybe we could spend a couple of minutes on a brief overview of diffusion models. [[00:04:35](https://www.youtube.com/watch?v=0tsIbarSPLk&t=275.72s)]
*  That might be helpful. [[00:04:40](https://www.youtube.com/watch?v=0tsIbarSPLk&t=280.36s)]
*  How do diffusion models differ from other types of generative models [[00:04:41](https://www.youtube.com/watch?v=0tsIbarSPLk&t=281.88000000000005s)]
*  and techniques like autoregressive models? [[00:04:45](https://www.youtube.com/watch?v=0tsIbarSPLk&t=285.88000000000005s)]
*  If you could just give us a little bit of context before we dive in, that would help. [[00:04:48](https://www.youtube.com/watch?v=0tsIbarSPLk&t=288.04s)]
*  Diffusion models are really the to-go models right now for visual media like images and videos. [[00:04:51](https://www.youtube.com/watch?v=0tsIbarSPLk&t=291.16s)]
*  They're kind of different to autoregressive models because they don't represent data as a [[00:04:56](https://www.youtube.com/watch?v=0tsIbarSPLk&t=296.84000000000003s)]
*  sequence of tokens or something, which we know from autoregressive models. [[00:05:04](https://www.youtube.com/watch?v=0tsIbarSPLk&t=304.6s)]
*  Since images and videos are composed as a pixel grid, this is really a good, beneficial property. [[00:05:10](https://www.youtube.com/watch?v=0tsIbarSPLk&t=310.68s)]
*  Also, they favor perceptually important details, which is inherently baked into these models [[00:05:21](https://www.youtube.com/watch?v=0tsIbarSPLk&t=321.08000000000004s)]
*  because their learning objectives is just tuned to favor these important aspects of images. [[00:05:27](https://www.youtube.com/watch?v=0tsIbarSPLk&t=327.32000000000005s)]
*  As we perceive it as humans. [[00:05:34](https://www.youtube.com/watch?v=0tsIbarSPLk&t=334.68s)]
*  And yeah, that is what we actually want. [[00:05:36](https://www.youtube.com/watch?v=0tsIbarSPLk&t=336.28s)]
*  Right. [[00:05:38](https://www.youtube.com/watch?v=0tsIbarSPLk&t=338.03999999999996s)]
*  But they also have some commonalities with autoregressive models. [[00:05:38](https://www.youtube.com/watch?v=0tsIbarSPLk&t=338.52s)]
*  They are iterative in their nature. [[00:05:43](https://www.youtube.com/watch?v=0tsIbarSPLk&t=343.64s)]
*  They apply some kind of iterative refinement to the data. [[00:05:45](https://www.youtube.com/watch?v=0tsIbarSPLk&t=345.88s)]
*  But as opposed to autoregressive models, which iteratively generate token by token [[00:05:50](https://www.youtube.com/watch?v=0tsIbarSPLk&t=350.03999999999996s)]
*  and or word by word for language, these models gradually transform in small steps. [[00:05:56](https://www.youtube.com/watch?v=0tsIbarSPLk&t=356.2s)]
*  So they gradually transform noise to data. [[00:06:02](https://www.youtube.com/watch?v=0tsIbarSPLk&t=362.59999999999997s)]
*  Maybe one point to add to the difference to the other very successful type, which is [[00:06:05](https://www.youtube.com/watch?v=0tsIbarSPLk&t=365.4s)]
*  in language models or autoregressive models in general, you have to [[00:06:12](https://www.youtube.com/watch?v=0tsIbarSPLk&t=372.35999999999996s)]
*  generate each token in a sequence. [[00:06:16](https://www.youtube.com/watch?v=0tsIbarSPLk&t=376.2s)]
*  Right. [[00:06:18](https://www.youtube.com/watch?v=0tsIbarSPLk&t=378.68s)]
*  So you train your model on a certain sequence length and then you iterate over that while [[00:06:18](https://www.youtube.com/watch?v=0tsIbarSPLk&t=378.91999999999996s)]
*  decoding a sentence. [[00:06:27](https://www.youtube.com/watch?v=0tsIbarSPLk&t=387.47999999999996s)]
*  And in the future models, you train the model on like initially you usually use like a thousand [[00:06:28](https://www.youtube.com/watch?v=0tsIbarSPLk&t=388.76s)]
*  different noise levels between like data and like pure noise. [[00:06:37](https://www.youtube.com/watch?v=0tsIbarSPLk&t=397.71999999999997s)]
*  There exist different formulations. [[00:06:42](https://www.youtube.com/watch?v=0tsIbarSPLk&t=402.28s)]
*  You can do this in a continuous space, but it's not too important. [[00:06:43](https://www.youtube.com/watch?v=0tsIbarSPLk&t=403.4s)]
*  The interesting thing is that at sampling time, you can actually use less steps. [[00:06:46](https://www.youtube.com/watch?v=0tsIbarSPLk&t=406.03999999999996s)]
*  You can use like 50 steps. [[00:06:49](https://www.youtube.com/watch?v=0tsIbarSPLk&t=409.71999999999997s)]
*  You can use there exists like specialized samplers for these models, which use far less [[00:06:51](https://www.youtube.com/watch?v=0tsIbarSPLk&t=411.32s)]
*  steps than recently. [[00:06:57](https://www.youtube.com/watch?v=0tsIbarSPLk&t=417.8s)]
*  There has been a lot of distillation work actually, which still reduces this further. [[00:06:59](https://www.youtube.com/watch?v=0tsIbarSPLk&t=419.48s)]
*  We ourselves, we have published a distillation work a week ago that actually shows that you [[00:07:03](https://www.youtube.com/watch?v=0tsIbarSPLk&t=423.8s)]
*  can go as low as one sampling step, which is, I would say like a big advantage of these [[00:07:09](https://www.youtube.com/watch?v=0tsIbarSPLk&t=429.56s)]
*  different models. [[00:07:16](https://www.youtube.com/watch?v=0tsIbarSPLk&t=436.84000000000003s)]
*  For folks who may not be familiar with why a single step sampling breakthrough is important. [[00:07:17](https://www.youtube.com/watch?v=0tsIbarSPLk&t=437.64s)]
*  Could you say a little bit about what benefits that leads to for creators or users of the model? [[00:07:23](https://www.youtube.com/watch?v=0tsIbarSPLk&t=443.47999999999996s)]
*  Oh, yeah, absolutely. [[00:07:28](https://www.youtube.com/watch?v=0tsIbarSPLk&t=448.59999999999997s)]
*  I think like the most intuitive thing is that you actually see what happens while you type [[00:07:29](https://www.youtube.com/watch?v=0tsIbarSPLk&t=449.32s)]
*  in your text prompt. [[00:07:35](https://www.youtube.com/watch?v=0tsIbarSPLk&t=455.32s)]
*  So think of like this text image model. [[00:07:36](https://www.youtube.com/watch?v=0tsIbarSPLk&t=456.59999999999997s)]
*  You type in your prompt one and a half years back, you had to wait for like a few seconds, [[00:07:40](https://www.youtube.com/watch?v=0tsIbarSPLk&t=460.12s)]
*  maybe even up to a minute. [[00:07:46](https://www.youtube.com/watch?v=0tsIbarSPLk&t=466.28s)]
*  Now you see what happens and the quality is even better than what we had like with the [[00:07:47](https://www.youtube.com/watch?v=0tsIbarSPLk&t=467.71999999999997s)]
*  first iteration of stable diffusion. [[00:07:52](https://www.youtube.com/watch?v=0tsIbarSPLk&t=472.36s)]
*  So super, super exciting actually to see that kind of trajectory, these kind of developments. [[00:07:53](https://www.youtube.com/watch?v=0tsIbarSPLk&t=473.96000000000004s)]
*  Like when I first sampled this model, I was actually shocked that it works so well. [[00:08:00](https://www.youtube.com/watch?v=0tsIbarSPLk&t=480.36s)]
*  To keep pulling on that thread for a bit, if we rewind the clock back to a year and a half ago, [[00:08:06](https://www.youtube.com/watch?v=0tsIbarSPLk&t=486.04s)]
*  which is when you guys first put out stable diffusion, between then and now, what has [[00:08:12](https://www.youtube.com/watch?v=0tsIbarSPLk&t=492.28000000000003s)]
*  surprised you most about image models that you didn't expect? [[00:08:21](https://www.youtube.com/watch?v=0tsIbarSPLk&t=501.24s)]
*  The pure improvements in performance and text understanding of these models, [[00:08:26](https://www.youtube.com/watch?v=0tsIbarSPLk&t=506.35999999999996s)]
*  in spatial compositionality of what these models can do just by typing in a single prompt, [[00:08:31](https://www.youtube.com/watch?v=0tsIbarSPLk&t=511.32s)]
*  you can describe a scene in really, really fine grained and it gives you a highly detailed like [[00:08:36](https://www.youtube.com/watch?v=0tsIbarSPLk&t=516.28s)]
*  instantiation, visual instantiation of it. [[00:08:43](https://www.youtube.com/watch?v=0tsIbarSPLk&t=523.4s)]
*  The developments has been huge. [[00:08:45](https://www.youtube.com/watch?v=0tsIbarSPLk&t=525.96s)]
*  We published SDXL in June and even then it was like a huge improvement in visual quality, [[00:08:48](https://www.youtube.com/watch?v=0tsIbarSPLk&t=528.12s)]
*  in prompt following. [[00:08:55](https://www.youtube.com/watch?v=0tsIbarSPLk&t=535.48s)]
*  Also other models which we see right now, as it's like most recent, [[00:08:56](https://www.youtube.com/watch?v=0tsIbarSPLk&t=536.92s)]
*  Dali 3 is like a huge improvement still. [[00:09:00](https://www.youtube.com/watch?v=0tsIbarSPLk&t=540.76s)]
*  But also like as Robin said, that there has been a lot of different samples proposed to [[00:09:03](https://www.youtube.com/watch?v=0tsIbarSPLk&t=543.24s)]
*  make these models faster and faster and faster. [[00:09:07](https://www.youtube.com/watch?v=0tsIbarSPLk&t=547.5600000000001s)]
*  And right now we're getting really close to 50 steps performance and even one step. [[00:09:10](https://www.youtube.com/watch?v=0tsIbarSPLk&t=550.2s)]
*  This is like a huge improvement. [[00:09:16](https://www.youtube.com/watch?v=0tsIbarSPLk&t=556.12s)]
*  And I think a really important part of this is the fact that these models have been accessible [[00:09:18](https://www.youtube.com/watch?v=0tsIbarSPLk&t=558.84s)]
*  to everyone. [[00:09:25](https://www.youtube.com/watch?v=0tsIbarSPLk&t=565.1600000000001s)]
*  So open sourcing, like a foundation model as stable diffusion initially, [[00:09:25](https://www.youtube.com/watch?v=0tsIbarSPLk&t=565.96s)]
*  that led to a whole lot of research on these models, which was in retrospect extremely [[00:09:30](https://www.youtube.com/watch?v=0tsIbarSPLk&t=570.52s)]
*  important to do this. [[00:09:38](https://www.youtube.com/watch?v=0tsIbarSPLk&t=578.28s)]
*  I think otherwise we wouldn't have seen the improvements we saw until now. [[00:09:40](https://www.youtube.com/watch?v=0tsIbarSPLk&t=580.2s)]
*  Even before that, I was surprised that text image with diffusion models work so well. [[00:09:44](https://www.youtube.com/watch?v=0tsIbarSPLk&t=584.12s)]
*  Just before we published the model. [[00:09:49](https://www.youtube.com/watch?v=0tsIbarSPLk&t=589.96s)]
*  Like when I first saw this myself, we had this latent diffusion approach that we developed [[00:09:52](https://www.youtube.com/watch?v=0tsIbarSPLk&t=592.28s)]
*  at the university. [[00:09:57](https://www.youtube.com/watch?v=0tsIbarSPLk&t=597.4s)]
*  And then we got a machine with like 8, 80 gigabyte A100s just after we put it in the [[00:09:58](https://www.youtube.com/watch?v=0tsIbarSPLk&t=598.36s)]
*  archive. [[00:10:05](https://www.youtube.com/watch?v=0tsIbarSPLk&t=605.72s)]
*  And then immediately we started working on, hey, we want to have this text image model, [[00:10:06](https://www.youtube.com/watch?v=0tsIbarSPLk&t=606.2s)]
*  but not train it on one GPU. [[00:10:11](https://www.youtube.com/watch?v=0tsIbarSPLk&t=611.48s)]
*  Let's use our little cluster with 8, 80 gigabyte A100s. [[00:10:13](https://www.youtube.com/watch?v=0tsIbarSPLk&t=613.5600000000001s)]
*  We trained this latent diffusion model on 256 by 256 pixels. [[00:10:20](https://www.youtube.com/watch?v=0tsIbarSPLk&t=620.12s)]
*  Really was the first time that we had to deal with large scale data loading and these kind [[00:10:25](https://www.youtube.com/watch?v=0tsIbarSPLk&t=625.24s)]
*  of things. [[00:10:29](https://www.youtube.com/watch?v=0tsIbarSPLk&t=629.8000000000001s)]
*  But yeah, we made it work. [[00:10:31](https://www.youtube.com/watch?v=0tsIbarSPLk&t=631.0s)]
*  And then using this model, combining it with classifier-free guidance, which is a sampling [[00:10:32](https://www.youtube.com/watch?v=0tsIbarSPLk&t=632.84s)]
*  technique that further improves the sample quality at basically no cost. [[00:10:38](https://www.youtube.com/watch?v=0tsIbarSPLk&t=638.44s)]
*  I was really surprised that we could do this on our own and achieve a pretty good model, [[00:10:42](https://www.youtube.com/watch?v=0tsIbarSPLk&t=642.2800000000001s)]
*  I would say. [[00:10:47](https://www.youtube.com/watch?v=0tsIbarSPLk&t=647.72s)]
*  And then two days later, OpenAI published Dolly 2 and all the hype was gone, but it [[00:10:48](https://www.youtube.com/watch?v=0tsIbarSPLk&t=648.84s)]
*  was a pretty nice experience. [[00:10:54](https://www.youtube.com/watch?v=0tsIbarSPLk&t=654.36s)]
*  Something you mentioned, Andreas, is that the fact that you guys chose to release stable [[00:10:56](https://www.youtube.com/watch?v=0tsIbarSPLk&t=656.12s)]
*  diffusion as an open source model resulted in this crazy ecosystem exploding around your [[00:11:03](https://www.youtube.com/watch?v=0tsIbarSPLk&t=663.56s)]
*  research, which is just something that doesn't happen as quickly or as fast with models that [[00:11:09](https://www.youtube.com/watch?v=0tsIbarSPLk&t=669.88s)]
*  aren't open source. [[00:11:18](https://www.youtube.com/watch?v=0tsIbarSPLk&t=678.4399999999999s)]
*  And so in the last year and a half, one of the things that's been really fun, at least [[00:11:19](https://www.youtube.com/watch?v=0tsIbarSPLk&t=679.8s)]
*  for me to watch, is all the really surprising things that developers and creators have done [[00:11:24](https://www.youtube.com/watch?v=0tsIbarSPLk&t=684.04s)]
*  with the base model that you guys put out. [[00:11:31](https://www.youtube.com/watch?v=0tsIbarSPLk&t=691.8s)]
*  You've provided folks a set of Lego blocks that they can mix and match in different ways, [[00:11:34](https://www.youtube.com/watch?v=0tsIbarSPLk&t=694.4399999999999s)]
*  things like ControlNet that give people more controllability, allowing your community to [[00:11:38](https://www.youtube.com/watch?v=0tsIbarSPLk&t=698.92s)]
*  build their own front end. [[00:11:43](https://www.youtube.com/watch?v=0tsIbarSPLk&t=703.4799999999999s)]
*  Out of all of that, I'm sure came a ton of requests for you guys. [[00:11:45](https://www.youtube.com/watch?v=0tsIbarSPLk&t=705.3199999999999s)]
*  As you guys were prioritizing all those asks that came in from the world and the community, [[00:11:49](https://www.youtube.com/watch?v=0tsIbarSPLk&t=709.4799999999999s)]
*  why was stable video the thing that you guys decided to prioritize above everything else [[00:11:54](https://www.youtube.com/watch?v=0tsIbarSPLk&t=714.3599999999999s)]
*  as your next major milestone? [[00:11:58](https://www.youtube.com/watch?v=0tsIbarSPLk&t=718.92s)]
*  I think video is like an awesome kind of data because to solve that task, to solve video [[00:12:01](https://www.youtube.com/watch?v=0tsIbarSPLk&t=721.08s)]
*  generation, a model needs to learn much about physical properties of the world, the physical [[00:12:09](https://www.youtube.com/watch?v=0tsIbarSPLk&t=729.72s)]
*  foundations of the world. [[00:12:18](https://www.youtube.com/watch?v=0tsIbarSPLk&t=738.52s)]
*  There is so much without knowing about, for instance, 3D scenes. [[00:12:20](https://www.youtube.com/watch?v=0tsIbarSPLk&t=740.2s)]
*  You cannot generate a camera pan around an object or you cannot make an object move. [[00:12:25](https://www.youtube.com/watch?v=0tsIbarSPLk&t=745.4s)]
*  If a person turns around, the model needs to hallucinate how this person looks from behind. [[00:12:29](https://www.youtube.com/watch?v=0tsIbarSPLk&t=749.8s)]
*  You need to know so much about the world by just including that additional temporal dimension. [[00:12:38](https://www.youtube.com/watch?v=0tsIbarSPLk&t=758.4399999999999s)]
*  This is what fascinated me most on working on videos. [[00:12:45](https://www.youtube.com/watch?v=0tsIbarSPLk&t=765.2399999999999s)]
*  I think it's also computationally, it's really the next level of computational demands because [[00:12:50](https://www.youtube.com/watch?v=0tsIbarSPLk&t=770.5999999999999s)]
*  you have an additional dimensionality which makes everything much harder. [[00:12:59](https://www.youtube.com/watch?v=0tsIbarSPLk&t=779.08s)]
*  I think we like challenges. [[00:13:05](https://www.youtube.com/watch?v=0tsIbarSPLk&t=785.08s)]
*  That's why we probably focused on doing that. [[00:13:06](https://www.youtube.com/watch?v=0tsIbarSPLk&t=786.2s)]
*  Something that's not known about you guys is by background, originally, I believe you're physicists. [[00:13:10](https://www.youtube.com/watch?v=0tsIbarSPLk&t=790.44s)]
*  I'm a physicist, but I haven't done much physics in a while, unfortunately. [[00:13:16](https://www.youtube.com/watch?v=0tsIbarSPLk&t=796.2s)]
*  I'm originally a mechanical engineer, but that is also really related to physics and I was always [[00:13:22](https://www.youtube.com/watch?v=0tsIbarSPLk&t=802.9200000000001s)]
*  inspired by physics and really fascinated by it. [[00:13:28](https://www.youtube.com/watch?v=0tsIbarSPLk&t=808.12s)]
*  Well, both of your backgrounds academically were spent studying the physical world. [[00:13:31](https://www.youtube.com/watch?v=0tsIbarSPLk&t=811.88s)]
*  I just think it's poetic that your primary interest in generative modeling came from [[00:13:39](https://www.youtube.com/watch?v=0tsIbarSPLk&t=819.5600000000001s)]
*  trying to understand at some deeper level the physical world. [[00:13:47](https://www.youtube.com/watch?v=0tsIbarSPLk&t=827.48s)]
*  It seems like that seems to have motivated at least some of the intuition and the research around [[00:13:50](https://www.youtube.com/watch?v=0tsIbarSPLk&t=830.2s)]
*  your approach to stable video. [[00:13:55](https://www.youtube.com/watch?v=0tsIbarSPLk&t=835.72s)]
*  Absolutely, actually. [[00:13:57](https://www.youtube.com/watch?v=0tsIbarSPLk&t=837.4s)]
*  I fully agree. [[00:13:58](https://www.youtube.com/watch?v=0tsIbarSPLk&t=838.1999999999999s)]
*  I think we're just actually scratching the surface with the kind of video models that [[00:13:59](https://www.youtube.com/watch?v=0tsIbarSPLk&t=839.48s)]
*  we have right now. [[00:14:03](https://www.youtube.com/watch?v=0tsIbarSPLk&t=843.64s)]
*  Having something like we are seeing in language modeling, but trained on pixels on videos, [[00:14:05](https://www.youtube.com/watch?v=0tsIbarSPLk&t=845.24s)]
*  will probably give super interesting downstream behavior. [[00:14:11](https://www.youtube.com/watch?v=0tsIbarSPLk&t=851.0799999999999s)]
*  Not only generating videos, but also understanding of the world. [[00:14:15](https://www.youtube.com/watch?v=0tsIbarSPLk&t=855.0s)]
*  Is it possible to derive something like a physical law from such a model? [[00:14:20](https://www.youtube.com/watch?v=0tsIbarSPLk&t=860.4399999999999s)]
*  I don't know. [[00:14:24](https://www.youtube.com/watch?v=0tsIbarSPLk&t=864.52s)]
*  Or such a model is also always predictive. [[00:14:25](https://www.youtube.com/watch?v=0tsIbarSPLk&t=865.72s)]
*  So you can start with an image or with a sequence of images and try to predict what happens next, [[00:14:31](https://www.youtube.com/watch?v=0tsIbarSPLk&t=871.0s)]
*  of course. [[00:14:38](https://www.youtube.com/watch?v=0tsIbarSPLk&t=878.44s)]
*  And then I think also coupling this with other modalities such as language will maybe provide [[00:14:40](https://www.youtube.com/watch?v=0tsIbarSPLk&t=880.68s)]
*  a way to ground these models more in the physical world. [[00:14:48](https://www.youtube.com/watch?v=0tsIbarSPLk&t=888.36s)]
*  Well, I think that's a good segue into what is the main focus of today's conversation, [[00:14:52](https://www.youtube.com/watch?v=0tsIbarSPLk&t=892.6s)]
*  which is generative video. [[00:14:58](https://www.youtube.com/watch?v=0tsIbarSPLk&t=898.2s)]
*  At least to folks who are early users of stable video, of stable diffusion. [[00:15:01](https://www.youtube.com/watch?v=0tsIbarSPLk&t=901.1600000000001s)]
*  Stable video was a much awaited natural progression from the original model. [[00:15:07](https://www.youtube.com/watch?v=0tsIbarSPLk&t=907.08s)]
*  Just take us back a little bit to the original conception of the project. [[00:15:12](https://www.youtube.com/watch?v=0tsIbarSPLk&t=912.84s)]
*  How long have you guys been working on video modeling? [[00:15:16](https://www.youtube.com/watch?v=0tsIbarSPLk&t=916.76s)]
*  I would say roughly half a year. [[00:15:20](https://www.youtube.com/watch?v=0tsIbarSPLk&t=920.12s)]
*  For this model that we just put out, I think the main challenge was that we actually [[00:15:23](https://www.youtube.com/watch?v=0tsIbarSPLk&t=923.5600000000001s)]
*  had to scale the data set and the data loading. [[00:15:28](https://www.youtube.com/watch?v=0tsIbarSPLk&t=928.0400000000001s)]
*  So if you train a video model on a lot of GPUs, you suddenly run into problems that you didn't [[00:15:30](https://www.youtube.com/watch?v=0tsIbarSPLk&t=930.7600000000001s)]
*  really have before. [[00:15:36](https://www.youtube.com/watch?v=0tsIbarSPLk&t=936.2800000000001s)]
*  Loading high resolution videos is just a difficult task if you do it at scale. [[00:15:39](https://www.youtube.com/watch?v=0tsIbarSPLk&t=939.8000000000001s)]
*  Also, only decoding videos is really hard. [[00:15:46](https://www.youtube.com/watch?v=0tsIbarSPLk&t=946.28s)]
*  A data loader has to transform the sprites it loads into a suitable representation for [[00:15:50](https://www.youtube.com/watch?v=0tsIbarSPLk&t=950.1999999999999s)]
*  the model. [[00:15:54](https://www.youtube.com/watch?v=0tsIbarSPLk&t=954.92s)]
*  To do so, you have to do a lot of computational work from the video codec where the video [[00:15:56](https://www.youtube.com/watch?v=0tsIbarSPLk&t=956.4399999999999s)]
*  was encoded to transform it into a suitable input sample for the generative model. [[00:16:03](https://www.youtube.com/watch?v=0tsIbarSPLk&t=963.9599999999999s)]
*  This is computationally really expensive. [[00:16:11](https://www.youtube.com/watch?v=0tsIbarSPLk&t=971.64s)]
*  And since we have so fast GPUs right now, the CPUs were just in the beginning too slow. [[00:16:14](https://www.youtube.com/watch?v=0tsIbarSPLk&t=974.36s)]
*  Building an efficient data pipeline for video was really a challenge. [[00:16:24](https://www.youtube.com/watch?v=0tsIbarSPLk&t=984.12s)]
*  Actually, there's interesting bugs that you can encounter during training. [[00:16:28](https://www.youtube.com/watch?v=0tsIbarSPLk&t=988.12s)]
*  So we had one where we would actually... [[00:16:31](https://www.youtube.com/watch?v=0tsIbarSPLk&t=991.48s)]
*  If you train a diffusion model, you have your data and then you add noise to that data [[00:16:35](https://www.youtube.com/watch?v=0tsIbarSPLk&t=995.5600000000001s)]
*  that the model tries to remove. [[00:16:40](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1000.6800000000001s)]
*  And if you do that on a video, you add noise to each frame of the video. [[00:16:43](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1003.24s)]
*  And then we had a bug where we added different amounts of noise to different frames in the [[00:16:47](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1007.0s)]
*  video, which just complicates the learning task further or unnecessarily. [[00:16:52](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1012.1999999999999s)]
*  Things like this is just one line of code that can go wrong. [[00:16:57](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1017.0s)]
*  What was the biggest difference between the image model research project and your video [[00:17:00](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1020.36s)]
*  work? Because noise sampling and noise reduction, these are diffusion techniques that are shared [[00:17:09](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1029.48s)]
*  across images and video. It would be helpful to understand what were unique to the video challenge. [[00:17:18](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1038.1200000000001s)]
*  First of all, the pure dimensionality of videos. I mentioned that before with this additional [[00:17:25](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1045.4s)]
*  dimension. This introduces, of course, a lot of higher GPU or memory consumption. [[00:17:31](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1051.4s)]
*  This was really a challenge. For diffusion models, it's really important to have a high batch size [[00:17:41](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1061.88s)]
*  because you can approximate the gradient, which drives the learning much better. [[00:17:47](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1067.96s)]
*  The batch size is higher, especially for diffusion models. It's really an important [[00:17:56](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1076.36s)]
*  thing to have a really high batch size. But if you just add this additional temporal dimension, [[00:18:02](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1082.9199999999998s)]
*  if something breaks somewhere in one GPU, it will just throw down the entire training. [[00:18:10](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1090.4399999999998s)]
*  And the more GPUs you add to your cluster and the more GPUs you train, the higher the probability [[00:18:18](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1098.36s)]
*  will be that somewhere there's a hardware failure, which also happens. [[00:18:25](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1105.16s)]
*  So this additional dimensionality just introduces these new scaling challenges, [[00:18:31](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1111.0s)]
*  which were really interesting to come by, I would say. [[00:18:39](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1119.16s)]
*  Well, that's very helpful. I think one of the most valuable things that your guys' research [[00:18:43](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1123.96s)]
*  has done for the industry is that you often share in very excruciating detail [[00:18:52](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1132.84s)]
*  some of the infrastructure challenges that came with training the lab's work. [[00:18:59](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1139.9599999999998s)]
*  I think since scaling models at the magnitude that you guys are is a relatively new infrastructure [[00:19:04](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1144.36s)]
*  challenge, I think it's very, very helpful for other researchers to be able to hear the sort of [[00:19:12](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1152.6s)]
*  nuts and bolts that you had to figure out to get these models out. [[00:19:17](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1157.48s)]
*  Then there's the whole other set of data-related challenges that aren't about the data [[00:19:21](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1161.16s)]
*  pipeline per se, but it's about the representation of the data, the data set [[00:19:28](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1168.8400000000001s)]
*  curation, the data set mixture. Could you guys just talk a little bit about how you approached [[00:19:33](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1173.24s)]
*  picking your data set for this? [[00:19:39](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1179.4s)]
*  Yeah, that's a good question. We actually spent a lot of time talking about this in the paper [[00:19:41](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1181.0s)]
*  that we just put out. Roughly, what we also define in this paper is that we can divide this [[00:19:45](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1185.64s)]
*  training process into three stages. The first is that we actually train an image model. [[00:19:54](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1194.44s)]
*  For training video models, it's usually just helpful to reuse the structural spatial understanding [[00:20:00](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1200.92s)]
*  from image models. There are powerful image models that we should reuse [[00:20:06](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1206.92s)]
*  for then training the video model. Then there's next steps. Having an image model, like stable [[00:20:12](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1212.84s)]
*  diffusion, for example, you have to get this additional knowledge about the temporal [[00:20:19](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1219.1599999999999s)]
*  dimensionality and about motion. For that, we train on a large data set. I'm sure Andreas can [[00:20:25](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1225.8799999999999s)]
*  talk about how we filtered that in a second. The next stage is training on this data set [[00:20:35](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1235.3999999999999s)]
*  this really large data set that we still have to create a bit. We don't want, let's say, [[00:20:42](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1242.1200000000001s)]
*  optical characters. We don't want text in the video. We want nice object motion. [[00:20:47](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1247.88s)]
*  We also want nice camera motion. We have to filter for that. We do this in two regimes. We train on [[00:20:54](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1254.8400000000001s)]
*  a lot of videos in the first stage. In the second stage, we train on a more curated, very high [[00:21:02](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1262.1200000000001s)]
*  quality, smaller data set to really refine the model. It's similar to image models where you [[00:21:07](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1267.8s)]
*  also train pre-train on a large data set and then refine on a high quality data set. There was a paper [[00:21:13](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1273.8s)]
*  recently that Meta put out that also describes just this process for image models in detail. [[00:21:19](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1279.3999999999999s)]
*  One of the largest open questions in video generation for a while has been structural [[00:21:24](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1284.52s)]
*  consistency of 3D objects. When the camera is panning around a person or a car or any subject, [[00:21:29](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1289.72s)]
*  to make sure that it stays and looks like the same subject from various angles has been a challenge [[00:21:39](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1299.64s)]
*  for generative video. How did you guys approach that? You mentioned in the paper that 3D data [[00:21:46](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1306.3600000000001s)]
*  and multi-view data was important. Yeah, actually, I think the main point we want to make in the [[00:21:53](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1313.56s)]
*  paper is the one that we talked about earlier. Having a foundational video model actually [[00:21:59](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1319.24s)]
*  gives us much more than just a model that can generate nice looking clips or videos. [[00:22:07](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1327.48s)]
*  It learns a representation of the world. One aspect of that is that we tried to demonstrate [[00:22:15](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1335.48s)]
*  in the paper that given a video model which has seen a lot of objects from different views, [[00:22:24](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1344.12s)]
*  lots of different camera movements, it should be much more easy to turn that into a multi-view model. [[00:22:30](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1350.4399999999998s)]
*  That's kind of the main message. We take the pre-trained video model which has seen a lot [[00:22:36](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1356.36s)]
*  of different videos, a lot of different camera movements, and we fine-tune that on very specialized [[00:22:41](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1361.32s)]
*  multi-view orbits around 3D objects and kind of turn the video model into a multi-view synthesis [[00:22:47](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1367.48s)]
*  model. That works pretty well. One of the dominating approaches before that was that you [[00:22:56](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1376.12s)]
*  would take an image model like stable diffusion and turn that into a multi-view model. We showed [[00:23:03](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1383.08s)]
*  that it's actually helpful to incorporate this implicit 3D knowledge that is captured in all of [[00:23:07](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1387.8799999999999s)]
*  the videos into the model and then the model can learn much quicker than if you start from [[00:23:14](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1394.76s)]
*  the pure image model. That's kind of the main message. You're right, you can also try to use [[00:23:19](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1399.16s)]
*  this explicit multi-view data in the video training or maybe even something that we do in the paper, [[00:23:26](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1406.6s)]
*  train LORAS explicitly on different camera movements and then put this LORAS back into [[00:23:35](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1415.32s)]
*  the video model so you get control over the camera for your very general video model, [[00:23:44](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1424.12s)]
*  which is quite cool. This I found was one of the coolest pieces of the paper was [[00:23:50](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1430.4399999999998s)]
*  incorporating LORAS for fine-grain control in the creation process. Could you maybe give us a quick [[00:23:57](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1437.6399999999999s)]
*  overview of what LORAS even are conceptually, intuitively, and then what led you to the [[00:24:03](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1443.1599999999999s)]
*  intuition that LORAS would be an important part of the architecture? LORAS are just really lightweight [[00:24:08](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1448.28s)]
*  adapters which can be fine-tuned onto an existing base model which adapts the attention layers and [[00:24:12](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1452.92s)]
*  by that you can just like on a smaller kind of subset or a small really highly specialized [[00:24:19](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1459.3200000000002s)]
*  data set you can tune in a really really lightweight way different properties into the model. [[00:24:27](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1467.96s)]
*  And in this case we just like tuned this kind of [[00:24:35](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1475.96s)]
*  understanding of different kinds of camera motion into our video model. So if we use a [[00:24:42](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1482.3600000000001s)]
*  small data set which only contains like zooms or panning to the left or to the right, we can actually [[00:24:49](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1489.96s)]
*  tune such a LORAS as a small adapter to the attention layers of our model [[00:24:58](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1498.3600000000001s)]
*  to just like get exactly this behavior and this is a really awesome way to just in a really [[00:25:04](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1504.76s)]
*  lightweight way fine-tune these foundational models and it has shown to be like really effective and [[00:25:09](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1509.8799999999999s)]
*  accordingly it's like really highly appreciated in the community I would say to get these kind [[00:25:16](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1516.76s)]
*  of easy fine-tunes for these models. Yeah and I think like for image models it's like extremely [[00:25:22](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1522.36s)]
*  popular. There's so many different LORAS that people plug into these models. For video models [[00:25:27](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1527.1599999999999s)]
*  our goal was just to demonstrate that this is something that's possible. It's just like [[00:25:32](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1532.04s)]
*  at the beginning and there's much more than it's possible. That should be possible like very [[00:25:37](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1537.3999999999999s)]
*  specialized kind of motions. I don't know I think there's a lot of creative possibilities. [[00:25:42](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1542.76s)]
*  That's actually I think worth exploring for a little bit. One of the sort of windows that [[00:25:46](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1546.84s)]
*  you guys have into the future is by understanding where the research is going I think you get to see [[00:25:53](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1553.24s)]
*  and you get to live a little bit in the future you get to time travel and kind of get a [[00:25:59](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1559.48s)]
*  glimpse into the future of creativity and so having seen how effective LORAS are at least at [[00:26:05](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1565.24s)]
*  a few set of tasks like motion control right so you in the paper you propose using LORAS for [[00:26:11](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1571.88s)]
*  camera control, panning, zooming, etc. The history of video creation has usually required creators to [[00:26:18](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1578.2s)]
*  have a ton of different knobs and dials in their software that they use right whether it's an Adobe [[00:26:27](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1587.96s)]
*  After Effects or some other professional software you literally have hundreds of dials and buttons [[00:26:33](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1593.24s)]
*  that you can use to control and edit these videos and conceptually should people think about LORAS [[00:26:39](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1599.32s)]
*  as mapping to these controls in the future will a director or creator of videos basically be relying [[00:26:46](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1606.52s)]
*  on hundreds of different LORAS to express the control they want over the video or do you think [[00:26:54](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1614.84s)]
*  fundamentally LORAS will hit some scaling sort of limit how should creators think about these new [[00:27:00](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1620.36s)]
*  tools that you've given them? I mean you actually you actually said it right maintaining like a [[00:27:07](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1627.72s)]
*  library of hundreds of LORAS is maybe not like the most scalable approach. Actually if you look at [[00:27:13](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1633.08s)]
*  the model that we put out now it's just like taking an image and animating that right then we [[00:27:19](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1639.4s)]
*  can do some stuff like with these LORAS but what you actually want I think is giving the image [[00:27:25](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1645.3200000000002s)]
*  and some text prompts do exactly what I described in the text prompt there is also some there's [[00:27:31](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1651.72s)]
*  already some work that explores that but yeah giving like more control over what happens in the video [[00:27:37](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1657.48s)]
*  be it through LORAS but maybe through a text prompt or through like yeah you know like spatial [[00:27:44](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1664.1200000000001s)]
*  motion guidance like in Runways motion brush there are different ways of doing that but you [[00:27:50](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1670.44s)]
*  definitely want like more control over this whole creation process and then I think [[00:27:55](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1675.64s)]
*  you're at the stage where you can really start to generate personalized individual content [[00:28:01](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1681.5600000000002s)]
*  individual short movies I don't know and maybe even for like especially probably for [[00:28:08](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1688.76s)]
*  for video creation we want something like we just did with the image models you want like very fast [[00:28:15](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1695.0800000000002s)]
*  synthesis right because then this will become more like I don't know sometimes I think about this as [[00:28:20](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1700.92s)]
*  like like like a video game right you you type your prompt you immediately see what happens [[00:28:27](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1707.32s)]
*  given your input view and I think this might be a super nice user experience actually [[00:28:32](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1712.92s)]
*  so we want this additional control and we want fast rendering fast sampling [[00:28:37](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1717.32s)]
*  fast synthesis of this model of these models you know you said earlier that you're hoping that the [[00:28:44](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1724.36s)]
*  community explores more things now that you've actually put the model out there you know they're [[00:28:48](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1728.6799999999998s)]
*  going to be developers and creators who listen to this podcast what would you like them to explore [[00:28:54](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1734.12s)]
*  first and and most intensely now well I think just trying out the model rendering some awesome [[00:28:58](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1738.52s)]
*  stuff of course also further exploring maybe the representation we built we we in the paper we [[00:29:06](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1746.84s)]
*  mentioned that we trained this model on a whole lot of data and this has just seen really really [[00:29:12](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1752.2s)]
*  much motion be it on like low resolution but this this knowledge like just has seen all this and the [[00:29:18](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1758.68s)]
*  representation is really fruitful we showed that by our 3d fine tuning this was by the way this was [[00:29:25](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1765.4s)]
*  completely surprising for me seeing that model after thousand two thousand iterations like like [[00:29:30](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1770.92s)]
*  already get getting what is like 3d reasoning or like explicit 3d reasoning this this is really [[00:29:37](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1777.3200000000002s)]
*  really nice and so as we saw that it will be extremely interesting to see other such approaches [[00:29:43](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1783.88s)]
*  models open source people can try it and i i think it won't like give it another couple of weeks and [[00:29:52](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1792.12s)]
*  then we will we will see what happens but i'm excited for it you know my personal favorite for [[00:29:58](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1798.76s)]
*  what people did on day one was obviously animating memes i'm sure you guys have seen all that that [[00:30:03](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1803.6399999999999s)]
*  that was really funny yeah what are your guys's favorite creations so far that you've seen [[00:30:09](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1809.8799999999999s)]
*  anything that jumps to mind the one where the man is looking after another woman it's the man looking [[00:30:14](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1814.2s)]
*  behind right the man looking behind the other one i think it just like visualizes just like the [[00:30:19](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1819.32s)]
*  additional experience that a video model can provide right so it's actually really nice to [[00:30:25](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1825.32s)]
*  people are used to like 2d memes but then you see oh i can i can actually try to animate this and [[00:30:32](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1832.04s)]
*  see what happens also if you think about famous artworks or something just bringing them to life [[00:30:38](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1838.52s)]
*  is a really really uh like nice property and it's now enabled everyone can just like [[00:30:43](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1843.48s)]
*  poke around a bit on monalisa and see what she's looking from the side oh that's cool i haven't [[00:30:49](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1849.16s)]
*  explored that one but that that's a you're saying prompting the model with an image of a of a [[00:30:55](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1855.0s)]
*  notable art piece um exactly just just like making van gogh's uh starry night like like make the [[00:30:59](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1859.64s)]
*  stars shine and and glimmer and stuff so i think it's really cool the world is pretty lucky that [[00:31:08](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1868.44s)]
*  you guys have gifted you know this the model to the developer and open source ecosystem [[00:31:14](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1874.28s)]
*  it's it's already such an incredible sort of step leap forward right in what you can do with [[00:31:21](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1881.0800000000002s)]
*  with images and with with video what do you think are the top the two or three biggest sort of open [[00:31:27](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1887.16s)]
*  challenges that you guys want to prioritize next that are still limitations in video generation [[00:31:34](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1894.76s)]
*  i think a really important thing is to get these models to generate longer videos to process longer [[00:31:39](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1899.8799999999999s)]
*  videos in general not only generate them also like see them and and and process them because [[00:31:46](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1906.44s)]
*  i think eventually processing longer videos is like key to understanding [[00:31:52](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1912.84s)]
*  what we talked about earlier fundamental aspects of this world better this is a really important [[00:31:59](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1919.6399999999999s)]
*  part um to just like make these models or enable these models to generate longer content more [[00:32:04](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1924.9199999999998s)]
*  coherent content also with the lots and other kinds of motion um and what robin already said [[00:32:12](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1932.12s)]
*  i think making them fast will just like it will just like unlock so much more exploration yeah [[00:32:18](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1938.84s)]
*  yeah there are simple things like thinking about like multimodality um adding an audio track to [[00:32:27](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1947.48s)]
*  your generated video so you're talking andrea i said earlier about the infrastructure challenges [[00:32:34](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1954.52s)]
*  here if you had a magic wand what infrastructure improvements do you wish the industry could solve [[00:32:39](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1959.08s)]
*  for you hard question i mean we could we could we could ask for more gpus per more cpus per gpu [[00:32:45](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1965.1599999999999s)]
*  and this would like solve solve much of the data loading issues also for more like like not only [[00:32:52](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1972.84s)]
*  gpu memory is always good but also cpu memory but i think like hitting these limits is just a [[00:32:59](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1979.24s)]
*  some form of natural way you you always want to try to improve your efficiency you always want [[00:33:07](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1987.8s)]
*  to try to to to train faster um and and at some point you will face a bottleneck a limit and you [[00:33:14](https://www.youtube.com/watch?v=0tsIbarSPLk&t=1994.6s)]
*  have to like come up with a with a nice algorithmic way maybe or with another way of like like [[00:33:21](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2001.3999999999999s)]
*  overcoming this for many years data loading was not a a big thing because this gpus were too slow [[00:33:27](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2007.0s)]
*  but now we have extremely nice accelerators and and like like with the newest h100s this is it's [[00:33:34](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2014.36s)]
*  insane how fast these these gpus are actually running not how fast you can train models on those [[00:33:40](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2020.36s)]
*  and then you will just like hit the next bottleneck it's actually good to see this [[00:33:47](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2027.0s)]
*  that we hit limits we have to overcome this and then you improve and this is how you learn and [[00:33:50](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2030.8400000000001s)]
*  and this is how you you can yeah make things much more efficient in the end if you only rely on like [[00:33:56](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2036.8400000000001s)]
*  more compute um it's a bit boring um i think like compute constraints can also drive innovation [[00:34:04](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2044.8400000000001s)]
*  right so uh for example the latent diffusion framework we developed it at the university [[00:34:12](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2052.52s)]
*  because uh we just like we had like single gpus uh right where we train on and um yeah i mean uh [[00:34:17](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2057.64s)]
*  that kind of naturally leads to some kind of innovation and in this case this is something [[00:34:26](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2066.7599999999998s)]
*  that everyone uses right now this is it's actually crazy to see a dolly three uses a model that like [[00:34:30](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2070.68s)]
*  the autoencoder that was trained on a single gpu this is i think how intelligence also arises if [[00:34:36](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2076.6s)]
*  you have a constrained environment you have to come up with a smarter way of um doing things [[00:34:44](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2084.28s)]
*  and this is how how without without any limitations you yeah that there wouldn't be as as like [[00:34:51](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2091.0800000000004s)]
*  those nice solutions for many problems we have right now yeah no constraints no creativity right [[00:35:00](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2100.6000000000004s)]
*  exactly and i and i do think one of the underappreciated parts of your guys's group [[00:35:07](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2107.0s)]
*  ever since your university days has been just how compute efficient a lot of your research has been [[00:35:13](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2113.32s)]
*  um i i certainly have talked to so many university level uh researchers you know grad students post [[00:35:19](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2119.0800000000004s)]
*  docs um who saw that research that you guys put out a year and a half ago with stable diffusion [[00:35:25](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2125.48s)]
*  and felt really inspired university and academic environments are somewhat compute constrained [[00:35:30](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2130.04s)]
*  and so i i think even though now you have access to tons of compute um the sort of self-imposed [[00:35:35](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2135.6400000000003s)]
*  compute constraints uh leading to it makes me very happy to hear that those those are something [[00:35:41](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2141.48s)]
*  those constraints are something you guys think as a our feature not a bug right i will probably keep [[00:35:47](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2147.72s)]
*  the open source ecosystem pretty vibrant but in addition you're often sort of racing and responding [[00:35:53](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2153.32s)]
*  to other labs as well in the field some of these labs are much better funded than you are bigger [[00:35:58](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2158.52s)]
*  than you um so how do you think about prioritizing your research pipelines and your timelines [[00:36:04](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2164.2s)]
*  and how would you say that's different than labs that are largely academic yeah that's a it's a very [[00:36:10](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2170.04s)]
*  good point i think like um actually this whole competition it also drives the field of ai [[00:36:16](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2176.4399999999996s)]
*  it's probably very important to not get distracted but it's too much but of course like since one and [[00:36:21](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2181.56s)]
*  a half years everyone is doing something with diffusion it can actually be quite fun to yeah [[00:36:27](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2187.24s)]
*  work in this competitive environment um i think everyone here enjoys doing that it's it's quite [[00:36:33](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2193.16s)]
*  fun that we have like this this lab here in germany actually we compete with i don't know open ai [[00:36:38](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2198.68s)]
*  um google other research labs across the world it's it's quite fun um it's intense definitely [[00:36:44](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2204.9199999999996s)]
*  but it's it's a lot of fun i think in the end we're not a big lab but i think we're really like [[00:36:51](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2211.48s)]
*  all having kind of the same spirit and we're feeling like that that we're working on something [[00:36:59](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2219.8799999999997s)]
*  which makes sense and which in the end gives not only us something for us it's also really cool [[00:37:05](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2225.16s)]
*  but also we can give something back to the community to other researchers which might have [[00:37:11](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2231.72s)]
*  not the resources we have well what i what i love about um the lab and the group you guys have put [[00:37:17](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2237.7999999999997s)]
*  together the philosophy of of the rising tide lifts all boats right because you guys publish [[00:37:25](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2245.24s)]
*  your research for the world to use and i thought one of the coolest things about the dali3 paper [[00:37:31](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2251.8s)]
*  was the citations list which included your work right and so they were saying they were sort of [[00:37:37](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2257.6400000000003s)]
*  thanking you for the work that the the stable diffusion group had put together put out i think [[00:37:42](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2262.52s)]
*  your work ends up benefiting all kinds of labs across the industry and so while the competition [[00:37:48](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2268.92s)]
*  can be intense it's it's also one of the best most most inspiring examples of industries [[00:37:53](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2273.88s)]
*  of an industry helping each other out and it comes up all the time in conversations with [[00:38:01](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2281.32s)]
*  researchers at many of the labs we just talked about that they're very grateful for the research [[00:38:06](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2286.1200000000003s)]
*  you guys do so i hope you keep doing that yeah just shows that it is super important to have this kind [[00:38:10](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2290.1200000000003s)]
*  of um contribution to open and accessible models and everyone in our team is super motivated to [[00:38:17](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2297.2400000000002s)]
*  contribute to that so um couldn't couldn't imagine doing anything else right now thank you so much [[00:38:25](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2305.32s)]
*  for listening to the a16z podcast if you've made it this far don't forget to subscribe so that you [[00:38:31](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2311.88s)]
*  are the first to get our exclusive video content or you can check out this video that we've hand [[00:38:37](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2317.48s)]
*  selected for you [[00:38:44](https://www.youtube.com/watch?v=0tsIbarSPLk&t=2324.12s)]
