---
Date Generated: June 13, 2025
Transcription Model: whisper medium 20231117
Length: 2133s
Video Keywords: ['anthropic', 'anthropic ai', 'anthropic claude', 'ai', 'artificial intelligence', 'openai', 'claude', 'anthropic ai ceo', 'Anthropic CEO Dario Amodei', 'wsj', 'wsj interview', 'dario amodei', 'dario amodei interview', 'human level ai', 'ai news', 'ai tools', 'anthropic news', 'new ai tools', 'claude ai', 'new claude 3.5 sonnet', 'ai agent startup', 'ai desktop control', 'daniela amodei', 'generative ai', 'ai automation', 'conference', 'big tech', 'techy']
Video Views: 144268
Video Rating: None
Video Description: At WSJ Journal House Davos, Anthropic CEO Dario Amodei outlines Claude’s next chapter—from web browsing, voice to more advanced models—while predicting that AI could reach human-level intelligence within just a few years.
#AI #Tech #WSJ
---

# Inside Anthropic's Race to Build a Smarter Claude and Human-Level AI | WSJ
**The Wall Street Journal - Interviews:** [January 21, 2025](https://www.youtube.com/watch?v=snkOMOjiVOk)
*  Well, I want to start with a thank you because I built a Davos assistant using Claude and [[00:00:00](https://www.youtube.com/watch?v=snkOMOjiVOk&t=0.0s)]
*  I mean really my boss should thank you. [[00:00:06](https://www.youtube.com/watch?v=snkOMOjiVOk&t=6.72s)]
*  It doesn't get jet lagged, we don't have to pay for it, any lodging and it's been so helpful [[00:00:08](https://www.youtube.com/watch?v=snkOMOjiVOk&t=8.3s)]
*  to me. [[00:00:13](https://www.youtube.com/watch?v=snkOMOjiVOk&t=13.9s)]
*  It's got my calendar, it's got my notes, it's got all of these things but there are some [[00:00:14](https://www.youtube.com/watch?v=snkOMOjiVOk&t=14.9s)]
*  shortcomings and I wanted to go through some of the product questions and I wanted to first [[00:00:19](https://www.youtube.com/watch?v=snkOMOjiVOk&t=19.26s)]
*  get a sense of people in the crowd. [[00:00:22](https://www.youtube.com/watch?v=snkOMOjiVOk&t=22.94s)]
*  How many people here use Claude? [[00:00:24](https://www.youtube.com/watch?v=snkOMOjiVOk&t=24.7s)]
*  Any power users? [[00:00:26](https://www.youtube.com/watch?v=snkOMOjiVOk&t=26.12s)]
*  Okay, perfect. [[00:00:27](https://www.youtube.com/watch?v=snkOMOjiVOk&t=27.7s)]
*  And this is going to be a great conversation for everyone. [[00:00:29](https://www.youtube.com/watch?v=snkOMOjiVOk&t=29.0s)]
*  So I wanted to go through some of the list of the stuff that I wanted. [[00:00:33](https://www.youtube.com/watch?v=snkOMOjiVOk&t=33.08s)]
*  It's going to be a fun game. [[00:00:37](https://www.youtube.com/watch?v=snkOMOjiVOk&t=37.56s)]
*  You can decide if you want to tell me exact timing, you can tell me if it's coming, not [[00:00:38](https://www.youtube.com/watch?v=snkOMOjiVOk&t=38.62s)]
*  coming but so here we go. [[00:00:42](https://www.youtube.com/watch?v=snkOMOjiVOk&t=42.480000000000004s)]
*  Access to the web. [[00:00:45](https://www.youtube.com/watch?v=snkOMOjiVOk&t=45.28s)]
*  Access to the web. [[00:00:46](https://www.youtube.com/watch?v=snkOMOjiVOk&t=46.78s)]
*  So yeah, that is an area that we're working on and is going to come relatively soon. [[00:00:47](https://www.youtube.com/watch?v=snkOMOjiVOk&t=47.78s)]
*  I think one thing to understand about Anthropic is the majority of our business is enterprise [[00:00:54](https://www.youtube.com/watch?v=snkOMOjiVOk&t=54.18s)]
*  focused and so often enterprise focused things get prioritized first. [[00:00:59](https://www.youtube.com/watch?v=snkOMOjiVOk&t=59.3s)]
*  Web is more on the consumer side of things but we've been working on it for a while. [[00:01:04](https://www.youtube.com/watch?v=snkOMOjiVOk&t=64.25999999999999s)]
*  We have some ideas that we think are different from what other model providers have done [[00:01:08](https://www.youtube.com/watch?v=snkOMOjiVOk&t=68.82s)]
*  and so I'm not going to give an exact date but that's coming very soon. [[00:01:13](https://www.youtube.com/watch?v=snkOMOjiVOk&t=73.53999999999999s)]
*  We understand the importance of it for consumers and specifically for power users on the consumer [[00:01:17](https://www.youtube.com/watch?v=snkOMOjiVOk&t=77.14s)]
*  side and so we are in fact prioritizing it. [[00:01:23](https://www.youtube.com/watch?v=snkOMOjiVOk&t=83.32s)]
*  Love it. [[00:01:26](https://www.youtube.com/watch?v=snkOMOjiVOk&t=86.52s)]
*  Okay. [[00:01:27](https://www.youtube.com/watch?v=snkOMOjiVOk&t=87.52s)]
*  Two is a voice mode. [[00:01:28](https://www.youtube.com/watch?v=snkOMOjiVOk&t=88.52s)]
*  A voice mode. [[00:01:29](https://www.youtube.com/watch?v=snkOMOjiVOk&t=89.52s)]
*  So I can talk to it and talk back to me. [[00:01:30](https://www.youtube.com/watch?v=snkOMOjiVOk&t=90.52s)]
*  I think that will come eventually. [[00:01:32](https://www.youtube.com/watch?v=snkOMOjiVOk&t=92.88s)]
*  We currently have, you can, Claude can transcribe your voice and it can also read things out [[00:01:36](https://www.youtube.com/watch?v=snkOMOjiVOk&t=96.52s)]
*  but the kind of two way audio mode, that is something we are going to do at some point [[00:01:45](https://www.youtube.com/watch?v=snkOMOjiVOk&t=105.84s)]
*  but again, less usage of that on the enterprise side and some of the power users so that will [[00:01:51](https://www.youtube.com/watch?v=snkOMOjiVOk&t=111.82000000000001s)]
*  happen eventually. [[00:01:57](https://www.youtube.com/watch?v=snkOMOjiVOk&t=117.86s)]
*  A photo, let's say photo generation. [[00:01:59](https://www.youtube.com/watch?v=snkOMOjiVOk&t=119.46000000000001s)]
*  Photo generation. [[00:02:02](https://www.youtube.com/watch?v=snkOMOjiVOk&t=122.54s)]
*  Yeah, so I've often seen generation of images or video as somewhat separate from a lot of [[00:02:03](https://www.youtube.com/watch?v=snkOMOjiVOk&t=123.54s)]
*  the rest of the stuff in generative AI. [[00:02:13](https://www.youtube.com/watch?v=snkOMOjiVOk&t=133.58s)]
*  If you think about what humans do, humans ingest audio, photos, video but they can't [[00:02:15](https://www.youtube.com/watch?v=snkOMOjiVOk&t=135.04000000000002s)]
*  actually make photos. [[00:02:24](https://www.youtube.com/watch?v=snkOMOjiVOk&t=144.12s)]
*  I mean you have painters but they can't actually make photos that easily. [[00:02:25](https://www.youtube.com/watch?v=snkOMOjiVOk&t=145.12s)]
*  I've often seen this as somewhat different and I think on the safety and security side [[00:02:29](https://www.youtube.com/watch?v=snkOMOjiVOk&t=149.32000000000002s)]
*  there are a number of unique issues associated with image generation, video generation that [[00:02:34](https://www.youtube.com/watch?v=snkOMOjiVOk&t=154.76000000000002s)]
*  are not associated with text. [[00:02:39](https://www.youtube.com/watch?v=snkOMOjiVOk&t=159.8s)]
*  That's why I think there's not that much enterprise use case for these. [[00:02:44](https://www.youtube.com/watch?v=snkOMOjiVOk&t=164.26000000000002s)]
*  So this is generally an area we don't plan to prioritize. [[00:02:46](https://www.youtube.com/watch?v=snkOMOjiVOk&t=166.94s)]
*  If it turns out to be important on the consumer side, we may simply contract with one of the [[00:02:50](https://www.youtube.com/watch?v=snkOMOjiVOk&t=170.46s)]
*  companies that specializes in working on this. [[00:02:54](https://www.youtube.com/watch?v=snkOMOjiVOk&t=174.18s)]
*  That makes sense. [[00:02:56](https://www.youtube.com/watch?v=snkOMOjiVOk&t=176.5s)]
*  So I also asked this on Twitter or X or whatever it's called these days and I got 200 responses [[00:02:57](https://www.youtube.com/watch?v=snkOMOjiVOk&t=177.5s)]
*  basically, I got 200 responses to everything but the majority of them were asking for higher [[00:03:03](https://www.youtube.com/watch?v=snkOMOjiVOk&t=183.14000000000001s)]
*  rate limits. [[00:03:07](https://www.youtube.com/watch?v=snkOMOjiVOk&t=187.46s)]
*  Yes. [[00:03:08](https://www.youtube.com/watch?v=snkOMOjiVOk&t=188.86s)]
*  So we are working very hard on that. [[00:03:09](https://www.youtube.com/watch?v=snkOMOjiVOk&t=189.86s)]
*  What has happened is that the surge in demand we've seen over the last year and particularly [[00:03:12](https://www.youtube.com/watch?v=snkOMOjiVOk&t=192.7s)]
*  in the last three months has overwhelmed our ability to provide the needed compute. [[00:03:17](https://www.youtube.com/watch?v=snkOMOjiVOk&t=197.72s)]
*  If you want to buy compute in any significant quantity, there's a lead time for doing so. [[00:03:23](https://www.youtube.com/watch?v=snkOMOjiVOk&t=203.76s)]
*  Our revenue grew by roughly 10X in the last year from something that was not, I won't [[00:03:30](https://www.youtube.com/watch?v=snkOMOjiVOk&t=210.16s)]
*  give exact numbers but from the order of 100 million to the order of 1 billion. [[00:03:37](https://www.youtube.com/watch?v=snkOMOjiVOk&t=217.23999999999998s)]
*  It's not slowing down. [[00:03:42](https://www.youtube.com/watch?v=snkOMOjiVOk&t=222.88000000000002s)]
*  So we're bringing on efficiency improvements as fast as we can. [[00:03:44](https://www.youtube.com/watch?v=snkOMOjiVOk&t=224.08s)]
*  We're also, as we announced with Amazon at reInvent, we're going to have a cluster of [[00:03:48](https://www.youtube.com/watch?v=snkOMOjiVOk&t=228.32000000000002s)]
*  trinium two, of several hundred thousand trinium two. [[00:03:53](https://www.youtube.com/watch?v=snkOMOjiVOk&t=233.3s)]
*  I would not be surprised if in 2026 we have more than a million of some kind of chip. [[00:03:56](https://www.youtube.com/watch?v=snkOMOjiVOk&t=236.18s)]
*  So we're working as fast as we can to bring those chips online and to make inference on [[00:04:03](https://www.youtube.com/watch?v=snkOMOjiVOk&t=243.28s)]
*  them as efficient as possible but it just takes time. [[00:04:10](https://www.youtube.com/watch?v=snkOMOjiVOk&t=250.34s)]
*  We've seen this enormous surge in demand and we're working as fast as we can to provide [[00:04:13](https://www.youtube.com/watch?v=snkOMOjiVOk&t=253.56s)]
*  for all that demand. [[00:04:18](https://www.youtube.com/watch?v=snkOMOjiVOk&t=258.59999999999997s)]
*  And probably even keeping, the experience will get better even for those that are in [[00:04:19](https://www.youtube.com/watch?v=snkOMOjiVOk&t=259.98s)]
*  the paying group now. [[00:04:23](https://www.youtube.com/watch?v=snkOMOjiVOk&t=263.65999999999997s)]
*  Yes. [[00:04:25](https://www.youtube.com/watch?v=snkOMOjiVOk&t=265.15999999999997s)]
*  Yes. [[00:04:26](https://www.youtube.com/watch?v=snkOMOjiVOk&t=266.15999999999997s)]
*  I'm going to hold you to that one. [[00:04:27](https://www.youtube.com/watch?v=snkOMOjiVOk&t=267.15999999999997s)]
*  And lastly, a memory feature, a way for Claude to remember things not only in a project but [[00:04:29](https://www.youtube.com/watch?v=snkOMOjiVOk&t=269.88s)]
*  across the... [[00:04:35](https://www.youtube.com/watch?v=snkOMOjiVOk&t=275.6s)]
*  Yeah, that's really important. [[00:04:36](https://www.youtube.com/watch?v=snkOMOjiVOk&t=276.6s)]
*  That's I think a part of our broader vision of kind of what we call virtual collaborators [[00:04:38](https://www.youtube.com/watch?v=snkOMOjiVOk&t=278.3s)]
*  and I think an important part of that is you think about when you talk to a coworker, that [[00:04:43](https://www.youtube.com/watch?v=snkOMOjiVOk&t=283.46s)]
*  coworker, they need to remember conversations that you've had before. [[00:04:48](https://www.youtube.com/watch?v=snkOMOjiVOk&t=288.3s)]
*  So that's very important and that is coming soon. [[00:04:51](https://www.youtube.com/watch?v=snkOMOjiVOk&t=291.59999999999997s)]
*  And you mentioned before that you really are focused on enterprise but as a consumer, and [[00:04:55](https://www.youtube.com/watch?v=snkOMOjiVOk&t=295.34s)]
*  I mean we're all consumers as even working in our companies, the character of Claude [[00:04:59](https://www.youtube.com/watch?v=snkOMOjiVOk&t=299.5s)]
*  is just so compelling. [[00:05:05](https://www.youtube.com/watch?v=snkOMOjiVOk&t=305.26s)]
*  It's one of the things that makes it so useful and makes it that I find it, myself going [[00:05:06](https://www.youtube.com/watch?v=snkOMOjiVOk&t=306.58s)]
*  back to Claude versus another LLM or a product. [[00:05:13](https://www.youtube.com/watch?v=snkOMOjiVOk&t=313.14s)]
*  How are you thinking about that though? [[00:05:18](https://www.youtube.com/watch?v=snkOMOjiVOk&t=318.14s)]
*  I mean you've designed this character, you've designed a character that works in enterprise. [[00:05:19](https://www.youtube.com/watch?v=snkOMOjiVOk&t=319.54s)]
*  What about the consumer? [[00:05:24](https://www.youtube.com/watch?v=snkOMOjiVOk&t=324.7s)]
*  Yeah. [[00:05:25](https://www.youtube.com/watch?v=snkOMOjiVOk&t=325.9s)]
*  So actually I think Claude character is important on both sides, right? [[00:05:26](https://www.youtube.com/watch?v=snkOMOjiVOk&t=326.9s)]
*  Because it's obviously very important to consumers interacting. [[00:05:30](https://www.youtube.com/watch?v=snkOMOjiVOk&t=330.22s)]
*  I think it's also important on the enterprise side because we serve some companies like [[00:05:33](https://www.youtube.com/watch?v=snkOMOjiVOk&t=333.46s)]
*  Intercom and many others that do customer service. [[00:05:39](https://www.youtube.com/watch?v=snkOMOjiVOk&t=339.7s)]
*  And so the model then gets used for customer service and the mode of interaction is important [[00:05:43](https://www.youtube.com/watch?v=snkOMOjiVOk&t=343.5s)]
*  there as well. [[00:05:49](https://www.youtube.com/watch?v=snkOMOjiVOk&t=349.06s)]
*  Even applications like coding, the character is really important. [[00:05:50](https://www.youtube.com/watch?v=snkOMOjiVOk&t=350.06s)]
*  There was a study from Stanford Medical School just last week where they compared between [[00:05:53](https://www.youtube.com/watch?v=snkOMOjiVOk&t=353.78s)]
*  Claude and other models, both the accuracy of the models for things like radiology image [[00:06:01](https://www.youtube.com/watch?v=snkOMOjiVOk&t=361.66s)]
*  analysis, but also how much do the doctors adopt what the model suggested. [[00:06:07](https://www.youtube.com/watch?v=snkOMOjiVOk&t=367.5s)]
*  And what they found is that Claude, the doctors actually listened to Claude, they adopted [[00:06:12](https://www.youtube.com/watch?v=snkOMOjiVOk&t=372.82s)]
*  its recommendations much more. [[00:06:18](https://www.youtube.com/watch?v=snkOMOjiVOk&t=378.1s)]
*  And I think that has something to do with the way they're interacting with it. [[00:06:20](https://www.youtube.com/watch?v=snkOMOjiVOk&t=380.14s)]
*  And so I think on both the enterprise side and the consumer side, we've put a lot of [[00:06:23](https://www.youtube.com/watch?v=snkOMOjiVOk&t=383.86s)]
*  effort into making sure that interacting with Claude is a good experience and an authentic [[00:06:28](https://www.youtube.com/watch?v=snkOMOjiVOk&t=388.22s)]
*  experience. [[00:06:33](https://www.youtube.com/watch?v=snkOMOjiVOk&t=393.18s)]
*  And I think one reason we've put so much effort into this is we see it as having a larger [[00:06:34](https://www.youtube.com/watch?v=snkOMOjiVOk&t=394.74s)]
*  significance. [[00:06:39](https://www.youtube.com/watch?v=snkOMOjiVOk&t=399.06s)]
*  In the world where individual consumers, especially for productivity, are interacting with models [[00:06:40](https://www.youtube.com/watch?v=snkOMOjiVOk&t=400.22s)]
*  for hours a day, they're acting like their assistants. [[00:06:48](https://www.youtube.com/watch?v=snkOMOjiVOk&t=408.78000000000003s)]
*  Or at work, again, you have these as your very smart assistants who can probably solve [[00:06:52](https://www.youtube.com/watch?v=snkOMOjiVOk&t=412.58000000000004s)]
*  math problems much better than you can. [[00:06:57](https://www.youtube.com/watch?v=snkOMOjiVOk&t=417.98s)]
*  The level of intertwinement there, I think we really need to get it right. [[00:07:00](https://www.youtube.com/watch?v=snkOMOjiVOk&t=420.66s)]
*  That doesn't just mean being engaging and friendly. [[00:07:05](https://www.youtube.com/watch?v=snkOMOjiVOk&t=425.02000000000004s)]
*  It means the model needs to interact with you in a way that after months and years of [[00:07:07](https://www.youtube.com/watch?v=snkOMOjiVOk&t=427.84000000000003s)]
*  interacting with the model, after it becomes part of your workflow, you are actually better [[00:07:13](https://www.youtube.com/watch?v=snkOMOjiVOk&t=433.08000000000004s)]
*  off as a person. [[00:07:16](https://www.youtube.com/watch?v=snkOMOjiVOk&t=436.90000000000003s)]
*  You become more productive, you learn things. [[00:07:19](https://www.youtube.com/watch?v=snkOMOjiVOk&t=439.40000000000003s)]
*  Of course the model gets work done for you, helps you do work. [[00:07:22](https://www.youtube.com/watch?v=snkOMOjiVOk&t=442.5s)]
*  In the long run, that relationship has to be productive. [[00:07:26](https://www.youtube.com/watch?v=snkOMOjiVOk&t=446.9s)]
*  I think that didn't go well for things like, for example, social media. [[00:07:29](https://www.youtube.com/watch?v=snkOMOjiVOk&t=449.94s)]
*  The biggest problem, I think, with social media is that it's really engaging. [[00:07:35](https://www.youtube.com/watch?v=snkOMOjiVOk&t=455.42s)]
*  It gives you that quick dopamine hit. [[00:07:40](https://www.youtube.com/watch?v=snkOMOjiVOk&t=460.74s)]
*  But I think we all suspect that over the long term, it's doing something unhealthy to people. [[00:07:43](https://www.youtube.com/watch?v=snkOMOjiVOk&t=463.14s)]
*  And I've really taken that lesson to heart and I want to make sure AI is not like that. [[00:07:48](https://www.youtube.com/watch?v=snkOMOjiVOk&t=468.3s)]
*  And Claude character is one of the, I think, the most important pieces of that. [[00:07:52](https://www.youtube.com/watch?v=snkOMOjiVOk&t=472.26s)]
*  Absolutely. [[00:07:55](https://www.youtube.com/watch?v=snkOMOjiVOk&t=475.58000000000004s)]
*  And you've mentioned models a little bit and this was also a big request of my ex in talking [[00:07:56](https://www.youtube.com/watch?v=snkOMOjiVOk&t=476.58000000000004s)]
*  about social media. [[00:08:01](https://www.youtube.com/watch?v=snkOMOjiVOk&t=481.90000000000003s)]
*  I won't lean on them too much, but I thought it was really, as we're seeing in the room, [[00:08:02](https://www.youtube.com/watch?v=snkOMOjiVOk&t=482.90000000000003s)]
*  you've got a huge group of power users. [[00:08:06](https://www.youtube.com/watch?v=snkOMOjiVOk&t=486.34000000000003s)]
*  And so lots of people wondering about your next models. [[00:08:07](https://www.youtube.com/watch?v=snkOMOjiVOk&t=487.94s)]
*  OpenAI has obviously gone with these new reasoning models, 01, 03. [[00:08:11](https://www.youtube.com/watch?v=snkOMOjiVOk&t=491.42s)]
*  What's your plans there? [[00:08:16](https://www.youtube.com/watch?v=snkOMOjiVOk&t=496.18s)]
*  Yeah. [[00:08:17](https://www.youtube.com/watch?v=snkOMOjiVOk&t=497.18s)]
*  So, you know, we are, I think, in the not too distant future going to release some very [[00:08:18](https://www.youtube.com/watch?v=snkOMOjiVOk&t=498.18s)]
*  good models. [[00:08:25](https://www.youtube.com/watch?v=snkOMOjiVOk&t=505.26s)]
*  I won't specifically say names. [[00:08:26](https://www.youtube.com/watch?v=snkOMOjiVOk&t=506.26s)]
*  To say a little about the reasoning models, our perspective actually is a little different, [[00:08:30](https://www.youtube.com/watch?v=snkOMOjiVOk&t=510.34s)]
*  which is that, you know, there's been this whole thing of like reasoning models and test [[00:08:36](https://www.youtube.com/watch?v=snkOMOjiVOk&t=516.5s)]
*  time compute. [[00:08:39](https://www.youtube.com/watch?v=snkOMOjiVOk&t=519.7s)]
*  As if there's the normal models and the reasoning models, there's somehow a totally different [[00:08:41](https://www.youtube.com/watch?v=snkOMOjiVOk&t=521.02s)]
*  way of doing things. [[00:08:44](https://www.youtube.com/watch?v=snkOMOjiVOk&t=524.98s)]
*  That's not our perspective. [[00:08:47](https://www.youtube.com/watch?v=snkOMOjiVOk&t=527.56s)]
*  We see it more as a continuous spectrum. [[00:08:48](https://www.youtube.com/watch?v=snkOMOjiVOk&t=528.74s)]
*  That there's this ability for models to think, to reflect on their own thinking and at the [[00:08:51](https://www.youtube.com/watch?v=snkOMOjiVOk&t=531.2s)]
*  end to produce a result. [[00:08:58](https://www.youtube.com/watch?v=snkOMOjiVOk&t=538.44s)]
*  If you use Sonnet 3.5, already sometimes it does that to some extent. [[00:09:00](https://www.youtube.com/watch?v=snkOMOjiVOk&t=540.1600000000001s)]
*  But I think the change that we're going to see is that we're going to see a larger scale [[00:09:05](https://www.youtube.com/watch?v=snkOMOjiVOk&t=545.4000000000001s)]
*  use of reinforcement learning. [[00:09:11](https://www.youtube.com/watch?v=snkOMOjiVOk&t=551.3000000000001s)]
*  And when you train the model with reinforcement learning, it starts to think and reflect more. [[00:09:14](https://www.youtube.com/watch?v=snkOMOjiVOk&t=554.48s)]
*  And so it's not like reasoning or test time computer, the various things that this is [[00:09:19](https://www.youtube.com/watch?v=snkOMOjiVOk&t=559.38s)]
*  called, is a totally new method. [[00:09:23](https://www.youtube.com/watch?v=snkOMOjiVOk&t=563.86s)]
*  It's more like an emergent property, a consequence of training the model more in an outcome-based [[00:09:26](https://www.youtube.com/watch?v=snkOMOjiVOk&t=566.8s)]
*  way at a larger scale. [[00:09:35](https://www.youtube.com/watch?v=snkOMOjiVOk&t=575.18s)]
*  And so I think what that's going to lead to, and we may, you know, still have to see how [[00:09:37](https://www.youtube.com/watch?v=snkOMOjiVOk&t=577.04s)]
*  it works, how it pans out. [[00:09:43](https://www.youtube.com/watch?v=snkOMOjiVOk&t=583.82s)]
*  I think something that more continuously interpolates between them, that more fluidly combines reasoning [[00:09:46](https://www.youtube.com/watch?v=snkOMOjiVOk&t=586.18s)]
*  with all the other things that models do, I think that's what's going to be the most [[00:09:53](https://www.youtube.com/watch?v=snkOMOjiVOk&t=593.3199999999999s)]
*  effective thing here. [[00:09:59](https://www.youtube.com/watch?v=snkOMOjiVOk&t=599.12s)]
*  As you said, we've often focused on, you know, make sure using the model is a smooth experience, [[00:10:00](https://www.youtube.com/watch?v=snkOMOjiVOk&t=600.92s)]
*  that people can get the full thing out of it. [[00:10:06](https://www.youtube.com/watch?v=snkOMOjiVOk&t=606.5s)]
*  And I think you may see with reasoning models, we will have a similar spin and may do something [[00:10:09](https://www.youtube.com/watch?v=snkOMOjiVOk&t=609.76s)]
*  that's different from what others are doing with reasoning models. [[00:10:17](https://www.youtube.com/watch?v=snkOMOjiVOk&t=617.3s)]
*  I want to just back up for a second because you just mentioned your, I should probably, [[00:10:20](https://www.youtube.com/watch?v=snkOMOjiVOk&t=620.0s)]
*  so you won't give me a date? [[00:10:24](https://www.youtube.com/watch?v=snkOMOjiVOk&t=624.88s)]
*  I will not. [[00:10:26](https://www.youtube.com/watch?v=snkOMOjiVOk&t=626.2s)]
*  I will not give you any dates on anything in this interview. [[00:10:27](https://www.youtube.com/watch?v=snkOMOjiVOk&t=627.6s)]
*  Will you give any of them a date? [[00:10:30](https://www.youtube.com/watch?v=snkOMOjiVOk&t=630.3199999999999s)]
*  Or any other that I'm doing at this event. [[00:10:31](https://www.youtube.com/watch?v=snkOMOjiVOk&t=631.68s)]
*  Okay, no dates specifically. [[00:10:33](https://www.youtube.com/watch?v=snkOMOjiVOk&t=633.52s)]
*  But we heard in the not so distant future, I think was the quote. [[00:10:35](https://www.youtube.com/watch?v=snkOMOjiVOk&t=635.2s)]
*  So it's, in your mind, is that like six months? [[00:10:37](https://www.youtube.com/watch?v=snkOMOjiVOk&t=637.92s)]
*  Less. [[00:10:41](https://www.youtube.com/watch?v=snkOMOjiVOk&t=641.52s)]
*  Less, is that like three months? [[00:10:42](https://www.youtube.com/watch?v=snkOMOjiVOk&t=642.52s)]
*  So between three and six months? [[00:10:45](https://www.youtube.com/watch?v=snkOMOjiVOk&t=645.76s)]
*  Stop. [[00:10:47](https://www.youtube.com/watch?v=snkOMOjiVOk&t=647.28s)]
*  Okay, all right, all right. [[00:10:48](https://www.youtube.com/watch?v=snkOMOjiVOk&t=648.28s)]
*  But if we zoom out, and I think there's a lot of talk about how fast this stuff is progressing. [[00:10:49](https://www.youtube.com/watch?v=snkOMOjiVOk&t=649.28s)]
*  Yes. [[00:10:57](https://www.youtube.com/watch?v=snkOMOjiVOk&t=657.12s)]
*  We're hearing, you know, just this week, you know, some reports OpenAI's got a PhD level [[00:10:58](https://www.youtube.com/watch?v=snkOMOjiVOk&t=658.12s)]
*  agent. [[00:11:02](https://www.youtube.com/watch?v=snkOMOjiVOk&t=662.56s)]
*  Is this stuff really moving as fast? [[00:11:03](https://www.youtube.com/watch?v=snkOMOjiVOk&t=663.56s)]
*  Yeah, so, you know, if I were to describe my own kind of journey in how fast this stuff [[00:11:06](https://www.youtube.com/watch?v=snkOMOjiVOk&t=666.6s)]
*  is moving, my own kind of like, I don't know, almost like epistemic and emotional journey. [[00:11:12](https://www.youtube.com/watch?v=snkOMOjiVOk&t=672.8s)]
*  If we go back all the way to 2018, 2017, a couple years after I was first in this field, [[00:11:18](https://www.youtube.com/watch?v=snkOMOjiVOk&t=678.48s)]
*  my perspective then, you know, I was one of the first to document, along with my, you [[00:11:25](https://www.youtube.com/watch?v=snkOMOjiVOk&t=685.44s)]
*  know, co-founders at Anthropic, the scaling laws, which says you pour more compute into [[00:11:29](https://www.youtube.com/watch?v=snkOMOjiVOk&t=689.72s)]
*  these models, they get better at everything. [[00:11:33](https://www.youtube.com/watch?v=snkOMOjiVOk&t=693.84s)]
*  And really everything. [[00:11:35](https://www.youtube.com/watch?v=snkOMOjiVOk&t=695.96s)]
*  My view from about then to six months ago was, I suspect based on these trends that [[00:11:37](https://www.youtube.com/watch?v=snkOMOjiVOk&t=697.5600000000001s)]
*  we are going to get to models that are better than almost all humans at almost everything. [[00:11:44](https://www.youtube.com/watch?v=snkOMOjiVOk&t=704.6s)]
*  I, you know, I guess that it would happen sometime in the 2020s, probably mid 2020s. [[00:11:49](https://www.youtube.com/watch?v=snkOMOjiVOk&t=709.6s)]
*  But the way I always said it was, I don't know, I'm not sure. [[00:11:57](https://www.youtube.com/watch?v=snkOMOjiVOk&t=717.24s)]
*  Like we could be wrong. [[00:12:00](https://www.youtube.com/watch?v=snkOMOjiVOk&t=720.6s)]
*  I think that's the right attitude to have. [[00:12:01](https://www.youtube.com/watch?v=snkOMOjiVOk&t=721.6s)]
*  Like, you know, particularly when you're the CEO of one of these companies, like if [[00:12:03](https://www.youtube.com/watch?v=snkOMOjiVOk&t=723.96s)]
*  you just say it is definitely going to happen, like, you know, it's going to be great. [[00:12:07](https://www.youtube.com/watch?v=snkOMOjiVOk&t=727.6800000000001s)]
*  You know, you just sound like a hype man, right? [[00:12:11](https://www.youtube.com/watch?v=snkOMOjiVOk&t=731.88s)]
*  So I think until about three to six months ago, I had substantial uncertainty about it. [[00:12:15](https://www.youtube.com/watch?v=snkOMOjiVOk&t=735.48s)]
*  I still do now. [[00:12:19](https://www.youtube.com/watch?v=snkOMOjiVOk&t=739.96s)]
*  But that uncertainty is greatly reduced. [[00:12:21](https://www.youtube.com/watch?v=snkOMOjiVOk&t=741.32s)]
*  I think that over the next two or three years, I am relatively confident that we are indeed [[00:12:24](https://www.youtube.com/watch?v=snkOMOjiVOk&t=744.5600000000001s)]
*  going to see models that show up in the workplace that consumers use that are, yes, assistance [[00:12:29](https://www.youtube.com/watch?v=snkOMOjiVOk&t=749.76s)]
*  to humans, but are gradually get better than us at almost everything. [[00:12:36](https://www.youtube.com/watch?v=snkOMOjiVOk&t=756.4s)]
*  And the positive consequences are going to be great. [[00:12:42](https://www.youtube.com/watch?v=snkOMOjiVOk&t=762.96s)]
*  The negative consequences, you know, we also will have to, we also will have to watch out [[00:12:45](https://www.youtube.com/watch?v=snkOMOjiVOk&t=765.12s)]
*  for. [[00:12:49](https://www.youtube.com/watch?v=snkOMOjiVOk&t=769.84s)]
*  I think progress really is as fast as people think it is. [[00:12:50](https://www.youtube.com/watch?v=snkOMOjiVOk&t=770.84s)]
*  One thing that I will criticize is I actually think it's very important now that fast progress [[00:12:55](https://www.youtube.com/watch?v=snkOMOjiVOk&t=775.38s)]
*  is relatively likely to appreciate it with the proper gravity and to talk seriously about [[00:13:01](https://www.youtube.com/watch?v=snkOMOjiVOk&t=781.7s)]
*  it. [[00:13:09](https://www.youtube.com/watch?v=snkOMOjiVOk&t=789.98s)]
*  Some of the other companies, I won't name any names, you know, there's just all these, [[00:13:11](https://www.youtube.com/watch?v=snkOMOjiVOk&t=791.16s)]
*  you know, it's all these weird Twitter rumors, like employees talk, you know, employees like, [[00:13:15](https://www.youtube.com/watch?v=snkOMOjiVOk&t=795.5s)]
*  you know, have this kind of sly, winking, like, you know, nod to like, oh, there's these [[00:13:20](https://www.youtube.com/watch?v=snkOMOjiVOk&t=800.52s)]
*  amazing things we're doing here. [[00:13:25](https://www.youtube.com/watch?v=snkOMOjiVOk&t=805.98s)]
*  I actually think that's dangerous because someone on the outside looking at it is like, [[00:13:28](https://www.youtube.com/watch?v=snkOMOjiVOk&t=808.24s)]
*  oh, man, that's just hype. [[00:13:32](https://www.youtube.com/watch?v=snkOMOjiVOk&t=812.84s)]
*  That kind of communication gives the impression that this stuff is not serious. [[00:13:35](https://www.youtube.com/watch?v=snkOMOjiVOk&t=815.4s)]
*  And I think it's like, that's really dangerous to do when it actually is serious. [[00:13:39](https://www.youtube.com/watch?v=snkOMOjiVOk&t=819.56s)]
*  I think the AI industry as a whole actually has an obligation to point to the seriousness [[00:13:45](https://www.youtube.com/watch?v=snkOMOjiVOk&t=825.56s)]
*  of the moment that we're in. [[00:13:51](https://www.youtube.com/watch?v=snkOMOjiVOk&t=831.06s)]
*  If we're really saying there are incredible positive things that are possible and inevitably [[00:13:52](https://www.youtube.com/watch?v=snkOMOjiVOk&t=832.4399999999999s)]
*  with any change this large, there are risks, we have an obligation to communicate seriously [[00:13:58](https://www.youtube.com/watch?v=snkOMOjiVOk&t=838.8399999999999s)]
*  about it and to say what we actually think. [[00:14:04](https://www.youtube.com/watch?v=snkOMOjiVOk&t=844.1999999999999s)]
*  And I want to get to what you want some of those, what the risks are and what you want [[00:14:06](https://www.youtube.com/watch?v=snkOMOjiVOk&t=846.3399999999999s)]
*  some of the rules and regulations to be. [[00:14:10](https://www.youtube.com/watch?v=snkOMOjiVOk&t=850.68s)]
*  Before we get that, I want to back up to what you said about coworkers and all the talk [[00:14:12](https://www.youtube.com/watch?v=snkOMOjiVOk&t=852.94s)]
*  here, they should just rename Davos Agent Palooza. [[00:14:18](https://www.youtube.com/watch?v=snkOMOjiVOk&t=858.3000000000001s)]
*  It's just, everything is agents. [[00:14:21](https://www.youtube.com/watch?v=snkOMOjiVOk&t=861.38s)]
*  Everyone's talking about they've got agents, they're coming agents. [[00:14:24](https://www.youtube.com/watch?v=snkOMOjiVOk&t=864.1s)]
*  You call this virtual collaborators. [[00:14:27](https://www.youtube.com/watch?v=snkOMOjiVOk&t=867.32s)]
*  You have a different term for that. [[00:14:29](https://www.youtube.com/watch?v=snkOMOjiVOk&t=869.1800000000001s)]
*  How are you progressing on agents or virtual collaborators? [[00:14:32](https://www.youtube.com/watch?v=snkOMOjiVOk&t=872.3000000000001s)]
*  And we saw some signs of that with computer use at the end of last year. [[00:14:35](https://www.youtube.com/watch?v=snkOMOjiVOk&t=875.82s)]
*  Give us a little roadmap and specifically, I know you don't want to talk about dates, [[00:14:39](https://www.youtube.com/watch?v=snkOMOjiVOk&t=879.8s)]
*  but what are we getting this year? [[00:14:43](https://www.youtube.com/watch?v=snkOMOjiVOk&t=883.8599999999999s)]
*  What are we really realistically going to see this year? [[00:14:45](https://www.youtube.com/watch?v=snkOMOjiVOk&t=885.54s)]
*  And your company is at the head of a lot of this. [[00:14:47](https://www.youtube.com/watch?v=snkOMOjiVOk&t=887.74s)]
*  So we can get rid of a lot of the hype. [[00:14:49](https://www.youtube.com/watch?v=snkOMOjiVOk&t=889.2199999999999s)]
*  First of all, I should just disambiguate a little. [[00:14:51](https://www.youtube.com/watch?v=snkOMOjiVOk&t=891.66s)]
*  I think agents, there are these terms that come up with the regular frequency in our [[00:14:53](https://www.youtube.com/watch?v=snkOMOjiVOk&t=893.74s)]
*  field that don't really mean anything. [[00:14:59](https://www.youtube.com/watch?v=snkOMOjiVOk&t=899.06s)]
*  I think agents is one, AGI is one, ASI is one, reasoning is one. [[00:15:01](https://www.youtube.com/watch?v=snkOMOjiVOk&t=901.18s)]
*  They sound like they mean something if you're outside, but they have no precise technical [[00:15:06](https://www.youtube.com/watch?v=snkOMOjiVOk&t=906.52s)]
*  meaning. [[00:15:11](https://www.youtube.com/watch?v=snkOMOjiVOk&t=911.12s)]
*  So agents are used for anything from Clippy++ to like, I press the button on this AI system [[00:15:12](https://www.youtube.com/watch?v=snkOMOjiVOk&t=912.12s)]
*  and it writes a complete app for me and founds a company or something. [[00:15:20](https://www.youtube.com/watch?v=snkOMOjiVOk&t=920.32s)]
*  I think what we have in mind is somewhat closer to the second one. [[00:15:24](https://www.youtube.com/watch?v=snkOMOjiVOk&t=924.92s)]
*  So our version of this is virtual collaborators. [[00:15:31](https://www.youtube.com/watch?v=snkOMOjiVOk&t=931.64s)]
*  And so you mentioned computer use. [[00:15:34](https://www.youtube.com/watch?v=snkOMOjiVOk&t=934.76s)]
*  Computer use is maybe an early instantiation of that. [[00:15:36](https://www.youtube.com/watch?v=snkOMOjiVOk&t=936.42s)]
*  It's one of the ingredients. [[00:15:40](https://www.youtube.com/watch?v=snkOMOjiVOk&t=940.14s)]
*  But I think the thing we have in mind is, and this could be an assistant in your workplace, [[00:15:41](https://www.youtube.com/watch?v=snkOMOjiVOk&t=941.66s)]
*  an assistant that you use personally, but there's a model that is able to do anything [[00:15:48](https://www.youtube.com/watch?v=snkOMOjiVOk&t=948.34s)]
*  on a computer screen that a kind of virtual human could do. [[00:15:52](https://www.youtube.com/watch?v=snkOMOjiVOk&t=952.86s)]
*  And you talk to it. [[00:15:57](https://www.youtube.com/watch?v=snkOMOjiVOk&t=957.34s)]
*  You give it a task. [[00:15:58](https://www.youtube.com/watch?v=snkOMOjiVOk&t=958.56s)]
*  And maybe it's a task it does over like a day where you say, we're going to implement [[00:16:00](https://www.youtube.com/watch?v=snkOMOjiVOk&t=960.02s)]
*  this product feature. [[00:16:05](https://www.youtube.com/watch?v=snkOMOjiVOk&t=965.12s)]
*  And what that means, it's writing some code, testing the code, deploying that code to some [[00:16:06](https://www.youtube.com/watch?v=snkOMOjiVOk&t=966.12s)]
*  test surface, talking to coworkers, writing design docs, writing Google docs, writing [[00:16:12](https://www.youtube.com/watch?v=snkOMOjiVOk&t=972.26s)]
*  Slack, sending emails to people. [[00:16:17](https://www.youtube.com/watch?v=snkOMOjiVOk&t=977.3199999999999s)]
*  And just like a human, the model goes off and does a bunch of those things and then [[00:16:20](https://www.youtube.com/watch?v=snkOMOjiVOk&t=980.14s)]
*  checks in with you every once in a while. [[00:16:23](https://www.youtube.com/watch?v=snkOMOjiVOk&t=983.8s)]
*  So I would think of it as an agent. [[00:16:26](https://www.youtube.com/watch?v=snkOMOjiVOk&t=986.3s)]
*  I would think of it as like an autonomous virtual collaborator that acts on your behalf [[00:16:28](https://www.youtube.com/watch?v=snkOMOjiVOk&t=988.5s)]
*  on a very long time scale and that you check in with every once in a while. [[00:16:34](https://www.youtube.com/watch?v=snkOMOjiVOk&t=994.22s)]
*  And you think of it as having all the piping, all the inputs and outputs of a human operating [[00:16:39](https://www.youtube.com/watch?v=snkOMOjiVOk&t=999.4599999999999s)]
*  virtually. [[00:16:45](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1005.5s)]
*  And so time scale on that, as you've mentioned. [[00:16:47](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1007.06s)]
*  I suspect. [[00:16:51](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1011.78s)]
*  I mean, you can never be sure with these things. [[00:16:53](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1013.64s)]
*  The process of research is incredibly unpredictable. [[00:16:55](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1015.4399999999999s)]
*  I don't know for sure what will happen with us. [[00:16:57](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1017.9599999999999s)]
*  I don't know for sure what will happen in the industry. [[00:17:00](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1020.0s)]
*  But I do suspect, I'm not promising, I do suspect that a very strong version of these [[00:17:02](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1022.0s)]
*  capabilities will come this year. [[00:17:09](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1029.2s)]
*  And it may be in the first half of this year. [[00:17:11](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1031.44s)]
*  So let's talk about the elephant or agent in the room, which is that that's going to [[00:17:13](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1033.82s)]
*  affect human work, right? [[00:17:17](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1037.8s)]
*  You just described a lot of jobs that we have people doing. [[00:17:20](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1040.96s)]
*  Yes. [[00:17:24](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1044.3200000000002s)]
*  So I generally separate this out into short-term and long-term. [[00:17:25](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1045.3600000000001s)]
*  So in the short-term, we've seen technological disruption of labor many times in the past. [[00:17:30](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1050.56s)]
*  And I think in the short-term, and in my kind of time-compressed world, short-term is maybe [[00:17:37](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1057.96s)]
*  a year or two years or maybe three. [[00:17:44](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1064.24s)]
*  On that time scale, I won't say we have a perfect solution, but we've seen before technological [[00:17:49](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1069.48s)]
*  disruption of labor. [[00:17:56](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1076.58s)]
*  And I think there it's important to think in terms of workers being adaptable, in terms [[00:17:58](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1078.26s)]
*  of taking advantage of comparative advantage. [[00:18:03](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1083.18s)]
*  Comparative advantage is remarkable. [[00:18:06](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1086.3600000000001s)]
*  Even if a machine does 90% of your job, what happens is that the other 10% becomes super-leveraged. [[00:18:07](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1087.98s)]
*  You spend all your time doing that 10%. [[00:18:15](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1095.1200000000001s)]
*  You do 10 times more of the 10% than you did before, and you get 10 times more done [[00:18:17](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1097.54s)]
*  because the other 90% is leveraged. [[00:18:22](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1102.5s)]
*  So that can be good for humans. [[00:18:24](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1104.24s)]
*  You can also learn how to sculpt your efforts around what the machine is doing. [[00:18:26](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1106.06s)]
*  And so we can also think from a product perspective how to design our products to enable complementarity [[00:18:32](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1112.06s)]
*  rather than substitution. [[00:18:39](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1119.58s)]
*  There was this interesting study from the economist Eric Bernjolfsson about a year ago [[00:18:41](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1121.06s)]
*  that said by default when companies deploy AI, they tend to deploy it in a replacement [[00:18:45](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1125.54s)]
*  mode. [[00:18:51](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1131.26s)]
*  But when they think more, when they pay more attention to deployment, they often find ways [[00:18:52](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1132.26s)]
*  to deploy it in a complementary mode. [[00:18:57](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1137.1399999999999s)]
*  When they do that, the productivity increases are greater. [[00:18:59](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1139.82s)]
*  So that suggests there's some path dependence. [[00:19:03](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1143.04s)]
*  There are different ways to deploy the technology. [[00:19:05](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1145.52s)]
*  And we are thinking about that as we design this virtual collaborator. [[00:19:08](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1148.3s)]
*  I'm not going to lie to you. [[00:19:13](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1153.46s)]
*  There's a brutally efficient market here. [[00:19:16](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1156.1000000000001s)]
*  If doing things in a complementary way is like somehow will be reliably outcompeted [[00:19:19](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1159.5s)]
*  in the market by not doing things in a complementary way, then the whole industry is in a difficult [[00:19:26](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1166.42s)]
*  position and we have to think about how to get around that. [[00:19:30](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1170.22s)]
*  So there are very powerful market forces here. [[00:19:32](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1172.74s)]
*  But I think there may be an opportunity compatible with those market forces to take a different [[00:19:35](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1175.5400000000002s)]
*  path that's better for the whole industry, that's innovative, that manages to be better [[00:19:40](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1180.98s)]
*  for humans while outcompeting the other approaches. [[00:19:46](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1186.78s)]
*  That's a short term. [[00:19:50](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1190.02s)]
*  In the long term, I feel very strongly that I don't know exactly when it will come. [[00:19:51](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1191.22s)]
*  I don't know if it will be 2027. [[00:19:57](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1197.58s)]
*  I think it's plausible it could be longer than that. [[00:19:59](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1199.54s)]
*  I don't think it will be a whole bunch longer than that when AI systems are better than [[00:20:01](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1201.38s)]
*  humans at almost everything. [[00:20:06](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1206.06s)]
*  Better than almost all humans at almost everything and eventually better all humans at almost [[00:20:09](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1209.0s)]
*  everything. [[00:20:12](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1212.68s)]
*  Even robotics. [[00:20:13](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1213.68s)]
*  We make good enough AI systems, they'll enable us to make better, better, better robots. [[00:20:14](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1214.68s)]
*  And so when that happens, we will need to have a conversation at places like this, at [[00:20:19](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1219.84s)]
*  places like this event about how do we organize our economy? [[00:20:25](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1225.64s)]
*  How do humans find meaning? [[00:20:32](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1232.32s)]
*  There are a lot of assumptions we made when humans were the most intelligent species on [[00:20:35](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1235.2s)]
*  the planet that are going to be invalidated by what's happening with AI. [[00:20:39](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1239.48s)]
*  And I think the only good thing about it is that we'll all be in the same boat. [[00:20:45](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1245.72s)]
*  I'm actually afraid of the world where 30% of human labor becomes fully automated by [[00:20:49](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1249.6000000000001s)]
*  AI and the other 70%. [[00:20:56](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1256.8000000000002s)]
*  That's going to cause this just incredible class war between the groups that have been [[00:20:58](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1258.88s)]
*  and the groups that haven't been. [[00:21:04](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1264.5s)]
*  If we're all in the same boat, it's not going to be easy, but I actually feel better about [[00:21:06](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1266.02s)]
*  it because we're going to have to sit down and say, well, it's not like we're randomly [[00:21:09](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1269.66s)]
*  picking every third person and saying you're useless. [[00:21:14](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1274.46s)]
*  We are all in the same boat. [[00:21:17](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1277.14s)]
*  We've recognized that we've reached the point as a technological civilization where the [[00:21:19](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1279.46s)]
*  idea there's huge abundance and huge economic value, but the idea that the way to distribute [[00:21:25](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1285.58s)]
*  that value is for humans to produce economic labor and this is where they feel their sense [[00:21:31](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1291.3s)]
*  of self-worth. [[00:21:36](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1296.74s)]
*  Once that idea gets invalidated, we're all going to have to sit down and figure it out. [[00:21:38](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1298.1s)]
*  I'm not going to lie. [[00:21:43](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1303.9s)]
*  My true belief is that that is coming. [[00:21:46](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1306.02s)]
*  I think AI companies are going to be at the center of that or maybe in the cross hairs [[00:21:49](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1309.26s)]
*  of that and we have a responsibility to have an answer there. [[00:21:53](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1313.54s)]
*  I was going to ask about that because as you talk about that though, right now there's [[00:21:57](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1317.1399999999999s)]
*  a war over human talent in your industry. [[00:22:01](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1321.82s)]
*  You've got OpenAI and Meta and all of these other competitors. [[00:22:05](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1325.1s)]
*  How do you think about attracting the best minds to Anthropic? [[00:22:09](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1329.1799999999998s)]
*  I would make two points. [[00:22:14](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1334.62s)]
*  One is that, and I've said this on podcasts, my philosophy of talent is that talent density [[00:22:16](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1336.98s)]
*  beats talent mass every time. [[00:22:22](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1342.46s)]
*  You want a relatively small set of people where almost everyone you hire is really, [[00:22:24](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1344.46s)]
*  really good. [[00:22:30](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1350.6200000000001s)]
*  Even if some other company is 10 times larger and has two times as many good people, there [[00:22:32](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1352.38s)]
*  is some magic, there is some alchemy to good people working together with each other. [[00:22:38](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1358.38s)]
*  When you hire an engineer, a researcher, they look around, everyone they look around at [[00:22:45](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1365.98s)]
*  is incredibly talented like them. [[00:22:50](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1370.7s)]
*  Talent has been our philosophy all along. [[00:22:53](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1373.42s)]
*  That is why I think talent has flocked to Anthropic. [[00:22:55](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1375.26s)]
*  The second thing is the values and the honest presentation of those values. [[00:22:58](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1378.0600000000002s)]
*  It's been very important to us from the beginning to get these kind of safety and societal questions [[00:23:05](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1385.6200000000001s)]
*  right. [[00:23:10](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1390.5800000000002s)]
*  I think there are multiple companies in the space that have, and I laud them for this, [[00:23:11](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1391.5800000000002s)]
*  have stated that these issues are important and even have made some concrete efforts. [[00:23:17](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1397.82s)]
*  But I think as time has gone on, as I've diverged from my previous employer along with my co-founders [[00:23:24](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1404.58s)]
*  and we've done things our way and they've done things their way and other companies [[00:23:30](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1410.74s)]
*  have done things their ways, the difference starts to emerge over time. [[00:23:34](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1414.62s)]
*  You see companies make promises and you see how those promises pan out over time. [[00:23:39](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1419.4199999999998s)]
*  You see what their internal employees think of the promises they've made. [[00:23:44](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1424.78s)]
*  Some things don't become public, but you see by their action, by how people vote with their [[00:23:49](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1429.06s)]
*  feet. [[00:23:54](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1434.46s)]
*  I think this is very important. [[00:23:55](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1435.46s)]
*  I think it's very important to really live these values, to have it be substantive and [[00:23:57](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1437.78s)]
*  not a marketing exercise. [[00:24:03](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1443.8999999999999s)]
*  I think the talent that has been inside multiple of these companies recognizes that and understands [[00:24:06](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1446.1s)]
*  that. [[00:24:11](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1451.46s)]
*  I think the talent goes is, can be, I'm an optimist, a signal about who's really sincere. [[00:24:12](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1452.46s)]
*  We don't have much time and there's so many things I want to get to, but I do want to [[00:24:17](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1457.74s)]
*  talk a little bit about regulation. [[00:24:21](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1461.98s)]
*  I did notice you weren't at the inauguration yesterday. [[00:24:23](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1463.82s)]
*  One way I think about things is I think Anthropic is playing a little bit of a different game [[00:24:30](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1470.14s)]
*  than some of the other players. [[00:24:36](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1476.38s)]
*  One of the ways I state it is Anthropic is a policy actor, Anthropic is not a political [[00:24:37](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1477.74s)]
*  actor. [[00:24:42](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1482.5s)]
*  Political actors are looking at how will I be treated by political players? [[00:24:44](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1484.94s)]
*  How do I make sure things I want to do don't get blocked by antitrust or that regulation [[00:24:50](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1490.42s)]
*  doesn't get in my way? [[00:24:56](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1496.18s)]
*  We have more of a perspective of we're looking at the global AI landscape and we have a set [[00:24:58](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1498.68s)]
*  of views, a policy platform, maybe at some point we'll even put out a policy platform [[00:25:05](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1505.78s)]
*  of what we think needs to be done, right? [[00:25:10](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1510.26s)]
*  From how we relate to China and maintain the lead over China to what the right testing [[00:25:13](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1513.5s)]
*  and measurement is for the dangers of our own AI system to some of these economic issues [[00:25:20](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1520.98s)]
*  that we've discussed. [[00:25:25](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1525.66s)]
*  Our view is that we have a certain perspective on these issues and we're going to describe [[00:25:28](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1528.3799999999999s)]
*  that perspective to whoever listens. [[00:25:34](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1534.02s)]
*  We describe that perspective to the Biden administration. [[00:25:35](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1535.98s)]
*  Now we're describing that perspective to the Trump administration. [[00:25:39](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1539.34s)]
*  For example, on China's issues, I wrote an op-ed with Matt Pottinger who was principal [[00:25:42](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1542.66s)]
*  deputy national security advisor in the first Trump administration just really to show that [[00:25:50](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1550.9s)]
*  across the political spectrum and in industry issues like export controls are of common [[00:25:56](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1556.58s)]
*  interest. [[00:26:02](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1562.26s)]
*  I think what you'll see is Anthropic is not rushing to declare its fealty to one side [[00:26:03](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1563.42s)]
*  or another. [[00:26:09](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1569.5s)]
*  Anthropic has a set of policy positions that we think if everyone understood the situation [[00:26:10](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1570.5s)]
*  there should be bipartisan support for because these issues are so central, because these [[00:26:16](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1576.54s)]
*  issues are so important. [[00:26:21](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1581.1s)]
*  Some political players, some individuals may be more or less sympathetic to them, but that [[00:26:24](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1584.66s)]
*  doesn't matter to us. [[00:26:29](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1589.46s)]
*  We're just going to go to everyone and describe what our preferred policies are because I [[00:26:30](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1590.94s)]
*  think if we don't get this right, society goes in a bad direction. [[00:26:35](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1595.7s)]
*  Sooner or later, if we're right, if we're wrong, then our policy platform is the wrong [[00:26:40](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1600.46s)]
*  platform. [[00:26:43](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1603.7s)]
*  But if we're right, then it will be clear enough, it will be clear in two or three years [[00:26:44](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1604.7s)]
*  that there will be a need for economic policies related to labor, that we really need to do [[00:26:49](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1609.5s)]
*  testing and measurement of these models for national security risks, that it is existential, [[00:26:57](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1617.98s)]
*  as the outgoing national security advisor Jake Sullivan said, to stay ahead of China, [[00:27:03](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1623.02s)]
*  which is why some of these export controls were put in place. [[00:27:08](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1628.5s)]
*  If those things are right, history will vindicate us and history will vindicate us pretty quickly, [[00:27:11](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1631.14s)]
*  like within the current presidential term. [[00:27:17](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1637.5800000000002s)]
*  So the thing we're going to do is we're going to say the things that we think are the right [[00:27:19](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1639.3s)]
*  policies to everyone increasingly in public and whatever people's ideology is, whatever [[00:27:24](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1644.42s)]
*  they might be in favor of or against now, if we're right, and that's the key part, it'll [[00:27:35](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1655.14s)]
*  become clear in a couple years and then we'll have been on the right side of history. [[00:27:40](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1660.18s)]
*  So that's how I think about policy. [[00:27:44](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1664.22s)]
*  Any reaction to the overturning of the executive order this morning? [[00:27:46](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1666.98s)]
*  The executive order actually was really, it didn't do that much, it was just the first step. [[00:27:50](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1670.34s)]
*  We were generally supportive of it because it imposed reporting requirements for training [[00:27:58](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1678.1399999999999s)]
*  large models, which I think is a sensible thing. [[00:28:03](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1683.62s)]
*  I think it makes sense for the national security apparatus to understand where companies are doing. [[00:28:06](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1686.34s)]
*  It wasn't very burdensome. [[00:28:10](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1690.7s)]
*  I think it probably took like, I don't know, like one person day of time, time at entropic, [[00:28:12](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1692.02s)]
*  but it also wasn't some super complicated regulatory regime. [[00:28:17](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1697.3000000000002s)]
*  It wasn't regulatory regime at all. [[00:28:23](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1703.26s)]
*  So I think while I'm generally, was generally supportive of that executive order, I think [[00:28:26](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1706.5s)]
*  it was relatively small, yeah, it was a relatively small potatoes in terms of what was being done. [[00:28:35](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1715.8600000000001s)]
*  What I think is more important is one, the export controls against China and preventing [[00:28:43](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1723.5s)]
*  smuggling of chips. [[00:28:49](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1729.7s)]
*  That was where I wrote in the op-ed with Pottinger. [[00:28:51](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1731.14s)]
*  I think that's absolutely existential. [[00:28:53](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1733.74s)]
*  We are just starting to see the value for military intelligence of these models. [[00:28:57](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1737.66s)]
*  I would also say that to the extent that we need to take care of the risks of our own [[00:29:03](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1743.74s)]
*  models, having a lead against China, which is becoming increasingly difficult, really [[00:29:09](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1749.18s)]
*  gives us the buffer to do that. [[00:29:14](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1754.18s)]
*  And if we don't have that lead, we're in this Hobbesian international competition where [[00:29:15](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1755.9s)]
*  it's like, you can be in the Catch-22. [[00:29:20](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1760.0600000000002s)]
*  Well, if we slow down three months to mitigate the risks of our own models, then China will [[00:29:22](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1762.0200000000002s)]
*  get there. [[00:29:26](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1766.98s)]
*  We don't want to end up in that situation in the first place. [[00:29:27](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1767.98s)]
*  Also important, I think, is internal testing and measurement. [[00:29:32](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1772.74s)]
*  So the AISIs, which we've worked with, which have done this internal testing and measurement, [[00:29:35](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1775.1s)]
*  I think those are very important. [[00:29:42](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1782.6999999999998s)]
*  And we've seen bipartisan support for preserving those into the new administration. [[00:29:45](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1785.98s)]
*  Again, these are mostly testing of the models for national security risk. [[00:29:50](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1790.4599999999998s)]
*  That should be a bipartisan thing. [[00:29:55](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1795.26s)]
*  There's been some misunderstanding that that kind of testing is about DEI issues or something [[00:29:57](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1797.1s)]
*  like that. [[00:30:04](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1804.7s)]
*  That's not actually what any of the AISIs are about. [[00:30:05](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1805.7s)]
*  It's about testing for national security risk. [[00:30:09](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1809.3400000000001s)]
*  And we need to continue to do that testing for national security risk. [[00:30:11](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1811.82s)]
*  And I've seen bipartisan support for it. [[00:30:14](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1814.8600000000001s)]
*  You mentioned, you just said something as a turn of phrase, and I just wanted to ask [[00:30:17](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1817.98s)]
*  about it. [[00:30:20](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1820.3s)]
*  You said it's a one-person day at Anthropic. [[00:30:21](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1821.3s)]
*  Yes. [[00:30:23](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1823.66s)]
*  Is that kind of how you guys think about things? [[00:30:24](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1824.66s)]
*  Yeah. [[00:30:26](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1826.3s)]
*  Yeah. [[00:30:27](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1827.3s)]
*  I mean, you know. [[00:30:28](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1828.3s)]
*  Is it like a one-Claude day? [[00:30:29](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1829.3s)]
*  Increasingly, as Claude does more things or helps people, I think we are thinking in terms [[00:30:30](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1830.8999999999999s)]
*  of like there's people and there's AI models and they're complementary to each other in [[00:30:40](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1840.74s)]
*  various ways. [[00:30:45](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1845.62s)]
*  There's a certain amount of compute and there's a certain amount of people. [[00:30:47](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1847.06s)]
*  I mean, I think the whole economy is going to have to start thinking in this style. [[00:30:49](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1849.46s)]
*  In my essay, Machines of Loving Grace, I have this phrase, the marginal returns to intelligence. [[00:30:53](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1853.46s)]
*  I think we should start thinking in terms of marginal returns to intelligence because [[00:30:59](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1859.98s)]
*  there are some tasks that intelligence helps with and there are others where there's some [[00:31:08](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1868.7800000000002s)]
*  physical limit or there's some societal limit where intelligence doesn't help. [[00:31:12](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1872.42s)]
*  And so I think that style of thought is becoming increasingly common within Anthropic and should [[00:31:16](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1876.94s)]
*  become increasingly common within the world. [[00:31:21](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1881.74s)]
*  I'm going to ask two last questions. [[00:31:23](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1883.46s)]
*  I know I'm going over and someone's going to start yelling at us in the back. [[00:31:25](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1885.94s)]
*  You recently, well, Amazon recently wrote you a four billion dollar check. [[00:31:29](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1889.98s)]
*  You raised more money. [[00:31:34](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1894.46s)]
*  I believe we reported two billion dollars. [[00:31:35](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1895.8600000000001s)]
*  Want to confirm that for us? [[00:31:37](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1897.8600000000001s)]
*  I can't confirm anything about ongoing raises, but if you do add up all the money we've raised [[00:31:40](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1900.5800000000002s)]
*  you know, it's definitely well into the double digit billions of dollars. [[00:31:51](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1911.7s)]
*  And just like zooming out, how are you dancing with these tech giants? [[00:31:56](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1916.46s)]
*  Are they leading? [[00:32:00](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1920.22s)]
*  Are you leading? [[00:32:01](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1921.22s)]
*  Yeah, I mean, you know, I think on one hand these partnerships make a lot of sense because [[00:32:02](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1922.22s)]
*  we need a lot of compute and there's a lot of forward capital needed to train these models. [[00:32:07](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1927.82s)]
*  And when these models are deployed, they're often deployed on the clouds because then [[00:32:12](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1932.42s)]
*  you can offer security standards there. [[00:32:16](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1936.24s)]
*  So economically the partnership, it just makes sense. [[00:32:18](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1938.36s)]
*  It just makes sense economically. [[00:32:21](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1941.1599999999999s)]
*  But our independence is also very important to us. [[00:32:23](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1943.12s)]
*  This is why we have a partnership with multiple cloud partners. [[00:32:25](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1945.6599999999999s)]
*  For example, we work with Amazon, but we also work with Google. [[00:32:28](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1948.4399999999998s)]
*  And every time we sign a contract with one of these cloud partners, we make sure that [[00:32:32](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1952.08s)]
*  some of the principles we've committed to, for example, our responsible scaling policy, [[00:32:36](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1956.1999999999998s)]
*  which governs how we test and deal with and provide security to every new model that we [[00:32:40](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1960.6799999999998s)]
*  build. [[00:32:45](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1965.9599999999998s)]
*  Every time we work with one of these cloud partners, every time we sign a new agreement, [[00:32:47](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1967.02s)]
*  we make sure that the way we deploy these models on the clouds are consistent with our [[00:32:50](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1970.98s)]
*  responsible scaling policy. [[00:32:55](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1975.62s)]
*  And actually that's had flow through effects where awareness, education about this policy [[00:32:57](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1977.5s)]
*  that we have has encouraged some of the cloud players to speed up their adoption of similar [[00:33:04](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1984.28s)]
*  policies as well. [[00:33:09](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1989.78s)]
*  So this is an example of what I've called Race to the Top, which is set an example and [[00:33:11](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1991.18s)]
*  other players may follow that example and do something good as well. [[00:33:16](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1996.46s)]
*  All right. [[00:33:19](https://www.youtube.com/watch?v=snkOMOjiVOk&t=1999.8200000000002s)]
*  I'm going to have one last question here because I feel like this is a question that I keep [[00:33:20](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2000.8200000000002s)]
*  getting and we've been talking about AI coming to the workforce. [[00:33:23](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2003.66s)]
*  And what is your best advice to a young person who's going to be starting their career in [[00:33:26](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2006.0200000000002s)]
*  the AI era or even before that? [[00:33:34](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2014.38s)]
*  I mean, you're in high school, you're in college. [[00:33:37](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2017.5800000000002s)]
*  What is your advice? [[00:33:40](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2020.0200000000002s)]
*  Yeah, a few things. [[00:33:41](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2021.0200000000002s)]
*  One is I would say, obviously learn to use the technology. [[00:33:42](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2022.54s)]
*  That's the obvious one, right? [[00:33:47](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2027.46s)]
*  Where it's changing so quickly and I think those who are able to keep up will be in much [[00:33:48](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2028.8999999999999s)]
*  better position than those who are not. [[00:33:55](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2035.3799999999999s)]
*  My second is I think the most important skill to cultivate is a critical skill, a kind of [[00:33:58](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2038.8999999999999s)]
*  critical thinking skill, right? [[00:34:07](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2047.1s)]
*  You're going to be critical about the information you see. [[00:34:09](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2049.7599999999998s)]
*  Now that AI systems are able to generate very plausible explanations, very plausible images, [[00:34:13](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2053.12s)]
*  very plausible videos, I think there's a sense in which the information ecosystem has really [[00:34:19](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2059.3599999999997s)]
*  kind of scrambled itself or inverted itself and you really have to try very hard to know [[00:34:26](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2066.9599999999996s)]
*  what's true and what's not true. [[00:34:31](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2071.7599999999998s)]
*  It's turned into more of a jungle than a kind of curated environment and I fear that some [[00:34:34](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2074.6s)]
*  aspects of AI may make that worse. [[00:34:40](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2080.36s)]
*  So, you know, looking at something and saying, does that make sense? [[00:34:44](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2084.44s)]
*  Is that really, you know, could that really be true? [[00:34:47](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2087.44s)]
*  Like I don't know, you just look on X or Twitter or whatever, you see all these things and [[00:34:51](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2091.08s)]
*  like 100,000 people like them and you're like, that doesn't make any sense, that just doesn't [[00:34:55](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2095.24s)]
*  make any sense at all, right? [[00:35:00](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2100.0s)]
*  And, you know, kind of the old world of things being curated is gone, so somehow we need [[00:35:02](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2102.88s)]
*  to make the new world, this marketplace, actually work and in some way converge to things that [[00:35:06](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2106.96s)]
*  are true. [[00:35:17](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2117.32s)]
*  And I think the critical thinking skills are going to be really important and can we use [[00:35:18](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2118.56s)]
*  AI to enhance those critical thinking skills rather than, you know, rather than it kind [[00:35:21](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2121.6000000000004s)]
*  of further corrupting the ecosystem. [[00:35:27](https://www.youtube.com/watch?v=snkOMOjiVOk&t=2127.76s)]
