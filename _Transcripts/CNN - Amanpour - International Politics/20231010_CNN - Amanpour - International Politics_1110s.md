---
Date Generated: January 16, 2025
Transcription Model: whisper medium 20231117
Length: 1110s
Video Keywords: ['interview', 'CNN', 'PBS', 'Christiane Amanpour', 'world news', 'news anchor', 'news show', 'news', 'public affairs', 'late-night TV', 'journalist', 'Chief International Correspondent', 'Imran Ahmed', 'Israel-Hamas War', 'Center for Countering Digital Hate', 'Hari Sreenivasan', 'Hate speech', 'social media']
Video Views: 12079
Video Rating: None
Video Description: Imran Ahmed, the CEO of the Center for Countering Digital Hate (CCDH), explains how misinformation about Israel’s war with Hamas is being amplified by social media, which is fueling hate speech on both sides. Ahmed tells Hari Sreenivasan why platforms like X (formerly Twitter) are facing a "tipping point,” and accuses its owner Elon Musk of trying to silence his organization under the guise of free speech.

Originally aired on October 10, 2023

----------------------------------------------------------------------------------------------------------------------------------------

Major support for Amanpour and Company is provided by Candace King Weir, the Leila and Mickey Straus Family Charitable Trust, Jim Attwood and Leslie Williams, Mark J. Blechner, Seton J. Melvin, Charles Rosenblum, Koo and Patricia Yuen, Barbara Hope Zuckerberg, Jeffrey Katz and Beth Rogers, Bernard and Denise Schwartz, the JPB Foundation, the Sylvia A. and Simon B. Poyta Programming Endowment to Fight Antisemitism and Josh Weston.

Subscribe to the Amanpour and Company. channel here: https://bit.ly/2EMIkTJ

Subscribe to our daily newsletter to find out who's on each night: http://www.pbs.org/wnet/amanpour-and-company/newsletter/

For more from Amanpour and Company, including full episodes, click here:  https://to.pbs.org/2NBFpjf

Like Amanpour and Company on Facebook: https://bit.ly/2HNx3EF

Follow Amanpour and Company on Twitter: https://bit.ly/2HLpjTI

Watch Amanpour and Company weekdays on PBS (check local listings).

Amanpour and Company features wide-ranging, in-depth conversations with global thought leaders and cultural influencers on the issues and trends impacting the world each day, from politics, business and technology to arts, science and sports. Christiane Amanpour leads the conversation on global and domestic news from London with contributions by prominent journalists Walter Isaacson, Michel Martin, Alicia Menendez and Hari Sreenivasan from the Tisch WNET Studios at Lincoln Center in New York City.

#amanpourpbs
---

# How Conflicts Like the Israel-Hamas War Escalate Hate Speech on Social Media  Amanpour and Company
**CNN - Amanpour - International Politics:** [October 10, 2023](https://www.youtube.com/watch?v=GPoDNYMf8xM)
*  Now, some Israeli parents have been told today to remove social media from their children's [[00:00:00](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=0.0s)]
*  phones with horrific videos spreading online and, of course, for fear of grisly hostage [[00:00:05](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=5.68s)]
*  videos emerging. [[00:00:10](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=10.72s)]
*  The misinformation and hate speech that can fill these platforms create a dangerous environment [[00:00:12](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=12.48s)]
*  that extends well beyond the internet. [[00:00:17](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=17.88s)]
*  Imran Ahmed is the CEO of the Center for Countering Digital Hate, and he's joining [[00:00:20](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=20.080000000000002s)]
*  Hari Sreenivasan to discuss the challenges of regulating this damaging content. [[00:00:24](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=24.8s)]
*  This interview is part of Exploring Hate, our ongoing series on anti-Semitism, racism [[00:00:30](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=30.52s)]
*  and extremism. [[00:00:36](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=36.04s)]
*  Christiane, thanks. [[00:00:37](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=37.04s)]
*  Imran Ahmed, thanks so much for joining us. [[00:00:38](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=38.04s)]
*  Your Center for Countering Digital Hate works kind of overtime when it comes to wars and [[00:00:40](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=40.92s)]
*  propaganda campaigns that are happening. [[00:00:47](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=47.44s)]
*  What are you seeing online over the past 48, 72 hours as this war has broken out? [[00:00:49](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=49.48s)]
*  We've seen a wave of disinformation that has really disfigured the information on social [[00:00:57](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=57.279999999999994s)]
*  media platforms. [[00:01:01](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=61.879999999999995s)]
*  I mean, it's kind of unusable at the moment. [[00:01:02](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=62.879999999999995s)]
*  We're seeing two different things. [[00:01:06](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=66.52s)]
*  We've got bad actors, and that's foreign states, it's extremists, both foreign and domestic. [[00:01:08](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=68.64s)]
*  There are clout chasers, people who are literally trying to sort of get as much engagement as [[00:01:16](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=76.04s)]
*  possible to boost their followers, which is incredibly cynical, but it's a business model [[00:01:20](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=80.44000000000001s)]
*  for some people. [[00:01:24](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=84.72s)]
*  And also people who just enjoy causing pain, so hate actors, trolls, et cetera. [[00:01:25](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=85.72s)]
*  And then we've got a bad platform problem. [[00:01:30](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=90.72s)]
*  We have platforms in which their algorithms amplify the most extreme content, which gets [[00:01:33](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=93.08000000000001s)]
*  the most emotion, and so that will get amplified into lots of news feeds. [[00:01:38](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=98.0s)]
*  They don't enforce their rules. [[00:01:41](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=101.80000000000001s)]
*  And in the very worst case, in the case of X, what's known as Twitter, Elon Musk himself, [[00:01:43](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=103.44s)]
*  the owner, told people to follow an anti-Semite and a disinformation actor to get the real [[00:01:50](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=110.68s)]
*  truth. [[00:01:56](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=116.92s)]
*  So you see this tidal wave of disinformation amplified by the way the platforms work, and [[00:01:57](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=117.92s)]
*  it's really overwhelming us. [[00:02:05](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=125.2s)]
*  But I don't think we're ready for what happens next, because these platforms, which have [[00:02:06](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=126.80000000000001s)]
*  been shedding trust and safety staff hand over fist in the last few months and years, [[00:02:11](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=131.88s)]
*  think what will happen once Hamas start live streaming executions? [[00:02:18](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=138.56s)]
*  Do you think that that is plausible or technologically? [[00:02:25](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=145.0s)]
*  I don't know actually the way to stop it. [[00:02:28](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=148.88s)]
*  I mean, YouTube says that they've gotten better algorithmically at trying to figure it out, [[00:02:30](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=150.6s)]
*  but usually, I mean, you can't predict what is going to be on a live stream. [[00:02:35](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=155.0s)]
*  So I don't know how you stop it. [[00:02:39](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=159.84s)]
*  And I think that's the problem is that the platforms have never spent any time thinking [[00:02:42](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=162.28s)]
*  about safety by design. [[00:02:46](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=166.64s)]
*  They have prioritized the amount of content that they can get out there. [[00:02:49](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=169.6s)]
*  They can place ads next to you. [[00:02:55](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=175.24s)]
*  But, you know, I've just come from a meeting with advertisers where I've warned them that [[00:02:57](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=177.51999999999998s)]
*  we're going to be seeing adverts next to a child, an elderly person, a woman being hurt. [[00:03:02](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=182.64s)]
*  And I don't think we're ready for the way in which corporate monetized disinformation [[00:03:12](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=192.67999999999998s)]
*  is going to disfigure our society, our geopolitics in this instance, even further. [[00:03:20](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=200.39999999999998s)]
*  Yeah. [[00:03:25](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=205.28s)]
*  So I want to point out, I know that Elon Musk, I think, deleted those specific tweets. [[00:03:26](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=206.39999999999998s)]
*  And then he tweeted, as always, please try stay as close to the truth as possible, even [[00:03:31](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=211.84s)]
*  for stuff you don't like. [[00:03:37](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=217.04s)]
*  Now, I mean, going to the platform that is Axe, that is controlled by him, his role in [[00:03:38](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=218.08s)]
*  spreading disinformation seems singular. [[00:03:45](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=225.04s)]
*  I mean, you don't see Sundar Pichai or Mark Zuckerberg or anyone on their platforms doing [[00:03:47](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=227.52s)]
*  the kind of stuff that he does. [[00:03:53](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=233.84s)]
*  But he's a personification of a systemic problem. [[00:03:56](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=236.56s)]
*  Like, you know, he's a single figure that we can focus our attention on. [[00:03:59](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=239.52s)]
*  I have no real issue with Elon Musk the man. [[00:04:02](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=242.88s)]
*  I'm sure that he is a quite brilliant engineer. [[00:04:05](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=245.44s)]
*  And I'm sure that to his many, many children, he's a great dad. [[00:04:09](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=249.28s)]
*  But, you know, I don't know about the industry that he represents. [[00:04:13](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=253.12s)]
*  And really what he represents is the indifference to human safety, the indifference to the harm [[00:04:17](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=257.68s)]
*  done on these platforms and the greed that prioritizes advertising bucks over human safety. [[00:04:24](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=264.24s)]
*  And I think that that's why he's been the focal point, because he is saying out loud [[00:04:32](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=272.4s)]
*  what Mark Zuckerberg, the head of Facebook and Instagram, Sundar Pichai, the head of [[00:04:37](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=277.59999999999997s)]
*  Google and YouTube, haven't ever said out but actually do all the time, which has put [[00:04:43](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=283.03999999999996s)]
*  profits before people. [[00:04:49](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=289.2s)]
*  So in the case of Axe, now there's been in the past six months a sort of total flip. [[00:04:51](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=291.59999999999997s)]
*  I once had a verified blue checkmark because I was a journalist. [[00:05:00](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=300.24s)]
*  I proved who I was. [[00:05:04](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=304.32000000000005s)]
*  That checkmark went away and was replaced by people who were willing to pay a few dollars [[00:05:05](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=305.76000000000005s)]
*  a month for that checkmark, myself not included. [[00:05:10](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=310.48s)]
*  So what does that do to how we think of a fact or something that's verifiable? [[00:05:13](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=313.12s)]
*  So this is something that's been going on for some years. [[00:05:21](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=321.12s)]
*  And when I set up CCDH seven years ago, what had become apparent to me was that the primary [[00:05:23](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=323.6s)]
*  place in which societies around the world now do things like share information, set [[00:05:29](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=329.12s)]
*  and develop our norms of attitude and behavior, negotiate our values, but most importantly, [[00:05:36](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=336.0s)]
*  negotiate the information that we decide to call facts had shifted to online spaces, to [[00:05:42](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=342.24s)]
*  groups, to the discourse that was controlled by social media algorithms. [[00:05:49](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=349.6s)]
*  And that what was being prioritized, one of the most extreme voices, to the lens through [[00:05:55](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=355.2s)]
*  which you see the world was actually distorted to bring the fringes further towards the middle. [[00:06:00](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=360.24s)]
*  But without any warning, like you would get in a rear view mirror, things may appear closer [[00:06:10](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=370.0s)]
*  than they really are. [[00:06:14](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=374.56s)]
*  Actually, what you were being told was this is the global debate. [[00:06:16](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=376.0s)]
*  And as a result, a lot of people were fooled into thinking that things that really are [[00:06:20](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=380.56s)]
*  quite fringe are quite mainstream and it has reshaped our politics globally as a result. [[00:06:24](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=384.4s)]
*  Now, that's true. [[00:06:29](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=389.2s)]
*  That's affected journalists, politicians, as well as members of the public. [[00:06:30](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=390.0s)]
*  So none of us can say that we saw the problem and that we dealt with it at the time. [[00:06:33](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=393.44s)]
*  So when this misinformation and sometimes disinformation spreads as rapidly as it does [[00:06:37](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=397.84s)]
*  on these social platforms, walk us through the consequences here. [[00:06:45](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=405.44s)]
*  What does that do to, I guess, the consumer of the information? [[00:06:50](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=410.96s)]
*  But what does that also do to kind of actors in the field if they also fall prey to it? [[00:06:55](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=415.36s)]
*  So, I mean, there are sort of different modes of disinformation. [[00:07:01](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=421.12s)]
*  There's the disinformation that is drip fed over time, the drip, drip of disinformation [[00:07:04](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=424.8s)]
*  that recolors the lens through which someone sees the world, makes them see the world in [[00:07:09](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=429.2s)]
*  a different way and gives them the precursors to hate. [[00:07:15](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=435.12s)]
*  You see, lies and hate have always been deeply interconnected. [[00:07:18](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=438.8s)]
*  They actually reflexively, so lies underpin hate. [[00:07:24](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=444.64s)]
*  They create the conditions for hate. [[00:07:28](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=448.72s)]
*  Then they lead to the operationalization of hate. [[00:07:30](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=450.8s)]
*  They lead to, they give people the motivation to act in a hateful way as well. [[00:07:35](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=455.12s)]
*  The creation of fake emergencies of threats to life and say, well, we must do something [[00:07:39](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=459.68s)]
*  about these people that I've been telling you about for a long time. [[00:07:46](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=466.72s)]
*  So the truth is that, and no one knows that better than Jewish people, believe me, [[00:07:49](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=469.52000000000004s)]
*  whether it be the blood libel 2000 years ago, or it be the protocols of the elders of Zion [[00:07:53](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=473.84000000000003s)]
*  that informed Adolf Hitler's ideology. [[00:08:00](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=480.0s)]
*  Lies have always been a critical part of hate. [[00:08:04](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=484.88000000000005s)]
*  Now, we've worked with platforms over years, urging them to adopt the most strict standards [[00:08:08](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=488.88000000000005s)]
*  possible of not amplifying or monetizing the lies that underpin hate, the incredibly familiar [[00:08:14](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=494.16s)]
*  conspiracy theories, because we know they lead directly to people doing terrible things. [[00:08:21](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=501.6s)]
*  Let's take the great replacement theory, which is the theory that Jews are trying to bring in [[00:08:29](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=509.76000000000005s)]
*  Muslims and black people to destroy the white race through migration. [[00:08:34](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=514.96s)]
*  That led directly to the slaughter of Jews in Pittsburgh, at the Tree of Life synagogue. [[00:08:38](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=518.5600000000001s)]
*  It led directly to the slaughter of Muslims in Christchurch, that very conspiracy theory. [[00:08:44](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=524.0s)]
*  And it led to the murder of my colleague, Joe Cox, MP, a 35-year-old mother of two [[00:08:49](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=529.36s)]
*  in the British EU referendum, which started me on my journey in this work seven years ago. [[00:08:55](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=535.12s)]
*  So now, I guess in the past 72 hours, have you seen examples of anti-Jewish social media content, [[00:09:02](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=542.0s)]
*  anti-Arab, anti-Palestinian? [[00:09:11](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=551.12s)]
*  There's an overwhelming wave of it. And the truth is that because platforms in the last few years [[00:09:15](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=555.04s)]
*  have become worse, they've reduced the amount of staff that they have working on trust and safety, [[00:09:20](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=560.16s)]
*  and because they've become less transparent, in part as a reaction to the growing awareness [[00:09:25](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=565.44s)]
*  amongst legislators, the media, and others that these platforms are actually quite problematic [[00:09:31](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=571.44s)]
*  at times. They've actually made themselves harder to study. And in the most extreme cases, [[00:09:36](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=576.56s)]
*  suing people who try to study them, as X has to us, that actually it's very difficult for me to [[00:09:43](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=583.3599999999999s)]
*  be able to quantify in this particular war what things actually look like in terms of the overall [[00:09:50](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=590.2399999999999s)]
*  universe of disinformation. What I can say, though, is that if you speak to anyone, as you and I have, [[00:09:57](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=597.1999999999999s)]
*  and you and I both use social media, if you look at your news feed, it's almost unusable, [[00:10:03](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=603.68s)]
*  because there's such a huge amount of disinformation intermingling seamlessly [[00:10:09](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=609.4399999999999s)]
*  with good information that it makes it almost a job unto itself to read social media and try [[00:10:15](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=615.28s)]
*  and work out what is actually true here. It's the first conflict I can remember where my first [[00:10:22](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=622.8s)]
*  instinct to switch on social media, very quickly I switched it off and turned on instead CNN [[00:10:30](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=630.4s)]
*  or the BBC, because I needed to have access to high quality, maybe not super, super up-to-date, [[00:10:38](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=638.16s)]
*  but still timely fact-checked and well-curated information. [[00:10:45](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=645.68s)]
*  I don't want you to amplify disinformation or misinformation that you're seeing, but give me [[00:10:51](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=651.1999999999999s)]
*  some examples of the kinds of things that you're seeing online now. [[00:10:55](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=655.68s)]
*  We have a policy internally not to talk about individual memes and to only ever talk about [[00:11:01](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=661.12s)]
*  themes, but we are seeing things which dehumanize Israelis and dehumanize Jews. It is the typical [[00:11:07](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=667.1999999999999s)]
*  stuff which will be to inflate the numbers of dead on one side, to deflate the numbers of dead [[00:11:15](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=675.76s)]
*  on the other side. But there are a lot of images and the truth is it's been very difficult to pass [[00:11:22](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=682.64s)]
*  between reality and falsehood. But the truth is that we're also seeing real images which are [[00:11:27](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=687.68s)]
*  beyond human understanding, atrocities, women raped, people murdered, and we're also seeing [[00:11:34](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=694.88s)]
*  people reacting to those with joy. Now, in any other walk of life, if you reacted to seeing [[00:11:43](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=703.6s)]
*  someone being murdered brutally by yippeeing and jumping for joy, if you did it in a pub, [[00:11:50](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=710.96s)]
*  on the street, anywhere else, you very quickly realize that there are consequences for behavior [[00:11:57](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=717.2s)]
*  like that. On social media, what it gives you is more amplification, more clout, more followers, [[00:12:02](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=722.32s)]
*  more money. That is a series of dangerous incentives which are misaligned with the public good. [[00:12:08](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=728.96s)]
*  Now, I should mention that Elon Musk's platform basically is trying to sue you and they say that [[00:12:19](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=739.9200000000001s)]
*  you are unjustly, I'm just quoting from their blog here, targeting people you don't agree with, [[00:12:27](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=747.2s)]
*  attempting to affect their business by attacking free speech and illegally gained people's passwords. [[00:12:32](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=752.24s)]
*  So, I know you have to respond legally in some cases, but what can you say about not just their [[00:12:38](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=758.96s)]
*  lawsuit, but what was the initial report that drew this action? The initial report that drew [[00:12:44](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=764.64s)]
*  the action was a study that we did on Twitter looking at the number of times that seven of the [[00:12:50](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=770.08s)]
*  most extreme slurs against black people, the N-word, against gay people, against women, [[00:12:56](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=776.72s)]
*  against Jews, were used on the platform on a daily basis, on average, how often a day, in the year [[00:13:04](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=784.4s)]
*  before he took over and the month after he took over. And what we found was that the use of the [[00:13:10](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=790.32s)]
*  N-word tripled after he took over because he put up the back signal, did he not, to hate hackers. [[00:13:15](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=795.7600000000001s)]
*  He let thousands of them back onto the platform who'd previously been suspended. He said, this is [[00:13:23](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=803.36s)]
*  a free speech zone. And in doing so, he gave them license to be as racist as possible, saying, [[00:13:28](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=808.32s)]
*  we're not really going to enforce our rules anymore. And as a result, there was an increase [[00:13:34](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=814.5600000000001s)]
*  in hate and we quantified it. Now, I think that that was basically us putting up a mirror to his [[00:13:38](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=818.48s)]
*  platform and saying, do you like the reflection you see in it? And whereas most people, you or I, [[00:13:43](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=823.76s)]
*  we don't like the reflection of the mirror, we comb our hair or go on a diet, he sued the mirror. [[00:13:50](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=830.5600000000001s)]
*  He said, this mirror, how dare you show my image to be ugly because I must be perfect for I am Elon [[00:13:56](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=836.88s)]
*  Musk. And I think that's the problem. He's literally suing us because he's annoyed that [[00:14:04](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=844.4s)]
*  we reflected back the reality of his platform to him. This big question of, you know, he's taking [[00:14:09](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=849.12s)]
*  us to court. We have absolutely every bit of confidence we'll defeat him in court and that the [[00:14:14](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=854.64s)]
*  truth, thankfully, that the truth still matters in the courts of the United States. And so we hope [[00:14:20](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=860.72s)]
*  that when he takes his argument off Twitter, where he is king of the castle and he can say [[00:14:31](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=871.12s)]
*  whatever nonsense he wants and literally reprogram the algorithms to boost himself in a court, [[00:14:35](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=875.28s)]
*  I have an equal voice. You know, Jim Jordan, a representative in the US House, he also subpoenaed [[00:14:42](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=882.4s)]
*  your organization because he believed that you were working with the government and big tech to [[00:14:48](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=888.88s)]
*  censor Americans. You've turned over those documents. [[00:14:56](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=896.64s)]
*  We've done it. We sent him about 100 emails. So in total, over three or four years, we've had [[00:14:59](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=899.68s)]
*  a hundred emails or so between us and the government, both the Trump administration and [[00:15:05](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=905.28s)]
*  the Biden administration. With the Trump administration, we worked on reducing the [[00:15:10](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=910.56s)]
*  amount of antisemitism online. And, you know, Mike Pompeo wrote me a very nice letter saying, [[00:15:14](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=914.88s)]
*  thank you for the work you're doing combating hate. And we've had similarly cordial but not [[00:15:19](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=919.2s)]
*  friendly conversations with the government. The truth is, I'm very critical of governments [[00:15:24](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=924.08s)]
*  all around the world for failing to get to grips with social media and the harms there. [[00:15:28](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=928.48s)]
*  Now, the EU and UK have legislated now. I'm very proud of that. [[00:15:32](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=932.64s)]
*  We don't take any money from governments or from tech companies. We are that rarest of flowers, [[00:15:36](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=936.0s)]
*  an independent organization that represents the people. [[00:15:42](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=942.96s)]
*  I wonder, considering how you are working with legislators in either the UK or the EU, [[00:15:48](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=948.1600000000001s)]
*  what is the role for the regulatory environment and what can the United States do? Because [[00:15:55](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=955.44s)]
*  it seems that across the pond, so to speak, there are attempts to try to figure this out, [[00:16:02](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=962.0s)]
*  whereas in the US, not so much. It's really interesting, actually. [[00:16:08](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=968.24s)]
*  Five or six years ago, I said to the British government, if you could just give me a law [[00:16:14](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=974.0799999999999s)]
*  that said that if the companies have to abide by their own rules, I would go off and retire [[00:16:18](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=978.88s)]
*  to Antigua with my cat and leave you alone. And five or six years later, they've delivered [[00:16:25](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=985.92s)]
*  us something which is really interesting. It basically says there are four components [[00:16:32](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=992.0s)]
*  to safety online, to a healthier and more productive and more prosperous existence online, [[00:16:36](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=996.96s)]
*  transparency. And without transparency, you can't have meaningful accountability to a [[00:16:44](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=1004.1600000000001s)]
*  democratic body. Now, Lindsey Graham and Elizabeth Warren actually have a bill, a joint bipartisan [[00:16:50](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=1010.32s)]
*  bill for a new accountability regulator in the US. The UK and EU have one now, but you need the [[00:16:55](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=1015.76s)]
*  transparency, otherwise they're not looking at the correct data. And then if there are harms [[00:17:01](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=1021.92s)]
*  and the platforms are clearly negligent in not doing anything about it, they should be held [[00:17:07](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=1027.04s)]
*  partly responsible because then you create an economic disincentive for them doing the wrong [[00:17:12](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=1032.24s)]
*  thing. Now, across all three of those components, transparency, accountability and responsibility, [[00:17:17](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=1037.44s)]
*  if you get them in place, then you actually don't need them anymore because you create a culture of [[00:17:23](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=1043.2s)]
*  safety by design. And that's what every other industry has. Every other industry has to think [[00:17:28](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=1048.16s)]
*  about safety before they release products. It's uniquely social media companies that believe [[00:17:33](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=1053.52s)]
*  they're free of it. And that star framework, safety by design, transparency, accountability, [[00:17:38](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=1058.16s)]
*  responsibility is what governments around the world are implementing. And we'd like to see it [[00:17:42](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=1062.8000000000002s)]
*  in the US too. I think we're some years away from it, but I'm going to spend the next few years [[00:17:47](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=1067.92s)]
*  showing it's worked overseas. It hasn't restricted freedom of speech. What it's actually made is [[00:17:53](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=1073.36s)]
*  healthier platforms online in which the algorithms, the enforcement, the way that they run [[00:17:59](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=1079.52s)]
*  actually helps humanity, doesn't hinder it. Imran Ahmed, co-founder and CEO of the Center [[00:18:04](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=1084.8s)]
*  for Countering Digital Hate. Thanks so much for joining us. It was my pleasure. [[00:18:11](https://www.youtube.com/watch?v=GPoDNYMf8xM&t=1091.28s)]
